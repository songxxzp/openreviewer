[
    {
        "id": "5L9gkOs9ao",
        "forum": "fLXpXa7iiz",
        "replyto": "fLXpXa7iiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_1wgY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_1wgY"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a proof for the convergence of the Bayesian Bi-level optimization (BBO). By modeling the excess risk of the SGD-trained parameters, a regret bound is established for BBO with EI function, which bridges the analytical frameworks of Bayesian optimization and Bi-level optimization. Moreover, the authors introduce adaptable balancing coefficients to give a sublinear regret bound for BBO the UCB acquisition function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is theoretically solid. Useful regret bounds are provided and a convergence framework for BBO is established.\n\n2. The paper is well-organized. The motivation, technique, proof schemes, and results are clearly stated.\n\n3. Some tricks presented in the paper are interesting. For example, the adaptation of balancing coefficients could be a useful technique in other Bayesian applications."
            },
            "weaknesses": {
                "value": "1. The regularity assumptions are not intuitive. It would be better if the authors provided some real applications and models satisfying these assumptions.\n\n2. Some assumptions are restrictive from the view of optimization, like the Lipschitz continuity and smoothness conditions in Theorem 1. Only a few classes of functions *simultaneously* satisfy them on $\\mathrm{R}^d$."
            },
            "questions": {
                "value": "What is the fundamental difficulty of establishing convergence of BBO compared with other Bi-level algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Non."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2458/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697778946179,
        "cdate": 1697778946179,
        "tmdate": 1699636182116,
        "mdate": 1699636182116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yCjPg4ejXb",
        "forum": "fLXpXa7iiz",
        "replyto": "fLXpXa7iiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_PGhy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_PGhy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the initial theoretical assurance for Bayesian bilevel optimization (BBO). It is proved sublinear regret bounds suggest simultaneous convergence of the inner-level model parameters and outer-level hyperparameters to optimal configurations for generalization capability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work conducts lots of theoretical analysis Bayesian bilevel optimization (BBO). Specifically, a novel theoretical analysis of convergence guarantees for generalization performance within a BBO framework is provided.\n\n2. A regret bound for BBO using the EI function is discussed in this work.\n\n3. A significant advancement in this research lies in the conceptualization of SGD excess risk as a form of noise within the framework of Bayesian optimization. This approach allows for the adjustment of noise assumptions to better match real-world scenarios and greatly simplifies convergence analysis."
            },
            "weaknesses": {
                "value": "I can't find any experimental results in this work. I understand this work puts more attention on the theoretical analysis in Bayesian bilevel optimization (BBO). However, the authors should also conduct experiments to substantiate the theoretical analysis."
            },
            "questions": {
                "value": "My concerns are about the experimental results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2458/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664142288,
        "cdate": 1698664142288,
        "tmdate": 1699636182048,
        "mdate": 1699636182048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mtiRS6jbsE",
        "forum": "fLXpXa7iiz",
        "replyto": "fLXpXa7iiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_66Ye"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_66Ye"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on Bayesian bilevel optimization (BBO), which combines outer-level Bayesian opti- mization for hyperparameter tuning with inner-level stochastic gradient descent (SGD) for model parameter optimization. The paper proves sublinear regret bounds for BBO using expected improvement (EI) and upper confidence bound (UCB) acquisitions. This provides theoretical assurance that BBO enables simul- taneous convergence of parameters and hyperparameters. For EI, the optimal number of SGD iterations is shown to be N \u224d T 2, balancing training and tuning. This achieves regret savings compared to previous works. For UCB, sublinear regret is proven even with fewer SGD iterations, showing UCB is more robust to SGD noise. The UCB balancing coe\ufb00icients are adapted based on the SGD/Bayesian iteration ratio. The analysis bridges the gap between Bayesian and bilevel optimization frameworks by modeling SGD excess risk, which enables adapting convergence guarantees to the BBO setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper provides a new theoretical analysis bridging the frameworks of Bayesian optimization and bilevel optimization by modeling the excess risk of SGD-trained parameters as noise to tackle challenges in convergence guarantees for BBO generalization performance.\n\n(2) Based on a noise assumption better suited to practical situations, the authors derive sublinear regret bounds for Bayesian bilevel optimization using the expected improvement function, which is better than previous work.\n\n(3) By introducing adaptable balancing coe\ufb00icients $\\beta_t$ for the UCB acquisition function, the paper establishes a sublinear regret bound for BBO with UCB that holds with fewer SGD steps, enhancing inner unit horizon flexibility and overcoming limitations of rapidly increasing coe\ufb00icients from previous analyses."
            },
            "weaknesses": {
                "value": "(1) The current paper is primarily theoretical with a lack of numerical experiments on actual data, which limits the persuasiveness. Experiments using real-world hyperparameter tuning tasks could offer tangible evidence of the convergence behavior and help assess how well the assumptions fit such scenarios.\n\n(2) This paper focuses solely on Gaussian process priors for the Bayesian optimization portion, but the choice of prior may significantly impact the convergence guarantees. The current analysis leverages nice properties of GP priors and posters but may not directly extend to other priors that require different proof techniques, which could limit wider applicability.\n\n(3) Bayesian optimization is adopted for hyperparameter tuning at the outer layer, so the algorithm in this paper may require extensive sampling and integration to estimate the posterior distribution, making it computationally demanding and di\ufb00icult to apply to high-dimensional complex problems."
            },
            "questions": {
                "value": "(1) How do the convergence guarantees extend to deep neural network training? Are there any unique challenges posed by DNNs?\n\n(2) For the inner-level SGD, will using SVRG or introducing acceleration techniques lead to better corresponding results compared to standard SGD?\n\n(3) Do assumptions such as the bounded RKHS norm of the objective function correspond cleanly to properties of other priors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2458/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677521427,
        "cdate": 1698677521427,
        "tmdate": 1699636181954,
        "mdate": 1699636181954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EcsIXG5c5l",
        "forum": "fLXpXa7iiz",
        "replyto": "fLXpXa7iiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_pR2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2458/Reviewer_pR2Q"
        ],
        "content": {
            "summary": {
                "value": "This is paper presents the first convergence analysis of Bayesian bilevel optimization where the outer level is hyperparameter tuning and the inner level is SGD. The key results are sublinear regret bounds showing the convergence behaviors of both outer and inner optimization problems. The key technical novelty is modeling the excess risk of SGD training as the noise of the outer Bayesian optimization. This paper doesn\u2019t have experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. First convergence analysis of BBO is important, which is the main contribution of this paper.\n2. I appreciate the innovation that modeling the excess risk of inner level SGD-trained parameters as the primary noise source of outer-level BO. It makes great sense in this problem setting.\n3. I like \u201cpractical insights\u201d sections, which are helpful.\n4. The whole paper is well written and easy to follow except some notation problems mentioned below."
            },
            "weaknesses": {
                "value": "1. Motivation of BBO is not very clear. No detail is shown in \u201csignificant promise in engineering applications\u201d in 2nd paragraph of Introduction.\n2. Convexity assumption in Definition 1 is strong. How can you assume the loss function is convex given potentially non-convex objective function? I want to learn more justification from the author.\n3. L is taken as both loss function and Lipschitz constant, which introduces some confusion.\n4. Upper bound in Theorem 1 is too vague, only showing dependence on N. How does it depend on other terms?"
            },
            "questions": {
                "value": "1. Why is modeling the noise as a martingale difference a key limitation? Why does this approach not align with hyperparameter optimization? See fourth line of page 2.\n2. In third line of Section 3.2, you assume function L has a uniquely determined value for each \\lambda. In my understanding, it is needed otherwise some \\theta rather than \\theta* may lead to lower value given some \\lambda and it would be hard to define regret. However, do you have more justification on this assumption especially in practical scenarios?\n3. What\u2019s $\\varphi(N)$ in Theorem 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2458/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2458/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2458/Reviewer_pR2Q"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2458/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774991264,
        "cdate": 1698774991264,
        "tmdate": 1699636181859,
        "mdate": 1699636181859,
        "license": "CC BY 4.0",
        "version": 2
    }
]