[
    {
        "id": "sHsp83idlY",
        "forum": "wfgZc3IMqo",
        "replyto": "wfgZc3IMqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_prcP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_prcP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach for the task of classification with noisy labeled training data. \nThe proposed method first uses a bijective function $f$ to map the discrete labels into a continuous label space. \nWith continuous labels, the method can be naturally formulated as a regression problem, the loss reweighting and label correction techniques are used for the regression task to uncover the true continuous label. \nLastly, the inverse of bijective function $f$ is used to transform the predicted true label to true discrete label.\nEmpirical experiments are conducted to show the effectiveness of the proposed method in comparison to existing approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper's presentation of the proposed method is straightforward and easy to follow.\n\n- The empirical experiments are relatively comprehensive."
            },
            "weaknesses": {
                "value": "- The primary limitation of the paper is the absence of a clear rationale behind the decision to transform the classification problem into a regression problem. The paper would significantly benefit from a more in-depth analysis of why this transformation is considered a \"better\" approach for handling noisy labels. Currently, the proposed method appears to be a compilation of established techniques, and addressing this aspect could enhance the quality and impact of the paper.\n\n- The reviewer has a general impression that some key details are missing in the description of the method and the experiments, the reviewer will point them out below.\n\n- Method description\n    - P4: The use of an equal weight vector $u$ in (4) for label smoothing should be clarified. What would be the effect of using a vector of all 1s scaled by other factors? Is this equivalent to adjust $\\gamma\"? The authors are encouraged to provide further clarification on this matter.\n    \n    - P4: Regarding loss reweighting via a Gaussian noise model, there may be a computational challenge with evaluating the density function of a multivariate normal due to the inversion of covariance matrices, especially in high-dimensional cases. It appears that the matrix inversion lemma might have been employed as a workaround. Can the authors please confirm and elaborate on this technique? Additionally, it is not entirely clear how $\\mu_{\\theta}$ and $r_{\\theta}$ correspond to the output of the neural network. Are they concatenated as a long vector, or are they separate outputs with distinct final linear layers?\n\n- Experiments:\n    - In Table 1, it is unclear why the symmetric and asymmetric noise rates are different. Moreover, it does not make sense to the reviewer that a noise rate is more than 50%, please clarify. \n\n    - While Table 1 presents interesting results, there remains a degree of skepticism regarding its persuasiveness. As the authors pointed out that \"SGN significantly outperforms CE when there is no added label noise.\" This prompts a reasonable inquiry into whether the relatively good performance of the proposed method is contingent on the suboptimal performance of the baselines. The authors posit that the EMA framework may contribute to this observed improvement. Consequently, the reviewer suggests conducting an additional experiment similar to the one presented in Table 4 under a noise-free setting to validate this conjecture and bolster the credibility of the results. Such an addition would further fortify the paper's empirical foundation.\n\n    - The description of the experiments could benefit from more clarity. While the reviewer understands there is a space limit for conference paper, providing essential details in the appendix, especially for a paper predominantly reliant on empirical results, is advisable. For instance:\n    \n        - A brief overview of the datasets would greatly assist readers who are new to the field of noisy label classification.\n        \n        - The authors should point out aggregate, random, worst corresponding to three sets of noisy labels in Table 2. Otherwise, the table is difficult to understand for people who does not know the dataset well.\n\n- Minor issues:\n    - P2: The last two sentences in the \"compositional data\" paragraph lack clarity regarding what \"several problems\" and \"this problem\" refer to.\n    - P2: The final sentence in the \"transforms\" paragraph could be more informative. Instead of stating that \"several solutions exist,\" the authors could refer to the specific technique employed later in the paper for a more insightful description.\n    - The expression in (2) on P3 is statistically incorrect. The conditional distribution of the transformed continuous label given features is a normal distribution rather than the mariginal distribution of the label itself.\n    - P5: The first sentence \"we describe our experimental setup of our experiments\" in Section 4, appears redundant.\n    - The LaTeX formatting for quotes, as exemplified by \"worst\" on P7, may require adjustment."
            },
            "questions": {
                "value": "Please refer to the questions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697620821056,
        "cdate": 1697620821056,
        "tmdate": 1699637152815,
        "mdate": 1699637152815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lq5YWDgOuz",
        "forum": "wfgZc3IMqo",
        "replyto": "wfgZc3IMqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_U8oG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_U8oG"
        ],
        "content": {
            "summary": {
                "value": "To address overfitting to label noise, this paper proposes to combine two popular methods together, namely loss reweighting and label correction. Specifically, the approach firstly uses isometric log-ratio transform to convert a classification task to a regression task, which is then solved with a shifted Gaussian Noise Model. The loss reweighting is achieved when learning sigma in the Gaussian Noise Model, and the label correction is implemented by adding a shift to the noise model to make up the difference between the noise target and the correct label. Experiments show that the proposed method outperform previous baselines and related works on both synthetically and naturally noisy datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is interesting and meaningful to use a (shifted) Gaussian Noise Model to achieve loss reweighting and label correction at the same time, which is very different from previous methods, where the two tasks are considered separately.\n2. The experiments show the effectiveness of the method on both CIFAR-10 and CIFAR-100 with both synthetic noise and natural noise."
            },
            "weaknesses": {
                "value": "1. The contribution is limited or needs refinement. To me, the log-ratio transformation, the Gaussian Noise Model for loss reweighting, and the estimation of the shift are all previous works. The authors should be more explicit on the novelty and contributions.\n2. The reason for the choice of the orthonormal basis for the Isometric Log-Ratio Transform is absent. The authors should explain why choosing the Helmert matrix as the basis, and better for analyzing the influence or conducting comparative experiments with different basis.\n3. The experiment results on Clothing1M and WebVision dataset with natural noise seem to fail compared with NAL, which though with a different evaluation setup, as Tabel 3 shows.\n4. As Figure 2 shows, it takes around 400 epochs for the model to converge. However, figure 2 just depicts the train accuracy, and there can be a potential overfitting problem that the authors should consider."
            },
            "questions": {
                "value": "1. It is not very clear to me that \"The authors propose to have a two-layer neural network with parameters \u03b8 to output \\mu and \\sigma per example\" in 3.3. How is the two-layer network trained and how does it work?\n2. The method converts the classification task to a regression task using isometric log-ratio transformation. When optimizing, the gradient is computed in the transformed space instead of the origin space. Under this condition, is the optimization still consistent with usual methods where no such transformation is used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698469076691,
        "cdate": 1698469076691,
        "tmdate": 1699637152697,
        "mdate": 1699637152697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TibitBFieB",
        "forum": "wfgZc3IMqo",
        "replyto": "wfgZc3IMqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_h549"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_h549"
        ],
        "content": {
            "summary": {
                "value": "This paper first uses log-ratio transformation to convert the classification data set into a regression data set. It treats the regression target as a Gaussian distribution with noise, learns the mean and variance of the Gaussian distribution, and then indirectly weights the loss through the variance. Secondly, this paper gives the above noise a shift and treats the regression target as a Gaussian distribution with offset (non-zero mean) noise to estimate the shift. This method indirectly corrects the labels when converting the regression task into a classification task.\n\nThe contributions of this paper include:\n(1) It turns loss reweighting and label correction into a unified method instead of being two independent processes.\n(2) It uses statistics and regression perspectives to look at the label noise problem in classification tasks, which is relatively novel."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The perspective of the paper is novel, using a regression perspective to perform classification tasks. The unified process of loss reweighting and label correction can also be understood as modeling noise.\n- The experiments in the paper are relatively sufficient."
            },
            "weaknesses": {
                "value": "- The experimental results is lower than SOTA performance, e.g., DivideMix, ELR, etc.\n- There exist some unified methods that combines loss reweighting and label correction to enhance robustness against label noise, e.g., CMW-Net [1]. \n-  What's merit of the regression task. Some strong results and theoretical explanations are necessary.\n-  The hyperparameter $\\alpha$ is sensitive in Fig.2(c).\n\n[1] Shu J, Yuan X, Meng D, et al. Cmw-net: Learning a class-aware sample weighting mapping for robust deep learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
            },
            "questions": {
                "value": "- $\\sigma^2$ of formula 6 should be written as $\\sigma_\\theta^2$.\n- What are the advantages of using regression to solve classification tasks rather than directly solving classification tasks? Would it be too cumbersome to convert the classification problem into a regression problem, and then convert the regression problem into a classification problem?\n- The previous methods regard loss reweighting and label correction as two independent processes. So what is the advantage of treating them as a unified method in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9158/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9158/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9158/Reviewer_h549"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678910720,
        "cdate": 1698678910720,
        "tmdate": 1700577050468,
        "mdate": 1700577050468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fYNkPapkML",
        "forum": "wfgZc3IMqo",
        "replyto": "wfgZc3IMqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_QVE6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9158/Reviewer_QVE6"
        ],
        "content": {
            "summary": {
                "value": "The paper presented a method adapted from log-ratio transform approach from compositional data analysis for classification with label noise. The method can be viewed as a combination of two widely studied categories of approaches in the field: loss reweighting and label correction. The authors performed extensive experiments and achieved competitive performance across several synthetic and real-world datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is a refreshing adaptation of a well-founded statistical technique (log-ratio transform) in a modern research problem in machine learning (classification with label noise). It should inspire a new line of work to the research area while adding a competitive baseline to other future work. The writing is well done and very easy to follow. The contribution is clearly presented and justified by drawing comparison to related work."
            },
            "weaknesses": {
                "value": "My main criticism is on the paper is in its experimental setup and results. First of all, I absolutely do not believe a paper deserves to be published only if it achieves SOTA results. But unless I missed something important (e.g., did the authors reimplement the method and rerun the numbers perhaps?) it seems that the selected baselines to report in the paper are a very biased set in an attempt to make the method look SOTA. More details in the Questions section below."
            },
            "questions": {
                "value": "#### Major comments:\n\nThe proposed method appears in most result tables as the best-performing method. However, a closer look raises a few questions regarding selection bias in the reported baselines. Taking ELR (Liu 2020) as an example (as I did not go through all the baselines).\n(a) At 60% symmetric noise in CIFAR10, Table 1 in the original ELR paper reports 86.12% mean accuracy while Table 1 in the current paper 81.87% which is significantly lower than the original authors' numbers.\n(b) The original ELR paper proposes a variant with a few add-ons, ELR+, which can outperform ELR but the current authors did not include. Admittedly such tricks should not be the main contribution of the ELR, but the reported SGN in the current paper also employed a few tricks (e.g., EMA) to improve upon its vanilla version. According to Table 5 of the original ELR paper, by adding some form of weight averaging alone, ELR's vanilla performance can be boosted by a few percentage points too.\n(c) There are many other baselines in learning with label noise, such as DivideMix, which can achieve much higher accuracy. For reasons that are not disclosed, such methods are simply not included in the current paper. If comparison to these methods do not make sense or is unfair, disclosing the rationale behind why some methods are selected over others as baselines in the paper still makes sense to me.\n\n#### Minor comments:\n\nAppendix A.6 is a very interesting discussion and adds a lot of value to the understanding of the method in the context of related works. I'd argue that it should be moved to the main text, while some results/tables can be moved to appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690885103,
        "cdate": 1698690885103,
        "tmdate": 1699637152461,
        "mdate": 1699637152461,
        "license": "CC BY 4.0",
        "version": 2
    }
]