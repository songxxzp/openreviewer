[
    {
        "id": "rjvcTxTQ0N",
        "forum": "xnhvVtZtLD",
        "replyto": "xnhvVtZtLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of local fairness, which ensures the model fairness on the subregion split via unknown attributes.  They proposed an approach based on the idea of distributional robust optimization, under an adversarial learning paradigm. The desired fairness is implemented of instance-wise sample reweights and the experiments are conducted to demonstrate the proposed method can achieve Pareto dominance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The local fairness introduced in this paper is interesting, and the analysis to existing literature is almost thorough and clear. The method looks good, and the writing is well prepared."
            },
            "weaknesses": {
                "value": "I have some doubts about concepts and formulations after reading the paper. Also, the technical contribution may need further clarifications."
            },
            "questions": {
                "value": "1.\tThe motivation of this paper is that global fairness methods usually cannot guarantee a consistent fairness value on split subregions, shown as Fig. 1. However, such observations are based on fine-grained partition (e.g., age categories) of training data and each subregion might be with a relatively small size. In this case, the disparity tends to happen. For example, DI might be very large if a bin only includes two samples. How do we confirm that biases do widely exist and then precisely define \u201clocal fairness\u201d?\n2.\tFollowing question 1, I think it might be necessary to use some prior for split subregions. For example, in DRO, they introduced a lower bound of the subgroup ratio. With accuracy as a utility, BPF demonstrated that very small group would lead to uniform classifier. But from sec 3.2, as Softmax weights can be regarded as applying a Shannon entropy regularization. It would be better if the authors can point out such behind insights.\n\n        Blind Pareto Fairness and Subgroup Robustness, ICML 2021.\n\n3.\tStarting from Eq. (2), the input of $g_{w_g}(.)$ directly takes the output of $f_{w_f}(.)$. So, how do you implement $w_g$? Considering a binary case, $f_{w_f}(.)$ is a only probability in output space. \n4.\tFollowing question 3, please check the constraint of eq. (4), where s should be properly positioned.\n5.\tAbove Eq. (4), why distribution $q$ can help characterize local fairness is not clear for me. Please note that in DRO, it serves any subregion\u2019s upper bound.\n6.\tSince $\\lambda_g$ is not learnable (Sec A.5.4), are the best results selected to have a Pareto dominance as claimed?\n7.\tPlease justify the novelty/contribution compared to Michel et al 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6517/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6517/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6517/Reviewer_Cpzk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672425162,
        "cdate": 1698672425162,
        "tmdate": 1700713485798,
        "mdate": 1700713485798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ml5Ip7Sum",
        "forum": "xnhvVtZtLD",
        "replyto": "xnhvVtZtLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_34eA"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on a notion of local fairness as opposed to group fairness. When local subgroups are unknown at training time, it is difficult to enforce local fairness constraints. This paper expands the method DRO to include an adversarial learning component that specifically minimizes the ability of an adversary to reconstruct sensitive attributes. The main idea is to boost the importance of regions where sensitive reconstruction is easiest. This is done at each optimization step. To do this efficiently, the paper introduces an adversarial importance re-weighting method. They provide the analytical formulation for this method as well as two implementations (one non-parametric and one parametric). Finally, they experimentally evaluate the proposed method including ablation studies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The main contribution is novel and clearly specified. Combining DRO with adversarial learning is simple yet effective.\n\n(2) The paper is written with clarity and exposition is easy to follow throughout. Problem statement as well as the two implementations are concise."
            },
            "weaknesses": {
                "value": "(1) The paper does not directly address limitations. For instance, is there a computational scalability issue with this technique compared to other more well known group fairness methods?\n\n(2) The reference to applied fairness research could be strengthened. There are many related works that are omitted. In particular, work on fairness and adversarial learning."
            },
            "questions": {
                "value": "(1) Is there a reason why the first paragraph of the introduction omits citations in statements like \"models...have been shown to unintentionally perpetuate existing biases and discriminations\"? \n\n(2) What do you see as the main use case for this method? What are the limitations you anticipate other than sensitive attribute not being available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685808840,
        "cdate": 1698685808840,
        "tmdate": 1699636732295,
        "mdate": 1699636732295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iSpcGelT0M",
        "forum": "xnhvVtZtLD",
        "replyto": "xnhvVtZtLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_bLrb"
        ],
        "content": {
            "summary": {
                "value": "Most prior work on group fairness measure the fairness discrepancy using global averages across pre-defined groups (e.g. males vs females).  However, in practice, one may care about fairness discrepancies in sub-regions of the feature space (e.g. males and females belonging to a particular age group). In this paper, the authors enforce fairness across such local sub-regions, by employing a form of distributionally robust optimization, equipped with an adversary that seeks to predict the sensitive attribute from the model predictions. Importantly, the proposed method does *not* assume the sub-regions to be known during training time, and instead optimizes over a set of possible sub-regions defined by an uncertainty set."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A practically relevant problem statement with a clearly presented problem formulation\n- The solution approach presented makes for a satisfactory contribution\n- Strong experimental results\n- Well-written paper"
            },
            "weaknesses": {
                "value": "- The problem formulation seems *overly complex*, with multiple levels of nested minimization problems. It is unclear if the complexity is absolutely necessary (see question 1).\n- Justifications needed for choice of uncertainty set $\\mathcal{Q}$.\n- *Lack of strong convergence guarantees* for the proposed optimization strategy. In fact, owing to the nested nature of the optimization problem, it appears (please correct me if I am incorrect) that it would be hard to provide theoretical guarantees even if under a simplistic scenario when $f$ and $g$ are linear models."
            },
            "questions": {
                "value": "(1) **Complexity of problem for formulation**: The authors seek to impose a robust version of the demographic parity constraint in (3), by introducing a maximization over an uncertainty set in the constraint. Instead of directly seeking to solve this problem, they formulate a equivalent problem with an additional argmin on an adversary's reconstruction loss (4). *Could authors elaborate why they chose to work with (4) instead of the simpler form in (3)?* It appears that the trick of applying instance-level weighting function $r$ could be applied as well to solving (3).\n\n(2) **Assumptions on $q$**: Could the authors formally state the assumptions on the class of perturbed distributions $q$, preferably early on in the paper (e.g. Sec 2). My understanding is that the group priors are required to be the same for both q and p, i.e. $q(s) = p(s)$. However, its unclear if the validity constraint described in Sec 3.1 would include **all distributions $q$ for with the same group priors as $p$**. It might be cleaner to have the assumptions on $q$ described first, and then argue why the particular choice of weighting functions $r$ satisfy those assumptions.\n\n(3) **Rationale for assuming equal group priors:** Does the assumption $q(s) = p(s), \\forall s$ amount to saying e.g. that the proportion of male and female samples would be the same across all sub-regions (e.g. across all age groups)? If so, is that a reasonable assumption to make? I think the authors need to better explain how their choice of uncertainty set aligns with practical applications where one may want to enforce the form of local fairness they describe.\n\n(4) **Distribution shift in Sec 4.2:** In the experiments with the Adult/census distribution shift in Sec 4.2, do we have reasons to believe that uncertainty set used would capture the \"real-world\" distribution shift considered? I am again particularly curious about the relevance of the assumption $q(s) = p(s)$ here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698903352999,
        "cdate": 1698903352999,
        "tmdate": 1699636732173,
        "mdate": 1699636732173,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rusTFuZqEn",
        "forum": "xnhvVtZtLD",
        "replyto": "xnhvVtZtLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_d8Gj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6517/Reviewer_d8Gj"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses a practical issue in methods that ensure group fairness in machine learning models. The methods that aim for group fairness can often be unfair to a subpopulation. To this end, authors combine adversarial learning and distributionally robust optimization (DRO) to learn globally fair classification models. DRO reweighs the sample, assigning more weights to samples that are easier to adversary. This ensures that fairness constraints would hold on even worst-case subgroups. Notably, the proposed method does not use subgroup labels. They propose two methods for reweighing the samples --- One exploits the loss of adversary to compute the weights (BROAD), and the other uses a separate neural network (which is optimized alongside during training) to compute the weights (called ROAD). \n\nThe authors performed experiments on three fairness datasets. They consider worst-case demographic parity on subgroups as a fairness constraint in one scenario. In the other scenario, they learn the fair model under distribution shift and use equalized odds as the fairness measure. In both cases, they show that ROAD provides a better trade-off between accuracy and fairness than other methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses an important practical issue in group fair learning approaches and is a step towards ensuring the reliability of fair learning methods. \n- ROAD is consistently better than the other approaches for different levels of fairness and all datasets. In particular, I like using trade-off curves to report the results. \n- The setting does not require specifying subgroups during training and can be fair w.r.t. to any choice of subgroup due to worst-case distribution consideration. To this end, combining DRO with adversarial learning is interesting."
            },
            "weaknesses": {
                "value": "- **Baseline**: In fig 3 and 4, some of the baselines do not span all ranges of fairness. Why do we see this behavior? Are the baselines tuned correctly?\n- The proposed approach can be tricky to implement with several additional hyperparameters and the adversarial nature (min-max optimization) of the problem. \n- **Results**: The experiments and reported results could be more elaborate:\n  - How does DRO affect global fairness compared to other baselines? Fig 3 only reports local fairness in the form of the worst DI. However, the global fairness results are not reported.  \n  - While extending this framework to EO measure is straightforward, results with EO and local fairness are not reported. Similarly, DI and distribution shift results would strengthen confidence in the method."
            },
            "questions": {
                "value": "- **ROAD vs BROAD**: It is quite unintuitive that BROAD, an exact solution, performs poorly than ROAD, which uses a neural network to predict weights. The authors argued that ROAD is better because of the smoothing effects. Could it be that what is easy and hard for adversaries may change rapidly (or adversaries' output may be noisy) due to noisy batch gradients, and using an NN in ROAD may just be smoothing that out? In that case, would it make sense to use an EMA-like estimate with BROAD?\n\n- Fig 4 (Left Fig): Are these unnormalized values? Isn't weight, i.e., $r$, positive and less than 1?\n- **Adapting the method for EO**: \n  - **Parameterization of weight network:** If we use BROAD for EO, it will use the adversary's loss, which uses labels $y_i$. Thus $r_i$ is a function of $x_i, s_i$ and $y_i$. Should the weight network (in ROAD) also use $y_i$ to predict r$?\n  - **Validity constraint:** Would the validity constraint change for EO? That is, would it require normalization over both $s_i$ and $y_i$ or only $s_i$ as explained in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699082192308,
        "cdate": 1699082192308,
        "tmdate": 1699636732050,
        "mdate": 1699636732050,
        "license": "CC BY 4.0",
        "version": 2
    }
]