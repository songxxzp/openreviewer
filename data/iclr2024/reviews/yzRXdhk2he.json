[
    {
        "id": "018Cr0mj3V",
        "forum": "yzRXdhk2he",
        "replyto": "yzRXdhk2he",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission847/Reviewer_TD84"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission847/Reviewer_TD84"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Matcher, a framework that leverages pre-existing vision foundation models to tackle diverse perception tasks. The Matcher framework encompasses three key components: bidirectional matching for precise matrix correspondences extraction, a range of sample prompt designs encompassing part-level, instance-level, and global-level prompts, and controllable mask generation through instance-level matching. Through extensive experimentation on multiple benchmarks, such as COCO-$20^i$ and LVIS-$92^i$, the proposed method's efficacy is demonstrated. Furthermore, quantitative results highlight Matcher's ability to handle images in real-world scenarios, showcasing its open-world generality and flexibility."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is well-written and presents its ideas in a clear manner, making it easy for readers to follow the proposed framework.\n\n2. The idea of Matcher model is straightforward and practical. The three key components within the Matcher framework are not only effective but also highly efficient. (1) Bidirectional matching for precise matrix correspondences extraction; (2) A range of sample prompt designs encompassing part-level, instance-level, and global-level prompts; (3) Controllable mask generation through instance-level matching.\n\n3. The paper demonstrates good performance not only on standard one-shot benchmarks but also on a Video Object Segmentation (VOS) benchmarks."
            },
            "weaknesses": {
                "value": "1. One crucial aspect of achieving high performance in the Matcher framework heavily relies on the utilization of DINO-V2 for accurate correspondence matrix extraction. However, it is worth noting that this approach may involve assembling multiple foundation models, potentially leading to a trade-off between accuracy and efficiency.\n\n2. In light of recent research [1], it has been suggested that Stable Diffusion models offer a promising alternative for accurate correspondence matrix extraction, which has also been validated in the context of image-matching tasks. Consequently, it becomes pertinent to compare the performance of DINO-V2, the current method employed in the Matcher framework, with a diffusion-based correspondence extraction approach. Such a comparative analysis would enable a comprehensive evaluation of the two methods, shedding light on their respective capabilities within the Matcher framework and determining which approach yields superior results.\n\n[1] Tang et al. Emergent Correspondence from Image Diffusion, in NeurIPS 2023."
            },
            "questions": {
                "value": "The running speed of the proposed method in terms of efficiency is an important aspect to consider. It would be valuable to compare the efficiency of the proposed method with related works to assess its performance in this regard. By conducting a comparative analysis, we can gain insights into how the proposed method fares in terms of running speed and efficiency when compared to existing approaches in the field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698457154810,
        "cdate": 1698457154810,
        "tmdate": 1699636012236,
        "mdate": 1699636012236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BYWQAv7IVg",
        "forum": "yzRXdhk2he",
        "replyto": "yzRXdhk2he",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission847/Reviewer_3ZxQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission847/Reviewer_3ZxQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Matcher,  which utilizes pre-trained vision foundation models for various perception tasks without requiring specific fine-tuning or training. Matcher is capable of segmenting images using in-context examples, demonstrating its adaptability and proficiency across multiple segmentation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper presents a good way to generate prompts for the visual fundation model Segment Anything Model (SAM). It is exquisite to plug-in and play with another pretrained vision model.\n2. The method do not require any training or fine-tunning.\n3. Extensive experiments and get reasonable results on many tasks."
            },
            "weaknesses": {
                "value": "1. Since there are a lot of engineering technique in the paper, one weakness is no open source code available. It would be nice to make the code public in the supplementary or in a public github repo. I would raise soundness score if the code is published.\n2. Some of the comparison in experiments section is not fair. For example, all the results from SegGPT[1] are used a smaller ViT-L backbone. Even though SegGPT's training data includes the Coco dataset, it's important to note that these pre-trained models like DINOv2 or CLIP have been pre-trained on much larger datasets. So it is hard to say if it is fair to compare such generalist model, but at least we should compare with model of the same size.\n3. Some of the details of implementation is missing. It lacks some of intresting abblation study. I will mention in quesions section below.\n\n[1] X, Wang et al. SegGPT: Segmenting Everything In Context. ICCV, 2023"
            },
            "questions": {
                "value": "1. Since SAM has already been trained on very large dataset, why not use SAM's encoder to generate propmt for itsself? If SAM has bad semantics, any intuision why is it?\n2. How exactly do you use DINOv2? Where layers' output do you use as the feature?\n3. How do you decide the number of clusters used do you use for Robust Prompt Sampler? I sould assume different objects may need different number of clusters. Any ablative study and explanation of these?\n4. It would be nice to show some visualizations of different examples how exactly the points prompts are filtered by Patch-Level Matching, Robust Prompt Sampler and instance level matching. \n5. To my understanding, the mistake made by Matcher depends on how good the semantics of the pretrained model to generate points and the mistake made by SAM itself. One interesting ablation study would be random sample same number of positive points on SAM as the number of points Macher generated, and take a look at the uper bound performance Matcher could achieve.\n6. How does few-shot(>1) segmentation work?\n7. For few shot segmentation, are the mistakes from localizations or bad mask shape? Some metric of localizaiton would be nice or visualizing more failure cases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813959907,
        "cdate": 1698813959907,
        "tmdate": 1699636012160,
        "mdate": 1699636012160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kNZsqBwUIg",
        "forum": "yzRXdhk2he",
        "replyto": "yzRXdhk2he",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission847/Reviewer_zVyp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission847/Reviewer_zVyp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a general perception paradigm Matcher that utilizes off-the-shelf VFMs to address various segmentation tasks, such as few-shot/one-shot semantic/part segmentation. The VFMs in Matcher include DINOv2 as s feature encoder and SAM a promptable segmenter. Matcher involves no training, and achieves good results on several datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Performance is good: Matcher achieves the highest scores over different segmentation benchmarks.\n* Every designed component is reasonable and works well from ablation studies.\n* Matcher doesn't need training. This improves its efficiency."
            },
            "weaknesses": {
                "value": "* This work seems like a combination of existing vision foundation models and a set of engineering tricks. The pipeline can be summarized as \"encode by DINOv2 -> select prompt (matching and sampling) -> prompt SAM -> select mask\". Although the result proves its effectiveness, the paper is more like a technical report and lack academic insight.\n\n* From the ablation in table 7(a), the performance of Matcher is largely influenced by different image encoders: MAE (18.8%), CLIP (32.2%), DINOv2 (52.7%). Does this mean the the selection of encoders would outweigh all the designs in Matcher?\n\n* More ablation is needed for the selection of promptable segmenters, such as SEEM and Semantic-SAM. As the author claims Matcher as a general paradigm, does the designs still work on different segmenters?\n\n* What would happen if the test image contains no reference concept? Can Matcher solve this situation, or output a false positive?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861600490,
        "cdate": 1698861600490,
        "tmdate": 1699636012074,
        "mdate": 1699636012074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cqr3IsMoZF",
        "forum": "yzRXdhk2he",
        "replyto": "yzRXdhk2he",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission847/Reviewer_PEjB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission847/Reviewer_PEjB"
        ],
        "content": {
            "summary": {
                "value": "A perception paradigm is introduced name Matcher. It can segment anything by using in-context learning and two pretrained large vision models, which are DINO and SAM. There are three effective components within Macther: Correspondence Matrix Extraction (CME), Prompts Generation (PG), and Controllable Masks Generation (CMG). The whole pipeline is without training and showcases state-of-the-art  performance among other generalist models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The target task in this method is interesting to me. How be become a vision generalist is a hotspot in nowadays community. Matcher follows SegGPT to transfer off-the-shelf VFMs into a general segmentation model.\n\n2. This work constructs several new benchmarks for one-shot or few-shot in-context segmentation. They are challenging and meaningful to future works.\n\n3. The experiments demonstrate impressive generalization performance across various segmentation tasks."
            },
            "weaknesses": {
                "value": "1. The whole pipeline is too cumbersome to me. So many prompts seem unimportant from Table 7b in appendix. The final performance is significantly rely on the complicated ILM postprocessing from Table 4a.\n\n2. The impressive performance is likely due to an engineering ensemble of models. For example, the post merging of masks seems like a sort of output ensemble. Improvement from ensemble is common in previous segmentation works.\n\n3. I'm concerning about the inference time comparing to SegGPT, since Matcher needs to run the large-scale DINOv2 and SAM for every test image."
            },
            "questions": {
                "value": "Can Matcher segment a background concept just like SegGPT, such as blue sky or pavement? How does Matcher perform on more complex MOSE dataset for VOS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865157063,
        "cdate": 1698865157063,
        "tmdate": 1699636011993,
        "mdate": 1699636011993,
        "license": "CC BY 4.0",
        "version": 2
    }
]