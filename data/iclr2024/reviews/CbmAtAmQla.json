[
    {
        "id": "skUaalNxlp",
        "forum": "CbmAtAmQla",
        "replyto": "CbmAtAmQla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_b58r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_b58r"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a peer evaluation method is proposed to improve LLM-based evaluations on open-ended question answering and alleviate potential bias and self-enhancement. The proposed peer rank process produces a more reasonable ranking of model capabilities. The peer discussion process is also proposed to help models reach mutual agreements that are more consistent with human judgement. Interesting insights are provided in experiment results and analysis, such as the LLM reviewer who leads the discussion tends to hold its opinion and Stronger LLMs tend to hold their opinions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper focuses on a really interesting problem, which is to improve LLM based evaluations and mitigate bias by taking advantage of peer rankings and peer discussions of LLM. The proposed methods are simple, but effective. \n\n2.  Many examples are provided to illustrate the process of peer rank and peer discussion. \n\n3. A few popular and strong LLMs are employed in experiments and results are promising."
            },
            "weaknesses": {
                "value": "1. The study seems to be limited to question answering only. LLM has many other applications such as writing reviews and generating essays. Would the proposed methods be applicable to improve evaluations on those applications as well? \n\n2. How significant are the result comparisons, such as in Table 3, 4 and 5?  It'd be great if the authors could present t-test results such as p-values of the comparisons."
            },
            "questions": {
                "value": "1. Is there any data contamination issues for the LLMs being tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1164/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731504463,
        "cdate": 1698731504463,
        "tmdate": 1699636042950,
        "mdate": 1699636042950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u1iWORFKrC",
        "forum": "CbmAtAmQla",
        "replyto": "CbmAtAmQla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_bfhA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_bfhA"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel approach, Peer Rank and Discussion (PRD), for evaluating LLMs answers. The idea behind PRD is to involve peer LLMs as reviewers to create a more fair and unbiased evaluation. PRD has two versions, Peer Rank (PR) and Peer Discussion (PD), both of which share the goal of mitigating biases in automated evaluations. Peer Rank (PR) operates in a tournament-style benchmarking setting, where LLMs compete in pairwise matches to answer open-ended questions, while in Peer Discussion (PD)  two reviewer LLMs engage in multi-turn discussions to reach a mutual agreement on pairwise scoring or preference. Experiments show that within this framework the automatic evaluation achieves higher accuracy and aligns better with human judgments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed evaluation framework provides good insights to improve automatic evaluation and reduce the bias introduced with large language models. It has the potential to enhance the reliability and robustness of LLM assessments while providing a well-reasoned exploration of the framework and valuable analysis (e.g, section 5)."
            },
            "weaknesses": {
                "value": "While the human agreement can be higher than in other approaches, PRD is computationally costly. This work uses SOTA models that are available upon payment without discussing trade-off between cost and benefits compared to human evaluation. \nOn the same line, this work does not consider open-source LLMs. Knowing what open-source LLM can be used here could be beneficial for the community. The effectiveness of PRD depends on the quality and diversity of the peer LLMs available. If there is a limited pool of peer models, this work does not discuss what options can be used. This quite limits its applicability and utility."
            },
            "questions": {
                "value": "- What other options can be used from the open-source scenario?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1164/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699283045767,
        "cdate": 1699283045767,
        "tmdate": 1699636042888,
        "mdate": 1699636042888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SBAntl25VG",
        "forum": "CbmAtAmQla",
        "replyto": "CbmAtAmQla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_qoLU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_qoLU"
        ],
        "content": {
            "summary": {
                "value": "I believe that this paper uses various approaches to produce answers to questions using interacting groups of LLMs. In one approach, an LLM ranks competing answers generated by two other LLMs, and in another approach, two LLMs debate and hopefully converge on an answer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "In general, clever decoding strategies for LMs have been successful, such as Chain-of-Thought prompting, maieutic prompting, beam search, etc. This may be yet another useful framework; I cannot tell for sure."
            },
            "weaknesses": {
                "value": "From the outset, this paper's writing is highly confusing. It's next to impossible for me to parse what the argument is, or what many of the sentences mean. I don't feel equipped to evaluate its conceptual weaknesses because I cannot tell what the motivation, the problem, the proposed solution, or the result is."
            },
            "questions": {
                "value": "Is the aim of your method to help LMs produce better answers to questions than other methods currently achieve?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1164/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1164/Reviewer_qoLU",
                    "ICLR.cc/2024/Conference/Submission1164/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1164/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699592869003,
        "cdate": 1699592869003,
        "tmdate": 1700603028961,
        "mdate": 1700603028961,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WypUcrfxrt",
        "forum": "CbmAtAmQla",
        "replyto": "CbmAtAmQla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_mefY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1164/Reviewer_mefY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes two new LLMs-based methods for evaluating LLMs in a way that addresses current biases and limitations. They can be applied when the form of evaluation is pairwise comparison.\n\n- Peer Rank (PR): This algorithm selects several LLMs to be both contestants and reviewers. At each time, a reviewer compares the outputs generated by two contestants and decides which is better. The weight of the evaluation made by a reviewer LLM is based on its performance. The weights are computed in an iterated manner. This algorithm can be used when the scoring methods are both win rates and Elo ratings. \n\n- Peer Discussion (PD): Given two answers generated by two contestant LLMs, two reviewer LLMs assess which answer is better separately, and then the two reviewer LLMs engage in a structured discussion to reach a mutual agreement on the preferences for two given answers. \n\nThe methods are intended to produce evaluations that are more aligned with human judgment and can mitigate self-enhancement bias and position bias."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes two targeted solutions to the current issues of LLMs-based evaluations, such as self-enhancement bias and position bias, which will give fairer more reliable evaluation results. The two proposed methods are inspired by the educational domain. They are easy to understand. \n\n- This paper conducts human annotations to extend the existing datasets, which may benefit further research."
            },
            "weaknesses": {
                "value": "- I see limited improvements when PR and PD are applied. In Table 2, the Elo rating becomes closer to human raters' rating, but the ranking of the five LLMs does not change. In Table 3, the example-correlation does not increase too much, comparing \"All Reviewers (Weighted)\" to \"GPT-4 & Claude & GPT-3.5\". These indicate that the core idea of PR may not be very useful. In Table 5, the performance boost with PD is not that great.\n\n- The two proposed methods are weakly related, except that they both introduce \"peers\" to some extent. The paper doesn't combine the two, which makes me feel cut off.\n\n- Introducing the communication between LLMs to a better solution will substantially increase computational cost. The paper does not seem to take this into consideration."
            },
            "questions": {
                "value": "- Question A: For PR used in the win rate calculation, how will you determine whether it has converged? I only see the number of iterations in Algorithm 2 in Appendix E. Have you ever analyzed the impact of the number of iterations on the results?\n\n- Question B: I need more information about the human annotation process. How exactly did the annotation process work? How did you recruit the participants and how about their backgrounds? How much were they paid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1164/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1164/Reviewer_mefY",
                    "ICLR.cc/2024/Conference/Submission1164/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1164/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699677956198,
        "cdate": 1699677956198,
        "tmdate": 1700211567885,
        "mdate": 1700211567885,
        "license": "CC BY 4.0",
        "version": 2
    }
]