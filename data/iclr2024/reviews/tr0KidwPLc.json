[
    {
        "id": "HfRg1N5wVh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
        ],
        "forum": "tr0KidwPLc",
        "replyto": "tr0KidwPLc",
        "content": {
            "summary": {
                "value": "This paper proposes the LLMBar, aiming to evaluate if large language models (LLMs) can serve as an evaluator of the LLMs' instruction-following ability. LLMBar consists of two instruction sets, one collected from other benchmarks that are easy for LLM to identify and another generated with different strategies, incorporating sentence similarity, LLMs, etc, which is hard for LLM to identify. To further improve the ability to evaluate the instruction-following ability, the authors also propose a new prompting strategy by introducing a metric set generated by LLM itself to assist the evaluation. The benchmark experiments are performed on human-level and common open-sourced and proprietary LLMs with various prompting strategies, revealing a distinct ability between different LLMs and showing a significant gap between LLMs and human evaluators on the difficult dataset."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The soundness of this paper is good. Instruction-following ability is an important ability of LLMs that has not been well-studied. This paper fills the gap in this area by introducing a manually constructed instruction dataset supervised by human annotators.\n2. The experiment results are from various common LLMs and prompting strategies, which are enough for understanding if a LLM is a good evaluator.\n3. This paper is well-written, easy to follow, and will have a wild interest in the LLM community."
            },
            "weaknesses": {
                "value": "1. The instructions used for dataset construction contain ambiguous words. For example, the word \"imaginative\" will have different explanations from different people or LLMs in different aspects; such examples should also be deleted from the dataset.\n2. The authors use different prompting strategies for evaluation, but the dataset generation only uses plain prompts. It is interesting yet important to discover how different input instructions enhanced by CoT or other prompting strategies (i.e., $(I_{\\text{enhanced}}, O_1, O_2, p)$) affect the judgment of different LLMs.\n3. The proposed prompting strategy is not as good as the authors claim (there is no significant improvement on GPT4 and LLaMA2, from my point of view, and the output consistency is also not good enough when changing the order of inputs). I think the authors should rephrase the description of the proposed prompting strategies from different perspectives."
            },
            "questions": {
                "value": "I found a large performance variance on Base-9 and Base-10. I think this is because the generated metrics largely affect the LLMs' performance. Could authors provide the generated metrics by different LLMs? \n\nAlso, the performance of the proposed method on GPT-3.5 and GPT-4 does not seem robust on the GPTOut, I'm curious about whether there exists some bias (or preference) of the generated metrics that affects the performance on different evaluation sets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Reviewer_65gL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697455493237,
        "cdate": 1697455493237,
        "tmdate": 1699636826802,
        "mdate": 1699636826802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rq2t6ZZbjQ",
        "forum": "tr0KidwPLc",
        "replyto": "tr0KidwPLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a challenge meta-evaluator benchmark, LLMBar, used to assess the quality of the LLM-evaluator (LLM + prompt strategies) for instruction following. In addition, the paper provides empirical experiment result on various combination of the LLM model (open-sourced and non-open) and prompt strategies and shows improvement for a novel suite of prompt strategies"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strength: \n* The paper is overall well-written, easy to follow \n* The paper addresses an important current problem of scalable evaluation of the LLM-evaluator\u2019s quality"
            },
            "weaknesses": {
                "value": "Weakness\n* There is some confusion in how the evaluation set was generated, for example, in Figure 2. \u201cWe often use weaker models to generate O1 and stronger model to generate O2 \u2026\u201d  it is unclear what is weak and strong models are referring. I tried to search that definition in the text on page 4 \u201cNeighbor Instruction\u201d, but still couldn\u2019t find any. \n* Similarly, point 2 on page 4, not sure what \u201cmanual filtering and modification\u201d entail \n* The human evaluators are co-authors of the papers, which can raise issues of objectivity compared with other crowd-sourced human evaluators. What measures are in place to prevent the bias?\n* I am wondering whether it is a fair comparison to other benchmarks if the instructions are qualitatively different. For example, it is relatively easy to achieve high inter-rater reliability if the response is factual information rather than subjective\n* The experiment result part spent the majority of the session on the performance of the LLM evaluator on the newly proposed benchmark, while only a small section on comparing with other benchmarks. Since the benchmarks are qualitatively different (objective vs subjective), I am not sure whether the result is meaningful. It would be ideal to have another benchmark with comparable features (e.g., objectiveness and similar inter-rater agreement)"
            },
            "questions": {
                "value": "Minor issues to be fixed, not factored in evaluation \n* Figure 5, end of the sentence, typo evaluators\u2019 capabilities \n* Please state the metrics used for calculating inter-rater reliability, e.g. Cohen Kappa etc. making sure it is consistent with other papers to be comparable"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Reviewer_WsKe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760582809,
        "cdate": 1698760582809,
        "tmdate": 1699636826677,
        "mdate": 1699636826677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qXXjPXSltK",
        "forum": "tr0KidwPLc",
        "replyto": "tr0KidwPLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a challenging meta-evaluation benchmark, consisting of 100 natural and 319 adversarial <instruction, preferred output, non-preferred output> triplets, for evaluating instruction-following capabilities of LLMs. 100 natural instructions are carefully chosen from the existing benchmarks, AlpacaFarm and LLMEval. Adversarial set consists of 319 samples collected through 4 different strategies. 1) Uses neighbor search within the dataset to select a similar instruction, and then uses a strong LLM on the retrieved instruction to generate the non-preferred output. 2) Uses GPT-4 to generate a variant of instruction which is relevant but not similar and then uses the new instruction to generate the non-preferred output. 3) Uses GPT-4 to produce a superficially good but unhelpful response (non-preferred output). 4) Manual construction. The new dataset has very high inter-annotator agreement.\n\nOn the adversarial set they found that ChatGPT- and LLAMA-2-70B-based evaluators perform worse than the random chance. So, authors also propose new prompting strategies: Rules (a list of general rules to follow), Metrics (prompt LLM to generate a set of instruction-specific metrics and use them to evaluate the outputs) and Swap (generate scores for both the outputs orders o1,o2 and o2,o1; use LLM to generate final score by using combinations of both responses if they are contradictory). Their new prompting strategies improve the LLM-based evaluator's performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A possibly useful benchmark for evaluating instruction-following capabilities of LLM.\n\n- Useful insights on the capabilities of LLM-based evaluators (e.g., Llama-based and Chat-GPT-based evaluators incompetency in evaluating instruction-following capabilities of LLMs.)\n\n- Proposed prompting strategies improve evaluator's performance."
            },
            "weaknesses": {
                "value": "Are LLMs likely to sample non-preferred output for instructions in the adversarial pool? Could you provide the distribution of generating preferred and non-preferred outputs for a few LLMs? It is unclear, whether the findings of the paper (or to say, improving performance of evaluator-LLMs on adversarial samples) would actually lead to improved reliability of LLM-based evaluations."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7039/Reviewer_kxW7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812155865,
        "cdate": 1698812155865,
        "tmdate": 1699636826552,
        "mdate": 1699636826552,
        "license": "CC BY 4.0",
        "version": 2
    }
]