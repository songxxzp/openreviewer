[
    {
        "id": "N41CeAVXC3",
        "forum": "fH2wf2w2Ss",
        "replyto": "fH2wf2w2Ss",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_c6YU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_c6YU"
        ],
        "content": {
            "summary": {
                "value": "The paper revisits the unCLIP paradigm proposed by Ramesh et al., 2022, which consists of two cascaded diffusion models, one trained on CLIP latent text embeddings and another one mapping from latent text embeddings to the image space. Unlike unCLIP, the paper proposes to train the latent diffusion models on CLIP image embeddings rather than text embeddings, which enables unconditional image generation. The proposed model is evaluated on different variants of AFHQ, FFHQ, and ImageNet, and is compared to EDM (Karras et al. 2022) among other baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Improving unconditional image generation models is an active research area, and lags substantially behind class/text conditional generation. Making progress in this area is important. Further, techniques relying on multi-stage modeling/latent modeling like the proposed one have proven effective in making diffusion models more efficient."
            },
            "weaknesses": {
                "value": "I see two main weaknesses: \n1. The lack of novelty. The proposed method is very similar to unCLIP. \n2. The method is arguably more complicated than (Hu et al., 2022) which simply clusters image embeddings obtained by a self-supervised representation and uses the cluster indices as conditioning signal. While the paper compares to this approach on AFHQ/FFHQ, I\u2019m not fully convinced that the proposed method is superior. I would expect a comparison on ImageNet to be convinced that the additional complexity of the proposed approach is justified, since Hu et al. get similar improvements.\n\nGiven these two points, I\u2019m leaning towards rejecting this paper.\n\n\nMinor points:\n- Typo page 2 bottom \u201cthat the all images\u201d\n- I found the terms lightly/strongly conditional somewhat confusing. Maybe it would be simpler to just use class/text conditional?"
            },
            "questions": {
                "value": "- Do the authors have any explanation why the 2SDM outperforms 2SDM with oracle in Figure 5 right?\n- Did the authors consider any other image embeddings besides CLIP? For example DINO might be better aligned with ImageNet. Also it would be interesting to see how well the first diffusion model can learn the embedding, and how this affects the quality of the end-to-end model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781439988,
        "cdate": 1698781439988,
        "tmdate": 1699636974050,
        "mdate": 1699636974050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xyrBIwldgZ",
        "forum": "fH2wf2w2Ss",
        "replyto": "fH2wf2w2Ss",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_5Hdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_5Hdf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an unconditional image generation pipeline which is split into two parts: first, a model generates a random condition (in this case a CLIP image embedding), then, a second model is conditioned on this condition to generate the actual image. Compared to baseline unconditional single-stage models, the new approach performs better while leading only to a small overhead in training and sampling cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The approach tackles unconditional image generation, and improvements in that area could potentially also translate to conditional generation pipelines.\n\nThe approach of splitting the unconditional generation into two parts seems novel and the results indicate that this does indeed lead to improvements, at least on the relatively small datasets and image resolutions that it was tested on."
            },
            "weaknesses": {
                "value": "While the approach seems to lead to improved performance it's not clear to me why this is the case and there is only very little analysis around this.\nIs it that unconditional sampling of CLIP image embeddings is somehow important or easier than sampling an image directly? Or is it the two-stage pipeline itself that is the important part? Could the condition generation and subsequent image generation be done in a single pipeline with end-to-end training? What exactly is the interaction between the first and second stage models?"
            },
            "questions": {
                "value": "How well do you think this would work for more complicated domains and datasets?\nHow do you think this approach could benefit/improve conditional generation pipelines such as text-to-image?\nHow well do you think this would work with more specific conditions in the first stage (e.g., depth maps, edge maps, etc)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804100496,
        "cdate": 1698804100496,
        "tmdate": 1699636973933,
        "mdate": 1699636973933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RJPZCJDXJv",
        "forum": "fH2wf2w2Ss",
        "replyto": "fH2wf2w2Ss",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a two-stage approach, 2SDM, for sampling from diffusion models. The goal is to improve the performance of unconditional generation, which has a gap in performance compared to conditional generation. In the first stage, an auxiliary diffusion model is used to generate an embedding, which is subsequently used in the second stage by a conditional diffusion model to synthesize an image. The authors demonstrate that 2SDM yields better performance across almost all experiments in terms of quality and diversity with little to no increase in sampling speed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed two-stage approach is straightforward, extending UnCLIP to the unconditional setting.\n- The authors provide some context and insight into what it means for conditional generation to be better than unconditional generation, and why this may be the case.\n- Experiments demonstrate superior performance over the baselines with negligible impact on the sampling speed."
            },
            "weaknesses": {
                "value": "While the method is straightforward, the paper is a bit difficult to understand overall. The finer details are unclear. For example:\n- It is unclear what the authors mean when they mention \"discarding\" the conditional embedding y after sampling.\n- The details about the auxiliary model in Section 4 are unclear. For example, what is a_\\sigma? Maybe reiterating some of the variables in Equation 4 would be helpful, too.\n- In the results overview of Section 5, the authors describe that Figure 4 (which seems to actually be referring to Figure 5) \"compares against 'Class-cond', which is an ablation of 2SDM that applies to unconditional tasks\". Given the label \"Class-cond\" it seems more intuitive that this would refer to the \"lightly-conditional\" task instead."
            },
            "questions": {
                "value": "- For explicit clarification, are the two models (auxiliary and conditional image) trained sequentially?\n- The authors mention that they did not use classifier-free guidance in their results, which is common practice for diffusion sampling. It would be helpful to get some sense of how it affects the quality of the outputs.\n- The experimental results are compelling and the method is straightforward, but the paper could greatly benefit from clearer communication of the proposed ideas and details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7930/Reviewer_WB76",
                    "ICLR.cc/2024/Conference/Submission7930/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813826253,
        "cdate": 1698813826253,
        "tmdate": 1700686501910,
        "mdate": 1700686501910,
        "license": "CC BY 4.0",
        "version": 2
    }
]