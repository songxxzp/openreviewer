[
    {
        "id": "Pn5gD8wJT6",
        "forum": "3y1K6buO8c",
        "replyto": "3y1K6buO8c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m"
        ],
        "content": {
            "summary": {
                "value": "The authors perform MEG conditioned visual decoding.\n\nCompared to other works that leverage fMRI, MEG is a different information source that presents unique challenges.\n\nThe authors use an align then generate strategy, where they learn a function that takes as input the MEG signal, and train it to align with a CLIP latent using a weighted sum of infoNCE and MSE loss. For image generation, they use Versatile Diffusion and regress the needed conditioning variables from MEG. \n\nThey observe that it is possible to recover high level semantics in the reconstructed images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "MEG decoding of full images is an under-explored area compared to fMRI based decoding, and it is a harder task, given the low channel count relative to the tens of thousands of voxels in fMRI. To my knowledge, this is the first time that image decoding has been demonstrated using MEG.\n\nThe paper is methodologically sound, outlining different training objectives for different parts of the proposed pipeline. The paper provides systematic benchmarks, showing that their MEG decoder leads to reasonable image retrieval and image generation.\n\nI applaud the authors for showing \"representative\" retrieval and best/mean/worst decoding results, which helps gauge the effectiveness of the method.\n\nIt is also interesting that they found MEG capable of recovering high level semantics. Although it is not fully clear if this is a limitation of MEG or their method (should probably discuss more)."
            },
            "weaknesses": {
                "value": "As with other deep decoding papers, it is not super clear what the ultimate scientific insight is. This is not a criticism specific to this paper, but more generally aimed at current decoding works which leverage powerful image priors and deep non-linear decoding/embedding functions. \n\nIn this aspect, this paper is better than most, as their Figure 3 provides some insight on the temporal dynamics of decoding. I think it would benefit the paper to add some discussion (not necessarily experiments) on extending this to EEG based decoding, or other potential practical applications or scientific insights. \n\n**General clarifications:**\n\nThe clarity of many of their methods could be improved. The author repeatedly references Defossez et al. in reference to their methods. But in the main text it is not super clear. Concretely I would like the authors to clarify the following:\n1. What the the MEG conv \"encoder\" convolving over?\n\n2. How do you combine the MEG channels?\n\n3. What temporal aggregation layer did you use? You mention global pooling, affine, and attention. Which layer did you end up using? Because you discuss this and then never talk about which method you ended up using.\n\n4. How do the different aggregation layers work? I ask this question in the context of Figure 3. Because you discuss using a 1500ms window, then shift to a 250ms window. Do you train a new model? Do you re-use the 1500ms model but change the aggregation? If you do train a new model, are you taking multiple 250ms windows and supervising with the same image target? For the sliding window, what is the step size?\n\n5. For Figure 2, in the supervised models (VGG, ResNet, etc.) are you using the last layer (1000 imagenet classes layer), or the post-pooled layer. \n\n6. For retrieval, are you always using cosine/dot-product similarity?\n\n**Minor format error:**\n1. The authors have ICLR 2023 in the header, when it should be ICLR 2024. And they have line numbers, which do not seem to be present in the default ICLR template.\n\n**Minor clarifications:**\n1. Can you clarify if $N$ (line 75) denotes the number of images? It doesn't seem like you define $N$ prior/after using it.\n2. To provide more context, can you mention in line 84 that you are using the infoNCE loss, rather than just mentioning the CLIP loss.\n3. In section 2.2, can you clarify if you are normalizing $\\hat{z}$ to norm = 1 for eq. 1, and not assuming a fixed norm for eq. 2? Otherwise it seems like the two losses would have trivially the same optima, but I guess you are trying to have one loss align the direction, and have a second loss align the direction + norm.\n\n**Additional citations:**\n\nThe author discusses one approach towards decoding, but I would appreciate if the author could also discuss the brain gradient conditioned image generation work listed below, the most recent of which also leverage GANs/Diffusion models:\n\nInception loops discover what excites neurons most using deep predictive models (**Nature 2019**); Neural population control via deep image synthesis (**Science 2019**); Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences (**Cell 2019**); Computational models of category-selective brain regions enable high-throughput tests of selectivity (**Nature Communications 2021**); NeuroGen: Activation optimized image synthesis for discovery neuroscience (**Neuroimage 2022**); Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models (**NeurIPS 2023**); Energy Guided Diffusion for Generating Neurally Exciting Images (**NeurIPS 2023**)\n\nOverall I think the paper is sound, interesting, and provides good insight on neural decoding from an often overlooked modality."
            },
            "questions": {
                "value": "Please see the weakness's section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_F12m",
                    "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786038835,
        "cdate": 1698786038835,
        "tmdate": 1700604582098,
        "mdate": 1700604582098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "idTWMdd4zt",
        "forum": "3y1K6buO8c",
        "replyto": "3y1K6buO8c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_hWdQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_hWdQ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors developed a model based on contrastive and regression objectives to decode MEG, resulting in 7X improvement in image retrieval over a classic linear decoder. The promising results in image retrieval and generation are significant in that the presented approach allows the monitoring of the unfolding of visual processing in the brain based on MEG signals, which have much higher temporal resolution than fMRI. The work yields two potentially interesting observations: (1) late responses are best decoded with DINOv2, and (2) MEG signals contain high-level features, whereas 7T fMRI allows the recovery of low-level features, though it would be worthwhile to articulate or speculate what these findings mean for understanding the cascade of visual processes in the brain."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The work is significant in that there is no MEG decoding study that learns end-to-end to reliably generate an open set of images. Thus, it can potentially be considered a ground-breaking in this area of research, even though the techniques used are not necessarily novel from an ML perspective."
            },
            "weaknesses": {
                "value": "The decoding work is supposed to provide new insights to the cascade of visual processing and the unfolding of visual perception in the brain.  The authors need to articulate better what insights the current observations (mentioned in the Summary) actually provide us."
            },
            "questions": {
                "value": "What do the two observations tell us about the unfolding of visual perceptual processes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798493329,
        "cdate": 1698798493329,
        "tmdate": 1699636580536,
        "mdate": 1699636580536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MkclGYsUfK",
        "forum": "3y1K6buO8c",
        "replyto": "3y1K6buO8c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors propose a method to decode brain activity. The main idea is to train an MEG decoder which maps MEG signals to a feature space which is then used to reconstruct images using a pretrained image generator. \n\nThe authors show that MEG decoder which is a DNN leads to 7 times improvement over linear decoders which is a common approach in neuroscience studies. Image generation results suggests that it is possible to reconstruct semantically accurate from MEG activity while low-level details are difficult to reconstruct."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. 7x improvement in decoding accuracy over linear decoders. This is an important result which will encourage neuroscience researchers to use DNNs for decoding MEG/fMRI signals.\n2. Clear presentation of methods (Figure 1, Section2)."
            },
            "weaknesses": {
                "value": "1. The reconstruction results are not impressive. Even the best examples shown in Figure 5 often do not have the reconstructions of image of same or related category.  Therefore, the title is misleading as the main contribution of this paper in my opinion is DNN based MEG decoder and retrieval results and is not correctly reflected in the title.\n2. The decoder is trained using a combination of two loss functions : MSE loss and CLIP loss (equation 3, line 91). There seems to be no ablation study investigating what is the impact of each loss function in retrieval performance. There is one figure in supplementary material Fig S2 E but I am not sure whether it indicates two terms of CLIP loss or two terms of overall loss (CLIP + MSE).\n3. In Line 110 authors mention that they select  lambda by sweeping over {0.0, 0.25, 0.5, 0.75, 1.0} and pick the model whose top-5 accuracy is the highest on the large test. Is the hyperparameter search for lambda done on test data?\n4. The claim in the abstract \"MEG signals primarily contain high-level visual features\" does not have sufficient evidence based on the reconstruction results only. It has been shown in literature (even in Things dataset paper Figure 8) that fMRI responses of early visual cortex (which can decode low-level features) are correlated with MEG responses (Cichy et al. 2014, Hebart et al. 2023) in early time windows. Therefore, a stronger evidence is required to back this claim. A possible explanation why the reconstructions can not recover low-level details might be that temporal aggregration layers leads to suppresion of low-level features which are present in a smaller time-window around 100ms. Another possible explanation is that we are predicting a high-level feature  (DINOv2/CLIP etc.) from MEG which may not need information from low-level features and thus the image generated also lack these details. \n5. The main result of the paper is 7x improvement over linear decoders. It is not clear where exactly this result is in the paper. A reader needs to compare results in supplementary and Figure 2 in the main text. Simply adding shaded bar in Figure 2 for linear decoder next to each bars can improve clarity"
            },
            "questions": {
                "value": "Please refer to weaknesses section for points to address in rebuttal. \n\nOverall this paper has some new contributions but authors make some claims which do not have sufficient support in the results. Therefore, my recommendation would be to either tone down the claims or present good evidence to back them up"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_hpxq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698885298588,
        "cdate": 1698885298588,
        "tmdate": 1700603429428,
        "mdate": 1700603429428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aZRciEFWrN",
        "forum": "3y1K6buO8c",
        "replyto": "3y1K6buO8c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
        ],
        "content": {
            "summary": {
                "value": "This contribution concerns the interesting topic of decoding/retrieval and reconstructing of visual input from MEG data (THINGS-MEG data set). The approach is based on representations of images and MEG data using multiple architectures and multiple levels of generalization.\n\nThere is a rich literature on decoding and reconstructing visual and audio stimulus from brain recordings, so novelty is somewhat limited.\n\nDecoding is evaluated as retrieval in closed and open set conditions (the latter using zero-shot setting).\nRetrieval is based on linking by learning to align MEG and image representations \n\nThe reconstruction of visual input is based on generative models, using frameworks that have been developed elsewhere (Ozcelik and Van Rullen). \n\nCompared to the very rich literature on methods based on MEG and other modalities, this study has an increased focus on temporal resolution of the retrieval process and furthermore, they use diffusion models for conditional generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Compared to the very rich literature on methods based on MEG and other modalities, this study has increased focus on temporal resolution of the retrieval and furthermore using sota diffusion models for conditional generation.\nIt is concluded that retrieval interesting peaks following image onset and image offset (the latter based on the after-image presumably). Retrieval performance is good for several image representations (VGG and DINOv2)\nThe generative performance is evaluated in a number of metrics, there is good consistency among the metrics.\nVisually the generation makes sense.\nUseful to see examples  stratified over good, bad and ugly cases."
            },
            "weaknesses": {
                "value": "There is a rich literature on decoding and reconstructing visual and audio stimulus from brain recordings, so novelty is somewhat limited.\n\nBased on MEG we have high time resolution and SNR. In the temporally resolved analysis, it is interesting that VGG outperforms the more advanced representations for the direct image (after image onset) while the more complex image representations dominate retrieval based on the after-image (following image offset). We miss a discussion of this interesting finding.\n\nThe generative performance is evaluated in a number of metrics with good consistency among the metrics. Yet, we are missing uncertainty estimates to weigh the evidence in this case\n\nVisually the generated imagery is intriguing. However, we miss a discussion of the notable lack of fine grained semantic relatedness (generation seems primarily to pick up on texture, object scale(?) and high-level semantics eg. man-made vs natural)"
            },
            "questions": {
                "value": "Based on MEG we have high time resolution and SNR. In the temporally resolved analysis, it is interesting that VGG outperforms the more advanced for the direct image (after onset) while the more complex image representations dominate retrieval based on the after image (after image offset). Missing a discussion of this interesting finding.\n\nThe generative performance while evaluated in a number of metrics with good consistency among the metrics. Yet, we are missing uncertainty estimates to weigh the evidence in this case\n\nVisually the generated imagery is intriguing. However, we miss a discussion of the notable lack of fine grained semantic relatedness (generation seems primarily to pick up on texture, object scale(?) and high-level semantics eg. man-made vs natural)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5619/Reviewer_4PGh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698929155683,
        "cdate": 1698929155683,
        "tmdate": 1700734985513,
        "mdate": 1700734985513,
        "license": "CC BY 4.0",
        "version": 2
    }
]