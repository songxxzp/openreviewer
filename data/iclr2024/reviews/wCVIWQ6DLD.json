[
    {
        "id": "k3Z0hnihiC",
        "forum": "wCVIWQ6DLD",
        "replyto": "wCVIWQ6DLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_wyU7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_wyU7"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a data attribution method, for attributing classification performance of a neural network on a given sample to training samples, that provides a better efficiency-accuracy trade-off compared to existing attribution methods. The proposed method computes the affinity of a given evaluation image with all images in the training set, in the latent space of a pretrained neural network and under a particular metric. The paper provides experiments on CIFAR-10 and ImageNet showing on-par performance with state-of-the-art data attribution methods using only a fraction of their memory and computational budget. The main finding of the paper is that relying only on visual similarity is effective in discovering the smallest set that affects the classification performance of a given image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles an interesting problem, namely making data attribution practical, and provides an insightful comparison with state-of-the-art data models. The writing is also clear and easy to follow."
            },
            "weaknesses": {
                "value": "1- The main scientific finding of this paper is an expected result: that training images with high visual similarity to an evaluation image are important for correctly classifying that image. If this is a surprising finding (the abstract seems to claim it is), the paper should try to emphasize the arguments against it in prior works, and discuss the fault in those arguments. As it stands, I find its main scientific contribution not very significant.\n\n2- While the main practical contribution of the paper, its method, cannot outperform state-of-the-art in the studied datasets, the paper correctly explains that it is more practical. However, the paper does not provide any real-world experiments to show the usefulness of its method in an application. This makes it hard to judge the significance of the proposed method in practice.\n\n3- The paper does not explain its related works in sufficient detail, and as a result, a lot of the methodologies it borrows from prior works is unclear. In particular, a detailed discussion of metrics and their relation to the considered metrics is missing. The Appendix provides some more detail, specially on LDS, which I think must be part of the main paper."
            },
            "questions": {
                "value": "My three concerns mentioned in the weaknesses section contain my suggestions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7980/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810370279,
        "cdate": 1698810370279,
        "tmdate": 1699636982699,
        "mdate": 1699636982699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LP0EtWk1zS",
        "forum": "wCVIWQ6DLD",
        "replyto": "wCVIWQ6DLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_TdDF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_TdDF"
        ],
        "content": {
            "summary": {
                "value": "The paper studies (and revisits) the effectiveness of similarity-based baselines for the problem of data attribution---understanding\nhow training data influence model predictions. The paper introduces two new metrics based on brittleness, and evaluate their baseline relative to recent SOTA methods (datamodels, TRAK) in terms of performance and other cost measures (time, memory)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is very written and placed well within the context of prior work (though there are some misleading claims throughout, which I highlight below).\n- The experiments and evaluations are thorough and well documented"
            },
            "weaknesses": {
                "value": "At a high level, I have serious concerns about the claims made in the paper and the way the overall message is presented:\n\n- The paper claims that the model agnosticity of their approach (and similarity based methods) is a feature, but I strongly disagree. Data attribution at a most basic level is about understanding why the given specific model/algorithm behaves, so it *has* to be model dependent. Otherwise, just by definition, the method cannot capture any biases unique to the model/algorithm (and as many prior works show, different models have very different biases, for example CNNs vs ViTs, etc.). The paper acknowledges this point in passing, but I think this needs to be much more prominent.\n\nNow, as the authors acknowledge, it's possible that there is a largely \"model-independent\" component that can account for model behavior (it's not crazy to think that different DNNs leverage data in similar ways), but I think it's a long jump to conclude this just from the brittleness metrics (I expand on this below).\n\n- The paper also continues to propagate the misunderstanding in some prior works in this area that similarity is same as influence, which is just not true! This is not even true when you consider the simplest case of a linear model: there, the similarity is given by the natural euclidean product, whereas the influence looks and behaves very differently (it involves the inverse of a gram matrix).\n\n- Also, a similarity-based approach cannot readily surface negative influencers, or even assign relative weights to the positive influencers. It's almost misleading to call even call the approach \"attribution\" when you cannot assign quantitative weights to examples (that reflect their counterfactual importance). Similarity (used directly) can only capture the relative ordering among positive influencers. \n\n- The issue of metric: while I agree that brittleness-based metrics are also informative, it seems misleading to base most of the paper's claims on two new metrics, which do not capture the points I mentioned above (negative influencers and calibration among positive influencers), while delaying discussion of metric (linear datamodeling score) considered in prior works to the Appendix.\n\nIn in fact, their evaluation shows that the similarity baseline only achieves an LDS of 0.05 on CIFAR-10, which is hardly significant. \nTRAK ([2], App. E.3) using just 5 independently models can achieve an LDS of 0.329. It might be true that at the same level of minimal compute (a single checkpoint), the baseline method outperforms prior methods. But importantly, the proposed approach method cannot improve even with more compute! (prior work[1] indeed shows that more \"ensembling\" has marginal effect on similarity-based approaches).\nSo one could claim that at the certain level of budget, the proposed approach is the best performing, but it is very misleading to say throughout the paper that this simple approach also beats SOTA, which is clearly not the case (and only shown in the Appendix).\n\nAll of these concerns considered, I think a more reasonable (and less misleading) conclusion of the paper would have been more along:\nsimilarity-based approaches can be an effective baseline; rather than \"similarity is all you need\" message that seems more prominent and is misleading given the various reasons above.\n\nOther concerns:\n- Not sure why space efficiency is a big consideration at all. Storing both embeddings (for computing similarities) and storing projected gradients (for a single checkpoint) both require same order of memory. \n\n[1] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, Aleksander Madry. \"Datamodels: Predicting Predictions from Training Data.\"\n[2] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, Aleksander Madry. \"TRAK: Attributing Model Behavior at Scale\n\""
            },
            "questions": {
                "value": "Concerns were raised above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7980/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947522611,
        "cdate": 1698947522611,
        "tmdate": 1699636982585,
        "mdate": 1699636982585,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3wcyryQNNS",
        "forum": "wCVIWQ6DLD",
        "replyto": "wCVIWQ6DLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_eNuE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_eNuE"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of data attribution, i.e., estimating how individual training data points influence model outputs. The main contribution of this paper is a compute-efficient and storage-efficient data attribution baseline for images.  Specifically, the baseline identifies \u201cimportant\u201d training data points via visual similarity using self-supervised feature extractors (i.e., distance in embedding space).  The experiments suggest that this baseline can outperform existing data attribution methods in identifying small data-removal and data-mislabel support sizes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written in that the problem setting is clear and the experiments (figures, plots, etc) are easy to follow. The design choices in the proposed baseline are described in detail as well.\n- The experiments show that for standard image classification tasks, visually similarity (measured via distance in some embedding space) can surface training data points with high positive influence on model outputs."
            },
            "weaknesses": {
                "value": "- \u201cOur work shows that strong data attribution can be achieved solely based on knowledge of the training set.\u201d In general, the effect or influence of a training data point on a model output is a function of the learning algorithm used to train the model. The data attribution scores of a standard ERM classifier trained on CIFAR would be very different from a random CIFAR classifier. The proposed method, however, would output the same attribution scores for the random classifier and an ERM classifier trained on CIFAR. This is an issue for at least two downstream applications.\n    - As noted in the paper, data attribution can be used for debugging model biases (https://arxiv.org/abs/2211.12491) where you want to compare data attributions of two learning/training algorithms (e.g., with and without data augmentation) and see how data changes in the learning algorithm change data attributions. This method, however, would output the same data attribution scores for both algorithms, so it cannot be used to compare data attribution scores in general.\n    - Another application of data attribution is to identify backdoor attacks in training data (https://arxiv.org/abs/2307.10163), as data attributions of backdoored test examples would rely on backdoored training examples. By relying solely on visual similarity (not a good proxy in this case) and not the learning algorithm, the proposed baseline is unlikely to succeed in identifying visually dissimilar + backdoored training points that have high influence.\n- This method only focuses on examples with high positive influence. In general, data attribution methods identify data points with high influence, but also data points with ~zero influence and negative influence. Identifying points with almost no influence can be used to prune the dataset, whereas identifying points with negative influence can be used to understand what in the training dataset causes a model to misclassify a test example. The appendix suggests a heuristic to identify negative influence, but it is unclear if this works as well as the data brittleness experiments because the LDS score is quite low (0.05) and Figure 7 only visualizes the positive influencers. One concrete way to check this would be to identify how many negative influencers need to be removed to make an incorrectly classified test point correct.\n- The method makes a strong implicit assumption: The data attribution score of training example j and test example i does not depend on other training examples in the dataset. However, if there are multiple copies of training example j in the dataset, then the influence of each copy is down-weighted. Intuitively, this is because the effect of removing a single copy on the model output is small if there are other copies in the training dataset that aren\u2019t removed. The proposed method does not account for this, so it will not estimate the influence of individual training data points in scenarios like the one above.\n- The actual method identifies high-influence datapoints by comparing visual similarity of a test example to other examples in the same class. This implicitly assumes that training data points from other related classes cannot be positive influencers. However, even for standard image classification tasks, it is possible to have training data points with positive influence that do not belong to the same class. Furthermore, given that this heuristic is \u201ccritical\u201d (S3.1) it is unclear how one would extend this method to vision tasks (e.g. data attribution for CLIP) that does not have a fixed class set."
            },
            "questions": {
                "value": "Writing is vague at times; a few examples below. It would be great to get some clarity about these statements:\n    - \u201cIt is important to highlight that while Datamodels and TRAK outperform our baseline in terms of LDS with extensive model ensembles, this metric provides limited insights into understanding machine learning models.\u201d\n    - \u201cThus, the latter metric [data removal and data mislabel support size] serves as a better proxy [than LDS] for the data attribution method\u2019s usefulness as a debugging tool.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7980/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699030634737,
        "cdate": 1699030634737,
        "tmdate": 1699636982477,
        "mdate": 1699636982477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TRvywwDs43",
        "forum": "wCVIWQ6DLD",
        "replyto": "wCVIWQ6DLD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_n2Gq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7980/Reviewer_n2Gq"
        ],
        "content": {
            "summary": {
                "value": "This paper uses self-supervised models as a feature extractor for data attribution. It matches or outperforms previous baselines (TRAK and data models) on CIFAR and ImageNet while being much more efficient in terms of compute and storage."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Results are comparable to the baselines, while reducing cost of data attribution\n- Shows that a straight forward approach that was previously abandoned can be effective by using self supervised models\n- Architecture transfer ablation is important\n- Presentation is clear"
            },
            "weaknesses": {
                "value": "- Unsure of some of the assumptions made in the paper - see questions\n- Lacks quantitative comparison with baselines of samples chosen - could be good to compute some statistics across this approach and the baselines. For example, are the subsets chosen by this approach more similar to or different than TRAK and data models"
            },
            "questions": {
                "value": "1. Why is smallest subset the right thing to do? Isn't it possible for there to be two disjoint subsets of different size that both can cause a misclassification? In this case, shouldn't there be some attribution to samples in both groups?\n\n2. Is there an explanation for why self-supervised extractors work better for attribution, and why DINO is an exception?\n\n3. Subsetting on images of the same class is justified by empirical results, but in ImageNet there are classes that are very similar. How can you be sure that there is no attribution in this case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7980/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7980/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7980/Reviewer_n2Gq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7980/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699319514689,
        "cdate": 1699319514689,
        "tmdate": 1699636982383,
        "mdate": 1699636982383,
        "license": "CC BY 4.0",
        "version": 2
    }
]