[
    {
        "id": "5Nue14V8Lo",
        "forum": "bVzLZr0S8s",
        "replyto": "bVzLZr0S8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission525/Reviewer_5Ksg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission525/Reviewer_5Ksg"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the problem of how to select suitable training actions for reinforcement learning. The authors of this paper proposed to use the idea based on the Shapley value to guide the selection of training actions. The concept of Shapley value was proposed back in the 1950s. On several cloud computing related problems, the usefulness of selecting training actions based on the Shapley value has been experimentally demonstrated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is interesting and important to study possible ways of selecting useful training actions for efficient and effective reinforcement learning. The proposed use of Shapley value for selecting training actions is an interesting attempt towards solving the action selection problem."
            },
            "weaknesses": {
                "value": "The motivation for using the Shapley value for selecting training actions is not sufficiently detailed in the introduction section. To a large extent, it remains unknown why it is necessary or important to use Shapley value to guide the selection of training actions, especially when existing research works have already studied various ways of selecting training actions for model-based and model-free reinforcement learning.\n\nThere is a clear lack of review of relevant research works, especially cutting-edge technologies for selecting training actions. Hence, the real technical contribution of this paper remains highly questionable.\n\nThe mathematical definition of the action Shapley value in eq. (1) is not sufficiently clear. In fact $\\phi$ is originally declared as a function of D and A, where A represents the learning algorithm. However, eq. (1) is clearly irrelevant to any learning algorithm. Meanwhile, $\\mathcal{U}$ in eq. (1) is introduced as a valuation function. However, this paper did not give a clear idea regarding how $\\mathcal{U}$ is defined or learned for general reinforcement learning problems. Meanwhile, eq. (3) is quite confusing. Hence, it is hard to judge on the practical value of using eq. (1) for arbitrary real-world reinforcement learning problems.\n\nThe authors stated that the best possible training action set includes as many training actions as the global cut-off cardinality with the highest Action Shapley values. However, this important algorithm design decision is only explained intuitively without clear theoretical justifications. It remains questionable whether this is the best way to select the set of training actions and what can be guaranteed by using such a set of selected actions.\n\nFor the experiments, no comparison with existing baselines was reported, making it hard to understand whether the new algorithm can achieve state-of-the-art performance on any benchmark reinforcement learning problems. The experimented benchmark problems are specific to cloud computing.  As a result, the general applicability of the proposed algorithm on various different reinforcement learning problems is doubtful.\n\nMeanwhile, the authors stated on page 5 that they assume the next environment state depends purely on the agent action. This assumption is often wrong for reinforcement learning. Therefore, the validity of their algorithm is also questionable.\n\nThe English presentation is not sufficiently clear for many parts of this paper. Substantial changes are required to improve the presentation quality and clarity of this paper."
            },
            "questions": {
                "value": "Why is it necessary or important to use the Shapley value to guide the selection of training actions?\n\nHow is $\\mathcal{U}$ is or learned for general reinforcement learning problems? Can the newly developed technique be easily applied to many different reinforcement learning problems and why?\n\nTheoretically, why should the best possible training action set include as many training actions as the global cut-off cardinality with the highest Action Shapley values?\n\nWhy is it possible to assume that the next environment state depends purely on the agent action?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698537967643,
        "cdate": 1698537967643,
        "tmdate": 1699635979949,
        "mdate": 1699635979949,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cOiDO86pam",
        "forum": "bVzLZr0S8s",
        "replyto": "bVzLZr0S8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission525/Reviewer_8N7h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission525/Reviewer_8N7h"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a method based on evaluation the Shapley value of\nthe actions, in order to rank them and select high value actions.\n\nEvaluation the Shapley value of an action requires summing over all\npossible subsets of value functions with and without that action. This\nis clearly computationally very expensive so the authors propose an\nincremental approach where subsets are tested for failures under a given\n\\epsilon parameter. If all, but \\epsilon subsets for a given\ncardinality produce unsuccessful RL agents, the computation is\nterminated. In this sense, they can cut-off some evaluations.\n\nThe proposed approach is tested of four similar domains with a\nrelative small set of actions. The authors showed that given the\nShapley values for the actions, in general, the system can achieve\nbetter and faster performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Apply Shapley values to the selection of actions\n- Show that some actions may not be needed, producing saving is training\ntime, without affecting performance.\n- Selecting the actions with best Shapley values have in general\nbetter performance"
            },
            "weaknesses": {
                "value": "- Computationally expensive\n- Applicable to very simple domains (discrete and deterministic with\nfew actions)"
            },
            "questions": {
                "value": "It is not clear why the authors mention that trial-and-error for RL\nin domains like Go or StarCraft are relatively inexpensive.\n\nAll the tests are performed in very similar domains, which questions\nthe applicability to other domains.\n\nIt is not clear how to select \\epsilon. A high \\epsilon means more\ncomputation, while a low \\epsilon.\n\nIt seems to be applicable only to discrete domains with a small number\nof possible actions. Also, the tests are performed on deterministic\nenvironments. Such conditions seem very restrictive for real world or\neven simple, domains.\n\nEvaluating the Shapley values is still computationally expensive, even\nwith the proposed algorithm. It is not clear how much an agent gains\nwith this approach. Once the values are known, the gain is quite\nclear, however, the authors do not report how expensive is to obtain\nsuch values. \n\nThe use of the Shapley value is not new in the literature. The main\ndifference in this paper is to use it for the selection of actions.\n\nSome terms are not properly described in the paper, e.g., g(a_t), \\phi\nThe paper has several English errors that need to be corrected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613799496,
        "cdate": 1698613799496,
        "tmdate": 1699635979884,
        "mdate": 1699635979884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t1tuIKUgbe",
        "forum": "bVzLZr0S8s",
        "replyto": "bVzLZr0S8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission525/Reviewer_mr6C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission525/Reviewer_mr6C"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to give the understanding of 'Superior interpretability demands granular under- standing of the differential impact of the training actions on the resulting RL agent performance'. To achieve it, this work provides an agnostic metric for the selection of training actions and provides a feasible method to calculate. The authors demonstrate the effective of their method in real-world tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper targets a valuable problem, that is selecting high performance training action set for RL. \n2. This work conducts experiments on real-world tasks to verify their effectiveness."
            },
            "weaknesses": {
                "value": "1. The presentation of this work is very poor. I have the following suggestions to greatly improve readability: (1) Add a Background and Notation section before methodology section. It is difficult to understand this method directly without relevant background knowledge. (2) The text description of the article is divided into appropriate paragraphs. This manuscript has only one paragraph for almost every chapter, which makes it tiring for the reader. (3) Table 3, Table 4, and Table 5 must be carefully arranged. (4) A label should be added to the display of pictures.\n\n2. The motivation of this paper is not explained well. I cannot get the deep insight from the current version of this manuscript. I believe this manuscript was hastily completed, and I believe the author may have solved some interesting problems. Please revise this manuscript according to my comments above before reviewing it."
            },
            "questions": {
                "value": "Please refer to the above weakness.\n\n--------\n\nThanks for the explanation. I maintain my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission525/Reviewer_mr6C"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684393171,
        "cdate": 1698684393171,
        "tmdate": 1700896735573,
        "mdate": 1700896735573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RZuLKWsMhV",
        "forum": "bVzLZr0S8s",
        "replyto": "bVzLZr0S8s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission525/Reviewer_oVx8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission525/Reviewer_oVx8"
        ],
        "content": {
            "summary": {
                "value": "Authors present a Shapley value calculation framework integrable in arbitrary RL algorithms. The randomized variant of the Shapley value computation method is applied in four settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality:\nTo the best of my knowledge, this is the first work to directly apply Shapley value constructs on action and use the computed values as a selection criterion.\n\n- Quality:\nSome interesting real-world application scenarios have been set up for evaluation.\n\n- Clarity:\nEvaluation setting is straightforward, and the randomized Shapley value calculation is explained clearly, with few-action examples.\n\n- Significance:\nIt is difficult to measure the significance of the paper, as the comparative analyses are weak."
            },
            "weaknesses": {
                "value": "Comparative analyses across several dimensions are rather lacking.\nAlong the conceptual axis, state-action values (Q-values) have long served as action selection criterion, but there is no mention as to how the Shapley construct offers any theoretical advantages or empirically observed performance gain. Moreover, in the full RL setting, the marginal contribution of any action is assumed to be under the influence of the state, hence q-values are a mapping from a state-action pair to a real value. However, the authors recede (and actually collapse) the problem into a contextual multi-armed bandit, where \u201cthe next environment state is determined purely by the agent action\u201d. One natural baseline in the CMAB setting would be the UCB algorithm and its variants, but none is compared against.\nAlong the evaluation axis, while the provided examples are motivating, some of the better known scenarios could help position the work more strongly.\nAlong the algorithmic analysis axis, it is hard to exactly measure the effects of the randomization, as there are only very few actions to begin with. Perhaps some asymptotic analysis between the baseline and the proposed algorithm could build a stronger scalability argument."
            },
            "questions": {
                "value": "What is a training action? How is it different from an action? What is the role of this new terminology?\nWhat is meant by \u201cmodel-based\u201d in the Conclusion section? Do we start with the MDP fully known? If that\u2019s the case, then what is the advantage of action Shapley over dynamic programming methods?\nIf we have so few actions to begin with, what is the advantage of action Shapley over Monte Carlo tree search, which will provide an exact solution?\nHow are the action Shapley values aggregated over different states?\nHow does action Shapley fare in, say, DQN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251813028,
        "cdate": 1699251813028,
        "tmdate": 1699635979729,
        "mdate": 1699635979729,
        "license": "CC BY 4.0",
        "version": 2
    }
]