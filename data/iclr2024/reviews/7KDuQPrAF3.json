[
    {
        "id": "Wzgl5HEZwI",
        "forum": "7KDuQPrAF3",
        "replyto": "7KDuQPrAF3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_jdxb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_jdxb"
        ],
        "content": {
            "summary": {
                "value": "The paper builds on top of Error Correction Code Transformer(ECCT), made a few structural change to build a \"foundational\" ECCT (FECCT) model for block code decoding. \n\nFECCT is a generalized version of ECCT:\n\n(1) FECCT is not length-dependent, H-dependent, and not even code-dependent, thus can be trained once can be used for wide range of block codes, with good performance (matching/beating ECCT mostly, and beat BP by a large margin.). Some finetuning can even lead to better performance.\n\n(2) FECCT could an important milestone of deploying neural decoders to real world given its potential, still a lot of hard work is required till that day."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Overall, FECCT is very plausible method, since FECCT is more like an \"decoding algorithm\" rather than simply \"neural decoder\". My definition of \"decoding algorithm\" means the input is H matrix  and received codeword  (just like BP algorithm), and the output should be decoded message. While other neural decoder has dependency on code/length/H/etc. As of my understanding, FECCT has learned some interesting advanced BP-like algorithm, that can be beneficial for a wide family of block codes.\n\n(2) FECCT is built on top of ECCT, the generalization performance on non-zero codewords are preserved, which makes training feasible. FECCT's H-dependent attention is a generalized version of ECCT's attention, which lead to better decoding capability. The proposed neural structure makes sense, and lead to good generalization performance.\n\n(3) The experiment on unseen code with different code family and block length are interesting, which make (1)'s claim stronger that FECCT is more of an \"decoding algorithm\".\n\nOverall, this becomes an interesting work, at least for neural decoder research, first time shows that a \"decoding algorithm\" rather than a complicated mapping can be learned."
            },
            "weaknesses": {
                "value": "1. The experiments are mostly built on short block codes (<128, test unseen for 255 at most), while typical capacity-approaching codes such as QC-LDPC has much longer block length. Performance on long block length is going to make this paper stronger, due to long block code's capacity-approaching performance.\n\n2. Interpretability: FECCT should have been learned some interesting algorithm, that can be interpreted as an advanced version of BP. We do see some part of interpretations in the appendix, but not solid enough to get insight on what FECCT's algorithm means.\n\n3. Complexity of network: attention-based neural network are very complex. Deploying the FEECT to any real world production requires some hard work on complexity reduction. In its current form, I am not seeing FECCT can be deployed to modem in short time.  Note that channel coding are heavily used in all modern wireless communication systems, which requires minimal latency, high throughput, and low cost."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627560274,
        "cdate": 1698627560274,
        "tmdate": 1699636097872,
        "mdate": 1699636097872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8bY4TIg8Rt",
        "forum": "7KDuQPrAF3",
        "replyto": "7KDuQPrAF3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to develop foundation model for Error Correction Codes that is trained on large data so that it can be used later for any downstream task. Specifically, authors aim to adapt the Transformer input embedding for robustness to code length variations. To learn the code structure, they use the positional embedding, that is integrated into the self-attention via a learned mapping of the node distances in the Tanner graph. Moreover, for code awareness and channel noise prediction, the paper employs a size-invariant prediction module that is conditioned on the parity-check matrix. In simulations, they tested on codes that are unseen during training. They showed that the proposed FECCT method matches or sometimes perform better than the\nstate of art."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-the paper takes a foundational approach to the decoding problem in error correcting codes, which is intellectually interesting. Clearly being able to decode any type of code is an interesting intriguing  exercise.\n- the paper advanced the design of generalist decoders relative to existing generalist decoders by using new embedding and grounding techniques."
            },
            "weaknesses": {
                "value": "-Although the design of foundational decoders are very interesting intellectual exercise, the real world impact of it is close to none if not zero.  The reason is that error correcting codes are designed and deployed once and their training is not a big deal even if someone takes deep neural network decoders as opposed to classical BP methods. But more importantly, there is another argument against the value of these generalist decoders: The important thing to recall is that capacity achieving codes exists for long codes and their BP decoders are close to ML performance, I.e., optimal decoders. So there is no gain of these deep neural decoders in long codes. The focus should be short codes for which we do not have good BP decoders. However, for short codes, we really do not need foundational decoders as one can design and easily train specialized neural decoders for the short codes that will very likely beat the performance of generalist decoders for all lengths. It is clear to believe that a generalist decoder will not be able to perform a specialized deep neural decoder for short lengths, unless the authors can show their generalist decoder can beat the performance of state of art short length (less that 200 bits) code decoders, specialized for that specific code length.\n\n-what is the performance in the tables? I see that 3 different values of Eb/N0 is used as the channel input signal (bit) power to noise ratio but What about reported numbers as performance. What are they? I like to see how these reported numbers translate to the error rate performance, as it is the only thing that matters in communication. It does not look like that the authors picked a particular error rate and report the corresponding Required Eb/N0 to achieve such an error rate. Because in that case the lower number is associated with the better scheme not the higher (as the authors stated in the paper).\n\n-the authors need to compare their scheme with Choukroun & Wolf (2022b) which is shown to be superior to ECCT."
            },
            "questions": {
                "value": "please compare the proposed generalist decoder at short lengths (less than 200 bits) with that of specialized decoders at those lengths. Because as I pointed out this is where these foundational decoders would show value if any.\n\n-please plot error rate plots rather than the reporting used in the paper which is not insightful.\n\n-it would be helpful to compare your proposed work with that of\nChoukroun & Wolf (2022b) which extends and enhances ECCT via Denoising diffusion in error correction codes and have far superior performance than ECCT.\n\n-the paper novelty is arguable in light of ECCT design architecture. For the most part following similar development as ECCT. Can the authors elaborate on the novelty relative to ECCT."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is none."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719635093,
        "cdate": 1698719635093,
        "tmdate": 1699636097793,
        "mdate": 1699636097793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YdrpK0fTYU",
        "forum": "7KDuQPrAF3",
        "replyto": "7KDuQPrAF3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
        ],
        "content": {
            "summary": {
                "value": "This work embarks on a very ambitious journey towards creating a foundation code for all downstream ECCs. The suggested structure here largely depends on a prior NeurIPS paper ([NeurIPS\u201922] Choukroun et al., Error Correction Code Transformer, presumably by the same authors) in that it is a specialized Transformer with bitwise embedding and parity-check-matrix-dependent masking, as appropriate for ECC. Albeit similar to prior work, the present proposal contains enough new materials and gives a highly convincing architecture based on code-aware aggregation that depends on the parity-check matrix as well as code-invariant bitwise embedding."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed idea is very ambitious, and is based on a highly innovative specialization of the Transformer to the classical problem of decoding received codewords of linear codes. The proposed ideas/strategies are very interesting and convincing (such as bitwise embedding independent of particular codes and incorporation of the parity check matrix in the embedding function). The impact on the field of digital communication and data storage could be large."
            },
            "weaknesses": {
                "value": "The main issue is that the training of the model is done using codes with lengths up to 150 only, hardly a sufficient length to reflect many modern codes of important applications (testing is also done on relatively small codes, with the largest being a 255 bit BCH). The popular LDPC codes are also curiously missing in the training as well as in the performance evaluation. Likewise for meaningfully long Polar codes. In this sense, I am not sure if the term \u201cfoundation model\u201d is justified here. In sum, the idea seems very good, but the validation comes short of a reasonable expectation. I do not feel just saying \"we have limited computing resources\" would be a good enough excuse for such an ambitious title.\n\nWritings on various parts seem direct copies from [NeurIPS\u201922] Choukroun et al., Error Correction Code Transformer. Try to differentiate."
            },
            "questions": {
                "value": "Please respond to the mentioned weaknesses above.\n\nIn Tables 1 and 2, the proposed method seem noticeably worse than ECCT on larger codes. Also, In Table 3, ECCT+DM+II gives the best results. Explanations would be  good."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859863764,
        "cdate": 1698859863764,
        "tmdate": 1699636097726,
        "mdate": 1699636097726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Oob4Qm7cH",
        "forum": "7KDuQPrAF3",
        "replyto": "7KDuQPrAF3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a model and is trained on multiple codes and can then be applied to an\nunseen code.\nTransformer architecture in multipleways: \n(1) a code-invariant initial embedding, which is also position- and lengthinvariant,\n(2) a learned modulation of the attention maps that is conditioned on the Tanner graph\n(3) a length-invariant code-aware noise prediction module that is based on the parity-check matrix"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.Error control coding implemtation on the deep learning technique is highly encourgaing.\n2.Authors got the optimized Results in terms of BER."
            },
            "weaknesses": {
                "value": "1.Conclusion should be rewritten based on the results presented mentioning the future scope."
            },
            "questions": {
                "value": "1.Conclusion should be rewritten based on the results presented mentioning the future scope."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1694/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1694/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699070780000,
        "cdate": 1699070780000,
        "tmdate": 1699636097660,
        "mdate": 1699636097660,
        "license": "CC BY 4.0",
        "version": 2
    }
]