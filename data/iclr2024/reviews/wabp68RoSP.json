[
    {
        "id": "v1nJAage4i",
        "forum": "wabp68RoSP",
        "replyto": "wabp68RoSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_1P1b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_1P1b"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method of incorporating active learning into the exemplar selection process for CoT prompts. Considering that current CoT method relies on a fixed set of human-annotated exemplars, which lacks adaptability for different tasks. The authors propose Active Prompting, selecting the most important and informative samples from the dataset as prompts. Believing that samples with the highest uncertainty are the most helpful, the authors introduce an effective strategy for selecting uncertain samples, along with four metrics for measuring uncertainty."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tCombining active learning with the selection process of prompt exemplars is a very intriguing and novel perspective.\n2.\tExperimental results from multiple datasets, coupled with comprehensive analysis, demonstrate the effectiveness of Active Prompt from various angles.\n3.\tIn Section 5.3, the authors' experimental results indicate that uncertain exemplars are transferable, showcasing the superior generalization capability of the method."
            },
            "weaknesses": {
                "value": "1.\tNeed for Additional Corpora: One significant advantage of CoT is its ability to leverage the model's generalization capabilities, requiring only a minimal number of task-related samples to teach the model the paradigm for solving tasks. In scenarios without corpora of the same distribution, such as ASDiv, SVAMP, and SingleEq, Active Prompt still needs to capitalize on the model's generalizability. However, it struggles to achieve the same performance boosts as datasets with training corpora like GSM8K and AQuA.\n2.\tDifferences in Uncertainty Measurement Methods: The authors introduced four methods of uncertainty measurement but did not delve deep into their differences. It appears that \u201cEntropy\u201d has better generalizability on datasets like StrategyQA compared to \u201cDisagreement\u201d. However, \u201cDisagreement\u201d outperforms on datasets like SVAMP and CSQA using code-davinci-002, as discussed in Question 2.\n3.\tConcerns Over Costs: Active Prompt seems to require an additional 1000*k API calls. Given the recommended value of k = 10, an extra 10,000 API calls seems to be a considerably high cost. Additionally, there's the cost associated with extra data annotation, as outlined in Question 3."
            },
            "questions": {
                "value": "1.\tActive learning is an iterative process. However, the method in the article undergoes only a single iteration. The authors also observed that 'the existing annotation of GSM8K is of high quality.' A concerning scenario arises when modifying prompt exemplars causes the model to become uncertain about samples it was previously confident about.\n2.\tWhat leads to the performance disparities between Active-Prompt (E) and Active-Prompt (D) across different datasets?\n3.\tFrom a cost perspective, does Active Prompt hold any advantages over methods like AutoCoT?\n4.\tLogically, the larger the value of k, the more accurate the model's uncertainty assessment should be. However, on the SingleEq dataset, a k value of 15 led to a noticeable performance decline. The reason given, 'In careful observation of the dataset, when k > 10, the number of the most uncertain questions is scarce, where confusion is no longer a problem,' is perplexing.\n5.\tMany current studies have adopted GPT-4 for label generation. For uncertain samples from datasets, can GPT-4 replace human annotation and achieve similar results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6624/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565397213,
        "cdate": 1698565397213,
        "tmdate": 1699636756363,
        "mdate": 1699636756363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ffyp2oEkpM",
        "forum": "wabp68RoSP",
        "replyto": "wabp68RoSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_hZjq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_hZjq"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method called Active-Prompt for adapting LLMs to different tasks with specific example chain-of-thought prompts. This method includes determining a subset of examples for each task/dataset based on uncertainty estimation and having human annotators annotate these examples with chain-of-thought reasoning. The authors present four methods for uncertainty estimation: disagreement, entropy, variance and self-confidence, but mainly apply disagreement and entropy based approaches stating that these outperform the rest. The authors compare their approach against baselines (CoT, Self-Consistency, Auto-CoT, and RandomCoT) on different math and commonsense reasoning problems, showing improved performance across different tasks. They also present an ablation study, discussing the effects of few-shot prompts, active selection, annotations, and uncertainty metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Overall the paper is written clearly and proposes an approach for example selection for chain-of-thought prompting. The method uses existing approaches from active learning and shows improvements over baselines.\n- The authors evaluate their approach on a range of mathematical and commonsense reasoning tasks, and conduct ablations to understand the effect of different factors."
            },
            "weaknesses": {
                "value": "- The approach seems to have limited applicability as it requires the existence of either large enough datasets for a particular task or similar task to sample from. The authors also report variations between different annotators, further attesting to the difficulty of the task.\n- Some details in the paper are missing. For example, how is the variance based approach applied to textual answers? There are no results presented with the self-confidence approach and only an example is given, etc."
            },
            "questions": {
                "value": "1- How will the approach generalize to new tasks?\n2- How is the variance based approach applied to textual response?\n3- In Figure 2, what is the intuition for accuracy decreasing with more number of predicted answers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6624/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762384505,
        "cdate": 1698762384505,
        "tmdate": 1699636756251,
        "mdate": 1699636756251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7npC6M6GGh",
        "forum": "wabp68RoSP",
        "replyto": "wabp68RoSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_DkmB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_DkmB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an uncertainty-based few-shot example selection/annotation method for LLMs. The motivation is annotating/selecting in-context examples for LLMs could be time-consuming and challenging. Since the few-shot examples significantly influence the downstream performance of LLMs, the authors propose to leverage uncertainty as the indicator to decide which examples should be selected from a large pool of candidate data.  Empirical evaluations demonstrate that the proposed method outperforms previous short and simple chain-of-thought annotations and improve the performance of LLMs."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea is straightforward and the motivation is clear. The method makes sense."
            },
            "weaknesses": {
                "value": "1. **Baselines are too weak, leading to a misunderstanding of the effectiveness of the proposed method.** I would like to urge the authors to include more powerful baselines in the experiment rather than hide them. ALL the reviewers are experts in this domain and familiar with the state-of-the-art performance of LLMs on these benchmarks in this domain. In the experiment section, the authors only include the CoT annotations from [1] as the most important baseline. It is widely acknowledged and studied that the complexity (i.e., the length or reasoning steps of the CoT annotations) significantly influences the performance of the LLMs. The annotations from [1] are very simple and short, only including some easy examples as in-context examples. In comparison, if we look at Page 17, the actual annotations from the authors are very long and detailed. Previous work [2] has already shown that by selecting the most complex examples from the training dataset, the performance can be largely improved compared to the original annotations from [1]. For example, by selecting the most complex examples, the performance of ChatGPT (i.e., gpt-3.5-turbo) can easily achieve more than 80% accuracy (without self-consistency) compared to the number 77.1% in Table 1. One may also refer to https://opencompass.org.cn/leaderboard-llm for the performance of LLMs (I acknowledge that the performance of ChatGPT on GSM8K from that website is possibly still underestimated). Without comparison with SOTA's performance, I will try my best to reject this paper. Please do not try to hide the best baselines.\n\n2. **More ablation study is required.** Again, the performance improvement may come from two aspects. The first is selecting the most uncertain examples, and the second is making the CoT annotations longer. The annotations in baseline [1] are much shorter compared to the annotations by the authors. Without the ablation studies on these two aspects, we cannot determine whether the performance improvement truly comes from the author's contribution or just longer CoT annotations.\n\n3. **The method is simple with limited contribution, while performance improvement is not significant.** The method is quite intuitive and can be regarded as an in-context example selection method (followed by annotations). The authors should discuss the relationship with other in-context example selection methods and compare the performance. Existing performance improvement is quite limited. Once more baselines are included, it is very possible that the performance will be surpassed.\n\n[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022b.\n\n[2] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In ICLR 2023"
            },
            "questions": {
                "value": "Please refer to the weakness above. Without my concerns properly addressed (more sufficient and reasonable baselines), I will strongly reject this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6624/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871141905,
        "cdate": 1698871141905,
        "tmdate": 1699636756132,
        "mdate": 1699636756132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q9hhaYKQsD",
        "forum": "wabp68RoSP",
        "replyto": "wabp68RoSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_Wb4R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6624/Reviewer_Wb4R"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new few-shot prompt construction method for LLMs that is inspired by active learning. Assuming access to some training instances, the paper proposes to include as in-context learning examples those that the model is most uncertain about. If these instances do not come with labels, they are manually annotated. This is achieved by testing the model on (a subset of) the training data and finding the instances that yield the highest uncertainty measured by (1) entropy or (2) disagreement. (1) and (2) lead to two variants of the proposed model.\n\nExperiments are conducted on reasoning and QA tasks, with the OpenAI models. The analysis is extensive and insightful. Overall the paper presents an interesting and intuitive idea, and the execution is great. However, I have three major concerns that lead me to vote for a rejection (details below). I am happy to revisit this if the authors can address my concerns."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Combining active learning with prompt construction is interesting and novel to me\n- With the extensive experiments and analysis, the execution is definitely above average\n- Writing is clear"
            },
            "weaknesses": {
                "value": "- [Major] An important and very relevant baseline is missing: https://arxiv.org/abs/2210.00720. Their method is very similar to Active Prompt and simply selects the longest training instances. I would be curious to see how it compares to this work.\n- [Major] One can imagine that if the model is reasonably good, the demonstrations selected by Active-Prompt will be more useful. I wonder whether this is still the case for \u201cweaker\u201d models. If the model does not know too much about the task, will the prompts selected by its uncertainty still be useful? This can be tested out by trying Active-Prompt on, e.g., one of the smaller Llama models.\n- [Major] The conclusion drawn on the transferability of prompts found by Active-Prompt in 5.3 needs more evidence. All the models tested are from the GPT-3 family, which are finetuned from the same base model. It is unclear whether, e.g, the prompts found by GPT-3.5 perform well for Llama. This concern is important since it directly determines how useful Active-Prompt is in practice. If the prompts do not transfer across different model families, it will have a huge overhead annotating a new set of instances for a different model. Besides, it makes it impossible to do fair comparisons among models controlling the prompts. I suggest adding an experiment studying the transferability between GPT and Llama models.\n- To draw conclusions on the transferability of the prompts, Table 3 should compare, e.g., CD-002->TD-002 (SC) with TD-002->TD-002 (SC), instead of the non-Active-Prompt baseline.\n- Some of the wordings are confusing, even misleading. Please see the details below. \n- A clear limitation of Active-Prompt is the high cost associated with doing inference runs over the training set. A discussion about this would be nice."
            },
            "questions": {
                "value": "Below are comments instead of questions, and the authors do not need to answer them.\n\n- The end of page 2, $q_i$ is overloaded, and it is hard to distinguish between instances from training and test data. Adding a superscript or changing the letter could help.\n- Above Eq. 2: is \u201cArabic answers\u201d a typo? Do the authors mean \u201carithmetic\u201d instead?\n- Below Eq.3, $P_{\\theta}$ is a distribution, not a random variable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6624/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698893482731,
        "cdate": 1698893482731,
        "tmdate": 1699636756018,
        "mdate": 1699636756018,
        "license": "CC BY 4.0",
        "version": 2
    }
]