[
    {
        "id": "wCe12Ato6W",
        "forum": "0Nui91LBQS",
        "replyto": "0Nui91LBQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_TFPg"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method (SEED) to augment large language models (LLMs) with the capability of processing and generating visual data, i.e., images. The core contribution of SEED is a quantized tokenizer that learns to encode images into discrete visual tokens which can be again decoded using a pre-trained generative model. Once the tokenizer is trained, a LLM is trained and fine-tuned on interleaved image-text data such that the LLM can both process and generate the visual tokens which make it applicable to a variety of vision language tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- SEED is well motivated in learning a 1D token representation that better aligns with the auto-regressive generative process of LLMs.\n- Architectural choices are reasonable and have been validated with ablation studies to the most extend, i.e., text vs visual embedding reconstruction, embedding vs. discrete code, causal vs. bilateral visual codes, full fine-tuning after LoRA, instruction fine-tuning.\n- The quantitative evaluation convinces in either surpassing or being competitive in image-text, video-text tasks as well as generative image-to-image and text-to-image tasks.\n- The qualitative results showcase some interesting multi-modal capabilities of SEED, including compositional and in-context image generation.\n- Publishing both code and checkpoints of large-scale models enables future research and empowers the open-source community."
            },
            "weaknesses": {
                "value": "- Some architectural design choices are missing an explanation.\n    - In general, it would help the clarity of the paper if all loss functions would be written out.\n    - What is the reason behind using two different image encoders, one for encoding the input to the causal q-former (BLIP-2 ViT) and one for the image generation conditioning (unCLIP-SD vision encoder)? This requires loading more weights into memory during training so an explanation is needed. Can we use the unCLIP-SD vision encoder for both cases?\n    - How important is the contrastive loss between the vision and text embeddings? An ablation could justify it's inclusion.\n    - Why was the original VQ codebook loss replaced by a simple reconstruction loss with cosine similarity? How are collapsing codebooks avoided? Are there any stop-gradient operation in the loss for the codes?\n- The arguments and ablation for using causal vs. bilateral visual codes is not convincing. In general, previous work on VQ models have demonstrated that transformers can learn complex and even low-level dependencies of non-causal codes. Enforcing a causal mask in the q-former restricts the information flow to fully utilize the tokens efficiently and effectively. The argument made in Sec. 4.3 is that the LLM struggles with generating the correct sequence length for images which should always be 32. It is surprising that this happens because the start token for images (as in Fig. 4) should be a clear signal to the LLM that 32 image tokens follow. In practice however, it would be straightforward to enforce the generation of 32 image tokens after the start token is observed by restricting the possible output tokens. How do these two models compare when the number of image tokens is enforced to be always 32?\n- It is not clear how videos are being processed. Are individual frames used to train the tokenizer or are multiple frames passed to the causal q-former? If multiple frames are passed, how do you adjust the reconstruction loss for the generative embedding (1 embedding per frame from unCLIP-SD vs. one embedding per video from your tokenizer)? Do you simply append the encoding of multiple frames when passing videos to the LLM?"
            },
            "questions": {
                "value": "- How did you decide on using 32 tokens per image and 8192 codes? \n- Can you confirm that you are using a start and end token for images as shown in Fig. 4? Do you use the same start/end tokens for both images and videos? This information should be included in the paper.\n\nSuggestions:\n- It would help the read flow to already mention the codebook size in section 3.1.2 instead of only in 3.2.1 (i.e., 8192 visual codes).\n- When Table 2 is first discussed in Sec. 4.1, it is unclear what $\\text{SEED}^{\\text{text}}$ refers to. A short description and reference to the ablation in Sec. 4.3. would better facilitate immediate understanding for the reader. Similarly for the \"I\" suffix referencing the instruction-tuned model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705889770,
        "cdate": 1698705889770,
        "tmdate": 1699636082825,
        "mdate": 1699636082825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OFcjdHZdiE",
        "forum": "0Nui91LBQS",
        "replyto": "0Nui91LBQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
        ],
        "content": {
            "summary": {
                "value": "This work introduces an image tokenizer, which is capable of discretizing images into a series of tokens. These image tokens are transformed by Q-Former into pseudo-causal tokens that can serve as the input for Large Language Models (LLMs), most importantly, they can also act as the target. This allows the model to unify both visual understanding and generation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is the first (at least to my knowledge) to use an image tokenizer to unify visual understanding and generation tasks in LLM, providing a feasible pipeline."
            },
            "weaknesses": {
                "value": "1. The authors claim that the use of q-former can establish a causal dependency, which I find questionable. This is because the attention in the visual encoder stage is bidirectional, leading to potential information leakage.\n\n2. Regarding the visual understanding task results shown in Table 3, why does SEED-LLaMA-I (14B) perform no better (or nearly the same) as SEED-LLaMA-I (8B) on some Image-Text Tasks? Does the proposed method not yield much gain on larger models, or has it already reached saturation?\n\n3. In Table 2, SEED-LLaMA-I achieves good results. However, I believe that CLIP similarity does not effectively reflect the quality of generation. Fr\u00e9chet Inception Distance (FID) is a widely accepted better evaluation method, but unfortunately, this paper does not provide it.\n\n4. Regarding Section 4.3 (Causal Visual Codes vs. Bilateral Visual Codes), the authors mention that some mode collapse may occur for generation tasks, but what about understanding tasks?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1543/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1543/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1543/Reviewer_i2L6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762803796,
        "cdate": 1698762803796,
        "tmdate": 1700661306542,
        "mdate": 1700661306542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SCfwSnUuRp",
        "forum": "0Nui91LBQS",
        "replyto": "0Nui91LBQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_y9Sh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1543/Reviewer_y9Sh"
        ],
        "content": {
            "summary": {
                "value": "Making the language model to see the words is one of the key research direction. This paper present new image tokenization on this direction. Specifically, unlike prior attempts that uses simple 2d style image tokenization (usually VQ-VAE), this paper propose SEED, which makes image embedding to be left-to-right 1d tokenization similar to the text while keeping semantic meaning of images but discarding low-level information. This paper claim that capturing too low-level information hider the performance of LLMs to effectively perform multimodal comprehension."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Unifying vision and text representation is one of the hot research topic. \n(2) The assumption behind the proposal is reasonable. \n(3) The paper is generally well-written."
            },
            "weaknesses": {
                "value": "(1) The methodology is not religiously explained and is not self-contained. Especially, section 3.1 is hard to follow. There are no equation, and it is hard to track which components are trained on which objective function. \n\n(2) In section 4.1, they compared SEED tokenization on image-text retrieval. As described in the paper, SEED generally outperform BLIP-2, in some case BLIP-2 exceed the proposed method. However, there are no explanation on this point. Similar criticism can be applied for the analysis on Table 3. \n\n(3) Regarding the Figure 7, I'm afraid the actual prompt is hard to imagine. \n\n(4) The proposed method contains several components, including image encoder, codebook, text encoder, and generation module. However, the importance of all components is less discussed, making me hard to access the importance of the specific choice of each component. \n\n(5) I'm afraid that I found a statement in the introduction is not fully validated. \"Moreover, we empirically found that the dominant tokenizer VQ-VAE (Van Den Oord et al., 2017) in existing works captures too low-level information for LLMs to effectively perform multimodal comprehension task\". Could you please clarify again which results support the above statement? \n\n(6) While the paper motivate to learn good 1D representation is key to incorporate visual information into pre-trained LLMs, but less discuss on why we should make the representation discrete rather than continuous. Table1 also seems to show that the continuous representation is generally better than discrete representation."
            },
            "questions": {
                "value": "See weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1543/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699282327755,
        "cdate": 1699282327755,
        "tmdate": 1699636082659,
        "mdate": 1699636082659,
        "license": "CC BY 4.0",
        "version": 2
    }
]