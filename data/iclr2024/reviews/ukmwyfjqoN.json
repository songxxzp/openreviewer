[
    {
        "id": "AvUrXhWWiM",
        "forum": "ukmwyfjqoN",
        "replyto": "ukmwyfjqoN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission737/Reviewer_ki94"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission737/Reviewer_ki94"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a fast and efficient framework titled Recurrent Bottleneck Mixer Network (ReBotNet) for performing real-time video enhancement. This can carry practical applications in areas such as live video calls and video streams. The novelty of ReBotNet lies in its dual-branch system. The first branch utilizes a ConvNext-based encoder to learn spatio-temporal features by tokenizing the input frames along the spatial and temporal dimensions. These tokens are then processed with a bottleneck mixer. The second branch enhances temporal consistency by directly employing a mixer on tokens extracted from individual frames. The branches converge, with a common decoder merging the features to predict the enhanced frame.\n\nAdditionally, the authors use a recurrent training approach where the prediction of the last frame is utilized to efficiently improve the current frame while enhancing temporal consistency. The effectiveness of this method is evaluated on two newly curated datasets representing real-world video calls and streaming scenarios. The results obtained indicate that ReBotNet outperforms existing techniques with less computation, minimal memory requirements, and faster inference time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Overall, the method proposed in this paper is novel and unique. Previous methods usually embed optical flow estimation explicitly or implicitly.\n2. The goal of the paper is to propose a real-time video enhancement model, which I think has practical value.\n3. The comparative experiments in the paper include different computational complexity levels and user studies.\n4. The authors provide two new datasets."
            },
            "weaknesses": {
                "value": "1. The paper only conducts experiments on the newly proposed datasets. But these two datasets are not larger or more extensive than previous datasets, so I worry that the experiments will not be convincing enough.\n2. The paper does not seem to have submitted a demonstration video. Judging from the pictures in the paper, the improvement in visual effects is not significant."
            },
            "questions": {
                "value": "1. \"Unlike these works that require compute intensive optical flow, we develop a simple and efficient frame-recurrent setup with low computational overhead.\" Optical flow calculations do not seem to be necessarily linked to high computational overhead. Refer to \"Optical flow estimation using a spatial pyramid network\" or \"Real-time intermediate flow estimation for video frame interpolation\".\n2. \"A major use case for real-time video enhancement is videoconferencing where the video actually contains the torso/face of the person. \" This sentence does not seem to be enough to support the paper's experimentation in this scenario only. I am particularly worried that the background of these scenes is static, and the movement of faces is different from ordinary objects, making it impossible to judge the generalization of the proposed method in general scenes.\nEven though real-time video conferencing is an important requirement, low-overhead video enhancement makes sense for many other users.\n3. The author mentioned \"The training is parallelized across 8 NVIDIA A100 GPUs, with each GPU processing a single video. \", is this training method fair to other models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission737/Reviewer_ki94",
                    "ICLR.cc/2024/Conference/Submission737/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698039808438,
        "cdate": 1698039808438,
        "tmdate": 1700643480030,
        "mdate": 1700643480030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yGydVvUGws",
        "forum": "ukmwyfjqoN",
        "replyto": "ukmwyfjqoN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission737/Reviewer_PgtF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission737/Reviewer_PgtF"
        ],
        "content": {
            "summary": {
                "value": "This work aims to break through the limitations of existing video enhancement methods in terms of processing speed and introduces a real-time video enhancement method designed for video calls and streaming scenarios. The authors achieve this by constructing a dual-branch framework, each addressing spatial and temporal feature information separately, and then merging them to obtain the final result. A recursive training strategy is further proposed to effectively utilize the predictive information from the previous frame to enhance the prediction quality of the current frame. Additionally, the authors introduce two new video enhancement datasets based on existing data to assist in evaluation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The author provides a clear explanation of the motivation behind network architecture and training strategy design.\n2. The experimental validation section is relatively comprehensive, and the description of relevant settings is sufficiently detailed."
            },
            "weaknesses": {
                "value": "1. Although the author has conducted a thorough analysis of the constructed network in terms of details and motivations, it appears challenging to avoid the fact that the techniques used in this work seem to be readily available. It is hoped that the author can further refine the contributions and strengths of the proposed method in this regard. In other words, there may still be room for improvement in terms of innovation in this work.\n2. The author uses a direct addition approach to achieve fusion after the two branches, and I'm curious whether the author has tried other methods to better integrate information related to tubelet tokens and image tokens. In other words, I hope the author can provide a brief explanation or analysis of their choice of fusion method.\n3. The author's introduction to the curated dataset is not sufficiently clear. It is recommended that the author provide a more intuitive comparison between the curated dataset and existing datasets in terms of data quantity, types of data degradation, and whether they are paired, possibly in the form of a table. Some visual examples should also be included in the manuscript. Additionally, the author does not seem to mention whether these two datasets will be made open-source, which has a certain impact on the contribution of this work.\n4. The scenarios targeted by this work are closely related to everyday life. I am quite curious about the performance of the proposed method on some real video data captured using mobile devices. Could the author possibly add relevant experimental results to more comprehensively validate the effectiveness of the proposed method?\n5. There appear to be some typographical errors in the manuscript. In the analysis of ReBoNet in Section Five, the author mentions \"Table 3 illustrates these results where gray rows correspond to ...\", but there doesn't seem to be any gray rows in Table 3. We hope the author can carefully review the manuscript to prevent such situations from occurring."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604278104,
        "cdate": 1698604278104,
        "tmdate": 1699636000688,
        "mdate": 1699636000688,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WYBkHliTNM",
        "forum": "ukmwyfjqoN",
        "replyto": "ukmwyfjqoN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission737/Reviewer_e4UV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission737/Reviewer_e4UV"
        ],
        "content": {
            "summary": {
                "value": "This paper presents ReBotNet, designed for real-time video enhancement. The proposed dual-branch system utilizes spatio-temporal tokenization of frames and combines features from both branches to improve the output. A recurrent framework is employed to include previous frame predictions, ensuring better temporal consistency. The methods have been tested on newly curated datasets, demonstrating state-of-the-art results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The effectiveness of the proposed method was demonstrated through testing on two specially created datasets, mimicking real-world video scenarios. The results reveal that ReBotNet surpasses existing methods, offering faster performance, less computation, and minimized memory use. ReBotNet can be significantly useful in practical applications. The authors also tried to conduct a fair evaluation by optimizing the results of previous research."
            },
            "weaknesses": {
                "value": "There are few weaknesses observed in this paper. \n\n1. The novelty of this research compared to other studies is not clear, because there have been many studies on the two-branch framework that processed video cubes with two different temporal dynamics as input. \n\n2. Rather, it seems that the authors have optimized module that were already working well, and it does not appear that a comprehensive experiment has been conducted to justify their approach, by elucidating the importance of the optimized modules shared with the previous studies, if any.\n\n3.  Since the paper focuses on practicality, it is necessary to show experiment results on more open datasets to demonstrate its more general applicability. \n\n4. Although they claimed to have created a video dataset with real-world nose, the experiment only compared using only the video restoration task and a few simple metrics, failing to effectively demonstrate its significance."
            },
            "questions": {
                "value": "It would be appreciated if the authors could resolve the reviewer's concerns above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659053374,
        "cdate": 1698659053374,
        "tmdate": 1699636000601,
        "mdate": 1699636000601,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yYj8X919cM",
        "forum": "ukmwyfjqoN",
        "replyto": "ukmwyfjqoN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission737/Reviewer_SWLx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission737/Reviewer_SWLx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an efficient video enhancement framework. The network architecture utilizes ConvNext for Spatial-Temporal tokenization of the video and employs a Mixer structure to process the tokens. To evaluate the algorithm, the method introduces two datasets, on which the algorithm performs well (and better than baselines). Overall, it is an effective framework for video enhancement and does make some contributions. However, it is difficult to pinpoint the novelty of the algorithm. Currently, I find it hard for me to make a final decision."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+: Well-written. I can clearly understand the design motivations behind most parts.\n\n+: On the two datasets, this algorithm achieves a good balance between efficiency and effectiveness, surpassing many previous algorithms.\n\n+: There is some ablation study to analyze the role of each branch."
            },
            "weaknesses": {
                "value": "-: The explanation of the contribution to efficiency and effectiveness is not clear enough. The introduction mentions that the combination of ConvNext and Mixer avoids quadratic complexity and guarantees performance. Does it mean that using Mixer speeds up the process (avoiding quadratic complexity), while ConvNext ensures performance? Is the fundamental reason for the acceleration avoiding quadratic complexity?\n\n-: The PSNR and SSIM results are similar to RBRT.\n\n-: The evaluation of temporal consistency seems lacking in the paper. For instance, although the abstract claims that the second branch improves temporal consistency, there are no experiments to support this result.\n\n-: I believe the ablation study is not detailed enough. Combining ConvNext and Mixer is a direct and straightforward idea. In the process of parameter tuning, what analytical experiments are worth providing to the community? I expect to see many detailed experiments on this aspect. Can we use other backbones to replace the ConvNext?"
            },
            "questions": {
                "value": "-: Creating their own dataset is good, but why not compare it with public datasets and other baselines at the same time? Theoretically, this model can also be run and compared on other datasets, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission737/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission737/Reviewer_SWLx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764039768,
        "cdate": 1698764039768,
        "tmdate": 1699636000516,
        "mdate": 1699636000516,
        "license": "CC BY 4.0",
        "version": 2
    }
]