[
    {
        "id": "RpMaS19mBD",
        "forum": "PQbFUMKLFp",
        "replyto": "PQbFUMKLFp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_yaab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_yaab"
        ],
        "content": {
            "summary": {
                "value": "The conjugate gradient method, a critical first-order optimization technique, typically exhibits faster convergence compared to the steepest descent method and demands significantly lower computational resources than second-order methods. Nevertheless, despite extensive research on various forms of conjugate gradient methods in Euclidean spaces and Riemannian manifolds, there has been limited exploration of such methods in distributed scenarios. This paper introduces a novel approach called the Decentralized Riemannian Conjugate Gradient Descent (DRCGD) method, designed to minimize a global objective function defined on the Stiefel manifold. This optimization problem is distributed among a network of agents, each associated with a local function, with communication occurring over a connected, undirected graph. Global convergence of DRCGD over the Stiefel manifold is proved. Numerical experiments demonstrate the advantages and efficacy of the DRCGD approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written: basic definitions of optimization on Riemannian manifolds are recalled and the proposed algorithm is well explained. Furthermore, the latter is quite simple and hence has a great practical interest. Classical conjugate gradients on Riemannian manifolds have much faster convergence compared to the plain Riemannian gradient descent. Hence, proposing a decentralised Riemannian conjugate gradient descent can be well received by the community. Moreover, the global convergence of the proposed algorithm is proved whereas it is far from being trivial. Finally, numerical experiments show a practical interest to the proposed method.\n\nThe paper is overall of great quality."
            },
            "weaknesses": {
                "value": "The claim 2 in the introduction as well as the section 4.1 are misleading. Indeed, the authors mention they don't use retraction or vector transport to reduce the computation. However, they use the orthogonal projection onto the Stiefel manifold, which is a retraction, and the othogonal projection onto the tangent space, which is a vector transport (see \"Optimization on matrix manifolds\" from Absil et al. 2008). This claim should be removed.\n\nIn section 4.2, the equation (20) is not clear since $T_{alpha_k\\eta_{i,k}^R$ is not defined. Hence, it is hard to appreciate if this hypothesis is reasonable or not. Same thing for assumption 3 (iii).\n\nThe paper lacks an overview of the poof to get the global convergence. The different proofs are long and technical and an overview would help the reader.\n\nThe numerical experiments section lacks the presentation of DRDGD and DPRGD. It would be interesting to better understand the differences with the proposed method.\n\nSeveral passages in the proofs are unclear. See the questions."
            },
            "questions": {
                "value": "- Assumption 1: usually a doubly stochastic matrix is defined with positive elements and row and columns that sum to 1. Can you comment how does it relate to your definition?\n- Section 4.1: \"The Riemanian gradient step with a unit step size, i.e., ...\" is it a unit step size or a null/zero step size?\n- Assumption 3 (iii): what is $T_{alpha_k\\eta_{i,k}^R$ ?\n\nProofs:\n- Lemma 1, eq (24): how do you get second and third inequalities. For me, there is something wrong here.\n- Theorem 2 is independent from the conjugate gradient. Is it new or is it a known result from a different paper?\n- Theorem 2 assumes that $\\eta_{i,k}=0$. In the proposed algorithm, you jointly do a gradient descent and average the iterates of the different nodes, hence $\\eta_{i,k}\\neq 0$. Can you comment this?\n- After eq (31), you mention that $x_{i,k_0+1} \\to x_{i,k_0}$. I don't understand at all this limit. Is it a mistake?\n- Second inequality in eq (36), can you explain how do you get it?\n\nTypos:\n- Definition 3: (ii) $x_x$ is $0_x$.\n\nNotations:\n- $P_{St}$ and $P_M$ are the same.\n- Section 4.2: $g_{i,k+1}$ is not introduced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1095/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1095/Reviewer_yaab",
                    "ICLR.cc/2024/Conference/Submission1095/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1095/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698092096463,
        "cdate": 1698092096463,
        "tmdate": 1699963985082,
        "mdate": 1699963985082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "reK3O6pmRk",
        "forum": "PQbFUMKLFp",
        "replyto": "PQbFUMKLFp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_86eb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_86eb"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests an extension of conjugate gradient on Stiefel manifold for distributed setting and provides convergence guarantees for this algorithms. The approach is based mainly on the Xiaojing Zhu's original papers"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The setting of decentralized optimisation is crucial in applied optimisation, and extending one of the most practically efficient algorthms to it is topical. Riemannian generality here requires carefull trheoretical justifications and proper choice of tools to prevent big computational complexity. Paper indeed propose a good solution for solving optimisation problems on Stiefel manifold."
            },
            "weaknesses": {
                "value": "Empirical study is not comprehensive: there was not presented a comparison of the proposed approach with alternatives. Besides, form of convergence guarantees is not exhaustive, because the rate of the convergence is not established. Theoretical framework is mostly inherited from Zhu's original papers, but that analysis does not allow providing guarantees on convergence rate, so does not this paper, which means that there were no significant extending of that framework."
            },
            "questions": {
                "value": "1. Typo in \"Lemma 3 In Alogrithm\"\n2. What about time-varying case? Conidering the case of time-varying graph would be important for all-around extending CG for decentralised setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1095/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1095/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1095/Reviewer_86eb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1095/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777962441,
        "cdate": 1698777962441,
        "tmdate": 1699636035812,
        "mdate": 1699636035812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bPyzhnKKkp",
        "forum": "PQbFUMKLFp",
        "replyto": "PQbFUMKLFp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_hbTT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1095/Reviewer_hbTT"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a decentralized Riemannian conjugate gradient descent (DRCGD) algorithm for distributed optimization, and proves the global convergence of the algorithm. Compared with existing state-of-the-art algorithms, DRCGD uses a projection operator that searches the direction instead of retraction and vector transport, thus reducing computational costs. Through the simulation of eigenvalue problem, the paper shows that DRCGD has better performance than state-of-the-art algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "originality and significance: This paper has good originality and significance since it presents the first decentralized Riemannian conjugate gradient descent (DRCGD) algorithm for distributed optimization and proves the global convergence of the algorithm.\n\nquality: The proposed algorithm is supported by solid theory and verified by simulation.\n\nclarity: The overall narrative logic of the article is clear."
            },
            "weaknesses": {
                "value": "1. There is still room for improvement in the clarity of the proof. Some symbols that appear in the convergence analysis section of the text, such as $g_{i,k+1}$, $\\mathcal{N}$, and $C$, are not defined in the text.\n\n2. It seems that there are some assumptions about the step size $\\alpha_{k}$ that are not mentioned in Assumption 3 about $\\alpha_{k}$ in the body of the proof, such as the assumption about $\\alpha_{k}$ in Lemma 2. This leads to unclear assumptions about $\\alpha_{k}$.\n\n3. The measures mentioned in the simulation should converge towards 0, which is not well demonstrated in the experimental results. For example, the measures in Figure 3 tend to be constant after it drops to a certain level. This does not support the theoretical results very well.\n\n4. The definition of doubly stochastic matrix seems to be $\\sum_{i} x_{ij}=\\sum_{i} x_{ij}=1$, which is different from the definition in Assumption 1."
            },
            "questions": {
                "value": "1. What is the definition of $x_{x}$ in Definition 3 (ii)?\n\n2. It seems like there are many assumptions of the parameters such as $\\alpha_{k}$. Are these assumptions easy to satisfy?\n\n3. In this paper, the decreasing step size is used in the convergence proof, while the fixed step size is used in the simulation. Does this difference affect convergence?\n\n4. What is the significance of the eigenvalue problem used in the simulation in real life?\n\n5. In general, since the problems solved are the same, the convergence result of the distributed algorithm should be independent of the structure of the graph if the assumptions about the graph are satisfied. In Figure 3 of the simulation, the same algorithm seems to converge to different solutions under different graphs. Why did this happen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1095/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698929789547,
        "cdate": 1698929789547,
        "tmdate": 1699636035707,
        "mdate": 1699636035707,
        "license": "CC BY 4.0",
        "version": 2
    }
]