[
    {
        "id": "G3LsiDFmOs",
        "forum": "VPx3Jw2MSk",
        "replyto": "VPx3Jw2MSk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_7mFW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_7mFW"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on the Q-function value overestimation issue. Observing that the overestimation issue will latter becomes underestimation during the learning process. Thus motivated, this work proposes the Blended Exploitation and Exploration (BEE) operator to take advantage of the historical best-perforation actions. The proposed operator is then used in both model-free and model-based settings and show better performance than previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed BEE operator utilizes the Bellman exploitation operator and exploration operator to address the under-exploitation issue. The proposed operator can be easily incorporated into the RL algorithms.\n2. The experiments show that the proposed operator can effectively reduce the estimation error and achieve better performance comparing with other RL algorithms."
            },
            "weaknesses": {
                "value": "1. The terminology can be misleading. The overestimation issue in the Q-value approximation generally is due to the changing order of expectation and $\\max$. It is incorrect to say that  the $Q$-function will have \"underestimation when encountering successes\" in Fig 1 (a). The authors need to clarify the context and difference of the statement in order to avoid confusion.\n2. In order to investigate on the under-exploitation, the metric $\\Delta(\\cdot,\\cdot)$ is defined on the current Q-function approximation. Intuitively,   $\\Delta(\\cdot,\\cdot)$ shows that the current Q-function approximation can be either overestimate or underestimate given different policy, i.e., $\\mu_k$ and $\\pi_k$. It is unclear what is the meaning of this metric. Considering most of the algorithm will update the policy and Q-function approximation at the same time, e.g., Actor-Critic, the Q-function should be evaluated under the current policy instead of the policy obtained earlier. The authors need to clarify why the definition here makes sense for the under-exploitation investigation."
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737963334,
        "cdate": 1698737963334,
        "tmdate": 1699636492844,
        "mdate": 1699636492844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lPjjQX1eD6",
        "forum": "VPx3Jw2MSk",
        "replyto": "VPx3Jw2MSk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_FAWm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_FAWm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the Blended Exploitation and Exploration (BEE) operator, which addresses the issue of value underestimation during the exploitation phase in off-policy actor-critic methods. The paper highlights the importance of incorporating past successes to improve Q-value estimation and policy learning. The proposed BAC and MB-BAC algorithms outperform existing methods in various continuous control tasks and demonstrate strong performance in real-world robot tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses an important issue in off-policy actor-critic methods and proposes a novel approach to improve Q-value estimation and policy learning. \n- The BEE operator is simple yet effective and can be easily integrated into existing off-policy actor-critic frameworks.\n- The experimental results demonstrate the superiority of the proposed algorithms in various continuous control tasks and real-world robot tasks."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed approach is limited. \n2. The choice of $\\lambda$ is largely empirical and requires extra manipulation in new tasks.\n3. The paper only provides basic theoretical analysis, such as the accurate policy evaluation and the guarantee of policy improvement. The benefit of linearly combining two Q-value functions is not discussed theoretically.\n4. The experiments are conducted in continuous control tasks with dense rewards. The exploration ability can be better evaluated in environments with sparse rewards.\n5. There is a lack of discussions with related papers (See Question 3)."
            },
            "questions": {
                "value": "1. Emprically, the BAC algoithm will only be more efficient in exploiting the replay buffer. The exploration still rely on the maximum-extropy formulation in SAC. Then why can BAC perform significantly better than SAC in failure-prone scenarios such as HumanoidStandup, as if BAC can better explore the unknown regions?\n2. Can you discuss or exhibit the performance of BAC in some tasks with sparse rewards? This can demonstrate the generalizability of the proposed approach.\n3. What are the advantages of BAC compared with prioritized replay methods [1,2] or advantage-based methods [3]? These methods are related to BAC in that they also exploit the replay buffer with inductive bias, so they should be mentioned in the paper.\n\n[1] Sinha, S., Song, J., Garg, A. &amp; Ermon, S.. (2022). Experience Replay with Likelihood-free Importance Weights. Proceedings of The 4th Annual Learning for Dynamics and Control Conference.\n\n[2] Liu, X. H., Xue, Z., Pang, J., Jiang, S., Xu, F., & Yu, Y. (2021). Regret minimization experience replay in off-policy reinforcement learning. Advances in Neural Information Processing Systems, 34, 17604-17615.\n\n[3] Nair, A., Gupta, A., Dalal, M., & Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.\n\n\nI am willing to raise my score if my concerns for weaknesses and questions are adequately discussed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5034/Reviewer_FAWm",
                    "ICLR.cc/2024/Conference/Submission5034/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742958255,
        "cdate": 1698742958255,
        "tmdate": 1700565416102,
        "mdate": 1700565416102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u0vqaE4gNM",
        "forum": "VPx3Jw2MSk",
        "replyto": "VPx3Jw2MSk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_kjkr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5034/Reviewer_kjkr"
        ],
        "content": {
            "summary": {
                "value": "Motivated by the problem of underestimating values in the training of SAC, this paper introduces the Blended Exploitation and Exploration (BEE) operator, which calculates the TD target based on a combination of the standard TD target and a high expectile of the return distribution. The authors integrate this operator in both model-free and model-based scenarios, followed by a comprehensive experimental evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper contains extensive experiment results on both simulation and real-world environments.\n2. The paper is written clearly and easy to follow. Figure 1 provides a decent visualization of the underestimation issue."
            },
            "weaknesses": {
                "value": "1. The BAC method tunes its $\\lambda$ and $\\tau$ differently for tasks in MuJoCo and DMC (Table 1 & 5). It's questionable to claim superiority over other state-of-the-art (SOTA) methods like SAC and TD3, which use consistent hyperparameters (HP) across tasks. Adjusting HP for each task can inflate results as seen in Figure 5, which can be misleading. Why not showcase the automatic $\\lambda$ tuning methods from Appendix B.3.3 in the main text if they're effective?\n\n2. Figure 23 reveals that SAC, without the double-Q-trick, still underestimates in the Humanoid task. It's unclear if this is universally true. More convincing results would come from testing this across multiple tasks and providing absolute Q value estimates. I still suspect that Q underestimation largely stems from the double Q techniques, as suggested by the RL community [1]. For instance, OAC [1] introduces $\\beta_{\\text{LB}}$ to manage value estimation issues.\n\n3. Presuming the Q value underestimation problem is widely recognized (which I invite the authors to contest), the paper seems to lack innovation. The BEE operator, at its core, appears to be a fusion of existing Bellman operators.\n\n4. The statement \"BEE exhibits no extra overestimation\" seems conditional on specific $\\lambda$ and $\\tau$ values. For instance, using $\\lambda = 1$ and $\\tau = 1$ could induce overestimation.\n\n[1] Ciosek, Kamil, et al. \"Better exploration with optimistic actor critic.\" Advances in Neural Information Processing Systems 32 (2019)."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5034/Reviewer_kjkr",
                    "ICLR.cc/2024/Conference/Submission5034/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822425147,
        "cdate": 1698822425147,
        "tmdate": 1700668216355,
        "mdate": 1700668216355,
        "license": "CC BY 4.0",
        "version": 2
    }
]