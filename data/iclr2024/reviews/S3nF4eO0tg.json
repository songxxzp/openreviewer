[
    {
        "id": "ZzVfVujpVt",
        "forum": "S3nF4eO0tg",
        "replyto": "S3nF4eO0tg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_cZ1J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_cZ1J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework for generating stylized heads based on a single image. To convert the image into an avatar, two submodules are introduced for image-to-nerf and nerf stylizing. For the first module, a gan-based 360-degree head image synthesis method is adopted to convert the single-view image to multi-view head images for initializing the corresponding nerf. In the next module, the instruct pix2pix method is used with a LORA subnet for generating high-quality stylized nerf. Experimental results demonstrate that the input images are successfully converted into stylized 3D nerf-based avatars."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ This paper is well-organized and clearly explains the process of converting a single image into multi-view images and the styling of a Nerf.\n\n+ The proposed framework is technically sound, where a Lora is utilized to enhance generation quality.\n\n+ The experimental results demonstrate that the new framework can generate stylized Nerfs with richer details."
            },
            "weaknesses": {
                "value": "- The contribution of this work is limited. Although a new benchmark dataset is presented as a contribution, it contains very few images and styles, which makes this benchmark unconvincing.\n\n- The proposed method is quite incremental. This paper simply combines four previous works, including PanoHead, Instant-NGP, Instruct Pix2Pix, and ProlificDreamer, while the main improvement (i.e., the richer details shown in Fig. 4) is attributed to the VSD loss.\n\n- The experiment in Fig. 4 is not fair enough. The Nerf used in Instruct Pix2Pix is DVGO, while the Nerf used in this work is Instant NGP, which may introduce extra bias into the comparison."
            },
            "questions": {
                "value": "1. Why not include more images and styles into the new benchmark dataset? For example, only one photo of elderly people is included in this dataset, which is far from enough to represent the diversity of this group.\n\n2. Please provide more comparison results, since the Fig.4 only provides one identity and two styles. Video results are also encouraged.\n\n3. What would happen if the DVGO is adopted in this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698597864449,
        "cdate": 1698597864449,
        "tmdate": 1699636896433,
        "mdate": 1699636896433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k2WpY0iTCB",
        "forum": "S3nF4eO0tg",
        "replyto": "S3nF4eO0tg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_Sr2S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_Sr2S"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for 3D Avatar generation using a text prompt captured from a single angle. The argument that the authors put forward is that multi-view data is hard to acquire which warrants the need for methods that work on a single image. In this context, the paper focuses on two types of consistencies i) content consistency, which is the method proposed to solve by using a 3D GAN inversion method that projects a single aligned image into 3D using the 3D-GAN latent spaces. This renders a view-consistent 3D model of the face. ii) Style consistency, which uses diffusion-based consistency score distillation loss to maintain the consistency of the styles in different views. The paper shows the result of 3D avatars under different artistic prompts throughout the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper focuses on a single image to stylized 3D avatar generation, which is a challenging task compared to a multi-image 3D stylization task. The multi-image 3D Stylization assumes the availability of such data, which is difficult to acquire in the real-world scenario. Moreover, techniques developed on top of such data may very well overfit and not generalize.\n\n2) The paper shows the results using different artistic prompts throughout the paper. The results show that the output respects the content in the prompts and renders the results at a higher resolution without the blurry artifacts as noticeable in the competing works.\n\n3) The method can handle geometric deformations like elf ears. This shows that the method can produce additional geometric details than just painting textures on the 3D model it is optimizing."
            },
            "weaknesses": {
                "value": "1) The paper falls short in providing quantitative comparisons with the methods it references. While the paper discusses relevant metrics, it lacks a clear presentation of results in tabular form. A comprehensive quantitative analysis is essential to assess the method's quality. Additionally, considering the subjective nature of stylization, a user study could be valuable.\n\n2) The paper assumes the availability of a 3D GAN to generate multiple views, which is suitable for the specific scenario described. However, this assumption may not hold for other datasets, particularly in cases such as full-body humans, where maintaining quality and multiview consistency can be challenging.\n\n3) Multiview consistency appears compromised, as indicated by the supplementary videos. The results in Figure 1 and the accompanying videos show graininess. Furthermore, the normal maps also show this graininess.\n\n4) The framework lacks a dedicated module explicitly designed to preserve identity, as suggested in the introduction. Although a 3D GAN may be capable of producing view-consistent images, there is no guarantee that the stylized output can preserve the identity of the subjects.\n\n5) Despite the use of an editable 3D GAN, the avatars generated by the method are not editable. Instead, the method trains a separate Nerf, which limits the manipulation abilities inherent to a 3D GAN, both in terms of local and global edits to images."
            },
            "questions": {
                "value": "1) Regarding the lack of quantitative comparisons, could you provide insights into why there are no tables showing results in the paper despite discussing metrics? How do you plan to address this limitation, and do you have plans for a quantitative analysis or a user study to assess the method's performance more rigorously?\n\n2) The paper assumes the availability of a 3D GAN for generating multiple views. How do you anticipate this assumption impacting the applicability of your method to datasets where this assumption may not hold, particularly in cases involving full-body humans? \n\n3) For the concerns about multiview consistency and the graininess observed in results and normal maps, could you explain potential reasons for these issues and any steps taken to mitigate them?\n\n4) The paper mentions identity preservation in the introduction, but there doesn't appear to be an explicit module designed for this purpose. Could you elaborate on how the method aims to maintain identity throughout the stylization process, especially after using a 3D GAN as a base?\n\n5) Can the outputs be editable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7456/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7456/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7456/Reviewer_Sr2S"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775705373,
        "cdate": 1698775705373,
        "tmdate": 1699636896205,
        "mdate": 1699636896205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "76buJu09aF",
        "forum": "S3nF4eO0tg",
        "replyto": "S3nF4eO0tg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_hk3d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_hk3d"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a way to generate a stylizable 3D head from a single image. The method utilizes 3D-GAN inversion to obtain a 3D head representation from the input image. A text-guided nerf-based representation is then trained using the prior and a text prompt. To this end, the authors utilize an instant-NGP-based representation and a score distillation-based loss. The qualitative results highlight the efficacy of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The application of generating stylazable heads from a single image is interesting and relevant.\n- The qualitative results indeed show that the approach outperform the baselines in terms of visual quality.\n- I appreciate the ablation study on the need for a separate NeRF."
            },
            "weaknesses": {
                "value": "- The main issue of the paper is the lack of novelty. Although I do believe that a method that combines SOTA components to solve a novel problem is sufficient, the proposed task is not a sufficient contribution in itself.\n- In the same vain, I believe the contribution of the benchmark is a bit overstated. The size of the benchmark is not sufficient and the paper lacks proper evaluation of different methods. This would be vital to evaluate the contribution of a benchmark.\n- I am missing a comparison to text-guided head models, even explicit ones, e.g., ClipFace [1]\n- The authors proposed a quantitative evaluation protocol, however I am missing a table comparing the baselines.\n\n\n\n[1] Text-guided Editing of Textured 3D Morphable Models"
            },
            "questions": {
                "value": "I suggest that the authors soften the contribution claims and add further comparison to more baselines as well as quantitative evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786372436,
        "cdate": 1698786372436,
        "tmdate": 1699636895979,
        "mdate": 1699636895979,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oR4Kv66AEX",
        "forum": "S3nF4eO0tg",
        "replyto": "S3nF4eO0tg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_7NEF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7456/Reviewer_7NEF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called StyleDreamer, to lift a provided 2D image of a face into 3D and then to create a stylized version of the 3D representation of the face conditioned on a text prompt. The proposed method involves first using PanoHead with Pivotal tuning to create a 3D tri-grid representation of the provided face. Then, multiple views of the face are rendered and input into InstructPix2Pix to learn an InstantNGP-style Nerf representation for the face using a proposed CSD loss. The CSD loss guides image generation based on the scores of the reference image, the input text prompt and the camera viewpoint using the variation score distillation loss (VSD) + LoRA formulation of ProlificDreamer. The authors provide various qualitative results for comparisons of their method to some alternative approaches and ablations of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The paper proposes a new method for what the authors call the task of \"One-to-Style\", that is of simultaneously lifting a 2D face into 3D and of stylizing it with a text prompt. They also propose a new method, which combine various several state-of-the-art models to solve this task.\n\nClarity: The presentation of the technical details of the method and the overall organization of the paper are good."
            },
            "weaknesses": {
                "value": "1. Originality: The authors claim \"Despite promising results, these methods are unable to generalize to out-of-domain styles since they solely\nconcentrate on one style during training.\" This is not entirely true. In Trevithick et. al. Real-Time Radiance Fields for Single-Image Portrait View Synthesis, SIGGPRAH 2023, Figure 8., which proposes an encoder into EG3D, many examples of out-of-domain generalization for stylized facial images of are shown. A straight-forward alternative baseline to the proposed approach in this paper is then to first use InstructPix2Pix on 2D facial images to stylize them with a prompt and then to lift them into a 3D facial GAN's latent space (EG3D, PanoHead) via an encoder or GAN inversion. The authors don't show any results of actually doing this, but simply claim that it doesn't work. In my experience, this also works fine and produces much better texture and shape quality. So I am not entirely convinced that the proposed approach is needed.\n\n2. Results: We results of the proposed method are not convincing for the following reasons:\n(a) In general the quality of the learned texture and normals is very noisy.\n(b) Only qualitative results of a few cherry-picked examples are shown. No aggregate quantitative results via a user-study or using the proposed ID Score, and Style Score are shown.\n(c) The curated evaluation dataset of 10 subjects, presented in the supplementary document are all from the FFHQ dataset, which was used to train PanoHead. Ideally the evaluation dataset should not comprise of training images, to understand the ability of the proposed method to generalize to faces not seen during training.\n(d) An ablation of the contribution of the \"camera\" conditioning in the proposed StyleDreamer is not presented.\n(e) An ablation of using InstructPix2Pix with VSD instead of SDS is not presented.\n(f) The caption for Figure 4, says that results of Instruct Nerf2Nerf are also present, but the visual results are absent.\n\n3. Significance: Overall the quality of the results is not sufficiently high enough to warrant acceptance. The value of stylizing a face image in the style of \"Starry night\" is of limited value."
            },
            "questions": {
                "value": "The authors mention that optimizing the tri-grid doesn't work and hypothesize that this happens because the tri-grid has too many parameters. Did they try with EG3D and its tri-plane? In our experiments, we've experimentally verified that tri-grid is not required for full 360 rendering and both methods suffer from artifacts when rendering the back view. The back view rendering issue stems more from the lack of training data from the back view versus any fundamental limitation of the tri-plane representation.\n\nI would also like to hear the authors' responses to the three weaknesses that I've pointed out in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699232439111,
        "cdate": 1699232439111,
        "tmdate": 1699636895730,
        "mdate": 1699636895730,
        "license": "CC BY 4.0",
        "version": 2
    }
]