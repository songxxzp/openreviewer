[
    {
        "id": "cAdPxbZ3sj",
        "forum": "wsRXwlwx4w",
        "replyto": "wsRXwlwx4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission324/Reviewer_8uZQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission324/Reviewer_8uZQ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an innovative prompt learning technique that integrates a consistency mechanism between trainable and pre-trained models to mitigate the risk of overfitting. This approach employs a consistency term applied to two altered inputs within the text and visual spheres. In text modality, the method leverages existing language models like GPT-2 and GPT-3 to introduce variations, whereas for images, it uses standard image augmentation techniques prevalent in self-supervised learning. The authors have skillfully merged two distinct approaches to adaptation - prompting and adaptation - demonstrating that this synergy, coupled with a consistency loss, enhances the method's ability to generalize. The improved generalization capability of this approach is evident in various prompt learning tasks, including adapting from base to new tasks, cross-dataset evaluation, and domain generalization, with consistent enhancements observed across these applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clear and of high quality, with significant numerical results. The authors offer a thorough analysis of their proposed method's various components, which overall appear sensible and well-founded."
            },
            "weaknesses": {
                "value": "While the paper is clear, and the numerical results are noteworthy, its novelty isn't entirely clear. The paper's self-consistency terms seem similar to those in self-supervised learning (SSL) methods. The authors' claim of differentiating their approach from SSL, where two perturbed inputs within a single encoder are used, doesn't fully convince. In SSL, typically there are two encoders: an online encoder and a momentum encoder. The paper\u2019s pre-trained and trainable encoders appear analogous to SSL\u2019s momentum and online encoders, respectively. The authors should clarify this similarity.\n\nAdditionally, the paper omits recent relevant studies like Bayesian Prompt Learning [1] and Prompt Distribution Learning [2], which address overfitting in vision and language models. Discussing these in the related work and comparing them in sections like domain generalization are necessary, especially given that in some cases, such as Bayesian Prompt Learning, they outperform the methods in this paper. For example, in the domain generalization task, the Bayesian Prompt Learning method (%60.44) works better than the paper performance (%60.42). \n\n[1]. Bayesian Prompt Learning for Image-Language Model Generalization, ICCV 2023\n\n[2]. Prompt Distribution Learning, CVPR 2022"
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission324/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696288076,
        "cdate": 1698696288076,
        "tmdate": 1699635959163,
        "mdate": 1699635959163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9FEkTm66qG",
        "forum": "wsRXwlwx4w",
        "replyto": "wsRXwlwx4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission324/Reviewer_sWY6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission324/Reviewer_sWY6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a consistency-guided prompt learning (CoPrompt) method to transfer CLIP to downstream tasks in a few-shot setting. Experimental results show the capacity of consistency-guided prompt learning to imporve the generalization comparing with the SOTA methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. CoPrompt achieves the SOTA results on base-to-novel generalization and cross-domain setting. \n\n2. Ablation studies demonstrate the effectiveness of consistency constrain to prevent overfitting on the downstream tasks."
            },
            "weaknesses": {
                "value": "1.The prompt learning and adapter learning method mentioned in this paper are introduced by MaPLe and CLIP-Adapter. The primary contribution of this paper only lies in the introduction of consistency constraint learning.  Thus, regarding the method as prompt learning is a bit ambiguous in my opinion.\n\n2. The comparison between CoPrompt and Zero-shot CLIP is not fair enought. The diverse text prompts generated by LLM can imporve the zero-shot classification ability of CLIP on downstream tasks. It is important to consider this aspect when evaluating the performance of  Zero-shot CLIP.\n\n3. On small scale dataset like Eurosat, the higher value of \u03bb leads to worse performance, accroding to Table 7. However, the analysis regarding this observation is missing from the paper. Can CoPrompt reaches better result on Eurosat if \u03bb=0?"
            },
            "questions": {
                "value": "1. What if combined consistency constrain learning with other existing methods, like CoCoOP. Can CoPrompt improve the generalization of CoCoOP?\n\n2. What is the training overhead in terms of time? What is training and test-time inference speed compared with prior methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Reviewer_sWY6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission324/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726245161,
        "cdate": 1698726245161,
        "tmdate": 1700626852960,
        "mdate": 1700626852960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "joriVMdI2Y",
        "forum": "wsRXwlwx4w",
        "replyto": "wsRXwlwx4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission324/Reviewer_D25R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission324/Reviewer_D25R"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a consistency-enforced fine-tuning method for large foundation model CLIP that enables learning a new task from a few samples while maintaining the zero-shot generalizability. The proposed method incorporates the knowledge of a pretrained LLM with consistency constraints on the text branch and data augmentations on the image branch to improve the generalization further along with learnable adaptors on both image and text branches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The  paper is well-written and easy to follow.\n- The authors have shown decent results on base-to-novel generalization."
            },
            "weaknesses": {
                "value": "- The idea of adaptors and prompt-tuning already exist in the literature. Merely combining the two ideas seems an incremental work and not novel.\n- The idea of retaining the generalizability of the CLIP using consistency loss has already been explored in the paper \"Self-regulating Prompts: Foundational Model Adaptation without Forgetting\" (ICCV 2023) [1]. Hence,  the consistency loss doesn't contribute towards the novelty.\n- The authors have not compared their approach to the above paper and there is also no reference to the paper.\n- The improvements in the Domain generalization is marginal given that authors have fine-tuned the model. Same is true for cross-dataset evaluation.\n\n\n[1] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. Self-regulating Prompts: Foundational Model Adaptation without Forgetting. ICCV 2023 (https://arxiv.org/abs/2307.06948)"
            },
            "questions": {
                "value": "- Are the vision side prompts conditioned on text side? Do authors follow MaPLe settings or Independent VL prompting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Reviewer_D25R",
                    "ICLR.cc/2024/Conference/Submission324/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission324/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766905299,
        "cdate": 1698766905299,
        "tmdate": 1700600066181,
        "mdate": 1700600066181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yW1TCZREUW",
        "forum": "wsRXwlwx4w",
        "replyto": "wsRXwlwx4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission324/Reviewer_iShv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission324/Reviewer_iShv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new adaptation method for CLIP like large scale vision-language models for generalization benchmarks. Specifically, the authors propose 3 techniques to improve generalization of CLIP. Firstly, they observe that the main cause of poor generalization is the lack of consistency constraints between the learned embeddings and the original pretrained embeddings. To overcome this issue, consistency losses are used at the text side as well as the image side separately. Secondly, the inputs to the original models are perturbed with the help of augmentations and LLM captions for image and text side respectively. Lastly, the proposed method combines the adapter and prompt learning modules with-in the same architecture for improved performance. \n\nExtensive benchmark comparisons are conduced on 3 different generalization tasks where the proposed approach shows improvements against prior methods. Furthermore, ablation studies are provided for analyzing contributions of each component separately and motivating the design choices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) This paper addresses an important aspect of generalization of pre-trained CLIP like models for downstream task adaptation. Most of the prior methods struggles to achieve good performance on unseen classes and datasets, while this method explicitly add training constraints to mitigate the issue.\n2) The proposed framework is motivated fairly, and the strength of its individual components have been demonstrated clearly in the ablation studies.\n3) The method shows impressive performance against the previous prompt learning methods.\n4) Paper is easy to read."
            },
            "weaknesses": {
                "value": "1. The authors mentioned that their baseline is MaPLe, which uses coupling functions between vision and text branches, but in Figure 3, no coupling functions are visible. It will be good to clarify the exact architecture used in the proposed framework. Also I think there is graphic error in image encoder as the visual prompts (orange color) are not shown in intermediate layers of CLIP visual encoder. \n\n2. It will be good to see the proposed method generalization for a newer V-L model. CLIP is relatively outdated and the authors are encouraged to show result on at least another recent CLIP variant. For example on EVA-CLIP[1] model. \n\n3. There is a recent prompt learning method PromptSRC [2], which also seems to introduce consistency constraints to prompt learning to improve generalization. How is the proposed method different from this work? Also, all fair comparisons should be added in the main paper. \n\n4. The diagrams in the paper are of very poor quality. Specially the text in the Figure 2. graph is very small and the color scheme used is confusing. Also in the Figure 1, their is no indication of using adapters in Fig. 1b. \n\n5. I think there is some writing logical errors in the paper. For example, in the Adapters heading in section 3, adapter based method are being mentioned but prompts have been written instead of the adapter blocks. \n\n[1] Exploring the Limits of Masked Visual Representation Learning at Scale (CVPR-23)\n[2] Self-regulating Prompts: Foundational Model Adaptation without Forgetting (ICCV-23)"
            },
            "questions": {
                "value": "Please refer to the weaknesses section for additional questions and queries!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission324/Reviewer_iShv"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission324/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817041711,
        "cdate": 1698817041711,
        "tmdate": 1699635958874,
        "mdate": 1699635958874,
        "license": "CC BY 4.0",
        "version": 2
    }
]