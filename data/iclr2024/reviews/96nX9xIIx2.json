[
    {
        "id": "6wzBtJRPsg",
        "forum": "96nX9xIIx2",
        "replyto": "96nX9xIIx2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
        ],
        "content": {
            "summary": {
                "value": "To upgrade vision model sparsification, the paper proposed a data-model co-design sparsification paradigm, where integrating input image with the learnable perturbation, and a network tuning strategy is designed to optimize this issue.\n\nThe algorithm has demonstrated excellent performance on CIFAR-10 and CIFAR-100 datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The manuscript exhibits a commendable level of writing proficiency, featuring well-crafted graphics that enhance the overall presentation and a compelling narrative.\n\n2. The algorithm has showcased remarkable efficacy when applied to the CIFAR-10 and CIFAR-100 datasets, achieving good performance."
            },
            "weaknesses": {
                "value": "1. **Inadequate experiments.** This is a primary concern for the reviewer. The paper only presents experiments on CIFAR-10 and CIFAR-100, which, in the era of big data, are considered insufficient. These experiments do not adequately demonstrate the performance of the proposed method. Conducting experiments on larger datasets, such as ImageNet-1k, is essential.\n\nAdditionally, it's worth noting that the method utilizes ImageNet-1K pre-trained weights, which were trained on a resolution of 224. However, the method is tested on CIFAR data with a resolution of 32. It is evident that padding the data to a resolution of 224 can significantly boost performance. From this perspective, experiments specifically conducted on ImageNet with a resolution of 224, and direct performance comparisons with fine-tuning on this resolution, are crucial.\n\nFigure 10 further substantiates this conclusion, showing that the optimal performance is achieved at a resolution of 224, with diminishing performance as the resolution decreases. Therefore, padding CIFAR data with a resolution of 32 to 224 doesn't necessarily demonstrate the superiority of the method. The gains observed in this case could be attributed to the ImageNet-1K pre-trained weights at a resolution of 224.\n\n2. The reviewer also suggests providing performance comparisons with smaller models, as performance metrics on sparser models, such as using MobileNet, would be more indicative and informative.\n\n\n3. Furthermore, the method exhibits significant limitations, as it necessitates the use of pre-trained weights from a larger dataset. The current version seems to require transforming the ImageNet model to CIFAR. It would be insightful to explore the performance without pre-trained weights. Additionally, conducting comparisons on a larger pre-trained dataset, such as ImageNet, appears necessary for the current version."
            },
            "questions": {
                "value": "In each figure, the authors have plotted curves labeled as \"our best,\" which may not be entirely necessary as this information can be inferred from the VPNs curve. \n\nMoreover, this plotting style has the potential to cause confusion; upon initial review, it might be perplexing why the curve representing \"ours\" appears as a straight line."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_hiB3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697875808840,
        "cdate": 1697875808840,
        "tmdate": 1699636208594,
        "mdate": 1699636208594,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DLDBrrYCtV",
        "forum": "96nX9xIIx2",
        "replyto": "96nX9xIIx2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a joint model pruning and visual prompting learning method. By combining these two methods, it could recover the performance loss by pruning. This method is validated on several pruning methods and datasets to demonstate its universality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea is easy to follow and effective.It is interesting to see that only a small number of learned parameters could improve the performance."
            },
            "weaknesses": {
                "value": "1. I think the authors should focus more on the structured sparse case, unstructured pruning is well known that will not contribute to any acceleration in practice.\n2. For structured pruning, the speedup ratio should use latency, not theoritical FLOPs. And more recent methods should be compared.\n3. The visual prompting method essentially uses lower resolution for the input images. It is necessary to compare a baseline that using a lower resolution image as input, and then prune less parameters to maintain the same FLOPs as the proposed VPN. \n4. Following the previous point, I also wonder whether this method will deteriorate some applications that are senstive to resolution, such as object detection or etc."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_eRqj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744864491,
        "cdate": 1698744864491,
        "tmdate": 1700651246869,
        "mdate": 1700651246869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Fh5Lr228k",
        "forum": "96nX9xIIx2",
        "replyto": "96nX9xIIx2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_sGzb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_sGzb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use visual prompts to improve performance of the pruned model by applying visual prompts earlier in the process, aka, before the model fine-tuning. This effort was motivated by the experiments of applying post-pruning prompt to the sparse models with and without fine-tuning. As post-pruning prompts showed only marginal gains to subnets that went through fine-tuning, authors proposed to apply the visual prompts earlier in the process. The proposed scheme was compared with eight pruning baselines on eight classification tasks. Numerical comparisons show the potential of the proposed scheme."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of applying visual prompts to identify a subnet which further leads to a better pruning results is interesting.\n- The idea of using visual prompts to control a pretrained vision model is an interesting direction to pursue."
            },
            "weaknesses": {
                "value": "- The paper is not easy to read. There's a particular emphasis on \"data model co-design\", but it takes quite a while to understand what this refers to concretely.\n- Why is the visual prompts essential? How about learning additional parameters without using the visual prompt?\n- How would visual prompts be different from data augmentation?"
            },
            "questions": {
                "value": "Please see my questions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789990511,
        "cdate": 1698789990511,
        "tmdate": 1699636208441,
        "mdate": 1699636208441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "izPKUiwMba",
        "forum": "96nX9xIIx2",
        "replyto": "96nX9xIIx2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new post-pruning method by introducing visual prompts into the pruning pipeline. The authors introduce visual prompts into trained vision models. During pruning, not only weight masks but also visual prompts are optimized. The whole pipeline includes two steps: (1) fixing pretrained weights of vision models, tuning masks and visual prompts; (2) fixing the mask, fine-tuning both weights and visual prompts. The authors adopt the proposed method on several downstream classification tasks and compare the performances of pruned models with other pruning methods. The experiments show that the proposed method can achieve better performance than other methods with the same sparsity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The writing and presentation of this paper are quite good. The logic of the article is very clear, and the choice of words in the writing is also very precise.\n2. The idea of introducing tunable visual prompts into the pruning pipeline is intriguing. The experiments validate the effectiveness of the proposed strategy.\n3. The authors compare the proposed method with other pruning methods and additionally apply the proposed visual prompt pruning strategy to these methods, demonstrating the transferability of their approach."
            },
            "weaknesses": {
                "value": "1. Introducing visual prompts into vision models seems to boost their performance. However, comparing models with visual prompts (the proposed method) to those without (other baseline methods) might not be entirely fair. What if we apply both the proposed and baseline methods to a model that has already been fine-tuned with visual prompts?\n2. I have some doubts regarding the generality and performance of this paper.\n(1) Why must we conduct experiments on downstream tasks of ImageNet? Why not directly on ImageNet itself, since most pruning work actually focuses more on performance on ImageNet?\n(2) The method proposed in this paper seems to be limited to scenarios where visual prompts can be applied, with their main application currently being in classification tasks. How can the proposed approach be used for other tasks, such as detection, segmentation, etc.?"
            },
            "questions": {
                "value": "Please refer to the weaknesses. I hope the authors can provide more experiments to demonstrate the effectiveness of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2675/Reviewer_8b5d"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2675/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816110016,
        "cdate": 1698816110016,
        "tmdate": 1699636208352,
        "mdate": 1699636208352,
        "license": "CC BY 4.0",
        "version": 2
    }
]