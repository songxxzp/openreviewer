[
    {
        "id": "TXmjtgC6z0",
        "forum": "x3LxHdZX0f",
        "replyto": "x3LxHdZX0f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_tTd1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_tTd1"
        ],
        "content": {
            "summary": {
                "value": "This paper brings together a bunch of recent techniques for semi-honest, honest majority 3 server multi party computation and supplements them with a few custom designed gadgets. These are then used to approximate GeLU based neural networks in MPC. They have an experimental section analysing how quickly it runs and the accuracy/precision of the resulting protocol."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper brings together a bunch of SOTA work well.\nIt does provide some new approximations, e.g. of GeLU, which seem like useful components for future work.\nThe paper is well laid out and easy to follow.\nThe resulting protocol is runnable with fairly large models and gets good approximations."
            },
            "weaknesses": {
                "value": "The paper seems some what incremental in nature with the new contributions being slightly limited in scope.\nIt is dubious whether the ability to generate one token from a language model in 5 minutes between three parties in the semi-honest honest majority model is likely to have any applications in the imminent future. But this is a step closer to something practical being possible."
            },
            "questions": {
                "value": "Are you aware of any plausible near term application that this technology is close to good enough to be deployed for?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749662623,
        "cdate": 1698749662623,
        "tmdate": 1699636433439,
        "mdate": 1699636433439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mOBHHhUoqA",
        "forum": "x3LxHdZX0f",
        "replyto": "x3LxHdZX0f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_bbdZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_bbdZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes PUMA, a 3PC inference protocol for Transformers."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The 3PC setting for LLM is timely."
            },
            "weaknesses": {
                "value": "1. Limited novelty. The provided protocols are straightforward and contain no novel design or construction.\n2. Compared with the baseline, the protocol advantages seem to all come from RSS. \n2. Overclaim the experimental performance. The title of this paper is Secure Inference of LLaMA-7B in Five Minutes. However, it is overclaimed. 5 minutes is only when the input and output are 4 and 1 token respectively, which is obviously not in line with the practical service setting."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834242103,
        "cdate": 1698834242103,
        "tmdate": 1699636433373,
        "mdate": 1699636433373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mi2BKIZ84H",
        "forum": "x3LxHdZX0f",
        "replyto": "x3LxHdZX0f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_DyAw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_DyAw"
        ],
        "content": {
            "summary": {
                "value": "This work presents a secure Transformer inference framework in 3PC."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Simple but effective approximations for GELUs.\n+ End-to-end framework for Secure LLM Inference.\n+ Extensive evaluations."
            },
            "weaknesses": {
                "value": "The protocols in this work seem limited contributions and are mainly taken from prior works."
            },
            "questions": {
                "value": "1. What is the cost of secure inference on LLaMA-7B when extending it to a common input length e.g., 128?\n2. Does the polynomial approximation of GELU affect the accuracy of large models such as LLaMA-7B because it seems to cause a relatively large error?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834357321,
        "cdate": 1698834357321,
        "tmdate": 1699636433269,
        "mdate": 1699636433269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rUZdhdT0Ph",
        "forum": "x3LxHdZX0f",
        "replyto": "x3LxHdZX0f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a model that utilizes multi-party computation techniques to perform the LLAMA-7B model while preserving the privacy of the client's data. In this process, an optimization of the approximation method for the GeLU function was carried out, and softmax, embedding, and layer normalization methods were all implemented using MPC, achieving an end-to-end implementation. Through these techniques, the computational time has been reduced by approximately 2 times compared to the existing implementation, MPCformer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Completion of end-to-end implementation of a large language model using multiparty computation techniques.\n2. Achieving inference that is twice as fast as the previously published MPCformer model.\n3. The operation of the GeLU function in a different manner compared to the conventional approach."
            },
            "weaknesses": {
                "value": "1. Most of the methods appear to be simple adaptations of existing techniques, lacking any distinctive novel approach. While the results are impressive from an industrial perspective, it raises doubts about whether they are suitable for ICLR, which places a strong emphasis on academic contributions.\n\n2. While it claims to perform twice as fast as MPCformer, the paper lacks precise explanations of why each technique is superior to the existing ones, making it challenging to assess their effectiveness. \n\n3. The primary technical contribution seems to be the approximation of the GELU function. However, without a clear comparison to existing approximations, it is challenging to assess the value of this technique. The paper introduces variations in polynomial computation based on the range of x, but it is not evident how this approach is superior to the conventional method of approximating the GELU function in terms of computational efficiency.\n\n4. Regarding the meaningful academic contributions in softmax, embedding, and layerwise normalization, it is not clear where they lie, making it difficult to discern the significance of these contributions."
            },
            "questions": {
                "value": "1. Provide numerical evidence of how the computations involved in our method for approximating the GELU function offer advantages in terms of computational and communication costs compared to existing techniques that achieve the same level of accuracy.\n\n2. Convince that the techniques employed in softmax, embedding, and layerwise normalization go beyond mere combinations of existing methods and provide non-trivial technical contributions.\n\n3. Explain what specific factors contributed to the 2x performance improvement compared to MPCformer and quantitatively specify the performance gains achieved by each factor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4557/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4557/Reviewer_G7JD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699271155257,
        "cdate": 1699271155257,
        "tmdate": 1699636433202,
        "mdate": 1699636433202,
        "license": "CC BY 4.0",
        "version": 2
    }
]