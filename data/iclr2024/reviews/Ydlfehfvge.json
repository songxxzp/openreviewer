[
    {
        "id": "yvMKR2j1GO",
        "forum": "Ydlfehfvge",
        "replyto": "Ydlfehfvge",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_SnPL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_SnPL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the \"twin TD-regularized actor- critic (TDR) method\" which chooses the target Q-values in temporal difference (TD) learning based on the Q-function attaining the lower TD error, and penalizes the actor network based on the chosen actions' TD errors. This method is combined with priorly proposed practices such as distributional critic functions and LNSS, a slight modified version of n-step returns. The authors evaluate these combined methods on top of  the TD3, SAC, and D4PG algorithms in six environments from the DeepMind Control suite."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Finding the right balance between pessimism and optimism in TD-based algorithms is a relevant problem that has been the focus of much recent work.\n\n2) The methodological contribution is simple and easy to understand.\n\n3) The experiments are conducted with a fair number of random seeds (10) and include an ablations showing that each of the introduced components contributed positively to the reported performance."
            },
            "weaknesses": {
                "value": "Major:\n\n1) The experiments are limited to small set of 6 of the easier environments from the DeepMind Control suite. Hence, I believe the empirical results are very far from sufficient for asserting general effectiveness, and appear very much in contrast to the claims made in the paper, e.g., the authors state that one of the main contribution is 'extensive experiments' showing \"TDR enables SOTA performance [...] across a wide variety of control tasks\" (end of Page 2-Page 3). As a minimum, to make any sort of such claims, I would expect to see results on the full set of DeepMind Control environment, including for the more complex humanoid stand/walk/run tasks. \n\n2) The employed baselines use implementations that have not been optimized for the DeepMind Control suite (nor for the sample-efficiency setting considered as the authors only train for 1000 steps). This makes the reported results notably inferior than what considered standard for what these algorithms can achieve (e.g. see the learning curves in [1]).\n\n3) Section 4  introduces some quite intuitive considerations in the form of convoluted Theorems which I did not find very clear (e.g., in Theorem 1 the authors introduce what they refer to as the \"step random estimation bias \u03c8\", but do not use  \u03c8 in any of the main statements of the Theorem. They then re-introduce the same quantity by copying the exact same description in Theorem 2). I would suggest rewriting the Section to convey the paper's consideration more concisely. When using Theorems, I would make sure there is a clear separation between the assumptions and \n the exact theoretical considerations that are being shown (which I found unclear in Theorem 3) \n\n3) The main novelty of the paper is the TDR procedure which foregoes dual TD-learning and chooses the target Q-function based on the lower TD error. However, I believe the paper does not convey a solid intuition as to why this heuristic is superior than standard minimisation of the target Qs. Hence, I am worried the proposed methodology might simply trade-off slightly improved sample-efficiency for stability and convergence (in harder environments/longer training regimes) due to 1) employing n-step returns 2) lowering the pessimism to counteract the actor's Q-maximization. In connection to Point 1 above I would really like to see experimental evidence showing how the proposed algorithms fare in harder environments and for longer training horizons, to empirically validate their efficacy.\n\n4) The authors claim several times that the dual TD-learning (target Q-function minimization from TD3) \"promotes a new problem of underestimation, which usually occurs during the early stage of learning, or when subjected to corrupted reward feedback or inaccurate states\" (page 4). Yet this claim is not supported by any empirical or theoretical evidence in the text.\n\nMinor:\n\nThe authors incorporate  what they refer to as 'Long N-step surrogate state' (LNSS), which appears to be a simple modification to n-step returns that takes the discounted average rather than the discounted sum of the rewards. While they remark several times this is a core component of their methodology, they never explain this simple procedure in the main text and relegate its description to a Section in the Appendix I found unnecessarily convoluted. Am I missing something about this method?\n\nSome typos are still present, I suggest making use of a spell-checker to parse the text (e.g., measureed. page 2)\n\n[1] https://github.com/denisyarats/pytorch_sac"
            },
            "questions": {
                "value": "I would appreciate if the authors could address the questions and criticism I have included in connection to the weaknesses Section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698482282113,
        "cdate": 1698482282113,
        "tmdate": 1699636636022,
        "mdate": 1699636636022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rhhsUiCbtO",
        "forum": "Ydlfehfvge",
        "replyto": "Ydlfehfvge",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new mechanism; instead of directly selecting the minimum value from twin Q values, it selects the target value with smaller TD errors. The authors claim that it could mitigate both overestimation and underestimation issues. The authors implement this mechanism in some popular algorithms and evaluate it in six tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think it is a new try, and this paper gives some theoretical explanations."
            },
            "weaknesses": {
                "value": "Major Weaknesses lie in the experiments listed in the Questions. Also, the presentation should be improved."
            },
            "questions": {
                "value": "1. **[Problematic claim in the introduction]** The authors claimed that \"The clipped double Q-trick is designed to solve the overestimation caused by the max operator.\"  Actually, the max operator is not the culprit for the overestimation; we would encounter overestimation even using the bellman evaluation operator. The culprit is function approximation errors. \n\n2. **[Unreliable Baseline Performance]** I think this author's report of SAC and TD3 performances is not consistent with what is reported in other papers.\n   For example: \n\n   * AcrobotSwingup task, the author reported SAC performance is ~4@1M steps. However, refer to the TD-MPC[1] paper (Figure 3), ~100@500k steps. **This performance is much higher than the authors' own dTDR algorithm.**\n\n   * Quadruped Walk task, the authors report SAC performance of ~196@1M steps. However, refer to the BAC[2] paper, ~600@400k steps (Figure 32).\n\n   \n\n3. **[Need More Experimental Results]**  The evaluation of just 6 medium and easy DMControl tasks is not enough to support the effectiveness of this proposed practical mechanism. Add more experimental results on MuJoCo and DMControl tasks.\n\n   \n\n4. **[Arbitrary Claim on baseline D4PG]** The claim that D4PG is SOTA is an overly arbitrary one in abstract. There are different SOTA methods for different tasks, and I don't agree with the author's claim in the abstract. At least in the BAC [2] I pointed out above, it outperforms D4PG on some tasks. Also, RND[3], and REDQ achieve SOTA on some tasks. \n\n   \n\n5. **[Add related Works]** BAC is also a paper dedicated to underestimation and overestimation, and I see that it reports good performance and does many experiments on a wide range of benchmark tasks, so I suggest the authors add it to the related work discussion. And possibly do some experimental comparisons if the BAC authors are willing to provide the code or the experimental data.\n\n\n\n6. **[Move Table 1 to Appendix ]** Figure 2 and Table 1 are the results of the same set of experiments, and there is no need to occupy half a page in the text.\n\n\n\n7. **[Better not to use the limited results to answer Q1-Q6 questions repeatedly]** Your Experiment Section begins by proposing to answer six distinct questions. However, for several of these questions, you only have 1-2 supporting evidence repeatedly from your main experiments. Furthermore, the reader is required to make indirect associations to find these supporting evidence. To enhance the clarity and focus of your work, it may be advisable to pare down the number of questions you aim to address to a more manageable 2-3, then put the unimportant ones in the Appendix and conduct some more experiments to support, not repeatedly use the limited results to answer a lot of questions. \n\n[1] Hansen N, Wang X, Su H. Temporal difference learning for model predictive control[J]. arXiv preprint arXiv:2203.04955, 2022.\n\n[2] Ji T, Luo Y, Sun F, et al. Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic[J]. arXiv preprint arXiv:2306.02865, 2023.\n\n[3] Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. arXiv preprint arXiv:1810.12894, 2018\n\n[4] Chen X, Wang C, Zhou Z, et al. Randomized ensembled double q-learning: Learning fast without a model[J]. arXiv preprint arXiv:2101.05982, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_KXvd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698592326794,
        "cdate": 1698592326794,
        "tmdate": 1699636635928,
        "mdate": 1699636635928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4NEB55BPEE",
        "forum": "Ydlfehfvge",
        "replyto": "Ydlfehfvge",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT"
        ],
        "content": {
            "summary": {
                "value": "The paper first proposes a TD-Regularized Double Q Networks to effectively control the error of Q-value estimation, and then regularizes the actor based on TD-error to further control the error of Q-value. The paper demonstrates the effectiveness of the method in error control through a large amount of theoretical evidences. The experiments demonstrate that the combination of the method with distributional RL or LNSS also has significant improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper provides a simple yet effective method for mitigating estimation errors. The proposed method is easy to implement.\n2) Sufficient theoretical derivations are provided."
            },
            "weaknesses": {
                "value": "1) The comparative experimental results are not sufficiently reliable. None of the baselines are designed for addressing the estimation error problem.\n2) The paper emphasizes that the Twin TD-Regularized method can effectively control errors, but the effectiveness of bias control has not been demonstrated in the experiment section.\n3) The ablation experiment is unreasonable, that is, the paper proposes to combine TDR with LNSS method effectively, but LNSS is not the work of this paper and is not suitable for ablation experiments.\n\n[Supplementary review] Thanks for the author's responses to my comments. Based on the comments of all reviewers and the author's reply, I decided to revise my score (Marginally below the acceptance threshold -> Reject, not good enough)."
            },
            "questions": {
                "value": "1) The first Q-network is traditionally chosen to compute the actor TD regularization term. Will it have a negative impact on the selection of Q-network target values in the proposed method?\n2) In subsection 3.4, the paper only analyzes the advantages of TDR in theory compared to the previous TD-Regularized actor network method. It should be demonstrated through some experiments.\n3) In page 7, the meaning of \"TDR has helped successfully address the random initialization challenge caused by random seeds\" is confusing and difficult to understand, and it needs further explanation.\n4) It is very sensitive to parameters for DRL algorithms, and using different parameters can have different effects. Will the parameters of the comparison algorithms in the experiment be consistent with the original paper (the best situation), and will their performance be worse than the original paper?\n5) The paper only demonstrates that TDR can improve the performance of baselines. You should compare some SOTA  bias control algorithms in recent years.\n6) The ablation studies are not quite thorough. You should provide more experimental results about approximate estimation error for clearly suggesting the benefits of different components of the algorithm."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5958/Reviewer_LkhT",
                    "ICLR.cc/2024/Conference/Submission5958/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658649613,
        "cdate": 1698658649613,
        "tmdate": 1700721113928,
        "mdate": 1700721113928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vgjUvirqf7",
        "forum": "Ydlfehfvge",
        "replyto": "Ydlfehfvge",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_2Vrh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5958/Reviewer_2Vrh"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on addressing the problem of estimation bias in DRL. The authors developed the TD-regularized actor-critic (TDR) method, which aims to minimize both overestimation and underestimation errors. The paper also incorporates TDR with other effective DRL techniques, such as distributional learning and the long N-step surrogate stage reward (LNSS) method. The authors evaluate their method with different baselines in the DMC suite."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper studies how to improve value estimation in DRL, which is a core topic in the RL community. The method is clearly explained."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper are the novelty of the approach and the significance of experimental results. It seems a A+B+C work, which modifies previous TD-regularized AC method (Parisi et al., 2019) marginally, and then combines distributional learning and N-step learning into the proposed approach. The experimental part is also a bit weird, where the replicated baselines seem worse than original papers."
            },
            "questions": {
                "value": "> 3.1 DOUBLE Q IN ACTOR-CRITIC METHOD\n\nThis subsection is under the method section (Sec 3). However, the contents in Section 3.1 is actually a summary and description of the literature instead of the proposed method, which can confuse the readers.\n\n> Note from Equation (7) that TDR always uses a target value associated with a smaller target TD value (regardless of the error sign) between the two. ... TDR is naturally positioned to address both overesdiation and underestimation errors.\n\nIn fact, the estimation error is measured by comparing the estimated value with an expectation of the true value. How can this sample-based TD error serve as a sound measure for reducing the estimation error? Doesn't this lead to a large variance? In addition, there is a typo in here, where overesdiation should be overestimation.\n\n> Our TD-regularized actor network directly penalizes the actor\u2019s learning objective whenever there is a critic estimation error.\n\nThis statement is weird. It would be better to state this as \"whenever the critic estimation error is non-zero\". In addition, it is unavoidable to have estimation errors in practice when learning these objectives."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5958/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744451747,
        "cdate": 1698744451747,
        "tmdate": 1699636635690,
        "mdate": 1699636635690,
        "license": "CC BY 4.0",
        "version": 2
    }
]