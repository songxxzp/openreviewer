[
    {
        "id": "QV4oBdSnQ2",
        "forum": "ADDCErFzev",
        "replyto": "ADDCErFzev",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_YU1u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_YU1u"
        ],
        "content": {
            "summary": {
                "value": "In this study, the researchers manipulated the degree of dropout in AlexNet, and then computed the eigenspectrum of the population responses in fc6, following Stringer, 2019. The results showed that increasing the degree of dropout reduced the representational dimension in fc6 and that dropout=0.7 achieved the best test results during the inference process. The authors also computed the eigenspectra of voxel responses in human visual cortex using NSD data and found that it was similar to AlexNet at dropout=0.7, while the correspondence between fc6 and voxel responses was strongest at dropout=0.7.\n\nI am generally positive about this paper. It replicates the analysis of mouse V1 data in human fMRI data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Using a large-scale fMRI dataset and computational analyses to address the coding principles of human visual cortex\n2. A novel approach is developed to estimate the signal correlation matrix and and the noise correlation matrix in population responses"
            },
            "weaknesses": {
                "value": "see my questions below"
            },
            "questions": {
                "value": "1. In Figure 1, I am not sure why dropout was only implemented in fc6 and fc7 in model training? In a typical training, dropout was applied to all layers\n2. I am wondering about the results of fc7 using the similar analysis in Figs. 2&3\n3. In Figure 2, dropout=0.5 is the best in terms of top-5 ImageNet accuracy (Fig. 2A) but dropout=0.7 is the best in terms of accuracy with unit lession (Fig. 2D). It is debatable what exactly the criteria we emphasize when talking about coding efficiency.\n4. In Figure 3, how many voxels and how many fc6 units are used? I suspect that the numbers are very different.\n5. The original study by Stringer, 2019 only recorded neurons in mouse V1. However, the ROI used here included several low- and high-level cortex. Do you expect the eigenspectrum to be different across visual areas in humans. Actually, I would say that this is the novel point to make compared to simply replicating the analyses in Stringer, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Reviewer_YU1u"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582888284,
        "cdate": 1698582888284,
        "tmdate": 1699637141472,
        "mdate": 1699637141472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BSD4AAp4Mp",
        "forum": "ADDCErFzev",
        "replyto": "ADDCErFzev",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_h1kR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_h1kR"
        ],
        "content": {
            "summary": {
                "value": "To understand the trade-off between high-dimensional and uncorrelated efficient codes and distributed codes known better for generalization and robustness, the authors trained a family of object recognition models with parametrically varying dropout proportions. They found that a higher proportion of dropouts results in more smooth and low-dimensional representational space, with 70% of dropouts offering optimal robustness (against simulated brain lesions). Interestingly, this is also associated with the highest degree of emergent brain alignment for fMRI data in humans. Furthermore, the match between the model and brain representations is associated with a common balance between the efficiency of a high-dimensional code and the robustness of a low-dimensional code."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea and approach are simple, and the results are compelling and conceptually reasonable. The study illuminates the relationship between sparse codes and distributed codes. Using simulated lesions to evaluate robustness is an interesting innovation. The paper is well written. The presentation is clear and well-organized. The best alignment between human fMRI representation and of the representation of the model with the greatest robustness is very interesting, suggesting robustness, in addition to efficiency, is indeed a very important criterion for learning brain representation. So, the study does provide insights into the brain. I rank this an acceptable paper, with \"grade\" somewhere between 6 and 7, so I round it up to 6, because there is no \"7\"."
            },
            "weaknesses": {
                "value": "The idea seems obvious and intuitive, and the approach and the work are perhaps too simple.  \nThe work is most empirical and does not have much theoretical analysis."
            },
            "questions": {
                "value": "While it is still fantastic to prove empirically a simple and intuitive idea is true elegantly,  is it possible to show this analytically?  What would be the analytical approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801557120,
        "cdate": 1698801557120,
        "tmdate": 1699637141352,
        "mdate": 1699637141352,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gz0oqnDVuO",
        "forum": "ADDCErFzev",
        "replyto": "ADDCErFzev",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_yavH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_yavH"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a neurobiological link between dropout, a common ML regularization method and human brain processing. \n\nKey findings include:\n* There was a reduction in object representation dimensionality with increased dropout levels for NNs\n* NN performance was relatively consistent across different levels of dropout, first monotonically increasing and then monotonically decreasing\n* Representational trajectory analysis revealed most differences in the fully connected layers where dropout was applied \n* Dropout mitigates effect of [neuron] lesioning, but to a limited extent\n* A model that was most robust to [neuron] lesioning (dropout p=0.7) mostly closely matched human brain representations obtained from an FMRI study and evaluated using Representational Similarity Analysis (RSA)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a systematic analysis of dropout and [neuron] lesioning with a neuro-biological perspective in mind. Experiments highlight interesting and novel findings with respect to model variations due to dropout and newly discovered connections to human brain processing. Overall, the paper makes advances in understanding spectral properties learnt by humans and neural networks.  \n\nAuthors explain the limitations of their work in terms of considering only one type of regularizations."
            },
            "weaknesses": {
                "value": "The paper would benefit from more clarity and better organization. It was somewhat difficult to understand key components of the paper. For instance, the definition of \u201clesioning\u201d, i.e., pruning neurons in an NN layer, which is central to the paper message, is found in a figure caption (Figure 2), but should appear in the main paper. \n\nSpecific points:\n* It is not clear how the layer for lesioning (relu6) was selected or whether results will still hold if a different layer would be selected. \n* Results are reported using a single architecture (AlexNet), and may not generalize to other NNs. Also, the statement in the beginning of conclusion \u201cschemes within a family of deep neural network (DNN)\u201d needs to be adapted to better reflect the type of networks considered. Specifically, here family seem to refer not to architecture variations, but changes in the level of dropout within an architecture. \n* Authors should discuss the significance of the reported findings for the ML community. I thought that many of the reported analyses were cleverly executed, but I struggled to understand the main message behind the paper."
            },
            "questions": {
                "value": "I did not understand why human responses were compared to network responses but using different datasets (as discussed in section 2.4, human fMRI was obtained using Natural Scenes Dataset and compared to results on ImageNet using networks)? Authors should either explain the motivation of not running networks on the Natural Scenes dataset to match human fMRI, or report results using a single dataset.\n\nIt would be useful to understand the effect of dropout on the trajectories from representational trajectory analysis reported in Figure 1B. Specifically, is it dropout that induces the highest variation in the fc layers, or the underlying network architecture?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816536038,
        "cdate": 1698816536038,
        "tmdate": 1699637141243,
        "mdate": 1699637141243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tV8ItZlZYF",
        "forum": "ADDCErFzev",
        "replyto": "ADDCErFzev",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_wjr3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9065/Reviewer_wjr3"
        ],
        "content": {
            "summary": {
                "value": "The paper use dropout as an approach to control the sparsity of neural representation in deep neural network (AlexNet), and evaluated the robustness of the resulting networks to lesions to one of the layers. The dropout rate of 0.7 was found to be most robust to lesion at inference time. Coincidently, at this level of dropout, the geometry of neural representation in the same layer also shows the highest correlation to the neural representational geometry to a set of COCO dataset images, in occipitaltemporal cortex of human. Further investigating the rate of decay in the eigenspectrum of the brain region and those of DNNs with different dropout rates, the decay rate of the model with 0.7 dropout rate also matches the decay rate of the human brain. Together, the result appears to suggest that, if dropout rate can be considered as a valid way of controlling for the sparsity of neural encoding, then perhaps the brain chooses its level of sparsity at a level most robust to lesioning of \"neurons\"."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The result shows a high consistency among three results: the dropout rate yielding the highest robustness of the DNN against high-level lesion, the dropout that yields neural representation best matching that of the object-recognition related brain region, and the the dropout rate yielding the same decay rate in the eigenspectrum of the covariance structure of fMRI spatial patterns across images.\n\nIt used a novel approach to separate the spatial covariance structure due to potentially noise and that due to signal in fMRI activity"
            },
            "weaknesses": {
                "value": "Although the decay rate of eigenspectrum is an intuitive way of showing the effective dimensions being used to encode stimuli, I still feel there is a lack of direct demonstration of the level of sparseness resembling the orientation tuning example in Figure 1A. If indeed the neural network learns a sparse coding, I suppose you will find that the number of positive responses in the relu6 (or a layer that the authors consider appropriate) at inference stage varies according to the dropout rate at training time, or at least the number of units showing response above a threshold (not necessarily zero) should show such a dependency on dropout rate.\n\n\nThe robustness drop due to lesion at inference time only appears apparent at very high level of lesion. What is the implication of this for the brain: do we expect that such level of unexpected inactivation of neurons is common for our object recognition regions, so that it is necessary to adjust the region's selection in the continuum between sparse and distributed coding?"
            },
            "questions": {
                "value": "Although the GSM is not a focus in this paper, I think in principle it is not guaranteed that the subtraction of the empirical noise covariance matrix from data covariance matrix results in a positive definite matrix. In other words, the eigenvalues can be negative. So I am worried that this approach is not generalizable if any readers want to test the hypothesis of this paper on other neural networks.\n\nAlthough AlexNet only applied dropout to the two layers investigated here, in principle it can be applied to any layer. One natural question is how dropout at earlier layers influence the coding scheme at the layer of investigation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9065/Reviewer_wjr3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9065/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699681624855,
        "cdate": 1699681624855,
        "tmdate": 1699681624855,
        "mdate": 1699681624855,
        "license": "CC BY 4.0",
        "version": 2
    }
]