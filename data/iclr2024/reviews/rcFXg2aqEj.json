[
    {
        "id": "KjO4YvfECS",
        "forum": "rcFXg2aqEj",
        "replyto": "rcFXg2aqEj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to extract information from visually rich documents with the information and their position. The method extracts information from the OCR'ed document by prompting the PaLM2-S LLM to perform completion tasks. With the LLM with entity extraction training, the model can achieve strong performance even with no training data, comparable to or better than a few baselines. With a few-shot setting, the method shows a high performance with a large margin compared to existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes a novel method to document information extraction from visually rich documents using LLMs. \n- The absolute performance is greatly higher than existing methods, and some of the proposed enhancements are shown effective through the ablation study. \n- The paper is well-written with the details of algorithms, schemas, sample outputs, etc."
            },
            "weaknesses": {
                "value": "- The model's performance highly depends on the off-the-shelf OCR and PaLM2-S large language model, but they are unavailable, so the results are not reproducible. Also, there is no detailed explanation or evaluation of these modules. \n- The authors mention the support of the hierarchical entity and entity localization, but their effect is not directly evaluated since there is no evaluation without them."
            },
            "questions": {
                "value": "- Is there any performance assessment of the OCR model? \n- How does the model differ from baselines in, e.g., the number of parameters and runtime? Although the authors remark on using open-source LLMs as future work, how is it difficult to run the model with publicly accessible OCR and existing LLMs? \n- Did the authors try other prompts, schema, or target formats during the development? How are the current settings chosen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No problem."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_5Y8u"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698284517643,
        "cdate": 1698284517643,
        "tmdate": 1699636135299,
        "mdate": 1699636135299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B05L9EkvJG",
        "forum": "rcFXg2aqEj",
        "replyto": "rcFXg2aqEj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
        ],
        "content": {
            "summary": {
                "value": "When applying LLM to Visually Rich Document (VRD) understanding, many methods use the two staged approaches: first execute the text recognition/serialization step and then execute the parsing step. However, lots of methods suffer from the need for large training data or are not able to predict hierarchical entities or hallucinations in domains other than text-only data. These problems are due to the absence of layout encoding within LLMs and the absence of a grounding mechanism ensuring the answer is not hallucinated. To overcome these challenges, the authors propose the five staged frameworks: OCR - chunking - prompt generation - LLM inference - decoding. The suggested framework is experimented with PaLM 2-S and compared to several publicly available baseline models on Visually Rich Document Understanding (VRDU) and Consolidated Receipt Dataset (CORD), resulting in a bigger performance margin than baseline methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Suggest reasonable methods to tackle the challenges of visual document understanding.\n- Provide rich information to reproduce experiments"
            },
            "weaknesses": {
                "value": "* Though the suggested method seems agnostic to specific LLM, the authors experimented only with PaLM 2-s. To verify the superiority of the suggested framework, additional experiments using LLMs other than PaLM are needed (I think the additional experiment would enhance the presentation of the robustness of the proposed method)."
            },
            "questions": {
                "value": "* In document representation, when generating prompts, how well do coordinate tokens work? Line-level segments with 2 coordinates are enough for various VRD data?\n* Schema representation is important in the perspective of getting information in VRD. However, it would be vulnerable to hallucination. Does LLM properly parse JSON format?\n* When doing Top-K sampling, we can choose Top-K sampling for individual N chunks and then merge, or do Top-K sampling for entire N chunks. I guess the latter method is better for the semantic integration quality (the similar reason that authors used the entire predicted tree value from a single LLM completion for hierarchical entities), but the authors used the former method. Is there a reason? I think the comparison may be interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_W15R"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824294042,
        "cdate": 1698824294042,
        "tmdate": 1699636135224,
        "mdate": 1699636135224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mYU2MJAyl3",
        "forum": "rcFXg2aqEj",
        "replyto": "rcFXg2aqEj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LMDX \u2014 a mechanism for information extraction from documents leveraging off-the-shelf Optical Character Recognition service and LLM prompt engineering approach with PALM LLM for processing the extracted information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n* The paper shows potential of using LLMs for information extraction from documents\n* Ablation studies are interesting and show the value of fine-tuning the PALM LLM for document information extraction"
            },
            "weaknesses": {
                "value": "Weaknesses:\n\n* This paper proposes a mechanism for information extraction from documents leveraging off-the-shelf Optical Character Recognition service and complicated LLM prompt engineering approach for processing the extracted information. The main underlying assumption driving the complexity of the prompt engineering approach is limited context length of LLMs. However, models like Claude 2 are capable of working with 100K token context windows. Additionally, methods like RoPE scaling and other context length expansion approaches allow to increase the context size for other LLMs including open-source models. As there are effective ways to address the context length limitation, the presented prompt engineering approach is a somewhat incremental engineering contribution, especially given its complexity. The fine-tuned model, however, is of interest.\n* While the proposed approach outperforms other baselines on VRDU and CORD benchmarks, the performance advantage clearly comes from using a powerful LLM. It would be important to compare this method to OCR+long-context LLMs such as Claude 2.\n* Another reasonable baseline with the potential to achieve high performance on these benchmarks is a multi-modal vision-text LLM, for example GPT-4. It has potential to work out of the box, without requiring fine-tuning, and significantly outperform other baselines.\n* Code is not provided.\n* Many unexplained abbreviations: e.g., IOB, NER. Readers would benefit from expanding these abbreviations the first time they are used."
            },
            "questions": {
                "value": "Questions:\n* In-context learning is likely to significantly improve performance on this task, have you tried any experiments with in-context demonstrations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no ethics concerns"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2034/Reviewer_Vt8a"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698905360679,
        "cdate": 1698905360679,
        "tmdate": 1700549401827,
        "mdate": 1700549401827,
        "license": "CC BY 4.0",
        "version": 2
    }
]