[
    {
        "id": "xIE3FHT1V7",
        "forum": "qW9GVa3Caa",
        "replyto": "qW9GVa3Caa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_5oJb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_5oJb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a methodology called Prototype Generation for enhancing the interpretability of deep learning models. The authors demonstrate the effectiveness of this approach in identifying biases, spotting spurious correlations, and enabling rapid iteration in model development. The paper also discusses the advantages of this methodology in terms of understanding what a model has learned and facilitating targeted retraining. However, the authors acknowledge the limitations of their method and suggest future work to address these limitations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the paper presents an interesting approach to improving the interpretability of deep learning models through Prototype Generation. The methodology is well-explained, and the examples provided effectively demonstrate its potential benefits. The paper also highlights the importance of interpretability in high-stakes applications and the need for a deeper comprehension of model behavior.\n\n1. The paper addresses an important problem in the field of deep learning - interpretability. \n2. The Prototype Generation methodology is well-described and provides meaningful insights into model behavior. \n3. The examples and case studies presented in the paper effectively demonstrate the usefulness of prototypes in identifying biases and understanding model failures. \n4. The discussion on targeted retraining and the iterative feedback loop adds practical value to the proposed methodology."
            },
            "weaknesses": {
                "value": "1. The limitations of the method are acknowledged but not thoroughly discussed. It would be helpful to provide more insights into the potential challenges and drawbacks of Prototype Generation.\n2. The paper could benefit from a more detailed comparison with existing interpretability techniques to highlight the novelty and advantages of the proposed approach."
            },
            "questions": {
                "value": "Please help to check the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649803616,
        "cdate": 1698649803616,
        "tmdate": 1699636866790,
        "mdate": 1699636866790,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kdtJHT2VnT",
        "forum": "qW9GVa3Caa",
        "replyto": "qW9GVa3Caa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_fFHp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_fFHp"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a method called \"prototype generation\" to visualize the prototypical input (called the \"prototype\") that would maximally activate the logit of a particular class c. To find such a prototype for each class c for a given neural network, the method starts from a baseline image (generated by optimizing a probability variance loss, which encourages the generated baseline image to have an equal probability to be classified into any class by the network), and then optimizing the baseline image by minimizing the negative logit of the desired class c and the high-frequency loss that penalizes large differences between adjacent pixel values. The expected result is a prototypical input image for class c that maximizes the logit for class c and is smooth and \"natural\" looking. The authors also performed experiments using a trained ResNet-18 and a trained InceptionV1."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The authors made an attempt to formalize the notion of \"path similarity\" between a generated prototype and a \"natural\" image, using L1 distance or spearman similarity."
            },
            "weaknesses": {
                "value": "- The proposed method lacks novelty. It is exactly the same as activation maximization applied to the logit of each class c, with regularizations to smooth generated images. This idea has been well explored in past work (e.g., Nguyen et al., 2016).\n- There is only one prototype generated per class. In reality, each class could have multiple prototypical images.\n- While there is an attempt to make generated prototypes more \"natural,\" they do not actually look natural.\n- While the authors made an attempt to formalize the notion of \"path similarity\" between a generated prototype and a \"natural\" image, this notion is not used to generate prototypes."
            },
            "questions": {
                "value": "N/A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788995408,
        "cdate": 1698788995408,
        "tmdate": 1699636866671,
        "mdate": 1699636866671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RJkNO2Wv7k",
        "forum": "qW9GVa3Caa",
        "replyto": "qW9GVa3Caa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7267/Reviewer_WSWR"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a technique for global interpretability of image classification networks via prototype generation. The main idea is to optimize for an input image (called a prototype for a class) which maximizes the logit corresponding to that class while keeping model parameters constant. The authors then determine a \u2018good\u2019 prototype by measuring the spearman correlation between activations of the prototype across all layers ($A_P$) vs the activations of a sample of images from  the class across all layers ($A_I$). They also use L1 distance as another distance metric to measure similarities on $A_P$ and $A_I$. \n\nExperiments are performed on pertained ResNet-18 and Inception V1 on ImageNet. The authors visualize the prototypes on the \u2018academic gown\u2019 and \u2018mortarboard\u2019 (graduation cap) classes in ImageNet. The gown prototype shows a patch resembling a face with a light skin and the authors conclude that this would result on the model being biased against darker skinned people wearing gowns. This is confirmed via experiments on a hold out test set which shows 12.5% higher performance on lighter skinned individuals."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Significance of the problem: The paper introduces a technique to very important problem in the interpretability community (easy-to-understand global interpretability). \n2. The ideas in the paper are simple to understand and the intuition is well explained.\n3. Novelty: Global interpretability via visualization for image classification models has not been done before to my knowledge. I think this makes the technique interesting."
            },
            "weaknesses": {
                "value": "1. My main concern lies with the qualitative nature of the findings presented in the paper and the resulting ambiguity in the explanations. The authors claim that \u201cthe academic gown prototype in Figure 6a shows a lighter-skinned face prominently\u201d. Here 6a refers to the image of the prototype for \u2018academic gown\u2019.  Just by looking at that image, I cannot definitively say that the light colored patch is definitely a human face. Without looking at images in the dataset, one cannot assume that there is no other object that shows up in a prototype. Since one of the main claims of the paper is that their method allows us to explain models without combing through the dataset, I\u2019m unconvinced about the practical gain the authors claim their method provides.\n2. The second experiment in the paper is about the \u2018mortarboard\u2019 (graduation cap) class where the authors look at the prototype and conclude that the model would get confused if they saw faces with caps in the image instead of just caps. Again, the prototype images only contain light colored patches so it is impossible to conclude that they are faces with certainty unless you are explicitly looking for faces in the first place. Thus, in this case, the user already had an explanation in mind and they are only looking at the model for a confirmation of their pre-existing biases rather than a novel explanation.\n3. Experiments not comprehensive enough: The authors report experiments on two classes from ImageNet (no justification was given for why these two were chosen specifically). To make sure their method works, we would need to know if this method generalizes to more classes.\n4. Details lacking: At several points while reading the paper, I felt there were some significant details lacking. For example: Equations for the loss are not stated (what is the high frequency loss?). Similarly, the authors report having adapted the visualization technique from Olah et al. It would be nice to have a brief description of technique so that the paper is more self contained. I was also confused by Fig 3 which talks about optimizing parameters. I was under the impression that we are optimizing the input image w.r.t output logits. What parameters are we talking about here? Please correct me if I am wrong.\n5. Minor: Notation in equation 1 is not clearly explained. I think I understand what each of the terms mean from the context but clearly spelling out what each term means (including the subscripts) is important for good readability.\n6. Limitations: Since the activations of the prototype is compared to the average of a sample of images from the input class distribution, the prototype is going to be biased towards covering the average of the most common features and ignore outliers."
            },
            "questions": {
                "value": "1. Why have a single prototype for each class? I would imagine having multiple prototypes would enhance coverage over the full diversity of class images.\n2. What about outliers? When we take a mean over activations, we would effectively start to ignore outliers in our explanations.\n3. Wouldn\u2019t using the high frequency loss (no large difference in adjacent pixel values) smooth out the prototype image and, thus, cause the prototype to look more diffuse? I\u2019m not sure if this is a good or a bad thing so I\u2019m curious to know what the authors think."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297742343,
        "cdate": 1699297742343,
        "tmdate": 1699636866574,
        "mdate": 1699636866574,
        "license": "CC BY 4.0",
        "version": 2
    }
]