[
    {
        "id": "5JLpCcxrLv",
        "forum": "p4S5Z6Sah4",
        "replyto": "p4S5Z6Sah4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_6SWP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_6SWP"
        ],
        "content": {
            "summary": {
                "value": "Over the past decade, recurrent neural networks, trained by backpropagation through time, have been used to infer mechanisms employed by networks of biological neurons to perform cognitive tasks. Yet, most existing literature on biological RNNs focus on attaining fixed or slow points at steady-state, and neglects the other well-known solution -- oscillations. On the other hand, traveling waves have been observed in the brain and studied for multiple decades. Waves in general are well-understood mechanistically, but their computational role in the brain remains a hot debate. Here, the authors bridge this very obvious gap by introducing an RNN architecture that produces traveling waves. Wave-RNNs store waves in multiple rings (which they call \"channels\") and seemingly perform reasonably well across a variety of sequence-based tasks.\n\nI am supportive of the general idea and the rigor of this work. However, the model introduction was very hard to parse and requires several rereads and returning to after reading future sections in order to reconcile any confusion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This work is timely and important for the biologically-plausible RNN community to acknowledge the possibility of oscillatory/wave-like solutions that can arise from gradient-based training. The work is rigorous and well-motivated."
            },
            "weaknesses": {
                "value": "While the general narrative makes sense, I find the introductory narrative very hard to understand. \n\n(1) Since $\\mathbf{u}$ has dimensions $c \\times c \\times f$, this means that there are convolutional kernels between rings, but the intuition provided does not seem to acknowledge this.\n\n(2) The usage of identity-initialized RNNs was also not clear. From the parameter count of such models, it seems like every element of the weight matrix is being optimized. That is not mentioned anywhere in the text (or if it is, it should be highlighted more clearly). Indeed, that is the case for Le et al 2015, but this would detract from the bump model which the authors originially intended to be the main baseline model to compare with wave-RNNs.\n\n(3) The way wave-RNNs are named is confusing and inefficient. At some times, it is labeled as $n = 100, c=6$ to represent $n$ as the total number of neurons and $c$ as the number of channels. This means that there are 16 neurons per channel, and where does the last 4 neurons go (how does the floor function in page 3 work)? At other times it is called $16c$ which I assume refers to the same thing. In both ways of naming, there is no mention about the dimensionality of the kernel -- I suspect it may not be constant because of the remaining 4 neurons, but there has to be a better way to label everything.\n\n(4) In Figure 2, neurons in the iRNN are sorted according by time of maximum activation. How are the neurons in the wRNN sorted and where is the channel separation?\n\n(5) The training curves in Figures 3,5,6 are extremely transient and subject to random \"resets\" in performance. This means that in a single training session, the efficiency of training depends (by luck) on number of resets that happen, which is very apparent in Figure 3. This makes any conclusions drawn about training efficiency unconvincing. More models should be trained and the loss curves averaged."
            },
            "questions": {
                "value": "(1) Did the authors really initialize $\\mathbf{V}$ to be zero (as claimed on page 3)? I think it is a typo and they actually meant initializing $\\mathbf{b}$ to be zero instead?\n\n(2) There seems to be no additional point in training wRNN + MLP? It provides a small improvement and a single sentence explaining that linear decoding is a bottleneck (which is in fact an important bottleneck to prevent overfitting in neural data) -- am I missing something?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Reviewer_6SWP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682329504,
        "cdate": 1698682329504,
        "tmdate": 1699636895048,
        "mdate": 1699636895048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yoCPtrhnqK",
        "forum": "p4S5Z6Sah4",
        "replyto": "p4S5Z6Sah4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_2PWQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_2PWQ"
        ],
        "content": {
            "summary": {
                "value": "The present study uses a neurally inspired convolutional recurrent network model with traveling wave states to accomplish a set of tasks. The authors claim the traveling wave recurrent network outperforms the one without waves."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The present study is a good and novel example of brain-inspired computation in that it uses the wave RNN (inspired by the brain) to implement a set of benchmark tasks. It also considers a set of experiments that support the wave RNN indeed outperforms its counterparts without traveling waves. It provides us insight that structured spatiotemporal dynamics can have its advantages in real applications."
            },
            "weaknesses": {
                "value": "### Major\n- Although the concept of this study is novel, I feel the present paper can be strengthened by conducting a deeper analysis to show why internally generated traveling waves in RNNs can improve those computational tasks. For example, since the network model is small (I see the limitation discussed in the end), the authors could perform a dimensionality analysis to demonstrate the network's evolution in the low-dimensional manifold. I do see the author providing an intuitive explanation of the benefit of traveling waves in the introduction, and showing neurons' response in Fig. 2, but they are not enough from my point of view. \n\n- The illustrative example in Fig. 1 (middle) is probably not a perfectly matching example in explaining the benefit of traveling waves. The Fig. 1 illustrates an example of a two-way wave equation, however, the traveling wave in the RNN is only a one-way wave equation that the network state moves in a single direction. I am not clear about why the traveling wave could maintain the spatiotemporal information.\n\nI'd like to raise my rating if the two concerns can be properly addressed.\n\n### Minor\n- Eq. 2: it seems that the matrix $\\Sigma$ misses the time step $\\Delta t$ in discretizing Eq. 1. Otherwise the speed $v$ cannot exceed 1 (the term $1-v$ in $\\Sigma$). I mean you can absorb the $Delta t$ into a new parameter in Eq. 2 without explicitly expressing it, but in this case the $v$ in Eq. 2 is not the same $v$ in Eq. 1.\n\n- Below Eq. 2: I am confused about what the input channel means.\n\n- Below Eq. 3: it is not clear how the convolution between $u$ with dimension [c,c,f]  and $h$ with dimension [c,n'] is calculated."
            },
            "questions": {
                "value": "- Do you need to retrain the RNN to copy sequences with different lengths, or produce waves with different speeds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778653014,
        "cdate": 1698778653014,
        "tmdate": 1699636894941,
        "mdate": 1699636894941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GcFjXpmNdP",
        "forum": "p4S5Z6Sah4",
        "replyto": "p4S5Z6Sah4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_ovPq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_ovPq"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Wave-RNN, in which the hidden states are organized in time to resemble a traveling wave.  Because the position of the wave enables one to reconstruct the time of the event that triggered it, the network can maintain precise information about what happened when in the past.   A one-layer wRNN is evaluated on some artificial tasks (e.g., psMNIST) to evaluate its ability to solve problems with long-range dependencies.  It is compared to other models, especially Identity RNN which the authors argue provides a fair comparison because it has long memory but does not exhibit waves."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This model can remember not only retain information about input to the network but also the time at which it was experienced as long as the waves persist.  Information about time is very useful."
            },
            "weaknesses": {
                "value": "The connection between this computational model and traveling waves in the cortex is extremely tenuous.  The problem is that the goal of this model is to allow information to be remembered for a long time whereas the traveling waves over the cortex (and hippocampus and striatum) are much faster.  For instance, the Siapas & Lubenov paper shows that theta travels the length of the hippocampus in on the order of 200 ms.  \n\nThere is some evidence for very slow oscillations in MEC, but there is no evidence these are traveling waves.\nhttps://doi.org/10.1101/2022.05.02.490273 \nhttps://doi.org/10.1016/j.celrep.2023.113271\n\nOn the other hand, there is extremely robust evidence for reliable sequences of firing in the brain over time scales relevant for memory.   \nhttps://doi.org/10.1038/s41467-018-07020-4\nhttps://doi.org/10.1016/j.cub.2021.01.032 \nThis phenomenon is often (but not always) referred to as ``time cells.'' There is not evidence that these sequence are anatomically organized, but that doesn't seem to be important for the model.  \n\nThere is a critical difference between the sequences observed with time cells and the traveling waves here.  In particular, the sequences in the brain slow down as they unfold.  This is as if the wave was traveling through a fluid whose properties change systematically from one end of a channel to the other.  This does not seem to be a property of this model.  Although Figure 9 in Appendix C shows waves that travel at different speeds in different channels, the waves within each channel proceed at a constant velocity (the right panel is more complicated but certainly doesn't slow down the way the neural data do)."
            },
            "questions": {
                "value": "Suppose that the velocity v in Eq. 2 changed systematically as a function of the row of the matrix.  In particular, what if v went down like 1/x?  How would that wave behave?  How would this change how the longest time horizon of the memory scales with number of weights/units?  Would this model behave better or worse on the problems in this paper than wRNN does?   Presumably worse because the temporal resolution at long delays would be poor and problems like psMNIST require precise timing information.  Could those problems be mitigated with a deep network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7452/Reviewer_ovPq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819113741,
        "cdate": 1698819113741,
        "tmdate": 1700718497766,
        "mdate": 1700718497766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5bwSrmILsA",
        "forum": "p4S5Z6Sah4",
        "replyto": "p4S5Z6Sah4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_h1zv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7452/Reviewer_h1zv"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors explore how patterns of wavelike activity, observed in brains, might help artificial neural networks learn and recall sequences of inputs. They discretize the 1-dimensional wave equation, finding a common structure between the discretization matrix and convolutions that they exploit to set up a simple RNN that naturally supports wavelike activity patterns.  After demonstrating that the network indeed exhibits waves, they test how such waves may facilitate sequence learning and recall in a number of different tasks by comparing the network to a very similar one initialized in a manner that does not naturally support waves.  Overall, the authors\u2019 RNN performed impressively well across a number of sequence tasks, including a more complex permuted MNIST sequence task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors overall provide a very good job in providing motivation, relevant biological background, and mathematical intuition for their network setup.  The results of their simple setup are impressive, and are generally carefully presented and analyzed, including with ablation studies.  Moreover, the authors provide a fuller characterization of the wave activity and performance of their network in the Appendix, including, transparently, their network result distributions, which is generally far too lacking in the field. Similarly, they provide their code in an anonymous repo and specify the parameters of the network, increasing the reproducibility profile of the work."
            },
            "weaknesses": {
                "value": "Note, the below concerns have resulted in a lower score, which I would be happy to increase pending the authors\u2019 responses. \n\n**A. Wave fields**\n\nThe wave-field comparisons, claims, and references seem a bit strained and unnecessary.  Presumably, by \u201cwave-field,\u201d the authors simply mean a vector field that supports wave solutions.  In any case, since this term is not oft-used in neuroscience or ML that I am aware of, a brief definition should be provided if the term is kept.  However, I am unsure that it is necessary or helpful.  That the brain supports wavelike activity is well-established, and some evidence for this is appropriately outlined by the authors.  Many computational neuroscience models support waves in a way that has been mathematically analyzed (e.g., Wilson-Cowan neural fields equations).  The authors\u2019 discretization methodology suggests a similar connection to such analyses.  However, appealing to \u201cphysical wave fields\u201d to relate waves and memory seems to be overly speculative and unnecessary for the simple system under study in this manuscript.  The brain is a dissipative rather than a conservative system, so that many aspects of physical wave fields may well not apply.  Moreover, the single reference the authors do make to the concept does not apply either to the brain or to their wave-RNN.  Instead, Perrard et al. 2016 describe a specific study that demonstrates that a particle-pilot wave system can still maintain memory in a very specific way that does not at all clearly apply to brains or the authors\u2019 RNN, despite that study studying a dissipative (and chaotic) system. Instead, the readers would benefit much more from gaining an intuition as to why such wavelike activity might benefit learning and recalling sequential inputs.  Unfortunately, Fig. 1 does little to help in this vein. \n\nHowever, the concept certainly is simple enough, and the authors provide a few intuitions in the manuscript that help.  I believe the manuscript would improve by removing the discussion of wave fields and instead providing / moving the intuitive explanations (e.g., the \u201cregister or \u2018tape\u2019\u201d description on p. 20) as to how waves may help with sequential tasks to the same portion of the Introduction.  \n\n**B. Fourier analysis**\n\nOverall, I found the wave and Fourier analysis a bit inconsistent and potentially problematic.  While I agree that the wRNNs clearly display waves when plotted directly, the mapping and analysis within the spatiotemporal Fourier domain (FD below) does not always match patterns in the regular spatiotemporal plots (RSP below).  Moreover, it\u2019s unclear how much substance they add to the analysis results.  In more detail: \n\n1. Constant-velocity, 1-D waves don\u2019t need to be transformed to the FD to infer their speeds.  The slopes in the RSP correspond to their speeds.  For example, in Fig. 2 (top left), there is a wave that begins at unit 250, step ~400, that continues through to unit 0, step ~650, corresponding to a wave speed of ~1.7 units/step, far larger than the diagonal peak shown in the FD below it that would correspond to a speed of ~0.3 units/step, as indicated by the authors.  \n\n2. Similar, seemingly speed mismatches can be observed in the Appendix.  E.g., in Fig. 9 (2nd column, top), the slopes of the waves are around 0.35-0.42 units/step (close enough to likely be considered the same speed, especially as they converge in time to form a more clustered wave pulse) from what I can tell, whereas the slopes in the FD below it are ~0.3 for the diagonal (perhaps this is close enough to my rough estimate) and ~0.9, well above any observable wave speed. Perhaps there is a much faster wave that is unobservable in the RSP due to the min/max values set for the image intensity in the plot, but in that case the authors should demonstrate this.  Given (a) the potential mismatch in the speeds for the waves that can be observed, (b) the mismatch in the speeds discussed above in Fig. 2, and (c) the fact that some waves may be missed in FD (see below), I would worry about assuming this without checking.\n\n3. As alluded to in the point above, iRNN in Fig. 2 appears to have some fast pulse bursts easily observed in the RSP that don\u2019t show in the FD. For example, there is a very fast wave observable in the RSP in units ~175-180, time steps 0-350.  Note, the resolution is poor, but zooming in and scrolling to where the wave begins around unit 175, step 0 makes it clear.  If one scrolls vertically such that the bottom of the wave at step 0 is just barely unobservable, then one can see the wave rapidly come into view and continue downwards.  Similarly some short-lasting, slower pulses in units near 190, steps 0-350 are observable in the RSP.  None of these appear in the FD.  Note, this would not take away from the claim that wRNNs facilitate wave activity much more than iRNNs do, but rather that some small amounts\u2014likely insufficient amounts for facilitating sequence learning\u2014of wave activity might still arise in iRNNs.  If the authors believe these wavelike activities are aberrant, it would be helpful for them to explain why so.\n\n4. I looked over the original algorithm the authors used (in Section III of \u201cRecognition and Velocity Computation of Large Moving Objects in Images\u201d\u2014RVC paper below\u2014which I would recommend for the authors to cite), and I wonder if an error in the initial calibration steps (steps 1 & 2) occurred that might explain the speed disparities observed between the RSPs and FDs.\n\n5. There do seem to be some different wave speeds\u2014e.g., in Fig. 9, there appear to be fast and narrow excitatory waves overlapping with slow and broad inhibitory waves. But given that each channel has its own wave speed parameter $\\nu$, it isn\u2019t clear why a single channel would support multiple wave speeds.  This should be explored in greater depth, and if obvious examples of sufficiently different speeds of excitatory waves are known (putatively Fig. 9, 2nd column), these should be clearly shown and carefully described and analyzed.\n\n6. Is there cross-talk across the channels?  If so, have the authors examined the images of the hidden units (with dimensions __hidden units__ x __channels__) for evidence of cross-channel waves?  If so, perhaps this is one reason for multiple wave speeds to exist per channel?\n\n7. Overall, it is unclear overall what FT adds to the detection of 1-D waves.  If there are such waves, we should be able to observe them directly in the RSPs.  In skimming over the RVC paper, it seems like it would be most useful in determining velocities of 2-D objects and perhaps wave pulses.  That suggests that one place the FD analysis might be useful is if there are cross-channel waves as I mention above.  If so, the waves should still be observable in the images (and I would encourage such images be shown), but might be more easily characterized following the marginalization decomposition procedure described in the original algorithm in Section III of the RVC paper.  Note, the FDs might also facilitate the detection of multiple wave speeds in the network, as potentially shown in Fig. 9.  However, in that case it would seem they should only appear in Fig. 9, and if the speeds are otherwise verified.\n\n8. The authors mention they re-sorted the iRNN units to look for otherwise hidden waves.  This seems highly problematic.  If there are waves, then re-sorting can destroy them, and if there is only random activity then re-sorting can cause them to look like waves.  \n\n**C. Mechanisms**\nFinally, while the results are overall impressive, and hypotheses made regarding the underlying mechanisms for the performance levels of the network, there is too little analysis of the these mechanisms.  While the ablation study is important and helpful, much more could be done to characterize the relationship between wavelike activity and network performance.\n\n**D. Minor**\n1. Fig. 2: Both plots on the right have the leftmost y-axis digits obscured\n2. Fig. 9, top, plots appear to have their x- and y- labels transposed (or else the lower FD plots and those in Fig. 2 have theirs transposed.\n3. Fig. 15 needs axis labels"
            },
            "questions": {
                "value": "Please see **Weaknesses**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980756114,
        "cdate": 1698980756114,
        "tmdate": 1699636894709,
        "mdate": 1699636894709,
        "license": "CC BY 4.0",
        "version": 2
    }
]