[
    {
        "id": "tLQHO6ds03",
        "forum": "rtx8B94JMS",
        "replyto": "rtx8B94JMS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission972/Reviewer_kkY2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission972/Reviewer_kkY2"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel variational inference method for stochastic differential equations (SDEs) under fractional Brownian motions (fBMs). The key property of fBMs lies in long-term dependence in its stochastic generation. In order to devise the proposed variational inference, the paper suggests to use approximation based on the result of Harms & Stefanovits. That is, fBMs can be represented as integral (infinite linear combination) via Ornstein-Uhlenbeck processes. The SDEs under fBMs are then rewritten as a system of SDEs under Brownian noise."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is overall well-written, providing an excellent introduction of fractional Brownian motion including type I, and type II. \n- Showcase a practical use of Harms & Stefanovits, 2019. This is a nice approach compared to Tong et al. 2022 suffered from some limitations including the choice of solvers.\n- Interesting application learning for SDE."
            },
            "weaknesses": {
                "value": "- Perhaps variational inference may not be the main contribution of the paper since it is still a result of Girsanov's theorem like presented in Li et al. 2020. However, I believe that the idea of finding SDE approximation outweights this point."
            },
            "questions": {
                "value": "Minor points:\n-  In the first paragraph of Background section, do you mean \"learning comunity\" as \"machine learning comunity\"?\n- At Proposition 1, \u201cMarkov rocesses\" should be \u201cMarkov processes\u201d.\n- At Proposition 2, \"R^D\" should be \"\\mathbb{R}^D\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698215652337,
        "cdate": 1698215652337,
        "tmdate": 1699636023142,
        "mdate": 1699636023142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tovLNcnC7T",
        "forum": "rtx8B94JMS",
        "replyto": "rtx8B94JMS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission972/Reviewer_9F2a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission972/Reviewer_9F2a"
        ],
        "content": {
            "summary": {
                "value": "Generative modeling of sequential data (i.e., time series data) is a vibrant research area and has made remarkable progress since the introduction of neural SDEs in recent years. Related to this are variational Bayesian inference methods combined with the application of Girsanov\u2019s change of measure theorem, which have paved the way for learning representative function distributions through stochastic gradient descent. Most of the progress in this area has been on models based on the use of (standard) Brownian motion, i.e., a Markov process. The present work extends existing approaches by building on *fractional* Brownian motion (fMB; a non-Markovian process), questioning the everlasting validity of the Markov property assumption. The key to success lies in the application of Markovian embedding of the fBM and its strong approximation which makes the traditional machinery of neural SDEs accessible for working with non-Markovian systems. Conducted experiments on real data sets as well as ablation studies provide empirical evidence for doubts about the unrestricted model suitability of previous approaches in real world applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I enjoyed reading this paper because (i) I like the idea of challenging common theoretical assumptions (in this case, the Markov property), (ii) the very well-organized writing style that makes it easy to follow the arguments and (iii) the thorough presentation of the theoretical aspects. Further, the paper content is original to the best of my knowledge and well positioned to existing work, as well as with very few exceptions, has no spelling or grammatical flaws. Empirical evidence is accessed through experiments on synthetical and real-world video data. The results are on a par with compared existing methods and reproducibility is granted by submitted code files. Finally, on a positive note, the conjecture that data contain information that cannot be represented by Markov processes is empirically tested by estimating the Hurst index."
            },
            "weaknesses": {
                "value": "A major drawback of (traditional) NSDE approaches in the context of variational Bayesian inference is the computational and storage costs associated with learning almost arbitrarily complex representative function distributions. Recent approaches address this problem and attempt to find solutions, e.g., (Li et al. 2020) and (Kidger et al. 2021). However, learning large datasets still seems to be very resource intensive. In the context of the present work, I wonder to what extent the additional Markov approximation scheme exacerbates the aforementioned problem. For example, in the case of the video experiment presented, the authors report (p. 19, Appendix E.3) \"Models were trained on a single NVIDA GeFORCE RTX 4090, which takes about 39 hours for one model.\" Can you please provide a more detailed evaluation of runtime in addition, including a comparison to baseline approaches?"
            },
            "questions": {
                "value": "1. Can you elaborate on the difference between a Type I and a Type II fBM and which form delivers benefits in which circumstances?\n2. It would be of importance to comment on the assumptions that guarantee a finite KL term inside of the ELBO, i.e., the diffusion terms of prior and posterior have to be equal. \n3. According to (Oksendal 2003, The Girsanov Theorem II), the control function $u$ needs to meet Novikov's condition. In the proposed work $u$ is parameterized by a neural net. How is the aforementioned condition controlled during training?\n4. Can you elaborate more on the following: \"Unlike Tong et al. (2022) our approach is agnostic to discretization and the choice of the solver.\"\n\nMinor:\n- Missing parenthesis in Eq. (5)\n- Proposition 1 : Markov rocesses -> Markov processes\n- $\\text{d} t$ vs $dt$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "--"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission972/Reviewer_9F2a",
                    "ICLR.cc/2024/Conference/Submission972/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708876696,
        "cdate": 1698708876696,
        "tmdate": 1700678933559,
        "mdate": 1700678933559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4t8vLsXPhk",
        "forum": "rtx8B94JMS",
        "replyto": "rtx8B94JMS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission972/Reviewer_3XSn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission972/Reviewer_3XSn"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel variational framework for conducting inference in stochastic differential equations driven by Markov-approximate fractional Brownian motion. This framework combines SDEs with variational methods to enable the learning of representative function distributions through stochastic gradient descent. Unlike conventional SDEs that assume Brownian motion, this framework extends to fractional Brownian motion to capture long-term dependencies. Authors derive the evidence lower bound for efficient variational inference and provide a closed-form expression for determining optimal approximation coefficients. The framework is validated on synthetic data, and it also presents a novel architecture for variational latent video prediction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper introduces generative models that utilize fractional Brownian motion (fBM) as a noise injection, as opposed to conventional score-generative models based on Brownian motion.\n\n2. This paper offers a solution to the limitations of standard Brownian motion by extending the framework to fractional Brownian motion, which better captures long-term dependencies and complexities in real-world data.\n\n3. This paper provides a clear and detailed explanation of the proposed framework and its mathematical foundations."
            },
            "weaknesses": {
                "value": "1. The experiment lacked sufficient data, and the basis for asserting the presence of long-term dependency was inadequate.\n\n2. An approximation was applied in deriving the theory, and there is no analysis regarding the errors introduced by this approximation."
            },
            "questions": {
                "value": "1. Why is it believed that Brownian motion cannot capture long-term dependencies, while fBM can?\n\n2. Why does fBM encompass non-Markovian dynamics?\n\n3. Is it necessary to perform the Markov approximation for fBM? What is the extent of error introduced by this, and does it significantly impact accuracy? Did the use of Markov representation not compromise the ability to capture long-term dependencies?\n\n4. Why was latent video generation chosen for experimentation, and why were experiments not conducted on datasets other than MNIST?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission972/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission972/Reviewer_3XSn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761152328,
        "cdate": 1698761152328,
        "tmdate": 1699636022946,
        "mdate": 1699636022946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hNIx9DQVPv",
        "forum": "rtx8B94JMS",
        "replyto": "rtx8B94JMS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission972/Reviewer_J9Kf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission972/Reviewer_J9Kf"
        ],
        "content": {
            "summary": {
                "value": "A variational framework for learning latent SDEs driven by fractional Brownian motion (fBM) is introduced. To efficiently infer the variational parameters, an approximation of fBM by Markov processes is utilized, which reduces the problem to inferring parameters of SDEs driven by (non-fractional) Brownian motion.\nIn applications, the drift, diffusion and a control term are implemented as neural networks. Empirically, the methods capability to recover a fractional Ornstein-Uhlenbeck bridge and the Hurst index are evaluated. The method is also applied to a video modelling task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work addresses a difficult problem and proposes a novel solution. The inference of latent SDEs driven by Brownian motion is already challenging, in this work the setting is extended to fractional Brownian motion while avoiding limitations of prior work (Tong et al, 2022)\n- The background section of the paper is well written and does a good job in condensing the involved theory of stochastic calculus (for fBM) into a short conference paper.\n- While the method is evaluated on video prediction, it appears to be applicable to any type of sequential data."
            },
            "weaknesses": {
                "value": "From my point of view, there are 2 main weaknesses in this submission (for details, see below).\n \n 1. The method and the experiments are insufficiently described, and I have some questions in this regard. However, I am convinced, that the manuscript can be updated to be much more clear.\n \n 2. The empirical evaluation is of limited scope. Qualitatively, the method is evaluated on 2 toy problems (fOU & Hurst index); quantitatively on a single synthetic dataset (stochastic moving MNIST). For the latter, only two baseline models from 2018 and 2020 are compared to. In consequence, the usefulness of the method is not established. On the plus side, there are 3 ablations / further studies.\n \nMinor weaknesses.\n- The method appears to be inefficient, with a training time of 39 hours on an NVIDIA GeForce RTX 4090 for one model per model trained on stochastic moving MNIST. \n\n- A detailed comparison with Tong et al. (2022) (who also learn approximations to fBM) is missing. So far, it is only stated that Tong et al. did not apply their model to video data and that it is completely different. A clear illustration of the conceptual differences and a comparison of the pros and cons of each approach would be appreciated (summary in main part + mathematical details in supplementary material).\n\n# Summary \nFor me, this is a borderline submission. On the one hand, the proposed method is novel, significant and of theoretical interest. On the other hand, there are clarity issues, a weak empirical evaluation and no clear use case. Expecting the clarity issues to be resolved, I rate the submission as a marginally above the acceptance threshold, as the theoretical strengths outweigh the empirical flaws.\n\n----\n# Details on major weaknesses\n\n\n## Point 1 (clarity). \n\n**Regarding the method.** After reading the method and experiment section multiple times, I still have no idea, how to implement it. I am aware of the provided source code, nevertheless I found the paper to be insufficient in this regard.  \n\nWhat I got from section E and Figure 4 is that:\n- First, there is an encoding step that returns $h$, a sequence of vectors/matrices over time. Somehow, these vectors are used to compute $\\omega$. I have no idea how this $\\omega$ is related to the optimal one from Prop. 5.\n- $h$ is given to a temporal convolution layer that returns $g$ and this $g$ has as many 'frames' as the input and is used as input to the control function $u$.\n- The control, drift and the diffusion function are implemented as neural networks. \n- An SDE solution is numerically approximated with a Stratonovich\u2013Milstein solver.\n- $\\omega$ is used in the decoding step, I do not understand why and how.\n- Where do the approximation processes $Y$ enter. How are they parametrized, is $\\gamma$ as in Prop 5? Are they integrated separately from $X$?\n- The ELBO contains an expectation over sample paths. How many paths are sampled to estimate the mean?\n- Fig. 6: Why do the samples from the prior always show a 4 and a 7? Does the prior depend on the observations?\n\n**Regarding Moving MNIST.**\nWhat precisely is the task / evaluation protocol in the experiment on the stoch. moving MNIST dataset? I did not see it specified, but from the overall description it appears that a sequence of 25 frames is given to the model and the task is to return the same 25 frames again (with the goal of learning a generative model).\n\n## Point 2 (empirical evaluation)\n- The method is not evaluated on real world data.\n- Quantitatively, the method is only evaluated on one synthetic dataset (stoch. moving MNIST).\n- While the method is motivated by \"*Unfortunately, for many practical scenarios, BM falls short of capturing the full complexity and richness of the observed real data, which often contains long-range dependencies, rare events, and intricate temporal structures that cannot be faithfully represented by a Markovian process\"*, the moving MNIST dataset is not of this kind. It is not long range (only 25 frames) and there is no correlated noise.\n- The method is only compared to 2 baselines (SVG, SLRVP) on moving MNIST.\n- Table 1 does not show standard deviations.\n\nOverall, this would be a far stronger submission, if the experiments were more extensive. This includes: \n- Evaluation on more task and datasets, and specifically on datasets were this method is expected to shine, i.e., in the presence of correlated noise. The pendulum dataset of [Becker et al., Factorized inference in high-dimensional deep feature space, ICML 2019] would be one example.\n- Comparison with more baselines. In particular, more recent / state-of-the-art methods that do not model a differential equation and the fBM model by Tong et al.\n\n\n----\n\nThere is a typo in Proposition 1: \"Markov rocesses\""
            },
            "questions": {
                "value": "**Extension to different tasks**\n\nIt seems that in its current form the method can only be applied when the model output should match the model input. This is because the control function $u$ at each time point depends on the evidence at that time point. How would you extend the method to forecasting or interpolation tasks, where this is not possible?\n\n---\n\n**Proposition 5**\n\nIf I understand correctly, then $\\hat B$ depends on $\\omega$ and $\\gamma$ and the goal is to find $\\omega$ such that the L2 error of the approximation is minimal. The minimum is achieved when $\\omega$ solves $A\\omega=b$.  \nDoes this equation have a solution? How large is the approximation error? How is $\\omega$ computed; is it required to first compute the entire matrix $A$?\n\n---\n\n**Runtime**\n\nI wonder how the runtime compares to Tong et al., and to a latent ODE.   \nDoes K=0 in Fig. 9 correspond to a latent SDE with non-fractional Brownian motion?  \n\nIs Fig.9 qualitatively the same, if training time (forward+backward) is measured instead of inference time (only forward)?  \n\nIn the \"Impact of K and the #parameters on inference time.\" experiment, it is stated that *\"the run-time is still dominated by the size of the neural networks\"*. Presumably, this refers to the networks that implement the drift, diffusion and control and not the encoder / decoder networks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698932287857,
        "cdate": 1698932287857,
        "tmdate": 1699636022869,
        "mdate": 1699636022869,
        "license": "CC BY 4.0",
        "version": 2
    }
]