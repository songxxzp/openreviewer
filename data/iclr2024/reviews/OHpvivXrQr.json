[
    {
        "id": "cNPmw7OuPM",
        "forum": "OHpvivXrQr",
        "replyto": "OHpvivXrQr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce an interesting task of multimer structure prediction in the form of assembly graph where each node is a monomer and each edge represents an assembly action. They propose to first pretrain a model to predict the TM-score between the structure obtained from the given assembly graph and the ground truth, then finetune with prompt and meta-learning to perform link prediction to construct the assembly graph step by step. The prompt is crafted with $l=3$ path to form a 4-node graph so that the link prediction can be implemented as graph-level prediction on small graphs which is well aligned with the pretraining phase."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written and clear. I really enjoy reading the paper.\n2. The paper introduces an interesting task (i.e. multimer structure prediction) to the community with clear formalization (i.e. prediction of the assembly graph given pairwise dimers).\n3. The experiments are solid, testing the performance on multimers ranging from 3 chains to 30 chains. The authors also compare the results when given ground-truth dimers or alphafold-predicted dimers as inputs. The results are promising, exhibiting obvious improvement over baselines."
            },
            "weaknesses": {
                "value": "1. The $l=3$ graph prompt is proposed to tackle the distribution shift of chain numbers. However, I notice that in section 4.2 the initial embeddings are obtained from the last layer of the pretrained GIN encoder with the full assembly graph as input. This step may already suffer from the distribution shift and produces out-of-distribution embeddings.\n2. The ablation of the pretraining phase is missing. An experiment without pretraining should be conducted to demonstrate the necessity of the proposed pretraining strategy."
            },
            "questions": {
                "value": "1. Can you show the correlation between the number of chains and the node degrees to directly validate the claim \"multimers with more chains are more likely to yield assembly graphs with high degrees\" in section 3.3?\n2. How is the ablation of the C-PPI modelling strategy implemented?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_y71p"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697599315690,
        "cdate": 1697599315690,
        "tmdate": 1699636164031,
        "mdate": 1699636164031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sydWKlDPJr",
        "forum": "OHpvivXrQr",
        "replyto": "OHpvivXrQr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
        ],
        "content": {
            "summary": {
                "value": "The paper treats the problem of multimer assembly: given a set of sequences, and the structure of all possible dimers (e.g. from AF2), we wish to assemble the multimer by iteratively selecting the next chain and aligning dimer structures\u2014represented with an assembly graph. The paper proposes a multi-stage solution to this problem. (1) A GNN is pre-trained to predict the multimer TMScore from an assembly graph (2) The \u201cnext link\u201d prediction problem is  framed as a TMScore prediction over a fictitious assembly graph, i.e., akin to \u201cprompting\u201d the pretrained GNN. This fictitious assembly graph is created by a \u201cprompting model\u201d and its design is inspired by network-based PPI prediction in bioinformatics. (3) The prompting model\u2013which is specific for each multimer size\u2013is obtained via meta-learning, where the meta-training tasks are small multimer sizes, and the meta-tuning tasks are large multimer sizes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper proposes a novel solution to the difficult problem of multimer structure prediction. Multiple strategies are employed to make this extremely data-scarce problem tractable for deep learning. These strategies are impressive in their sophistication and the bar for originality / novelty has clearly been surpassed.\n* The experimental results are good in terms of both performance and runtime relative to the best existing methods.\n* The paper is a nice illustration of the concept of learning on top of foundation models such as AF2, a paradigm which arose in NLP and is becoming increasingly useful in biological ML."
            },
            "weaknesses": {
                "value": "* The paper integrates multiple technical ideas with a complex problem domain, but unfortunately the presentation is very confusing.\n   * The paper relies on many ideas that are less familiar to the average reader in protein ML. There should be an extensive background section explaining meta-learning, prompt learning, L=3 PPI prediction, etc.\n    * For a procedure with this many moving parts, it is absolutely essential to provide an explicit inference algorithm somewhere.\n    * The paper is made even more confusing by certain particular choices of emphasis which serves only to distract the reader on a first pass.\n        * It is not clear why it is important to emphasize C-PPI vs I-PPI. Perhaps the authors are trying to draw a distinction with MCTS, but this is really not necessary or within scope. Fully appreciating the difference would require a detailed explanation of the MCTS method, which the paper has no time (or need) to fully explain.\n       * The extended discussion in Section 3.3 seems disconnected from the context of the paper and serves only to make it more confusing.\n       * The authors repeatedly distinguish between oligomers and multimers based on size, which is very unconventional and should be fixed.\n    * L=3 PPI prediction is not obvious and is very confusing when referred to in-passing the first few times it is brought up. \n\n* The pipeline seems unnecessarily complicated and poorly justified. All else held equal, solutions to hard problems should be as simple as possible, and complexity (even if novel) should at least be sensible and easy to justify once understood. Here, it is really not clear why the problem requires such a complex formulation. The so-called source task is a nice way of framing the multimer assembly problem to make it much more data-rich. But then, the most natural solution would seem to be to run the TMScore predictor on all possible next-link additions to the current assembly graph. It seems quite convoluted to instead obtain a prompting model to convert each possible next-link prediction to a fictitious 4-graph when a real (N+1)-graph would also seem to work.\n\nJustification for score. Although I like the wealth of ideas presented in the paper, the presentation is too unclear and the complexity insufficiently justified to recommend acceptance in its current form."
            },
            "questions": {
                "value": "* Can the authors confirm that there is only one pretrained model, despite the discussion in section 3.3?\n* Where are the node embeddings H is used in prompt model? Are only $H_u, H_d$ used?\n* Is there precedent for learning a prompt _model_ that generates a different prompt for each input, as opposed to simply learning a _prompt_?\n* Are the encoder parameters $\\theta$ and task head parameters $\\phi$ ever separated? If not, then denoting them separately only makes the paper more confusing.\n* How is runtime calculated? I assume the dimer structures are completed \"lazily.\" What explains the large gap in runtime relative to MCTS? It would be nice to report the total number of dimer structures \"required\" by MCTS vs the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2316/Reviewer_UpSS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698184716977,
        "cdate": 1698184716977,
        "tmdate": 1699636163929,
        "mdate": 1699636163929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AKQldrUZUQ",
        "forum": "OHpvivXrQr",
        "replyto": "OHpvivXrQr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_6yks"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a sequential protein complex assembly method called PromptMSP. In each assembly step, PromptMSP predicts where a protein should be assembled to the current complex. During training, PromptMSP learns a continuous score for a given protein assembly graph and during testing, it uses the learned score model to find the most likely assembly graph. To avoid training and testing distributional mismatch, PromptMSP employs prompt learning to reduce the gap of input formats. PromptMSP is compared with existing multimer prediction baselines and outperforms AlphaFold-multimer baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed method outperforms AlphaFold-Multimer (AFM), which is impressive.\n* The evaluation setting is comprehensive. It includes both ground truth dimer setting and predicted dimer setting, which ensures a fair comparison with AFM.\n* Ablation studies show that each proposed component is effective.\n* Incorporation of L=3 PPI rule into the inference procedure is an interesting contribution."
            },
            "weaknesses": {
                "value": "* The method description is very confusing. Figure 5 is very crowded and rather uninformative.\n* It is very hard to understand what meta-learning part (section 4.3) is actually doing. A visual step-by-step illustration of prompt fine-tuning can be helpful.\n* The introduction of prompt fine-tuning seems an overkill. A simpler approach should work equally well. For example, we can adopt a standard autoregressive link prediction algorithm to this problem. In each step, you predict the link between a pair of proteins and train the model to predict the right link given different prefix graphs.\n* Analysis in section 3.3 is unclear. How did you compute Centered Kernel Alignment between two models?\n* It's unclear how a new protein is docked to the current assembly in each step. Did you use EquiDock? If so, how do you ensure that EquiDock is not trained on any of your test set instances?"
            },
            "questions": {
                "value": "* At test time, what prompt do you provide to the model? It seems that the prompt is basically the assembly graph that model predicted. I don't see why prompt engineering is useful during training.\n* It would be helpful to report model performance for each number of chain (from 3 to 30)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791868932,
        "cdate": 1698791868932,
        "tmdate": 1699636163828,
        "mdate": 1699636163828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IDjhIDh9CG",
        "forum": "OHpvivXrQr",
        "replyto": "OHpvivXrQr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_PqHe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2316/Reviewer_PqHe"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new algorithm to predict multimer structure with multiple chains via a pre-training and prompt tuning framework. The overall idea is novel and interesting. Different from MoLPC, where proteins docking are independent without the consideration of other protein, this method considers the influence of third-party proteins when performing docking. This paper compared several baselines on N chains datasets (N>=3). The experimental results show improvement on AlphaFold-Multimer and MoLPC. Although this paper introduces some new idea, many details are unclear. Also, the baselines are so weak and the experimental setting is not realistic. \nI vote to reject this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Solving multimer structure prediction via pre-training and prompt tuning is interesting.\n\n- It's reasonable to consider conditional docking for multiple protein."
            },
            "weaknesses": {
                "value": "- defintion 1 is problematic. Because in real-world setting for docking, monomer's ground-truth structures could not be provided. So that the correctness could never be 1 in real-world setting.\n\n- the baselines are so weak. when taking ground-truth structure as input, HDock[2] and xTrimoDock[3] are strong baselines. \n\n- it could be interesting if you can compare different baselines over different the number of chains. The performance could be reduced when increasing the number of chains. \n\n- missing some related references: [1], [3], [4] [5]\n\n[1] Ghani, Usman, et.al. Improved docking of protein models by a combination of alphafold2 and cluspro.\n\n[2] Yumeng Yan, et.al. The HDOCK server for integrated protein\u2013protein docking.\n\n[3] Yujie Luo, et.al. xTrimoDock: Rigid Protein Docking via Cross-Modal Representation Learning and Spectral Algorithm.\n\n[4] Mohamed Amine Ketata et.al. DIFFDOCK-PP: RIGID PROTEIN-PROTEIN DOCKING WITH DIFFUSION MODELS. use torsional diffusions to solve rigid protein docking, and the source code is released. \n\n[5] Lee-Shin Chu et.al. Flexible Protein-Protein Docking with a Multi-Track Iterative Transformer."
            },
            "questions": {
                "value": "- when comparing with AlphaFold-Multimer, do you input monomer's ground-truth structure as the template?\n\n- how does your method perform when using predicted monomer structure? is the method robust?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2316/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699534831634,
        "cdate": 1699534831634,
        "tmdate": 1699636163758,
        "mdate": 1699636163758,
        "license": "CC BY 4.0",
        "version": 2
    }
]