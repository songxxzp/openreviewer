[
    {
        "id": "FAgUIzTQOy",
        "forum": "gbrHZq07mq",
        "replyto": "gbrHZq07mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_yxZd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_yxZd"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically analyses the formal languages that transformer encoders can recognize. \nThe paper analyses two classes of transformers depending on the attention mechanism. The first class is \nunique hard attention transformers (UHAT), and the second class is average hard attention transformers (AHAT). \nIt is known that UHAT encoders can only recognize languages inside the circuit complexity class $AC^0$. AHAT\ncan recognize languages outside $AC^0$, but its expressive power still lies in the circuit complexity class $TC^0$.\n\nThis paper gives new theoretical results on the topic. The main findings of the paper are:\n- There is a language in $AC^0$ that UHAT cannot recognize.\n- UHAT can recognize all languages definable in first-order logic with arbitrary unary numerical predicates. The class includes all regular languages in $AC^0$.\n- AHAT can recognize all languages definable by LTL(C, +), which is an extension of the logical language acceptable by UHAT with counting terms."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Important topic, impressive results:**\n Since the transformer is one of the most important neural architectures, understanding its expressiveness in a formal way seems an important topic. This paper brings progress on this topic by relating the expressive power of transformers with logical languages.\nThis connection results in identifying some important language classes that can be acceptable by UHAT, e.g.,  all regular languages in $AC^0$.\n\n**A clearly written paper:** The paper is very clearly written. I feel no difficulty in reading the paper. \nThe background needed to understand the contribution is concisely explained in the paper. \nProofs of important theorems are shown in the main body of the paper, and they are easy to follow.\n\n**New approaches for proofs.**\nThe paper uses the relationship between first-order logic and linear-time logic to prove the main results. This technique seems not\nused in the previous papers analyzing the expressive powers of transformer encoders."
            },
            "weaknesses": {
                "value": "Currently, I have no clear reason to reject the paper."
            },
            "questions": {
                "value": "On p.6, the paper says, \"Observe that $(\\bar{w}, i) \\models \\phi U \\psi $ if and only if $(\\bar{w}, j_i) \\models \\psi$.\" I wonder what happens if there exists $i \\leq j^\\prime < j_i$ such that $(\\bar{w}, j^\\prime) \\models \\psi$\"? Following the definition on page 5, I think $(\\bar{w}, i) \\models \\phi U \\psi$ holds if there exists such $j^\\prime$.\n\n\n**Minor comments:**\n- **Abstract:** outside AC^0) -> AC^0 ?\n- **page2, before related work:** it have been shown before that parity can be accepted by an AHAT -> it has not been shown before that parity can be accepted by an AHAT?\n- **p.5, first paragraph:** at at least 2n/3 -> at least 2n/3\n- **Appendix, proof of lemma1:** max{0, x_i + i - (n+1)} -> max{0, x_i + 1 - (n-1)}?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Reviewer_yxZd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7474/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633112756,
        "cdate": 1698633112756,
        "tmdate": 1699636901861,
        "mdate": 1699636901861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sJ9CfhkWyt",
        "forum": "gbrHZq07mq",
        "replyto": "gbrHZq07mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_auhY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_auhY"
        ],
        "content": {
            "summary": {
                "value": "The papers is a contribution to the exciting and challenging domain of characterizing expressivity of transformers. The authors focus on the  Unique Hard Attention Transformers (UHAT). Previous results have shown that $UHAT \\subseteq AC^{0}$, where $AC^{0}$ is the circuit complexity class of circuits with constant depth, with unlimited fan-in  $AND$, and $OR$ gates. The author's first result is to show that  $UHAT \\subset AC^{0}$. The key contribution of the paper is that First Order Logic on words with unary numerical predicates (i.e. unary boolean functions on positions of the symbols in a word, and the length of the word) is readable by UHAT. They denote this fragment with FO(Mon), the proof strategy used by authors relies on a classic result in logic known as the Kamp's theorem, which says that FO(Mon) is equivalent to another type of logic, known as the Linear Temporal Logic (Mon) i.e. the extension of LTL with unary numerical predicates. The authors then design vector encodings and positional encodings that allow evaluation of any LTL (Mon) formula using a UHAT. Finally, they show the applications of their results by comparing them to other types of formal languages."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Formal properties of transformers are not very well-understood, and analyzing them w.r.t. the formal languages they accept is a very exciting and challenging direction. The author's provide significant contributions in this direction, and the paper contributes many new results and ideas that can contribute to theoretical investigation of transformers. Writing is quite clear, the proof though hard, seems to consist of clear arguments (I mention my confusions in the questions)."
            },
            "weaknesses": {
                "value": "The proof on Page 6 and Page 7 could be further clarified. Although, the structural induction arguments are clear, but I am not sure how this is consistent with the meaning of accepting a word as part of the language --- which authors define earlier (See questions)"
            },
            "questions": {
                "value": "- In my understanding, in section 2.2 paragraph 2, it is unclear to me why you set $T(\\bar{w})$ to $\\langle \\mathbf{t},\\mathbf{v}_{0}\\rangle$. \n- [Minor Comment] Page 6, \"reverses the third coordinate\" is not a very precise statement. \n- When is the notion of $T(\\bar{w}) > 0$ (as introduced in section 2.2), used as the criterion in proof on page 6 and page 7. From this proof, I just see that you can perform LTL operations on input strings, but I am not sure how this shows that a string in the language will never be mapped to a string outside the language?\n- Does Kamp's theorem give any bounds on the length of equivalent FO(Mon) equivalent formula in LTL(Mon)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Reviewer_auhY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7474/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689293832,
        "cdate": 1698689293832,
        "tmdate": 1699636901738,
        "mdate": 1699636901738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5eUXqQmF27",
        "forum": "gbrHZq07mq",
        "replyto": "gbrHZq07mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_maoo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_maoo"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the expressiveness of Transformer encoders by establishing their relation with a class of formal languages called circuits. Based on prior work, the main theoretical claims of the paper are:\n\nPrevious works have demonstrated that UHAT transformers (similar to Transformers with hard attention) cannot recognize languages beyond AC^0 (informally: circuits of polynomial size and constant depth). This paper further finds an example problem class within AC^0 that cannot be recognized by UHAT, which demonstrates that AC^0 is not a \u201clower bound\u201d of UHAT.\n\nThe paper further establishes a class of problems that can be recognized by UHAT.\n\nResults are then extended to AHAT (Transformers with averaged head attention)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Justifying the expressiveness of Transformers through the length of formal language is a very important topic and could lead to better understanding of the working mechanisms of Transformers. This paper strengthens prior theoretical results and better bounds the expressivness of UHAT and AHAT."
            },
            "weaknesses": {
                "value": "Although the theoretical results themselves sounds interesting, I found some definitions and assumptions are not approprately stated, which potentially leads to incorrect results. While it is possible that the theoretical results still hold after fixing all the problems, I think the paper needs a major revision to ensure its validity.\n\nMissing important restrictions when defining the transformer model. \n\n- Precision of the number processed by the transformer. The paper does not include any restriction on the precision of the numbers processed by the transformer. This could make the model unrealistically expressive as discussed in many related work (e.g., proving Turing completeness of RNNs require relaxations on the numerical precision). In related works, a realistic assumption could be log-precision transformers, i.e., the number of floating/fixed-point bits scale logarithmically with the sequence length.\n\n- No assumptions have been made about the number of transformer layers. Prior work usually assume constant depth or logarithm depth (w.r.t. sequence length). Related to this assumption, it seems that the proof of Proposition 2 constructs a Transformer whose number of layers depends on the form of input LTL. This makes it particularly important to make the correct assumption.\n\n- Structure of the model. The model does not include residual connections and only uses single-head attention. Also the ReLU layer only applies ReLU to a single element of the input vector. Although these might be able to adapt in the proof, it would still be nice to make these assumptions as practical as possible.\n\nMany related works are missing. The paper states that \u201cthere has been very little research on identifying logical languages that can be accepted by transformers\u201d. However, with a quick google scholar search, I found the following highly-related papers not cited in the paper. It would be nice to discuss the relation of this paper\u2019s results with these prior works.\n\n[1] Merrill, William, Ashish Sabharwal, and Noah A. Smith. \"Saturated transformers are constant-depth threshold circuits.\"\u00a0Transactions of the Association for Computational Linguistics\u00a010 (2022): 843-856.\n\n[2] Merrill, William, and Ashish Sabharwal. \"The parallelism tradeoff: Limitations of log-precision transformers.\"\u00a0Transactions of the Association for Computational Linguistics\u00a011 (2023): 531-545.\n\n[3] Strobl, Lena. \"Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits.\"\u00a0arXiv preprint arXiv:2308.03212\u00a0(2023)."
            },
            "questions": {
                "value": "Some important assumptions seem to be missing.\n\nIt would be nice to discuss the relation between this paper and the missed related work mentioned in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7474/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776649939,
        "cdate": 1698776649939,
        "tmdate": 1699636901611,
        "mdate": 1699636901611,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AKDtHrK1lL",
        "forum": "gbrHZq07mq",
        "replyto": "gbrHZq07mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_wdZx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_wdZx"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates whether circuit language can be accepted by existing transformer encoders with hard attention. Specifically, this work concludes that UHATs cannot accept all languages in $AC^0$, but they can still accept all languages in a \u2019monadic\u2019 version. Besides, \nthis work finds out that AHATs, other transformer encoders, can express any language definable in a powerful counting logic. Moreover, this work provides sufficient theoretical justifications to support their findings and conclusions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study about whether circuit language can be accepted by existing transformer encoders is interesting and impressive. \n2. The paper offers comprehensive theoretical justification to demonstrate and validate their findings."
            },
            "weaknesses": {
                "value": "1. The structure of this paper is very messy, which is very hard to follow. Let me take the Section Introduction as an instance:\n\n1.a In Section 1 Introduction, the paper claims that \"the expressive power of transformer encoders\nhas not been fully elucidated to date.\". I am curious about that. What do you mean they are not fully elucidated?\n\n1.b I am very confused about the challenges of studying the circuit language. I could not find any information to discuss the existing challenges and related works, which makes it hard to understand the motivation for this work.\n\n1.c What are your contributions to this work? I could not find any conclusions about contributions after reading this section, or even the whole manuscript.\n\n\n2. The paper has a very weak introduction to related works, making this work hard to understand and compare.\n\n3. I would suggest that an illustration figure be provided to clearly show the main idea of this work."
            },
            "questions": {
                "value": "Please refer to the weakness of the questions that I proposed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7474/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819657512,
        "cdate": 1698819657512,
        "tmdate": 1699636901477,
        "mdate": 1699636901477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4gymZUSaOq",
        "forum": "gbrHZq07mq",
        "replyto": "gbrHZq07mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_ceMz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7474/Reviewer_ceMz"
        ],
        "content": {
            "summary": {
                "value": "Review\n======\n\nAs a non-IA specialist but a circuit-complexity and automata specialist,\nI found those connection amusing but slightly artificial.\nI will not express myself on the pertinence of analizing the expressivity\nof those models from IA-perspective but provides a bit of insight about\nthe automata/circuit complexity side.\n\nFirst I found some claim dubious:\n\nIn a footnote you claim that only rational numbers where used in Hao (2022)\nbut generalize it real numbers.  It is known in complexity theory  that going from rational to real number is always complicated. Even\ncalculability theory can become weird when considering real numbers.\n\nWhile it is rather clear that computation performed by UHAT can be computed\nin AC0 when restricted to rational numbers, it is much less clear when going\nwithin rational numbers. If it holds, it by the sake of some continuity and\napproximation by real numbers arguments, but it deserves some details...\nI believe it should be explained.\n\nBecause I don't really get that, I have assumed that the remaining where\ndone on rational numbers;\n\nAbout the question about \"what fragment of AC0, UHAT can belongs to\", since it\ncontains all regular languages in AC0, it is improbable that you can find\na sound answer for that since it is already wide open for regular languages.\nIndeed:\n- regular language are complete for each level of the depth hierarchy AC0, \n- regular language are in quasilinear AC0 and proving that they are linear AC0\nis a long standing open problems (see the survey of Koucky on the topic).\n\nAbout Proposition 1. I believe a much simpler argument can be used using\na simple padding argument: \n\n>AC^0 recognized all languages up to exponential padding. \n>That is, Pad(L) = { u #^{2^|u|} \\mid u in L } is always in AC^0 whatever is L. \n>Your UHAT shouldn't be able to capture all Pad(L) as it only can remember a small\n>piece of information and convolute it. Basic information theory/pigeon hole might\n>help to conclude without a hammer of sensitivy of circuits within AC0.\n\nAbout proposition 5: I don't really understand why the Parikh\nclosure/permutation closure of languages is meaningful here. Sounds like an\narbitrary property to me.\n\n\nLogic with counting operators has been introduced in the past (Majority logic)\nand a study of its expressivity with respect to circuit classes.\nSee for instance (https://link.springer.com/chapter/10.1007/978-3-642-02737-6_7)\nIs there a connection?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper contribute to a fun connection between formal language theory in order to analyze the expressivity of transformers. This line of research sounds more like research performed in TCS than in IA tracks but since it is apply to IA-defined model it makes some sense. Understanding the expressivity of those model might be enlightening to people actually playing with them."
            },
            "weaknesses": {
                "value": "Some of the contribution sounds really artificial to me. In particular, why should we study the commutative closure of language defined by transformers? While it makes sense in TCS, I fail to grasp the importance in this context."
            },
            "questions": {
                "value": "Comments \n========\n- The last sentence of the second paragraph of section 2.2 makes no sense.\nf is a function from Sig -> R^d, T: Sig^+ -> R et the last sentence\nsays that T get an input sequence that type as a sequence of vectors of R\"e.\nThe value of T(w) = (t, v0) only depending of t and v0 which is plain weird\nas it gives the impression it depends of a constant (t) and v0 which is f(a_0) + p(0, n).\nThe whole paragraph is thus buggy.\n\n- You can have some feelings on what is going on with logic extended with monadic\npredicates through this paper.\n\nhttps://dl.acm.org/doi/10.1145/3091124\n\n- About your open question: \nAdditionally, does there exist a language in the circuit complexity class TC0, the\nextension of AC0 with majority gates, that cannot be recognized by AHATs\n\nI would go for Dyck language. Sounds hard for a AHATs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7474/Reviewer_ceMz",
                    "ICLR.cc/2024/Conference/Submission7474/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7474/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699457839557,
        "cdate": 1699457839557,
        "tmdate": 1699943413917,
        "mdate": 1699943413917,
        "license": "CC BY 4.0",
        "version": 2
    }
]