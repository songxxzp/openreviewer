[
    {
        "id": "gWakadDjSk",
        "forum": "d3UGSRLbPo",
        "replyto": "d3UGSRLbPo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_XFUz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_XFUz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a multi-step procedure to reduce hallucinations in the output of LLMs.\nThe procedure first identifies key concepts in a sentence, filters them to uncertain ones using the model output logits, retrieves information using web search, validates the concepts by prompting, and finally corrects the output sentence by prompting with the retrieved context as support. The main experiments are conducted on article generation using a closed dataset (promised to be released after publication) using GPT-3.5 and Vicuna 1.5."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The multi-step approach presented is generally sound. The approach can use black-box models hidden behind an API, and several possible solutions for each individual step are presented and evaluated to some extent.\n- The experiments are primarily based on GPT-3.5, but there are also experiments with Vicuna-1.5 to validate the results. In addition, the use of an open model supports easier reproduction and improves the overall accessibility of the presented methodology.\n- Hallucinations are a relevant problem with current LLMs and are a limitation to their general applicability."
            },
            "weaknesses": {
                "value": "- The proposed multi-step approach is likely to increase generation latency significantly. While this is noted superficially, and an improvement for one of the many steps is roughly sketched out, an in-depth discussion is missing - in particular, there are no experiments or theoretical discussions about the overall latency. I am not of the opinion that high latency is a problem for all use cases, but it would be important to have a proper discussion about this limitation and where it is a problem.\n- The overall experimental design is not described in sufficient detail. In particular, it is not clear how the data used for Section 3.1 relate to those used for Sections 3.2 and 3.3. If they were the same data, I would be concerned about the reliability of the results in the later sections, since the hyperparameters of each step, such as the aggregation method used to obtain concept uncertainty, are chosen to maximize the metrics in a data set.\n- It is not clear to what extent retrieval alone explains the reduction in hallucinations. Given that the proposed method uses (multiple) web search queries, a natural baseline would be to consider the article generation task based on retrieved facts about the article topic, which would have some favorable properties (e.g., lower latency, less technical complexity) compared to the proposed multi-step approach. A proper ablation/evaluation against this baseline could help to delineate this effect.\n- Some of the design decisions seem to be taken quite ad-hoc; for instance, the choice of a method for key concept identification seems to be based on qualitatively looking at a few examples (Table 4, Section B.1)"
            },
            "questions": {
                "value": "- Table 1: How can sentence-level recall (85.96) be smaller than concept-level recall (87.68) if a sentence is considered hallucinated as soon as a single concept is hallucinated?\n- Section 3 describes the data selection process. In particular, the topics for article generation are selected based on the longest articles in WikiBio or Wikipedia. I would expect this selection strategy to select topics with high prevalence in most LLMs training data: either because they train directly on Wikipedia, but also because long Wikipedia articles are likely to be about a topic of general interest with high coverage in web data as well. How does this affect hallucination, or in other words, how representative are the results for hallucination detection and mitigation based on these topics?\n- Regarding labeling hallucinations, how do you handle sentences that are correct given the content of previously hallucinated sentences?\n- Your methodology works on the unit of a sentence. The (initial) output of the model would be a paragraph/full text. How do you segment it into sentences? What is your motivation for the sentence unit? Since you are processing the key concepts sequentially, do you need the sentence separation? Or could the approach work directly on the whole paragraph?\n- Since the uncertainty calculation serves as a filter on the concepts sent for verification, I wonder about the relative importance of precision and recall. Intuitively, I would expect this to be a recall-oriented scenario, but the decision seems to be based on the area under the ROC.\n- Section 2.1.5.\n  > However, running this in parallel would require starting multiple threads which may not be supported by all machines.\n\n  shouldn't this be easily solved by batching the requests?\n- Section 3.1.1.: Choosing more descriptive names than `A`, `B`, `C`, and `D`, such as `YY`, `YN`, `NY`,  and`NN`, or directly using the conditional probability notation $p(H | H)$, $p(H | \\neg H)$, ... would greatly improve the readability of the plots and discussion.\n- Figure 5: A bar chart would be more appropriate here; I would also be interested in the confidence/dependence on the selected sentences. One way to study this would be to use bootstrapping.\n- In general, I disliked the overuse of bold type, as it reduced readability quite a bit. The same goes for the use of free-floating figures. An extreme example is page 6.\n- Table 1: Accuracy seems to be in $[0, 1]$, while precision and recall are given in $[0, 100]$ (i.e., in percent).\n- Section 3.2, \"Mitigation\" + Table 2: The numbers in the text do not seem to match those in the table.\n- For Section 4.2 / QA, it is not clear how the multi-step approach works at all? I would guess that the answer is always a single sentence (or even a sentence fragment), so how is the iterative sentence-level method applied?\n- Section 4.3\n  > Importantly, we also show that our approach does not incorrectly modify a true premise question.\n  \n  where is this shown?\n\n- Appendix A: Related Work; It would be nice if the ones listed under \"concurrent work\" were part of the main paper description, as they seem to be the most related.\n- B.3.2: \n > Our preferred technique for retrieving knowledge is web search because the web is more likely to\ncontain the updated knowledge in comparison to a knowledge corpus whose information can become\nstale, outdated, and obsolete.\n\n  If (one of) the main reasons for hallucinations is outdated knowledge, wouldn't we notice that the model uncertainty does not reflect this properly, i.e. the model is very certain about its outdated knowledge?\n\n- G.1: The sentence-level baseline uses the minimum probability over all tokens; I think it would make more sense to consider the other aggregations as well.\n\n#### Minor Remarks\n- Section 2, first paragraph: there is an inconsistent use of \"Section\" vs. \"section\"\n- Section 2, first paragraph, line 3: typo: \"shwon\" should be \"shown\"\n- Section 2.1.2; \"normalized product of probabilities\" seems to be equivalent to the geometric mean of probabilities; the latter may be the preferred term for some, so it would be nice to make this more clear (e.g., footnote, rename, ...)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Reviewer_XFUz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4279/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698492176310,
        "cdate": 1698492176310,
        "tmdate": 1699636395564,
        "mdate": 1699636395564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mcbg2Brtfa",
        "forum": "d3UGSRLbPo",
        "replyto": "d3UGSRLbPo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_EApd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_EApd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for detecting and revising hallucinations of LLMs. The proposed method takes a generated sentence as input, extracts key concepts (text spans) and identify low-confidence ones, generates a question for each span, retrieves knowledge, and revise the sentence. This is applied every time when the LLM finishes generating a sentence, so that future generations is conditioned on revised, more factual sentences. This is motived by an \"propagation of hallucination\" analysis , which shows that if the previous sentence is hallucinated, it is likely the next sentence generated by an LLM will also contain hallucination. \n\nFor each component (key phrase extraction, confidence estimation, query generation, retrieval), the authors empirically compared a couple variations, which suggests using prompted LLMs for all tasks, and web search for retrieval. End-to-end evaluation is done by prompting GPT 3.5 or Vicuna-13B for long article generation and manually evaluating factuality. The proposed method greatly reduces hallucination.  Additional experiments show that the proposed method can improve multi-hop QA tasks as well as identifying false-premise questions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is clearly described.\n- The \"propagation of hallucination\" analysis very nicely show the necessity of actively reducing hallucination from the generation. Although  sentence-by-sentence actively doing retrieval and rewrite has been explored in prior work, there's little quantitive analysis studying how previous hallucination can affect future generations. \n- Experimental results indicate that the proposed method is very effective at reducing span-level hallucinations for long-form generation. \n- The improvements on multi-hop QA is large, and the gains can be well explained by the \"active\" hallucination detection and revision mechanism."
            },
            "weaknesses": {
                "value": "- It would be nice to highlight the novelty of proposed framework from existing work. A very related work is [1], where the authors do active retrieval and rewrite actively when decoding each sentence, and they also use LLM output logits to find low-confidence spans for query generation. There are also several previous works that reduces LLM hallucinations at the response-level, using a similar framework as this work by prompting LLMs for span extraction, query generation, retrieval, and revise. For example, [2] and [3] uses such a framework to revise LLM responses and reduces hallucination; [4] prompted LLMs for extracting and checking claims as an automatic evaluation framework. This paper should discuss these related work, discuss the main differences, and maybe consider them as baselines.\n\n- The paper lacks ablations to justify some of its key components. For example, though there is a strong motivation for applying the method \"actively\" when generating every sentence, the end-to-end evaluation does not show how it helps reduce hallucination compared to applying it at the end of the generation. Similarly, I couldn't find ablation for only fact-checking low-confidence phrases v.s. fact checking all key phrases. \n\n- The presentation quality can be improved. Section 2 enumerates many modeling choices for each component, but it is difficult to tell what is the final method being used, and why it works better than the others. A suggestion is to describe the best approach in section 2, and leave other choices to ablation studies.  Section 3 and 4 cover many experiments, making it confusing to tell which is the most experiment and what are the main messages. \n\n[1] Jiang, Zhengbao, et al. \"Active retrieval augmented generation.\" arXiv preprint arXiv:2305.06983 (2023).\n\n[2] Gao, Luyu, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao et al. \"Rarr: Researching and revising what language models say, using language models.\" In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508. 2023.\n\n[3] Chen, Anthony, et al. \"PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions.\" arXiv preprint arXiv:2305.14908 (2023).\n\n[4] Min, Sewon, et al. \"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.\" arXiv preprint arXiv:2305.14251 (2023)."
            },
            "questions": {
                "value": "- Is there an ablation study comparing running the model actively vs running it at the end of the generation?\n- Is there an ablation study comparing fact checking only the low-confidence spans vs all extracted spans?\n- Since the method operates at sentence level, how does it know the context of each sentence? \n- The proposed method fact-checks key phrases / named entities. I'm interested to see how it works when the hallucination is not on named entities, or when the entire sentence is made up."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4279/Reviewer_EApd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4279/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798478739,
        "cdate": 1698798478739,
        "tmdate": 1699636395480,
        "mdate": 1699636395480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "30KmVVyPDS",
        "forum": "d3UGSRLbPo",
        "replyto": "d3UGSRLbPo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_5fdS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_5fdS"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to reduce LLM hallucinations using an early detection approach. The paper demonstrates that active detection and mitigation of hallucinations using logit output values is a viable path. The paper presents results on GPT 3.5 and Vicuna."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper works on an important problem of mitigating hallucinations \nThe paper presents an early detection approach and demonstrates effectiveness with two LLMs \nThe paper is extremely well-written, with clear goals, a well-described approach, and a detailed Appendix\n\nWhile it is always possible to nitpick on experimental design issues, we need to be mindful of the fact that this work is presented within the scope of a single ICLR submission. With that in mind, the paper does an excellent job."
            },
            "weaknesses": {
                "value": "It is unclear how effective this method will be for generations beyond the first five sentences."
            },
            "questions": {
                "value": "It is unclear how effective this method will be for generations beyond the first five sentences. Will it be more useful to distribute these checkpoints across the generated text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4279/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699119736606,
        "cdate": 1699119736606,
        "tmdate": 1699636395406,
        "mdate": 1699636395406,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wAzNud1tNl",
        "forum": "d3UGSRLbPo",
        "replyto": "d3UGSRLbPo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_pdV3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4279/Reviewer_pdV3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for detecting and mitigating hallucinations in LLM outputs. The detection consists of finding \"important\" concepts in the output, filtering them based on the model uncertainty, conducting web-search and feeding the info to the model to answer if the output contained hallucinations. The paper also proposes to use this knowledge from web-search to mitigate hallucinations."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper addresses the topic of hallucinations which is a relevant and timely topic."
            },
            "weaknesses": {
                "value": "1. The paper does not mentioned the highly relevant work of Kadavath et al., [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221), which also uses the model uncertainty to detect hallucinations. Since uncertainty is used as a major signal in the proposed pipeline, the novelty of the proposed approach is not clear.\n\n2. The paper does not study important choices in details. For instance, the web search procedure is not very clear. The paper says \u201cIn some cases, multiple web searches were required to check the correctness of different facets of a sentence\u201d. Are these searched human-supervised? What are the stopping criteria? I would suggest adding the web-search procedure in an algorithm block so that the readers can understand it better.\n\n3. Similarly, the paper does not discuss exactly what kind of \"important\" concepts are identified by the model. Could you provide some examples? Are the models supposed to extract all relevant concepts? Is the concept extraction supposed to work well across different application domains (e.g., questions answering)? What if we are working with non-instruction tuned models?\n\n4. It is not clear how good the instruction models were at following different instructions in Table 3. Did the authors perform a systematic analysis here?"
            },
            "questions": {
                "value": "See points 1-4 under \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4279/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699468413971,
        "cdate": 1699468413971,
        "tmdate": 1699636395334,
        "mdate": 1699636395334,
        "license": "CC BY 4.0",
        "version": 2
    }
]