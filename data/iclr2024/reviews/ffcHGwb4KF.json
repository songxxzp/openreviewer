[
    {
        "id": "JqvfUiWuIn",
        "forum": "ffcHGwb4KF",
        "replyto": "ffcHGwb4KF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
        ],
        "content": {
            "summary": {
                "value": "This work introduces SPADE, a per-sample sparsification method towards feature disentanglement. To this end, given a selected input sample, SPADE performs augmentation to create a \"batch of samples\"; then, for each layer of a considered network, a custom sparsity solver aiming to find a sparse set of weights to best approximate the output is considered. This results in a sparse model, specialized for the considered sample, upon which saliency maps and other neuron visualization techniques can be applied. Experimental evaluations on a variety of models (ResNet, MobileNet, ConvNext) and datasets (ImageNet-1k, CelebA, Food-101) are shown to yield accuracy improvements in various Saliency Maps methods, while human studies further support the quantitative findings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors propose a method for disentangling the features in each layer of a pretrained network on a per sample basis. This is a very intuitive approach, bypassing the typically considered limiting assumption of a per-class sparsity. The paper is overall well written and easy to follow. \n\nThe experimental evaluation considers a variety of datasets and architectures (albeit only convolutional ones) and the model exhibits improvements compared to a baseline and the alternative method of Wong et al. 2021."
            },
            "weaknesses": {
                "value": "The approach falls under the umbrella of sparsity-aware methods towards interpretability. To this end, the authors consider a post-hoc per-example sparsity scheme to disentangle representations in the context of multifaceted neurons. To do so, they consider a custom sparsity solver, namely OBC to solve the constrained optimization problem. \n\nThroughout the paper, the authors highlight the efficiency of the proposed mechanism compared to alternative methods (specifically [1]). At one point, the authors note:\n\n\"using our approach, it takes 41 minutes to preprocess the ResNet50 network for a single example, on a single RTX 2080 GPU (Table F.15). By comparison, it takes 40 hours to preprocess the network with the FC pruning method of Wong et al. (2021).(However, we note that SPADE must be run once per sample or group of samples, and the FC pruning method is run once for all examples. Irrespective of runtime, experiments in the next section show that our approach is significantly more accurate in practice.)\". \n\nIn this context, and considering for example the ImageNet validation set, the proposed SPADE algorithm would require a **Massive** number of weeks to train for all the examples. At the same time, it would require the user to re-run the inference algorithm for each example in the set or save a different set of weights for all the different examples. This is evidently impossible and greatly limits the applicability of the approach for any real-world setting. \n\nMoreover, the usage of post-hoc custom sparsity solvers (and as the authors note) require ad-hoc thresholds. The authors try to mitigate this issue by using \"100 calibration samples\" to maximize the average input pixel AUC score for the saliency method of interest in cases where the ground truth is known\". There are three major issues with this approach: (i) the authors consider additional information of how to tune the sparsity ratio, rendering the comparison with [1] unfair, since the aim of the latter is a balance between sparsity and accuracy and not optimization in the context of the considered saliency method (ii) to compute these sparsity ratios, they use all the 100 examples, thus utilizing augmented \"global\" information, somewhat undermining the per-example sparsity argument, and (iii) this adds further complexity to the already computationally intensive formulation of the method. \n\nIt is important to note that, in my understanding, the work of [1] aims to create **highly** sparse networks, while retaining the accuracy of the model, allowing for inspecting individual neurons. At the same time it does this only for the last linear layer. Thus, comparing SPADE to [1] is not exactly appropriate. \n\nFinally, the per-example sparsity rationale is not novel in the community. Indeed, the work of [2] recently proposed a data-driven mechanism for per-example sparsity based on a Bernoulli formulation. This was applied in the context of Concept Bottleneck Models (CBMs) allowing the model to select which \"features\" are considered in the last linear layer, also bypassing the limitation of [1], while being highly efficient and generalizable. This requires training a single linear layer to do the selection and can possibly mitigate many of the issues of the considered approach. \n\n\n[1]  Wong et al., Leveraging sparse linear layers for debuggable deep networks, In Proc. ICML 2021\n[2] Panousis et al., Sparse Linear Concept Discovery Models, In Proc. ICCVW 2023"
            },
            "questions": {
                "value": "Apart from the concerns raised in the previous section, some specific questions are:\n1) What are the sparsity ratios for each different method and layer arising through the introduced calibration method?\n2) How did the authors decide the samples in the held-out calibration samples?\n3) Is there a way to efficiently store the sparse structure for each example?\n4) What are the differences in the sparse structure between semantically similar images?\n5) What are the limitations for the considered operations? The method seems to not work for depthwise convolutions, as the authors exclude them from the considered architectures (Appendix B). Is application of the SPADE rationale possible in the ViT setting? If so, only in the MLP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698004241752,
        "cdate": 1698004241752,
        "tmdate": 1699636999348,
        "mdate": 1699636999348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b6k49ZhN4k",
        "forum": "ffcHGwb4KF",
        "replyto": "ffcHGwb4KF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes SPADE for preprocessing a given DNN model with respect to the prediction of a given input image. SPADE attempts to prune the weights in the model under the constraint that the prediction as well as the intermediate activation patterns do not change as much as possible for the given input. Experimental results suggest that applying existing XAI methods, including input saliency mapping or neuron visualization, to the model preprocessed by SPADE leads to a better understanding of the prediction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of pruning the model for a specific input for better explanation is clear.\n- Extensive experimental results demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The computational effort required for preprocessing by SPADE is relatively high, making it difficult to use in practical situations.\n- I could not fully understand the details of the human study."
            },
            "questions": {
                "value": "- What is the definition of $W_\\text{sparse}$ in Eq.(1)?\n\n- Is the re-calibration of batchnorm mentioned in page 4 also applied to the model after SPADE preprocessing?\n\n- Figure 2 (left) implies that image rotation is used as data augmentation, which is not the case according to Section 3.2.\n\n- In page 9, I could not understand the meaning of \"the image patches were always generated from the dense model\". What are \"image patches\" in this context?\n\n- In page 9: \"there were were\" -> \"there were\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698312502990,
        "cdate": 1698312502990,
        "tmdate": 1699636999233,
        "mdate": 1699636999233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ClxLWhUqoi",
        "forum": "ffcHGwb4KF",
        "replyto": "ffcHGwb4KF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_5ZSW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_5ZSW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SPADE, which recommends conducting sample-wise targeted pruning to obtain a sparse version of the original network, before interpreting the network's predictions (w.r.t. the specific sample) using any interpretation method. Given an image to interpret, SPADE first applies various augmentation techniques to generate a batch of different views of the given image, then uses the OBC sparsity solver to find a sparse set of weights which matches the layer-wise activations of the original model. With this sparsified model, different path-based and perturbation based interpretation techniques are applied to generate input saliency maps and neuron activation maps. The authors perform experiments on 3 convolutional architectures, 3 vision datasets to demonstrate improved saliency map accuracies (as measured by AUC and Pointing Game scores) and enhanced usefulness of neuron activations (as measured by human task success rate)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. **Reproducibility** \u2014 The authors describe experimentation settings (including human experiments, datasets, metrics, sparsity ratios, etc) in detail, open-source their code and provide model weights (with Trojan backdoors) for reproducibility.   \n2. **Organisation and writing** \u2014 This paper is very well-written and meticulously organised. The Appendix includes a table of contents and is highly readable.     \n3. **Evaluation** \u2014 SPADE evaluates on a variety of path-based and perturbation-based saliency attribution methods; on 3 convolutional architectures; on AUC and Pointing Game metrics; on human task performance. Evaluation is relatively thorough, though I have critical concerns about how evaluations are conducted on a very small fraction of the validation set of ImageNet-1K, CelebA and Food-101 (see W1)."
            },
            "weaknesses": {
                "value": "1. **Small-scale ImageNet experiments?** \u2014 Please clarify if I misunderstood but it appears that the main result (i.e., saliency map \"accuracy\" of SPADE vs. Dense vs. Sparse FC on ResNet50/ImageNet) is only calculated for **140 test samples out of the available 50,000** in the ImageNet-1K validation set. It seems bold to claim AUC improvements and Pointing Game score gains when evaluation is done on 0.0028 of the actual validation set, especially when it is unclear how/why these 140 chosen samples are to be representative of the wider dataset.  \n   \n2. **Non-negligible cost for constrained optimisation of sparse network** \u2014 As described in Section 3, SPADE relies on activation matching for every layer and takes 41 minutes for single example preprocessing. This method becomes prohibitively costly with increasing cost proportional to dataset size and the number of network layers, rendering it impractical for large-scale deep neural network debugging.   \n  \n3. **Limited novelty** \u2014 Using sparsity to factorise / disentangle concepts is a known direction in literature. SPADE adopts this perspective, then leverages an existing OBC sparsity solver to obtain sparse subnetworks w.r.t. every example (and its augmentations), then applies off-the-shelf interpretability techniques to generate various interpretation maps. SPADE does not seem to present novel insights, methods or findings.  \n  \n4. **Fairness and fidelity of input saliency maps (not neuron visualisations)** \u2014 SPADE requires sample-wise sparsification before interpreting each different image sample, meaning that the saliency map is generated with respect to a different subnetwork for each image. Sparse networks A and B could exhibit drastically different performance (accuracy and interpretability) on examples A and B, it therefore seems unfair to interpret different examples using different subnetworks. SPADE saliency map visualisations can only be matched / replicated by using the exact same sparse subnetworks for every example and is hence costly to reproduce. It is furthermore unclear to me why SPADE saliency maps would be representative of those of the original dense model."
            },
            "questions": {
                "value": "1. The motivation of SPADE is to reduce the multifacetedness of neurons through pruning but it is unclear to me how SPADE is able to accomplish this. Could the authors elaborate on the intuition of why/how does the constrained optimisation in Equation 1 disentangle multifaceted neurons? Multifaceted neurons encode richer and more concepts; these concepts typically generalise not only across samples but also across augmentations, whereas concept-specific neurons might activate only for 1 or few specific augmented view(s). For maximal sparsification while matching layer-wise activations, wouldn't the optimiser retain multifaceted neurons and discard highly specific neurons?  \n  \n2. Do interpretation maps differ significantly for dense and sparse networks? Are saliency maps obtained from different sparse subnetworks representative of the interpretability of the original dense network?   \n  \n3. Does the sparsity solver find highly similar or dissimilar sparse subnetworks for different image examples? In other words, are the same set of neurons consistently retained for different images?    \n  \n4. Are there any specific patterns or properties of retained neurons in the sparse subnetworks? Do they have larger activation magnitudes, or perhaps do they belong to earlier / later or wider / narrower layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698495996192,
        "cdate": 1698495996192,
        "tmdate": 1699636999061,
        "mdate": 1699636999061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sjEnsJrSRP",
        "forum": "ffcHGwb4KF",
        "replyto": "ffcHGwb4KF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
        ],
        "content": {
            "summary": {
                "value": "The authors propose SPADE: a new post-hoc local explanation methodology for deep neural networks.  Specifically, the authors propose training a new sparse DNN to approximate the predictions of the original DNN under examination.  Importantly, they argue in favor of training a new sparse DNN for each individual example, and later show the value of this approach for detecting Trojans.  The authors propose that one can then run typical post-hoc explanation methods (like saliency maps or neuron visualizations) on the sparse approximation network.\n\nThe authors present several empirical comparisons that use proxy measures and human subject evaluations of their method to common approaches (such as explaining the original dense network, or other post-hoc explanations based on learning a sparse explanation model)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors' proposed method, SPADE, is straightforward to understand and makes use of recent innovations in related bodies of work (e.g., sparsity solvers) in a clever way.  The authors introduce SPADE as a general and customizable approach, and clearly outline different design decisions that one could make to change SPADE in practice (like use of different solvers, objectives, or explainers once the sparse network has been learned).  They also thoroughly ablate their approach which I appreciated.\n2. The authors conduct a thoughtful evaluation of their proposed method by using Trojans to design a scenario where they have access to ground-truth information about the model's behavior.  By doing so, they avoid common pitfalls in related work that evaluates a new proposed explanation method.\n3. I appreciate the authors' commitment to reproducibility.  All methods and experiments are clearly described, and substantial additional information about each experiment is provided in the Appendices."
            },
            "weaknesses": {
                "value": "I am happy to consider adjusting my score if my concerns are addressed.\n\n* **Weakness #1: Transparency about limitations of the proposed approach**.  I believe that the present draft would be made much stronger if it dedicated more time to thoughtfully discussing limitations and implications of the author's proposed approach.  I list what I believe are significant limitations below.  I do not think that these limitations weaken the proposed method (all explanation approaches have their own limitations!), but being clear about them will help readers and potential users of this work.\n  * _Selecting an appropriate sparsity ratio for each layer_.  (Section 3.2) I am hesitant of how you chose to use cross-validation given ground-truth information to select the optimal \"sparsity ratio\". I think this may lead to an overly optimistic representation of SPADE's performance because in practice, a user of SPADE would not have such ground truth information available. Can you acknowledge this as a limitation and also examine SPADE's sensitivity to different sparsity ratio values (e.g., if I use the wrong ratio, is it now useless) in the draft?\n  * _Computational cost_.  Given that SPADE is so computationally expensive, how would you recommend that users choose individual samples to explain? \u2013 is there a more principled strategy than explaining all of the misclassified or \"surprising\" examples?\n  * _Improvement over baselines is not that strong_. I am surprised that the baseline saliency and visualization methods actually establish a pretty strong baseline (i.e., the difference between the SPADE vs. baseline settings is actually not that large in Table 1 and Figure 4).  Can you further elaborate on this in your paper text \u2013 is it because existing explanations are actually quite well-suited for the Trojan task, but maybe less appropriate in settings where the \"bug\" is something more subtle (like presence of a spurious object)?\n* **Weakness #2: Clarity about experimental design**.  Overall, I found the paper to be well-written.  However, I believe there are some experimental design details that should be surfaced more clearly in the main text.\n  * _Clarify motivation for the Trojan experiment set-up_.  My understanding from reading [1] is that in real-life scenarios where we may wish to find unknown \"backdoors\" or Trojans in a model, we don't have access to individual examples where the Trojan is present \u2013 i.e., if we had a datapoint with the Trojan in it, then we would know that something is up.  (Casper et al. says, \"Finding trojans using interpretability tools mirrors the practical challenge of finding flaws that evade detection with a test set because Trojans cannot be discovered with a dataset-based method unless the dataset already contains the trigger features. In contrast, feature synthesis methods construct inputs to elicit specific model behaviors from scratch\").  This entire problem set-up is seemingly incompatible with the data-dependent workflow you're proposing, where I would need to \"explain\" the network on an individual data-point that has the Trojan, in order to see an explanation that reveals that the model is relying on it. Can you please clarify why you chose to use this Trojan set-up to motivate and evaluate your data-dependent explanation method?\n  * _Questions about saliency map experiments (Section 4.1)_. In this set-up, do we only calculate the \"accuracy\" of saliency maps for the images with the Trojans in them, as these are the only images where we have ground-truth information (i.e., are the \"140 examples\" in Table 1 all images with Trojans)? How exactly do you calculate each saliency map? \u2013 are you taking the gradient of the neuron that is the predicted class, or true (Trojan) class of the image?\n  * _Questions about the user study (Section 4.2.2)_. What is an \"image patch\"? How is it computed? How is it an accurate representation of the \"ground truth\" reasoning of the dense model? Can you provide more detail inline about what the \"correct answer\" is for the question we asked users (\"which of the two regions activates the neuron\")? Here are the \"regions\" the image patches for each class, and the ground truth is which class the neuron responds to?\n\n[1] https://arxiv.org/pdf/2302.10894.pdf"
            },
            "questions": {
                "value": "See the above \"Weaknesses\" section for my high priority questions and concerns.\n\nI also had a few lower priority suggestions that did not affect my score:\n* In general, when you say that your method is more \"accurate\" than others in this work (e.g. the statement \"experiments in the next section show that our approach is significantly more 'accurate' in practice\"), can you be more specific about what exactly you mean by \"accurate\"? Do you simply mean that the sparse models learned by SPADE are a more faithful approximation of the true prediction model; or that the explanations produced by SPADE allow users to complete some task more accurately?\n* A nit about language:\n  * \"debugging\": \"interpreting a model's predictions on specific examples\" is actually more commonly known in this community as a \"local explanation\" (see Section 4.2 of [1]).  I believe the term \"debugging\" implies that the end user is a developer, who is hoping to understand what action should be taken to improve the model (e.g., as used in [2]).   I think it is fine to call your method \"sparsity-guided debugging\" as the interpretations provided could be used downstream to fix the model.  Maybe you could consider \"sparsity-guided interpretability\" or \"sparsity-guided explanations\" instead? :-) \n  * \"preprocessing\": I was confused by how the authors used the term \"preprocessing\" to describe the model pruning step of SPADE, given that I've typically only ever seen this term used to discuss data preprocessing that occurs before model training (vs. here you are referring to a post-hoc interpretation method that learns a sparse model).  Maybe you can use the term \"pruning\" instead?\n* Section 3.1: Can you provide a more formal definition of what you mean by \"sparse\" in this section? I am assuming you mean \"sparse\" in an L0 norm sense = \"the majority of weights are 0\", rather than an L2 sense = \"the weights have very small values\"? (I see later in Section 3.2 that you use an L2 sparsity constraint.  Can you provide intuition as to why you used L2 (rather than a different norm)?)\n* Section 3.1: Can you provide further intuition or evidence for the \"thinning\" hypothesis? I don't understand why sparsity discourages 'multifacetism'.  Intuitively, if the neurons at every layer at 'multifaceted', then even a sparse combination of multifaceted neurons will still be multifaceted\u2026\n* Section 3.2: Can you clarify in your draft what the 96.5% \"agreement percentage\" measures? Is it the agreement of the single final class prediction?\n\n[1] https://arxiv.org/abs/1702.08608\n[2] https://scholar.google.com/citations?view_op=view_citation&hl=en&user=y1bnRg4AAAAJ&sortby=pubdate&citation_for_view=y1bnRg4AAAAJ:aqlVkmm33-oC"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986321672,
        "cdate": 1698986321672,
        "tmdate": 1699636998800,
        "mdate": 1699636998800,
        "license": "CC BY 4.0",
        "version": 2
    }
]