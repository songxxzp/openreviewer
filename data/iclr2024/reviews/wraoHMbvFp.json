[
    {
        "id": "j6yfP3USrZ",
        "forum": "wraoHMbvFp",
        "replyto": "wraoHMbvFp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5388/Reviewer_1ve1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5388/Reviewer_1ve1"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an LLM-based explainability approach for vision models. It uses LLMs to explain different visual concepts contained in the image in a tree-like manner. \n1) Given a predicted output from a vision model, LLM is used to explain predicted concept and its constituent parts using natural language\n2) Text-to-image model is then used to identify the visual representations of the constituent parts. \n3) These visual representations are then passed to the vision model for prediction.\n\nThen steps 1 - 3 repeat.\n\nThis type of recursive procedure helps to explain visual concepts in a hierarchical tree-like structure.\nThe authors also propose to prune infrequent nodes and expand the tree based on LLM prompting. In addition to that the paper also proposes to retrain the model based on the refined explanation trees to improve the model\u2019s interpretability. Plausibility, faithfulness, and stability are the metrics used to evaluate the explanations against baseline approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has a number of interesting contributions:\n\n1) It uses a combination of a variety of models such as vision, text-to-vision and LLMs to generate tree-like explanations for the vision models.\n2) Semi-automatically curates annotated datasets for CIFAR10, CIFAR100 and Imagnet.\n\n3) Shows that the explainability-guided regularizer can help with both model explainability and accuracy."
            },
            "weaknesses": {
                "value": "1) The paper seems a bit crowded with different contributions that do not read coherently.\n2) Overall it is known that all models( text2image, vision and LLMs) have prediction errors. In this case it will lead to error propagation in the tree of the explanations which can result in error amplification. It would be good to study the impact of the erroneous predictions on the explanation tree. E.g. how much the errors from LLM model get propagated down to text to image and vision models.\n3) The abstract seems a bit too crowded and could be refined and simplified. For example the following sentence: `This tree is dynamically pruned and grown by querying the LLM using language templates, \u2026 `.\nIt is unclear what `language templates` is meant here.\n4) Figure 1 is hard to interpret, the order of the arrows is not very clear. I\u2019d recommend using numeric numbers on the arrows. This will help to better understand the sequence of the actions.\n5) It\u2019s unclear why the authors choose `Concepts, Substances, Attributes, and Environments.` attributes.\n6) The explanation tree can potentially become very large and there can be different ambiguous cases. It would be good to discuss the problems and solutions related to scale and ambiguity. A discussion section can be helpful.\n7) The same concept can be described in different ways through text. It would be interesting to study and discuss those aspects in the paper.\n8) The evaluation part is a bit unclear. It would be good to clearly showcase the use cases (examples) where other baseline approaches fail and proposed method is able to handle those challenges better."
            },
            "questions": {
                "value": "1) How was the quality of the annotated dataset established ? Since it is semi-automated it can still have a high error rate or there might be many ambiguous cases. How are the ambiguities handled ?\n2) In terms of calibration that leads to accuracy improvement is it total accuracy or class based accuracy ? Overall accuracy might be high but subgroups might perform poor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644142365,
        "cdate": 1698644142365,
        "tmdate": 1699636545093,
        "mdate": 1699636545093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EPDrXDNjNi",
        "forum": "wraoHMbvFp",
        "replyto": "wraoHMbvFp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5388/Reviewer_dC7U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5388/Reviewer_dC7U"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to provide structured and human-understandable explanations for vision models and introduces a new and challenging task of generating visual explanatory tree. \nThis work collects data used for the explainability for the existing dataset ImageNet and CIFAR complementing the lack of hierarchy annotation. \nThe approach leverages LLM and text-to-image API as a bridge between language and vision domains. \nThis paper also introduces new benchmarks and metrics for assessing the quality of predicted tree-structured explanations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Compared to previous explainability approaches, by leveraging the strengths of LLM, this method can construct abundant parsing tree used for the explanation of the visual models. \nThe building approach of the new dataset can automatically collect hierarchical annotations is significant."
            },
            "weaknesses": {
                "value": "As claimed as a work to generate human-understanable explanable parsing tree, this paper should include human evaluators results of assessing whether the generated results are reasonable. Without human judgment, these outputs cannot be properly evaluated."
            },
            "questions": {
                "value": "How does this model perform on out-of-domain categories? Can it still produce interpretable results if the category is not within ImageNet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5388/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5388/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5388/Reviewer_dC7U"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755663792,
        "cdate": 1698755663792,
        "tmdate": 1699636544991,
        "mdate": 1699636544991,
        "license": "CC BY 4.0",
        "version": 2
    }
]