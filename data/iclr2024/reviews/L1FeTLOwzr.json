[
    {
        "id": "CG9s8dVv13",
        "forum": "L1FeTLOwzr",
        "replyto": "L1FeTLOwzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission375/Reviewer_yoGd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission375/Reviewer_yoGd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dynamic adapter merging framework for domain-incremental VideoQA learning. The framework is capable of obtaining multiple domain-specific adapters and dynamically integrating different domain information through model merging techniques. Experiments results on multiple public datasets verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe logic of the paper is reasonable.\n2.\tThe experiments are relatively adequate."
            },
            "weaknesses": {
                "value": "The technical details of this paper are not described clearly enough, my concerns are as follows:\n1.\tWhy do you set up N adapters for each domain instead of one?\n2.\tWhy do you choose to insert domain-specific adapters after the self-attention and feed-forward layers, respectively? What are the considerations?\n3.\tWhat exactly is meant by the pre-trained model f in Eqn. (1)?\n4.\tWhat does the symbol k in the baselines section on page 6 refer to? I cannot find a definition in the previous text.\n5.\tWhat is the exact structure of the adapter?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697695285405,
        "cdate": 1697695285405,
        "tmdate": 1699635964682,
        "mdate": 1699635964682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aTd5wkSMN0",
        "forum": "L1FeTLOwzr",
        "replyto": "L1FeTLOwzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
        ],
        "content": {
            "summary": {
                "value": "The paper studies VideoQA in a domain continual-learning setting. The task encourages VQA models that can quickly adapt to new domains/datasets while simultaneously prevent catastrophic forgetting on learned domains. To achieve the goal, the paper proposes the dynamic adapter merging (DAM) method. Given a random instance, DAM dynamically (learning-free) merges a series of domain-specific parameter adapters for answer prediction, where the adapters are continually learned across datasets of different domains. The authors conduct extensive experiments on 6 VideoQA datasets and additionally 4 ImageQA datasets to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper conducts the first study on domain-incremental learning in VideoQA. It also presents a nice solution to benchmark the task.\n2.\tThe DAM method is simple, easy to understand and shows strong results as well. Also, the experiments and analyses are in-depth.\n3.\tThe paper is well-presented and easy to read."
            },
            "weaknesses": {
                "value": "1.\tThe definition of domain regarding VideoQA is not clear. The authors simply treat different datasets as different domains. This is certainly problematic and prevents detailed model analysis. For example, regarding the question type, all datasets define similar questions except for LSMDC with fill-in-blank setting.  Regarding the video type, there are instructional videos (iVQA), social videos (MSVD, MSRVTT, TGIF), movie videos (LSMDC) and activity videos. Regarding video length, all videos are short (3~15s) except for ActivityNet(3 mins). It would be better to experiment with more clarified domains instead of datasets.\n\n2.\tWhile the \u2018dynamic merging\u2019 design mitigates the problem of catastrophic forgetting and improves the overall performance as well, it necessitates all the learned adapters for inference. This resembles more on model ensemble versus continual learning a \u2018single\u2019 model. It is necessary to show the size of the adapters and analyze the efficiency.\n\n3.\tThe authors obtain the upper-bound results by individually finetuning on target datasets. My concern is that this \u2018upper-bound\u2019 may not be the actual upper-bound for incremental-learning because of data augmentation. Moreover, the gap between DAM' results and this upper-bound results is too small to show that there is need for future efforts as a novel task setting. The authors need to find a more convincing upper-bound or just mention the current one as a reference.\n\n4.\tAccording to the task setting, providing more analyses /comparisons on an OOD setting (aside from Fig.3(b)) would make the experiments more sound."
            },
            "questions": {
                "value": "Minor:\n1. Why is the performance on ActivityNet not as good as on other datasets? \n\n2. In Sec. 3.2, what specific model does \u2018f\u2019 refer to? \n\n3. Analyses of table 5, 6 should be moved from the appendix to the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission375/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission375/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission375/Reviewer_DkBm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699101378360,
        "cdate": 1699101378360,
        "tmdate": 1699635964622,
        "mdate": 1699635964622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZRBOHTmEmU",
        "forum": "L1FeTLOwzr",
        "replyto": "L1FeTLOwzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission375/Reviewer_thbM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission375/Reviewer_thbM"
        ],
        "content": {
            "summary": {
                "value": "The article presents to address continual video question-answering (VidQA) learning with a simple framework, named DAM. Through sequentially training domain-specific adapters and leveraging a video-language router to merge the adapters for inference, DAM outperforms prior methods by 9.1% while forgetting less by 1.9%."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper assumes that this is the first attempt to address the issue of continual learning in VideoQA.\n2) Comprehensive Ablation Studies: The article includes sufficient and in-depth set of ablation experiments, which provide a thorough understanding of the method's performance and help identify critical components.\n3) Clear Method Framework: The method's framework is straightforward and well-explained, making it accessible to readers and researchers in the field."
            },
            "weaknesses": {
                "value": "1) Limited Dataset Diversity: The article's experimental use of six datasets with relatively small differences between them, especially MSVD and MSR-VTT, raises concerns about the method's domain adaptation and continual learning capabilities. The use of internet-sourced videos in the datasets does not fully explore the potential challenges posed by more diverse datasets, such as those collected in virtual environments (e.g., Env-QA[1]), traffic scenarios (e.g., TrafficQA[2]), or indoor human activities (e.g., AGQA[3]). What\u2019s more, the out-of-date issue proposed in Figure 1 hasn\u2019t been evaluated, also.\n2) While the article demonstrates the effectiveness of the adapter and router, their simple design might not generalize well to more challenging datasets. The reviewer has doubts about their applicability in more complex scenarios.\n3) The article does not provide a fair comparison with backbone models under few-shot learning setting. A direct comparison between in-context learning using FrozenBiLM and the proposed approach could offer a more comprehensive evaluation.\n[1] Gao, Difei, et al. \"Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n[2] Xu, Li, He Huang, and Jun Liu. \"Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[3] Grunde-McLaughlin, Madeleine, Ranjay Krishna, and Maneesh Agrawala. \"Agqa: A benchmark for compositional spatio-temporal reasoning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."
            },
            "questions": {
                "value": "1) The article does not provide sufficient evidence of severe catastrophic forgetting in current large models.\n2) It is worth discussing whether there are unique challenges related to continual learning in the domain of VideoQA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699116323095,
        "cdate": 1699116323095,
        "tmdate": 1699635964565,
        "mdate": 1699635964565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L5HvVUH8eE",
        "forum": "L1FeTLOwzr",
        "replyto": "L1FeTLOwzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission375/Reviewer_t6Nd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission375/Reviewer_t6Nd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Dynamic Adapter Merging (DAM) for video question-answering under Domain-Incremental Learning scenario, which is a rehearsal-free approach. DAM leverages the fusion of parameters from multiple adapters to mitigate the interference of erroneous predictions, thereby enhancing the performance of the model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well organized and the proposed method is verified through many experimental results.\nThe DAM is straightforward and easy to follow."
            },
            "weaknesses": {
                "value": "The paper provides a detailed elaboration to the framework of the model. However, the authors do not explicitly mention the loss function used during the training of adapters.\n\nThe contributions of the paper may be insufficient.  Although the Introduction section mentions four contributions, these contributions revolve primarily around one aspect, i.e. related to combining domain-specific adapter learning and model merging techniques.\n\nThe proposed method may lack innovation as the idea of model merging techniques in deep/machine learning is frequently used. The non-parametric router function is simply based on cosine similarity with no improvements. However, the application of such a concept to Continual Learning does introduce somewhat novelty."
            },
            "questions": {
                "value": "Can such ideas bring about desired performance improvements when extended to class-incremental learning and task-incremental learning scenarios? Can the author incorporate some results to demonstrate the generalizability of the idea in the context of continual learning?\n\nHow were the experimental results in the article obtained? Were multiple runs conducted to obtain an average, or was only a single experiment performed? I would like to know the stability of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699440815529,
        "cdate": 1699440815529,
        "tmdate": 1699635964492,
        "mdate": 1699635964492,
        "license": "CC BY 4.0",
        "version": 2
    }
]