[
    {
        "id": "9q3zp7yCkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
        ],
        "forum": "9zEBK3E9bX",
        "replyto": "9zEBK3E9bX",
        "content": {
            "summary": {
                "value": "The paper introduces SPOT (Scalable Pretraining via Occupancy prediction for learning Transferable 3D representations), a method for easing the annotation of 3D LiDAR point clouds, which is typically resource-intensive. The central idea is to leverage large-scale pre-training and then refine these models for different downstream tasks and datasets. Main contributions can be summarised as follows:\n\n- Occupancy Prediction: The paper underscores the potential of occupancy prediction as a means to learn general representations. The efficacy of this approach is validated through comprehensive experiments on numerous datasets and tasks.\n- Techniques for Point Cloud Augmentation: SPOT employs a beam re-sampling method to augment point clouds. It also implements class-balancing strategies to counteract the disparities arising from diverse LiDAR sensors and annotation methodologies across datasets.\n- Scalability of Pre-training: An interesting observation made is that as the amount of pre-training data increases, the performance on downstream tasks improves consistently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses a significant challenge in 3D LiDAR point cloud research, specifically the task of annotating 3D LiDAR point clouds for perception. The approach of performing large-scale pre-training and then fine-tuning the pre-trained backbone on various downstream datasets is novel.\n\n- Comprehensive experiments on multiple pre-training and downstream datasets (WOD, KITTI, SemanticKITTI, NuScenes, ONCE) are presented, along with thorough ablation studies.\n\n- The structure and writing of the manuscript are clear, making it easy to follow.\n\n- The figures, visualizations and illustrations are exemplary, with a particular appreciation for Fig. 1."
            },
            "weaknesses": {
                "value": "- Some of the contributions highlighted by the authors appear to be not novel enough, e.g., the class-balancing strategies. Existing studies [1, 2], have showcased similar strategies. It would be good to acknowledge, cite, and compare their work with these prior studies.\n\n- In Sec. 4.1, the authors claim that \"our experiments are under label-efficiency setting.\" However, contemporary dataset subsampling techniques seem to encompass more than just the \"randomly selected\" method utilised in this paper. In fact, drawing from semi-supervised methodologies, the \"uniform sampling\" technique appears to be more prevalent. Moreover, Li et al. introduced the ST-RFD method, which aims to extract a more diverse subset of training data frame samples. I believe it would be beneficial for the authors to explore different sampling techniques and consider integrating the ST-RFD method to potentially achieve better downstream subset(s).\n\n- While the authors introduce a two-stage approach of first pre-training followed by fine-tuning, I'm still uncertain about the main advantages of that idea. For instance, current research employing semi-supervised or active learning techniques for point cloud semantic segmentation [3, 4] seems to achieve superior results  on 10% of the SemanticKITTI dataset (mIoU: 62.2 [3], mIoU: 60.0 [4]). Furthermore, Unal et al. [2] obtain a 61.3 mIoU on ScribbleKITTI (weakly annotated with 8% of the SemanticKITTI points). These methods also seem to efficiently utilise labels and alleviate the burden of extensive labelling.\n\n- Lack of related work, e.g., [1-4].\n\n[1] Zou, Y., Yu, Z., Kumar, B. V. K., & Wang, J. (2018). Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference on computer vision (ECCV) (pp. 289-305).\n\n[2] Unal, O., Dai, D., & Van Gool, L. (2022). Scribble-supervised lidar semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 2697-2707).\n\n[3] Li, L., Shum, H. P., & Breckon, T. P. (2023). Less is more: Reducing task and model complexity for 3d point cloud semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 9361-9371).\n\n[4] Kong, L., Ren, J., Pan, L., & Liu, Z. (2023). Lasermix for semi-supervised lidar semantic segmentation. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 21705-21715)."
            },
            "questions": {
                "value": "Refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewer_FWLb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697210478332,
        "cdate": 1697210478332,
        "tmdate": 1699635976346,
        "mdate": 1699635976346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DUprGvFIpR",
        "forum": "9zEBK3E9bX",
        "replyto": "9zEBK3E9bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a scalable pre-training method called SPOT for learning transferable representations for 3D perception tasks. SPOT pre-trains a model on the task of occupancy prediction, which is a general task that can be used to learn useful representations for various 3D perception tasks. To mitigate the gaps between pre-training and fine-tuning datasets, SPOT uses beam resampling augmentation and class-balancing strategies. The authors evaluate SPOT on various 3D perception tasks and datasets. SPOT outperforms training from scratch by a significant margin on all tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and presents extensive studies on pre-training occupancy prediction on the Waymo Open Dataset and fine-tuning on Nuscenes, KITTI, ONCE, and SemanticKitti. The authors make the interesting observation that occupancy prediction outperforms detection pre-training, even on downstream detection tasks. This highlights the ability of occupancy prediction to mitigate the domain gap between pre-training and fine-tuning datasets."
            },
            "weaknesses": {
                "value": "1. Unfair comparisons in Table 1 and 2: SPOT is compared against BEV-MAE and AD-PT, which are self-supervised and semi-supervised pre-training methods, respectively. This is an unfair comparison, as SPOT is a supervised pre-training method that benefits from human-labeled data. A more fair comparison would be to compare SPOT against other supervised pre-training methods.\n\n2. Supervised occupancy prediction is not new, and It is hard to argue scalability when pre-training is based on a labeled dataset.  On the other hand, self-supervised occupancy pretrainig was demonstrated in previous works, e.g. [1], which had shown unlabeled occupancy pretraining via MAE work. \n\n3. Another critical limitation on scalability is that the pre-training dataset must have a more expensive lidar setup (64-beam with high density) compared to the fine-tuning dataset. \n\n3. Low train from scratch baseline performance: As shown in Fig 7, with 100% nuScenes, training from scratch leads to mAP ~ 50, which is far from SOTA (>70) [2]. \n\n4.  Given the low train from scratch performance, could the baseline model be undertrained or that it is using a weaker data augmentation? \n\nPre-training 30 epochs on WOD requires a lot more computation resources compared to fine-tuning 30 epochs on NuScenes. \n\nIf you use the same computation resources ( as pre-training + fine-tuning) to train a model on NuScenes from scratch with a stronger data augmentation, I guess the performance gap between pre-training and training from scratch will be much smaller. \n\nAlso, previous studies have shown that with strong data augmentation during fine-tuning, the benefit from pretrianing diminishes [3]. \n\n[1] Min, Chen, et al. \"Occupancy-MAE: Self-Supervised Pre-Training Large-Scale LiDAR Point Clouds With Masked Occupancy Autoencoders.\" IEEE Transactions on Intelligent Vehicles (2023).\n\n[2] https://paperswithcode.com/sota/3d-object-detection-on-nuscenes\n\n[3] Zoph, Barret, et al. \"Rethinking pre-training and self-training.\" Advances in neural information processing systems 33 (2020): 3833-3845."
            },
            "questions": {
                "value": "Please see weaknesses.\n\n\n\n-------------------------------------\nThank you for running the additional experiments. I increased my score. \n\nHowever, I cannot recommend acceptance \n1.  the improvement is marginal (only + 0.25 NDS) for a stronger baseline (VoxelNext), Table 17.\n2.  As reviewer y7L9 suggested, `the improvements and results showcased appear anticipated`."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewer_4p2k"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607851087,
        "cdate": 1698607851087,
        "tmdate": 1700080905785,
        "mdate": 1700080905785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fEV6rt330k",
        "forum": "9zEBK3E9bX",
        "replyto": "9zEBK3E9bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
        ],
        "content": {
            "summary": {
                "value": "The paper presents SPOT (Scalable Pre-training via Occupancy Prediction), aimed at learning transferable 3D representations from LiDAR point clouds for autonomous driving tasks. SPOT leverages occupancy prediction as a pre-training task to learn general representations, employs beam re-sampling for point cloud augmentation, and class-balancing strategies to bridge domain gaps caused by varying LiDAR sensors and annotation strategies across different datasets. The authors extensively test SPOT across multiple datasets and 3D perception tasks, demonstrating its scalability and effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Leveraging occupancy perception as a pretraining task is interesting.\n2. The tricks of beam re-sampling augmentation and class-balancing strategies are useful.\n3. The authors did large-scale experiments on five datasets."
            },
            "weaknesses": {
                "value": "My major concern is that this pretraining is not self-supervised representation learning. Although the authors tout SPOT as a label-efficient solution, the labor-intensive nature of building a large-scale semantic occupancy dataset seems to contradict this claim. Furthermore, the improvements and results showcased appear anticipated, especially given the employment of the extensively annotated large-scale Waymo open dataset."
            },
            "questions": {
                "value": "What will the results look like if only using binary occupancy labels without any human labeling? How can the pretraining be made fully self-supervised? Meanwhile, what will the results look like if pretraining on a different dataset (not Waymo)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission496/Reviewer_y7L9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649806214,
        "cdate": 1698649806214,
        "tmdate": 1699635976180,
        "mdate": 1699635976180,
        "license": "CC BY 4.0",
        "version": 2
    }
]