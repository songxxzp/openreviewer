[
    {
        "id": "xBsovmBrPN",
        "forum": "vSwu81S33z",
        "replyto": "vSwu81S33z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
        ],
        "content": {
            "summary": {
                "value": "In the context of transfer learning, the choice of the prior distribution for downstream data is crucial when employing Bayesian model averaging (BMA). Prior strategies have limitations in handling distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL) that addresses distribution shift issues within the framework of nonparametric learning. The nonparametric learning (NPL) method, which uses a nonparametric prior for posterior sampling, efficiently deals with model misspecification scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes the use of nonparametric learning techniques to address the issue of distribution shift in transfer learning. The writing in the article is of high quality and is easy to understand. I partially reviewed the mathematical derivations in the paper, and the technical aspects appear to be sound."
            },
            "weaknesses": {
                "value": "I did not find any significant drawbacks in the paper, except for some confusion in the nonparametric learning section. Please see below for details."
            },
            "questions": {
                "value": "I feel confused about the authors' claim that \"the NPL posterior is robust to model misspecification through the adoption of a nonparametric model and gives asymptotically superior predictions to the regular Bayesian posterior on $\\theta$\".\n\nIn my understanding, NPL is just another way to compute the posterior for $\\theta$. In parametric Bayesian methods, a prior $p(\\theta)$ is placed on $\\theta$, and various methods are employed to compute the corresponding posterior $p(\\theta|\\mathcal{D})$. NPL, on the other hand, establishes a deterministic mapping between the data distribution $F$ and $\\theta$ using MLE (as shown in Equation 2). Then, a prior $p(F|\\alpha, F_\\pi)$ is placed on the data distribution $F$ to analytically derive the posterior $p(F|\\mathcal{D})$, which subsequently leads to the posterior $p(\\theta|\\mathcal{D}$).\n\nBoth of the above methods assume a parameterized model with parameters $\\theta$. In other words, the assumed parameterized model is still susceptible to potential discrepancies with the true data distribution, which could result in model misspecification. Therefore, if my understanding is correct, NPL may not inherently address the issue of model misspecification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698376282447,
        "cdate": 1698376282447,
        "tmdate": 1700448076630,
        "mdate": 1700448076630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nyWr6KSeIO",
        "forum": "vSwu81S33z",
        "replyto": "vSwu81S33z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
        ],
        "content": {
            "summary": {
                "value": "In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. Through extensive empirical validations, the authors demonstrate that the approach surpasses other baselines in BMA performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The authors maintain high quality of presentation. The motivation and algorithms are clearly explained. \n\n(2) Extensive experiments and ablation studies are conducted in this paper. The authors considered different model architectures, datasets, evaluation metrics, tasks."
            },
            "weaknesses": {
                "value": "(1) A detailed discussion of limitations is lacking. A possible aspect could be the computation cost. A careful discussion of pros and cons could be very helpful to the community."
            },
            "questions": {
                "value": "(1) Following the experiments of [1], could the proposed method also uses self-supervised pretrained models and include experiments about segmentation?\n\nreferences:\n\n[1] Shwartz-Ziv, Ravid, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew G. Wilson. \"Pre-train your loss: Easy bayesian transfer learning with informative priors.\" Advances in Neural Information Processing Systems 35 (2022): 27706-27715."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722011230,
        "cdate": 1698722011230,
        "tmdate": 1699636804428,
        "mdate": 1699636804428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3uyNRM4aFV",
        "forum": "vSwu81S33z",
        "replyto": "vSwu81S33z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the applicability of nonparametric transfer learning (NPTL) in the context of Bayesian NNs and transfer learning. The general idea is that having pre-trained models based on NNs, once can obtain good performance on Bayesian model averaging (BMA) prediction on different tasks. For instance, pre-training on Imagenet and testing performance on CIFAR or similar vision datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, I think it is a good paper with positive ideas and contributions. I particularly see the interest behind the application of NPTL in this context for transfer learning. To me, the paper is clear in the details concerning the sampling methodology, and perhaps not that much in the problems related to scalability or computational cost (see my comments below)."
            },
            "weaknesses": {
                "value": "In my opinion, I think there are several points that are not clear enough while reading the manuscript and they could be also potential weaknesses of the method.\n\n[w1] --- Access to the posterior distribution given pre-trained models. In general, I see the idea, but it is not entirely clear to me how we can get posterior samples from any model that has been pre-trained without a particular prior before. Are we in the MAP solution? similarly to the Laplace approximation. Are there conditions or requirements for pre-training the models?\n\n[w2] --- I appreciate the details and the sincere comments on the heuristics used and so on. However, I do not get a good feeling on the scalability and the computational cost. Is really the methods playing a role given huge models with large number of parameters? or is it just bc the pre-trained models are doing still well on similar vision tasks. In that regard, I'm not entirely convinced by the empirical results."
            },
            "questions": {
                "value": "see my prev. comments"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761592473,
        "cdate": 1698761592473,
        "tmdate": 1700222241247,
        "mdate": 1700222241247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RD2MMLB7ry",
        "forum": "vSwu81S33z",
        "replyto": "vSwu81S33z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces nonparametric transfer learning (NPTL), which uses a Dirichlet process prior with centering measure $F_{\\pi}$ defined by downstream samples and their outputs from a linear-probed model. Then each posterior sample defines a weighted loss of the downstream samples and pseudo samples from the linear-probed model. The weighted loss is optimized for each posterior sample. Then Bayesian model averaging is performed over $M$ posterior samples. Extensive experiments show the superior performance of the proposed method over traditional Bayesian sampling methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Using downstream samples and a linear prob model to construct prior for transfer learning is an interesting idea, and with Dirichlet Processes, the posterior gives a simple form. The paper provides extensive numerical experiments to support the superior performance of the proposed method. The limitation of Bayesian model averaging: the computational cost is also properly discussed."
            },
            "weaknesses": {
                "value": "I found some details of the proposed algorithm a bit confusing, especially in the context of transfer learning. I want to clarify the following points\n\n1. For the linear prob model, my understanding is that $\\phi^*$ is from the pre-trained model, how do we get $W^*$, is it obtained by only fitting the last fully connected layer on the downstream task?\n\n2. In step 7 of algorithm 1, how is $\\theta$ initialized, randomly or by $(\\phi^*, W^*)$, i.e. is the pre-trained model only used to create pseudo samples for the prior, or is it also used to initialize parameter as well. Looking at the training setting, e.g. ResNet-50, with a cosine learning rate decay schedule, it looks like the parameters are trained from scratch, otherwise, I imagine the large initial learning rate would drive parameters away from the initialization. \n\n3. For the SGHMC baseline, how is the prior specified and how does the pre-trained model help the SGHMC method? And how does the pre-trained model help the ensemble baseline?"
            },
            "questions": {
                "value": "Please see the questions in the weakness above. Some minor questions\n\n1. In section 2.1 paragraph 1. the zero mean isotropic Gaussian is called a non-informative prior. If I remember correctly, the non-informative prior usually refers to something else.\n\n2. For the Bayesian inference, one criterion of setting prior can be ensuring posterior concentration, e.g. in the sense of theorem 2.1 in [1]. Can the authors comment a bit on how the proposed prior related to those concepts?\n\n### Reference\n[1] Ghosal, Subhashis, Jayanta K. Ghosh, and Aad W. Van Der Vaart. \"Convergence rates of posterior distributions.\" Annals of Statistics (2000): 500-531."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821250287,
        "cdate": 1698821250287,
        "tmdate": 1699636804096,
        "mdate": 1699636804096,
        "license": "CC BY 4.0",
        "version": 2
    }
]