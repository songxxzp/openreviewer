[
    {
        "id": "6NcpfhI1xc",
        "forum": "o8tjamaJ80",
        "replyto": "o8tjamaJ80",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_sHE4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_sHE4"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new augmentation technique. First it proposes hard samples to train and secondly a robustification of the classifier. The method is evaluated on 4 datasets"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The idea to augment with hard examples is interesting. Furthermore, to iterate between augmentation and classifier is also interesting.\nShowed results are strong."
            },
            "weaknesses": {
                "value": "I do not see any significant weakness. The method is harder to implement and it requires more resources that other augmentation techniques, but given the timeline of augmentation, it is expected"
            },
            "questions": {
                "value": "None. I see this paper as a clear contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698611244920,
        "cdate": 1698611244920,
        "tmdate": 1699636809037,
        "mdate": 1699636809037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rmaDAurcs5",
        "forum": "o8tjamaJ80",
        "replyto": "o8tjamaJ80",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_xmzW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_xmzW"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an adversarial data augmentation strategy that builds on top of AutoMix. The framework alternates between training a mixed example generator and a target classifier. The mixed sample generator aims to produce hard examples and the target tries to improve generalization by learning robust features. With automatic mixup approaches (based on saliency or otherwise) the combination is deterministically selected, and there is no sample diversification. To mitigate this, the method proposes an adversarial generator instead. \nTo prevent a collapse from the generator, an EMA teacher and a weighted cosine similarity term between the mixed sample and individual samples is used for end-to-end learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Results are consistently better than AutoMixup and the evaluation (Table 1) is thorough.\n\n\n\n--------\nPost-rebuttal:\n\nThe authors have adequately addressed the concerns in the review. Useful experiments and ablations have been added as well. I'm still a little skeptical about the actual impact of the paper, from the methods and corresponding evaluation numbers in the paper I believe that we're at the point of diminishing returns. \n\nI've therefore increased my score to a 6."
            },
            "weaknesses": {
                "value": "There is no evaluation compared to Adversarial data augmentation approaches [1, 2, 3, 4]. At least an introduction or related works section should be added as relevant approaches to the problem.\n\nThe term \u201ccross attention module\u201d (CAM) should not be used as it can be confused with \u201cclass activation mapping\u201d (CAM) which is generally used in saliency-based data augmentation methods. \n\nSome notation is confusing - the encoder weight is updated with an EMA of the weights of the classifier - $\\hat{\\phi} = \\xi \\hat{\\phi} + (1-\\xi) W$. Is it unclear if the encoder refers to the generator or the classifier. Later near Equation 12, $\\psi$ is referred to as a target classifier with weights $W$. \n\nEquation 7 and 8 refer to the same value of $y$ used in cross entropy. It is better to keep the form of the loss consistent, since $y_{mix} = \\sum_i y_i \\lambda_i$, implies \n\n$\\sum_i L_{ce}(\\psi(x_{mix}), y_i) \\lambda_i = \\sum_i -\\lambda_i y_i \\log(\\psi(x_{mix})) = \\log(\\psi(x_{mix})) \\sum_i -\\lambda_i y_i = -\\log(\\psi(x_{mix})) y_{mix} = L_{ce}((\\psi(x_{mix}), y_{mix}))$\n\n\nEquations 10 through 15 are badly formatted and hard to read. It is also unclear what the individual contribution of the four cross-entropy terms are, and a suitable way to choose $\\alpha$ and $\\beta$. \n\nSection 4.2 mentions the proposed method has the lowest calibration error, but there is no table showing the ECE of other baselines. Fig.4. shows the ECE of only the proposed method.\n\n\nTypos and minor mistakes:\n\u201cto facility representation\u201d\nMxiup \u2192 mixup\nbad formatting  in eq 5\nnotation in eq 6, 7 is unclear. is the $*$ scalar multiplication?\nwhat is meant by \u201cinherent meaning of images\u201d - this sounds slightly unscientific, this should be explained in a bit more detail \n\nCurrently, I think the paper needs a lot of work - both in terms of coherence and motivation for the method. There are too many elements all over the place and it is unclear what the improvement actually comes from. The evaluation criteria is not standard (see Questions) and needs more justification. Therefore, I recommend an initial reject.\n\n_______\n\n\n[1] Zhang, Jiajin, et al. \"Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023.\n\n[2] Xu, Minghao, et al. \"Adversarial domain adaptation with domain mixup.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.\n\n[3] Zhao, Long, et al. \"Maximum-entropy adversarial data augmentation for improved generalization and robustness.\" Advances in Neural Information Processing Systems 33 (2020): 14435-14447.\n\n[4] Antoniou, Antreas, Amos Storkey, and Harrison Edwards. \"Data augmentation generative adversarial networks.\" arXiv preprint arXiv:1711.04340 (2017)."
            },
            "questions": {
                "value": "The method compares the median of the top-1 test accuracy in the last 10 training epochs. Since adversarial methods are generally brittle and have unstable training dynamics, does the mean test accuracy fluctuate a lot? Also, it seems that there is no validation set used to choose the best checkpoint. This evaluation criteria is not justified in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Reviewer_xmzW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628653128,
        "cdate": 1698628653128,
        "tmdate": 1700490294500,
        "mdate": 1700490294500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YJ5yQf8OwG",
        "forum": "o8tjamaJ80",
        "replyto": "o8tjamaJ80",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_vYYM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6937/Reviewer_vYYM"
        ],
        "content": {
            "summary": {
                "value": "Mixup data augmentations are widely used and usually require well-designed sample mixing strategies, e.g., AutoMix optimized in an end-to-end manner. However, using the same mixup classification loss as the learning objective for both the mixed sample generation and classification tasks might cause consistent and unitary samples, which lack diversity. Based on AutoMix, this paper proposes AdAutomixup, an adversarial automatic mixup augmentation approach that generates challenging samples to train a robust vein classifier for palm-vein identification by alternatively optimizing the classifier and the mixup sample generator. Meanwhile, the authors introduce an EMA teacher with cosine similarity to train AdAutomixup preventing the collapse of the inherent meanings of images. Extensive experiments on five mixup classification benchmarks demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* (**S1**) This paper provides an interesting view of improving mixed sample qualities through adversarial training in the close-loop optimized mixup augmentation framework. The overall presentation of the manuscript is easy to follow, and the proposed methods are well-motivated.\n\n* (**S2**) Extensive experiments on mixup benchmarks verify the performance gains of the proposed AdAutoMix compared to existing mixup methods. Popular Transformer architectures are included in experiments."
            },
            "weaknesses": {
                "value": "* (**W1**) More empirical analysis of the proposed methods can be added. Despite the authors visualizing the mixed samples and CAM maps of various mixup methods, it can only reflect the overall performances and characteristics of different methods. I suggest the authors provide a fine-grained analysis of each proposed module to demonstrate its effectiveness, e.g., plotting the classification accuracy of using adversarial training or not.\n\n* (**W2**) Small-scale experiments. The authors only provide comparison results on CIFAR-10/100, Tiny-ImageNet, and fine-grained classification datasets. More experiments on ImageNet-1K or other large-scale datasets are required. Meanwhile, the evaluation tasks or metrics can be more diverse, such as more robustness evaluations with adversarial attacks and transfer experiments to downstream tasks.\n\n* (**W3**) Some minor drawbacks in writing formats, and I suggest the authors take more time to polish the writing. As for Section 3, the arrangement of Sec. 3.1 and Sec. 3.2 can be reversed. Or the authors can provide a Preliminary section to introduce the background knowledge (e.g., mixup classification problem). As for equations, the text subscripts (e.g., $argmin_{\\theta}$, $L_{amce}$) should be in bold format, i.e., using `\\mathrm{}` as $\\mathrm{argmin}_{\\theta}$. As for tables and figures, there are some untidy arrangements, like Table 3, 4, and 5, and Figure 5 and 6. The author might zoom in on Figure 1 to show the detailed improvement of AdAutoMix.\n\n================== Post-rebuttal Feedback ==================\n\nSince the rebuttal feedback and revised version have almost addressed the weaknesses and concerns I mentioned, I raise my score to 8 and encourage the authors to further polish the writing issues and add more discussion of the limitations & future works."
            },
            "questions": {
                "value": "* (**Q1**) Do the authors provide the hyper-parameter settings of AdAutoMix (e.g., the mixing ratio $\\lambda$, the mixed sample number $N$, and $\\beta$ in Eq. (12)? The authors might provide a sensitivity analysis of the hyper-parameters in the Appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6937/Reviewer_vYYM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6937/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699236609127,
        "cdate": 1699236609127,
        "tmdate": 1700410011003,
        "mdate": 1700410011003,
        "license": "CC BY 4.0",
        "version": 2
    }
]