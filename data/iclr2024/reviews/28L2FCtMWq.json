[
    {
        "id": "RZizOHM0sn",
        "forum": "28L2FCtMWq",
        "replyto": "28L2FCtMWq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
        ],
        "content": {
            "summary": {
                "value": "this paper aims to address the challenge of multi-attribute editing in video editing. By incorporating the outputs of grounding models like GLIP and employing a designed attention mechanism, the proposed approach enables precise and temporally consistent video attribute editing. The authors conducted comparative experiments with recent methods, demonstrating superior performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper addresses a highly significant problem of achieving consistent fine-grained attribute editing in videos. The introduction of grounding conditions proves to be a direct and effective approach.\n2. In addition to the design of grounding conditions, the authors propose mechanisms such as noise smoothing and cross-attention."
            },
            "weaknesses": {
                "value": "This paper lacks substantial technical innovation as its main contribution lies in expanding the experiments on the input conditions, whether it is the depth or grounding results. The approach used for injecting conditions is based on ControlNet or cross-attention, which is a common practice in stable diffusion and related applications of ControlNet."
            },
            "questions": {
                "value": "1. The authors provided a visual comparison of the flow smooth effect in Figure 6, but it only includes a single example. Are there additional examples and comparisons with baseline and other methods (excluding flow smooth) demonstrating their effects under the same prompt?\n2. during the smoothing process, thresholds are introduced. Are there objective metric comparisons for different thresholds and experiments to evaluate the robustness of the thresholds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698494137191,
        "cdate": 1698494137191,
        "tmdate": 1700740657420,
        "mdate": 1700740657420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "coB6uPkEv4",
        "forum": "28L2FCtMWq",
        "replyto": "28L2FCtMWq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_TXKT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_TXKT"
        ],
        "content": {
            "summary": {
                "value": "Ground-A-Video proposes a training-free framework for grounded multi-attribute editing. The grounding ability is achieved by introducing the pretrained GLIGEN gated self-attention module into existing video editing pipelines. The paper also proposes several techniques including \"cross-frame attention\" and \"flow-guided latent smoothing\" to improve temporal consistency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Grounded video editing is a novel task. Bounding box grounding allows users to accurately select the regions to edit. It is a useful feature that allows better location controllability and content disentanglement over the existing popular sentence-level video editing works.\n\n2. The overall framework is training-free. The proposed framework is built upon several pretrained models like Stable Diffusion, Control Net, GLIGEN, and training-free inversion techniques like Null-text Inversion, so the framework itself requires no additional training on video data.\n\n3. The framework effectively associates editing prompts with the grounded areas. The results in the paper show better text alignment compared to previous sentence-level video editing approaches like Tune-A-Video, and ControlVideo."
            },
            "weaknesses": {
                "value": "1. Although under the same framework, the technical contributions are quite disconnected from one another and some of them might not be closely related to the grounded video editing task. Two main technical contributions (Modulated Cross-Attention and Flow-guided Latent Smoothing) in the paper lie in finding temporally smooth and faithful latent noise during inversion, which serves as a preparatory step and seems to be less relevant to the grounded editing task. On the other hand, the grounded editing capability mainly comes from the pretrained GLIGEN gated self-attention module, which brings limited addition to the previous image grounding task. \n\n2.  Although the proposed framework is training-free, it is still noteworthy that the per-frame null-text inversion requires gradient-based optimization on the null-text embeddings and could be time-consuming for longer videos. Moreover, the new Modulated Cross-Attention mechanism requires jointly optimizing all frames, which requires large memory.\n\n3. I am not very clear about the flow-guided smoothing after reading the description: does it only work on static areas? If not, why there are not any warping operations mentioned? I also find the terms \"spatially continuous\" and \"spatially discrete\" a bit confusing and hard to understand what continuous and discrete refers to.\n\n*For reproducibility, the code has not been released in the provided link at the point of this review."
            },
            "questions": {
                "value": "Please kindly address the questions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n.a."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727108455,
        "cdate": 1698727108455,
        "tmdate": 1699636056008,
        "mdate": 1699636056008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rZxjV8qT0T",
        "forum": "28L2FCtMWq",
        "replyto": "28L2FCtMWq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
        ],
        "content": {
            "summary": {
                "value": "The paper presents the \ufb01rst grounding-driven video editing framework, which is intended to solve the problem of neglected editing, wrong element editing and temporal-inconsistency in context of the complexities of multi-attribute video editing scenarios. Moreover, the proposed method is training-free which overcomes the obstacle of excessive computational cost on video tasks. Spatial-Temporal Attention, Cross-Frame Gated Attention and Modulated Cross Attention are introduced to further enhance consistency, depth map is used as an additional condition to better preserve structure and 3D information and the binary mask calculated through the optical \ufb02ow map can help maintain the consistency of the background area. E\ufb00ectiveness has been proven by su\ufb03cient experiments and convincing qualitative results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. This paper presents the \ufb01rst training-free grounding-driven video editing framework, which is relatively innovative.\n\nS2. The proposed method and experimental results are consistent with their motivation and e\ufb00ectively solve the problem of multi-attribute video editing in complex scenes.\n\nS3. The method is clearly stated, and the details are comprehensive."
            },
            "weaknesses": {
                "value": "W1. Since the introduced depth map and optical \ufb02ow map both re\ufb02ect pixel-level structural information, will this cause the structure before and after editing to be too consistent, so that if the foreground is replaced by an object with inconsistent structure, the editing result will be poor and lack of \ufb02exibility? e.g. replace the \u201crabbit\u201d in the phrase \"A rabbit is eating a watermelon on the table\" with an animal without long ears.\n\nW2. For similar reasons, this may also limit the editing method to the task of adding or deleting objects."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759176138,
        "cdate": 1698759176138,
        "tmdate": 1699636055937,
        "mdate": 1699636055937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n7zhcGcGsg",
        "forum": "28L2FCtMWq",
        "replyto": "28L2FCtMWq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on an interesting problem, namely multi-attribute editing in the video domain. The proposed method relies on recent techniques, such as GLIP, and ControlNet, and integrates the grounding information to perform a sequence of editing operations. The aim is also to propose a training-free pipeline by using pre-trained models in a zero-shot setting. For video-level editing, a stable diffusion backbone is extended for video data with DDIM inversion, and optical flow and depth maps are integrated as conditional to improve video editing quality. The evaluation is conducted on a set of videos and the evaluation is performed qualitatively with a comparison to SOTA and quantitatively by a user study with 28 participants."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic of paper, multi attribute editing in videos, is a challenging and interesting problem. \n\nThe paper seems mostly the integration and extension on available recent T2I pipelines for video domain. But, the selected models are recent and they fit well within the proposed pipeline. Moreover, the pipeline avoids additional training stages that is good for zero-shot pipeline."
            },
            "weaknesses": {
                "value": "The paper is a well-designed integration of mostly existing techniques for video editing pipeline. The authors explain the steps in subsections with details. However, I found the overall text flow confusing as the stable-diffusion model and the layers of it are explained in multiple  sections, e.g. 3.2 and 3.4, rather than a whole. Additionally, the integrations of the controlNet and optical flow into the whole pipeline are not clear. I think a revised figure with math representations consistent with the text may help to explain the model better. \n \nA more detailed qualitative evaluation of the model could be presented in the experimental section. For instance, to assess the impact of a particular component on the pipeline, the ablation section (Section 4.3) only includes the edited video outputs generated using with and without this component. However, this evaluation could also be conducted quantitatively (as in Table 1) to see the impact of some evaluated components on the pipeline."
            },
            "questions": {
                "value": "What is the video set used for comparison and evaluation in Table 1? \n\nHow similar are the paper's evaluation metrics with CAV (Chen et.al 2023)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909230254,
        "cdate": 1698909230254,
        "tmdate": 1699636055801,
        "mdate": 1699636055801,
        "license": "CC BY 4.0",
        "version": 2
    }
]