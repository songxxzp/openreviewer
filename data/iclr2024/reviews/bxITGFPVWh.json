[
    {
        "id": "c8bzfNmdlQ",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Twq5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Twq5"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a new approach, called Sharpness-aware Data Poisoning Attack (SAPA), that use loss landscape sharpness to improve the effect of data poisoning attack under the worst scenario (model re-training). The authors elaborate on achieving SAPA in targeted, untargeted and backdoor attacks. SAPA can be incorporated with existing data poisoning approaches and achieve better performance. Evaluations are performed on CIFAR-10 and CIFAR-100 over different tasks (targeted,  untargeted, backdoor) and the results show that SAPA yields better ASR compared to existing attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes an effective approach to tackle the long-last issue that poisoning attack effect may drop significantly due to worst-case training settings (e.g., re-training).\n\n2. The proposed SAPA is applicable to multiple attacks (i.e., targeted, untargeted, backdoor).\n\n3. The authors perform comprehensive experiments and ablations to show the performance of SAPA and the results are sufficient to support their claim that SAPA can enhance ASR by a large margin."
            },
            "weaknesses": {
                "value": "1. SAPA aims to solve the effectiveness of certain attacks (e.g., clean-label integrity and backdoor attacks) that suffer from re-training. On the other side, there existing a line of attacks (e.g., dirty-label attacks such as LIRA, WaNet, BadNets, etc.) that do not have such performance drop. I would suggest the authors clarify the discrepancy between different attacks.\n\n2. The authors only use CIFAR-10 and CIFAR-100 in the evaluations. However, in prior works such as the Hidden trigger backdoor and Gradient matching, ImageNet is always used to evaluate the attack performance. I suggest the authors also include ImageNet results in the experiments.\n\n3. It would be desired to evaluate SAPA against SOTA defenses. The authors provide the result against adversarial training, which is not the best option to defend against poisoning attacks. There are many defenses specifically designed for poisoning attacks such as ABL [1], Fine-pruning [2], Deep KNN[3], etc, it would be nice to evaluate SAPA against these defenses.\n\n[1]. Anti-Backdoor Learning: Training Clean Models on Poisoned Data\n\n[2]. Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks\n\n[3]. Deep k-NN Defense against Clean-label Data Poisoning Attacks"
            },
            "questions": {
                "value": "Please see weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Reviewer_Twq5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698161259347,
        "cdate": 1698161259347,
        "tmdate": 1699636658482,
        "mdate": 1699636658482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zd3pByR5pV",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_kdwN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_kdwN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data poisoning attack by leveraging sharpness-aware minimization. The method can be applied to backdoor, untargeted, and targeted attacks to improve performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces a unique approach using loss landscape sharpness to enhance poisoning attacks.\n\n2. The method can be applied to various attack settings. \n\n3. The performance shows the proposed method is effective against existing defenses."
            },
            "weaknesses": {
                "value": "The authors have improved the paper from their NeurIPS submission. The remaining concern is that many of the reported results are still not consistent with the results presented in the original papers without proper justification, including both Table 1 and Table 2."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698549526083,
        "cdate": 1698549526083,
        "tmdate": 1699636658368,
        "mdate": 1699636658368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Qfrgw8mds",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_X4g5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_X4g5"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a modification to the objective function of several poisoning attacks wherein poisons are crafted at a parameter vector that has been modified from a standard (ERM trained parameter vector) to increase the attacker's target loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors do a good job of comparing to a wide range of existing attacks in several poisoning settings. \n\n2. The authors also do thorough experimentation in each of these settings, and include some results under existing defenses.\n \n3. Taking the results of the work at face value, the proposed method improves over SOTA poisoning methods, sometimes by a significant margin. \n\n4. The paper is easy to follow and generally well presented."
            },
            "weaknesses": {
                "value": "1. I think the motivation of poisoning the \"worst\" retrained model isn't fleshed out enough. Why does crafting on the worst-case poisoned model intuitively lead to more generalizable/potent poisons on *average*? \n\n  2. The authors should be more careful when talking about sharpness of minima in this context, as sharpness depends on the *objective* in question. Many might read this work and assume the sharpness the authors are referring to is the sharpness of the minima of the \"standard\" loss landscape, when this is not what is being discussed. \n\n3. I would avoid statements like \"with a high possibility, the re-trained model can converge to a point where the poisoning effect presents\" , and \"Therefore, for the models which are re-trained on the poisoned dataset, the poisoning effect is also very likely to persist\" as these haven't been justified anywhere in the work.\n\n4. A concern raised about existing attacks  in the introduction was that they are not architecture agnostic. While I agree with this concern, this work does nothing to explicitly mitigate this. \n\n5. Figure 2 shows that when using ensembles, and restarts, existing methods can beat SAPA. Did you try SAPA with these additions? In general I'm not sure poison crafting time is something to include in the main body/claim as an advantage, especially when the worst times are ~20 minutes.\n\n6. Gradient matching also considers ensembles."
            },
            "questions": {
                "value": "1. Does SAPA also improve poisoning success when paired with objectives other than gradient matching? \n\n2. Eq. 7 is unclear - you craft $D_p$ on at a parameter vector $\\theta^* + \\nu$, but $\\theta^*$ was the result of ERM on $D_{tr} + D_p$? How was the initial $D_p$ (used in ERM) calculated? Do you have to generate poisons/train a victim model twice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698630011945,
        "cdate": 1698630011945,
        "tmdate": 1699636658262,
        "mdate": 1699636658262,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0S9DUbmRhb",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_jHfE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_jHfE"
        ],
        "content": {
            "summary": {
                "value": "Previous studies have developed different types of data poisoning attacks, but they are limited by the uncertainty of the retraining process after the attack. To address this challenge, the paper proposes a novel method called Sharpness-Aware Data Poisoning Attack (SAPA), which leverages the concept of deep neural networks' (DNNs') loss landscape sharpness to optimize the poisoning effect on the worst possible retrained model. Extensive experiments are conducted to show the method's effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of leveraging the concept of loss landscape sharpness to improve data poisoning's efficiency is intriguing. The proposed method is also applicable to different attack objectives.\n2. The paper includes vastly extensive experiments with different attack goals, model architectures, re-training variants, etc. \n3. Overall, the experimental results show this method could constantly outperform other baselines.\n4. The paper is quite well-writtenm. Especially, the proposed method is explained quite clearly and detailedly."
            },
            "weaknesses": {
                "value": "1. The attack success rates are quite unimpressive when the perturbation budgets are low (4/255 for targeted attack and 8/255 for backdoor attack). I recognize there are improvements compared to other baselines, but I find it quite hard to considered the attacks are successful.\n2. The number of defense strategies evaluated in the paper is quite lacking. All defense strategies considered are different re-training variants. While the adversary in this theat model acts as the data provider, I think there should be experiments with data filtering defenses, such as [1], [2], [3], [4]. There is also no poisoned model detection defense, such as Neural Cleanse or STRIP, mentioned in the paper.\n3. The performance against adversarial training is also quite underwhelming.\n\n[1] Chen, Bryant, et al. \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering.\" (AAAI 2019)  \n[2] Tran, Brandon, Jerry Li, and Aleksander Madry. \"Spectral signatures in backdoor attacks.\" (NeurIPS 2018)  \n[3] Hayase, Jonathan, et al. \"Spectre: Defending against backdoor attacks using robust statistics.\" (ICML 2021)  \n[4] Zeng, Yi, et al. \"Rethinking the backdoor attacks' triggers: A frequency perspective.\" (ICCV 2021)"
            },
            "questions": {
                "value": "1.  Regarding my concern about evalutation against more defense approaches, I would recommend adding experiments with some of the aforementioned defenses\n2. It would be better if there are qualitative comparison between clean and poisoned images.\n3. I find this sentence in section 5.5 confusing: \"In this part, we compare the efficiency and effectiveness trade-off between SAPA and E&R, when they are incorporated to existing defenses, such as Gradient Matching and Sleeper Agent.\" Do the authors mean \"existing attacks\" instead?\n4. Table 3 is quite hard to comprehend since it does not have best performances highlighted or average accuracy drops. The authors could improve its presentation a bit."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837267116,
        "cdate": 1698837267116,
        "tmdate": 1699636658159,
        "mdate": 1699636658159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fRSNCXiYrk",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Ae7N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Ae7N"
        ],
        "content": {
            "summary": {
                "value": "The paper induces sharpness-aware training method towards 3 poisoning tasks: targeted attack (perturb few data to misclassify a sample), backdoor attack (perturb few data to misclassify a class of samples), and unlearnable examples (perturb all data to misclassify all clean test samples). The key design is an additional step to calculate the worst poisoning model before using it to update the poison samples. Experiments show that when plugged in to existing methods, SAPA improves the poison performance steadily."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper extensively studies three poison attacks to showcase the effectiveness of SAPA. It would be very helpful for the audience since most papers only focus on one task but term the task as poisoning.\n2. The introduction of the sharpness-aware idea to poison attacks is straightforward and easy to plug in by adding an adversarial loop. But please make the additional computation amount (X + SAPA v.s. X) more clear.\n3. Experiments are comprehensive to demonstrate the steady improvement by SAPA, even under various training strategies. The study on efficiency is informative."
            },
            "weaknesses": {
                "value": "1. It would be good if the authors clearly distinguish between different poison tasks before introducing the method. Currently, the threat model is not clear for 3 tasks, and it may be confusing to distinguish the contribution of SAPA in a specific task.\n2. To better demonstrate the plug-in property of SAPA, it is good to be described without a specific attack method, e.g., gradient matching or error-min. And the results should be shown as X v.s. X+SAPA also in targeted and backdoor attacks.\n3. \"Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors\" also focuses on efficient and effective poisoning in retraining. How does SAPA compare to it in the unlearnable examples task in terms of attack performance and efficiency?"
            },
            "questions": {
                "value": "Response to rebuttal: Thanks for the good rebuttal and revision of the paper. I have no future concerns and thus keep my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Reviewer_Ae7N"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698882259350,
        "cdate": 1698882259350,
        "tmdate": 1700767195556,
        "mdate": 1700767195556,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5HaRfUsdzR",
        "forum": "bxITGFPVWh",
        "replyto": "bxITGFPVWh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Y6BU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6100/Reviewer_Y6BU"
        ],
        "content": {
            "summary": {
                "value": "This paper propses revisit exisitng \"end-to-end\" data poisoning attacks, finds that existing data poisoning attacks suffers from uncertanty issues in poison effects during the re-training process.  Then the reviewer proposes to leverage existing study on loss lanscape for DNNs and propose sharpness-award data poisoning attacks. Specifically, the authors improve upon previous work (e.g., Grad-Match, etc) with replacing original gradients with sharpness-aware gradients. Such sharpness aware loss is calculated by previous work. Through extensive experiements, sharpness-aware poisoning attackd can lead a mild improvement compared with existing approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method is intuitive and sound.\n\n2. The evaluation is comprehensive.\n\n3. The results are good."
            },
            "weaknesses": {
                "value": "1. The presentation needs improvement.\n\n2. Limited Novelty. The only contribution for this work is that the author combines sharpness-award loss function (proposed by previous work) with existing poisoning approach (e.g., Grad-Match) to make stablize the pisoning effects during the re-training peocess.\n\n3. Lack of theoretical analysis compared with previosu work on data poisoning attacks, such as witches brew (ICLR 2020)"
            },
            "questions": {
                "value": "Can you evaluate the transferability of poison effects, such as crafting poisoning samples in VGGNet but used for training ResNet.  Can you test your approach across different surrogated models and target models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6100/Reviewer_Y6BU"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698883426241,
        "cdate": 1698883426241,
        "tmdate": 1700709765362,
        "mdate": 1700709765362,
        "license": "CC BY 4.0",
        "version": 2
    }
]