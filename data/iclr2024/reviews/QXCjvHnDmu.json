[
    {
        "id": "353yGnlYhE",
        "forum": "QXCjvHnDmu",
        "replyto": "QXCjvHnDmu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission915/Reviewer_1qTf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission915/Reviewer_1qTf"
        ],
        "content": {
            "summary": {
                "value": "While LLMs are designed to provide safe and helpful responses, they can be manipulated for malicious purposes due to their alignment techniques. This paper introduces a genetic algorithm-based approach to manipulate LLMs even when their internal architecture and parameters are hidden. The motivation lies in the desire to expose these vulnerabilities and understand the limitations of LLMs by providing a diagnostic tool to assess and enhance their alignment with human intent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces an approach to manipulate LLMs in a black-box scenario. Many users and organizations interact with LLMs in a black-box manner. This approach has the potential to reveal vulnerabilities in LLMs.\n\nThe paper provides a thorough and well-structured account of how GA are utilized for the black-box jailbreaking of LLMs. This meticulous methodology description allows readers to gain a clear understanding of the research process, from the use of GAs for optimization to the formulation of fitness functions and the evaluation of attack success rates. \n\nThe use of publicly available datasets enhances the transparency and reproducibility of the research."
            },
            "weaknesses": {
                "value": "While the paper outlines the use of GA to optimize the universal adversarial prompt, it seems does not provide sufficient information on the initial selection of the adversarial suffix that constitute the core component of the method. \n\nGiven the potential ethical implications of the research, consider including more details about the ethical considerations and safeguards implemented to prevent misuse of the research findings.\n\nMinor Issues:\nThe reference should have been revised. Some preprints have been published, e.g., '(Certified!!) Adversarial Robustness for Free!' is published on ICLR 2023."
            },
            "questions": {
                "value": "Is there any existing work that can be compared? Like comparing to Zou et al.(2023) in the white-box case.\n\nLLaMA2-7b has a higher SR than Vicuna-7b. I'm curious what causes this difference, training/fine-tuning method? Dataset? It might be helpful to understand the Transferability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Reviewer_1qTf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697947683639,
        "cdate": 1697947683639,
        "tmdate": 1699636018531,
        "mdate": 1699636018531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pT6o1Xyn3v",
        "forum": "QXCjvHnDmu",
        "replyto": "QXCjvHnDmu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission915/Reviewer_HedS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission915/Reviewer_HedS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the use of genetic algorithms to jailbreak black box large language models (LLMs) in an automated way.  The authors provide an optimization-based formulation, and describe the steps of their algorithm in detail.  They also conduct a set of experiments using the harmful behaviors dataset to demonstrate the effectiveness of their method.\n\n\n**Summary of my rating.**  Based on the discussion in this review, my initial assessment is as follows.  This paper is among the first to propose an automated black box jailbreak for LLMs.  The setting is of interest, the authors can claim novelty in their problem selection.  The empirical results also seem quite strong.  However, the problem formulation and notation are somewhat loose/non-rigorous, the paper lacks comparisons to baseline algorithms (which they open the door to earlier on in the paper), the experiments admit methodological concerns, particularly WRT the training and test split, the discussion of related work is sparse, and the conclusions don't necessarily align with the evidence.  For this reason, despite the novelty of this approach, my assessment is that this paper is not yet ready for publication, and could be significantly strengthened in a subsequent draft."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Black box setting.**  This paper focuses on black-box attacks.  This setting is of particular interest, especially nowadays, since many popular LLMs are only available as black boxes.  If methods can be designed that reliably jailbreak black box LLMs, and particularly if they are relatively query efficient, this paper could have strong impact.\n\nJailbreaking LLMs is also a relatively new problem setting, and the authors are correct that they are among the first to propose a black box algorithm for accomplishing this task.  This constitutes novelty, and will factor into my overall assessment of this paper.\n\n**Experimental results.** At face value, the empirical results are quite strong.  The authors report ASRs in the mid-ninety-percent range for both Llama2 and Vicuna.  These numbers are notably stronger than white-box attacks like GCG (although the experimental set up is different, as discussed in more detail below).  The authors also report relatively strong transferability in the appendix.\n\n**Writing.** The paper is generally free of typos and grammatical errors."
            },
            "weaknesses": {
                "value": "### Problem formulation\n---\n\nThe problem formulation could benefit from a rewrite.  The notation is relatively loose/non-standard, and the writing could be polished.  Here are some comments:\n\n* In the equation following \u201cHence, the universal attack. . .\u201d on page 5, the optimization variable is not explicitly included in the objective, which will likely confuse readers.  \n* The authors use the words \u201clexicon\u201d and \u201cvocabulary\u201d to refer to the same set.  \n* The fitness function $\\mathcal{L}$ is defined in two different ways\n* Calligraphic notation is used for functions and for integers.  Consider using lower-case letters, e.g., $n$ and $m$, for your hyper parameters and calligraphic fonts for your losses, e.g., $\\mathcal{L}\n* It would be clearer if the authors used equation numbers.\n* The algorithms are written out in words.  It may be clearer if the authors use the notation used in the paper to write out the steps of the algorithm.\n\n### Lack of experimental evidence\n---\n\n**Comparisons to GCG.** Regarding comparisons to GCG, it\u2019s arguable that as the proposed algorithm is meant for the black-box settings whereas GCG is a white-box attack, a comparison is not needed.  However, there are a few reasons why I think a comparison to GCG is necessary for this work.\n\nFirstly, while discussing their past work, the authors say the following:\n\n> \u201cOur black box approach does not rely on a model\u2019s internals, and thus we do not need to deal with these kinds of difficulties. Other recent works of ours have focused on leveraging evolutionary algorithms in black box scenarios, showing that we can match or surpass white box results, without recourse to model internals, and using no costly gradient descent.\u201d\n\nIf a comparison between white-box and black-box algorithms had not been mentioned, one could make a stronger argument regarding why the comparison is not needed.  However, since the authors opened the door to this, and note that they offered such a comparison in past work, it seems reasonable to expect such a comparison here.\n\nSecondly, although GCG is a white-box attack, the authors of the GCG paper show that attacks optimized for Vicuna and Llama2 can be transferred to black box LLMs, resulting in non negligible ASRs.  In this way, one could view GCG as a black box attack for models like GPT, Claude, and PaLM when the suffixes are transferred from Llama2 or Vicuna.  \n\n**Black-box attacks.**  One of the main arguments in the paper is that the proposed attack works in the black-box setting, wherein the attacker doesn\u2019t have access to the parameters of the LLM.  The authors motivate this in the following way:\n\n> \u201cThough successful, the attack is limited because due to its white-box nature, meaning full access to the targeted model, including architecture, gradients and more. Such access is often not granted in real life.\u201d\n\nGiven this, one would certainly expect to see experiments on LLMs that are exclusively available as black boxes, i.e., models that are only available via API calls (e.g., ChatGPT, Bard, Claude, etc.).  However, the experiments are only performed on open-source LLMs\u2014specifically, Llama2 and Vicuna.  The paper feels incomplete without these experiments.\n\n**Concerns about performance.**  The scores reported by the authors are impressive \u2014 they claim to be able to achieve ASRs in the high 90% range for both Llama2 and Vicuna.  However, I think that more context is needed before these results are publishable.  Specifically, how was the ASR measured here?  That is, how did the authors determine whether or not a jailbreak occurred?  In the GCG paper, the authors check for various strings (e.g., \u201cI\u2019m sorry\u201d or \u201cAs an AI language model\u201d) in the response.  Was the same criteria used here?  If the authors used the same method as in the GCG paper, then the results would indicate that this black-box attack is *significantly* more effective than the white-box GCG attack, which is unintuitive, given that GCG has more information about the network at its disposal.\n\n**Repeats.**  The authors split the harmful behaviors dataset into training and test subsets.  However, it\u2019s arguable as to whether this split makes sense, especially because harmful behaviors contains numerous duplicates.  For instance, there are several dozen queries that\u2014using nearly identical language\u2014ask the LLM to generate instructions detailing how one would build a bomb.  Therefore, there is undoubtedly some mixing between the training and test set, and so the extent to which we should consider these attacks to be universal is questionable.\n\n### Paper feels unfinished\n---\n\n**Is this paper finished?**  This paper feels unfinished.  Aside from stating their results for direct and transferred attacks, there are no other experiments.  One would expect to see detailed ablation studies, including (1) testing different embedding functions, (2) measuring the query efficiency of the method, (3) comparisons to baselines (e.g., GCG), (4) a more detailed discussion/set of results regarding how to choose the hyper parameters, and so on. Table 2 is also deferred to the appendix, which seems strange given that the authors did not hit the nine page limit.  \n\nThe paper is also sparse on examples of prompts outputted by this algorithm.  The appendix contains several abbreviated examples, but given that this attack is black box and presumably query efficient, one would expect to see quite a few examples of this behavior on a range of LLMs.\n\n**Discussion doesn\u2019t align with experiments.** The authors claim the following:\n\n> \u201cOur results almost consistently demonstrated that, as the added prompt length increased, the effectiveness of the attack improved.\u201d\n\nThis doesn\u2019t seem to hold based on Tables 1 and 2, both of which show numerous parameter settings which lead to the opposite of the described behaviors.  That is, there are many cases where increasing $\\mathcal{M}$ (the prompt length) decreases the ASR.\n\nThe authors also say that\n\n> \u201cThroughout the generations we observed progressive improvement of generated outputs. . .\u201d\n\nI\u2019m not sure what \u201cprogressive\u201d means here.  If it means that as $\\mathcal{N}$ and $\\mathcal{M}$ increase, the ASR increases, as stated above, I don\u2019t think that this claim is emphatically backed by evidence.  If the authors mean something else, then at the very least it\u2019s unclear what this part of the discussion contributes.\n\n### Related work\n---\n\n**Self-citation.** The authors cite three anonymized versions of their past work.  I\u2019ve never seen this before.  It\u2019s unclear how I should evaluate these claims, given that there is no link provided to these papers.  Perhaps a better avenue would be to cite other works that (a) use genetic algorithms in the context of black-box attacks on deep learning systems and (b) are not written by the authors.  That way, the authors could mix in their own work with the work of others without having to anonymize.  This would give a more balanced view of related work, because as written, the paper reads as if the authors\u2019 anonymized works are the only relevant black box attacks which use genetic algorithms; a quite google scholar search reveals that there is plenty of work by authors at numerous different institutions.\n\n**Other LLM attacks.**  It would be worth at least mentioning that there are numerous other attacks (both white and black box) that have been shown to be effective on different LLMs.  \n\n* The website https://www.jailbreakchat.com/ lists quite a few black box attacks, although notably, most of these aren\u2019t automated.  \n* Aside from GCG, there are other white box algorithms that can jailbreak LLMs, including AutoPrompt (https://arxiv.org/abs/2010.15980) which accomplishes roughly the same goal as GCG (although it has been shown to be less effective).  See also GBDA (https://arxiv.org/abs/2104.13733), PEZ (https://arxiv.org/abs/2302.03668), TSP (https://arxiv.org/pdf/2302.04237.pdf), and https://arxiv.org/pdf/2306.15447.pdf. \n\nWhile you don\u2019t necessarily need to compare to these methods, it would strengthen the paper to include these papers in your related work section.\n\n**Discussion of other attacks.**  The paper could write about past work in a more positive way.  The authors say that \n\n> \u201cAnother problem with a white-box attack involves the enormous number of LLM parameters, resulting in very high GPU and memory consumption. Thus, a white-box approach is extremely costly. Moreover, due to the tokens\u2019 discrete nature, it is impossible to use standard gradient descent directly on the tokens and the algorithm needs to be drastically modified.\u201d\n\nWhat does \u201cdrastically\u201d mean here.  The coordinate ascent algorithm used in GCG is a relatively straightforward adaptation of gradient ascent, in that the only extra step is the top-k sampling used after computing the gradients in token space.  Indeed, given the use of the cross-entropy loss, one could argue that the attack is remarkably similar to attacks used in the adversarial examples literature, given that jailbreaking considers an entirely different problem over discrete tokens.\n\nRelatedly, the authors say that\n\n> \u201cNonetheless, the success of these attacks on the aligned models under scrutiny has proven to be somewhat limited (Kaddour et al., 2023). This limitation stems from the intricacies of optimizing discrete tokens for language-model attacks, as well as from the fundamental distinction that\u2014unlike in image-based attacks\u2014subtle textual perturbations are rarely imperceptible nor well-defined. In numerous classification tasks, e.g., sentiment analysis, this necessitates modifications to the attack to guarantee that token substitutions do not alter the underlying text class.\u201d\n\nAt this point, it\u2019s more or less established that attacks like GCG and AutoPrompt are quite effective at breaking aligned LLMs.  Moreover, is the fact that token-optimization-based approaches don\u2019t result in imperceptible perturbations a limitation?  These attacks\u2014to be the best of my understanding\u2014are not designed to produce imperceptible attacks.  Moreover, the proposed genetic-algorithm-based approach also has this limitation, in that a human can generally discern an unattached prompt from an attacked prompt.\n\n### Writing\n---\n\nThere are some weaknesses associated with the writing.\n\n**Bullet points.**  The bullet points on pages 1-2 are relatively vague, and the result is that they don\u2019t seem to carry much information content.  What do the authors mean by \u201cnuanced biases?\u201d Could you include a citation or definition or a broader discussion?  What does it mean for prompts to be \u201ccontextually relevant to trigger-specific responses?\u201d  \u2014 in particular, what is the \u201ccontext\u201d here?  What does it mean to \u201c*effectively* manipulate\u201d this context (emphasis is mine)?  What do the authors mean by \u201cinternal mechanisms\u201d and by \u201c*dynamic* model behavior (again, emphasis is mine)?  What are the \u201cevolving defense mechanisms\u201d mentioned by the authors? \u2014 could you cite something here?  Do such defenses even exist, beyond alignment-based fine-tuning/RLHF?  Overall, all of this text feels overly generic, and the paper could be improved by tightening this up with specific claims that are backed by citations and evidence."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Reviewer_HedS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698426815120,
        "cdate": 1698426815120,
        "tmdate": 1700683467810,
        "mdate": 1700683467810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TwOKgaNyxN",
        "forum": "QXCjvHnDmu",
        "replyto": "QXCjvHnDmu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission915/Reviewer_53vJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission915/Reviewer_53vJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a blackbox attack for jailbreaking based on Genetic Algorithm. The attack assumes no whitebox access to the weights or the logits. The attack aims to find adversarial tokens that when appended to a harmful prompt would cause the model to respond to the harmful prompts. These tokens are universal (not prompt-specific). Experiments were done on LLaMA2-7b and Vicuna-7b."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written. \n- Having a universal attack against blackbox models can be impactful and very important to study as it can work on closed models via APIs and that are used in other applications."
            },
            "weaknesses": {
                "value": "- The paper needs to report the number of queries that were done to find the adversarial suffix. The threat model assumes that an adversary would attack a blackbox model (e.g., via API). Such repeated queries might trigger detection (see: https://arxiv.org/pdf/2306.02895.pdf).\n\n- The paper could show stronger experimental evaluation. For example, the attack could be compared against a whitebox version as a baseline or an upper bound of a stronger attack. The paper also does not extensively evaluate the transferability (e.g., against the API models, or across more diverse options of source vs target models). This might help to understand when attacks are more successful/transferrable, also it would motivate the attack (is it more transferrable than optimizing on an ensemble of whitebox models?)."
            },
            "questions": {
                "value": "- how many unique adversarial tokens can be found? this is important because specific tokens can be filtered out. \n- what is the target sentences? is it a full continuation of the harmful answer or just \"yes, sure...\"?\n- How is the test set selected? In order to test if the attack is \"universal\" across different prompts, I think it makes sense to partition the examples by topic. \n- how is the success rate computed? the paper mentions that the attack succeeds if the target string is produced exactly. I imagine this has some limitations. Examples in fig. 4 do not show the output starting from \"Yes, sure...\". Also, is it possible that the model would start by \"Yes, sure...\" and not answer the harmful prompt? I imagine it might be possible especially when the length of the adversarial tokens increases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper contains an ethical statement about the attack. I think this is adequate."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Reviewer_53vJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698437376233,
        "cdate": 1698437376233,
        "tmdate": 1699636018367,
        "mdate": 1699636018367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oESMfJ5pkS",
        "forum": "QXCjvHnDmu",
        "replyto": "QXCjvHnDmu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission915/Reviewer_CXBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission915/Reviewer_CXBa"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the automatic generation of a universal adversarial prompt which is able to overcome alignment in open source LLMs under a blackbox setting. The paper proposes an approach based on genetic algorithms, in which sentences from a set of initial prompts are combined to maximize a fitness function. The paper utilizes a text embedding to define a fitness function which does not depend on model output probabilities. The method is evaluated on Llama-7b and Vicuna-7b models, demonstrating strong success rates on a test set of harmful behaviors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Paper works on interesting and challenging problems, with realistic consequences for deployment of black box open source LLMs\n- The methodology using GAs is unique, and powerful\n- The paper is intuitive and clearly motivated, with easy to follow exposition."
            },
            "weaknesses": {
                "value": "**Lack of experiments** - I am concerned about the lack of experimental results for larger models, or for more realistic models (e.g. Llama-7b-chat). The paper acknowledges this limitation in the future work section, but more results would still be appreciated; see questions for some possible experiments which would help in this regard."
            },
            "questions": {
                "value": "- I\u2019m wondering how easy it is for the method to generate different universal adversarial suffixes; Are there multiple possible solutions? Is this dependent on the choice of training prompts the universal prompt is fit on?\n- How long does the attack take (i.e. how many generations, and how long approximately in wall clock time per generation)? Would this be practical in a realistic scenario - especially for larger models?\n- Have you tried transferability to larger models or to closed-source models (e.g. ChatGPT?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission915/Reviewer_CXBa"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission915/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699275647951,
        "cdate": 1699275647951,
        "tmdate": 1699636018296,
        "mdate": 1699636018296,
        "license": "CC BY 4.0",
        "version": 2
    }
]