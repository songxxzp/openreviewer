[
    {
        "id": "2GVGf7QNJU",
        "forum": "y33lDRBgWI",
        "replyto": "y33lDRBgWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_CqXY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_CqXY"
        ],
        "content": {
            "summary": {
                "value": "This paper...\n- proposes AdjointDPM for differentiating through the diffusion sampling process,\n- reparametrizes PF ODE and augmented ODE to reduce numerical errors,\n- applies AdjointDPM to a wide variety of tasks, such as vocabulary expansion, security auditing, stylization, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- This paper is a nice application of adjoint sensitivity methods to diffusion models. To the best of my knowledge, such application of adjoint sensitivity methods has not been explored before.\n- The proposed method can potentially be applied to a wide variety of downstream tasks for diffusion models."
            },
            "weaknesses": {
                "value": "I must clarify that I am not very familiar with application of diffusion models to tasks such as vocabulary expansion, security auditing, etc. Hence, I am not sure whether the authors have chosen an appropriate and comprehensive set of baselines, or have followed proper evaluation protocol. So, my current score for this paper is \"marginally above the acceptance threshold\", and I will adjust my score based on other reviews and authors' reply to my concerns.\n\n- NFE and wall-clock time for AdjointDPM and the baselines is missing, so it is difficult to gauge the efficiency of AdjointDPM.\n- A background section explaining and comparing other related backpropagation methods, such as DOODLE, FlowGrad, DEQ_DDIM in detail would help readers understand the position of AdjointDPM w.r.t. previous work. Specifically, this paper lacks a discussion of theoretical and practical advantages/disadvantages of AdjointDPM w.r.t previous work on backpropagation through diffusion sampling. For instance, the authors state DEQ-DDIM require the diffusion sampling process to have equilibrium points -- is this a significant drawback? Are there certain tasks where this equilibrium assumption do not hold, so AdjointDPM is applicable while DEQ-DDIM is not?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Reviewer_CqXY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698400484193,
        "cdate": 1698400484193,
        "tmdate": 1699636167693,
        "mdate": 1699636167693,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ep4mPDIu6T",
        "forum": "y33lDRBgWI",
        "replyto": "y33lDRBgWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_5NNi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_5NNi"
        ],
        "content": {
            "summary": {
                "value": "This work leverages the adjoint methods for optimizing the parameters and/or samples of diffusion ODEs under a given differentiable scalar-valued function. To efficiently solve the adjoint ODE, this work also leverages the expoenential integrators and introduce a change-of-variable formula to obtain a simpler ODE. Experiments show that the proposed method can be used for classifier-based sampling, adversarial sampling and stylization with a single reference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is easy to understand and the writing is clean and easy to follow.\n- The proposed adjoint method is novel to the diffusion model community and the combination with exponential integrator is useful.\n- The studied topic is important to the field."
            },
            "weaknesses": {
                "value": "- Major:\n\n  - The proposed method seems to be quite **inefficient** because it needs to optimize the model / sample at each specific task, while other guided sampling methods (e.g., classifier guidance or classifier-free guidance) do not. Note that the optimization procedure needs to solve the whole ODE at each training step, the training cost (i.e., total training time) seems to be quite expensive.\n\n  - The proposed method cannot guarantee the property of diffusion models, i.e., the noise-pred network corresponds to the score functions, because it directly train the neural ODE. Thus, it may be hard to leverage the other properties of diffusion models, such as classifier / classifier-free guidance, and it may be hard to further use diffusion SDEs for better sample quality. Instead, other guided sampling methods introduce another guidance model at each time step, which is based on the score functions and thus can maintain the diffusion property.\n\n- Minor:\n\n  - Equation(9) is exactly the EDM sampler so it is not a new method. It should be compared and discussed in detail.\n\n[1] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, 2022."
            },
            "questions": {
                "value": "1. What is the training time for each experiment?\n\n2. After training, can the model be used for classifier / classifier-free guidance and diffusion SDEs?\n\n==========\n\nThanks for the detailed reply! After reading the authors' rebuttal, I raised my score to 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Reviewer_5NNi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824569226,
        "cdate": 1698824569226,
        "tmdate": 1700665535177,
        "mdate": 1700665535177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1DScnbxrW8",
        "forum": "y33lDRBgWI",
        "replyto": "y33lDRBgWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_6hcA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2354/Reviewer_6hcA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an interesting idea, named AdjointDPM, merging Diffusion Models and techniques from Neural ODE literature. The core offering of the paper is a way of backpropagating gradients of any loss computed using the output of a (trained) Diffusion model. Specifically the authors used the well-known Adjoint Backpropagation method from Neural ODE, which is a backprop algorithm with $\\mathcal{O}(1)$ memory w.r.t the *discretization* of the ODE solver.\n\nThe authors applied their AdjointDPM method on three tasks that either require gradients w.r.t initial state $X_T$ of the reverse process, all intermediate states $\\\\{ X_t \\\\}\\_{t=1}^T$ of the reverse process or the parameters $\\theta$ of denoising model $\\epsilon_{\\theta}(\\cdot)$. They showed good performance in terms of quantitative metrics and also showed qualitative results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposal of the paper is overall good, theoretically sound and shown to have worked well.\n\n- Theoretically, it makes sense to use the Adjoint method on the reverse ODE.\n- The authors exploited the semi-linear nature of the ODE even in the Adjoint backprop, following DPM-Solver/DEIS."
            },
            "weaknesses": {
                "value": "- While the proposal is quite novel, one might still argue that it is not really necessary to use Adjoint Backprop. One can very well accomplish the same task by backprop-ing through the solver machinery (maybe by deceasing sampling steps and using better sampler), which of course, won\u2019t be very efficient. So, at the end, it all boils down to compute/memory efficiency. While I understand the memory advantage, sadly, the paper barely talks anything about computational requirements of the method. BTW, Neural ODEs (and Adjoint Backprop) are known to be not very scalable.\n- Experiments are okay-ish, but not really extensive. Qualitative samples are lacking in some experiments (e.g. vocabulary expansion). Also Vocabulary Expansion is shown for only two classes.\n- No comparison or mention of methods that DO backprop through the ODE solver."
            },
            "questions": {
                "value": "I have the following questions for the authors.\n\n- I am confused about section 3.4. That is not AdjointDPM \u2014 that is just unconditional generation with an already known sampler (DEIS, which used Adam-Bashforth) that exploits the semi-linear nature. What part of this is your contribution ? Am I missing something here ?\n- In the \u201csecurity auditing\u201d application, what exactly is the guidance $L$ ? What is the meaning of \u201cdistance between a harmful prompt and a prediction score\u201d ? Also, what exactly is the NSFW filter $f(\\cdot)$ ? Can you provide more details please ?\n\nMinor questions or suggestions:\n\n- The unnumbered eq b/w Eq. 5 & 6 \u2014 what is the meaning of that line (\u201dSimilarly, for $\\theta$, we can regard ..\u201d) ?\n- \u201cadjoint state $\\mathbf{a}(t) = ..$, which represents how the loss depends on the state ..\u201d \u2192 \u201cadjoint state $\\mathbf{a}(t)$ = .., which represents how the loss **changes w.r.t the** state ..\u201d.\n- The AdjointDPM eq. 8 shows gradient w.r.t $t$ \u2014 is it acutally used anywhere ?\n- Why is it called \u201cvocabulary expansion\u201d ? I still don\u2019t get it.\n- \u201cSecurity Auditing\u201d is just fancy name for Adversarial Samples ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2354/Reviewer_6hcA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699049891602,
        "cdate": 1699049891602,
        "tmdate": 1699636167532,
        "mdate": 1699636167532,
        "license": "CC BY 4.0",
        "version": 2
    }
]