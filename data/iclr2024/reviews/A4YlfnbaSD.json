[
    {
        "id": "OGouznetNt",
        "forum": "A4YlfnbaSD",
        "replyto": "A4YlfnbaSD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_9Qg6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_9Qg6"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of stability gap [1] in the context of efficient continual learning with deep neural networks. As an effort to understand and mitigate the phenomenon, the work validates two hypotheses and their equivalent solutions: (a) the stability gap arising from larger loss values due to new output classes --  the mitigation strategy of which involves initializing the new class output class units with the mean of unit length class embeddings and using soft-targets for training the network; (b) the gap arising from excessive network plasticity in which case the level of plasticity of the different network layers are controlled in a dynamic fashion. The algorithmic contribution is demonstrated with a range of experiments using a pre-trained ImageNet-1K model.\n\nReferences:\n\n[1] De Lange, Matthias et al. \u201cContinual evaluation for lifelong learning: Identifying the stability gap.\u201d ArXiv abs/2205.13452 (2022): n. pag."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The work proposes a number of strategies to alleviate the negative effect of the stability gap. As such, their target domain involves the parameter regularization space (weight init, limiting the plasticity) as well as the function regularization space (using soft targets).\n2. The simplicity of the proposed weight initialization strategy is indeed impressive in light of its contribution towards bridging not only the stability gap but also the plasticity gap.\n3. Detailed supporting experiments considering the diverse settings of non-rehearsal, rehearsal, memory constraints, backbone architectures, etc."
            },
            "weaknesses": {
                "value": "1. As different weight initialization techniques tend to be prone to their specific limitations (e.g., a saturation of the activation function, suitability to sigmoid vs softmax activation, etc.), it would be interesting to see how the proposed initialization method stands against other existing ones (Kim's [2] for CL, He's [4] for non-CL) under these circumstances. Such a study could be beneficial to the continual learning community in understanding when (not) to use the proposed init method.\n2. The main table of results lacks a proper comparison with the existing literature [3]. \n2. Since LoRA restricts the number of trainable parameters in hidden layers, I believe that it might be susceptible to situations where if the lower layer parameters have been frozen at the earlier tasks, then it might affect the network's capacity to learn new low-level feature compositions for later tasks. This is indeed one of the reasons why freezing lower-layer features is not preferable in continual learning [2]. Can the authors comment on this? \n3. What concerns me is the efficacy of the proposed SGM method in terms of the computational overhead involved. While the work compares the computational efficiency in terms of the number of iterations required to overcome the stability gap (Figures 1 and 3), there seems to be no mention of the more common measures of computational overhead  (e.g. complexity, training time). For a practitioner, a few additional iterations of training might be preferable to a complex training algorithm with a lesser number of iterations yet a significantly longer training time. Can the authors comment on this?\n4. In Figure 3, it would have been better to also show the performance graphs in comparison to the rehearsal-based methods of Table 3 (DERpp and GDumb).\n\nReferences: \n\n[2] Kim, Sohee and Seungkyu Lee. \u201cContinual Learning with Neuron Activation Importance.\u201d International Conference on Image Analysis and Processing (2021).\n\n[3] Li, Depeng et al. \u201cIF2Net: Innately Forgetting-Free Networks for Continual Learning.\u201d ArXiv abs/2306.10480 (2023): n. pag.\n\n[4] He, Kaiming et al. \u201cDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\u201d 2015 IEEE International Conference on Computer Vision (ICCV) (2015): 1026-1034."
            },
            "questions": {
                "value": "Please see the weaknesses section. \n\nOverall, this is an interesting work in the direction of training efficiency in CL. However, my major concerns remain with the limited experimental comparisons with the existing literature. Given these clarifications in the rebuttals, I would be willing to increase the score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Reviewer_9Qg6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698629974019,
        "cdate": 1698629974019,
        "tmdate": 1699636420662,
        "mdate": 1699636420662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "voZRREs6Hv",
        "forum": "A4YlfnbaSD",
        "replyto": "A4YlfnbaSD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_EKNY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_EKNY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors aim to analyze and address the stability gap in class incremental learning. They put forth two hypotheses: first, that the stability gap arises partly from a significant loss at the output layer for the new classes, and second, that it is exacerbated by excessive network plasticity. The authors introduce two new metrics and conduct a series of experiments to investigate these hypotheses. Building on these ideas, they propose a novel method SGM to mitigate the stability gap. SGM combines four distinct techniques: (1) initializing weights of the output layer for new classes, (2) employing dynamic soft targets when learning a new task, (3) utilizing a network adaptor to limit the number of learnable parameters in hidden layers (LoRA), and (4) freezing weights in output layer for previously encountered classes. Techniques (1) and (2) target a reduction in loss for new classes, aligning with the first hypothesis. On the other hand, (3) and (4) aim to curtail excessive plasticity, in line with the second hypothesis. The authors present an extensive set of experiments to validate the efficacy of their proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The exploration of the stability gap is a compelling avenue that merits further exploration.\n- The authors have undertaken thorough experiments to substantiate their contributions."
            },
            "weaknesses": {
                "value": "- In section 3.1 (second paragraph), the rationale behind setting f=0.3 is not clearly elucidated. A more detailed explanation in the experimental section would be beneficial.\n- While the main experiments utilize ConvNeXtV2-Femto, which has undergone unsupervised pertaining on ImageNet1k followed by supervised fine-tuning, it would be advantageous to also present results for training ResNet18 from scratch. This is important given the common use of ResNet18 in the CL community.\n- Table 1 could benefit from including results from normal fine-tuning as a lower-bound reference. Additionally, explicitly reporting accuracy on ImageNet-1k is recommended due to the class imbalance between ImageNet-1k and Places365-LT. In light of this, could the authors discuss whether stability takes precedence over plasticity due to this imbalance? Furthermore, given the context of continual learning with a pre-trained model on a large dataset like ImageNet-1k, it would be beneficial to discuss why other relevant methods [1-3] were not investigated.\n- Related to the dataset imbalance issue, it would be interesting to examine and analyze the stability gap for the initial batch of Places365-LT.\n\n[1] Learning to Prompt for Continual Learning, CVPR 22 \n[2] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 22 \n[3] A Closer Look at Rehearsal-Free Continual Learning, CVPR 23 workshop"
            },
            "questions": {
                "value": "While the exploration of the stability gap is intriguing, my primary concern lies in the generalizability of the conclusions drawn from the experimental results, given the use of imbalanced datasets and an unsupervised pre-trained model with a large dataset. Even with the inclusion of experiments using a supervised pre-trained model in section 6.3, the findings may still lack generalizability, as the model might inherently learn more general features with a sufficiently large dataset. For example, this type of pre-trained model may possess good plasticity (transferability) without extensive weight updates. I would recommend the authors conduct experiments with a random initialized model and balanced tasks for a more comprehensive evaluation. Looking forward to the authors' response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4454/Reviewer_EKNY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763471637,
        "cdate": 1698763471637,
        "tmdate": 1699636420552,
        "mdate": 1699636420552,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bfgSXyzqo7",
        "forum": "A4YlfnbaSD",
        "replyto": "A4YlfnbaSD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_wwBV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_wwBV"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose to mitigate the issue of the stability gap for continual learning.  To achieve that, the authors first illustrate two hypotheses for the reasons that result in a stability gap: 1) high loss values for new classes; and 2) excessive network plasticity. Moreover, to examine the hypotheses, the authors introduce three stability metrics and further propose SGM to mitigate the stability gap. Specifically,  SGM combines weight initialization, dynamic soft targets, LoRA, and freezing of the output layer of old classes. In the experiments, SGM shows better accuracy and stability compared to prior methods on three benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It's good to see the author's efforts to address a recently proposed stability gap issue for continual learning\n2. The writing is relatively clear and easy to follow\n3. The proposed three metrics to measure the stability gap seem novel."
            },
            "weaknesses": {
                "value": "1. The extremely relaxed setting for continual learning limits the contribution of the proposed method.  For this setting, the continual learner has a strong pre-trained model and can access all prior source data for data replay(e.g., ImageNet-1K) which is unusual in continual learning.  \n   1.1 Such a setting narrows the proposed hypothesis for continual learning with the pre-trained model. Meanwhile, the proposed techniques are specially designed for that.  With the pre-trained model, some of the proposed techniques like weight initialization and LoRA are natural selections that are hard to be considered as contributions.    \n   1.2  Although the authors also conduct experiments on CIFAR-10 for training from scratch, the performance is not convincing compared to Vanilla Rehearsal only. \n   !.3  Such extremely relaxed settings still can not reach the accuracy of offline settings as shown in Table.1\n\n\n2. Lack of comparisons with most related works on continual learning with pre-trained models. In the paper, the authors mainly compare the proposed method to DERpp and GDumb which is designed for training from scratch. Thus, comparisons with the most recent works on continual learning with pre-trained models are needed. \n\nR1: A Unified Continual Learning Framework with General Parameter-Efficient Tuning (ICCV\u201923)\nR2:  Learning to prompt for continual learning. CVPR 2022\nR3: Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV 2022\nR4: Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference. AAAI 2023\n\n3. Lack of ablation study to show the performance of partially combined techniques in SGM.  In the Table.1, the authors list the ablation study of each component of SGM. Since the proposed SGM includes four different techniques, it's unclear what is the performance of partially combined techniques in SGM."
            },
            "questions": {
                "value": "Questions regarding the Weakness above:\n1. The extremely relaxed setting for continual learning limits the contribution of the proposed method. \n1.1 Why do the authors use an unusual setting for continual learning (i.e., a combination of ImageNet-1K and Places365 with special CL ordering)? \n1.2 It's better to conduct experiments in more general settings, for example, 1) with a pre-trained model:  a combination of ImageNet-1K and CIFAR-100 or ImageNet-R 2) without a pre-trained model: Split CIFAR-10/CIFAR-100 with the comparison with SOTA methods. \n\n3. Lack of ablation study to show the performance of partially combined techniques in SGM. For example, what is the performance if only combining weight initialization and LoRA? \n\n4. It's unclear why improving the stability gap can help to improve the final performance. For example, will it also mitigate the forgetting and improve forward and backward transfer capacity?\n\nMinor questions: \n\n1. \"Batches\" and \"Rehearsal cycles \" seem to have the same meaning but are used at different places on the paper.  It's better to keep the concept consistent to avoid confusion. In addition, the concept of \"Batches\" in the paper is easy to get confused with the hyper-parameter \"Batch size\" for training models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811226473,
        "cdate": 1698811226473,
        "tmdate": 1699636420465,
        "mdate": 1699636420465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "STH5jtDijO",
        "forum": "A4YlfnbaSD",
        "replyto": "A4YlfnbaSD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_2UBD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4454/Reviewer_2UBD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method to reduce the stability gap, which is a phenomenon that occurs in continual learning when learning new data, where accuracy on previously learned data drops significantly before recovering. The proposed method, Rehearsal with Stability Gap Mitigation (RSGM), consists of 5 different techniques: Weight Initialization, Dynamic Soft Targets, old output class freezing (OOCF) and LoRA. The experiment results show that the proposed method reduces the stability gap in class incremental learning and reduces the number of network updates."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Stability gap phenomenon is a recently discovered. Mitigating it can play a crucial role in enhancing the efficiency of continual learning.\n2. It propose new metrics, which differ slightly from those proposed by De Lange et al., to measure the stability gap in class incremental learning.\n3. It offers extensive ablation studies to facilitate a comprehensive understanding of the effect on each compoenent."
            },
            "weaknesses": {
                "value": "1. The proposed method is evaluated in constrained settings. The network is ConvNeXtV1-Tiny, which is seldomly used in previous continual learning literature. The applicability of the proposed method to other network architectures, such as ResNet or simple CNNs, remains unclear. Additionally, the buffer size is much larger than the one used in previous literature.\n\n2. The effectiveness in reducing the stability gap depends heavily on the utilization of a pretraining model from ImageNet-1K. As shown in Tab. 10, the improvement is very limited when training from scratch, due to the fact that LoRA and freezing of old class units cannot be used in this case."
            },
            "questions": {
                "value": "1. It is interesting that though the main goal of the proposed method is not to improve performance, it performs slightly better than the vanilla method. Can you explain this?\n\n2. Can you explain why the metric in (De Lange et al. 2023) is model-dependent and cannot be used to compare different approaches? In (De Lange et al. 2023). Please provide further elaboration on why the proposed new metrics are considered superior to the metrics presented in (De Lange et al. 2023).\n\n3. What does the \u2018Best Mitigation Method\u2019 shown in Figure 1 refer to? Is it the proposed RSGM?\n\nA suggestion: the term \"Offline\" may cause some confusion, despite the clarification in the main text. It would be clearer to use \"jointly\" training, instead as \"offline\" is commonly used to distinguish between online and offline continual learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830403524,
        "cdate": 1698830403524,
        "tmdate": 1699636420383,
        "mdate": 1699636420383,
        "license": "CC BY 4.0",
        "version": 2
    }
]