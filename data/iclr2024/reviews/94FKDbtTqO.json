[
    {
        "id": "3owTzhNHcl",
        "forum": "94FKDbtTqO",
        "replyto": "94FKDbtTqO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_kyqC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_kyqC"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the growing trend of applying large-scale NLP-style pretraining to life sciences, particularly in DNA sequence analysis. It highlights the limitations of existing methods, the advantages of K-mer overlapping tokenization in downstream tasks, and the introduction of \"RandomMask,\" a novel approach that significantly improves pretraining performance in life sciences applications, achieving remarkable results in epigenetic mark prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Thorough analytical background on the method"
            },
            "weaknesses": {
                "value": "Not much applicable to general machine learning, too specific in bioinformatics\n\n- In the description of Table 2, what are MCC and PCC?  The reviewer is aware that they are later explained in Experiemnt section, but what they are and what they do need to be briefly explained in the description of Table 2 as well for the readers who are not in life sciences field. \n\n- The authors say they progressively expand the masking boundary to prevent easy learning. Shouldn't it be the other way around? The reviewer believes that the masking boundary should progressively contract. What is the point of showing the shorcut first and complicating the learning? The model would already learn the shortcut if the masking boundary progressively expanded. An additional experiment on this needs to be conducted.\n\n---\n\n**Post Rebuttal**\n\nWell addressed!"
            },
            "questions": {
                "value": "Refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Refer to Weaknesses"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Reviewer_kyqC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698134084694,
        "cdate": 1698134084694,
        "tmdate": 1700955958740,
        "mdate": 1700955958740,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KyfeUtgURB",
        "forum": "94FKDbtTqO",
        "replyto": "94FKDbtTqO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_45WA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_45WA"
        ],
        "content": {
            "summary": {
                "value": "The author presented three observations by comparing the overlapping with non-overlapping tokenization method: (a) overlapping tokenization is always better than non-overlapping tokenization in the fine-tuning phase; (b) the loss overlapping tokenization method drops rapidly and result in a fast convergence of embedding space; (c) focused attention has been observed in the overlapping tokenization method. Both (b) and (c) indicate the overfitting problem in overlapping tokenization. The author proposed a novel RandomMask method to mitigate the overfitting issue by gradually increasing the complexity of the task. Based on the author\u2019s experiment, the RandomMask+BERT outperformed the other benchmarks algorithms."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[Originality and Significance] The author presented two original findings. One is that the overlapping tokenization always performs better in the fine tuning stage, regardless of the tokenization method in the pre-training. This is different from the conventional wisdom and could provide the insights for many related research and applications. The second contribution is that gradually increasing the complexity during the training could mitigate the overfitting issue and achieve better performance by combining fast convergence and generalizability. This method could inspire other researchers to consider similar techniques to balance the convergence and generalizability. The performance of the RandomMask algorithm is better than the precedent algorithms in different tasks, which is a significant result for the DNA sequence analysis. \n\n[Organization] The paper is presented in a well-organized way, from the observations to the new algorithms. Both the them bring new knowledge to the field."
            },
            "weaknesses": {
                "value": "[Clarity] The experiment and result section is relatively short. Some more clarifications would be helpful. For example, (a) when describing the baseline, the author mentioned \u201cAll models are trained on human genome and fine-tuned on the benchmark datasets with identical settings.\u201d The \u201cidentical\u201d setting does not bring clarity on how the mask is generated. Does this mean all the algorithms are trained and fine tuned with 15% token masked? This setting is slightly different than the original setting in the \u201cNucleotide Transformer\u201d paper. (b) \u201cFinetuning\u201d section discussed the dataset in a detailed way; however, the details of the algorithm setup in the fine tuning is not discussed. \n\n[Quality] The value of RandomMark (or training with gradually increased difficulty) can be better verified through experiments. For example, the author presented the result using the training with 5 phases. It would be great if the author could compare the result with different numbers of phases: one phase with maximum difficulty, two phases with two different difficulties, and etc."
            },
            "questions": {
                "value": "Q1: Algorithm 1 is not very clear to me. For example, if the r <= p at position i, [i \u2212 m/2 + 1, i + m/2] will be masked. The following nucleotide should be i+1, or I+m/2+1? If the next one is i+1, the mask range could be overlapping and can grow to be very long."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Reviewer_45WA",
                    "ICLR.cc/2024/Conference/Submission2654/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725428088,
        "cdate": 1698725428088,
        "tmdate": 1700692166001,
        "mdate": 1700692166001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fXj1hLDRFX",
        "forum": "94FKDbtTqO",
        "replyto": "94FKDbtTqO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_g7iT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_g7iT"
        ],
        "content": {
            "summary": {
                "value": "The authors of the paper initially conduct empirical experiments and analysis on the use of overlapping tokenization in BERT-like pretraining for DNA sequences. Their observations reveal that: 1) overlapping tokenization consistently enhances fine-tuning performance; 2) models trained with overlapping converge more rapidly; 3) overlapping can result in sparse attention within intermediate layers.\n\nThe authors claim that above observations demonstrate the limitation of existing overlapping tokenization. Subsequently, the authors introduce a dynamic overlapping strategy, referred to as RandomMask, for the pretraining of DNA sequences. Experimental evidence from a range of downstream tasks suggests that RandomMask consistently improves performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Extensive empirical results and analysis, providing some findings about overlapping strategy in DNA tokenization, could benefit the community.\n* The proposed method RandomMask achieves SOTA on various downstream tasks.\n* The proposed RandomMask is effective but simple. It could be easy to be re-implemented and deployed for further research."
            },
            "weaknesses": {
                "value": "While this paper provides extensive empirical results and quantitively demonstrates the effectiveness of RandomMask, there are several areas where it could be further enhanced. My main concerns are as follows:\n\n1. The authors might consider refining the focus of this work. The true contribution appears to be the improvement of the overlapping strategy tokenization for DNA pretraining, which diverges from the broader theme of \"rethinking the pretraining for DNA sequence.\"\n\n1. The motivation behind the study is somewhat unclear. Although the authors identify three potential challenges -- rapid convergence, the risk of under-training, and the potential for sparse attention -- they do not adequately explain how RandomMask addresses or mitigates these issues.\n\n1. There is a lack of experimental analysis supporting the source of the observed improvements, which is crucial for substantiating the paper's main claims. For example, besides the quantative improvements, does the rapid convergence and under-training still exist after applying RandomMask?\n\n1. The comparison in Observation 1 does not seem to be an apples-to-apples comparison. Overlapping represents more patterns and creates longer sequences for the same DNA length. It would be beneficial to understand if the conclusion holds for different lengths of k-mer.\n\n1.  The paper's presentation could be improved in several ways:\n    1. The introduction is somewhat verbose, indirectly causing the first two weaknesses and making the paper hard to read.\n    1. Placing Figure 1 and Table 1 on page 1 would improve readability, given that the main content describing Figure 1 and Table 1 is in the first page.\n    1. The separate table on the left in Table 2 appears to be redundant.\n    1. The experimental settings in Section 3 lack detailed descriptions, potentially making reproduction difficult and potentially misleading.\n    1. A thorough proofreading could enhance the clarity of writing and word choice.\n\n1. It would be beneficial to further explore whether sparse attention is indeed a problem for DNA sequence representation. Sometimes, sparse attention can improve generalization [1]. This might depend on different sub-sequences and the various functions of different layers when modeling cross-attention. I would appreciate further elaboration on the limitations of sparse attention in DNA sequence representation.\n\nI would appreciate the explanation and further evidence to address these concerns.\n\n[1] Correia, et al. \"Adaptively Sparse Transformers.\" Conference on Empirical Methods in Natural Language Processing (2019)."
            },
            "questions": {
                "value": "See Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2654/Reviewer_g7iT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698927418221,
        "cdate": 1698927418221,
        "tmdate": 1700739925610,
        "mdate": 1700739925610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0gERyqV2PL",
        "forum": "94FKDbtTqO",
        "replyto": "94FKDbtTqO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_SHyo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2654/Reviewer_SHyo"
        ],
        "content": {
            "summary": {
                "value": "The paper conducted an extensive study on using BERT-like models for pretraining with DNA sequences. Their experiments showed that using a tokenization approach that overlaps K-mers gives better results during the fine-tuning stage, regardless of whether the pretraining involved overlapping or not. However, they found that the commonly used method of overlapping tokenization during the pretraining phase caused the model to converge too quickly, which resulted in inadequate training.\n\nTo tackle this issue, the authors introduced the Random-Mask method. This method involves pretraining with dynamically changing the boundaries of the masked sections, which pushes the model to assimilate richer knowledge. They observed that when they expanded the mask boundaries during different training phases, there was a notable increase in the loss value. This increase in loss suggests that the model encounters new challenges and continues to learn, as evidenced by a downward trend in the training curve for each phase where the mask boundary is expanded.\n\nThey tested their approach on a total of 28 datasets spanning 7 downstream tasks. Across these tasks, we consistently achieved top-tier performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and presents its findings in a clear and logical manner, effectively explaining all observations and results. I especially like how it points out important observations step by step until it introduces the new technique. The graphs showing how the model's errors changed during training, attention maps and t-sne plots that help visualize the data made it easier to get what the paper is saying."
            },
            "weaknesses": {
                "value": "The evaluation section is lacking in clarity. It would be helpful to answer the questions listed below and help readers understand how RandomMask overall improves the performance of downstream tasks."
            },
            "questions": {
                "value": "(1) How does the RandomMask method compare with alternate tokenization approaches such as BPE (proposed in DNABERT-2)?\n(2) How does the RandomMask improve the internal representations - Can you see visible differences in embedding representations for downstream tasks (e.g Biotype Embeddings shown in HyenaDNA)\n(3) How does RandomMask compare on the benchmark datasets listed in  HyenaDNA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699002143275,
        "cdate": 1699002143275,
        "tmdate": 1699636205550,
        "mdate": 1699636205550,
        "license": "CC BY 4.0",
        "version": 2
    }
]