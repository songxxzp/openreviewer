[
    {
        "id": "xQgGylJjiv",
        "forum": "ZZCPSC5OgD",
        "replyto": "ZZCPSC5OgD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a new lip-to-speech method that leverages diffusion models via classifier and classifier-free guidance to reproduce accurate speech from silent videos. The base diffusion model is built upon the DiffWave architecture but generates mel-spectrograms instead. It receives a video of the speaker's mouth (encoded into feats via a lipreading backbone) and a still frame representing the speaker's identity (encoded via a simple ResNet) as the condition for generation. This condition is randomly removed during training to perform classifier-free guidance during inference, as proposed in many other diffusion models. After this is trained, the model is further guided via classifier guidance during inference so that the text extracted by a lip reading model from the video matches the text extracted via a speech recognition model from the generated audio. This model achieves SOTA performance on LRS2 and LRS3, and the design decisions are justified by a set of thorough ablations. Demos are also provided, which help contextualize these results empirically."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, I believe this paper is strong. It clearly sets a new state-of-the-art for lip-to-speech, which is a highly competitive field. \n\nI think the paper is well-written and the motivation for the task and each specific decision in the methodology is concise and meaningful. The methodology is clear and the discussion around the results is welcome. The model figure is adequate in my opinion, and the demos are also very welcome.\n\nThe method here is clearly novel - I don't think I've seen a lip-to-speech paper before with a similar architecture and that leverages classifier guidance from text in such an effective way. The choice for each model component makes sense and the training hyperparameters are described in detail to aid reproducibility. \n\nThe results are clearly strong and are compared with other works via subjective and objective metrics, which are very convincing. The ablations in tables 5, 6, and 7 are insightful and provide some further information about the importance of the weight of the classifier-free (w1) and classifier (w2) guidance, as well as the lip reading model that is used for the classifier guidance. The avoidance of intrusive measures such as PESQ or STOI is well-justified.\n\nThe limitations and social impacts are well addressed, and the conclusions are succinct and valid."
            },
            "weaknesses": {
                "value": "First and foremost, it is unfortunate that the authors do not compare directly with ReVISE in their tables, although this is fully justified by the lack of code, difficulty in reproducing their results from scratch, and the lack of samples for comparison. Therefore, I don't think it's fair for me or other reviewers to let this affect our judgment of the paper, as this is not the authors' fault. The presented model seems to compare favorably against ReVISE in the demos, which is encouraging.\n\nThe use of the DiffWave vocoder is reasonable, but it seems to be outperformed by HiFi-GAN and especially the recent BigVGAN. Would be interesting to see a comparison with these, or at least to justify why DiffWave was chosen, as HiFI-GAN is the typical choice in the majority of papers.\n\nIt would also be interesting to scale the model to larger datasets such as LRS3+VoxCeleb2 as was done in SVTS. This would help demonstrate the model's scalability to larger datasets, which is an important aspect of models in this field since audio-visual data is so plentifully available.\n\nI could not find any typos or substantial errors in the writing."
            },
            "questions": {
                "value": "The authors mention \" To encourage future research and reproducibility, our source code will be made publicly available\". Will this include training code, inference code, and pre-trained models? These would all be hugely helpful to the community in reproducing the authors' state-of-the-art results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1192/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1192/Reviewer_nskp",
                    "ICLR.cc/2024/Conference/Submission1192/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698306681732,
        "cdate": 1698306681732,
        "tmdate": 1700569237420,
        "mdate": 1700569237420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BsPSf71ZUb",
        "forum": "ZZCPSC5OgD",
        "replyto": "ZZCPSC5OgD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_gU1H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_gU1H"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to generate a natural-sounding speech from silent video called LipVoicer. The method is different from previous work in a two key ways: (1) the proposed method uses a lip reading model during inference to generate guidance for the generation model, (2) the generative model is based on a diffusion model. The model is trained on LRS2 and LRS3 datasets, which contains challenging examples from near in-the-wild conditions. The proposed system significantly outperforms the baselines.\n\nThe two key ideas actually appeared in accepted recent/concurrent papers, and authors acknowledge these works. Lip reading-based text guidance is proposed in (Kim et al., ICASSP 2023), although it is not exactly the same in that this paper uses the text guidance during inference, whereas Kim et al. uses the guidance during training. The use of diffusion-based model for the lip-to-speech task has been proposed in (Choi et al, ICCV 2023a). Authors are not required to compare their own work to that paper under ICLR rules."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The key ideas are reasonable, and well-engineered combination of proven methods.\n- The use of pre-trained state-of-the-art lip reading model significantly lowers the WER significantly compared to existing methods. \n- The diffusion model generates natural-sounding output, according to the qualitative results reported."
            },
            "weaknesses": {
                "value": "- It is not clear if the performance improvement comes from the key improvements, or the replacement of the vocoder, which can be seen as a post-processing step rather than a key part of the algorithm. It is well known that DiffWave produces much more natural-sounding output compared to the Griffin-Lim algorithm used by the previous works.\n- The authors request subjective assessors to rate Intelligibility, Naturalness, Quality and Synchronisation, but it is not clear what the difference between Naturalness and Quality are. There is a screenshot of the evaluation page in the appendix, but it does not make it clear what 'quality' means. \n- The baseline models appear to be using pre-trained model weights. However, the models are not trained on the same data, so the results cannot be compared directly.\n- The method appears to apply Guided-TTS techniques to the problem of lip-to-speech. Although this is well engineered, in my opinion this work is better suited to speech or CV conference compared to ICLR."
            },
            "questions": {
                "value": "- It is not clear why the addition of text guidance helps sync performance.\n- If lip reading networks are used, what is the advantage of the proposed system over a cascaded lip reading + TTS system apart from obtaining duration prediction from sync."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698589612081,
        "cdate": 1698589612081,
        "tmdate": 1699636045648,
        "mdate": 1699636045648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oaeHGe3PqA",
        "forum": "ZZCPSC5OgD",
        "replyto": "ZZCPSC5OgD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_x1vF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_x1vF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LipVoicer, which incorporates the text modality by predicting the spoken text using a pre-trained lip-reading network and conditioning a diffusion model on both the video and the extracted text. To utilize the text modality into the diffusion model, the authors apply classifier-guidance mechanism, where a pre-trained automatic speech recognition (ASR ) serves as the classifier. The results demonstrate the effectiveness of LipVoicer in producing natural, synchronized, and intelligible speech."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. LipVoicer greatly improves the intelligibility of the generated speech and outperforms existing lip-to-speech baselines on challenging datasets, demonstrating its superior performance. \n\n2. The paper provides detailed implementation details, making it easier for others to reproduce and further improve upon the LipVoicer method. \n\n3. By introducing a pre-trained ASR model, this paper realizes a good application of classifier-guidance diffusion model in lip2speech task."
            },
            "weaknesses": {
                "value": "1. After listening to Demo page, it is found that the gap between different models is mainly in sound quality. The baselines are too weak in sound quality. However, the problem of sound quality can be solved by many existing generative models based on VAE/GAN/FLOW model. If the sound quality problem of baselines is solved, the advantage of the model proposed in this paper may not be so great.\n\n2. In previous studies, a very important motivation for lip2speech tasks was to dispense with text modality (otherwise, this task can be transformed into lipreading+TTS), because 80% of the world's languages have no written text. However, this paper still depends on the text modality, so it is difficult to give a high score to this article."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760154384,
        "cdate": 1698760154384,
        "tmdate": 1699636045576,
        "mdate": 1699636045576,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NOdpAbVWa1",
        "forum": "ZZCPSC5OgD",
        "replyto": "ZZCPSC5OgD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_eBPV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1192/Reviewer_eBPV"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to perform the lip-to-speech task by incorporating the predicted text to guide the diffusion model based learning process. Experiments on the large scale LRS2 and LRS3 show its superiority over others. The results are indeed appealing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. The performance is good, with a large margin over other methods. It\u2019s also a nice try to include the predicted text into the learning process."
            },
            "weaknesses": {
                "value": "(1) I am a little confused with fig1.a. The output of the lipreading module is the predicted text. The output of the ASR modules is also the predicted text. There should be no connections from the output of the predicted text to the ASR module? The ASR module should take the output of MelGen as input? without the text predicted from LR module?\n(2) Lip2speech (Kim et al.(2023)) takes the ground-truth text as input to constrain the learning process and has shown the success of the role of text modality in this task. In this paper, the work uses the predicted text instead of the ground truth as Lip2Speech. But the manner is similar to Kim et al.(2022). So, besides using the predicted text with an existing method, is there some new contributions in the view of methodology?"
            },
            "questions": {
                "value": "(1) I am a little confused about the fig.1(a) as described above.\n(2) The modules and manners in the framework seems to be not new in the view of methodology, with the lipreading module, MelGen, text alignment manners already proposed by other works. Could the authors give a clarification of the contributions? Maybe I miss something?\n(3) The performance using the predicted text is already very appealing, but the involved lip reading model are almost the best two ones at present, with WER=19% and 26%. if the lip reading performance has been a much low value, e.g. WER=50%, what would be the performance here like?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699096339309,
        "cdate": 1699096339309,
        "tmdate": 1699636045492,
        "mdate": 1699636045492,
        "license": "CC BY 4.0",
        "version": 2
    }
]