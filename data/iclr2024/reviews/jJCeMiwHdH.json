[
    {
        "id": "qIUJCbMdPN",
        "forum": "jJCeMiwHdH",
        "replyto": "jJCeMiwHdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission989/Reviewer_L5qe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission989/Reviewer_L5qe"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to bridge multiple biological modalities through KG. The motivation is that most of the data are uni-modal, paired data is scarce and due to the combinatorial explosion, infeasible to collect and train a multi-modal model. The author learns a transformation layer among the uni-modal representations to achieve multi-modal learning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Interesting problem setting on bridging multi-modal FMs in biology. Makes intuitive sense.\n- Interesting approach on the bridge module\n- Lots of interesting applications and analysis"
            },
            "weaknesses": {
                "value": "- The method proposes several new modules that lack motivation and seem ad hoc.\n- The clarity of the technical methods of the paper can be improved"
            },
            "questions": {
                "value": "- Looking at equation 1, the transformation seems dependent on the (1) target modality node type and (2) relation type and (3) individual node in the target modality. For example, given the same node drug, it could have one transformed representation for protein A, a different one for protein B, and a different one for disease, etc. Is that right? Could the authors describe the design choice of this? why not map all of them into a single unified space? This brings a separate question: how do you conduct a similarity search if the embedding is dependent on individual node?\n- The transformation loss function (eq.2) makes intuitive sense, but there seem to be numerous other options to achieve similar goals such as self-supervised link prediction. Have the authors experimented with other transformation techniques? if so, could the authors provide any additional information? if not, could the authors describe the motivation on why choosing this particular approach? \n- The author uses a transformer model to achieve the transformation. It seems unnecessary since it is just encoding 4 embeddings? Have the authors experimented with other simpler approaches? \n- How are the negative samples created? \n- For individual application, is the model trained on every possible bridge transformation or is the model different for each application and individual task?\n- Cross-modality retrieval seems to be exactly link prediction. In that case, there are numerous approaches for GNN-based link prediction that is missing as a baseline and has shown much better performance compared to the KG embedding methods. Have the authors compared to any of the latest link prediction method? \n- For semantic similarity task, I worry that there is data leakage. Since the protein node is connected to these GO terms in the PrimeKG, the embedding already implicitly knows the labels during training. Have the authors addressed this concern by conducting some holdout protein-go links?\n- For PPI, since it is the same modality between the head and tail nodes, why do we need bridge module? \n- For cross-species task, why is phenotype not available? it seems to be available in PrimeKG? \n- For the multi-modal generation, it is an interesting application, but have the authors checked if the retrieved list is novel predictions or existing links? It will be great to compare with a baseline that is just retrieving the top K entities in the KG and show the difference on answers. \n\n\n\nI am happy to raise score if the authors address my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Reviewer_L5qe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission989/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688243854,
        "cdate": 1698688243854,
        "tmdate": 1700633441805,
        "mdate": 1700633441805,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Jbw3aUEKib",
        "forum": "jJCeMiwHdH",
        "replyto": "jJCeMiwHdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission989/Reviewer_xFRj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission989/Reviewer_xFRj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes BioBRIDGE to bridge unimodal FMs, keeping each FM fixed. BioBRIDGE utilizes a multi-modal knowledge graph to learn cross-modal relationships of entities. Compared to multi-modal FMs and ImageBind, BioBRIDGE is parameter efficient. The experiments on cross-modal retrieval tasks demonstrate the effectivness of BioBRIDGE. The paper also shows that BioBRIDGE has good generalization ability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of bridging several unimodal FMs is novel. \n2. The paper is well written and easy to follow.\n3. Compared to existing studies, BioBRIDGE keeps all unimodal FMs fixed and thus is parameter efficient."
            },
            "weaknesses": {
                "value": "1. The reasons of using contrastive learning is not clear. It would be better to provide further explanation and experimental supports.\n2. The baselines in Section 4.1 are several years ago. No recent studies are included."
            },
            "questions": {
                "value": "1. What are the contributions of the contrastive learning?\n2. How about the influence of the knowledge graph on this method? From your perspective, what could be the challenges if BioBRIDGE is adapted to other domains?\n3. For the cross-modality retrieval tasks, are the baselines trained on single modality or multiple modalities? Why not include multi-modal embedding methods published recently?\n\n    a. Lu X, Wang L, Jiang Z, et al. MMKRL: A robust embedding approach for multi-modal knowledge graph representation learning[J]. Applied Intelligence, 2022: 1-18.\n\n    b. Cao X, Shi Y, Wang J, et al. Cross-modal knowledge graph contrastive learning for machine learning method recommendation[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 3694-3702.\n\n    c. Zhu J, Huang C, De Meo P. DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment[J]. Information Fusion, 2023, 90: 111-119.\n4. Though it is easier to collect unimodal data than to collect paired data from two modalities, could the model trained on the paired data perform better or competitively to model trained on the large unimodal data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Reviewer_xFRj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission989/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725008770,
        "cdate": 1698725008770,
        "tmdate": 1699636024971,
        "mdate": 1699636024971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z0oRHoQJ6f",
        "forum": "jJCeMiwHdH",
        "replyto": "jJCeMiwHdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission989/Reviewer_VYFb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission989/Reviewer_VYFb"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces BioBridge, a novel framework for training across modalities (entities) that bridges independently trained uni-task models to establish cross-task abilities in biomedical domains. BioBridge employs contrastive learning to align entity representations and to facilitate learning of transformations between them. The resulting model demonstrates strong performance relative to several baseline knowledge graph (KG) embedding methods in retrieval tasks and highlights potential applications in the guided discovery of new drugs. Overall, BioBridge represents a promising approach for the integration of biomedical data resources and the enhancement of performance in various downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes a novel concept for learning across modalities via the bridging of knowledge graphs.\n- The authors have conducted extensive experiments on various types of entity mapping and numerous approaches to tail entity prediction.\n- With only the bridge module requiring updates during training, and all base feature models (FMs) remaining fixed, the proposed method is computationally efficient.\n- Overall, the paper is well-written, with clear explanations of the methodology and empirical results."
            },
            "weaknesses": {
                "value": "- The term \"multimodal\" as mentioned in this paper is confined to different types of biomedical entities. While the authors compare their work with \"ImageBind,\" the experimental section lacks tasks that bridge text and image modalities, which are more complex and crucial for multimodal foundation models.\n- The learning process is guided by knowledge graphs, limiting the scope of \"modalities\" to those represented within biomedical knowledge graphs. Therefore, instead of the broad term \"biomedical foundation model,\" it would be more accurate to describe it as a \"biomedical knowledge graph foundation model.\"\n- The paper does not present ablation studies, such as evaluations of the contrastive learning objectives or hyper-parameter tuning.\n- The case study focusing on molecule generation is intriguing. Quantitative assessments of generation performance would be beneficial, for instance, by making direct comparisons with general-domain foundation models, or by offering more qualitative examples to demonstrate the efficacy of the proposed framework.\n- The baseline comparisons are predominantly with knowledge graph link prediction methods. It is unclear whether the observed advantages stem from the effective transformation learning of the proposed method or from the knowledge supervision of the biomedical knowledge graph.\n- This paper omits a discussion on limitations and potential failure modes. The authors are strongly encouraged to offer deeper insights into the generalizability of BioBridge."
            },
            "questions": {
                "value": "- Can the proposed method be applied to other types of downstream tasks, such as image captioning and visual question answering? If applicable, could the authors provide empirical results and case studies?\n- Could the author offer a more detailed explanation of the compared baselines? Specifically, how are they trained, and can they also learn from an external biomedical knowledge graph to ensure a fair comparison?\n- Would employing different contrastive learning objectives, such as SimCLR or MoCo, in place of InfoNCE, impact the performance?\n- Also, please refer to weaknesses for other concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Reviewer_VYFb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission989/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698962901981,
        "cdate": 1698962901981,
        "tmdate": 1700655865432,
        "mdate": 1700655865432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iraaGuFLMG",
        "forum": "jJCeMiwHdH",
        "replyto": "jJCeMiwHdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission989/Reviewer_cgGU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission989/Reviewer_cgGU"
        ],
        "content": {
            "summary": {
                "value": "This submission tackles the problem of learning large multi modal ML models without requiring the pairwise cross modal dataset (as it is infeasible in where no of models >2). unlike recent work that aligns all modalities to a single modality, this submission takes a different approach of learning cross modal alignment transformation in embedding space while keeping the underlying unimodal fixed/frozen. Given Given an input embedding that was encoded by a unimodal model, the proposed submission transforms it to the embedding space of the target modality accounting for their relations. the cross modal alignment transformation is parametrized by a vanilla transformer module and learned with a contrastive loss. The proposed method is evaluated on several benchmarks/tasks such as protein protein interaction, protein-phenotype matching, cross modal retrieval where it outperforms several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the problem of aligning large unimodal models efficiently is very relevant in general and even more so when the modalities are proteins, drugs and diseases (focus of this submission) as it opens up plethora of clinical applications.\n\n- the problem is well motivated in introduction and contextualised. The paper is clearly written and easy to follow except one section (see below). \n\n- the idea of aligning the different embedding space of unimodal pretrained models with a cross modal transformation is sensible and simple. \n\n- experimental validation: evaluation of proposed s convincing on several benchmarks where the proposed method outperforms several baselines and in some cases is the only applicable solution. evaluation and applicability of the proposed method on cross modal retrival and gene-phenotype matching are quite interesting."
            },
            "weaknesses": {
                "value": "- Presentation of Related work : Currently the submission only has one paragraph on knowledge graph learning and barely describes the embedding alignment literature e.g. in the context of cross modal retrieval (one of the application of proposed method), one can also mention deep CCA literature as Canonical correlation analysis (CCA) is the core of many cross modal retrieval methods.  \n\n- Presentation of Methodology:  the submission should motivate the solution somewhat intuitively. Section 3.2 on encoding and transformation is very to the point and concise. the proposed methodology could be motivated better e..g by contextualizing wrt some prior work on KGE literature. Although there is a paragraph in the related work on KGE completion, the proposed method is not contexualized. Similarly, It is not very clear to me what parameters are optimized with contrastive loss since the submission keeps the pretrained model frozen. \n\n- Parameterization of alignment transformation as a transformer : the submission should also somehow motivate this choice wrt other options starting with simplest one such as a MLP or a vanilla autoencoder."
            },
            "questions": {
                "value": "Could the submission place the main assumptions and theorem from appendix into the main text? The appendix could include the proof.  I imagine this will make main paper more self contained and detailed. In the current form, methodology section is quite short."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission989/Reviewer_cgGU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission989/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698969146474,
        "cdate": 1698969146474,
        "tmdate": 1700662274353,
        "mdate": 1700662274353,
        "license": "CC BY 4.0",
        "version": 2
    }
]