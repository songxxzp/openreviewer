[
    {
        "id": "r1i31ADlgH",
        "forum": "LnLySuf1vp",
        "replyto": "LnLySuf1vp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
        ],
        "content": {
            "summary": {
                "value": "This work presents SpikeGCL which targets on optimizing GCL with SNN. They provide detail experiments to demonstrate the efficiency of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tConnect SNN and GNN is a very important topic, since GNN is closer to the neuron system and SNN is closer to the neuron dynamic.\n2.\tIt is interesting to combine feature dimension and temporal axis."
            },
            "weaknesses": {
                "value": "1.\tThe detailed background does not provide in background section, and there are too many existing work descriptions in the method sections.\n2.\tSome design choices do not present."
            },
            "questions": {
                "value": "1.\tIn Sec3, does the problem formulation special for this work, or it is general for all GCL application? Please make it clearly. \n2.\tI think author should formulate the GCL problem in background (instead of giving a brief introduction). Also, it is better to highlight which part is optimized by the proposed methods.\n3.\tWhy T encoders? Usually, neurons adopt the same weight among T time-steps, this design may increase the model size. Also, it is not clear how computation complex relates to time step size, since the author claim that they partition the feature dimension into T blocks. In my opinion, the computation complexity would not change when modifying T, which is not consist to Fig4.\n4.\tFig3(b) \u2018y1->y2\u2019. Also, it is not clear how the entire backpropagation work. whether all y1\u2026yn can directly receive gradient from the loss? Sec 4.4 introduces too much previous studies, it is better to clarify the gradient diagram in revision, i.e. for different blocks, where the gradient come from\n5.\tHow SPIKEGCL can reduce parameter size? Usually, SNN can reduce the activation size but keep parameter size unchanged. Author should provide a diagram of how to compute the parameter size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654736314,
        "cdate": 1698654736314,
        "tmdate": 1699636235332,
        "mdate": 1699636235332,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BzSZ4ISiU8",
        "forum": "LnLySuf1vp",
        "replyto": "LnLySuf1vp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
        ],
        "content": {
            "summary": {
                "value": "The paper presents SpikeGCL, a GCL framework built upon SNNs to learn 1-bit binarized graph representations and enable fast inference. The authors shows that Spike GCL achieves high efficiency and reduces memory consumption, and is also theoretically guaranteed with powerful capabilities to learn representations. Extensive experimental results verified that spikeGCL achieves comparable or superior performance to full-precision competitors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a new framework for learning on graph data, the spikeGCL. The paper is well written with clear introduction of the model and the learning algorithm to prevent the vanishing gradient problem, and presents both theoretical guarantees and extensive numerical results to demonstrate the capabilities of the model."
            },
            "weaknesses": {
                "value": "The paper focuses on the learning algorithm and performance of SpikeGCL, I think it would be interesting to further explore the properties of the 1-bit node representations themselves and compare them with other baseline models, to better understand why the learned graph representations are superior to other binary GNNs."
            },
            "questions": {
                "value": "How much does the result rely on detailed implementation of the SNN (such as reset to 0/reset by subtraction/IF or LIF)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718758730,
        "cdate": 1698718758730,
        "tmdate": 1699636235255,
        "mdate": 1699636235255,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OBngzI0iUZ",
        "forum": "LnLySuf1vp",
        "replyto": "LnLySuf1vp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_jihE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_jihE"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of learning full-precision representations in graph neural networks,\nwhich can be computationally and resource-intensive. The authors propose a new approach that\ncombines graph contrastive learning with spiking neural networks to improve efficiency and accuracy.\nThe proposed framework, SPIKEGCL, learns binarized 1-bit representations for graphs and provides\ntheoretical guarantees to demonstrate its comparable expressiveness with full-precision counterparts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is clear. The paper combines graph contrastive learning with spiking neural\nnetworks to improve efficiency and accuracy.\n2. The proposed method is tested on several benchmarks.\n3. The paper is well-written and provides a promising direction for graph contrastive learning with\nspiking neural networks."
            },
            "weaknesses": {
                "value": "1. The author divided the original graph in time in the feature dimension and obtained T graph\nstructures with the same structure and reduced the node feature dimension to N/T. Compared with\ncopying T copies, it saves storage resources. However, the author did not explain the reason for\nthis approach. For example, from my personal understanding, the author's approach can be\nunderstood as for a 1xd feature vector, there is a temporal relationship between the 0th value and\nthe N/T-th value, which we can\u2019t understand.\n2. The author used the SNN method to compress the original representation. One problem is that\nSNN considers the accumulation in time and does not take into account the distribution\ncharacteristics in time. What I mean is, if the characteristics of time T-1 and time T-2 are\nexchanged. It seems that the value of time T will not be affected, but they will become two\ncompletely different vectors. In this way, will there be a many-to-one situation during the\ncompression process?\n3. I think the article lacks some quantitative analysis, such as what is the connection between the\ncompressed binary vector and the original vector, what is the distribution of the conventional"
            },
            "questions": {
                "value": "Please see the weakness section for detailed questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823846893,
        "cdate": 1698823846893,
        "tmdate": 1699636235164,
        "mdate": 1699636235164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fo5JXcyc1I",
        "forum": "LnLySuf1vp",
        "replyto": "LnLySuf1vp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_n3GK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2917/Reviewer_n3GK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel graph contrastive learning (GCL) framework called SPIKEGCL, which leverages sparse and binary characteristics to learn more biologically plausible and compact representations. The proposed framework outperforms many state-of-the-art supervised and self-supervised methods across several graph benchmarks, achieving nearly 32x representation storage compression. The paper also provides experimental evaluations and theoretical guarantees to demonstrate the effectiveness and expressiveness of SPIKEGCL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper propose a novel GCL framework called SPIKEGCL that leverages sparse and binary characteristics to learn more biologically plausible and compact representations. \n2.\tThis paper provides theoretical guarantees to demonstrate the expressiveness of SPIKEGCL.\n3.\tSpikeGCL nearly 32x representation storage compression and outperforming many state-of-the-art supervised and self-supervised methods across several graph benchmarks. \n4.\tExtensive experimental evaluations to demonstrate the effectiveness of the proposed framework."
            },
            "weaknesses": {
                "value": "1.\tIn Section 4.1, to reduce the complexity of SNNs by sampling from each node, the authors uniformly partition the node features into T groups, which is unreasonable. Features of different dimensions may represent different meanings, and operations after grouping these features may lead to inconsistencies in the feature space between different groups. On the contrary, the traditional mask method retains most features by randomly masking some features, ensuring the consistency of feature distribution. Therefore, in this section, the author can consider using random masks to reduce computational complexity while ensuring the consistency of data distribution.\n2.\tIn table 2, the authors compare the parameter size and energy consumption between proposed method with traditional unsupervised/self-supervised mehtods. Howvere, from table 1, the performance of spikeGCL is worse than the spike-based mehtods in most cases, there\u2019s no evidence that SpikeGCL is better than other mehtods. The authors should add the comparision between SpikeGCL with spike-based mehtods in Table 2.\n3.\tAn intuitive question: Contrastive learning usually generates rich features from multiple perspectives to represent the target. However, spike-based methods usually lose a large amount of data, that is, a large number of learnable features are lost. Why can SpikeGCL still achieve similar results compared with traditional contrastive learning methods?\n4.\tGenerally speaking, combining Spiking and GCL is a good idea, but the novelty is not enougt. Compared with traditional methods, SpikeGCL only groups features and then uses the traditional GCL method for learning, which does not present the special nature of contrastive learning in the scenario where spike and graph are combined."
            },
            "questions": {
                "value": "check the comments above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825157801,
        "cdate": 1698825157801,
        "tmdate": 1699636235035,
        "mdate": 1699636235035,
        "license": "CC BY 4.0",
        "version": 2
    }
]