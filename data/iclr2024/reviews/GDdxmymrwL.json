[
    {
        "id": "2LlrFhQ7PN",
        "forum": "GDdxmymrwL",
        "replyto": "GDdxmymrwL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_Lg3j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_Lg3j"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes *Corex*, a framework to promote collaborations among LLMs as agents for solving complex reasoning problems. *Corex* is composed of three collaboration paradigms, Debate, Review, and Retrieve modes. In Debate mode, LLM agents are divided into two groups with one judge agent. The agents will start interactive discussions and modify their predictions. The refined predictions will be presented to the judge agent for decision. In Review mode, two agents will be involved, where one agent serves as the primary agent, and the other agent reviews the prediction and provides feedback. Finally, for the Retrieve mode, the retriever agent will examine all the candidates generated by other agents by providing confidence scores. *Corex* is evaluated on eighteen datasets of four reasoning categories, and demonstrates its effectiveness. Detailed analyses are conducted with respect to different LLMs, the number of rounds for interactions, and efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes multi-model collaborations for solving complex reasoning problems, which is an interesting and attractive direction for the current community.\n- A fairly diverse set of results across several benchmarks are provided, with several LLMs explored. The analysis is detailed and provides some insights into the proposed method. I especially like the analysis for cost-effectiveness.\nCode is provided for reproducibility.\n- The paper is generally well-written and organized, and most of the content is clear to me. Code is also provided for reproducibility."
            },
            "weaknesses": {
                "value": "- The proposed three modes are not that novel to me. There are many existing works that have already explored or at least share similar ideas to the three modes. I only list several representative works here. For Debate mode, [1] already explored this setting. For Review mode, [2] has a similar method. And finally for Retrieve mode, [3] also shares similar ideas. I don't think Section 2 Related Works is well-written either. It only lists related works without thoroughly discussing the relatedness and differences. Also, I don't see why \"External knowledge and tool utilization\" is that related to this work.\n- The performance improvement is not that consistent. It seems that in most cases, *Corex-Debate* and *Corex-Review-NL* do not perform that well. Instead, *Corex-Review-Code* and *Corex-Retrieve* seems to be better. I think it demonstrates the advantage of using PL, which is a well-acknowledged fact in the community. The paper also does not provide explanations or understandings of why these methods work well or not. I think it is important to have [1,2,3] as baselines and better explain why *Corex* works as a paradigm of multi-model collaboration. \n\n[1] Du, Yilun, et al. \"Improving Factuality and Reasoning in Language Models through Multiagent Debate.\" arXiv preprint arXiv:2305.14325 (2023).\n\n[2] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" arXiv preprint arXiv:2303.17651 (2023).\n\n[3] Yang, Chengrun, et al. \"Large language models as optimizers.\" arXiv preprint arXiv:2309.03409 (2023)."
            },
            "questions": {
                "value": "1. The notations and methods are somehow confusing in Section 3.1. For example, what is the decision-making process $h$ mentioned in Section 3.1? What does $k$ mean in the first paragraph of Section 3.1?\n\n2. There is some notable performance gap of *Corex* variants. Such as  *Corex-Review-Code* v.s. other variants for GSM-Hard in Table 2, and for Repeat Copy in Table 4. Any intuition or explanations on this?\n\n3. Have you tried the collaborations of different LLMs in the same mode? For example, using GPT-3.5-turbo, GPT-4, and Claude-Instant-1.2 as different agents for Retireve mode. Do you think there exists an issue of diversity when using the same LLMs as agents?\n\n4. It would be great to see the results for open-sourced models such as Llama.\n\n5. How to prove that *Corex* enhances the \"factuality, faithfulness, and reliability\"? Any case study or evaluation on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Reviewer_Lg3j"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698471702455,
        "cdate": 1698471702455,
        "tmdate": 1699636095968,
        "mdate": 1699636095968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yqcmh3ZuTU",
        "forum": "GDdxmymrwL",
        "replyto": "GDdxmymrwL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_kCUS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_kCUS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Corex - a suite of three general strategies for multi-agent collaboration: Debate, Review and Retrieve. The strategies aim to improve the factuality, faithfulness, and reliablity of the reasoning process. The Corex strategies are evaluated on 18 tasks from 4 categories, showing improvement over the baselines. Further analysis shows that the debate method converges after a small number of rounds, that the method fares better in terms of effectiveness/efficiency trade-off than other methods, and that the impact of the model size changes depending on the model role."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1) The coverage of the related work in the paper is very extensive and seems complete.\n\nS2) The paper lists a lot of challenges, which are indeed relevant, including misunderstanding the question, or generating a faulty reasoning process. Using multiple LLMs, treated as agents, is an exciting direction to explore to address these challenges.\n\nS3) The paper is overall relatively well-written and easy to follow.\n\nS4) The method should be relatively easy to reproduce based on the details provided in the paper. \n\nS5) The results are informative as they cover a large set of tasks and categories. The additional analysis sheds light on the method's behavior and computational efficiency."
            },
            "weaknesses": {
                "value": "W1) Contribution unclear - It is clear that the Corex variants fare better against the baselines, but it is less clear what is the main contribution of Corex. All three components are based on ideas that are present in prior work, and Corex does not integrate the components. So methodologically, it is indeed only a suite of what has been done before. To make the contribution further complicated, the abstract and the introduction mention (often vaguely) a number of issues, including \"the limitations of its internal representation\", \"limitations in solving reasoning tasks\", \"unreliable answers\", \"think outside the box\", \"prevalent obstacles\". The three issues that are specifically listed in Figure 1: wrong calculation, misunderstanding the relationship between variables, and codes fail to accurately reflect the problem statement - are actionable, but there is no analysis on whether the improved performance of Corex has a qualitative impact on any of these issues. The desired features reviewed in Table 1 (e.g., reference free, multiple LLMs) are again different from what the introduction was arguing. In the other sections, aspects like factuality, task-agnosticity, and reliability are argued for, but again, there was no experiment to validate these claims.\n\nW2) Comparison to baselines - the paper compares against one set of baselines in Table 1, then another set of baselines is referred to in the method section (e.g., Du et al. 2023 for the Debate module), and then the results and the analysis focus on general approaches like CoT and SC. It is unclear why the other baselines from Table 1 and prior works that designed the individual components are not included in the evaluation. This is even more important because Corex diverges from prior works to design these components differently (e.g., the Debate component), and it is important to know if this different approach fares better or worse, and why.\n\nW3) Originality - the proposed method reads like a more complete combination of heuristics compared to prior work, but these heuristics are already present in recent methods. In that sense, it is unclear what is the methodological delta between this method and prior work. This novelty gap is further blurred by the absence of a clear problem statement in section 1, and the lack of direct comparison to related work in section 2. \n\nW4) Premise - The overall premise of Corex is also confusing. If LLMs cannot reason reliably (as stated in section 1), then what makes these same LLMs suddenly able to reason in Corex? Moreover, in some of the Corex variants, like the Debate module, it is unclear what reasoning means exactly - because the design here explicitly opts for a majority voting to suppress reasoning. The Retriever compares different chains of thought, but whether these are scored based on their reasoning soundness, is not clear. \n\nW5) Result takeaways - The results often say \"our method\" but in fact Corex is a multitude of methods, whose probability of outperforming the baselines is generally around 50% (e.g., table 5 has 4 Corex variants and three baselines). Moreover, the best Corex variant is largely unstable over the tasks, though the Debate one is typically the weakest, while the Retrieve and Review-code are usually performing better. The results need a discussion that dives deeper into these distinctions in performance across tasks.\n\nMinor:\n* Retrieve is not a paradigm\n* Footnote 3 - not clear what is meant by the \"nature of commonsense tasks\" - prior works have used code representations to also address these"
            },
            "questions": {
                "value": "Q1) What exact problem(s) Corex is trying to solve, and how is this evaluated in the paper?\n\nQ2) Why are the baselines from Table 1 and other works that propose similar modules for debating, retrieval, and review not included in the evaluation?\n\nQ3) What is the main novelty of Corex compared to prior work?\n\nQ4) How do the authors explain the differences in the Corex variant performance across the tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Reviewer_kCUS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758405766,
        "cdate": 1698758405766,
        "tmdate": 1699636095888,
        "mdate": 1699636095888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i2ncMSJAPg",
        "forum": "GDdxmymrwL",
        "replyto": "GDdxmymrwL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_aQai"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_aQai"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a few strategies to make use of multi-agent communication for complex task-solving. Corex consists of three different strategies including debate, review, and retrieve. The authors experiment with OpenAI GPT as multiple agents and use the proposed methods to let the agents collaborate to solve tasks including math problems, commonsense reasoning, and symbolic reasoning and get improved performance compared to CoT and some of CoT variants."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The idea of using multi-agent collaboration to solve complex tasks is well-motivated and is a promising direction.\n2. The paper is in general well written and easy to follow.\n3. The experimental results show some improvement upon the compared baselines."
            },
            "weaknesses": {
                "value": "1. The paper lacks technical novelty. Multi-agent debate for reasoning and complex task-solving, the \"review\" method is also quite similar to recent work on self-reflection and self-refinement, the \"retrieve\" method is intuitively very similar to RAG (retrieval-augmented generation) and the main difference is incorporating it in the multi-agent framework using one agent as the retriever. \n\n2. The proposed components including debate / review / retrieve are not conceptually very much related. Instead they seem to be distinct methods. The authors also use them separately in the experiments without combining or integrating them into a single framework.\n\n3. The performance improvement over stronger baselines such as CoT-SC(10) is not very significant. Also, It is unshown whether variants such as CoT-SC(20/30) will lead to different conclusions. Maybe it's because the CoT-SC(10) baseline consumes similar number of tokens with OpenAI's API? But the authors did not show the total tokens consumed by different methods. And if so the comparison with CoT would not be fair enough.\n\n4. The manuscript lacks analysis of when (or for which kind of tasks) one of the methods among debate / review / retrieve outperform others and why it is the case. Adding some analysis about this question will bring more insights for the manuscript."
            },
            "questions": {
                "value": "See the above weaknesses for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698927494431,
        "cdate": 1698927494431,
        "tmdate": 1699636095798,
        "mdate": 1699636095798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eLOyNp1T9B",
        "forum": "GDdxmymrwL",
        "replyto": "GDdxmymrwL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_1uTs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1678/Reviewer_1uTs"
        ],
        "content": {
            "summary": {
                "value": "The main idea of this paper is to use multiple LLMs as if they are autonomous agents and let them interact with a prompting strategy that is structured into the Debate, Review, and Retrieve stages.  The multiple LLMs will collaborate using those modes of interaction to enhance the factuality faithfulness and reliability of the final answers.  The approach has been evaluated on multiple benchmarks including mathematical reasoning, symbolic reasoning, commonsense reasoning, and semi-structured reasoning. The collaborative approach is compared to exiting prompting strategies such as COT and self-consistency approachs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "--The idea of collaborative language models is very interesting and fairly novel.\n--Using the debate structure to guide the interactions is novel and effective.\n--The experiments are conducted on many and a variety of benchmarks with different tasks.\n--The analysis is interesting and insightful."
            },
            "weaknesses": {
                "value": "--The notation is a bit unclear in some places. In the very beginning explaining the debate,  what is k? Please denote this when you explain c^i_t.  c_i is the viewpoint or one step of the reasoning chain? Or both? Please make it explicit and use one term consistently.  \n\n--I expected the collaboration of multiple models to be based on 5 different LLMs. The paper uses GPT 3 and 4 and Claude.  It was not clear in the paper how the 5 different opinions were solicited. Are you using different temperatures or obtaining multiple samples from one LLM and looking at them as different heterogeneous agents? I see in the experiments that you use different LLMs to play the judge roles but was not sure if that is enough to have a real heterogeneous setting with multiple agents."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1678/Reviewer_1uTs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699634141526,
        "cdate": 1699634141526,
        "tmdate": 1699636095701,
        "mdate": 1699636095701,
        "license": "CC BY 4.0",
        "version": 2
    }
]