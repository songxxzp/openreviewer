[
    {
        "id": "KIcWYHIMbV",
        "forum": "hXYmlHxrQR",
        "replyto": "hXYmlHxrQR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_GoQM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_GoQM"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a BIQA model that is designed to emulate the perceptual mechanisms inherent to the Human Visual System (HVS) for the purpose of evaluating image quality. Specifically, the proposed method is composed of two components: 1) A quality-aware contrastive learning module; 2) A random mask mechanism. Experiemnts are conducted on 8 popular IQA databases."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ Rich experimental results are reported."
            },
            "weaknesses": {
                "value": "- The novelty of this work is limited for the following two reasons:\n\nFirst, contrastive learning has been extensively studied in the field of BIQA, except for the mentioned CONTRIQUE, at least two existing works are proposed in early 2023 [1,2].\n\n[1] Saha, A., Mishra, S., & Bovik, A. C. (2023). Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5846-5855).\n\n[2] Zhao, K., Yuan, K., Sun, M., Li, M., & Wen, X. (2023). Quality-aware pre-trained models for blind image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22302-22313).\n\nIn these previous works, the underlying motivation is to leverage unsupervised contrastive learning to enable BIQA model training on much larger data beyond the available IQA datasets with human quality annotations. This paper arrives at a similar approach from a seemingly different perspective. As stated in Sec. 3.3, this paper tries to use contrastive learning to better utilize the label information, resulting in a supervised solution. The explanation of this practice is very far-fetched, and the theoretical analysis is not rigorous either. In Theorem 2, the formulation of rank learning is not correct. Consequently, I think the use of supervised contrastive learning here is just doing something for the sake of doing it.\n\nSecond, the mask mechanism is a trivial application of MAE [3], but it is not even referenced in the manuscript. What makes things even worse is that the authors attempt to forcefully interpret this as a mechanism inspired by the HVS.\n\n[3] He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).\n\n- The combination of knowledge distillation and ground-truth labels is fundamentally flawed. If the pseudo labels generated by the teacher model are not accurate enough, this will definitely pollute the training process. However, if we already have a very strong teacher model, why should we additionally train a student model using such a complex scheme? For model compression? Apparently not in this work.\n\n- SRCC and PLCC range from -1 to 1, instead of 0 to 1.\n\n- It is very difficult to follow the definition of the hard-easy samples part. \n\n- The overall framework is an extremely entangled thing, from which I do not think the community can learn anything."
            },
            "questions": {
                "value": "It is unclear how the authors report the results of other methods in Table 1. Do all of them have the same training/val/test splits?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698155334879,
        "cdate": 1698155334879,
        "tmdate": 1699636801662,
        "mdate": 1699636801662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xl2wSbWz3N",
        "forum": "hXYmlHxrQR",
        "replyto": "hXYmlHxrQR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_8aSJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_8aSJ"
        ],
        "content": {
            "summary": {
                "value": "A contrastive learning approach is used to develop BIQA models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The performance seems to look great.\n2. The cross-dataset evaluation and the adoption of gMAD competition to verify the generalization is a plus."
            },
            "weaknesses": {
                "value": "1. The proposed method may be less motivated. For \"context,\" more fine-grained contrastive relationships between different quality samples have been extensively investigated in the field of IQA, under the learning to ranking formulation and starting from Gao et al.'s paper back to 2015. For  \"Sensitivity,\" the authors do not provide empirical evidence that ImageNet initialization + finetuning is weaker than their contrastive learning setting. The better performance of the proposed method may simply stem from the fact that a more powerful network architecture with much larger model parameters is adopted.\n\n2. The authors argue that ImageNet-like initialization is not good but start a pre-trained encoder based on ViT-S.\n\n3. Both the teacher and the student networks are trained. Which one is adopted for testing?\n\n4. The details of gMAD competition seem to be vague. Fig. 3 and Fig. 10 compare two models with the same setting, but some of the gMAD pairs are different.\n\n5. For each computational module, there are several hyperparameters, e.g., $\\tau$ in Eq. (4), $\\gamma_1$, and $\\gamma_2$ in Eq. (6). How do these affect the final performance?"
            },
            "questions": {
                "value": "1. Section 3.5 seems less relevant.\n\n2. 8:2 training: test set splitting without validation set may be an issue."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679187555,
        "cdate": 1698679187555,
        "tmdate": 1699636801543,
        "mdate": 1699636801543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YezCRw3Oya",
        "forum": "hXYmlHxrQR",
        "replyto": "hXYmlHxrQR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_xjPV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_xjPV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a Quality Context Contrastive Learning module to capture potential quality correlations in the global context of the dataset and a Quality-aware mask attention module to improve the model\u2019s perception of local distortions. Some experiments are conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper proposes a novel BIQA framework and achieves state-of-the-art performance.\n2.\tExtensive and sufficient experimental analysis demonstrates the effectiveness of the proposed methods"
            },
            "weaknesses": {
                "value": "1.\tLack of sufficient review of recent related methods.\n2.\tThe writing of the paper is not clear enough and not easy to understand."
            },
            "questions": {
                "value": "1.\tCVRKD-IQA [1] and UNIQUE [2] also learn image quality representations through the process of comparing the quality of different images. It is better to review them.\n[1] Yin G, Wang W, Yuan Z, et al. Content-variant reference image quality assessment via knowledge distillation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(3): 3134-3142.\n[2] Zhang W, Ma K, Zhai G, et al. Uncertainty-aware blind image quality assessment in the laboratory and wild[J]. IEEE Transactions on Image Processing, 2021, 30: 3474-3486.\n2.\tThe relationship between the part of knowledge distillation and the other lacks clarity. The authors may need to be added to some diagrams or descriptions.\n3.\tHow does the easy-hard sample mitigate overfitting? Can further explanation be given? Is there any relevant basis?\n4.\tWhat dataset was the teacher network pre-trained on? Is it the same data used for training the student network? As well as the fact that normally the teacher network is supposed to have better performance than the student network, it is clear in this paper that the student network is stronger than the teacher network, so the addition of the logit loss is supposed to reduce the performance. The authors may need further clarification on this point.\n5.\tIn the Ablation study, comparing index c) and d), index f) and g), and index i) and k), the performance gain from MA is almost negligible. Besides, there is a writing error: \u201cwe compare index b) and c)\u201d -> \u201cwe compare index a) and b)\u201d\n6.\tIt can be seen in Fig. 4, that although BEIQT is weaker than the proposed algorithm in MSE, the ranking is more accurate compared to the proposed method (the proposed method is incorrect in ranking the quality of image 2 and image 3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741391910,
        "cdate": 1698741391910,
        "tmdate": 1699636801413,
        "mdate": 1699636801413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FffmIla45t",
        "forum": "hXYmlHxrQR",
        "replyto": "hXYmlHxrQR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_GAJw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_GAJw"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces CSIQA ( Perceptual Context and Sensitivity in Blind Image Quality Assessment Algorithm).  The work aims to look into image quality from both a local and global context. They introduce a novel quality-aware mask attention and use a quality-aware contrastive learning module in this paper. The paper is well organized & written and complete with a substantial number of experiments/ ablation studies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces the novel quality-aware mask attention. \n2. Usage of distillation strategy to improve training\n3. First paper in IQA leveraging contrastive learning that tries to define hard vs easy positive and negative samples and study its effect on the training process. \n4. Decent attempt at providing mathematical proofs/intuitions behind the CSIQA design"
            },
            "weaknesses": {
                "value": "Comment on Paper Introduction :\nFigure 1 of the paper is misleading with respect to the state-of-the-art IQA methods in 2023/2024.  It is borderline incorrect to argue that current state-of-the-art methods in 2023/2024 use transfer learning using pre-trained ImageNet models. In 2021, we could agree, but not in 2023. I think the authors should cite the current state of the methods that employ contrastive/self-supervised learning (exemplar methods [1],[2],[3]) and discuss how each of the existing techniques differs from the proposed method clearly describing the shortcomings and issues addressed in this paper.  This will help readers better understand how this work is novel/different with respect to the existing results employing self-supervised learning. The authors should change Fig 1 as it is misleading with respect to the state-of-the-art in 2023/2024.\n\nComment on Contribution : \n#1 [1],[2]. [3] methods are already comparing across images, so broadly saying that existing methods have this as a limitation is untrue. \nInstead, the authors should argue that their networks were pretrained only on the ImageNet and not any other extra specific datasets used in [2],[3]. \n\nComment on Related Work\n[3] attempts to contrast based on local patch quality and is very similar to the proposed method; hence, it should be discussed in a related section on IQA with a local perspective. [4] another popular IQA method that studies local to global image quality needs to be discussed. \n\nComment on Results :\nThe results show superior performance of the CSIQA framework. However, the results shown are very similar to some existing methods, especially DEIQT. I think authors should conduct t-tests to verify if the differences in the results are statistically significant. Otherwise, it is difficult to conclude if this method performs better than existing methods. Authors are requested to conduct t-tests to verify the results are statistically superior as compared to DEIQT, [1], [2] and [3] and report them in the paper. Also, a performance vs complexity trade-off graph should be shown along with the results. Methods like [2], and [3] are ResNet-50 based, so the authors should compare objective results with the complexity of the proposed methods with other compared methods. Results in Table 12 attempt to compare the results of [2] with CSIQA. However, the results are pretty close, and again, conducting t-tests is needed to verify the statistical superiority of CSIQA. \n\n\n[1] Quality-Aware Pre-Trained Models for Blind Image Quality Assessment, Zhao et al. CVPR 2023 \n\n[2] Image Quality Assessment  Using Contrastive Learning, Madhusudana et al., 2022 IEEE TIP 2022\n\n[3] Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild, Saha et al. CVPR 2023\n\n[4] From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality, Ying et al. CVPR 2020"
            },
            "questions": {
                "value": "Question on Positive/Negative Sampling Strategy :\n\nThe positive and negative examples obtained are based on some fractions (<20% vs >60%). Consider an anchor image I and another image J; the images I & J can be positive and negative samples based on the other images in the batch. Please explain how this inconsistency should be regarded as a valid sampling strategy. \n\nThe same holds for easy vs complex example sampling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Reviewer_GAJw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257864961,
        "cdate": 1699257864961,
        "tmdate": 1699636801298,
        "mdate": 1699636801298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VGV6nCZeAm",
        "forum": "hXYmlHxrQR",
        "replyto": "hXYmlHxrQR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_NFhk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6891/Reviewer_NFhk"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a novel image quality assessment method. It relies on the use of contrastive learning for better emulating HVS proficiency in comparing image quality. To mitigate overfitting, it exploits an hard negative mining strategy within a knowledge distillation framework. Furthermore, a patch-level masking strategy is applied to enforce the model focusing on local distortions rather than semantic information. Experiments on eight benchmark datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well written and organized.\n+ The achieved performance highlight the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The proposed method lacks innovation, and it is simply the combination of existing approaches (token masking, model distilling, hard negative mining, and contrastive learning).\n- The manuscript lacks an explanation of how various design choices improve the representation obtained peculiarly for IQA.\n- The improvement in correlation is marginal compared with the first SOTA method (i.e. DEIQT)."
            },
            "questions": {
                "value": "- How does the easy-hard sample mitigate overfitting? Can the authors provide further details and explanations?\n- Why do authors change the batch size according to the cardinality of the dataset? Above all, what sense does it make to decrease the batch size without complementarily decreasing the learning rate?\n- Are the authors sure about the correlation range [0,1]? Is not possible to obtain negative correlation for Pearson and Spearman?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I believe this submission does not require any further specialized ethics review."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6891/Reviewer_NFhk"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6891/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699640467558,
        "cdate": 1699640467558,
        "tmdate": 1699640467558,
        "mdate": 1699640467558,
        "license": "CC BY 4.0",
        "version": 2
    }
]