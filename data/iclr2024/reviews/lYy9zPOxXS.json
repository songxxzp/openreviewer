[
    {
        "id": "eHLwRPOOq0",
        "forum": "lYy9zPOxXS",
        "replyto": "lYy9zPOxXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_Cu2Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_Cu2Y"
        ],
        "content": {
            "summary": {
                "value": "Current graph transformers lack the ability to distinguish isomorphisms of graphs, thus affecting the predictive performance of the methods. To address this problem, this paper proposes Topology-Informed Graph Transformer (TIGT). TIGT contains four components: a topological positional embedding layer, a dual-path message-passing layer, a global attention mechanism, and a graph information layer. Also, a mathematical analysis of the discriminatory ability of TIGT is given in this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.This paper is well structured.\n2.In this paper, experiments are conducted on several datasets and a rich theoretical proof is given."
            },
            "weaknesses": {
                "value": "1.Lack of review of related work and comparison of TIGT with other work.\n2.A graph is vertex-biconnected if it is connected and does not have any cut vertex. The definition of vertex biconnected in this paper is wrong.\n3.The case used in Appendix A.2 for the proof of Theorem 3.2 is too particularized, so that the generalization of the theory and the proof cannot be guaranteed.\n4.Missing ablation of the component \u201cglobal attention layer\u201d in ablation study.\n5.The performance improvement of TIGT is not significant and in many cases it is not as good as GRIT."
            },
            "questions": {
                "value": "1.Can you describe the main innovations of your method compared to previous graph transformer methods?\n2.The theoretical proof of Theorem 3.1 in Appendix A.1 is highly similar to the reference[1], can you specify the innovation of the theory in this paper?\n3.Are the values shown in Table 1 the F1 value? How are the results obtained for the TIGT achieve 100% with variance 0.0? How are the results obtained for GRIT+RRWP?\n\nReferences:\n[1]Yun Young Choi, Sun Woo Park, Youngho Woo, and U Jin Choi. Cycle to clique (cy2c) graph\nneural network: A sight to see beyond neighborhood aggregation. In The Eleventh International\nConference on Learning Representations, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698140437873,
        "cdate": 1698140437873,
        "tmdate": 1699636260060,
        "mdate": 1699636260060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DBdJnyWVLD",
        "forum": "lYy9zPOxXS",
        "replyto": "lYy9zPOxXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_apGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_apGs"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Topology-Informed Graph Transformer (TIGT), a novel graph transformer architecture designed to improve the discriminative capability for detecting graph isomorphisms. The TIGT model consists of four key components: a topological positional embedding using cyclic subgraphs of graphs, a dual-path message-passing layer, a global attention mechanism, and a graph information layer. Experimental results on various graph classification tasks illustrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing of this paper is easy to follow.\n- The problem of strengthening the discriminative power of distinguishing isomorphisms of graphs is crucial.\n- This paper conducts extensive experiments."
            },
            "weaknesses": {
                "value": "- Originality/Novelty: The main theoretical results (Theorem 3.1 and Theorem 3.2) are mainly based on previous work (Choi et al., 2023).\n- The assumptions made in Theorem 3.3, which requires two graphs to have the same number of nodes and edges, and Theorem 3.4, which assumes that all graphs have the same number of nodes with degree $d$, may have limitations in practical applications. \n- It would be valuable if the authors could discuss the presence of cyclic structures in graph datasets and its impact on the proposed architecture's performance, whether it is missing or not.\n- Ablation study: The authors have provided an ablation study for key components of the method. However, the topological positional embedding layer, the main contribution of this paper, appears to have a marginal effect.\n- Although the authors have provided an analysis of the computational complexity for the proposed method, they have not provided empirical results regarding running time compared to baselines. I would suggest the authors also measure the running time of the method and other baselines.\n- Baselines: Some baselines on graph transformer are either missing or not adequately discussed:\n1. Kong, Kezhi, et al. \"GOAT: A Global Transformer on Large-scale Graphs.\" ICML 2023.\n2. Zhang, Zaixi, et al. \"Hierarchical graph transformer with adaptive node sampling.\" NeurIPS 2022."
            },
            "questions": {
                "value": "See weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695572634,
        "cdate": 1698695572634,
        "tmdate": 1699636259982,
        "mdate": 1699636259982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oV3nipWykz",
        "forum": "lYy9zPOxXS",
        "replyto": "lYy9zPOxXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_8s8z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3132/Reviewer_8s8z"
        ],
        "content": {
            "summary": {
                "value": "The authors have introduced an innovative Graph Transformer designed to enhance its discriminative capabilities. This paper demonstrates the ability of the proposed method to effectively distinguish graph isomorphisms through a novel dual-path message-passing layer. Both experimental and theoretical findings substantiate the authors' assertions. Moreover, the study delves into a novel positional embedding layer, aiming to harness topological information more efficiently within the Graph Transformer framework. Experimental evaluations further underscore the method's proficiency in graph-level benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Performance.** The introduced methodology exhibits outstanding performance, notably excelling on the CSL dataset and outperforming the expressive power of contemporary Graph Transformers.\n2. **Theoretical Development.** Comprehensive theoretical exploration confirms that the proposed approach encompasses and advances beyond current graph transfomers."
            },
            "weaknesses": {
                "value": "1. **Novelty.** This work is merely a simple adoption of Cy2C-GNNs with multi-head attention to package that as a graph transformer. The authors present the same layer, except for edge features E^{l-1}, as two different layers in Section 3.1 and Section 3.2. They are called Topological positional embedding layer and Dual-path MPNNs, respectively. After getting node features, then some multi-head attention-based graph transformer.\n2. **Scalability.** The authors discuss the scalability of the proposed method regarding the number of nodes, edges, and edges in cyclic subgraphs. The method has a quadratic complexity in terms of the number of nodes. This indicates that the method does not scale well, and it might be difficult to apply it to large-scale graphs.\n3. **Applicability.** The authors showed the effectiveness of the proposed methods only on graph-level benchmarks. As the authors mentioned, the demonstration in other-level tasks, such as node classification/clustering, link prediction, and community detection, is needed to check the applicability."
            },
            "questions": {
                "value": "1. Provide more baselines, including GNNs. Especially the proposed method is very similar to Cy2c-GNN and the key module is from that paper, but the authors did not compare their method with Cy2-GNN in Table 1, where the proposed method show significant improvement against graph transformers. In Table 2 and 3, Cy2-GNNs are missing. \n2. Please provide more implementation details of Cy2-GNN-1 and compare its computational cost and model [parameters.Is](http://parameters.Is) it fair to compare between TIGT and Cy2-GNN-1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769727881,
        "cdate": 1698769727881,
        "tmdate": 1699636259877,
        "mdate": 1699636259877,
        "license": "CC BY 4.0",
        "version": 2
    }
]