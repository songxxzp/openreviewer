[
    {
        "id": "R3gRlPIi0R",
        "forum": "Z73ymB1C7G",
        "replyto": "Z73ymB1C7G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_cDNu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_cDNu"
        ],
        "content": {
            "summary": {
                "value": "This work present a novel framework of DynVideo-E that for the first time introduces the dynamic NeRF as the video representation for large-scale motion- and view-change human-centric video editing. With a set of customized designs and training strategies, it outperforms SOTA approaches by a large margin on human preference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Long term video editing consistency has been improved by a large margin;\nQualitative video results shows great performance gain."
            },
            "weaknesses": {
                "value": "The whole customized process is a bit lengthy with a lot of customization, which potentially makes the reproductivity difficult."
            },
            "questions": {
                "value": "Could you explain a bit more about the part of Text-guided Local Parts Super-Resolution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Reviewer_cDNu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5791/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667367802,
        "cdate": 1698667367802,
        "tmdate": 1699636609508,
        "mdate": 1699636609508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0tsc60Qqnx",
        "forum": "Z73ymB1C7G",
        "replyto": "Z73ymB1C7G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_6Wks"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_6Wks"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a dynamic NeRF-based approach to handle video editing in 3D space. The proposed method uses a deformation field to propagate the edits to the entire video. The authors introduce several design improvements that enhance the editing performance. The experimental results demonstrate the effectiveness of the proposed approach, as shown by both qualitative and quantitative evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, easy to follow, and features illustrative figures that effectively convey the concepts. \n\n2. The proposed method is well-motivated and adeptly addresses existing limitations by employing 4D representations for video editing. This approach integrates motion and view changes, while the deformation field guarantees consistency throughout the edited video.\n\n3. The experimental evaluation is comprehensive, offering both qualitative and quantitative evidence that demonstrates the superiority of the proposed method over baseline approaches."
            },
            "weaknesses": {
                "value": "1. One limitation of the proposed method is the requirement for calibrated camera poses, which may not be readily obtainable for all videos. This constraint could potentially restrict the applicability of the approach in certain scenarios or require additional preprocessing steps to estimate camera poses.\n\n2. Similar to other recent NeRF-based generation and editing methods, the proposed approach can be time-consuming. This factor may hinder its adoption in real-time applications or situations where rapid editing is necessary.\n\n3. As a NeRF-based approach, the edited videos should ideally support free-viewpoint rendering. However, the qualitative results presented in the paper only show editing results with the aligned timestamp and viewpoint as the input video. This aspect raises questions about the method's ability to generate consistent and accurate results across different viewpoints, which is a key advantage of NeRF-based approaches."
            },
            "questions": {
                "value": "1. COLMAP can sometimes fail with moving objects in the scene for camera calibration. It would be helpful if the authors could provide more details on how to run COLMAP in such cases and if there are any specific parameters or settings that can be adjusted to improve its performance.\n\n2. It would be beneficial if the authors could provide an analysis of the time required for a single editing operation using the proposed approach. This information would help to understand the practical feasibility of the proposed method for real-world applications.\n\n3. It would be interesting to see an edited result with an arbitrary camera trajectory or a static human subject while the camera is moving (e.g., bullet time effect) instead of only showing results with viewpoints aligned to the input video. This would highlight the versatility of the proposed method and its potential for a wide range of applications.\n\n4. As another dynamic NeRF-based approach for video editing, the paper does not mention the work by Zhang et al., \"Editable free-viewpoint video using a layered neural representation\" (SIGGRAPH 2021). It would be helpful if the authors could provide a brief comparison between their proposed method and the approach presented in Zhang et al.'s work in the related works section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5791/Reviewer_6Wks"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5791/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829072310,
        "cdate": 1698829072310,
        "tmdate": 1699636609412,
        "mdate": 1699636609412,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pr0ZtGItHB",
        "forum": "Z73ymB1C7G",
        "replyto": "Z73ymB1C7G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_6oK3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5791/Reviewer_6oK3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for consistent editing of large-scale motion- and view-change human-centric videos. Specifically, the proposed method exploits dynamic NeRF for the video representation, and integrates several techniques including the Score Distillation Sampling (SDS) from both 2D personalized diffusion priors and 3D diffusion priors, reconstruction losses on the reference image, text-guided local parts superesolution, and style transfer for 3D background space. The experiments validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is clear and easy to follow.\n+ Multiple existing techniques are combined into the whole pipeline."
            },
            "weaknesses": {
                "value": "- About the novelty: The paper is a system paper that combines several existing works, including HOSNeRF, Zero-1-to-3, and Magic123, without too much novel insight. For example, the basic video representation follows the existing work HOSNeRF, and the only difference is the removal of object state designs for the specific task in the paper. Both 3D and 2D priors follow the existing works, Zero-1-to-3, and Magic123. From these points, the novelty of the paper mainly lies in the integration of such existing works.\n- About the application scenario: The proposed method relies on the dynamic human NeRF reconstruction, making it limited to human-centric video and less interesting. \n- About the experiment: A quantitative comparison is also expected for the ablation. Moreover, only a rather small dataset is utilized for the test, and more videos in the wild are expected."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5791/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699291302562,
        "cdate": 1699291302562,
        "tmdate": 1699636609311,
        "mdate": 1699636609311,
        "license": "CC BY 4.0",
        "version": 2
    }
]