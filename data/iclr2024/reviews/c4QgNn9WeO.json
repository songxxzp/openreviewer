[
    {
        "id": "H0PIyqs4OP",
        "forum": "c4QgNn9WeO",
        "replyto": "c4QgNn9WeO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_RpWw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_RpWw"
        ],
        "content": {
            "summary": {
                "value": "Recognizing the challenges of training Multimodal Large Language Models (MLLM) from scratch, the authors propose LMEye, an interactive perception network that acts as a \"human-like eye\" for LLMs. In contrast to previous methods, LMEye facilitates a dynamic interaction between the LLM and external visual information, akin to Visual LLM Agent . This is accomplished by enabling the LLM to specify the kind of visual information it needs based on the human instructions it receives. The LMEye system is composed of a basic visual mapping network for initial image perception and novel RVII modules that can receive visual information requests from the LLM, process these requests, and subsequently relay the interacted visual information back to the LLM. This method ensures that the LLM not only receives visual data but can actively seek out specific visual information in alignment with human instructions. Experimental evaluations on various multimodal benchmarks reveal that LMEye notably enhances zero-shot performance on diverse multimodal tasks (MM-Bench and SEED-Bench), suppressing prior methods, and doing so with fewer parameters (LMEye-4B vs. MLLMs >7B)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Overall Novelty and Contributions: \nThe LMEye approach stands out as a monumental stride in the integration of visual perception with LLMs. The unique design of Request-based Visual Information Request (RVII), allowing LLMs to 'request' specific visual information, is both intuitive and groundbreaking, which reflects a deep understanding of how multimodal processing might emulate human brain-like cognition. The motivation is clear and interesting, and the experiments are solid for supporting their ideas. The impressive results on two comprehensive multimodal benchmarks validate the effectiveness of LMEye (less parameters, better results). The section of Discussion and Future Work present the current development of Multimodal Large Language Models and give useful advice for constructing a multimodal language model, such as selecting language models and how to design interaction between human instructions and visual information. This paper might become a foundational reference for future studies about LLMs as the core processor of Multimodal Agent. \n\n2. Presentation:\nThe overall description of the paper is clear and the motivation of incorporating human-like eyes for LLMs is novel, different from construction of current multi-modal large model. The extensive experiments including two widely-used benchmarks, other downstream datasets, and self-constructed benchmarks, fully verified the author's motivation and proposed approach. The experimental analysis are solid.\n\n3. Method: \na. LMEye introduces dynamic visual information interaction between LLMs and objective Visual Information. By recognizing and addressing the gap between static and dynamic visual information processing, LMEye offers a more holistic approach to multimodal understanding. It allows LLMs to actively request and incorporate relevant visual content based on specific human instructions, ensuring a more tailored and context-aware response. By preserving the inherent structure of LLMs, LMEye ensures that their original performance on NLP tasks is not compromised, thereby maintaining the robust generalization abilities of LLMs.\nb. A novel Request-based Visual Information Interaction module (RVII) is introduced. This module enables the LLM to understand the basic visual information and human queries, send a request for additional specific visual information, and then construct a response based on the integrated understanding of the image, text instruction, and the interacted visual information. As we know, when humans complete tasks according to instructions, they usually interact with the external environment multiple times. \nc. The whole training process is parameter-efficient and RVII module could be injected into various LLMs, e.g., Llama, Bloom, BLIP-2, and FlanT5-xl. The scalability of the proposed method, LMEye: interactive perception network, is easy and stable.\n\n4. Experiments:\na. Superior Performance with Fewer Parameters: The results from the experiments are commendable. LMEye demonstrates superior performance in multimodal understanding and reasoning when tested on two comprehensive evaluation benchmarks: MMBench and SEED-Bench. What's striking is its ability to outperform other MLLMs while using substantially fewer parameters (4.4B for LMEye vs. >7B for others). This suggests that LMEye isn't an incremental improvement but offers a significant leap in the direction of efficient and effective multimodal modeling.\nb. Consistent Improvements in Ablation Studies: Ablation studies further solidify the effectiveness of LMEye. There is a significant improvement in zero-shot multimodal performances across different scales and types of LLMs. The exact gains mentioned, such as a 5.0% improvement on OK-VQA for LMEye (BLIP-2) in comparison to BLIP-2 alone, and a remarkable 20% gain on VQA with long answer for LMEye (LLaMA-7b) against LLaVA (Vicuna-7b), speak volumes about the robustness of the method. This consistency in improvement across different benchmarks and tasks emphasizes the versatility and generalizability of LMEye.\nc. The authors present intriguing cases that highlight several noteworthy aspects of their proposed method. These include multi-round interactions between Large Language Models (LLMs) and visual information, showcasing the model's ability to handle multilingual data, as well as its capability for artwork analysis. These examples provide valuable insights into the versatility and potential applications of the proposed approach.\nd. The comprehensive Section: Discussion and Future Work in the paper serves as a valuable synthesis of the key steps involved in constructing large multimodal models. It substantiates the claims made by the authors with corresponding experimental evidence."
            },
            "weaknesses": {
                "value": "1. The experimental part should offer a concise description of the structure of LMEye (BLIP-2).\n2. The caption and top line of tables 1 and 2 should be spaced wider.\n3. In the section discussing multimodal instruction-following part, it would be beneficial to provide a more comprehensive description of data, such as a figure. Specifically, elucidating the various categories of images within the dataset would offer readers a clearer understanding of its composition and scope.\n4. Whether the inference process will consume some time for different types of models, a brief inference efficiency description is given for the design of the LMEye variant for the encoder-decoder and the decoder-only models."
            },
            "questions": {
                "value": "1. See 3 and 4 in Weakness.\n2. Would this approach expand to video understanding and provide more detailed description of video and action inference? \n3. To alleviate the Hallucination problem shown in Appendix, could we introduce visual information request during text generation, like ReAct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725651130,
        "cdate": 1698725651130,
        "tmdate": 1699636514046,
        "mdate": 1699636514046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P7rwmvM2bI",
        "forum": "c4QgNn9WeO",
        "replyto": "c4QgNn9WeO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_DT9V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_DT9V"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multimodal LLM, LMEye. Different from preivous works using a simple mapping network to coneect vision and LLM, the visual information to LLM of LMEye is language-query conditioned. It first utilizes a LLM to extract the information of both vision and language, followed by a Request-based Visual InformationInteraction module to fuse vision and language signal. Finally the fused information and human queries are fed into another LLM to complete the instruction. The proposed method is evaluated on comprehensive multimodal benchmark like MMBench and SEED-Bench, traditional tasks like VQAv2 and a self-collected QA dataset, where LMEye achieves promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation and implementation are clear and straightforward. The visual information should interact with language queries in multimodal LLMs. Current MLLMs does not point out this issue but I think it is important for deeper understanding of the visual signal.\n- The evaluation on multiple tasks are promising. I believe LMEye can serve as a strong baseline for future works.\n- A new VQA dataset with long answer. Traditional VQA datasets are not suitable for current MLLMs since their answers are usually one word or phase, while MLLMs are tend to generate long and detailed answers. So this dataset will motivate the research community to pay more attention to improve MLLM's problem solving capability rather than fitting to traditional VQA benchmarks."
            },
            "weaknesses": {
                "value": "- As the core contribution of the paper, the authors did not carefully explore the contribution of RVII module. For example, the impact of  RVII module under the same data and training process.\n- In my view, I think there is no essential difference between RVII and LoRA. LoRA is too insert some parameters inside the LLM , while RVII is more like an Adapter module outside LLM. The authors claim that visual feature from RVII is dynamic and conditioned on human queries. Since the decoding process of LLM are autogressive, a token to be decoded is also conditioned on privous visual tokens and human queries. The LoRA layer will take as input both visual tokens and human queries."
            },
            "questions": {
                "value": "- What are the training parameters in the instruction tuning phase?\n- Since the visual information needs to be fed into LLM twice, will the model run slower?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766394957,
        "cdate": 1698766394957,
        "tmdate": 1699636513946,
        "mdate": 1699636513946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "McxR2ErEQj",
        "forum": "c4QgNn9WeO",
        "replyto": "c4QgNn9WeO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_c2o8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_c2o8"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the language-vision tasks by proposing a dynamic attetion-based model that uses human questions/instructions as input to dynamically query visual features for better visual information summarization. The authors claim the method is better than Blip model due to interactive attention and show promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The overall motivation of the paper is sound. The authors add dynamic attention to better summarize the information on the visual feature map.\n+ The presentation is mostly clear. \n+ The experimental results support the claims."
            },
            "weaknesses": {
                "value": "Overall, the paper lacks significant enough contribution to vision and language community:\n- The so called \"interactive perception model\" is actually a standard and common technique used everywhere. Early since attention was proposed, the visual information is dynamically summarized (attended). Some old works on VQA, such as NMN, have extensively used instruction-based condition to process visual information.\n- The network/module themselves are also pretty common, and do not convey any significant \"direction-shifting\" messages.\n\nThe paper's writing also needs some improvements. For example, section 3.1 gives way too many engineering details (which should've been put into supplementary) and lacks top-down summarization for easier read."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5180/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5180/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5180/Reviewer_c2o8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699480078848,
        "cdate": 1699480078848,
        "tmdate": 1699636513875,
        "mdate": 1699636513875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dDyxI3rGRd",
        "forum": "c4QgNn9WeO",
        "replyto": "c4QgNn9WeO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_E4sy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5180/Reviewer_E4sy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LLMEye, a trainable module designed to enable dynamic interaction between LLMs and external vision information. LLMEye employs a two-stage approach for enhanced interaction. The first stage involves feature alignment training, which utilizes a Q-former from BLIP2 and linear projection layers to map image features into text space, capturing static visual information. In the second stage, LLMEye introduces a linear layer and a multi-layer transformer block to facilitate request-based visual information interaction. The paper also presents an evaluation of LLMEye on various multimodal benchmarks, including MME, SEED-Bench, and VQA tasks, alongside a custom evaluation set focusing on VQA tasks with long answers and detailed descriptions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents a variety of evaluation benchmarks and shows the potential of LLMEye\n- Extending the capabilities of Multimodal-Models by training lightweight modules is an interesting direction also proposed by previous work (e.g MiniGPT4)"
            },
            "weaknesses": {
                "value": "- Figure 1 could be improved a lot. First, I will suggest including and specifying each component of LMEye. Adding this will help the reader to connect the notation mentioned in Section 3.1 and the flow of the figure\n- Ablations.\n\t- It would be beneficial to understand the contribution of the RVII module if authors include ablations with and without that module in MME and SEED-Bench benchmarks.\n\t- I also suggest specifying the Vision Model and Language Model with their corresponding parameters to have a better and fairer comparison while reading Tables 1 and 2.\n\t- Add ablations without RVII module in LLMEye (BLIP2) in Tables 3 & 4\n\t- Why not include the Qwen-VL model? I believe it was already published at the moment of the paper submission.\n- Minor:\n\t- > ... which shows supervisor performances on various multimodal scenarios. (Page 2)\n\n\t\tShould it be \"superior\"?"
            },
            "questions": {
                "value": "Please see above weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699624892661,
        "cdate": 1699624892661,
        "tmdate": 1699636513778,
        "mdate": 1699636513778,
        "license": "CC BY 4.0",
        "version": 2
    }
]