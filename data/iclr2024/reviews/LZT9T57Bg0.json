[
    {
        "id": "P06KaFZB2U",
        "forum": "LZT9T57Bg0",
        "replyto": "LZT9T57Bg0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_vGc3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_vGc3"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel neural logic query on Knowledge Graph by using LLMs. A complex logic query is decomposed into simple queries, each will be answered separately by LLM, and then these answers will integrated by following the logical relations among. Experiments were conducted on three benchmark datasets FB15K, FB15K-237, NELL995. Authors claimed the performances have greatly outperformed SOTA level."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper outlined an exciting application of LLMs to improve neural logic reasoning on knowledge graphs. Figure 1 vividly illustrates the procedure of using query prompts to get single query results from LLM. In most of the experiments, the performances indeed greatly outperform the SOTA level."
            },
            "weaknesses": {
                "value": "Authors assume that LLMs have the ability to reason and ascribe the improvement of their system's performances to this. However, whether LLMs can reason is still an open question. See below.\n\nC. Biever, The easy intelligence tests that AI chatbots fails, Nature 619 (2023) 686\u2013689\nM. Melanie, How do we know how smart AI systems are?, Science 381 (6654) (2023) adj5957\n\nIf we look carefully at the experiment results, it is not the case the proposed system consistently greatly outperforms SOTA level. In the FB15K dataset, CQD system outperforms the authors' system in 4 out of 9 tasks. In the NELL995 dataset (in ip task, CQD scores 70.0, while the authors' system scores 29.3), CQD system outperforms the authors' system in 2 out of 9 tasks. In the case of the negation query, BetaE outperforms authors' system in 2 out of 5 tasks in the FB15K datasets."
            },
            "questions": {
                "value": "Why is the performance of LARK not stable in some datasets but not in others?\n\nWhy is the performance of negative queries still much lower than that of positive queries, even supported by LLMs? \n\nIn the 3in task, BetaE performs the best in all three datasets, why?\n\n\"Our experiments on logical reasoning across standard KG datasets demonstrate that LARK outperforms the previous state-of-the-art approaches by 35% \u2212 84% MRR on 14 FOL\". How is 35% \u2212 84% calculated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Reviewer_vGc3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698161226841,
        "cdate": 1698161226841,
        "tmdate": 1699636785661,
        "mdate": 1699636785661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z65hRhU93p",
        "forum": "LZT9T57Bg0",
        "replyto": "LZT9T57Bg0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_gvan"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_gvan"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), for complex reasoning over knowledge graphs (KGs) using large language models (LLMs). The approach decouples KG search and abstract logical query reasoning to leverage the strengths of graph extraction algorithms and LLMs, respectively. The authors demonstrate that their approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Impressive results on NELL dataset. This approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs.\n2. Integration of LLMs: The performance of the proposed approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs."
            },
            "weaknesses": {
                "value": "1. The example query in Figure 1 equals to 3i. It is not a simplest form. Maybe another query structure is better.\n2. RQ3. Only two sizes of LLMs are not sufficient, though we do observe a significant performance improvement on 13B Llama2.\n3. The experiment to answer RQ4 is not a good design. GPT3.5 is different from Llama2-7B and Llama2-13B. It\u2019s hard to tell the importance of token limit. Because GPT3.5 outperforms various open-source LLMs (including Llama2-7B and Llama2-13B) on many reasoning benchmarks. A proper way to study how the token limit affects reasoning performance is to vary token limit on one LLM. For example, we could set 100%, 75%, 50%, 25% on max token limit of Llama2.\n4. Lack of comparison with other LLM-based approaches: The paper only compares the proposed approach with state-of-the-art KG reasoning methods and does not compare it with other LLM-based approaches for KG reasoning."
            },
            "questions": {
                "value": "1. The result on NELL dataset is quite impressive. But NELL is constructed from web data, which may have been used for pretraining the LLMs. Could you show that the results from LLM are given by logical reasoning instead of recalling hidden knowledge inside?\n2. Integration with other KG reasoning methods: It would be interesting to see how the proposed approach can be integrated with other KG reasoning methods, such as rule-based reasoning or probabilistic reasoning, to further improve the performance.\n3. Interpretability: The paper does not discuss the interpretability of the proposed approach, which is an important aspect of KG reasoning. It would be interesting to see how the approach can provide explanations for the reasoning process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6798/Reviewer_gvan"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688693378,
        "cdate": 1698688693378,
        "tmdate": 1699636785542,
        "mdate": 1699636785542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BXD0d6MmEr",
        "forum": "LZT9T57Bg0",
        "replyto": "LZT9T57Bg0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_5TPZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_5TPZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the LARK model that utilizes the reasoning abilities of large language models to efficiently answer FOL queries over knowledge graphs. The LARK model first extracts subgraph contexts and then performs chain reasoning over these contexts. Empirical results on three datasets on FOL query show that the proposed LARK model performs better than other methods that work without large language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is simple yet effective.\n\nThe obtained improvement in performance is significant.\n\nThe presentations and figures are clear and easy to follow.\n\nThe limitations of the proposed method are preliminarily discussed."
            },
            "weaknesses": {
                "value": "The technical novelty is neutral, as the paper seems to use LLM for FOL queries directly.\n\nThe writing of the paper can be largely improved. \n\nThe running-time efficiency of LARK is not reported.\n\nThe paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives.\n\nBesides, the paper does not provide satisfying insights or underlying properties of the LARK model.\n\nAs the paper uses entities and relations in queries to find pertinent subgraph contexts, it would be better to discuss some relevant subgraph sampling methods on KG, e.g., AStarNet (Zhaocheng Zhu et al., NeurIPS 2023) and AdaProp (Yongqi Zhang et al., KDD 2023)."
            },
            "questions": {
                "value": "Please refer to the above weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742074259,
        "cdate": 1698742074259,
        "tmdate": 1699636785423,
        "mdate": 1699636785423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HvebEZQzcg",
        "forum": "LZT9T57Bg0",
        "replyto": "LZT9T57Bg0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_2XTg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6798/Reviewer_2XTg"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a system for executing first-order logic queries on knowledge graphs by using an LLM (Llama-2). The proposed approach:\n1. Collects a subgraph around the query entities from the KG\n2. Anonymizes the entities and relations in the query and the collected subgraph by converting them into entity and relation ids\n2. Prompts the LLM to execute each step of the query graph and passes intermediate answer entities to the next step of execution\n\n- Results on a dataset of complex queries shows improvements over baseline approaches.\n- Without any fine-tuning, the LLM is shown to generalize across different underlying knowledge graphs (prior works require at least part of the pipeline to be fine-tuned for each KG)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed approach based on LLMs only observers a subset of the full KG with anonymized entity and relation IDs. This allows the system to:\n    - Easily generalize to new KGs\n    - Scale to larger KGs (since the LLM only observes a k-hop subgraph)\n    - Most existing approaches for KG reasoning need all or part (one-hop link prediction model) to be fine-tuned for each KG\n- Authors demonstrate that step-by-step query execution performs significantly better than directly attempting to execute the complex query\n- Authors show that other LLMs also seem to handle the task, demonstrating that the performance is not specific to Llama-2"
            },
            "weaknesses": {
                "value": "- The presentation of the paper lacks details of the evaluation protocol\n    - The paper evaluates the system on the queries generated by [1]. [1] highlights 2 types of answer entities: trivial answers (query can be directly executed on the available KG to gather answers) and non-trivial answers (queries require reasoning about missing edges).\n    - It is unclear if this paper reports results on the trivial, non-trivial, or combined subsets of answers.\n- While the paper reports strong results on all benchmarks, there is no indication of the mechanism by which an LLM \"could\" solve the task.\n    - The results are especially surprising given that the LLM only observes anonymized entity and relation names and cannot apply any semantic reasoning\n    - The LLM only sees a small subset of the KG, so it does not have sufficient information to learn the semantics of the anonymized relations \n- The paper misses some relevant work in query execution on KGs\n    - [2] is a strong baseline that uses learned one-hop link prediction models for each KG and uses them to execute complex queries. The method provides guarantees that the trivial answers will be predicted exactly and shows strong performance on the non-trivial answer entities\n    - Other competitive baselines include [3] and [4]. [2] provides MRR results for these approaches on the same datasets\n    - The MRR for the CQD baseline in this paper do not match the corresponding results in [2]\n\nI have reframed these weaknesses as a series of questions in the next section.\n\n---\n[1] Ren, H., Hu, W., and Leskovec, J. Query2box: reasoning over knowledge graphs in vector space using box embeddings. In International Conference on Learning Representations, 2020.\n\n[2] Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. 2023. Answering complex logical queries on knowledge graphs via query computation tree optimization. In Proceedings of the 40th International Conference on Machine Learning (ICML'23)\n\n[3] Chen, X., Hu, Z., and Sun, Y. Fuzzy logic based logical query answering on knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3939\u20133948, 2022.\n\n[4] Zhu, Z., Galkin, M., Zhang, Z., and Tang, J. Neuralsymbolic models for logical queries on knowledge graphs. In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 27454\u201327478, 2022."
            },
            "questions": {
                "value": "1. Please clarify the evaluation protocol. Is the reported MRR only considering the non-trivial answer set?\n2. What is the baseline MRR of directly executing the queries on the KG? e.g. for the 2p query (e1, r1, ?) -> (?, r2, ANS), explicitly returning entities that are connected to e1 by the 2-hop path (r1, r2)? Let's call this baseline DIRECT\n3. What is the MRR of returning the answer set of DIRECT combined with a random order of entities from the collected neighborhood subgraph? Let's call this baseline DIRECT+KNN\n4. Given that the LLM only observes anonymized entity and relation IDs and a neighborhood subgraph of the full KG, how does the LLM perform better than other approaches?\n    1. The paper is missing a discussion and examples of when and why it performs better than the baselines (including DIRECT and DIRECT+KNN).\n    2. In Sec 4.3 the paper claims that the improvement is from \"the LLM's ability to capture a broad range of relations...\". How is this possible with anonymized relations?\n5. Introduction, page 1, para 1: The introduction mentions that the proposed approach handles more complex queries than \"constrained\" FOL queries. The evaluation datasets in this paper are also FOL queries. How do you support this claim?\n    - Same claim made on page 2, para 2\n6. Section 3.2, Neighborhood retrieval: There seem to be errors in the equations here. It is unclear how the neighborhood graph actually grows.\n    1. Eq (5): By definitions 1-4, $Q_\\tau$ only contains the head entity and relation for each edge in the query graph. Then how do you constrain $t \\in E^1_r$\n     2. Eq (7): Constrains the new graph to contain entities and relations that lie in the previous neighborhood graph. How does the graph ever grow?\n     3. When collecting the neighborhood subgraph, do you only include edges that are of the same type as the edges in the query graph? \n7. Sec 4.5: The line says \"over 4096 and less than 4096\". I am assuming this is a typo and you only meant \"over 4096\"? Please clarify\n8. Fig 2: The text of the figure seems to convey that the model receives the logical form of the query as input. Why do you then need a query type identifier according to the description of Fig 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699515177062,
        "cdate": 1699515177062,
        "tmdate": 1699636785314,
        "mdate": 1699636785314,
        "license": "CC BY 4.0",
        "version": 2
    }
]