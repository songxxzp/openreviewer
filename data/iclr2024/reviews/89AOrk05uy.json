[
    {
        "id": "PBZuM8UOwZ",
        "forum": "89AOrk05uy",
        "replyto": "89AOrk05uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how spurious correlations are exploited and encoded in neural networks through the lens of the NTK in the infinite width limit, where the kernel governing the dynamics is fixed at initialization. This allows the training dynamics to be entirely predictable (in closed form), and allows for an eigendecomposition that allows for the extraction of eigenfunctions that are learned at different speeds based on the magnitude of the corresponding eigenvalue. In particular, through \"saliency maps\", the authors exploit this to identify that the leading eigenfunctions have high activations in the regions where the spurious features are present. This indicates that the network does make use of the spurious features to make predictions. The authors explain this in terms of the \"simplicity bias\" of the NTK: when the spurious feature can be explained with low-frequency functions, the signal will be picked up by the low-frequency eigenfunctions of the NTK, thus resulting in poor generalization. On the other hand, when the spurious feature is relatively complex compared to the generalizing signal, the model does generalize."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Presentation**: The paper is very well-written, and the results are extremely well-presented. The logical argument of how spurious correlations affect generalization in the NTK regime flows very nicely from the author's considerations on previously observed phenomena such as simplicity bias, and adversarial robustness of the NTK. \n2. **Quality**: the experiments are directly targeting the research question addressed by the authors. The visualization of the saliency map in Figure 2 explains well how the eigenfunctions encode the information of the spurious features. I also appreciated the in-depth discussion on the role of feature complexity, and how the spurious feature needs to be \"simple\" in order to cause problems in generalization for the NTK.\n3. **Originality**. The problem of explaining spurious correlation under the NTK framework is novel, to my knowledge."
            },
            "weaknesses": {
                "value": "Main concern:\n1. **Role of depth**. The authors state that deeper networks learn more complex features, and suggest that this can help to improve performances under the presence of spurious correlation as it is shown in Figure 5. However, it is unclear whether the performance improvement is due to the \"feature complexity\" argument put forth by the authors. There is no evidence provided that a similar improvement would happen even in the absence of the spurious feature. Also, there is no saliency map visualization confirming that the spurious features are not encoded in the leading eigenfunctions at larger depths. In Figure C.1, it is hard to confirm the authors' claim, as the gradient correlation seems to increase uniformly across all the groups. \nMinor:\n2. **Scope of the paper**. The paper adopts all existing techniques. For instance, saliency maps have been applied to NTK to study adversarial robustness (Tsilivis and Kempe, 2022), correctly cited by the paper. Also, the scope of the paper could have been more broadly enlarged to include the role of feature learning and changes in the NTK through training (some experiments are run in the Appendix, but the role of feature learning is not discussed). However, I consider these minor issues. I think that even without these minor improvements, the paper still deserves publication."
            },
            "questions": {
                "value": "1. Role of regularization. How does weight decay or other type of regularization affect the encoding of spurious features in the NTK?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698417119052,
        "cdate": 1698417119052,
        "tmdate": 1699636297631,
        "mdate": 1699636297631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x0K2AEeDo0",
        "forum": "89AOrk05uy",
        "replyto": "89AOrk05uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the problems posed by spurious correlations when learning with the neural tangent kernel and proposes a technique for improving NTK robustness when such correlations exist. The paper introduces a model of spurious correlation, where true labels $Y$ are independent of spurious features $S$ at test time, but have a planted dependence at training time modulated by parameter $\\alpha$. They consider several tasks that combine spurious features from a simpler learning task with real features from a more difficult task.\n\nThey demonstrate empirically that the spurious features are most strongly associated with low-frequency eigenvectors of the NTK matrix $H$ by giving a visualization of salience maps for each feature and computing eigenvector alignment scores. Given the association between spurious features and low-frequency features, they propose a modification of the kernel that scales eigenvalues to reduce the impact of low-frequency features. They demonstrate empirically that this approach improves generalization performance on the tasks and erases the gap between performance on different subgroups (characterized by the label $Y$ and the spurious features $S$). The appendix includes a hyper-parameter sweep over the kernel modification parameters $\\gamma$ and $\\beta$."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is overall well-written and its survey of the robustness literature appears to be strong. The NTK background is presented in an intuitive and understandable way. The tasks are cleanly presented and dataset visualizations in Figures 1 and 3 are useful. The thoroughness demonstrated by the hyper-parameter sweeps in Figures C.6 and C.7 is appreciated. The problem is certainly compelling, and the idea of tuning the spectrum of the kernel matrix is interesting. While I find the connections to deep neural networks limited, I think this work is a good foundation for a rigorous experimental study of spurious bias reduction for kernel methods."
            },
            "weaknesses": {
                "value": "I have two high-level critiques of the work, which I would be interested to hear the authors respond to. The first pertains to the relevance of the NTK and the methods of this work to neural networks, and the second to the generality of the paper's claims about the spectrum of spurious features.\n\n## Relevance of NTK and algorithm to DNNs\n\nThe authors motivate their work by introducing the important issue of spurious correlations in deep neural networks, but they restrict their focus to the NTK approximation of a neural network. While the neural tangent kernel is an appealing model that allows one to apply convex analysis tools to deep learning, both empirical and theoretical work shows that the NTK fails to match neural network performance and cannot perform key deep learning functions like feature learning; consider reading and citing [works](https://papers.nips.cc/paper_files/paper/2019/hash/c133fb1bb634af68c5088f3438848bfd-Abstract.html) [like](https://arxiv.org/abs/2011.14522) [these](https://arxiv.org/abs/2206.10012). The NTK and kernel methods broadly are certainly worthy of study, but this paper does not address the critiques of these models in their introduction or NTK discussion.\n\nThe feature learning critique of the NTK is a particular issue for this paper because the methods use the spectrum of the kernel matrix at initialization, without accounting for the fact that gradient descent tends to encode task-dependent features in the bottom layers of neural networks. Thus, the features analyzed in this work are unlikely to correspond to the actual features encoded in a trained neural network. In addition, the mitigation algorithm presented in the paper only applies when the optimization algorithm has direct access to the kernel, and it doesn't appear to work for neural networks trained with gradient descent. \n\nPerhaps the paper could be more strongly motivated if it were grounded in studying spurious correlations in kernel methods without needing the analogy to DNNs to hold? If the paper is to focus on DNNs, it needs to include a stronger accounting of the limitations of the NTK and a proposal for how it can be applied the feature learning that occurs in neural networks trained with gradient descent.\n\n## Generality of low-frequency spurious features\n\nI am concerned that the principal claim of the paper---that spurious correlations are aligned with low-frequency eigenvectors of the kernel matrix and that they can be mitigated by down-weighting the respective eigenvalues---is an artifact of the datasets considered in Sections 4 and 5. Specifically, the five datasets considered have artificially planted spurious features (e.g. digit color) that are far easier to learn than the true labels (e.g. 0-4 vs 5-9). The dominance of the spurious labels in the low-frequency features of the biased data is made evident for CMNIST and Biased-MNIST in Figure 4, but it's less strong for other tasks, and it's unclear whether this holds for \"real world\" datasets with spurious features. \n\nAs a result, it's unclear whether the algorithmic approach in Section 5 would perform well outside the collection of tasks considered. The paper does little to rule out the possibility that flattening the spectrum of $\\Lambda$ with $\\nu$ will cause problems on other tasks, since it's conceivable that salient features could be lower frequency elsewhere. For this paper to be more compelling, I would like to see whether these results still hold on less artificial datasets, such as Waterbirds and CelebA, which were used in addition to the MNIST-type tasks to evaluate the methods of [the](https://arxiv.org/abs/2210.00055) [papers](https://openreview.net/pdf?id=Zb6c8A-Fghk) they take dataset inspiration from.  \n\n## Other significant concerns\n\nSeveral figures in the paper have significant presentation issues. \n- Figures 4 and 5 are missing numerical axes. \n- Using the normalized index $k$ as the x-axis in Figures 3 and 5 adds some confusion, since it's unclear what number of features is represented by $k$, as it's given as a fraction. \n- There's no discussion in Appendix B about how the choices of planted bias $\\alpha$ were chosen and why choices differ for each dataset. \n- The CIFAR-MNIST task is introduced in Section 4.1 without being referenced elsewhere in the paper body.\n\n## Minor comments\n- In Section 3.1, the actual datasets do not strictly belong to the regime where $\\mathcal{X} = \\mathcal{X}_y \\times \\mathcal{X}_s$, since $\\mathcal{X}$ defines the input space, and several tasks (e.g. CMNIST) have both relevant and spurious features operating on the same pixels.\n- There is some confusing mathematical notation in Section 3.2. $\\dot{\\theta}_t$ is somewhat atypical notation, since $\\theta_t$ is the typical presentation of a step of a discrete dynamical system, rather than $\\theta(t)$ for continuous systems; perhaps $\\dot{\\theta}(t)$ is more appropriate? Equation (3) defines the time derivative as a discrete-time gradient step, which is slightly confusing without limit notation.\n- In Section 4.1, the terms \"bias-aligned\" and \"bias-conflicting\" are introduced, but are only used in the appendix.\n- In Section 4.2 in the sentence starting with \"Based on the decomposition in Eq (10), training a DNN with gradient descent...,\" I would recommend adding the qualification \"in the lazy or NTK regime.\" \n- The caption of Table C.3 ends with an incomplete sentence."
            },
            "questions": {
                "value": "Can the approaches presented in the paper by applied to all kernel methods, rather than just the NTK? Given the aforementioned issues of the analogy between the NTK and neural networks, perhaps the paper could be motivated more strongly as a method for kernel methods, rather than neural networks.\n\nIs the model of spurious correlation introduced in Section 3.1 novel? If not, I would recommend adding citations to where it's defined in the literature, and if so, I would recommend explaining how it differs from other formulations.\n\nDid you consider including theoretical results about the effectiveness of the algorithm? I suspect that there's a natural theorem to prove about when the approach will provably work for some reasonable data distribution assumptions on the spurious features aligning with the low-frequency eigenvectors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3451/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772005625,
        "cdate": 1698772005625,
        "tmdate": 1699636297540,
        "mdate": 1699636297540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lUC5MmEf5q",
        "forum": "89AOrk05uy",
        "replyto": "89AOrk05uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
        ],
        "content": {
            "summary": {
                "value": "This paper establishes a connection between spectral bias of neural networks and how that effects their subgroup robustness. They first analyze the eigenfunctions of the NTK Gram matrix and show that features that are spurious correspond to lower-order eigenfunctions, which correspond to larger eigenvalues. When such features are entangled with the label, it can cause the model to rely on these more strongly because eigenfunctions with larger eigenvalues are learned faster. In order to mitigate this effect, the authors propose a modification to the Gram matrix, to make the eigenvalues more similar to each other and encourage learning diverse features. They use some simple variants of MNIST and CIFAR datasets to demonstrate their observations and evaluate their approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper presents a connection between spectral bias of NNs and their tendency to learn spurious features which leads to low subgroup robustness, which is interesting. \n\n2. They propose an approach to improve subgroup robustness that seems effective on some variants of MNIST and CIFAR.\n\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. ****Although the connection between spectral bias and subgroup robustness is nice, it doesn't really offer much new insights.****\n\nFrom the spectral bias perspective, there are several works [2,3], which have theoretically shown that eigenvectors of the Gram matrix with larger eigenvalues correspond to 'simpler' features, using some notions of simplicity. From the subgroup robustness perspective, there are works [4,5] that show that spurious features are simpler and simplicity bias of NNs causes them to rely more on such features. Based on these works, one can expect that spurious features would correspond to lower order eigenfunctions which will have higher eigenvalues.  \n\n2. ****The idea to make the eigenvalues of the NTK Gram matrix more balanced to improve subgroup robustness is not new (see [1]). The proposed approach is not scalable.****\n\nThe paper is missing discussion about a very closely related work [1]. [1] mainly talks about how spectral bias can cause NNs to rely strongly on a subset of features, and also proposes an approach for spectral decoupling (making the eigenvalues of the NTK Gram matrix more balanced). They show that regularizing the logits of the model can help do that (in some simple settings) and also empirically validate their approach on a range of datasets (including subgroup robustness datasets). Their approach is also more efficient and scalable than the approach presented in the current work, which does not scale to large datasets. Based on this, the proposed method doesn't seem very valuable.\n\n3. ****Limited evaluation.****\n\nThe paper considers MNIST and CIFAR datasets, but evaluation on the usual subrgoup robustness benchmarks (CelebA and Waterbirds) seems missing. The paper also does not compare their approach with any other method to improve subgroup robustness.\n\n\n****References:****\n\n[1] M. Pezeshki et al. ****Gradient Starvation: A Learning Proclivity in Neural Networks**** https://arxiv.org/abs/2011.09468\n\n[2] Y. Cao et al. ****Towards Understanding the Spectral Bias of Deep Learning**** https://arxiv.org/abs/1912.01198\n\n[3] G. Yang et al. ****A Fine-Grained Spectral Perspective on Neural Networks**** https://arxiv.org/abs/1907.10599\n\n[4] Y. Yang et al. ****Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias**** https://arxiv.org/abs/2305.18761\n\n[5] H. Shah et al. ****The Pitfalls of Simplicity Bias in Neural Networks**** https://arxiv.org/abs/2006.07710"
            },
            "questions": {
                "value": "(See weaknesses above)\n\nCan the authors discuss the contributions of their work in light of this related work [1-5]? I suggest doing a more thorough review of related work and including a detailed discussion on the contributions and what are the insights that are new compared to these other works. And particularly, what is the contribution of this work compared to [1]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817918081,
        "cdate": 1698817918081,
        "tmdate": 1699636297449,
        "mdate": 1699636297449,
        "license": "CC BY 4.0",
        "version": 2
    }
]