[
    {
        "id": "CnPMtigEHp",
        "forum": "5BXAXOpaWu",
        "replyto": "5BXAXOpaWu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at the task of composed image retrieval (CIR) which retrieves images by providing a multimodal query such as a query image and additional text which describes the user's further query intention. Usually, such task is resolved by aligning the multimodal query and gallery features with vision-and-language pretraining and finetuning techniques. The authors argue that existing methods are not feasible to mobile applications due to expensive computational costs by forwarding large multimodal foundation models on mobile devices. The proposed solution is adopting a lightweight model to process the query while still maintaining the large foundation model for the gallery side. In order to bridge the representation gap between the lightweight and large model, the adaptive token learner is proposed to map an image to a sentence in the language model space. Finally, the authors verify their contributions on three evaluation benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[1] Overall, this paper is well written and easy to follow. \n\n[2] I like the motivation of this work since deploying original large foundation model is almost impossible in mobile applications. Different from pruning or distilling such heavy models, the authors proposed the lightweight encoder with a tunable adaptive token learner. The idea behind is borrowed from the LLM-Adapters. \n\n[3] The proposed modules are technically sound and the experimental resutls are sufficient. The training resources are extremly friendly with 4 RTX 3090 GPUs."
            },
            "weaknesses": {
                "value": "[1] Since this work is a retrieval task, it is important to report how the retrieval performance varies as the gallery size scales up. I understand that the three evalutation datasets are standard benchmarks. However, it would make the contribution more solid if millions distractors could be involved in the gallery, although most existing SOTAs didn't report such results. \n\n[2] The inference time should be reported including in the query side and in the cloud side.\n\n[3] Some minor issues are listed in the next part."
            },
            "questions": {
                "value": "[1] Figure 1 could be further improved. Specifically, the pink rectangle and trapezoid could be shrunk and the blue trapezoids could be enlarged. As a result, it is much easier to quickly grasp the idea at first glance for readers. \n\n[2] Table 5 provides a viriant mapping network which is actually a MLP. Are there other options?\n\n[3] Figure 3 reveals a fact that larger token lengths could degrade the retrieval performance. The authors attribute such fact to the background noise or the trivial patterns. Are there any deeper insights or visualization analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698053325478,
        "cdate": 1698053325478,
        "tmdate": 1700619451258,
        "mdate": 1700619451258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BHtq8sUKKo",
        "forum": "5BXAXOpaWu",
        "replyto": "5BXAXOpaWu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an asymmetric zero-shot composed image retrieval framework. The asymmetric retrieval pipeline is established using a lightweight model for query images and a large foundation model for gallery images, enabling feature extraction. Composed image retrieval is achieved by concatenating the sentence representation mapped from the image with a text modifier. To align the features extracted by the lightweight model and the large foundation model, two techniques, namely global contrastive distillation and local alignment regularization, are proposed. Extensive experiments and an ablation study conducted on benchmark datasets have demonstrated the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. An asymmetric zero-shot composed image retrieval framework is proposed. \n2. Global contrastive distillation and local alignment regularization techniques are proposed to align features from different models.\n3. Extensive experiments are conducted."
            },
            "weaknesses": {
                "value": "1. The clarity of the writing, particularly in the methods section, requires improvement.\n\n2. For image-only retrieval, could you provide results using the DINO-V2 and MoCo-V3 pretrained models? The CLIP model is typically used for content matching between image and text features."
            },
            "questions": {
                "value": "t-SNE visualizations could be shown to illustrate the differences between the different methods. This would provide a more intuitive understanding of the feature distributions and separability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5",
                    "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698559949738,
        "cdate": 1698559949738,
        "tmdate": 1700631528838,
        "mdate": 1700631528838,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FcDWJQNTXF",
        "forum": "5BXAXOpaWu",
        "replyto": "5BXAXOpaWu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to composed image retrieval (CIR), emphasizing on the challenges associated with composed image retrieval that needs understanding of both visual and textual data. To address data scarcity in CIR, the authors introduce a new task paradigm named \"zero-shot composed image retrieval\" (ZSCIR) that transforms image retrieval to a text-to-image format, allowing for a more intuitive mapping between images and descriptive text. However, the methods presented face challenges with large-scale models which are not suitable for deployment on resource-constrained platforms, such as mobile devices. To mitigate this, the authors propose an asymmetric approach, termed Image2Sentence based Asymmetric ZSCIR, that uses different models for query and database extraction. This method utilizes a lightweight model for the user's device and a heavier model for cloud processing. The core of this approach is an adaptive token learner which converts visual features into textual tokens, thus enhancing the representation. The proposed framework was tested on various benchmarks, demonstrating its efficiency and effectiveness compared to existing state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses the challenges in Composed Image Retrieval by introducing the zero-shot composed image retrieval (ZSCIR). This method offers a fresh perspective on image retrieval by transforming it to a text-to-image format, thereby providing a more direct linkage between descriptive text and its corresponding image.\n\n2. The introduction of an adaptive token learner, which effectively translates visual features into textual tokens, stands out as a major strength. This conversion mechanism is pivotal in enhancing the representation of images and ensuring that the retrieval process is both accurate and efficient. The adaptive nature of the learner means that it can adjust and improve over time, potentially leading to even better retrieval results in the future."
            },
            "weaknesses": {
                "value": "1. While the adaptive token learner is a strength in terms of converting visual features to textual tokens, there's a risk that the system could become overly reliant on this component. If the learner fails or encounters unanticipated scenarios, it might compromise the effectiveness of the entire retrieval process.\n\n2. Introducing an asymmetric text-to-image retrieval approach, while innovative, adds an extra layer of complexity to the system. This might present challenges in terms of maintainability, debugging, and further development of the system.\n\n3. The transformation of the retrieval problem from image-to-image to text-to-image inherently assumes that the descriptive texts are of high quality and detailed. Any inaccuracies or vagueness in the text could lead to inefficient or incorrect image retrievals.\n\n4. The paper presentation is not very attractive. It is difficult to understand the novelties / contributions after reading the introduction of the paper."
            },
            "questions": {
                "value": "1.  How does the ZSCIR approach compare in performance and efficiency with state-of-the-art image retrieval methods that don't employ a text-to-image asymmetry? Are there scenarios where a traditional symmetric approach might outperform ZSCIR?\n\n2. In terms of training data apart from the image augmentation, did you employ any data augmentation techniques to enhance the performance and robustness of the ZSCIR model? Furthermore, how did you ensure the diversity and representativeness of the descriptive texts used in the system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669489007,
        "cdate": 1698669489007,
        "tmdate": 1700655491805,
        "mdate": 1700655491805,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ljoI7SCM0D",
        "forum": "5BXAXOpaWu",
        "replyto": "5BXAXOpaWu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an image2sentence based asymmetric framework for zero-shot composed image retrieval tasks. In particular, a lightweight visual encoder and a consequent adaptive token learner are designed to effectively extract the visual features from query images for the mobile side. By doing so, the learned features could be generated as a good visual prompt as with the text intent for conventional LLM to deal with image retrieval tasks. In addition, a local alignment regularization term is added to further improve the training. The experiments conducted on several benchmark datasets verify the effectiveness of the proposed method compared with existing SOTA ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is of good written quality that makes the readers easy to follow. The logic, the notion expression, and the experiments are all very clear.\n\n2. The asymmetric design is interesting and this design has been proven an efficient way to deal with resource-limited circumstances.\n\n3. The experiments conducted are very convincing to support the contribution claimed by the authors. The properties of the proposed method are well demonstrated in the ablation study."
            },
            "weaknesses": {
                "value": "1. It could be better to discuss more about the number of token selections in detail. According to Fig 3, it seems the performance is a bit sensitive to the number of tokens used in the proposed method. Then, a more detailed discussion of this observation with a visual example of the same query but a different number of tokens could help to better demonstrate the impact brought by the tokens.\n\n2. It could be good to add a discussion of the relationship between the proposed adaptive token learner and the similar approach used in the following papers [1,2]. They have a similar structure, a discussion would help to better locate the position of the token learned in this work.\n\n [1] Wu, Hui, et al. \"Learning token-based representation for image retrieval.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 3. 2022.\n [2] Locatello, Francesco, et al. \"Object-centric learning with slot attention.\" Advances in Neural Information Processing Systems 33 (2020): 11525-11538."
            },
            "questions": {
                "value": "Please check the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Nil"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699266015265,
        "cdate": 1699266015265,
        "tmdate": 1699636190632,
        "mdate": 1699636190632,
        "license": "CC BY 4.0",
        "version": 2
    }
]