[
    {
        "id": "4J0SUnlsHS",
        "forum": "kVSmnzcXDK",
        "replyto": "kVSmnzcXDK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_yLhF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_yLhF"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a universal method for integrating detectors and evaluating approaches for detecting out-of-distribution (OOD) data and addressing data distribution shifts. The authors propose a technique that normalizes detector scores into p-values using quantile normalization, transforming the problem into a multivariate hypothesis test. They combine these tests using meta-analysis tools, improving the effectiveness of the detector and consolidating decision boundaries. The authors also create an interpretable criterion by adjusting the final statistics of in-distribution scores. Through empirical investigation, they demonstrate that their approach enhances robustness and performance across various domains, shift types, and OOD detection scenarios. The paper contributes to the field of machine learning by providing a flexible framework for integrating detectors and addressing OOD detection challenges."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a universal method for integrating detectors, which can be applicable across different domains, including OOD detection and two sample test. \n\nThe use of quantile normalization to transform detector scores into p-values seems interesting. This transformation allows for treating the problem as a multivariate hypothesis test and enables effective combination for a set of predefined scoring strategies.\n\nThe paper proposes adjusting the final statistics of in-distribution scores to create a fully interpretable criterion. This feature is valuable as it provides insights and explanations for the detection decisions, enhancing the transparency and interpretability of the method.\n\nThrough empirical investigation, the paper demonstrates that the proposed method significantly enhances overall robustness and performance across various domains, shift types, and out-of-distribution detection scenarios. This finding highlights the practical effectiveness of the approach."
            },
            "weaknesses": {
                "value": "This paper mainly uses the scoring strategies from OOD detection to build their method, which mainly considers the concept shift. However, in the discussion, they actually present another two different distribution shift, i.e., covariate shift and prior shift. Then, a natural question is why the basic method that handle concept shift can also be useful to tackling covariate shift and prior shift? Is it the attributed to the combination of different p values? More detailed discussion and empirical evaluation seem to be important. \n\nQuantile normalization seems interesting, while there exist many other ways, typically more simple ways, in doing normalisation. For example, we can simply normalise the score to follow the N(0,1) Gaussian distribution, which can also normalise the data. Therefore, more discussion about the theoretical superiority in using quantile normalisation is interesting.\n\nIt seems that extra parameters are introduced when combine different scoring strategies in Section 4.2 and 4.3, while I cannot find how to tune the related parameters. Moreover, how to conduct hyper parameter tuning, epically for the choice of evaluation dataset, should be discussed. If the proposed method does not introduce additional parameters, seemingly different scoring strategies are treated equally in combination. I am not sure if it is a proper setup. \n\nDifferent scoring strategies are effective for varying data, so another interesting question is that if the proposed method can be instance dependent in combination when facing different data points."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823438384,
        "cdate": 1698823438384,
        "tmdate": 1699636640481,
        "mdate": 1699636640481,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gCRlq1P6qS",
        "forum": "kVSmnzcXDK",
        "replyto": "kVSmnzcXDK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_UAHM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_UAHM"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach to combine multiple arbitrary out-of-distribution (OOD) detection scores. In particular, quantile normalisation is applied to obtain p-values from the individual detectors. Then, scores from multiple OOD detectors are combined using Fisher or Stouffer meta-analysis to obtain a single OOD estimate. As Fisher's method assumes independence of the p-values, a Brown correction is proposed, using the scaled chi-squared distribution. The approach is evaluated on standard OOD detection benchmarks where it shows good performance compared to the prior work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The idea of the paper is easy to follow. In addition, the problem is well presented, including a detailed explanation of the different types of data shifts.\n\u2028+ The different single-score methods and methods combining different detection scores are evaluated in detail using different data shift scenarios."
            },
            "weaknesses": {
                "value": "- (Major) Although the results of combining existing methods are interesting, the paper does not show any new idea. It therefore has limited novelty. \n\n- (Major) The paper claims that a major contribution is the correction for the assumed independence of Fisher's method. Therefore, Fisher's method should be compared with and without the correction in the experiments. \n\n- (Major) It is claimed that the method is interpretable as the distribution of the combined scores is known in advance. However, this is not demonstrated in the paper.\n\n- The clarity and notation of the method has room for improvement, e.g. in Section 4.1 the index i is used to iterate over the window examples and at the same time the doctor.\n\n- The experimental protocol is not very clear. For example, it should be clearly stated which combination of detectors is used in the evaluation. Is this adjusted according to the shift or is it sample or window based?"
            },
            "questions": {
                "value": "- The paper states that different detectors are ensembled. They have a study on the distillation of the best subset of detectors where different OOD detection scores are combined. Are different classifiers combined or are different OOD detection scores combined? So does the word \"detector\" refer to the different OOD detection scoring methods or does it refer to a classification model? This is not immediately clear. \n\n- It should be clarified how the indices in equation 6 are mixed up? Should they be W^r_1 and W^m_2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829202677,
        "cdate": 1698829202677,
        "tmdate": 1699636640341,
        "mdate": 1699636640341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RzuRI1lr91",
        "forum": "kVSmnzcXDK",
        "replyto": "kVSmnzcXDK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_2NKy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5984/Reviewer_2NKy"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of detecting distribution shifts in data streams that are inputted to deep neural networks. It emphasizes the importance of recognizing when the distribution of incoming data deviates from the distribution of the training data, which can impact the performance and reliability of the model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1, Instead of instance-level discrimination on OOD samples, this paper considers the OOD detection from an interesting perspective: the windows from the streamed data. \n2, In the poposed detection framework, the author leverages empirical cumulative distribution functions yo effectively compare the distribtuion from two windows, reference window and the test one. \n3.  The transformation into p-values is reasonable, and the calibration across detectors is well motivated. \n4. Extensive experiments are conducted to verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "This paper falls outside of my expertise sligtly, thus for now, I cannot find a clear weaknesses."
            },
            "questions": {
                "value": "1, In Fig 5 (c), I cannot find the curve correpsonding to Resnet-101.\n2, For the ablation study on window sizes, will listing the proportion of window size towards the whole dataset helpful for understanding the impact of window sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994423398,
        "cdate": 1698994423398,
        "tmdate": 1699636640231,
        "mdate": 1699636640231,
        "license": "CC BY 4.0",
        "version": 2
    }
]