[
    {
        "id": "Q7sdh8UAtQ",
        "forum": "l1U6sEgYkb",
        "replyto": "l1U6sEgYkb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_9oXi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_9oXi"
        ],
        "content": {
            "summary": {
                "value": "This paper covers lane detection task using attention mechanism from multi-modal queries of vision and LiDAR."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* Really novel idea that does this task of lane detection using query clustering from multiple perspective. \n* SOTA results, and code should be available online soon \n* Takes care of runtime optimization as well."
            },
            "weaknesses": {
                "value": "NA"
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698419109888,
        "cdate": 1698419109888,
        "tmdate": 1699636246364,
        "mdate": 1699636246364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O3IDcI7Iuu",
        "forum": "l1U6sEgYkb",
        "replyto": "l1U6sEgYkb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_Z8i9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_Z8i9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a multi-modal 3D lane detection method. A bidirectional feature fusion approach is proposed to incorporate multi-modal features into each view space. A unified query generation module is adopted that provides lane-aware prior knowledge from both views. A 3D dual-view deformable attention mechanism that combines discriminative features from both PV and BEV into queries for precise 3D lane detection. Comprehensive experiments on the public benchmark, OpenLane, prove the efficacy and efficiency of DV-3DLane."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "*  The performance of this article exceeded that of existing methods. A significant increase was also achieved in the multimodal feature fusion at the backbone level. \n*  The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "*  Most methods in Table 1 are based on camera. As can be seen from the results in Table 3, the performance of camera-only is not competitive. Multimodal inputs should be introduced to some methods for a comparative performance evaluation. Otherwise, presenting multimodal fusion as a key contribution is somewhat insufficient.\n\n* In Table 4, the performance of the queries generated based on PV and BEV is not as high as random queries, indicating that the adaptive generation of queries doesn't work. Although the queries after clustering have achieved performance improvement, it remains to be seen whether this improvement is brought about by the extra network. \n\n* The feature fusion in the backbone and decoder is quite tricky, making it difficult to be viewed as a major contribution point."
            },
            "questions": {
                "value": "*  The main performance improvement primarily comes from multi-modalities, but multi-modalities inherently achieve higher points than a single modality. Therefore, it would be best to prove that the multi-modal performance of existing methods is inferior to this paper. The camera result is not much competitiveness, especially when compared to some recent methods, such as Group Lane.\n*  Using Deformable attention to aggregate multi-modal features lacks innovation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748469722,
        "cdate": 1698748469722,
        "tmdate": 1699636246293,
        "mdate": 1699636246293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4FZ1LxLhkD",
        "forum": "l1U6sEgYkb",
        "replyto": "l1U6sEgYkb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel 3D lane detection algorithm that exploits a dual-view representation. The proposed algorithm consists of bi-directional feature fusion to aggregate view-specific features, unified query generation that focuses on coherent lane features, and 3D dual-view deformable attention to associate information across the viewpoints. The authors provided outperformed experimental results on OpenLane dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Achieved the best performance on a 3D lane detection benchmark\n- Leveraged multi-modal features to generate a unified query for 3D lane detection"
            },
            "weaknesses": {
                "value": "- It appears that the 3D DV deformable attention mechanism lifts PV features to 3D using known camera parameters. Also, when the authors concatenate DV features, they depend on these camera parameters. Have the authors attempted to test the tolerance of calibration parameters to noise?\n- Since the evaluation was only performed on OpenLane, it is difficult to check the generalizability of the proposed method."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_ECVn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773349206,
        "cdate": 1698773349206,
        "tmdate": 1699636246208,
        "mdate": 1699636246208,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w1B2Y4JW0g",
        "forum": "l1U6sEgYkb",
        "replyto": "l1U6sEgYkb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7"
        ],
        "content": {
            "summary": {
                "value": "This paper target 3D lane detection problem by proposing a dual view representation method utilizing both camera and Lidar as input. Their proposed method has achieve remarkable improvement over prior work with high efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Compared to prior work that mostly use only camera and feature learning in either Perspective View or Bird Eye View, this work fill the gap by fully investigate feature interaction of both sensor(Lidar+Camera) in both view(PV+BEV). And shows the superiority of their method by showing large improvement over prior work. \n2. The work is very well written, clearly introduce all related work as well as their problems, and how this work target those problem by proposing new solutions. \n3. This work include detail ablation study, make it clear to see the improvement over each step."
            },
            "weaknesses": {
                "value": "1. There\u2019re some inconsistent in the evaluation result: 1). Although this method have explicitly filling the gap of missing Lidar modality in prior work, it might not seem as an apple to apple comparison in the table. In table 1 1.5m, compare with prior SOTA LATR, the proposed method didn\u2019t achieve overwhelming improvement despite using one extra modality(Lidar). Does that mean the improvement in this range had been saturated to some extent? Also DV-3Dlane use a weaker backbone(ResNet-18/34) compare to ResNet50 LATR, make it even harder to see the full potential of proposed method compare to prior art. 2). Why Use OpenLane1000 in table 1(main experiment) but OpenLane300 for the ablation? Make it incomparable with result in Table1. \n2. For onboard application, it might not always viable, to access both Lidar and Camera information for joint feature, does this method also provide implications for camera only/Lidar only methods? Maybe it\u2019s more clear in ablation study if we could compare Table1 and Table2 directly. A follow up question, if camera-only ablation is possible, could it also show result on Apollo dataset? This is the dataset most commonly used in prior work. \n3. For Figure1, please also consider adding more methods to be more completed, for instance light weight model like GenLane.\n4. What is the \u2018lane-aware prior knowledge\u2019 in the abstract? I feel like \u2018prior knowledge\u2019 it is not discussed in the main text. Please explain or consider rephrase it."
            },
            "questions": {
                "value": "Please refer to weakness. In general I feel like this paper is well written, happy to raise my score if question addressed.\nSome related papers from prior venue is not properly cited, please consider citing the following works\n\n@inproceedings{liu2022learning,\n  title={Learning to predict 3d lane shape and camera pose from a single image via geometry constraints},\n  author={Liu, Ruijin and Chen, Dapeng and Liu, Tie and Xiong, Zhiliang and Yuan, Zejian},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={36},\n  number={2},\n  pages={1765--1772},\n  year={2022}\n}\n\n@inproceedings{yao2023sparse,\n  title={Sparse Point Guided 3D Lane Detection},\n  author={Yao, Chengtang and Yu, Lidong and Wu, Yuwei and Jia, Yunde},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={8363--8372},\n  year={2023}\n}\n\n@inproceedings{li2022reconstruct,\n  title={Reconstruct from top view: A 3d lane detection approach based on geometry structure prior},\n  author={Li, Chenguang and Shi, Jia and Wang, Ya and Cheng, Guangliang},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={4370--4379},\n  year={2022}\n}\n\n@inproceedings{ai2023ws,\n  title={WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels},\n  author={Ai, Jianyong and Ding, Wenbo and Zhao, Jiuhua and Zhong, Jiachen},\n  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},\n  pages={5595--5601},\n  year={2023},\n  organization={IEEE}\n}"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_7Rt7",
                    "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3017/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699253165838,
        "cdate": 1699253165838,
        "tmdate": 1700703291918,
        "mdate": 1700703291918,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ju3R2Fez4g",
        "forum": "l1U6sEgYkb",
        "replyto": "l1U6sEgYkb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a LiDAR Camera 3D-Lane detection model. This model consists of 4 main blocks: 1) two backbones (image in camera view (CV) and LiDAR (BEV) in bird-eye-view) linked by a bidirectional feature fusion, 2) a dual view query generation with clustering between BEV and CV queries, 3) a decoder with query clustering that produce point queries and 4) a 3D dual-view deformable attention producing a 3D lane prediction. The experimental part shows that the proposed model outperforms SOTA, including the last ICCV2023 papers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "One original contribution is the bidirectional feature fusion module used to train the backbones in both BEV and CV leveraging for each view information from both sensors. The experimental part shows that this mechanism increases the F1 score about 2%. \nAnother important part of the pipeline is a Unified Query Generation process that win less than 1% F1 score. \nThe authors a 3D Dual-view Deformable Attention model that slightly improves the F1 score of the model. \nRegarding the ablation study, the sensor fusion win more than 10% regarding the LiDAR only and about 20% regarding only the camera. \nExperiments have been achieved on the public dataset OpenLane. The proposed model outperforms SOTA."
            },
            "weaknesses": {
                "value": "there are no really weakness in the paper. \nI should be interesting to experiment your models on other datasets like  OpenLane-Huawei: a dataset with more horizontal lines.\n\nminor typo : \n\ninstruction <-> introduction\nIncrease or remove line (d) of fig 7. (figures are too small)"
            },
            "questions": {
                "value": "Did you try to experiment your models on other datasets like  OpenLane-Huawei: a dataset with more horizontal lines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3017/Reviewer_nYY6"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3017/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699808107182,
        "cdate": 1699808107182,
        "tmdate": 1699808107182,
        "mdate": 1699808107182,
        "license": "CC BY 4.0",
        "version": 2
    }
]