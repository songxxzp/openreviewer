[
    {
        "id": "Nti7ck4mMn",
        "forum": "hEpdbd4VTg",
        "replyto": "hEpdbd4VTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_CDfS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_CDfS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduce 3DiffTectio, a 3D detection model using posed images, based on a generative diffusion model. It overcomes the limitations of current diffusion models in 3D tasks and leverage two controlnet to refine the diffusion feature to be 3D-aware, ultimately excelling at identifying cross-view correspondences. The proposed method outperforms predecessors like Cube-RCNN by 9.43% on a specific dataset and showcases impressive data efficiency and cross-domain generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is quite novel, revealing that the features of generative models are also suitable for downstream perception tasks.\n2. The figures and datasets chosen in the paper effectively elucidate its motivation and the viability of the proposed method. \n3. The performance is quite good."
            },
            "weaknesses": {
                "value": "1. I am quite doubt whether the geometric ControlNet truly introduces 3D awareness. Although they trained the ControlNet on posed images using novel view synthesis, the inclusion of a warping operation in the ControlNet suggests that the diffusion model is simply performing an image completion on the warped features.\n2. The method is trained on video data, which means it posses the piror knowledge on general 3D scene. In contrast, the baseline method has not been trained on posed images, making this comparison somewhat unfair.\n3. For perception tasks, the size of the model and its runtime need to be considered. The combination of ControlNet + diffusion might make the model inefficient."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2327/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698322579475,
        "cdate": 1698322579475,
        "tmdate": 1699636165218,
        "mdate": 1699636165218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2eKDf04BT0",
        "forum": "hEpdbd4VTg",
        "replyto": "hEpdbd4VTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_T1bR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_T1bR"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"3DiffTection,\" an advanced methodology for 3D detection from posed images, leveraging features from a 3D-aware diffusion model. The approach adeptly addresses the challenges associated with annotating large-scale image data for 3D object detection. By integrating geometric and semantic tuning strategies, the authors have augmented the capabilities of existing diffusion models, ensuring their applicability to 3D tasks. The method notably surpasses previous benchmarks, demonstrating high label efficiency and robust adaptability to cross-domain data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The methodology effectively circumvents the challenges of annotating large-scale image data for 3D object detection.\n2. Through the integration of geometric and semantic tuning strategies, the authors have enhanced the capabilities of diffusion models"
            },
            "weaknesses": {
                "value": "1.The performance on a broader range of datasets is missing, and it should also be compared with more recent research.\n\n2.Semantic ControlNet lacks a more comprehensive analysis."
            },
            "questions": {
                "value": "1.Could you provide further explanations regarding how Semantic ControlNet and Novel View Synthesis assist in enhancing models, along with corresponding analyses?\n\n2.Could you present comparative performance results of the more models across the more datasets?\n\n1. In the stage2, is the input noised source image or pure gaussian noise?\n2. is there any other generative model can get the same improvement by embedding geometry and semantic control?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Reviewer_T1bR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2327/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839908503,
        "cdate": 1698839908503,
        "tmdate": 1699636165148,
        "mdate": 1699636165148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dg9EJF8cA1",
        "forum": "hEpdbd4VTg",
        "replyto": "hEpdbd4VTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_16V1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2327/Reviewer_16V1"
        ],
        "content": {
            "summary": {
                "value": "The manuscript addresses the task of 3D object detection from posed images by leveraging the 2D feature space of pre-trained large diffusion models and exploiting ControlNet to integrate 3D geometric awareness and auxiliary semantic infomation. Extensive experiments on Omni3D datasets demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The manuscripts first proposes to improve 3D awareness by aggregating features with ControlNet from auxiliary views \n* The method proposed in the manuscript achieve significant margins over comparable baselines."
            },
            "weaknesses": {
                "value": "* The novelty seems limited. Though with the insight of integrating 3D awareness and closing the domain gap with auxiliary semantic information, the actual practice is adopting existing work ControlNet (Zhang et al., 2023)[^1]. The proposed method is more like an application of ControlNet on a specific task (in this case, the task of 3D object detection from posed images).\n* The sampling strategy on the epipolar line needs clarification. If the line of sight is blocked by objects, it is unreasonable to include features sampled behind the blocking objects. It is recommended to provide more details on how to avoid aggregate sampling features from blocked views.\n* Minor problems in presentation. *Diffusiondet: Diffusion model for object detection* appears twice in the *Reference* section\n\n[^1]: Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023."
            },
            "questions": {
                "value": "See *Weaknesses* section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2327/Reviewer_16V1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2327/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699417939803,
        "cdate": 1699417939803,
        "tmdate": 1699636165078,
        "mdate": 1699636165078,
        "license": "CC BY 4.0",
        "version": 2
    }
]