[
    {
        "id": "8ten7NbN9s",
        "forum": "9NKRfhKgzI",
        "replyto": "9NKRfhKgzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_qq4Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_qq4Y"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method that is both adversarially robust as well as good at private attribute obfuscation while ensuring the information with respect to the utility variable is intact. They claim that unlike prior works they merge the two objectives using information theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "the overall idea of the paper is quite relevant as we need to unify the notion of robustness to adversaries of all sorts in a training procedure. However several questions remain unanswered in the current work. Hope the review comments here help authors improve their current manuscript."
            },
            "weaknesses": {
                "value": "- It is not clear from the paper how the different networks and losses of each network are implemented. E.g., Fig. 1 still defines networks in terms of mutual information while the networks themselves are actually approximating those via cross-entropy and MINE estimator. It would be better to also show the corresponding losses the networks are optimizing over.\n- Just like the adversarial private representation learning, the proposed method ARPRL does not have privacy guarantees. \n- Also while the authors start the initial analysis using an information theoretic approach they eventually resort to upper bounds that result in various cross-entropy losses and a MINE estimator. These upper and lower bounds are known to be quite loose. So in effect, while the authors claim that this is driven by information theory, the final optimization seems to be quite disjoint from this and depends on several networks some of which are adversarial while others are cooperative.\n- Some of the results are quite similar to prior works:\n\t- information theoretic analysis is quite similar to [1]\n\t- The intuition about random guessing based on eqn 5 is quite similar to [2]\n- Some possible use cases/baselines that exist but have not been considered by the authors include\n\t- Also unlike some prior works such as [2] the proposed method can only work with one utility and one sensitive target. \n\t- closed-form solutions by [3] give very robust benchmarks to compare against\n- The proposed method is also not compared against private learning algorithms if the representation learner is trained with adversarial examples. I believe a simple augmentation to existing works such as [1, 2, 3] might serve as good baselines to compare against.\n\n1. Azam, Sheikh Shams, et al. \"Can we generalize and distribute private representation learning?.\" _International Conference on Artificial Intelligence and Statistics_. PMLR, 2022.\n2. Bertran, Martin, et al. \"Adversarially learned representations for information obfuscation and inference.\" _International Conference on Machine Learning_. PMLR, 2019.\n3. Sadeghi, Bashir, and Vishnu Naresh Boddeti. \"Imparting fairness to pre-trained biased representations.\" _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_. 2020."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792348838,
        "cdate": 1698792348838,
        "tmdate": 1699636430974,
        "mdate": 1699636430974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UlxmSfOnH0",
        "forum": "9NKRfhKgzI",
        "replyto": "9NKRfhKgzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_jNV8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_jNV8"
        ],
        "content": {
            "summary": {
                "value": "The paper studied the setting of combining adversarially attack and privacy-preserving problems together and tackled this from an information theoretic perspective. Specifically, the author is inspired by the setting of adversarial perturbation under the framework of representation vulnerability and defines the notion of attribute inference advantage. The paper aims for three different goals, which are utility preservation, privacy, and adversarially robustness, and achieves them by formulating the objectives using mutual information, it further proposes the model ARPRL, by using different types of neural-network-based mutual information estimators. The paper studied the tradeoffs between utility, adversarial robustness, and privacy, by drawing several theoretical conclusions. Finally, the paper evaluated the method\u2019s performance and the tradeoff between utility, adversarial robustness, and privacy on synthetic and real-world datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of the paper is the formulation of studying both adversarial robustness and privacy and the systematic study of these tradeoffs through both theoretical analysis and experiments."
            },
            "weaknesses": {
                "value": "While the paper's formulation of the problem and the systematic study are of importance, the current version has several weaknesses. \n\n1. The presentation can be generally further improved. Several places in the current paper need further clarification, the details can be found in the Questions section. \n\n2. While the theoretical analysis is of importance, currently it is limited to the binary setting, which can be not practical. \n\n3. The paper writing can be further improved. Firstly, the paper studied several different objectives, including utility, adversarial robustness, and privacy, but there is not a consistent order of introduction in different sections, for example, in Sections 2 and 3, the order is first adversarial robustness then privacy. While in Section 4, the order becomes first privacy, then utility, then adversarial robustness. Also, both adversarial and privacy objectives are defined as something with an abbreviation \"Adv\". One stands for adversarial, and the other stands for advantage. These can introduce difficulty in understanding while reading."
            },
            "questions": {
                "value": "**Method:** \n\nI'm having difficulty understanding the reason for conditioning on u for formalizing Goal 2 and Goal 3, as the first goal's objective already minimizes the I(z,u), why does the method need to maximize/minimize the conditional mutual information instead of just the mutual information? Also, for example, considering Goal 3 alone, should we want the representation z to be robust to the adversarial perturbance regarding all the mutual information I(x;z) instead of just the mutual information excluding u while excluding u could make the representation not truly adversarially robust? What is the benefit of conditioning on u for formalizing Goal 2 and Goal 3?\n\nBesides, if not conditioning on u, then the combination of Goal 1 and Goal 2, will also be closely related to an information bottleneck objective, of which its relation to privacy (privacy-utility tradeoff) has been discussed in some previous literature, such as [1]. \n\nThis also raises the question of the necessity to use four different neural networks to achieve the goal of this paper.  The training of neural network-based mutual information method can require interactive training between MI estimation and maximization/minimization and training several different mutual information-based objectives at the same time could result in unnecessary unstability.  \n\nShould Equation 14 (first line as an example) be I(x;z|u) = H(x|u) - H(x|z,u)?\n\n\n**Experiments:** \n\nIn Section 5.4, the first paragraph \"Comparing with task-known privacy-protection baselines\", what is the alpha and beta used in this setting? The result with test acc 0.88 and infer acc 0.71 is not shown in the table. Is the result of ARPRL Section 5.4 computed as the optimal result from different combinations of alpha and beta? How sensitive is the result to different values of alpha and beta?\n\nFor the second paragraph \"Comparing with task-known adversarial robustness baselines\", as the proposed method is described as a task-agnostic method in the previous sections, how does the method include task labels during training in this setting?\n\nFor the third paragraph, what is the definition of the best \"utility, privacy, and robustness tradeoff\"? Why is it just one set of results?\n\n\nBesides, it says in the paper that the whole network is trained using SGD using learning rate 1e\u22123. Does it include all four networks (which means in Algorithm 1 lr1=lr2=lr3=lr4) or does each network need some extra different finetuning? As there are four different networks in the model, it would be good to discuss how sensitive is the training concerning these hyperparameters. \n\nMinor problem: I think the lr5 in Algorithm 1 is defined but not used.\n\n[1] Makhdoumi, Ali, et al. \"From the information bottleneck to the privacy funnel.\" 2014 IEEE Information Theory Workshop (ITW 2014). IEEE, 2014."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4539/Reviewer_jNV8",
                    "ICLR.cc/2024/Conference/Submission4539/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4539/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700035500570,
        "cdate": 1700035500570,
        "tmdate": 1700036263232,
        "mdate": 1700036263232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WTbfFED7Xt",
        "forum": "9NKRfhKgzI",
        "replyto": "9NKRfhKgzI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_qrh8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4539/Reviewer_qrh8"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the question of training models that are simultaneously robust to adversarial examples, and to attribute inference attacks. An objective function is proposed based on several (conditional) mutual information terms, one associated with each of three objectives (utility, robustness to adversarial examples, privacy). The objective is optimized using (previously proposed) approximations to these MI terms.\nA theoretical analysis shows that there are inherent trade-offs (for any learned representation) between these objectives."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem is well-motivated and relevant to the community.\n- The objective function is well-motivated, and the authors made an effort to give an intuitive explanation.\n- The paper explains how it builds on prior work.\n- The synthetic experiments further contribute to the intuition. The empirical evaluation on real data shows promising performance."
            },
            "weaknesses": {
                "value": "- The overall presentation needs improvement. The paper is centered around three objectives (utility, adversarial robustness, attribute inference) and these seem to appear in arbitrary and inconsistent order throughout the different sections. The paper can benefit from a more careful organization.\n- It seems that most (all?) of the results in section 4.4 state bounds that hold for any representation f (under norm constraints etc.), rather than for the particular learned representation that optimizes the proposed objective (17). The statements of Thm 4 and 5 seem to indicate that this is specific to the particular representation optimizing (17), but I don't see evidence of this in the proof. Please clarify.\n- The remark following Thm 3 and 4 are unclear. What is meant by \"a risk on at least a private attribute value\", and (in Thm 4) what does the \"adversarially learnt representation\" refer to?\n- Most of the conclusions offered by Section 4.4 are not surprising. For example, that maximizing the conditional entropy H(u|z) reduces vulnerability to attribute attack seems unsurprising. I expected that the analysis would offer some quantitative characterization of the trade-offs (ideally in terms of the $\\alpha, \\beta$ parameters in the optimized objective) but this is not the case. In summary, I am unsure what the goal of the analysis is precisely. Is it to provide additional motivation for the proposed objective? (motivation was already provided in Section 3).\n- In the experiments, there is a missed opportunity to precisely explore the trade-off between the three objectives. For example in Table 1 alpha and beta are changed simultaneously. Why choose these particular values of alpha, beta?\n- In the experiments: there seems to be some amount of speculation. Some conclusions are presented as facts without evidence. For example, it's unclear how the authors attribute the poor performance of DPFE to its assumption that the learned representation is Gaussian. It's also unclear why being task agnostic would necessarily mean sacrificing privacy.\n- Minor: (i) there are several typos. (ii) It's unclear why restricting to binary private attributes is without loss of generality."
            },
            "questions": {
                "value": "- I invite the authors to comment on the interpretation of the results of Section 4.4 and what they bring to the picture (see points above).\n- About the attack model that the paper considers: On the one hand, the \"robustness objective\" seems to assume that the adversary has access to the input x (since the adversary optimizes over a ball centered at x). On the other hand, the \"privacy leakage\" objective seems to assume that the adversary only has access to z but not x (since the classifier is assumed to only take z as input). This discrepancy needs some discussion. Can you clarify the attack model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4539/Reviewer_qrh8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4539/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700704292827,
        "cdate": 1700704292827,
        "tmdate": 1700704292827,
        "mdate": 1700704292827,
        "license": "CC BY 4.0",
        "version": 2
    }
]