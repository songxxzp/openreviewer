[
    {
        "id": "v7tsjMj3HR",
        "forum": "SgjAojPKb3",
        "replyto": "SgjAojPKb3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission366/Reviewer_AABJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission366/Reviewer_AABJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to address the task of 3D open set detection by harassing implicit reconstruction methods, and specifically NeRF.\nIf I am not mistaken, the method boils down to adding supervision/prediction-head of per-pixel CLIP features in addition to the RGB/color head, while keeping the per-voxel density $\\sigma$ prediction used in NeRF. \nThe authors use depth supervision, when available, as done in previous works.\n\nThe devil lies, as always, in the details of the training regime, and the authors stress some details that are important to make the network converge to the presented results:\nThe authors use OpenSeg feature maps which are more localized and therefore do not supervise them in a multi-scale fashion.\nAdditionally, one should refrain from using such features near the image edge as they are less stable.\nFinally, one of the main points in the training process, the authors claim that training on novel views (generated by a trained NeRF) makes as big difference. Specifically, novel views are selected to minimize the uncertainty of the OpenSeg features, measured as generalized variance on the in 3D. This help mitigate areas where open-set features \"disagree\" from multiple viewpoints.\n\nThe experimental section evaluates the method compared to the previous 3D open-set approaches, which are all explicit.\nThe authors divide the labels into 3 groups by frequency, which better emphasized the strength of the propose method on rare ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper was rather easy to follow, for an experienced NeRF reader (which is reasonable these days :-) ).\n* The base of the method is rather straight forward to understand (adding CLIP predictions)\n* The authors indicate uncertainty estimation of the open-set head is  a strong signal to help the network converge, with help of NeRF novel views.\n* The results show the advantage of implicit methods over explicit ones\n* The ablation study helps to understand which part of the training regime made the difference."
            },
            "weaknesses": {
                "value": "Many of the details of the paper were presented in previous works (e.g. per-pixel feature supervision was presented in NeRF-SOS fo DINO). and a big part of the novelty here is in the details of applying them to 3D segmentation.\nThis alone does not prevent acceptance, IMHO."
            },
            "questions": {
                "value": "A question on the novel-view based training: \nCan you please comment on the seemingly bootstrapped nature of this approach?\nIn other words - a NeRF model has to be trained in the first place to generate novel views, and these novel views are then trained to predict open-set features?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission366/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission366/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission366/Reviewer_AABJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698126702689,
        "cdate": 1698126702689,
        "tmdate": 1700633583947,
        "mdate": 1700633583947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1WLVw8ePS1",
        "forum": "SgjAojPKb3",
        "replyto": "SgjAojPKb3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission366/Reviewer_UKPx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission366/Reviewer_UKPx"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a NeRF-based scene representation for open-set 3D semantic scene understanding. The method extends previous work by leveraging pixel-aligned features and a mechanism to identify regions that require generating novel views. These additional views of the scene allow the proposed approach to extract more open-set features and improve the overall understanding of the 3D semantic scene. The results have been tested on the Replica dataset and have outperformed a mesh-based baseline and a NeRF-based baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-written and justified. All the technical details are well-elaborated.\n* The concept of using novel view synthesis capabilities to extract additional visual-language features for better scene understanding is interesting and straightforward.\n* The qualitative results appear satisfactory and the quantitative results outperform OpenScene and LERF."
            },
            "weaknesses": {
                "value": "My main concern is regarding the evaluation:\n* While the Replica dataset may be challenging for long-tailed settings, it would be great to evaluate on larger-scaled datasets and outdoor benchmarks such as Matterport3D, ScanNet, and nuScenes.\n* One of the main contributions of this work is the use of novel view synthesis to extract additional visual-language features. To ensure the quality of the learned features, it seems necessary to evaluate the quality of the rendered views and their effect on scene understanding.\n* To better evaluate the proposed method, it would be better to compare it with 2D methods such as LSeg, OV-Seg, and ODISE, as well as other 3D methods including Feature Field Distillation, Semantic-NeRF, and Panoptic Lifting (disregard whether these methods are open or closed-set). \n\n\nSome questions regarding the confidence estimation and novel camera view selection:\n* The uncertainty map in Figure 3 indicates that the door and cabinet are the most uncertain points, while the generated novel pose in Figure 2 seems to focus primarily on the table. Given high uncertainty points, how are the camera views sampled?\n* Although there is a positive correlation between per-point uncertainty and per-point error, the heat maps appear to emphasize all areas other than the wall and floor. It is also intriguing that the peaks of uncertainties don't correspond to object boundaries, but rather the entire object. It would be better to demonstrate more samples to showcase these findings."
            },
            "questions": {
                "value": "Please see the weaknesses part for my concern and confusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781921509,
        "cdate": 1698781921509,
        "tmdate": 1699635963824,
        "mdate": 1699635963824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "73Ttd4eMB1",
        "forum": "SgjAojPKb3",
        "replyto": "SgjAojPKb3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission366/Reviewer_Y2vi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission366/Reviewer_Y2vi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces OpenNeRF, an innovative approach for superior open-set 3D semantic scene understanding. OpenNeRF demonstrates its suitability for detecting small long-tail objects, surpassing mesh-based representations in performance. Additionally, OpenNeRF effectively represents interesting scene parts, leading to improved segmentation performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed OpenNeRF approach significantly improves 3D semantic segmentation results compared to baseline methods such as LERF and OpenScene. It can also detect challenging long-tail classes ignored by other methods.\n2. Confidence estimation and analysis of novel views bring new insights for improving the Open-set 3D scene understanding.\n3. OpenNeRF can be queried for arbitrary concepts, including object properties (eg. reflective, soft) and material types (eg. wood, cotton), showcasing its versatility. It shows impressive results in the wild scene using a phone scanner."
            },
            "weaknesses": {
                "value": "1. The performance drop when rendering novel views from random positions suggests that the approach is sensitive to the quality and meaningful context of the additional views."
            },
            "questions": {
                "value": "1. Could you provide more details on the evaluation protocol used to compare explicit-based (mesh/point cloud) and implicit-based (NeRF) methods for open-set 3D semantic segmentation? What were the key findings of this evaluation?\n2. Have you considered the scalability and computational efficiency of OpenNeRF? How does it perform in terms of runtime and memory requirements compared to other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830263164,
        "cdate": 1698830263164,
        "tmdate": 1699635963738,
        "mdate": 1699635963738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ua8o7e9n4j",
        "forum": "SgjAojPKb3",
        "replyto": "SgjAojPKb3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission366/Reviewer_Nxv7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission366/Reviewer_Nxv7"
        ],
        "content": {
            "summary": {
                "value": "This work explores a similar paradigm to LeRF (Language-embedded radiance fields). Rather than distilling CLIP features to NeRF, rather it uses a different backbone i.e. OpenSeg to distill OpenSeg features to the 3D using NeRFs. The idea of distilling similar open-set features to 3D has been shown by OpenScene which required 3D meshes as input 3D representation, however, the paper studies it in the setting of Neural Radiance Fields which requires posed 2D images as input. Qualitative open vocabulary segmentation comparisons are shown on the Replica dataset coupled with quantitative comparisons with baselines like LeRF and OpenScene"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The approach discussed very relevant problems i.e. distilling open-set 2D features to the 3D domain and moving away from traditional pipelines that work with a known number of categories i.e. approaches like Semantic-NeRF and Panoptic NeRF etc. The main benefits/strengths of the approach are as follows:\n\n1. Qualitative comparison with LeRF and Open-Scene clearly shows better segmentation results despite the fact that LeRF was not designed for segmentation (which should be clearly pointed out)\n\n2. Clearly better quantitative results on all, head, common, and tail sets\n\n3. A good measure of uncertainty to improve the open-set feature field distillation into 3D."
            },
            "weaknesses": {
                "value": "1.  Though the uncertainty measure is sound, I wonder if this improves LeRF and OpenScene's performance as well. A similar measure was introduced in Semantic-NeRF and the authors didn't highlight the difference between their formulation and Semantic NeRF's formulation. \n\n2. This looks like an incremental work that extends LeRF by using a different encoder backbone which is very straightforward to implement. Can the authors justify it with really good segmentation performance on long tail in-the-wild queries etc? I didn't see that comparison\n\n3. Not many in-the-wild examples could be seen in the paper. Replica dataset is easier since we have perfect viewpoint annotations. I wonder if the performance stays steady or breaks for more in the wild examples where there are imperfect viewpoint annotations. \n\n4. Does uncertainty measure really help? What if the initial NeRFs are not that good? Features tend to break for those viewpoints/areas. Do the authors have improved results in those cases?"
            },
            "questions": {
                "value": "Please see all the questions in the weakness section. Overall I would have liked to see a thorough comparison with LeRF and what technical improvement the work bring (in addition to changing the pre-trained backbone), a comparison with Semantic-NeRF's uncertainty formulation, more in the wild examples and examples where the quality of NeRF is not that good and do the semantic features break or does uncertainty help in those areas"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947562254,
        "cdate": 1698947562254,
        "tmdate": 1699635963670,
        "mdate": 1699635963670,
        "license": "CC BY 4.0",
        "version": 2
    }
]