[
    {
        "id": "ghEy4846jh",
        "forum": "B5Tp4WwZl8",
        "replyto": "B5Tp4WwZl8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_CuB9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_CuB9"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors showed that greedy sparsification (TopK compressor), together with error feedback, can beat distributed gradient descent in terms of theoretical communication complexity, by characterizing its fast convergence rate in a certain regime (that depends on the sparsity parameters c and r). \nSee Example 1 and Theorem 2.\nNumerical experiments are provided Section 6 to validate the proposed theoretical analysis."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses the important problem of communication complexity in distributed ML.\nThe \nThe paper is in good shape."
            },
            "weaknesses": {
                "value": "I do not see particular weakness for the paper but a few comments, see below."
            },
            "questions": {
                "value": "I do not have specific questions but the following general comments for the authors:\n\n1. when referring to the appendix, please specify which section/part of the appendix.\n2. I personally suggests the authors to further elaborate on the limitations of the analysis and future work, and move them to the main text (instead of leaving them in the appendix)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643569773,
        "cdate": 1698643569773,
        "tmdate": 1699636930114,
        "mdate": 1699636930114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XbR2PcJ5f8",
        "forum": "B5Tp4WwZl8",
        "replyto": "B5Tp4WwZl8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_XpPb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_XpPb"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the theoretical advantage of the error feedback mechanism for compressed distributed gradient descent (DGD). The authors first defined two quantities that measure the sparseness and rareness of the data features. Then the non-convex convergence rate shows that when the data has sparse and rare features, EF21 has better communication complexity than DGD. Some simple experiments are conducted to justify the theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strength:\n+ The writing is clear and easy to follow.\n+ The introduction to related works and existing results is comprehensive and helpful to understand the context.\n+ Introducing feature \u201crareness\u201d to the analysis of optimization methods is a good attempt."
            },
            "weaknesses": {
                "value": "Weakness: There are several limitations of this work.\n\n1.\tIn my opinion, the motivation for this work is not very strong. \n\na.\tThe authors claim that the goal is to explain why EF21 empirically performs much better than DGD in terms of communication complexity but theoretically does not. However, their analysis relies on some strong assumptions about the data which are uncommon in practice. Indeed, does the fact that EF21 performs well in practice on many types of data (without strong assumptions) indicate that the feature rareness and sparsity assumed in this paper are NOT the true reasons? While there are many engaging words like \u201cbreakthrough\u201d or \u201cmilestone\u201d, I don\u2019t really feel surprised because a better rate shall be expected when we limit the function class and propose strict data assumptions. But I doubt whether this is the correct direction given the points above.\n\nb.\tIt seems that Lemma 2 \u2013 5 are general results not specific to EF. Can we apply the same analysis and argument to DGD? In other words, can rare and sparse features also improve the rates of DGD?\n\n2.\tIt seems that the arguments are limited to simple linear models. This is because the feature sparsity would lead to model sparsity (which is a key component in the analysis, for example Lemma 5) only for linear models. For non-linear models (for example the DNNs), the arguments will not hold.\n\n3.\tEmpirically, the experiments also only used convex regression models but not more complicated neural networks. There is a discrepancy between the experiments with the non-convex theoretical analysis. So, the experiments are kind of limited.\n\n4.\tAs a theoretical paper, the setups and technicality are not comprehensive and strong enough. The paper only studied deterministic setting without stochastic gradients. SGD-type methods are more practical. What\u2019s the situation in the stochastic setting? Does the same problem exist? From the technical perspective, the main modification in the proof compared with prior works is improving Young\u2019s inequality and the smoothness constant $L$ using rareness/sparsity. This is not very challenging and novel in my evaluation."
            },
            "questions": {
                "value": "See as above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735144216,
        "cdate": 1698735144216,
        "tmdate": 1699636929992,
        "mdate": 1699636929992,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7qPGsRixxn",
        "forum": "B5Tp4WwZl8",
        "replyto": "B5Tp4WwZl8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_xzAm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_xzAm"
        ],
        "content": {
            "summary": {
                "value": "This paper studies error feedback in distributed optimization. It provides a first theoretical analysis of how greedy sparsification and error feedback can improve the communication complexity of distributed gradient descent. Specifically, when $\\sqrt{\\frac{c}{n}}L_+ \\leq L$, the communication complexity improves. Numerical experiments are conducted to validate the theoretical results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well written and most ideas are presented in a straightforward and easy-to-read manner.\n2. It provides meaningful observations to motivate the research.\n3. Theoretical results are solid and only rely on simple and standard assumptions."
            },
            "weaknesses": {
                "value": "1. I have some concerns about the novelty and contribution of the paper since it mostly builds on the previous work EF21. I hope the authors can clarify this and explain how this work advances error feedback algorithms or distributed optimization in the future.\n2. As the authors said in the paper, the experiments are rather toy and the practical applicability is limited because most real-world datasets are not sparse enough. It would be helpful and convincing to conduct some experiments on real-world datasets."
            },
            "questions": {
                "value": "in the above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Reviewer_xzAm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744631362,
        "cdate": 1698744631362,
        "tmdate": 1699636929898,
        "mdate": 1699636929898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "txe97KBEna",
        "forum": "B5Tp4WwZl8",
        "replyto": "B5Tp4WwZl8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_h2wk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7648/Reviewer_h2wk"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors try to make sense on the gap in our understanding of the theoretical and practical aspects of gradient descent algorithms in the distributed setting. The try to reason why the algorithm based on heuristics like the greedy sparsification and error feedback performs better in practice than the distributed gradient descent, but theoretically the opposite is observed. They identify scenarios when \"features are rare\" and prove that in these scenarios one can prove that the performance of the heuristic algorithms are better than the distributed gradient descent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is one of the few papers trying to understand, or rather prove theoretically, why is a heuristic algorithm performing better than another algorithm in practice though theory suggests otherwise."
            },
            "weaknesses": {
                "value": "The paper proves that under certain assumptions the EP21 algorithm performs better than the DGD algorithm. The assumptions are quite strong and hence it is not clear if this is best scenario to explain the performance of EP21 algorithm."
            },
            "questions": {
                "value": "Can you say how often one expects to find real life data satisfying the assumptions under which the improved theoretical study is done?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7648/Reviewer_h2wk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832169617,
        "cdate": 1698832169617,
        "tmdate": 1699636929763,
        "mdate": 1699636929763,
        "license": "CC BY 4.0",
        "version": 2
    }
]