[
    {
        "id": "hOuy77S4uB",
        "forum": "MnMWa94t12",
        "replyto": "MnMWa94t12",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_XJou"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_XJou"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Dynamic Scene Transformer that learns latent neural scene representations from monocular dynamic video without any pose information. Different from previous works, this paper mainly focuses on modeling the latent space for dynamic scenes. To achieve this, the authors utilized  a Camera Estimator and a Dynamics Estimator to produce the low-dimensional controllable latents for camera pose and scene dynamics. To separate dynamics from camera pose effectively, the author further design a swap training scheme and establish a multi-view, multi-dynamics dataset synthetic dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-motivated. The primary goal of this paper is the separation of scene dynamics and camera pose, while most of existing works only cover the static scenes.\n2. The authors proposes a novel training scheme that disentangles the camera pose from two views under the same camera while containing a moving object, and disentangles the scene dynamic from two views with still objects while under two different cameras. To fulfill this training strategy, the authors also establish a new synthetic data with multi-view, multi-dynamics data."
            },
            "weaknesses": {
                "value": "1. The method is quite similar to RUST[1]. The encoder, decoder and camera estimator are almost the same as the ones proposed in RUST. \n2. Inference procedure. From the method architecture, the target view is required to obtain the camera latents and dynamic latents. In this case,  I wonder if the specific novel view image is needed as the input to generate the novel view?  \n3. Control the latent code. In Fig7, the authors show the results of controlling camera latent and dynamic latent. The authors could explain how to control the latent code.\n4. Some quantitative results on real data should be provided.\n\n\n[1] RUST: Latent Neural Scene Representations from Unposed Imagery. CVPR, 2023."
            },
            "questions": {
                "value": "Reproducibility Statement should be put in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743858917,
        "cdate": 1698743858917,
        "tmdate": 1699636597449,
        "mdate": 1699636597449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yvG9QDOlfO",
        "forum": "MnMWa94t12",
        "replyto": "MnMWa94t12",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_Qe3y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_Qe3y"
        ],
        "content": {
            "summary": {
                "value": "- The authors propose Dynamic Scene Transformer (DyST) which makes a model infer the target view\u2019s control latents (camera pose, scene dynamics) with a pair of corresponding views (different camera pose and scene dynamics from target view).\n- Moreover, the authors propose a synthetic dataset, Dynamic Shapenet Objects (DySO), which consists of 5 scene dynamics and 5 camera views for each dynamic scene video to train the DyST.\n- By showing qualitative and quantitative results of the experiments on changing control latents, the authors validate that the DyST learns latent decomposition of the space into scene content."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors co-train synthetic and real-world datasets to transfer the dynamics and camera control potential of synthetic scenes to natural monocular video and the results shown in Fig. 5 indicate that the model has learned to encode dynamics independently of the camera pose.\n- Since there is no architectural difference between camera pose and scene dynamics, the authors propose to enforce separation through a novel latent control swap training scheme, and the results in Fig. 3 demonstrate their method with a high improvement in PSNR scores."
            },
            "weaknesses": {
                "value": "- [Generalization in different types of motions] Additional experiments are needed to see if the proposed DyST model can generalize to camera poses and scene dynamics that were not seen during training. so, it would be better to provide qualitative results on how the controlled view looks like when horizontal shifts are input after training without horizontal shifts. (DySO\u2019s camera motions consist of 4 horizontal shifts, panning, zooming motions, and random camera points)\n- [Cluttered background] Since the backgrounds of the DySO dataset in Fig. 2 and 3 are clean, the authors need to experiment to see if DyST can robustly control the view even when using videos with cluttered backgrounds. In addition, it would be better to have a distance analysis for unclean scenes to see how distinctly it separates camera pose and scene dynamics like the experiment in Figure 5.\n- [Quantitative comparison] As the authors mentioned in Sec. 5 Conclusion, unlike NeRF's output, the output of the proposed method has a quality gap, such as objects disappearing or blurring. Therefore, quantitative comparison results such as PSNR and LPIPS between NeRF and DyST are needed.\n- [Multiple objects] Also, as mentioned by the authors in the same section, the authors did not provide results for multiple object scenes. It would be helpful to see the results of latent distance analysis in Figure 5, PSNR, and LPIPS in Figure3 for multiple object scenes."
            },
            "questions": {
                "value": "- The latent control swap training scheme needs 3 input views. It would be helpful why 3 input views are needed and how the performance changes with less or more than 3 input views.\n- It would be better if the authors discuss why it needs the contrastiveness metric and what the authors are trying to show with the swap in Table 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814928609,
        "cdate": 1698814928609,
        "tmdate": 1699636597336,
        "mdate": 1699636597336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "72ac6PQsWq",
        "forum": "MnMWa94t12",
        "replyto": "MnMWa94t12",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_whxo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5714/Reviewer_whxo"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to estimate motion and object pose and shape from monocular videos using a latent neural representation. \nTwo modules are trained for camera parameter and object position and shape estimations. Then, these modules are then used as input to a third decoder module to generate novel views. To ensure the specialization of the modules, the authors proposed a training procedure where the data is organized to enforce which information is learned by each module. Evaluations are conducted on a newly created dataset which was used to train the model and qualitative results are presented with motion extraction, novel view generation, video manipulations where novel camera motions or object movements are generated."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Compared to the state of the art, this work investigates the more difficult setting of estimating moving objects and moving cameras only from a few motions pictures and a monocular camera. Moreover, they remove the assumptions of training one model for each scene. \n\nThe separation of 3D structure estimation and camera motion is an interesting property of the model. The training tricks illustrated by Eq. 5 and Eq. 6 provide an practical way of enforcing this while still retaining the benefit of end to end training.\n\nAlthough the videos are simple and are still far from the complexity of most real world data,it is still a good compromise as a next step toward more mature systems. Experiments shown in Fig.3 to assess the specialization of the different modules are convincing, it is also supported by the qualitative results shown in the videos in the supplementary material on video manipulation and image synthesis.\n\nExperiments seems reproducible, given code and training parameters, and available datasets will be provided."
            },
            "weaknesses": {
                "value": "The amplitude of the motion would probably limits the accuracy of the method. In Fig.7 the motion is tiny, and this is not evaluated by the authors. Although the encoder and decoder architectures are rather small for the \"simple\" cases covered by the paper, I have concerns on the scalability of this method to more real cases and more complex motions."
            },
            "questions": {
                "value": "Is the latent dynamic space somehow interpretable, and is it possible to generate one instance based on the object position in space, or does the latent space always be inferred from an existing image contained in the processed sequence ? \nIn the later case, this means the approach cannot be used to recreate dynamics that do not exist yet in the data ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5714/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840626436,
        "cdate": 1698840626436,
        "tmdate": 1699636597213,
        "mdate": 1699636597213,
        "license": "CC BY 4.0",
        "version": 2
    }
]