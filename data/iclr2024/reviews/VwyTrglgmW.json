[
    {
        "id": "9ShecYr4Xw",
        "forum": "VwyTrglgmW",
        "replyto": "VwyTrglgmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_ShRs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_ShRs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new PU learning method. Specifically, the authors develop a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified. They adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe studied problem in this paper is very important.\n2.\tThe experiments are sufficient."
            },
            "weaknesses": {
                "value": "1.\tThe authors claim that the existing PU learning methods will suffer a gradual decline in performance as the dimensionality of the data increases. It would be better if the authors can visualize this effect. This is very important as this is the research motivation of this paper.\n2.\tSince the authors claim that the high dimensionality is harmful for the PU methods, have the authors tried to firstly implement dimension reduction via some existing approaches and then deploy traditional PU classifiers?\n3.\tIn problem setup, the authors should clarify whether their method belongs to case-control PU learning or censoring PU learning, as their generation ways of P data and U data are quite different. \n4.\tThe proposed algorithm contains Kmeans operation. Note that if there are many examples with high dimension, Kmeans will be very inefficient.\n5.\tThe authors should compare their algorithm with SOTA methods and typical methods on these benchmark datasets.\n6.\tThe figures in this paper are in low quality. Besides, the writing of this paper is also far from perfect."
            },
            "questions": {
                "value": "see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697546037518,
        "cdate": 1697546037518,
        "tmdate": 1699636750990,
        "mdate": 1699636750990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UiFGOelLrF",
        "forum": "VwyTrglgmW",
        "replyto": "VwyTrglgmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_w13V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_w13V"
        ],
        "content": {
            "summary": {
                "value": "This paper works on PU learning and proposes a new representation learning method for it. The paper uses a codebook to store representations and forces P and U data to be similar to different codebook vectors, respectively. Then, they use a K-means algorithm to cluster feature representations and derive the classifier. Experiments validate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written.\n- The idea of introducing codebook representations into PU learning is novel."
            },
            "weaknesses": {
                "value": "- It is still unclear to me why the proposed method works for PU learning. Although the authors provided some theoretical explanations, I am still not clear why the proposed method can separate feature representations of P and N data.\n- The proposed method is influenced by the center representations of P and U data ($\\mu_P$ and $\\mu_U$). If the two representations are too close, there seems to be no guarantee that the method will work well. \n- In Eq.(6), the authors claim that they do not need $\\alpha$. However, they still need to know the labels of the unlabeled data. But if we know the labels of the unlabeled data, we can calculate $\\alpha$. So I do not think the analysis is useful here.\n- The experiment design is too simple. The authors should include more experiments, such as more compared approaches, and more experimental settings (such as different $\\alpha$). The current experiments are too simple to validate the effectiveness of the proposed approach."
            },
            "questions": {
                "value": "- Why does the proposed method work well? \n- Is the method affected by the feature separability of the training data?\n- Can the authors add more experiments to verify the proposal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697718778109,
        "cdate": 1697718778109,
        "tmdate": 1699636750863,
        "mdate": 1699636750863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dj9Vbv6vJ8",
        "forum": "VwyTrglgmW",
        "replyto": "VwyTrglgmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_Wr1j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_Wr1j"
        ],
        "content": {
            "summary": {
                "value": "The authors in this paper focus on positive-unlabeled (PU) learning and attempt to encode positive and unlabeled instances into a more discriminative representation space followed by a simple cluster method, such as K-means. They directly apply the existing vector quantization technique to project the unlabeled data into two distinct clusters. The experimental results show the effectiveness of the vector quantization method.\n\nThough the idea of learning a disentangling representation for PU learning may be interesting, applying the existing vector quantization technique directly limits the contribution of this paper."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-\tThis paper is well-written and quite easy to follow.\n-\tThe experimental results and ablation study show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "-\tThe innovation of this paper seems to be limited. In this paper, the authors directly employ the exited vector quantization technique [1] to learn a disentangling representation for PU learning with little modification. Though the idea of learning a disentangling representation for PU learning may be interesting, the contribution of this paper is very limited. Otherwise, there lacks of reference to the original paper \u201cNeural discrete representation learning\u201d [1] of the vector quantization technique.\n-\tThere lack of some current PU approaches as baselines in experiments, such as Robust-PU [2], Dist-PU [3], P3Mix [4].\n-\tEquation (1) misses a \u201c)\u201d, and should be $sg(\\mathbf{v}_j(\\mathbf{x}_{i_p};\\theta))$.\n\n[1] Aaron Van Den Oord, and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. \"Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction.\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3663-3673. 2023.\n\n[3] Yunrui Zhao, Qianqian Xu, Yangbangyan Jiang, Peisong Wen, and Qingming Huang. 2022. Dist-PU: Positive-Unlabeled Learning From a Label Distribution Perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14461\u201314470.\n\n[4] Changchun Li, Ximing Li, Lei Feng, and Jihong Ouyang. 2022. Who is your right mixup partner in positive and unlabeled learning. In International Conference on Learning Representations."
            },
            "questions": {
                "value": "Please see the weakness for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764938331,
        "cdate": 1698764938331,
        "tmdate": 1699636750568,
        "mdate": 1699636750568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BoJCR3JP2j",
        "forum": "VwyTrglgmW",
        "replyto": "VwyTrglgmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_NPjv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6596/Reviewer_NPjv"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a simple method to solve the problem of learning a binary classifier with positive and unlabelled data. The proposed method is based on vector quantiza- tion technique to perform dimension reduction first, and then apply standard k-means algorithm to cluster the unlabelled data into positive and negative 2 clusters. In addition to the experimental evaluation, the paper provides some math intuition and ablation study to support and explain how the proposed method works. In the experiment section, the paper shows that the proposed method can produce comparable results w.r.t state-of-the-art GAN based methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and easy to understand. The results sound good and perhaps easy to re-produce if the authors can publish their code."
            },
            "weaknesses": {
                "value": "1. The idea is simple and the novelty may not be strong enough to publish in such a high standard conference.\n2. The k-means algorithms need to keep running in each iteration. Although the idea is simple, it will be very slow if the data size is huge.\n3. The proposed method is not convinced to handle the case when the labels are imbalanced."
            },
            "questions": {
                "value": "1. According to figure 4, the proposed method seems to fall into an interesting situation where the validation is good but the center of two clusters are closer after more epochs. Can author explain the reason? \n2. Can the proposed method handle imbalanced labelled data? This happens in many real situations, such as CTR prediction. Typically clicks are much less than impressions. However, there will be lots of inventory that may not become impressions and therefore there is no label associate to it.\n3. Would the proposed algorithm sensitive to the initialization of the cluster center?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6596/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6596/Reviewer_NPjv",
                    "ICLR.cc/2024/Conference/Submission6596/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845769196,
        "cdate": 1698845769196,
        "tmdate": 1699892956508,
        "mdate": 1699892956508,
        "license": "CC BY 4.0",
        "version": 2
    }
]