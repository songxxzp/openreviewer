[
    {
        "id": "1RXddyEvWI",
        "forum": "6Gzkhoc6YS",
        "replyto": "6Gzkhoc6YS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_inm4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes PerSAM, a novel training-free method, to customize the general-purpose SAM for personalized object segmentation by using a single image with a reference mask. Additionally, the paper introduces PerSAM-F, an efficient variant that enhances performance by tuning just two parameters within 10 seconds. The effectiveness of the proposed method is demonstrated by comprehensive experiments and ablation studies."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* This paper is well-written and easy to understand.\n\n* This paper first studies an interesting task of customizing a general-purpose segmentation model for personalized scenarios. And the paper presents a highly effective method to address this task.\n\n* The method is simple and easy to follow. The proposed PerSAM can guide SAM to segment target objects by three effective training-free techniques.  By tuning 2 parameters within 10 seconds, PerSAM-F efficiently alleviates the mask ambiguity issue and improves the performance.\n\n* This paper has comprehensive discussions and experiments and shows good performances on various tasks."
            },
            "weaknesses": {
                "value": "The feature semantics of SAM might be limited due to SAM's class-agnostic training. While PerSAM and PerSAM-F demonstrate promising performance in personalized object segmentation, their effectiveness may be constrained by SAM's feature semantics in scenarios involving multiple different objects. This may require additional training to enable better transfer of SAM's features to downstream tasks. Alternatively, introducing other representations with stronger semantics, such as CLIP."
            },
            "questions": {
                "value": "* Can PerSAM be extended to do few-shot segmentation with more reference masks for achieving better performance? In the real scenario, utilizing 5-shot or 10-shot examples does not significantly increase the cost compared to 1-shot, but it does offer more precise visual information.\n\n* Is the fine-tuned PerSAM-F generalized only to a specific object? Can PerSAM generalize to different objects using the same parameters? What about using different fine-tuning methods such as LoRA for multi-objects generalization performance?\n\n* Can PerSAM help DreamBooth achieve more complex multi-object customization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583535079,
        "cdate": 1698583535079,
        "tmdate": 1699636160097,
        "mdate": 1699636160097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u064BYcFlG",
        "forum": "6Gzkhoc6YS",
        "replyto": "6Gzkhoc6YS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_Fp1Y"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a training-free Personalisation approach for SAM. For PerSAM, it uses a reference mask for obtaining the positive-negative location prior and then enable SAM for segmentation by: 1) target-guided attention and 2) target-semantic prompting. To reduce the ambiguity of segmentation scales, PerSAM-F is further proposed to fine-tuning SAM. PerSeg dataset is also constructed to evaluate the personalisation segmentation performance. The paper is also evaluated on the one-shot image and video segmentation benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well organised with clear motivation and easy to understand. The illustration and visualisation figures are well presented.\n\n2. PerSAM is training-free and computationally efficient, where the ablation experiment for PerSAM in Table 4, 5 and 6 are extensive.\n\n3. The paper demonstrates good performance not only on the constructed PerSeg benchmark, but also on many image/video segmentation benchmarks."
            },
            "weaknesses": {
                "value": "1. In the appendix, the author mentioned using dinov2 features. Can the authors also provide the results in Table 2 and 3 by using the default image encoder features of SAM?\n\n2. What is the running speed/ memory consumption of PerSAM comparing to SAM?\n\n3. In Table 2, can the author provide performance comparison to SAM-PT [a]? [a] is a related work in adapting SAM for video object segmentation.\n\n[a] SAM-PT: Extending SAM to zero-shot video segmentation with point-based tracking. arXiv, 2023.\n\n4. On the video object segmentation benchmark, how dose PerSAM differentiate objects with similar appearances? Can PerSAM also work on self-driving scenario with many similar vehicles/cars in the dense traffic?"
            },
            "questions": {
                "value": "Can the paper provide more failure cases visualisation for PerSAM? I am generally positive about this paper. If my concerns in the weakness section can be addressed, I will consider to further upgrade my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679476553,
        "cdate": 1698679476553,
        "tmdate": 1699636160017,
        "mdate": 1699636160017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5SFIujlmBv",
        "forum": "6Gzkhoc6YS",
        "replyto": "6Gzkhoc6YS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a lightweight method to leverage the Segment Anything Model (SAM), to perform single shot image segmentation. SAM is a powerful image segmentation framework, however it is becomes challenging to use it to perform personalized image segmentation by manually tuning its input prompts. In the approach proposed in the paper, the authors present Personalized approach for SAM (PerSeg) which is training free and can be used to segment a particular object (class) across images given only a single example image along with its binary segmentation mask. To do this, the authors present two approaches: (1) target-guided attention, and (ii) target-semantic prompting. Additionally, to alleviate the problem of objects being present in different scales the paper presents a simple fine-tuning technique that keeps SAM frozen, to combine information from multi-scale masks and improve the final segmentation outputs. \n\nA new dataset PerSeg is also introduced to test the proposed method, and finally the paper also shows how PerSAM can be used to improve DreamBooth [1], for the task of custom text-to-image synthesis. \n\nThe paper quantitatively and qualitatively demonstrates the efficacy of the proposed method across multiple datasets, and achieves competitive performance as compared to existing methods.\n\n[1] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem of single-shot image segmentation is an important problem to solve. This has many downstream utilities in real-world applications ranging from design to healthcare. And the paper introduces a simple but effective technique to solve this by leveraging the powerful Segment Anything Module (SAM) [1]. \n\nThe introduced method is called Personalization approach for SAM (PerSAM), and it takes as input a single example image of the desired object we want to segment, and its corresponding segmentation mask. This is then used to segment out the given object across multiple images automatically. To do this PerSeg involves 2 approachs:\n(1) target-guided attention\n(2) target semantic prompting\n\nAdditionally to handle the object occurring in different scales, the paper introduces a lightweight fine-tuning technique that keeps the SAM model frozen, and aggregates information across multiple scales for finer segmentation.\n\nA new testing dataset is also introduced called PerSeg. \n\nThe work also shows the utility of the proposed method for improving text-to-image synthesis [2].\n\nAnd finally the authors show qualitative and quantitative results for the proposed method, and it performs competitively as compared to existing approaches.\n\nOverall a nicely written paper, with a simple idea and good results.\n\n\n[1] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023\n[2] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022"
            },
            "weaknesses": {
                "value": "Overall it is a nicely written paper, with good results.\n\nHowever, it is somewhat lacking in it's quantitative evaluation. The choice of evaluation datasets is limited.\n\nIt would be worthwhile to also see the performance of the proposed method for one-shot segmentation on additional (more challenging) datasets like- MS-COCO, AED20K, CityScapes to also compare with more powerful existing state of the art models.\n\nAlso the comparison is lacking. It would be nice to compare against methods that do zero-shot or text guided segmentation like DenseCLIP [1].\n\n[1]. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting, Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, https://arxiv.org/abs/2112.01518"
            },
            "questions": {
                "value": "How comparable is this work or results against methods that do zero-shot or text guided segmentation like DenseCLIP [1]? And why / why not does it make sense to compare against such methods?\n\n[1]. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting, Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, https://arxiv.org/abs/2112.01518"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2273/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2273/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2273/Reviewer_XA1U"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823909650,
        "cdate": 1698823909650,
        "tmdate": 1699645872460,
        "mdate": 1699645872460,
        "license": "CC BY 4.0",
        "version": 2
    }
]