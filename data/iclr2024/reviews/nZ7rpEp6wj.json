[
    {
        "id": "9k9MnzH8gL",
        "forum": "nZ7rpEp6wj",
        "replyto": "nZ7rpEp6wj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_FcVp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_FcVp"
        ],
        "content": {
            "summary": {
                "value": "Deep operator networks (DeepONets or DONs) have a unique capability to be trained on multi-resolution data, a significant advantage in real-world contexts where high-resolution data may be challenging to acquire. However, traditional DeepONets face difficulties in maintaining dependencies over extended sequences. To address this, the paper introduces a novel architecture called DON-LSTM, which merges the benefits of DeepONets with the temporal pattern recognition of long short-term memory networks (LSTM). This combination allows the model to effectively utilize multi-resolution data and capture time-dependent evolutions. The newly proposed DON-LSTM aims to harness both multi-resolution data and temporal patterns, improving the predictive accuracy for long-time system evolutions. Results indicate that this architecture offers lower generalization errors than considered baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The problem is well motivated and the paper is nicely structured\n* This particular combination of LSTMs and DeepONets has not been done before\n* Code is submitted"
            },
            "weaknesses": {
                "value": "* The proposed model completely lacks novelty. It is simply a combination of DeepONets (which have already been around for several years) and the most prominent RNN architecture LSTM (which has been around for several decades). \n\n* The experimental results section is very weak. The considered baselines are not interesting nor meaningful. The paper should compare their results with other competing method that have been shown to perform well on these problems. In particular the paper should compare their results with FNOs (and their variants), and standard CNNs. \n    \n* The obtained results should be reported in relative errors (normalized by the scale of the problem). This is particularly important in engineering applications. Based on its current form one cannot check if the models obtain 1\\% error, 100\\% error, or even more.\n\n* No theory is provided.\n\n* A multi-resolution approach appears to be only applied during the training procedure, but is not tested during inference.\n\n* Instead of using LSTMs it would be interesting to use current state-of-the-art RNN architectures that are known to perform well on long-term dependencies."
            },
            "questions": {
                "value": "* How are the low-resolution data points obtained? Simply a down-scaling of high-resolution data? If so, what is the benefit of training with low-resolution data at all? Is it much faster? Please elaborate on that.\n\n* Why is the training procedure described in 2.4 chosen? Can you provide any ablations on alternative procedures (e.g., permutation of the roles and steps)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688545484,
        "cdate": 1698688545484,
        "tmdate": 1699637068113,
        "mdate": 1699637068113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M4aRL74dp9",
        "forum": "nZ7rpEp6wj",
        "replyto": "nZ7rpEp6wj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_qWGe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_qWGe"
        ],
        "content": {
            "summary": {
                "value": "The research introduces the multi-resolution DON-LSTM, a novel architecture designed to model time-dependent systems. By merging the strengths of DeepONet's discretization invariance and LSTM's memory-preserving mechanisms, the model leverages both high- and low-resolution training data for improved accuracy. Experimental results demonstrated that as training sample size increased, the generalization error decreased for all models. Notably, the multi-resolution DON-LSTM consistently outperformed benchmarks, achieving the lowest generalization error and requiring fewer high-resolution samples to match the accuracy of single-resolution methods. Key findings include the superior performance of models trained with early-stage low-resolution data and the pivotal role of LSTM mechanisms in enhancing model accuracy. The research also identified potential limitations, emphasizing the need for fixed location input data in DeepONets and suggesting possible solutions like encoder-decoder architectures. Conclusively, the DON-LSTM offers promising advancements in the realm of time-dependent system modeling, highlighting its potential in real-world applications and paving the way for future multi-resolution data studies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a new model, the multi-resolution DON-LSTM, that combines two powerful architectures, DeepONet and LSTM, tailored for time-dependent systems.\n\nAuthors propose a training mechanism to train the DON-LSTM on both low and high resolution data that leads to better performance.\n\nThe paper carries out thorough experimental evaluations against five benchmark models, assessing the proposed architecture. The authors have included the standard errors for the performance obtained.\n\nThe utilization of both high- and low-resolution training data in the model allows for enhanced learning, especially when high-resolution samples are limited. The paper offers multiple conclusions from its experiments, such as the superior performance of multi-resolution DON-LSTM over its benchmarks.\n\nAuthors have included the limitations and future work suggestion."
            },
            "weaknesses": {
                "value": "Based on the results, DON-LSTM trained on high resolution data only doesn\u2019t perform better compared to other benchmarks. What extra information does low resolution data provides to the model that leads to increased performance of the DON-LSTM trained on high- and low-resolution data. Also given that LSTM are used in the model, how long sequences can be trained with the model.\n\nThe authors can potentially include more baselines to compare their models with. For example, they can include ensemble of DON-LSTM trained on low- and high-resolution data separately or they can also include some other state of art methods used to solve the problem (if they exist) \n\nThe authors have described a training mechanism for the DON-LSTM. It would be interesting to analyze, how sensitive the model performance is with respect to training procedure described in the paper. For example, if DeepONet is trained first on high resolution data rather than low resolution data.\n\nAuthors have included the standard errors for the loss obtained. However, based on the standard errors, it's hard to conclude if the observed improvements are statistically significant. \n\nOverall, the paper is a smart combination of two different existing architectures to solve a problem and the paper is lacking the theoretical justification regarding the choice of architecture."
            },
            "questions": {
                "value": "I have listed my concerns and questions in weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801486394,
        "cdate": 1698801486394,
        "tmdate": 1699637067907,
        "mdate": 1699637067907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CAsQlCRq1l",
        "forum": "nZ7rpEp6wj",
        "replyto": "nZ7rpEp6wj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_LP9x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8539/Reviewer_LP9x"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a new architecture, DON-LSTM that combines the discretization invariance of deep operator networks and the ability of LSTMs to model dependencies in long sequences of multi-resolution data. The authors test their method on various models of non-linear systems, with multi-resolution data  and show improved generalization error, as well as needing fewer high-resolution samples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The core idea of the paper is simple and intuitive, combine the capabilities of DeepONets with LSTMs for more robust modelling of evolving systems. For the various PDEs considered, DON-LSTM performs better than using just naive LSTM or DeepONets."
            },
            "weaknesses": {
                "value": "The effect of the self-adaptive loss function, in particular the effects of step sizes $\\eta_\\lambda$ for the gradient ascent step (5) is not discussed. It would be nice to see how this choice effects the stability of the learned operator as well as the general gradient descent convergence behavior."
            },
            "questions": {
                "value": "While the aggregate errors have been provided, the stability of the learned operator over a long prediction sequence is not demonstrated. Is it possible to provide a figure similar to Figure 3, that shows the predicted values and training data for some initial-conditions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699059206875,
        "cdate": 1699059206875,
        "tmdate": 1699637067776,
        "mdate": 1699637067776,
        "license": "CC BY 4.0",
        "version": 2
    }
]