[
    {
        "id": "LvZaY3SfuP",
        "forum": "D2eOVqPX9g",
        "replyto": "D2eOVqPX9g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_pmTu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_pmTu"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the federated version of SARSA algorithm and analyses its convergent performance with the existence of heterogeneity in both transition dynamics and reward functions.\nDifferent from classical settings of federated reinforcement learning, the paper does not assume that agents have to share the same transition dynamics and reward functions.\nThe paper demonstrates that its proposed algorithm achieves a linear speedup for the convergence to the optimal answer in each local environment both theoretically and empirically."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper considers heterogeneity in both transition dynamics and reward functions. Moreover, it quantifies the degree of these heterogeneities and discusses their effect in the final convergence of FedSARSA.\n2. The paper discusses the convergence region and linear speedup of FedSARSA. It is claimed that smaller learning rates and a larger number of participating agents will help tighten the convergence region, which matches the intuition.\n3. The numerical experiment is carried out in settings with different degrees of heterogeneity."
            },
            "weaknesses": {
                "value": "1. What does MSE in the numerical experiments stand for? Does it mean the averaged MSE of current parameter to optimal parameters in different environments?\n2. The explanation of numerical experiments is not enough. For example, why the MSE of FedSARSA with a large number of $N$ ($N=40$) increase along the training process when $\\epsilon_p>0,\\epsilon_r>0$? And where is the confidence bound for the numerical experiments?\n3. The convergence MSE of FedSARSA with different number of agents are different from each other. What makes that difference?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8544/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8544/Reviewer_pmTu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669126950,
        "cdate": 1698669126950,
        "tmdate": 1699637068982,
        "mdate": 1699637068982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iVQFRSC94m",
        "forum": "D2eOVqPX9g",
        "replyto": "D2eOVqPX9g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_uJdU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_uJdU"
        ],
        "content": {
            "summary": {
                "value": "This work performs theoretical studies on federated reinforcement learning with clients facing heterogeneous environments. In particular, the classical SARSA algorithm is extended to the federated version. Theoretical analyses are established to demonstrate the finite-sample convergence of the proposed FedSARSA under linear function approximation. In particular, linear speedups are reported with the established results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work follows the interesting line of work of extending federated learning to the domain of decision-making under environmental heterogeneity. This setting is well-motivated and has wide practical implications.\n\n- The established results are solid and novel based on my reading. In particular, no similar results have been reported on FRL with heterogeneous clients in the planning task with both linear function approximation and linear speedup.\n\n- Despite the theoretical nature, the overall presentation is clear and the key intuitions are provided. The listed sketch of the proof especially facilitates the readability."
            },
            "weaknesses": {
                "value": "- I am overall satisfied with this work. There is just one minor question I have for the authors. As mentioned at the end of page 8, the obtained results from federated RL can be leveraged as initialization points for finetuning with just local data. I imagine the analyses would not be different given existing works, and wonder whether it would be possible to state the finetuning results, which may better highlight the impact of cooperation on accelerating individual learning."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765910910,
        "cdate": 1698765910910,
        "tmdate": 1699637068850,
        "mdate": 1699637068850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oVEQlnaJ6N",
        "forum": "D2eOVqPX9g",
        "replyto": "D2eOVqPX9g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_qewk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_qewk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies federated on-policy reinforcement learning with linear function approximation. It proposes FedSARSA that is a federated version of SARSA. It proves that FedSARSA converges to the neighborhood of the optimal parameter with a linear speed up."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ FedSARSA is intuitive and reasonable.\n+ The theoretical result that proves linear speedup is interesting."
            },
            "weaknesses": {
                "value": "- The theoretical results do not have the impact of periodic updating. \n- The authors do not specify the communication cost and how it trades off with the convergence.\n- The FedSARSA is a straightforward of single-agent SARSA -- the only difference is to aggregate the parameter estimation from all agents, which is a straightforward average.\n- The authors talked about how heterogeneity is captured in the convergence, but this relationship is not well articulated. In FL, one would first need to define the heterogeneity metric and then express the convergence bound as a function of this metric. Furthermore, such heterogeneity should be defined on the data, not the underlying distribution.\n- Paper writing needs some work. It is strange to not have a Conclusion section."
            },
            "questions": {
                "value": "- The motivation is unclear to me -- why do we want to learn a single universal policy? Each agent may interact with his/her own environment and learn a personalized policy for that environment. Isn't that better to be deployed on that environment than the single averaged policy across all environments?\n- I don't see the linear speedup in the simulation results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813037349,
        "cdate": 1698813037349,
        "tmdate": 1699637068735,
        "mdate": 1699637068735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4c2NRfAAHA",
        "forum": "D2eOVqPX9g",
        "replyto": "D2eOVqPX9g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_rA2b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8544/Reviewer_rA2b"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of on-policy federated RL with agents interacting with potentially different environments. A new algorithm FedSARSA is proposed, and shown to converge to a near-optima policy for all gents. Convergence speed analysis is also provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written. The formulation and ideas are explained clearly."
            },
            "weaknesses": {
                "value": "- It would be helpful if the authors could provide more intuition about where the speed up comes from. Specifically, what in the problem formulation/assumptions enable this speedup? Intuitively, this would be possible only when things are homogeneous (or close to that). \n- Is it possible to comment on the optimality of the finite-time error? Right now only upper bounds are provided."
            },
            "questions": {
                "value": "- It would be helpful if the authors could provide more intuition about where the speed up comes from. Specifically, what in the problem formulation/assumptions enable this speedup? Intuitively, this would be possible only when things are homogeneous (or close to that). \n- Is it possible to comment on the optimality of the finite-time error? Right now only upper bounds are provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699372038766,
        "cdate": 1699372038766,
        "tmdate": 1699637068613,
        "mdate": 1699637068613,
        "license": "CC BY 4.0",
        "version": 2
    }
]