[
    {
        "id": "MYQnPBL7cA",
        "forum": "CaP3CByuLp",
        "replyto": "CaP3CByuLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_bT9A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_bT9A"
        ],
        "content": {
            "summary": {
                "value": "This paper propose to enhance the low-resource language ability of  LLM with a multi-task tuning framework which combines the translation tasks and cross-lingual general task. Specifically two tuning method is introduced: cross-lingual instruction tuning and multilingual instruction tuning.  \nExperiment shows that the proposed x-LLaMA models achieves significant improvement on non-English languages,\nand the language alignment is improvement measured by the improvement in machine translation results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written and easy to understand. \nThe proposed CoIT and MuIT method is solid in improving the cross-lingual ability of pre-trained language models. \nAnd the formulated scaling laws to optimize the data allocation of data-constrained MuIT is novel to me.\nAnd I like the experiment design and analysis: 1. Extensive experiment on 6 languages and 3 kinds of dataset to show the improvement of cross-lingual ability.  2. Sophisticated experiment on machine translation task, with various evaluation metrics."
            },
            "weaknesses": {
                "value": "I have two concerns about this paper:\n1. There is not enough related work comparison: in Table 2, the proposed work is only compared to other LLaMA-based model tuned without multi-lingual dataset, so the improvement sees very significant, but I think it also should be compared with some other cross-lingual instruction tuning methods like \"Few-shot Learning with Multilingual Generative Language Models\", with the same multi-lingual training data to really reflect the merits of this work.\n2. The author argues that the CoIT could improve the semantic alignment.  I think only using translation performance to quantify the improvement in alignment is not enough, since the evaluation metrics like Comet and BLEU doesn't always correlate with better semantic quality. So maybe add the distance between the representations of positive/negative samples is worth trying."
            },
            "questions": {
                "value": "See weakness for detail:\n1. Is there a better related work comparison to justify this work?\n2. Could the improvement in semantic alignment could be better measure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666383389,
        "cdate": 1698666383389,
        "tmdate": 1699636921858,
        "mdate": 1699636921858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QebpS1Rd6m",
        "forum": "CaP3CByuLp",
        "replyto": "CaP3CByuLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_yxQQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_yxQQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a strategy to fine-tune a cross-lingual LLM (x-LLaMa) using cross-lingual instruction datasets and create multilingual LLMs (m-LLaMa) based on mixtures of training resources, including parallel corpora. The objective is to enhance the semantic alignment across multiple languages, thus improving the performance of a pre-trained LLM in non-English languages. Experimental results show that x-LLaMa and m-LLaMa achieve better performance on non-English tasks and translation tasks than previous LLaMa-based models. Ablation studies were conducted to demonstrate the effectiveness of the proposed strategy, including the use of different scales of data for tuning the models. Both the results and analyses reveal interesting findings for LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper systematically presents a study demonstrating the use of multi-lingual instruction datasets along with multi-lingual parallel datasets to fine-tune pre-trained LLMs for non-English languages, achieving comparable or even better performance than previous models.\n2.\tThe findings and conclusions drawn from this study provide valuable insights to the community, indicating that using translation data can enhance language abilities beyond English.\n3.\tOverall, the paper is well-organized and well-written, making it easy to follow."
            },
            "weaknesses": {
                "value": "1.\tOne of the contributions of the paper is the proposal of the scaling law to quantify the relationship between translation performance and the size of the training data. However, there is a lack of insightful discussions and analyses regarding this aspect. For instance, Eq. (2) considers the language similarity between English and the target language, but unfortunately, the computational details are not provided or explained, which makes it difficult to replicate the work.\n2.\tAnother concern is that the authors only verify their proposal using the LLaMa-7B model. It remains unclear if the conclusion still holds for other LLMs, such as Bloom."
            },
            "questions": {
                "value": "1.\tHow is the similarity of language computed, and how are the values of \u03b1 and \u03b2 estimated for the scaling law?\n2.\tIn Table 4, it is unclear how the optimal allocation of data is obtained. It would be helpful to provide further elaboration on this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NIL"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766446207,
        "cdate": 1698766446207,
        "tmdate": 1699636921736,
        "mdate": 1699636921736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hPNM0WoeP1",
        "forum": "CaP3CByuLp",
        "replyto": "CaP3CByuLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_55oU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_55oU"
        ],
        "content": {
            "summary": {
                "value": "The paper effectively achieves cross-lingual transfer by employing fine-tuning techniques on LLMs using a parallel corpus. The authors introduce two distinct forms of training data, specifically a parallel corpus and a translation corpus, which are employed to fine-tune the model. Subsequently, the model is evaluated on the XQuAD and MLQA datasets. The experimental results demonstrate the effectiveness of their approach in tackling these tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The collected parallel corpus in different languages is invaluable and can greatly contribute to future research.\n\n2. The performance of the tested tasks and languages has shown remarkable improvement."
            },
            "weaknesses": {
                "value": "1. The method employed lacks novelty, as it primarily involves the collection of a parallel corpus and subsequent fine-tuning of the model.\n\n2. The method lacks testing on high-resource languages closely related to English, such as French or German, as well as on low-resource languages like Thai or Swahili.\n\n3. The method has not been sufficiently tested on multilingual NLP tasks, including reasoning tasks such as MGSM and XCOPA, as well as NLG tasks like XLSum.\n\n4. The tested results on Flores are not convincing, especially when considering that the model has primarily been fine-tuned for translation tasks.\u00a0 Additionally, for the translation training data, please refer to the following question 3."
            },
            "questions": {
                "value": "1. Is the translator engine effectively aligned with human-annotated data in terms of quality? Furthermore, have any techniques been implemented to ensure the removal or filtration of irrelevant or meaningless data from the training process?\n\n2. In the CoIT example depicted in Figure 1, it is notable that the Chinese parallel corpus still includes English phrases such as 'Instruction:' and 'Input:'. It would be beneficial to analyze to assess the significance of these prompts and their impact on the overall performance.\u00a0\n\n3. Regarding the two types of training data, given the availability of a parallel corpus, it is worth investigating whether the translation corpus truly has a significant impact. Conducting an ablation analysis could provide valuable insights in this regard. Furthermore, expanding on this point would contribute to a more comprehensive understanding of the overall training process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699005644542,
        "cdate": 1699005644542,
        "tmdate": 1699636921623,
        "mdate": 1699636921623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1FFxnaOzxA",
        "forum": "CaP3CByuLp",
        "replyto": "CaP3CByuLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_Hw1M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7600/Reviewer_Hw1M"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a method for multilingual instruction tuning. The approach relies on jointly tuning parallel corpora and translated instruction-tuning data. The model is compared with other public multilingual instruction-tuned models. However, due to many lacunae in the experimental setup (described later) the paper cannot establish that the proposed approach is better than previously proposed approaches. The paper also introduces a multilingual benchmark (MI-EVAL) which is an automatically translated version of ALPACA (the automatic translation is a major limitation of this dataset)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper studies joint finetuning on parallel corpora and translated instruction data. Though this setting has been studied in previous work (Parrot), this paper makes an attempt to study various possibilities. \n* Ablation studies comparing monolingual pre-training, translation data instruction tuning, and multilingual task instruction tuning. These establish that tuning on translation data and translated instruction data is useful. However, the analysis still has some unanswered questions which are mentioned in the Weaknesses section."
            },
            "weaknesses": {
                "value": "* The proposed model is compared primarily with Bayling/Chinese Alpaca. Chinese Alpaca is trained only on Chinese. Hence, its results in Chinese are better than the proposed model, whereas it underperforms on other languages. On the other hand, Bayling is finetuned on 4 European languages, while this paper evaluates other languages. This evaluation setup is not a fair comparison to establish that the proposed approach is better than the previous work. This does not clearly answer any research question. Why not compare with truly multilingual models like Bactrian-X (or finetune alternative models to study various experimental configurations as described next).\n* To understand how/if the proposed approach is indeed better, the following additional ablations would help. Is finetuning on English IFT data (Aplaca-En) plus translation data (En-Zh) sufficient to achieve crosslingual transfer? Is cross-lingual instruction tuning necessary? \n* To establish the generalization of these results, the ablation results should be reported on all languages considered, not just Chinese. \n* The research questions the paper seeks to answer are not presented with clarity. How do the ablations answer all those questions?  In its current form, the paper proposed a model but does not clearly articulate how this model improves over current work. The experimental setup also doesn\u2019t lend itself to answering the research questions clearly as mentioned earlier. \n* With only limited exposure to foreign languages (via instruction tuning), how can the model achieve enough proficiency to generate fluent target languages? \n* It is known that a high-quality parallel corpus is needed for finetuning. It is also known that WikiMatrix is noisy. Given that there are many high-quality corpora available, why should they not be used for translation data finetuning? \n* The MI-Eval has been created via automatic translation. How good is the translation quality, and what is its impact on evaluation? Results on an error-prone translated dataset with no quality evaluation are not sufficient to measure model capabilities. Moreover, ChatGPT has been used for evaluation. How good is ChatGPT for the evaluation of open-ended tasks for non-English languages? There is no evidence of the efficacy of this evaluation methodology."
            },
            "questions": {
                "value": "* The paper mentions that the performance on English tasks is not impacted, and an example is provided. It would be good to report performance on English QA benchmarks before and after finetuning with cross-lingual data. \n* MI-EVAL: The English side looks like a replication of ALPACA. Why was ALPACA not directly used prior to translation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699172595708,
        "cdate": 1699172595708,
        "tmdate": 1699636921509,
        "mdate": 1699636921509,
        "license": "CC BY 4.0",
        "version": 2
    }
]