[
    {
        "id": "IspXF5H670",
        "forum": "7W4rbphLht",
        "replyto": "7W4rbphLht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel Newton-type algorithm called S5N, designed to tackle problems with potentially non-differentiable gradients and non-isolated solutions. This is particularly relevant for the sparse optimal transport problem. The S5N algorithm stands out from existing Newton-type methods due to its wide applicability, absence of hyperparameter tuning, and robust global and local convergence guarantees. Numerical experiments demonstrate that S5N outperforms in terms of convergence speed and computational efficiency when applied to sparse optimal transport problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Some of the strengths of the paper are \n* The paper is well written and the contributions are defined clearly \n* In this paper the authors measure the ratio between the actual and predicted reduction of function values similar to trust-region methods to decide whether to accept the next step or not\n* The proposed algorithm can handle non-smooth gradients and singular Hessian matrices\n* Does not rely on line search to guarantee global convergence \n* The stepsize $\\eta^k$ does not need to satisfy sufficient decrease or curvature conditions as line search-based methods. It only needs to be bounded to guarantee global convergence \n* The authors also show that under mild assumptions the proposed algorithm has local quadratic convergence\n\nThank you for providing the code!"
            },
            "weaknesses": {
                "value": "I want to know why the authors used only MNIST and Fashion-MNIST apart from Examples 1 and 2 to compare their algorithm experimentally. There are plenty of different datasets that the authors could explore and share the results of how their algorithm behaves compared to other algorithms. Moreover, why did they choose only image data to test their algorithm in addition to Examples 1 and 2?"
            },
            "questions": {
                "value": "Mentioned on weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_35vU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698510303173,
        "cdate": 1698510303173,
        "tmdate": 1699636523504,
        "mdate": 1699636523504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ULwdRs2pjk",
        "forum": "7W4rbphLht",
        "replyto": "7W4rbphLht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_h4ty"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a Newton method to minimize functions with non-smooth gradients, more specifically,  differentiable functions, not differentiable twice. The main application of such an algorithm is to solve the dual of the quadratically regularized optimal transport problem. The authors provide convergence rates for the proposed algorithm and optimal transport experiments."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The algorithm seems very tailored for the considered problem."
            },
            "weaknesses": {
                "value": "- 1 In equation 2, how is the linear system solved? Do you solve it exactly (with usual decomposition techniques) or approximately, using iterative algorithms? If you use iterative algorithms, which algorithms do you use? To which tolerance / number of iterations do you solve the linear system?\n\n- 2 The step size.\nFirst, in the main theorem 3, no clear assumption is given on the step size $\\eta_k$. I think the authors should write clearly under which assumptions the proposed algorithm converges.\n\nThen, I do not understand how the condition on the step size (loosly given in the main text) can be so loose: \"In fact, the only requirement is that $\\eta_k$ needs to be\nbounded\", how is it possible??\nIf I take a sequence of exponentially decreasing step size, how can the proposed algorithm converge? This is usually why one has to resort to costly line-search techniques.\nCould authors comment on this?\n\n- 3 Experiments. I think experiments should be reshaped: I do not know how significant/useful the experiments with run time below a second are: I guess that for these timings, implementation matters much more than the algorithms themselves: I would remove experiments with run time below 1 second.\nFor clarity, I would select around 5 algorithms, and display all the benchmark algorithms on a single graph, and multiple values of the regularization parameter $\\gamma$, like in Figure 1 of [1].\nThe block coordinate descent algorithm does not seem to converge, could you comment on this?\n\n[1] Lin, H., Mairal, J. and Harchaoui, Z., 2015. A universal catalyst for first-order optimization. Advances in neural information processing systems, 28."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691978857,
        "cdate": 1698691978857,
        "tmdate": 1699636523379,
        "mdate": 1699636523379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IIx9pUU2Ed",
        "forum": "7W4rbphLht",
        "replyto": "7W4rbphLht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
        ],
        "content": {
            "summary": {
                "value": "In this paper authors, motivated by optimal transport problems, propose Newton-type algorithm for objectives, that may have non-smooth gradients and singular Hessians. The proposed algorithm converges globally and has local quadratic convergence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes new Newton-type method, that can work with non-smooth gradients and singular Hessian matrices\n2. The experimental results show, that proposed method is superior over other existing approaches\n3. The results are mostly presented in a clear, understandable way."
            },
            "weaknesses": {
                "value": "1. Authors use grid-search to find the best step size on each step, which can be time-consuming in practice.\n2. Proposed algorithm converges globally, but it is not clear, what is the speed of global convergence.\n3. Lack of definitions of some important concepts. Maybe it is clear for those, who are very familiar with this topic, but I think, introduction of such definitions may improve the overall look of papers (see questions)."
            },
            "questions": {
                "value": "1. Probably, in the abstract you have a typo and you meant \"possibly non-smooth gradients\" instead of \"possibly non-differentiable gradients\"\n2. Please, increase the font size of text in the figures\n3. Please, provide the definitions of local Lipschitz continuity and upper semi-continuity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_7Z3B"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822497263,
        "cdate": 1698822497263,
        "tmdate": 1699636523291,
        "mdate": 1699636523291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "en0BVAMddy",
        "forum": "7W4rbphLht",
        "replyto": "7W4rbphLht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a general Newton-type algorithm named\nS5N, to solve problems that have possibly non-differentiable gradients and non-isolated solutions, a setting highly motivated by the sparse optimal transport problem. Compared with existing Newton-type approaches, the proposed S5N algorithm has broad applicability, does not require hyperparameter tuning, and possesses rigorous global and local convergence guarantees. Extensive numerical\nexperiments show that on sparse optimal transport problems, S5N gains superior performance on convergence speed and computational efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Compared with existing Newton-type approaches, the proposed S5N algorithm has broad applicability, does not require hyperparameter tuning, and possesses rigorous global and local convergence guarantees. Extensive numerical\nexperiments show that on sparse optimal transport problems, S5N gains superior performance on convergence speed and computational efficiency."
            },
            "weaknesses": {
                "value": "1. The results in Sec. 2.4 seems not new. They seem to be standard results of semi-smooth Newton. Thus, the author should cite some references.\n2. It seems that $f(x)$ in Eq. (4) is not a smooth function because $()_+$ operator is not smooth. Thus, Proposition 1 can not hold since $\\nabla f(\\alpha, \\beta)$ does not exist.\n3. The convergence analysis of this paper lies on Assumption 1 and Assumption 2 which requires that $f(x)$ is differentiable, so the convergence analysis can not be used in the problem that $f(x)$ is not smooth.\n4. This paper claims that S5N is used to solve problems that have possibly \\emph{non-differentiable} gradients and non-isolated solutions.\nHowever, the convergence analysis requires Assumption 1which supposes that $f(x)$ is differentiable."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_cPiV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699070087897,
        "cdate": 1699070087897,
        "tmdate": 1700224252464,
        "mdate": 1700224252464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P1xxiTIs7R",
        "forum": "7W4rbphLht",
        "replyto": "7W4rbphLht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors propose a new version of the gradient regularized Newton method with singular Hessian. It is applied to solve quadratically regularized optimal transport problems. It was proven that the method has asymptotical convergence of the gradient norm to zero. Additionally, the local quadratic convergence was proved under certain additional assumptions. The paper includes experiments with various data and setups."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem studied in the paper seems quite interesting, with real-life applications in optimal transport. The experimental results seem promising, and the code is implemented in an efficient, high-performance manner."
            },
            "weaknesses": {
                "value": "Unfortunately, I find it challenging to identify new and distinct results and contributions in the presented paper. Allow me to clarify my perspective. The paper consists of three main parts: 1) The S5N method for general problems with semi-smooth Hessian; 2) Application of S5N to quadratically regularized optimal transport; 3) Experiments. Next, I will discuss all of them in detail.\n\n1) I begin with the general problems and S5N; the presented results are very similar to those in papers [1],[2]. In both papers, gradient regularization is employed with some adaptive procedures. Across all three papers, the global rates are asymptotical, and the local rates are quadratic (in 2, it is even cubic). Moreover, all these rates are relatively weak compared to modern optimization methods with non-asymptotical global convergence rates. For example, first-order methods offer guaranteed non-asymptotical global rates with a much cheaper computational cost of every iteration. Thus, from a theoretical perspective, the first part does not seem to contribute significantly compared to the existing literature from my point of view. Please correct me if I overlooked any substantial improvement and theoretical challenge over the previous papers. \n\n2)  In the optimal transport (OT) section, the authors describe the OT problem and why it has the semi-smooth Hessian, as discussed in [3]. Therefore, the application of semi-smooth Newton to OT is also not novel (note that the authors do not claim it to be new).\n\n3) Numerical Experiments: I appreciate the inclusion of a large number of competitors' methods.  However, the presentation and comparison seem unfair to me. I believe that comparing the Iteration Number may be biased in the context of first-order and second-order methods. I would recommend using gradient computations as a more appropriate metric for comparison. Also, theoretical per-iteration computational complexities could help. I listed more questions in the next section to clarify the methods\u2019 performances. \n\nTo sum up, the presented contribution is not enough to be accepted for the ICLR, from my point of view. \n\n[1] Xiantao Xiao, Yongfeng Li, Zaiwen Wen, and Liwei Zhang. A regularized semi-smooth Newton method with projection steps for composite convex programs. Journal of Scientific Computing, 76:364\u2013389, 2018.\n\n[2] Weijun Zhou and Xinlong Chen. On the convergence of a modified regularized Newton method for convex optimization with singular solutions. Journal of Computational and Applied Mathematics, 239:179\u2013188, 2013.\n\n[3] Dirk A Lorenz, Paul Manns, and Christian Meyer. Quadratically regularized optimal transport. Applied Mathematics & Optimization, 83(3):1919\u20131949, 2021."
            },
            "questions": {
                "value": "1) It is quite counterintuitive for me: why do we want to use second-order methods for almost quadratic problems (quadratic + linear)? Why not use simple CG or PCG to solve it? \n\n2) Why in time plots the first-order methods are slower than S5N? \n\n3) Why do you not compare S5N with the classical baseline for OT, the Sinkhorn method?\n\n4) Why does the gradient time computation differ for different methods? \u201cthe semi-dual L-BFGS has a fast convergence in iteration number, it suffers from a long run time since its gradient computation is more difficult and time-consuming than other methods.\u201d\n\n5) Do you have any intuition: why \u201cASSN has a very slow convergence\u201d in practice? It seems to have the same structure with gradient regularization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n\\a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5242/Reviewer_tikC"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699556427849,
        "cdate": 1699556427849,
        "tmdate": 1699636523120,
        "mdate": 1699636523120,
        "license": "CC BY 4.0",
        "version": 2
    }
]