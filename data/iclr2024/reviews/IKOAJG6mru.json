[
    {
        "id": "ArpPFIeZnX",
        "forum": "IKOAJG6mru",
        "replyto": "IKOAJG6mru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the interesting question of enabling robots to use tools, taking into account constraints from both the robot and its environment. The authors propose the RoboTool system, which augments the coder module with additional analyzer, planner, and calculator modules. This paper presents a benchmark encompassing three tool-usage categories: tool selection, sequential tool usage, and tool manufacturing, evaluated across two types of robots. Through carefully designed experiments, the authors show that their system exhibits innovative tool use."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Leveraging LLMs to delve into robot tool usage is a compelling approach. The inherent common sense knowledge within LLMs may offer invaluable insights to the robot's tool utilization process.\n* The RoboTool System builds on the prior concept of LLM code generation (framed as code-as-policies) and merges it with new analyzer, calculator, and planner modules. This integration aids in breaking down the task, enabling the LLM to more effectively suggest beneficial solutions. The ablation studies provide evidence of the effectiveness of these newly introduced modules.\n* The categorization in the benchmark is well-designed as a starting point to explore the robot tool usage with LLMs."
            },
            "weaknesses": {
                "value": "* Although this paper focus on high-level planning using tools, the provided descriptions are too detailed on the targetted tools, which makes it hard to see if the hints make LLM propose the solutions. For example, for the Milk-Reaching example, the hammer is provided with detailed instructions on how to grasp, the descriptions on its layout, while other objects are not described that detailedly. Such bias can make the results unfair. And in all 6 experiments, the number of objects in the descriptions are limited, it may be not hard for the LLM to pick a related object.\n* The benchmark only contains 6 demos, with limited diversity on the layout of the objects. For example, for the milk-reaching demo, the hammer is always in the correct direction. With similar description, actually the hammer can be in multiple potential directions, which will definitely influence the planning the success rate of the task. Such challenging examples are not considered in the constructed benchmarks. And with the natural language description, it cannot avoid the limitation to describe the 3D world. Without access to the full information, it\u2019s hard to imagine the performance of the system on complicated tasks. Need to show more results on the robustness for the system on various examples.\n* The descriptions are sometimes confusing. For example, in the Cube-Lifting, the cube weight is 10kg, and the robot weight is also 10kg, then in the video, why the robot will fall down so quickly when it goes to another side. It\u2019s a bit confusing if the description reflects the real property and why not use the real physical attributes.\n* For some constraints mentioned in the description, it\u2019s unclear why the constraints make sense. For example, in the Cube-Lifting example, in the constraints, \u201cyou can push the chair only in the x-direction\u201d, it makes readers confusing if the input description is well-tuned for the specific example and how such things make the system generalizable across different tasks."
            },
            "questions": {
                "value": "* Regarding the benchmark, are there additional results showcasing varying object layouts for each demonstration while maintaining a consistent description format?\n* In the demos, how does the system respond when constraints in the input description are removed? How sensitive is it to changes in the description?\n* Why not for all objects, give the same set of attributes no matter if the attributes are useful enough? In this way, it can better show if the system is able to really extract the useful information from the descriptions without hints.\n* Although this work try demonstrating the tool usage ability in the high-level setting, it\u2019s hard to ignore the influence from different details. How to make sure the description describe the scene without heavy human designing?\n* How to make sure the given grasping point and other attributes or additional description for one object are not hints to the tool usage demo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_xccj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698282604493,
        "cdate": 1698282604493,
        "tmdate": 1699637128300,
        "mdate": 1699637128300,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fVP2mzdFJD",
        "forum": "IKOAJG6mru",
        "replyto": "IKOAJG6mru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg"
        ],
        "content": {
            "summary": {
                "value": "This paper presents RoboTool, a method for enabling tool use in robots using large language models (LLMs). Besides this prompt-based task and motion planning framework, the paper also proposes a benchmark of 6 tool use tasks evaluating tool selection, sequential tool use, and tool manufacturing capabilities. Tasks involve a robotic arm and a quadrupedal robot. Experiments in simulation and the real world demonstrate that RoboTool can successfully accomplish the tool use tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Leveraging the recent wealth of LLM research for improving robotics is a highly desirable research direction that is well-explored in this paper."
            },
            "weaknesses": {
                "value": "1. Experiments are weak: in particular, the authors propose a new benchmark, but only evaluate their method on it. To ascertain the value of the benchmark suite, additional baselines need to be included. To assess the strength of contributions of this \"learning-free\" approach, it should be run on existing, standardized benchmarks, such as those included in [3] or [6].\n\n2. Lack of novelty: works such as [1], [2], [3], [4] and [5] have taken similar approaches to neuro-symbolic learning and robotic manipulation, via LLM-generated programs or TAMP structures. Moreover, it's not particularly satisfying to me that the entire method interacts only with GPT-4 at the API level. The paper in effect becomes a \"prompt engineering\" work, which, while interesting, does not meet the bar for original technical contribution at ICLR.\n\n[1] [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753)\n\n[2] [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128)\n\n[3] [Programmatically Grounded, Compositionally Generalizable Robotic Manipulation](https://arxiv.org/abs/2304.13826)\n\n[4] [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559)\n\n[5] [Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model](https://arxiv.org/abs/2305.11176)\n\n[6] [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)"
            },
            "questions": {
                "value": "1. The ablations provided are interesting and welcome, but could the authors include some more well-established baselines such as [1] or [2] in this evaluation?\n\n2. Can the authors address why an API-only algorithm is sufficiently novel? In particular, I don't see where there is any learning of representations, which nominally is what ICLR is focused on.\n\n[1] [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753)\n\n[2] [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_b6vg",
                    "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712197182,
        "cdate": 1698712197182,
        "tmdate": 1700770028021,
        "mdate": 1700770028021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W9zeCk3cfE",
        "forum": "IKOAJG6mru",
        "replyto": "IKOAJG6mru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_QoGZ"
        ],
        "content": {
            "summary": {
                "value": "This works uses LLMs to generate code that is able to perform some reasoning and planning with a robotic simulated system. It is tested on three different experimental paradigms with two robots. The results provided are impressive. The only drawback of the work is the confusion on what is really planning and control with a tool in the real world and coding a set of skills in a programming environment. Furthermore, the baseline comparison is an ablation study."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tOriginal solution for planning with reasoning using LLMs.\n-\tIt is able to generate code with a level of reasoning that outperforms previous works.\n-\tResults are well described and deep."
            },
            "weaknesses": {
                "value": "-\tWhile the aim proposed by the authors is \u201cwe aim to solve a hybrid discrete-continuous planning problem\u201d, this is not solved in this work or at least not described properly.\n-\tThe focus of the paper should be improved. This is a of  language reasoner that generates code. So it is more a programming tool than a RoboTool.\n-\tBaseline comparison is an ablation study. Thus, the third contribution is not well described.\n\n\n**Focus**\n\nThe clarity of what is achieved should be more clear. The first contribution: \u201clong-horizon hybrid discrete-continuous planning\u201d is not solving the hybrid part. It is using predefined skills. Note that as the authors show just planning is not enough in a real set-up as the world has errors and skills have to be hardcoded. Furthermore, as it is well described in the Limitation this is a very powerful planner that generates code, but the continuous control of the execution is not addressed in this work. In essence this is a very sophisticated planner but it is not solving hierarchical control.\n\n**State of the art**\n\nFor completeness I am missing this LLM approach to Robotics: PaLM-E: An Embodied Multimodal Language Model\n\nAnd also recent works on planning with low-level control such as: Active inference and behavior trees for reactive action planning and execution in robotics. TRO2023\n\n**Results**\n\nFor a fair baseline comparison authors can use a PDDL planner as baseline. It may be misleading to call a baseline comparison an ablation study of the own algorithm.\n\nIt is not clear the type of randomization in the environment initialization to properly evaluate the accuracy of the planner."
            },
            "questions": {
                "value": "**Further comments:**\n\n-There is no mention on what type of LLM is being used and how it is pretrained and refined for each component. I think this is important information.\n\n-\u201cHierarchical Policies for Robot Tool Use\u201d there is no analysis of the combinatorial nature of the parametrized skills. We are talking about 4 skills with how many parameters? How many instances of objects? This is important to understand the level of complexity of the decision tree.\n\n-Do we need 4 LLMs to solve simple reasoning and generate a plan?\n\n*A high-level comment*\n\nProblem solving is the key point in this work. The citation to Josep Call is crucial. While it is shaped as a tool use the fact is that the robot does not understand a tool as a tool but a set of skills that can manipulate the world. Two things that are usually missing in this type of approaches are:\n\n-\tHumans we have mechanical/dynamics knowledge learnt from experience. Although the authors mention the affordances, note that it is not only about semantics but also about the real interaction in the environment.\n\n-\tThere is no analysis of how the system handles uncertainty resolution and trades off exploitation vs exploration (or intrinsic motivation). This is a important concept in creativity. Only reasoning is not enough to induce creativity, but probably reasoning and uncertainty resolution to try new things could be artificial creativity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760121301,
        "cdate": 1698760121301,
        "tmdate": 1699637128058,
        "mdate": 1699637128058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qZU4E9A3J6",
        "forum": "IKOAJG6mru",
        "replyto": "IKOAJG6mru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a prompting approach to enable creative tool use for robots. The approach constsists of four stages, using an \"analyzer\" prompt to extra objects, a planner planner prompt to generate a rough plan, a \"calculator\" prompt to populate it with action parameters, and finally a \"coder\" prompt to generate executable python code. The approach is demonstrated on a simulated and real robotics example."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Getting robots to solving complex tasks is an important and difficult problem\n- Many are interested in LLMs at the moment, and this paper provides some further information on how to use them\n- The results seem impressive, and it has an ablation study showcasing that each module seems to be required for success on these examples"
            },
            "weaknesses": {
                "value": "- A lot of engineering seems to have gone into these examples. The prompts (on separate github) contain a number of hints on what not to do when solving the problem, which seem engineered for the particular tasks. Examples:\n  - \"If you do not know the actual value, use an offset = 1m.\"\n  - \"You must be careful when calculating with negative values.\"\n  - \"You must understand that the distance between the two objects' center and the distance between the two objects' edges along an axis are different.\"\n  - The coder in Fig.2 also generates a seemingly arbitrary gripping offset for the hammer which I guess you engineered depending on the shape of the hammer.\n  - Some motion primitives are a bit contrived: As a roboticist, getting the robot to kick the surfboard in place to traverse the sofa seems like an extremely challenging tasks that I guess you just spent a lot of time engineering the motion primitives for. I don't think these are very realistic examples of your approach considering how much engineering must have gone into them. It is very difficult to say how much going on here is just simple symbolic task planning vs. motion planning (e.g. the real-valued positions and orientation parameters).  \n- It relies only on ChatGPT 4.0 which means that it is unclear to me if this architecture design and ablation study would generalize to other LLMs. GPT4.0 is much better than open source models so the decision is understandable but it is a weakness of the paper. It also makes reproducing it harder since ChatGPT is updated and has been observed to change behavior over time (OTOH it is very easy to use the current version of GPT4...).\n- Relation to other LLM works that do manipulation could maybe be clarified, e.g. VoxPose was kind of brushed off as being multi-modal, but isn't that a strength? IIRC they also show somewhat complex actions (grabbing a toast and puttig it on a cutting board). Your example is more complex but it is difficult to quantify since they seem heavily engineered.\n\nMinor issues with presentation/claims:\n- Why are the prompts in the appendix only links to github instead of actually in the appendix? IIRC there is no page limit on the ICLR appendix.\n- The language could be better, especially the examples in the intro do not actually seem to be grammatically correct calls for action, e.g., \"grasping a milk cartoon\" should be \"grasp a milk cartoon\", \"walking to the sofa\" should be \"walk to the sofa\" and so on. This is a bit problematic if a prompt is unintentionally ambigious as it could affect your results."
            },
            "questions": {
                "value": "- How much do you vary/randomize in your sim and real robot examples, does the robot and world always start in the same configuration?\n- Can you also say something about how much the action parameters are varied in response to these, or maybe show us some example python code from different runs of the algorithm on e.g. the milk example?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8963/Reviewer_pWWx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798001504,
        "cdate": 1698798001504,
        "tmdate": 1700504523441,
        "mdate": 1700504523441,
        "license": "CC BY 4.0",
        "version": 2
    }
]