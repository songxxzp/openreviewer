[
    {
        "id": "2SRETC9pCm",
        "forum": "ohdVLirfbz",
        "replyto": "ohdVLirfbz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalization of training overparameterized neural networks by gradient descent. This work follows the line of work of studying the generalization error via on-average argument stability. This work extends previous work by providing a relaxed condition for two-layer NNs and establishing an optimization and generalization result for 3-layer NNs. This work further shows that as the network scaling factor $c$ (in the $1/m^c$ scaling in front of NNs) increases, less overparameterization is needed in order to achieve the same generalization error."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is technical and the results are novel in my opinion. The proofs are solid and nicely structured in the Appendix. On the technical side, I appreciate the authors are able to establish almost co-coercivity results for 3-layler NN and the authors are able to use this to establish the uniform stability bound on $W_t - W_t^{(i)}$. On the other hand, I also like the results on the relationship between the network width and the scaling parameter $c$ and that the authors are able to show that larger $c$ requires less over-parameterization."
            },
            "weaknesses": {
                "value": "One of the drawback of this analysis is that since the network width $m$ is usually chosen first in practice, this result (such as theorem 2,3) basically says the training can't be too long and $\\eta T$ need to be upper bounded by some functions over the network width. Also, this work prove convergence via lower bounding the minimum eigenvalue of the Hessian and uses the Hessian at the initialization as a reference point. This limits how far the network weights can travel from the initialization value. For $L^2$ loss, I think this is OK but for losses like cross-entropy where the network achieve zero-loss at infinity, it is not clear how far this analysis can be generalized."
            },
            "questions": {
                "value": "1. How are the network weights initialized? It seems the only requirement on initialization is assumption 2. \n2. When the authors are talking about network complexity, how is it defined? Especially, the paragraph before section 2 \"...the larger the scaling parameter or the simpler the network complexity is...\". It seems from assumption 3 that the network complexity is defined as the $L^2$ norm of the weight matrices. Also, why the larger the scaling parameter corresponds to the simpler the network complexity?\n3. On page 2, the second bullet of contributions, the authors mentioned that \"... due to the empirical risks' monotonically decreasing nature which no longer remains valid for three-layer NNs\" Can't taking a smaller step in gradient descent steps help the empirical risks' monotonicity of decreasing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8613/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8613/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607197203,
        "cdate": 1698607197203,
        "tmdate": 1699637077721,
        "mdate": 1699637077721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BcjvZVbXdS",
        "forum": "ohdVLirfbz",
        "replyto": "ohdVLirfbz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_khQG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_khQG"
        ],
        "content": {
            "summary": {
                "value": "This paper gives guarantees to the excess risk of two-layer and three-layer neural networks. In particular, this paper compares the generalization error of the gradient descent algorithm for minimizing the empirical risk of the network and provides bounds on the generalization error.\n\nThe setup assumes that the activation functions are twice-differentiable, have bounded derivatives, and the inputs all lie within a bounded range.\n\nThe main result requires the network width to grow in a polynomial fashion with the number of epochs, the step size, and a certain scaling parameter used in the empirical loss.\n\nAdditionally, it is assumed that the population risk minimizer satisfies a certain norm-bounded assumption under $\\mu$.\n\nGiven these conditions, the paper then proves that the excess risk will converge by a rate as something like $O(T / n)$, where $T$ is the total number of iterations. [However, there is an additional dependence between $T$ and $m$ (the width of the network).\n\nAt a high-level, the proof proceeds by arguing that the gradient descent satisfies an \"on-average\" stability condition, and this condition implies that the loss is strongly smooth and weakly convex. This condition allows one to prove that the gradient descent optimizer will converge to the global minimum.\n\nThe scaling of $m^{-c}$ on the empirical loss is then used in a manner similar to the NTK analysis to get the required smoothness condition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A nontrivial result that expands on the literature of generalization guarantees for deep neural networks.\n\n- The paper is generally written with sufficient clarity for readers to follow.\n\n- A nice diagram to visualize the dependencies in the problem constants."
            },
            "weaknesses": {
                "value": "- The proof thus far only works for three layers. [Although it is believable that with more work, one may be able to extend the analysis for multiple layers, e.g., by arguing a similar weak convexity condition on the loss function.]\n\n- It is by now widely accepted that the NTK analysis is more of an analogy rather than a realistic characterization of what happens in reality. Thus, the practical relevance of this paper, which follows similar lines of arguments as the NTK (e.g., by rescaling the loss and expanding the network width), is very limited.\n\n- The abstract (and the paper's title) mentions multi-layer neural networks, but the analysis currently holds for three layers. This is a mismatch."
            },
            "questions": {
                "value": "- How much more difficulty would it take to extend the analysis to multiple layers?\n\n- Do you see a way to argue that the dependence on width (in equation (4)) is tight/necessary?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813598184,
        "cdate": 1698813598184,
        "tmdate": 1699637077620,
        "mdate": 1699637077620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4J7oSZc9Ev",
        "forum": "ohdVLirfbz",
        "replyto": "ohdVLirfbz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalization performance of multi-layer neural networks using stability-based techniques. \nCompared to previous works, this paper extends previous results to multi-layer neural networks. \nThe primary focus revolves around the width requirements, specifically the scaling parameter (denoted as \"m^-c,\" where \"m\" represents network width and \"c\" is closely associated with NTK and mean-field concepts), and the model capacity (quantified by the norm of W*, denoted as \"\u03bc\"). The overarching conclusion of this study is that wider width requirements are necessary when \"c\" and \"\u03bc\" are large. Additionally, the authors observe that the width requirements are less stringent for three-layer neural networks. In summary, I recommend a modest acceptance of this paper due to its foundational concepts and valuable technical contributions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper offers a comprehensive perspective by encompassing various existing works. Notably, it addresses the applicability of the scaling factor, including scenarios such as NTK and mean field. While the primary focus is on three-layer neural networks, the authors consider the potential for direct application to networks with multiple layers.\n\n2. A substantial technical contribution of this paper is the derivation of an upper bound for the W-norm, as presented in Lemma B.2. The writing is clear, and the proofs appear accurate, inspiring confidence in the results. However, a comprehensive validation of all details is still pending."
            },
            "weaknesses": {
                "value": "1. It seems that the authors primarily concentrate on three-layer neural networks. It would be beneficial if the authors explicitly discuss the potential applicability of their findings to networks with multiple layers.\n\n2. The uniformity of scaling factors across layers may appear somewhat unconventional. It would be helpful if the authors provided further insight into this choice and its implications.\n\n3. It would be beneficial if the authors could offer more intuitive explanations regarding how the upper bound on the W-norm improves from a t-scale to a sqrt{t}-scale, shedding light on the underlying mechanisms that drive this improvement."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826122854,
        "cdate": 1698826122854,
        "tmdate": 1699637077493,
        "mdate": 1699637077493,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1qnpvJvSfw",
        "forum": "ohdVLirfbz",
        "replyto": "ohdVLirfbz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_BPeY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8613/Reviewer_BPeY"
        ],
        "content": {
            "summary": {
                "value": "The paper provides generalization guarantees for the training of neural networks with one and two hidden layers (i.e., with two and three layers) under gradient descent. The paper shows what combinations of the scaling parameter and the so-called complexity parameter, achieves the generalization error bound $O(1/\\sqrt{n})$, where $n$ is the number of samples, for both over- and under-parameterized regimes. Moreover, the paper shows that when an adequate minimizer (weight values for a network) that zeroes the population risk exists, their generalization error improves to $O(1/n)$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-> The paper analyzes different scaling on the network\u2019s width by varying its exponent $c$, defining regimes that haven\u2019t been analyzed in previous works.\n\n-> The paper does a full formal analysis for both two and three layered networks, and show the difficulties on such analyses and their differences, particularly, when it comes to the (almost) co-coercivity property.\n\n-> Discussions are provided after the theoretical results and comparison to existing literature which aids the reader\u2019s understanding of their nature and contribution."
            },
            "weaknesses": {
                "value": "There are assumptions, concepts, and results that need to be better explained and for which I also raise concerns related to their understanding and implications.\n\n\n-> In Assumption 2, one of the conditions seem to depend on a specific initialization of the weights $W_0$, where that the loss should be less than $c_0$ when evaluated under the initialization. However, nowhere in the paper mentions (at least to that point) how the networks are initialized or if we are assuming a fixed initialization \u2013 I am not sure if the bound holds for $c_0$ uniformly over any initialization of the weights or not. Please, clarify.\n\n\n-> In Theorem 3, it says that equation (4) should hold, but then show another bound on $m$ in equation (5). How should we interpret this? Should we just take the maximum of both lower bounds on $m$?\n\n\n-> Regarding Theorem 4: I would imagine that it would be a good thing for the excess population risk to minimize as the width of the network increases, i.e., as there is over-parameterization. For this, $c>1/2$ is desirable. Then, why would someone choose $c=1/2$ from the perspective of the excess population risk or from the other terms that bound the generalization bound? Why did previous works considered $c=1/2$ to be a good thing?\n\nSo, it seems that having $c>1/2$ benefits decreasing the bound, or at least the excess population risk part of it, as $m$ increases. Therefore, I would expect in Corollary 4 some differentiation in terms of $c=1/2$ and $c>1/2$, but I don\u2019t see such difference (the only difference is made over the value $c=3/4$).\n\n\n-> Following the previous point, it is interesting that the excess population risk for a three-layer neural network in Theorem 8 (excess population risk) does not have a term $m$, and so no dependency on $c$ in the upper bound, unlike its counterpart Theorem 4. Moreover, Theorem 8 does not hold for $c=1/2$ according to its statement. Why is this? How is the derivation here different from the two-layer network? \n\nMoreover, I would expect a $c$ factor to appear in the upper bound because the scaling that depends on $c$ is on the output layer in both two or three layers. Is there any explanation for this? I haven\u2019t checked the math.\n\n\n-> I am trying to understand Assumption 3 and see if it makes sense. Right after equation (2) it is mentioned that $W^*$ has minimum norm. Is Assumption 3 just assuming that such minimum norm must satisfy an upper bound? \n\n\n-> Also, Assumption 3 introduces the parameter $\\mu$ almost arbitrarily \u2013 there is no mention whether the value of $\\mu$ comes from the architecture of the network of the loss function, it seems like a magical parameter! Moreover, it is afterwards that the paper calls $\\mu$ the network complexity, but I think this is a void term. For example, the value of $\\mu$ determines whether the minimizer with minimum norm will have its norm decreasing as the width $m$ increases or not. How is this possible? What is the motivation for this? Whether the norm increases or not depending on the width, since we simply have a two-layer network, should depend on the architecture and the loss itself, not on some external and extraneous parameter. This requires explanation.\n\n\n-> So the terms \u201cover-parameterized\u201d and \u201cunder-parameterized\u201d are used extensively in the paper without a proper definition. It may sound obvious that in the former you have more parameters than data samples, while in the latter the opposite; however, throughout the paper there are different conditions on the width $m$ that makes the distinction to both regimes. Can the exact definitions be defined? For example, under the conditions on $m$ in Corollary 5 and 9, how do I know in which regime I am? Could more explanation on how to define both regimes be given for Figure 1?\n\n\n==\n\n-> Scaling parameter is mentioned in the contributions without any explanation of what this exactly mean. I don\u2019t think it is a standard term: it could mean scaling of the initialization variance, the scaling of the pre-activation, etc.\n\n-> Also the term \u201cnetwork complexity\u201d is mentioned in the introduction without much explanation. Such term seems to be independent from whether the network is over- or under- parameterized (which is kind of strange, since I remember that \u201ccomplexity of the model\u201d usually refers to how many parameters or degrees of freedom it has).\n\n==\n\n-> Though the work (Taheri & Thrampoulidis (2023)) use algorithmic stability, it seems they make strong assumptions on the distribution of the data, which should be mentioned in the paper. \n\n-> The norm symbol for Euclidean norm is defined, but not for the spectral norm (when the argument is a matrix).\n\n-> Theorem 6 and Remark 4 mention the \u201ccondition (B.9)\u201d, however, such condition seems to not be on the main paper, making the paper not self-contained.\n\n-> Besides the introduction, the property of almost co-coercivity is not mentioned until section 3.2. It would be a good idea to introduce its definition from the beginning."
            },
            "questions": {
                "value": "Please, see the \"Weaknesses\" section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856497511,
        "cdate": 1698856497511,
        "tmdate": 1699637077373,
        "mdate": 1699637077373,
        "license": "CC BY 4.0",
        "version": 2
    }
]