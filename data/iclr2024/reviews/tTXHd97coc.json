[
    {
        "id": "ZONYVjwOfg",
        "forum": "tTXHd97coc",
        "replyto": "tTXHd97coc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission600/Reviewer_prkd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission600/Reviewer_prkd"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the domain of adversarial training for imbalanced dataset. For this purpose they propose the REAT framework. REAT has two main contributions. First, it tackles the imbalance of AE generation by employing a re-weighting scheme using the class effective number. This helps by increasing the number adversarial examples belonging to the tail classes. Next, they introduce a regularization term called TAIL to re-balance the feature distribution across classes. Finally, the authors demonstrate the efficacy of REAT through multiple experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The domain this paper is tackling is an important one. The motivation of the two main contributions of this paper (re-weighting and TAIL) are clear. In particular, the inclusion of effective number to handle AE generation is unique and well formulated.\n2. The empirical studies (in particular the ablation studies) display the efficacy of proposed method very well."
            },
            "weaknesses": {
                "value": "1. The authors provide experimental results that show the efficacy of the proposed method. However, the improvements are not consistent across the board. For example, the clean accuracy of REAT always seems to be lower than the baselines at UR = 10. Similarly the adversarial accuracy of REAT is not always better and in many cases the improvements are marginal. The experiments are also limited to the CIFAR-10-LT and CIFAR-100-LT dataset. Experimenting with further datasets would have been helpful. \n2. The proposed REAT method is intuitive to understand but it would have been better if there were some theoretical justification behind performance improvement. For example, the authors assume that predicted label distribution will not change much between two successive epochs but no reasoning is provided behind this assumption. \n\n3. There are multiple typos and grammatical mistakes in the manuscript."
            },
            "questions": {
                "value": "1. I want to know the authors thoughts about why REAT's clean accuracy is usually lower at UR = 10\n2. The authors stated that REAT can handle \"body\" classes better. I might have missed this but I didn't see a clear justification regarding why REAT would do this better than RoBal"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698121819062,
        "cdate": 1698121819062,
        "tmdate": 1699635987762,
        "mdate": 1699635987762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ANGn6Uuawz",
        "forum": "tTXHd97coc",
        "replyto": "tTXHd97coc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission600/Reviewer_1nCF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission600/Reviewer_1nCF"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on adversarial training with imbalanced datasets. It addresses two main motivations: (1) The prediction distribution of adversarial example generation during adversarial training is imbalanced; and (2) the feature space of the tail classes is smaller than that of the head classes. The paper proposes corresponding solutions to improve adversarial training in scenarios with data imbalance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper focuses on an important but somewhat overlooked problem in adversarial training (AT): the imbalanced dataset setting.\n2. This paper is clearly written and well-organized. The first two sections detail the introduction of related work and the motivation of the proposed method.\n3. The motivations of the proposed method, observations on the prediction distribution of adversarial examples generation during AT, and feature embeddings are clear and supported by empirical validation and theoretical analysis.\n4. The experimental settings are clear and diverse."
            },
            "weaknesses": {
                "value": "1. Adversarial training is known to result in a robust fairness issue. This means that even in a balanced dataset, the robustness of different classes varies significantly, which can be seen as an inherent data imbalance effect. Therefore, it would be beneficial to compare the proposed methods [2, 3] that address this issue (or at least mention them in the related work).\n2. The improvement of the proposed method appears to be limited compared to RoBal [1]. While this is not a major concern, it would be interesting to explore if combining RoBal and REAT can further enhance robustness.\n3. The experiments only focus on CIFAR-10 and CIFAR-100 datasets. It is recommended to include larger datasets like TinyImagenet to further validate the effectiveness of REAT.\n\n[1] Adversarial Robustness Under Long-Tailed Distribution. CVPR\n\n[2] To be robust or to be fair: Towards fairness in adversarial training. ICML\n\n[3] CFA: Class-wise calibrated fair adversarial training. CVPR"
            },
            "questions": {
                "value": "1. Can REAT also mitigate the robust fairness issue in adversarial training on **balanced** datasets?\n2. Can REAT be combined with Robal to further mitigate the effect of data imbalance on adversarial training?\n3. Please provide more details on the application of Re-Balancing Loss (*RBL*) in equations (1) and (2). How is this loss function used to generate adversarial examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Reviewer_1nCF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638173364,
        "cdate": 1698638173364,
        "tmdate": 1699635987696,
        "mdate": 1699635987696,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CCvHv6f3OX",
        "forum": "tTXHd97coc",
        "replyto": "tTXHd97coc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission600/Reviewer_yNqS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission600/Reviewer_yNqS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the adversarial training method under imbalanced dataset. In order to improve the model performance, the paper proposes two strategies: (1) reweighting and (2)  tail feature alignment"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and the proposed methods seems reasonable."
            },
            "weaknesses": {
                "value": "I have the following concerns:\n1. The performance improvement is very marginal compared to the original AT. \n2. There is no evidence provided to show why the tail feature alignment is necessary. For example, whether this will cause performance compromise is not clear. \n3. Similar studies are investigated previously, i.e., the paper [1]. \n\n[1] Imbalanced adversarial training with reweighting, Wang et al, 2022."
            },
            "questions": {
                "value": "Plz see the question above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696581555,
        "cdate": 1698696581555,
        "tmdate": 1699635987623,
        "mdate": 1699635987623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RfuWtuhUcp",
        "forum": "tTXHd97coc",
        "replyto": "tTXHd97coc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission600/Reviewer_h8eU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission600/Reviewer_h8eU"
        ],
        "content": {
            "summary": {
                "value": "The authors study the long-tailed distribution problem in the adversarial training scenario and show the drawbacks of unbalanced AE and feature space. Based on this insight, the authors propose a two-stage framework to improve the performance with unbalanced datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors show that the head class dominates the AE label and feature embedding space, leading to the underfitting of the tail class.\n\nThe author proposed RBL and TAIL corresponding to the maximization and minimization processes respectively to jointly solve this problem.\n\nThis paper is well-written and easy to understand, and the experimental results are extensive."
            },
            "weaknesses": {
                "value": "Fig. 3 shows the effectiveness of RBL is not signifient, could introducing a hyperparameter further accentuate the weight disparities to enhance robustness?\n\nThe TAIL only improve the TC\u2019s feature space, could TAIL be tuned to align the feature space for all classes, e.g. class-dependent maximizing the KL divergence between each class and the first head class?\n\nThe performance improvement of the REAT framework is relatively weak, using 25\\% additional computing overhead to improve performance by 1\\% compared to the baseline."
            },
            "questions": {
                "value": "Shown in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission600/Reviewer_h8eU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699085769597,
        "cdate": 1699085769597,
        "tmdate": 1699635987512,
        "mdate": 1699635987512,
        "license": "CC BY 4.0",
        "version": 2
    }
]