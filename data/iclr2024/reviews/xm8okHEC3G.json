[
    {
        "id": "10ukxUeR40",
        "forum": "xm8okHEC3G",
        "replyto": "xm8okHEC3G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
        ],
        "content": {
            "summary": {
                "value": "The authors delve into the task of dataset distillation from the perspective of sample cruciality, they argue that hard samples in the original dataset contain more information. To this end, they discard some easier samples and enrich harder ones in the semantic space through continuously interpolating between two target feature vectors during data distillation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors delve into the task of dataset distillation from the perspective of sample cruciality and propose the idea of adjusting the proportion of difficult and easy samples in the data distillation process; few papers considered this aspect before.\nThey put forward an infinite semantic augmentation method by continuously interpolating between two target feature vectors, requiring no extra computational costs while being effective.\nThe applicability of distilled data is considered, They demonstrated that their distilled data is capable of providing benefits to continual learning and membership inference defense."
            },
            "weaknesses": {
                "value": "The author only demonstrated through some simple experiments that in data distillation, the importance of difficult samples is stronger than that of simple samples. However, this conclusion cannot adequately explain that, in Figure 5 of the appendix, it can be observed that discarding difficult samples still allows the distilled data to achieve the comparable performance as the original distillation method, or even better.\n             \nCan the author provide more profound and solid explanations for the point mentioned above?\n\nWhen comparing with baseline methods, the author did not compare with the state-of-the-art methods like FTD and TESLA. Nevertheless, this method only achieved state-of-the-art performance in 8 out of the 14 experimental setups.\n\nCan the author complete the relevant comparative experiments and provide an objective analysis of the experimental results\uff1f\n\n\nDirectly using MSELoss as a criterion to discard a substantial proportion of samples.  Could this lead to a bias in distilled data?"
            },
            "questions": {
                "value": "Please refer to the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724026980,
        "cdate": 1698724026980,
        "tmdate": 1700544706225,
        "mdate": 1700544706225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1ykiueO1K",
        "forum": "xm8okHEC3G",
        "replyto": "xm8okHEC3G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
        ],
        "content": {
            "summary": {
                "value": "The paper innovatively tackles the challenge of Dataset Distillation (DD) with a focus on sample cruciality in the outer loop of the bi-level learning problem. Building upon the neural Feature Regression (FRePo) framework, the authors introduce the Infinite Semantic Augmentation (ISA) algorithm. This algorithm enriches harder-to-represent samples in the semantic space through a process of continuous interpolation between two target feature vectors. Importantly, the algorithm is highly efficient as it formulates the joint contribution to training loss as an analytical closed-form integral solution. The method is rigorously evaluated on five benchmark datasets including MNIST, Fashion-MNIST, CIFAR10, CIFAR100, and Tiny-ImageNet. It is also compared against six baseline dataset distillation algorithms: DSA, DM, MTT, KIP, RFAD and FRePo. The experimental results demonstrate that the proposed ISA method effectively reduces dataset size while maintaining or even enhancing model accuracy. The distilled data also proves to be beneficial for downstream applications such as continual learning and privacy protection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Originality**: The paper's focus on optimizing the outer loop in the bi-level optimization problem for Dataset Distillation is original.\n- **Quality**: The paper is methodologically sound, demonstrated by a comprehensive set of experiments across five benchmark datasets. It also includes an ablation study that pinpoints the contributions of different components. The derivation of the integral into an analytical closed-form solution makes the algorithm an efficient solution.\n- **Clarity**: The paper is well-organized and the algorithmic steps are outlined in detail.\n- **Significance**: The proposed method is efficient, achieving state-of-the-art results in dataset size reduction while maintaining or even improving model performance."
            },
            "weaknesses": {
                "value": "1. **Inconsistent and Marginal Gains in Test Accuracy**: The test accuracy of the proposed method doesn't consistently outperform existing techniques. When it does show an improvement, the margin is sometimes minimal.\n2. **Incomplete Review of Related Work**: The paper falls short in its coverage of existing literature. The need for a more comprehensive review is also detailed in the \"Questions\" section below.\n3. **Lack of Comparative Analysis with Data Selection Algorithms**: The experiments in the paper do not include comparisons with data selection algorithms, leaving a gap in understanding how the proposed method stacks up against these approaches."
            },
            "questions": {
                "value": "1. **Clarification on \"Data Extension\" Terminology**: The term \"data extension\" is unfamiliar and appears to be non-standard. Is it synonymous with commonly used terms like \"data augmentation\" or \"data interpolation\"? If not, what differentiates it, and why opt for this term?\n2. **Major Revisions in Related Work Section Needed**: The section on related work requires substantial updates for completeness and context.\n    1. **Coresets**: The paper cited is neither the seminal work nor the most recent in the field of coresets. It would be beneficial to include at least these two papers [1*] and [2*], and consider citing earlier foundational works they mention, perhaps in an appendix.\n    2. **Dataset Distillation**: In addition to [2*], works like [3*] and [4*] are missing from both the discussion and comparison tables. Also, MTT, which is covered in the experiments, lacks mention in the related work section. Please include these papers in both the textual discussion and comparative evaluations.\n\n[1*] Yang, Y., Kang, H. & Mirzasoleiman, B.. (2023). Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. *Proceedings of the 40th International Conference on Machine Learning*, in *Proceedings of Machine Learning Research* 202:39314-39330 Available from https://proceedings.mlr.press/v202/yang23g.html.\n\n[2*] Shin, S., Bae, H., Shin, D., Joo, W. & Moon, I.. (2023). Loss-Curvature Matching for Dataset Selection and Condensation. *Proceedings of The 26th International Conference on Artificial Intelligence and Statistics*, in *Proceedings of Machine Learning Research* 206:8606-8628 Available from https://proceedings.mlr.press/v206/shin23a.html.\n\n[3*] Kim, J., Kim, J., Oh, S.J., Yun, S., Song, H., Jeong, J., Ha, J. & Song, H.O.. (2022). Dataset Condensation via Efficient Synthetic-Data Parameterization. *Proceedings of the 39th International Conference on Machine Learning*, in *Proceedings of Machine Learning Research* 162:11102-11118 Available from https://proceedings.mlr.press/v162/kim22c.html.\n\n[4*] Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., ... & You, Y. (2022). Cafe: Learning to condense dataset by aligning features. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 12196-12205)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730865294,
        "cdate": 1698730865294,
        "tmdate": 1699636420020,
        "mdate": 1699636420020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8lCfO0of4b",
        "forum": "xm8okHEC3G",
        "replyto": "xm8okHEC3G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two techniques to boost the performance of kernel-based dataset distillation methods: discarding easy samples and infinite semantic augmentation. Experiments on several benchmarks demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed infinite semantic augmentation technique is mathematically elegant and effective.\n2. The experimental evaluations are comprehensive enough to validate the effectiveness.\n3. The writing is coherent and easy to follow."
            },
            "weaknesses": {
                "value": "My major concern is on discarding easy samples. \n1. On the one hand, this step requires computing the NFR loss in FRePo twice, which would require longer running time for dataset distillation and make the baseline complex. \n2. On the other hand, this technique is heuristic and seems counterfactual. Intuitively, easy samples should contain some common patterns that can reflect what a class of objects looks like in general. These samples should be more effective than hard samples to capture the major features of each class. In dataset distillation, major information is expected to be stored while other unusual patterns are discarded. It seems strange to me that discarding easy samples leads to better performance, especially when IPC is small. \n3. This strategy drops some samples, which destroys the original data distribution. However, according to Fig. 1, it makes the distilled data better follow the original distribution, which seems strange to me.\n4. Moreover, there seems to be a paper using a similar technique [a].\n5. I would like to see separate results of only using this strategy without ISA, such as in Tab. 2, 3, 4, and 8.\n6. I suggest the authors compare qualitative samples of the baseline, with selection, and with ISA together to better reflect the functionality of each part. Currently it seems that the qualitative results are not different from the original FRePo too much.\n\n[a] Prune then distill: Dataset distillation with importance sampling, ICASSP 2023."
            },
            "questions": {
                "value": "Please refer to Weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764804792,
        "cdate": 1698764804792,
        "tmdate": 1700625698851,
        "mdate": 1700625698851,
        "license": "CC BY 4.0",
        "version": 2
    }
]