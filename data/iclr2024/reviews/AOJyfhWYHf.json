[
    {
        "id": "vTYiBQH9U8",
        "forum": "AOJyfhWYHf",
        "replyto": "AOJyfhWYHf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_g6Xd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_g6Xd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents OpenChat, a framework that uses a conditioned-RLFT (C-RLFT) method to improve open-source language models with mixed-quality instruction tuning data. The authors demonstrate that their model, openchat-13b, outperforms other 13b open-source language models on extensive benchmarks, with notable advantages such as simplicity, RL-free training, and minimal reward quality requirements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed method, conditioned-RLFT (C-RLFT), is overall novel and simple. It regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. The optimal policy in C-RLFT can be solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. \n* The authors demonstrate that OpenChat achieves the highest average performance among all 13b open-source language models across commonly used instruction-following benchmarks."
            },
            "weaknesses": {
                "value": "* My biggest concern is that while the paper claims their approach to be based on single-stage RL-free supervised learning, they do use data generated by API models (GPT-4 & GPT-3.5), and these models are believed to have gone through multi-stage training with RLHF. If the C-RLFT method leverages these kinds of data for their training procedure, it's not very convincing to me that the method really works in a fundamentally different way from prior approaches (SFT + RLHF). It also makes the comparisons to baselines not fully fair, since many baselines (under the 13b model scale) being compared to have not leveraged training data generated by (much larger) API models."
            },
            "questions": {
                "value": "* Would the proposed method work if the mixed-quality data are generated by open-sourced (and potentially smaller) models, such as LLaMA-13b vs LLaMA-7b?\n* I'd like to see a discussion of Xu et al. (I understand that Xu et al. came out probably after this work, but it seems to me that there are some similarities in methodology between the two papers).\n\nReference:  \nXu et al. \u201cContrastive Post-training Large Language Models on Data Curriculum.\u201d ArXiv abs/2310.02263"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786004658,
        "cdate": 1698786004658,
        "tmdate": 1699636175836,
        "mdate": 1699636175836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UKTjZk6zPy",
        "forum": "AOJyfhWYHf",
        "replyto": "AOJyfhWYHf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_yFnS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_yFnS"
        ],
        "content": {
            "summary": {
                "value": "This study proposes OpenChat, a new framework that refines open-source language models using mixed-quality data, applying a unique C(onditioned)-RLFT method that bypasses the necessity for detailed preference labels. The proposed method allows for a more straightforward policy learning process, leading to a large improvement in model performance. The openchat-13b model, fine-tuned via C-RLFT, notably surpasses peers in average performance across benchmarks and exhibits superior generalization in AGIEval tests. The researchers plan to make their code, data, and models publicly accessible, facilitating advancements in the field."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is emphasizing data quality, which is a critical aspect of the model alignment problem.\nThe framework is straightforward and replicable, delivering significant improvements across various benchmark datasets.\nThe evaluation is comprehensive, conducted against several established models and on a wide range of benchmark datasets, with the experimental design being thoroughly considered."
            },
            "weaknesses": {
                "value": "The paper does not adequately address the challenge of estimating data quality when leveraging mixed-quality data sources. The method presented seems to oversimplify this estimation and the associated reinforcement learning (RL) reward mechanism:\n\nThe process of data quality estimation may be too simplified and not easily generalizable to realistic settings involving human feedback. The estimation relies on the assumption that GPT-4 provides better quality outputs than GPT-3.5, and GPT-3.5 surpasses the base model. Although this may be true in some instances, it's not universally applicable across all domains and might not hold as the base model evolves. This assumption could be problematic if the alignment is based on a superior base model or a larger one. Furthermore, presuming that the GPT series invariably delivers superior samples can inadvertently lead to the replication of any negative behaviors from these models, making it challenging to improve upon them with base model developments.\n\nThe method applies static, positive, example-level rewards for learning, foregoing the primary advantages of an RL framework. An RL approach can benefit from both positive and negative samples and can assign more nuanced, token-level rewards, rather than broad example-level ones. It also enables the formulation of objectives that are not readily translated into intermediate scores. The static, example-level rewards could potentially be reformulated into a supervised learning loss, combined with data weighting strategies. The supervised learning loss with data weighting strategies are"
            },
            "questions": {
                "value": "1.  One of interesting question when handling the mixed data quality problem is on the contribution of data with various quality. In this paper, one research question is what the performance will be if only GPT-4 samples are used even the size is ten times smaller than GPT-3.5.  As a follow-up, how does the quality of data from different versions of language models, like GPT-4 and GPT-3.5, impact the performance of fine-tuned models, and what is the optimal quantity ratio of high-quality to lower-quality data necessary for achieving equivalent performance enhancements?\n\n2. What strategies can be implemented within the proposed framework to mitigate the transfer of potentially negative behaviors from GPT-4 examples to the base model, and how can such behaviors be identified and counteracted effectively?\n\n3. In what ways can the proposed framework be adapted to more complex and realistic data environments, such as those involving human-generated data, which may exhibit significant variations in quality and where quality assessments may be more subjective?\n\n4. How does a supervised learning approach with data weighting, predicated on the assumption that GPT-4 provides higher quality data than GPT-3.5, compare in performance to the RL framework, and can the simplicity of supervised loss be reconciled with the advantages of an RL-based approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2405/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2405/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2405/Reviewer_yFnS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699200594499,
        "cdate": 1699200594499,
        "tmdate": 1700719229975,
        "mdate": 1700719229975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B8hxyc42tI",
        "forum": "AOJyfhWYHf",
        "replyto": "AOJyfhWYHf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_U8fK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_U8fK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes OpenChat, a framework for fine-tuning open-source language models with mixed-quality data. It proposes Conditioned-RLFT: A novel method that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. This method is simple, RL-free, and does not require costly human feedback. Using RLFT, they finetuned OpenChat-13b, a language model based on LLaMA-2-13b and the ShareGPT dataset, which consists of conversations from GPT-4 and GPT-3.5. This model achieves the highest average performance on three standard benchmarks for instruction following ability among all 13b open-source language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The paper introduces a new method, C-RLFT, that leverages coarse-grained rewards and class-conditioned policies to align the language model with human goals. The paper also provides a theoretical analysis and derivation of the optimal policy for C-RLFT. The resulting framework is a conditioned SFT with weighted loss, which is easy to implement and more stable than RLHF.\n- Quality: The paper presents evaluation results on three benchmarks to assess instruction following ability of the proposed model. The baselines include popular open and closed LLMs. The paper also performs ablation studies, representation visualization, prompt token effects, evaluator consistency, and data size effects to validate the effectiveness and robustness of OpenChat.\n- Clarity: The paper is well-written and organized.\n- Significance: The paper addresses an important and challenging problem of fine-tuning open-source language models with mixed-quality data. The paper demonstrates that OpenChat can achieve superior performance and generalization with simple and RL-free training, minimal reward quality requirements, and easily collectible data."
            },
            "weaknesses": {
                "value": "- The paper claims the superiority of C-RLFT over RLFT. But the claim is only supported by openchat-13b, which distilled its knowledge from two RLFT models: GPT-4 and GPT-3.5. So, it is only fair to say C-RLFT is better than SFT when distilling from GPT models, which is based on the fact the openchat-13b is better than vicuna-13b-1.5. However, one cannot say C-RLFT is better than RLFT, because openchat-13b learns from models trained by RLFT.\n- It is not clear if the \"low quality data\" is even useful at all. (see my question below)"
            },
            "questions": {
                "value": "- What is the performance if the llama model is only trained on GPT-4 data? I feel it should be another ablated model to demonstrate the value of \"low quality data\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699320994086,
        "cdate": 1699320994086,
        "tmdate": 1699636175700,
        "mdate": 1699636175700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "89DiT5dlAa",
        "forum": "AOJyfhWYHf",
        "replyto": "AOJyfhWYHf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_QA7s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2405/Reviewer_QA7s"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method to improve training quality of language models with mixed-quality data. Specifically, it consider the training data consists of good-quality expert data and low-quality data. Then it proposes conditioned RLFT that regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Experimental evaluation shows that the proposed method achieves the highest average performance among all 13b open-source language models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is easy and efficient to train.\n2. The method has many practical application scenarios where data quality can't be controlled very well especially for language model training.\n3. The experimental evaluation confirms the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. It requires the data quality to be divisible into two sets: good quality and average quality, which might potentially limit is applications."
            },
            "questions": {
                "value": "1. how sensitive is the results to the division of \"good\" data and \"average\" data? For example, how about if both sets of the data is of the same quality?\n2. When training with Reinforcement Learning, how stable is the training process, since RL is usually different to train in most cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699348431838,
        "cdate": 1699348431838,
        "tmdate": 1699636175634,
        "mdate": 1699636175634,
        "license": "CC BY 4.0",
        "version": 2
    }
]