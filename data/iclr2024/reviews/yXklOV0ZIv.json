[
    {
        "id": "T3hHG6upvJ",
        "forum": "yXklOV0ZIv",
        "replyto": "yXklOV0ZIv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_6GjB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_6GjB"
        ],
        "content": {
            "summary": {
                "value": "This paper is out of my knowledge, and I tend to not submit any reviews for this paper. Thanks for the submission. Please ignore my ratings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is out of my knowledge, and I tend to not submit any reviews for this paper. Thanks for the submission."
            },
            "weaknesses": {
                "value": "This paper is out of my knowledge, and I tend to not submit any reviews for this paper. Thanks for the submission."
            },
            "questions": {
                "value": "This paper is out of my knowledge, and I tend to not submit any reviews for this paper. Thanks for the submission."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1986/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636239702,
        "cdate": 1698636239702,
        "tmdate": 1699636130416,
        "mdate": 1699636130416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SxoLFDlmpD",
        "forum": "yXklOV0ZIv",
        "replyto": "yXklOV0ZIv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_i4Hf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_i4Hf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to extend counterfactual attention learning via a learnable counterfactual attention module to further improve the ability of counterfactual attention. To learn this learnable attention model, this paper designs a series of loss functions. Beyond the conventional cross-entropy and counterfactual losses, it adds three extra loss terms. First, a cross-entropy loss is applied to the counterfactual attention branch as the regularization term to make the counterfactual attention more meaningful. Second, a multiple classification loss for the counterfactual attention branch is designed to limit the performance of the counterfactual attention. Third, an L1 loss between the attention maps of factual and counterfactual branches encourages the difference.  This method is evaluated with the singer identification task which also requires the fine-grained identification ability.   Specifically, the benchmark artist20 dataset is employed for the comparison, including the comparison with other SOTA methods, and ablation studies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper proposes a learnable counterfactual attention module to achieve better performance. \n2) This paper provides detailed explanations of each extra loss-term."
            },
            "weaknesses": {
                "value": "Some concerns about this paper are summarized below:\n1) From the experiments in the original CAL method. It seems that the refinement of the counterfactual attention branch didn't improve the recognition performance. What is the motivation to refine the counterfactual attention branch? Did you find any insights from the preliminary experiments to support this enhancement path?\n2) As shown in Figure 2 and Section 3.2, CAL applies the counterfactual \"intervention\" to cut off the causal relations between image and attention.  The goal of this \"intervention\" is to obtain the independent effects and further benefit the calculation of the Total Direct Effect. Here, both the counterfactual and factual branches are learned from the image. From the perspective of causality, how can you guarantee the independence between counterfactual and factual attention for the causal inference? \n3) For the extra loss terms, there are two contradictory losses, one is to make the counterfactual attention meaningful and the other is to make it not too meaningful.  How can we balance these two losses?  Is it robust for different scenarios? Do the hyper-parameters of loss rates matter for the performance?\n4)  The motivation of this paper is to improve counterfactual attention learning. Given this goal, it is better to evaluate the proposed method and the key baseline (CAL) in the same settings, such as CUB, Cars, Aircraft, and so on.  \n5) As shown in Table 1, the improvement of the proposed LCA is very limited. Why these results can support the conclusion? Take the Best frame-level results as an example, the proposed method only improves 0.02 accuracy. \n6) Only one dataset is used.  The evaluation process seems to be not solid.  It is suggested to add more."
            },
            "questions": {
                "value": "Beyond the questions in the weaknesses part, there are some questions for the details. \n\n1) As shown in Table 1, which counterfactual attention is used as a baseline, random, mean, or shuffle?\n2) What are the loss rates used in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1986/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708081665,
        "cdate": 1698708081665,
        "tmdate": 1699636130343,
        "mdate": 1699636130343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8HHZquGyfX",
        "forum": "yXklOV0ZIv",
        "replyto": "yXklOV0ZIv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_5eKQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_5eKQ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a learnable counterfactual attention method to improve the recognition performance of the singer identification task. The method improves the existing counterfactual attention learning framework by replacing the random counterfactual attention with learnable ones. With some new and specifically designed loss functions, the method can better discover effective attention regions and show improvement on SID benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed learnable counterfactual attention is well-motivated and reasonable. Empirical results show the method is effective on the SID task.\n\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- My main concern is the generality of the proposed method. The proposed method is motivated by the limitations of the existing CAL method. However, this paper only evaluates the method on the SID task, which is a less popular and competitive task compared to the tasks considered in CAL. According to the analysis provided in Section 3.2, LCA didn't add assumptions on the data or task types over CAL. So it is not clear why the proposed method is only evaluated on the SID task. Considering ICLR is a machine learning conference, I think showing the generality of the proposed method is also helpful to make the paper more suitable for publishing on ICLR and interest the audience of the conference. \n\n- The authors claim the method can \"guide the main branch to deviate from those regions, thereby focusing attention on discriminative\nregions to learn singer-specific features in fine-grained vocals\". I think it would be better to provide some quantitative evidence to support this claim."
            },
            "questions": {
                "value": "Although I find the proposed method is well-motivated and reasonable, I still have some concerns about the experimental study and positioning of this paper. I think the paper can be further improved if the above problems can be solved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1986/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762285693,
        "cdate": 1698762285693,
        "tmdate": 1699636130212,
        "mdate": 1699636130212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9G1RrtYTbi",
        "forum": "yXklOV0ZIv",
        "replyto": "yXklOV0ZIv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_hb43"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1986/Reviewer_hb43"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a learnable counterfactual attention mechanism, specifically tailored for the singer identification task, aiming to address the limitations of existing counterfactual attention learning. Unlike the traditional approach that depends on random attentions, the LCA mechanism enhances the ability of counterfactual attention to identify fine-grained vocals through direct learning. The implementation involves integrating a counterfactual attention branch into the existing model. This addition is meticulously guided by multiple loss functions, ensuring that the counterfactual attention branch focuses on regions that are meaningful yet not overly discriminative, avoiding potentially misleading results. Meanwhile, it directs the main branch towards discriminative regions to learn singer-specific features effectively. The performance improvement is demonstrated through evaluation on the benchmark artist20 dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors provide a clear and intuitive analysis of the limitations present in the baseline method (CAL), proposing straightforward yet effective solutions to enhance its performance. Each proposed solution, addressing Characteristics 1, 2, and 3, demonstrates simplicity and efficacy. The experimental results solidify the effectiveness of the introduced loss functions, providing tangible evidence of improvement. Furthermore, the manuscript is well-structured and articulated, ensuring a smooth and comprehensible reading experience for the audience."
            },
            "weaknesses": {
                "value": "1. Regarding the second Characteristic 2 (Targeting biased regions without outperforming the main branch), I can not see the logic between forcing the class distribution to be smooth and targeting biased regions without outperforming the main branch. Moreover, the assertion that a smoother output distribution directly correlates to effectively targeting biased regions without overshadowing the main branch is questionable. Since the output distributions of the two branches lead to disparate final classification results, this assumption appears to be unfounded and requires further clarification.\n\n2. Regarding Characteristic 3 (Regions of focus should differ from the main branch\u2019s attention), the manuscript appears to implicitly assume superior performance of the main branch. However, empirical observations suggest that the main branch\u2019s performance is not as exemplary as presumed. A straightforward deviation from the main branch's attention map might inadvertently introduce inaccuracies. This aspect of the methodology warrants a more cautious approach and a thorough examination to validate its effectiveness and mitigate potential risks of error introduction."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1986/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1986/Reviewer_hb43",
                    "ICLR.cc/2024/Conference/Submission1986/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1986/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768965204,
        "cdate": 1698768965204,
        "tmdate": 1700550258280,
        "mdate": 1700550258280,
        "license": "CC BY 4.0",
        "version": 2
    }
]