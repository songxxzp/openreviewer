[
    {
        "id": "sypY6NuQto",
        "forum": "vpJMJerXHU",
        "replyto": "vpJMJerXHU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_dxG8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_dxG8"
        ],
        "content": {
            "summary": {
                "value": "-\tThe paper investigates a modern convolution architecture for time series data. The paper analyzes a pure convolutional architecture for time series analysis with the idea of modernizing the convolution structure to resemble that of a transformer. As existing temporal convolutional neural networks (TCNs) still lack the capability to capture long-term dependencies due to the limited effective receptive fields of the convolutional architecture, the paper proposes ModernTCN, a pure convolutional feed-forward block. It achieves this by optimizing depth-wise convolution with a large kernel size and two types of grouped point-wise convolutions, separately grouped on variable and feature dimensions. The three components capture cross-time, cross-variable, and cross-feature dependencies, respectively. The results demonstrate that the proposed method outperforms transformer-based models and other task-specific baselines in long-term and short-term prediction, classification, imputation, and anomaly detection, providing enhanced efficiency."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe paper investigates a recent idea of modernization in computer vision applied to time series analysis and identifies consistent performance and efficiency with sufficient comparison experiments and presentational supports."
            },
            "weaknesses": {
                "value": "-\tAlthough the primary claim for performance improvement centers on enlarging effective receptive fields, the paper lacks theoretical analysis as the authors acknowledge.\n\n-\tSome description details are not sufficiently clear. For example, in Table 4, it would be good to clarify which module the authors used as the \u2018undecoupled ConvFFN\u2019."
            },
            "questions": {
                "value": "-\tIn the experiments on regression tasks, it is concerned whether the RevIN (2021) normalization (as detailed in Appendix B.2) has a significant impact on the performance improvement. Therefore, an additional ablation study about RevIN is requested to enhance clarity. This request is based on the observation that RevIN normalization alone led to improvements in both qualitative and quantitative performance in some of the baseline models.\n\n-\tWhile the paper asserts the claim of enlarging the effective receptive field (ERF) as the key to using convolution in time series analysis in section 5.2, it would be beneficial to include an additional visualization of the ERF of TimesNet (2022) in Figure 1 to further substantiate the claim. TimesNet is another convolution-based model that utilizes shorter kernel sizes and more layers within a block, and it exhibits lower performance than ModernTCN.\n\n-\tSimple errata: Enlaring -> Enlarging (in section 5.2.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639779493,
        "cdate": 1698639779493,
        "tmdate": 1699636521238,
        "mdate": 1699636521238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e0uGSbG8E1",
        "forum": "vpJMJerXHU",
        "replyto": "vpJMJerXHU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_exNa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_exNa"
        ],
        "content": {
            "summary": {
                "value": "This paper uses a pure convolution structure (ModernTCN) to perform the time series analysis. Specifically, ModernTCN uses a 1D convolution stem layer, large kernel DWConv and ConvFFN to patchify embedding, capture the temporal dependency of each univariate time series independently and mix the information across feature and variable dimensions respectively. Five mainstream analysis tasks, including long-term and short-term forecasting, imputation, classification and anomaly detection are used to evaluate the effectiveness of ModernTCN."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper uses the pure convolutional neural network (CNN) for the time series analysis, which is seldomly explored. In addition, five mainstream analysis tasks for the time series data are considered for the evaluation."
            },
            "weaknesses": {
                "value": "1. The details of the blocks (DWConv and CovnFFN) should be introduced clearly (Some readers may not have related background Knowledge).\n\n2. It is not clear how to select the best results from different input lengths, based on the validation loss or the test results directly?\n\n3. The performance improvement is marginal. In addition, there is another MLP-based work beating PatchTST in same cases, which should be compared with. \n\nLi, Zhe, et al. \"Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping.\" arXiv preprint arXiv:2305.10721 (2023).\n\n4. Although it is said that \"we also include the state-of-the-art models in each specific task as additional baselines for a comprehensive comparison\", some SoTA models are missed in the paper, e.g.,\n\nShort-term forecasting: Wang Xue, et al. \"Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual\nTransformer.\" arXiv preprint arXiv:2305.12095 (2023).\n\nAnomaly Detection: Yang, Yiyuan, et al. \"DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection.\" KDD 2023.\n\n5. How about the short-term forecasting results on the multivariate time series datasets (e.g., the datasets used for long-term forecasting in the paper)?\n\n6. The code is not available for reproducibility."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5228/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5228/Reviewer_exNa",
                    "ICLR.cc/2024/Conference/Submission5228/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698710857123,
        "cdate": 1698710857123,
        "tmdate": 1700511766697,
        "mdate": 1700511766697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zwVjG83DqW",
        "forum": "vpJMJerXHU",
        "replyto": "vpJMJerXHU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_kELp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5228/Reviewer_kELp"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a pure convolution architecture for time series analysis. The proposed ModernTCN block is partially inspired by modern convolution blocks as in ConvNeXt. ModernTCN mainly utilizes Depthwise Convolution and Grouped Pointwise convolution to learn cross-variable and cross-feature information in a decoupled way. With the efficiency of convolution operations, ModernTCN achieves impressive performance on various time series analysis tasks. The authors also made a great effort to conduct comprehensive ablation studies to understand the different components of ModernTCN."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The writing is pretty clear and easy to follow.\n- State-of-the-art performance on various time series tasks and the convolution-based architecture enjoys good training efficiency.\n- Extensive and comprehensive ablation studies to understand different components of the proposed model\n- A valuable contribution to picking up pure-convolution based approach for time series analysis"
            },
            "weaknesses": {
                "value": "- It remains unclear how the proposed ModernTCN block would connect to existing literature. While pure convolution architecture is a sweet spot of efficiency and performance, based on what is claimed in the paper, it would be expected that the way to incorporate cross-variable information with ConvFFN would be useful on top of other variable-independent approaches such as PatchTST and DLinear. But this is missing in the current manuscript."
            },
            "questions": {
                "value": "- What does *Discard Variable Dimension* refer to in Table 4? In Table 4, it is shown that discarding variable dimensions leads to a significant performance degradation. However, in Appendix E, it is shown that univariate forecasting performance is still quite competitive. So what is the difference between the two?\n- From Table 4, one important piece of ModernTCN is to exploit cross-variable information. However, notice that some of the other models, e.g. DLinear are variable independent. So is the proposed modern TCN block orthogonal to other approaches and is it possible to apply it on top of variable-independent models like DLinear?\n- Based on the appendix, it seems that very different hyperparameters are used for different tasks, e.g. channel number, two kernel sizes, patching size and strides, number of TCN blocks, etc. What is your hyperparameter search space?\n- How is it different to use a projection layer (as in PatchTST) vs convolution as the stem layer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5228/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5228/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5228/Reviewer_kELp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5228/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715246775,
        "cdate": 1698715246775,
        "tmdate": 1699636521036,
        "mdate": 1699636521036,
        "license": "CC BY 4.0",
        "version": 2
    }
]