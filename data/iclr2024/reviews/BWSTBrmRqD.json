[
    {
        "id": "uu2JAO0ea7",
        "forum": "BWSTBrmRqD",
        "replyto": "BWSTBrmRqD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
        ],
        "content": {
            "summary": {
                "value": "The paper focus on visual language reasoning problems which requires extraction of text or numbers from information-dense images like charts or plots. The proposed method includes a dual-system for multi-step multimodal reasoning, which consists of a \u201cSystem-1\u201d step for visual information extraction and a \u201cSystem-2\u201d step for deliberate reasoning. By fine-tuning LLaMA-2 70B on only a small amount of data on multi-step reasoning, the accuracy of the model surpasses the best fully-supervised end-to-end approach by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on ChartQA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022 The paper is well written and easy to understand. Figure 1 provides a good overview of the complete system.\n\u2022 The paper presents promising results on ChartQA and outperforms prior supervised baselines.\n\u2022 The paper includes ablation studies in Figure 3."
            },
            "weaknesses": {
                "value": "\u2022 Novelty: The core idea of the paper is very similar to prior work, including \u201cVisual Programming: Compositional Visual Reasoning Without Training, CVPR 2023\u201d which also uses a large LLM for reasoning and perception modules to extract information from images. Additionally, \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d also performs zero-shot multi-modal reasoning in a similar fashion. \u201cLook, Remember and Reason: Visual Reasoning with Grounded Rationales, ICML workshop 2023\u201d combines System-1 and System-2 inference in a single model using rationales. \n\n\u2022 It is unclear why the performance on PlotQA much worse compared to ChartQA. The paper mentions that PlotQA \u201cis a synthetic dataset with template based and restricted types of questions\u201d. But this should be easier to solve compared to ChartQA, as the proposed approach also follows templated reasoning steps. The paper should make it clear with ample qualitative examples why performance on PlotQA is lacking. \n\n\u2022 Fairness of the comparison to Few-Shot DePlot versions of GPT-4 and LLaMA: The proposed  DOMINO version of LLaMA has more information about the chart in question. Therefore, it is unclear if the evaluation is fair. \n\n\u2022 Qualitive examples: The paper is lacking qualitive examples from PlotQA in the main paper. The main paper only includes a single qualitative example from ChartQA in the main paper. Examples of failure cases in Table 7 are hard to follow as the associated charts are not available. The paper should include more quantitative examples  which are easier to follow. The format of \u201cGPT-4 Technical Report, arXiv 2023\u201d can serve as a guiding example.\n\n\u2022 Additional datasets: The paper evaluates performance only on two datasets. There are also more challenging datasets available: SciCap (http://scicap.ai/). The SciCap uses real-world data and requires high-level reasoning along with low-level understanding of scientific figures. It would be an ideal testbed to evaluate the performance of the proposed approach."
            },
            "questions": {
                "value": "\u2022 The paper should discuss prior work such as \u201cVisual Programming: Compositional Visual Reasoning Without Training, CVPR 2023\u201d, \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d, \u201cLook, Remember and Reason: Visual Reasoning with Grounded Rationales, ICML workshop 2023\u201d in more detail.\n\n\u2022 The should discuss the challenges associated with PlotQA in more detail, ideally with qualitative examples.\n\n\u2022 The fairness of the comparison to Few-Shot DePlot versions of GPT-4 and LLaMA should be discussed in more detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730246921,
        "cdate": 1698730246921,
        "tmdate": 1699636353095,
        "mdate": 1699636353095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MLDR1kTD18",
        "forum": "BWSTBrmRqD",
        "replyto": "BWSTBrmRqD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces DOMINO, a dual-system designed for charts/plots reasoning. DOMINO consists of two models: The first model, called system-1, uses vision and language to extract specific information from images. The second model, system-2, is a large language model that decomposes tasks and generates answers. Experimental results indicate that DOMINO surpasses traditional pipeline approaches in handling both in- and out-of-distribution data. With limited training samples, DOMINO also achieves SOTA results on ChartQA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This method is intuitive, and I am happy to see the introduction of dual-system into vision-language reasoning.\n2. The proposed method achieves SOTA results on ChartQA.\n3. Analysis shows that DOMINO is more robust in handling complex charts."
            },
            "weaknesses": {
                "value": "1. The author didn't discuss about the efficiency. How does the inference efficiency of DOMINO compare to the baseline method?\n2. The template seems relatively limited, more non-chartQA tasks are needed to confirm the potential of this method."
            },
            "questions": {
                "value": "1. What types of charts are included in ChartQA and PlotQA? I think adding relevant descriptions can help people have a more intuitive understanding of the capabilities of this method.\n2. Does the author consider the dual-system approach to be universally applicable? Can it replace other MLLM methods (such as BLIP2 [1], LLAVA [2]) and become a common solution for solving visual QA problems? For example, besides tasks like chartQA, can DOMINO also generalize to other tasks (such as VQA)?\n\n[1] Li, J., Li, D., Savarese, S., & Hoi, S.C. (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ArXiv, abs/2301.12597.\n[2] Liu, H., Li, C., Wu, Q., & Lee, Y.J. (2023). Visual Instruction Tuning. ArXiv, abs/2304.08485."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738645817,
        "cdate": 1698738645817,
        "tmdate": 1699636353013,
        "mdate": 1699636353013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0U9IyG7djF",
        "forum": "BWSTBrmRqD",
        "replyto": "BWSTBrmRqD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a two component system for chart/plot reasoning. The system is composed of a DePlot backbone and a LLaMa-2 model, the former used to extract information from the chart, while the later for decomposing the question and give final answer based on reasoning. After fine-tuning DePlot on instruction level tasks and LLM on a small number of hand written solutions, the system surpasses prompt-based baselines and some supervised methods. The performance gain is attributed to the improvement in both decomposition and answering."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written.\n2. The results are great, compared to few-shot baselines, and the performance gain is analyzed carefully.\n3. The paper proposed a demonstration of two stage reasoning using LLMs for task decomposition using the feedback from perception results, which is novel compared to similar LLM-guided systems without feedback, e.g., [1]. The efficiency of fine-tuning of LLM also supports the decomposition of System-1/2.\n4. The authors thoroughly discussed the functionality of each component in the reasoning process through ablation studies and analyzed the error made by the models.\n\n[1] https://arxiv.org/abs/2211.11559"
            },
            "weaknesses": {
                "value": "A few unclear points are raised in Questions."
            },
            "questions": {
                "value": "1. Why some of the results are not shown in Table 1?\n2. Why is the correct answer for arithmetic in Table 7 is -50752953286.0?\n3. What is the evaluation prompt used when there is no `Describe` step?\n4. It would be great if the authors could discuss the applicability of the proposed method on other VQA tasks, e.g. CLEVR.\n5. Is there any examples of the model failed at decomposing the problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796621973,
        "cdate": 1698796621973,
        "tmdate": 1699636352931,
        "mdate": 1699636352931,
        "license": "CC BY 4.0",
        "version": 2
    }
]