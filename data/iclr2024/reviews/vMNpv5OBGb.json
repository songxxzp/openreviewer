[
    {
        "id": "IhgbVXPqyy",
        "forum": "vMNpv5OBGb",
        "replyto": "vMNpv5OBGb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_Ye4S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_Ye4S"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a mixed integer quadratic programming (MIQP) based approach for automatic parallelization in transformer training, jointly optimizing both inter-layer and intra-layer parallelism. UniAP formulates the total time-per-iteration (TPI) as a quadratic function of the decision variables that indicate the intra-layer parallelization strategies (DP-Data parallelism, TP-tensor parallelism, FSDP-fully sharded data parallelism) and the pipeline stage partitioning strategies. The experiments show that UniAP achieves competitive or better performance with Alpa and Gavaltron on transformer-based models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work comprehensively optimizes intra-layer (DP, TP, and FSDP) and inter-layer(PP) partitioning decisions under one unified view. An advantage of this approach is that it can jointly optimize both targets. By doing so, it avoids the potential issue of optimizing one factor at the expense of the others like the bi-level optimization approach.\n3. The evaluation highlights the effectiveness of the formulation, with improved training performance and faster search time achieved using an off-the-shelf MIQP solver compared to a previous dynamic programming and greedy algorithm-based approach Galvatron or Alpa."
            },
            "weaknesses": {
                "value": "1. Since Alpa already uses ILP to formulate the intra-operator parallelism, it may be straightforward to extend it to MIQP since MIQP is more like a variant of ILP.\n2. The work makes certain assumptions about performance that are not always true. For instance, in Section 3.3, it states that \"For Transformer-like models that mainly consist of the MatMul operator, the computation time in the BP stages is roughly twice that of the FP stages\", which may not always be true depending on the optimizer\n3. Regarding the comparison to Alpa, the evaluation is not apple-to-apple. Alpa builds on JAX which uses XLA for JIT optimization, while Gavlaltron uses `torch.compile` for JIT. It would be best to compare Alpa and UniAP on the same framework, possibly done by porting to JAX."
            },
            "questions": {
                "value": "1. The pipeline parallelism formulation of UniAP was based on a GPipe-style schedule, which has a major drawback of large bubble (idle) time. Would it be possible to adapt to more advanced PP schedules like DAPPLE or Chimera, and how would that affect your formulation? \n2. It states that UniAP \"partitions stages and determines micro-batch size using naive greedy algorithm\". Could you elaborate on the exact algorithm you used, and whether you have reported the time for searching the microbatch+#PP stage in the search time (Table 1&2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4433/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839313603,
        "cdate": 1698839313603,
        "tmdate": 1699636418080,
        "mdate": 1699636418080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ehQwXvPJKr",
        "forum": "vMNpv5OBGb",
        "replyto": "vMNpv5OBGb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_S34h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_S34h"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an automated parallelism framework that comprehensively considers both inter- and intra-parallelism.  In comparison to other parallel frameworks, this framework offers a larger strategy space. Additionally, a method based on Mixed Integer Quadratic Programming (MIQP) is introduced to obtain the optimal strategy within this strategy space. Through experiments, the proposed approach demonstrates a 1.7x throughput acceleration compared to other STOA parallel frameworks and a 261x improvement in strategy search efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This article raises a meaningful question and offers a reasonable solution.\n2. This article is well-written"
            },
            "weaknesses": {
                "value": "1. How is the accuracy of your cost model? \n2. Section 3.4 formalizes the MIQP; however, it does not incorporate constraints related to computing resources. More precisely, there is no restriction on the number of GPUs employed by the strategy, thereby allowing it to stay within the available GPU count.\n3. What is the form of the inter-parallel strategy? Is it a simple selection from DP  TP FSDP, or is it a more complex form?\n4.  The maximum parameter size of the models used in the experimental section is 1.02 billion. For the current trend in the development of large models, these models are slightly smaller. We look forward to the authors presenting the parallel performance of larger models. Perhaps your available computing resources limit the use of larger models, and in that case, simulated results would also be acceptable."
            },
            "questions": {
                "value": "1. How is the accuracy of your cost model? \n2. Section 3.4 formalizes the MIQP; however, it does not incorporate constraints related to computing resources. More precisely, there is no restriction on the number of GPUs employed by the strategy, thereby allowing it to stay within the available GPU count.\n3. What is the form of the inter-parallel strategy? Is it a simple selection from DP TP  FSDP, or is it a more complex form?\n4.  The maximum parameter size of the models used in the experimental section is 1.02 billion. For the current trend in the development of large models, these models are slightly smaller. We look forward to the authors presenting the parallel performance of larger models. Perhaps your available computing resources limit the use of larger models, and in that case, simulated results would also be acceptable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4433/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856181627,
        "cdate": 1698856181627,
        "tmdate": 1699636418005,
        "mdate": 1699636418005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V0MpyyN9bm",
        "forum": "vMNpv5OBGb",
        "replyto": "vMNpv5OBGb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_pnV8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4433/Reviewer_pnV8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents UniAP, a system for automatically parallelizing deep learning training workloads across devices. Compared to prior work in the area (e.g., Alpa), the key point is unifying inter and intra-operator parallelism within a single formulation for determining a parallel strategy. Experimental evaluation shows using UniAP delivers improved performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. UniAP aims to improve automatic parallelization for training, which has wide practical benefits for researchers and practitioners. Although not mentioned in the paper, I would add that it seems like it could be particularly beneficial for fine-tuning, which is often performed in situations with fewer resources than full training from scratch.\n2. Further, UniAP aims to unify the representation of parallelization strategies into a single formulation for optimization, a nice result.\n3. The experiments include a variety of transformer and vision transformer models and demonstrate improved performance over existing methods (e.g., Alpa)."
            },
            "weaknesses": {
                "value": "1. I found the paper somewhat hard to follow, and details are unclear. This is especially the case for the formulation of the actual optimization problem, which would benefit from an explanatory figure (and possibly a table of notation). There are also important details that are elided, such as the communication model (the paper states \"UniAP estimates the communication time by dividing the size of transmitting tensors by the communication efficiency for different communication primitives\"); what exactly is used as the \"communication efficiency\"? This seems especially unclear as common libraries (e.g., NCCL) implement different algorithms for the same operation, which can have very different performance (e.g., logarithmic versus linear terms in tree versus ring allreduces).\n2. The paper claims to unify inter- and intra-layer parallelism, but it does not seem that way to me. In particular, UniAP appears to define a cost model which can be used to evaluate the performance of a configuration, but then exhaustively enumerates different pipeline configurations (Section 3.5 & Algorithm 1) in order to find the one with least cost. The paper should address this, and also discuss how this approach compares to those of prior works (e.g., the dynamic programming formulation in Alpa).\n3. The experiments are conducted at quite small scale; the largest cluster (EnvC) has only four nodes and 16 GPUs. Meanwhile, the models are also quite small, with the largest being 1 B parameters (Table 4). While this might be plausible for a fine-tuning situation (although even there one should expect larger models), this seems unrealistic for training large models from scratch, a scenario the paper seems to be targeting. I would like to see experiments with both larger models and clusters (as a data point, the Alpa paper includes results on models up to 70 B parameters). Indeed, it may be best to add comparisons using the same models as in the Alpa paper to provide a solid baseline.\n4. A related point, given the search over pipeline configurations, it is not clear to me how UniAP's search would scale. For example, how would it perform if the cluster contained 10 000 GPUs? (This is not at all unrealistic for large training clusters.)\n5. The experiments omit error bars or any measure of variance, making it hard to judge how stable the results are.\n6. The paper omits important techniques used in practice from its evaluation, in particular, mixed precision (Section 4). While I appreciate the claim that this is orthogonal, using FP32 can result in significantly lower computation performance (due to not using tensor cores), which in turn can significantly change the communication/computation ratio and consequently the best parallel strategy. This is especially strange because Alpa evaluated transformers in FP16.\n7. The experimental evaluation lacks depth. There is not detailed discussion of _why_ UniAP outperforms prior solutions. For example, one could look at the resulting communication volume or GPU utilization. It is hard to disambiguate whether the improved performance is due to framework/configuration differences, or due to improved parallel strategies found by UniAP. Detailed profiling and analysis is necessary. While the paper does briefly mention some differences in the parallel strategies found, these are not discussed in detail."
            },
            "questions": {
                "value": "Please see the above for details.\n\n1. Please improve the clarity of the paper, particularly regarding the presentation of the search algorithm.\n2. Please clarify how UniAP \"unifies\" intra- and inter-layer parallelism search and how its method is superior to those in prior works?\n3. Please conduct experiments at larger scale and with larger models. How does UniAP's search scale with larger clusters?\n4. Please add error bars to all experimental results.\n5. Why do you not use AMP in the evaluation? If you do, what changes?\n6. Please provide a detailed analysis of _why_ UniAP's parallel strategies outperform those of other frameworks.\n\n-----\n\nIn light of the authors' response, I have raised my score; however, I still have concerns about the lack of larger-scale evaluation, as such environments are where automatic parallelism is most promising."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4433/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4433/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4433/Reviewer_pnV8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4433/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699040543550,
        "cdate": 1699040543550,
        "tmdate": 1700671556197,
        "mdate": 1700671556197,
        "license": "CC BY 4.0",
        "version": 2
    }
]