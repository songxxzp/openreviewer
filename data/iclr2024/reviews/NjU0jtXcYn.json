[
    {
        "id": "IV0RvG12xb",
        "forum": "NjU0jtXcYn",
        "replyto": "NjU0jtXcYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_TtXD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_TtXD"
        ],
        "content": {
            "summary": {
                "value": "This submission provides a unifying framework for encompassing user beliefs in Bayesian Optimization (BO) beyond the usual priors on kernel  hyperparameters. Previous approaches offered to augment BO with expert beliefs $\\pi$, like optimum value or location, but mostly focused on doing so at the acquisition function level. Here, the authors propose to integrate this at the surrogate level. Instead of the classical Gaussian Process prior $p(f)$, they introduce a user belief over functions $\\pi(f) \\propto \\frac{p(f|\\pi)}{p(f)}$. Thus, $p(f|\\pi)$ can be obtained by reweighting samples from $p(f)$ proportionally to their probability of occurring under $\\pi(f)$. \n\nAs the user belief is assumed independent of the data-generating mechanism, the resulting posterior $p(f|\\mathcal{D},\\pi)$ is naturally proportional to the user belief $\\pi(f)$ and the likelihood $p(f|\\mathcal{D})$. It is therefore non Gaussian for nontrivial user beliefs, an issue circumvented by the authors using a decoupled sampling scheme. Likewise, classical acquisition functions like Expected Improvement or Maximum Entropy Search are not tractable anymore, which led the authors to leverage and tailor their Monte-Carlo version to this specific case. \n\nIn the end, the proposed method, *ColaBO*, is then evaluated on a range of synthetic and real-world benchmarks, and demonstrates convincing performances against its competitors, particularly for misleading user beliefs.\nAs these benchmarks contain hyperparameter tuning of Deep Learning models, I believe this submission completely falls into the scope of ICLR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- To the best of my knowledge, this is the first attempt to integrate user prior beliefs directly at the level of the surrogate rather than at the acquisition function level. The approach is novel and encompasses multiple ways for the user to incorporate its expertise: knowledge of function optimum value, optimum location, and preferences.\n- I like Figures 1 and 2, they nicely illustrate the benefits of incorporating user beliefs and how this impacts the GP posterior and the acquisition function landscape."
            },
            "weaknesses": {
                "value": "I cannot think of any salient weakness in this work. *ColaBO* relies on several approximations due to non-Gaussianity of the posterior and these can probably be made more efficient, as mentioned in the limitations."
            },
            "questions": {
                "value": "I do not have questions.\n\nTypos or similar:\n\n- Can you clarify what \"DoE\" means in \"[...] we consider a smaller optimization budget of $10D$ iterations, and initialize all methods that utilize user beliefs with only one DoE sample, that being the mode of the prior\".\n- Figure 6: The y-axis gives \"Accuracy\" but performances are decreasing over the BO trial. It probably should be something like 1-accuracy?\n- A.3: \"by using a sampling an offset direction $\\boldsymbol{\\epsilon}$ - > \"by sampling an offset direction $\\boldsymbol{\\epsilon}$\"?\n- Concusion: \"[...] or pre-trainedp\" -> pre-trained"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3781/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3781/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3781/Reviewer_TtXD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3781/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591988464,
        "cdate": 1698591988464,
        "tmdate": 1699636334784,
        "mdate": 1699636334784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iNnosw6lrq",
        "forum": "NjU0jtXcYn",
        "replyto": "NjU0jtXcYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_ysEZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_ysEZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach to user (or prior) guided Bayesian optimization. Unlike previous approaches where the acquisition function was modified to incorporate priors, the paper proposes to sample from a modified posterior of the Gaussian process. This is achieved by combining rejection sampling with the recent line of work on path-wise conditioning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed technique is original and interesting. It certainly appears more principled than previous approaches to incorporating expert knowledge."
            },
            "weaknesses": {
                "value": "* The mathematical derivation is too informal in some places, affecting clarity. I believe this needs to be improved. For instance, $\\pi$ here represents a belief over functions. Then why is Eq. (4) a function that receives a point $x \\in \\mathcal{X}$? Shouldn't it be a probability distribution over functions? Also, Def 3.1 introduces a conditioning on $\\pi$. I found this quite confusing. Is $\\pi$ a density? a function? or a random variable?\n* There are concerns about the fairness of the experiments. The authors state that \"ColaBO and \u03c0BO are initialized with the mode of the prior followed by 2 Sobol samples, whereas LogEI and MES are conventionally initialized with D + 1 Sobol samples.\" Shouldn't logEI and MES also have been initialized on the mode of the prior? If we assume that such prior information is present in user-guided BO methods, it is fair to assume that conventional BO methods also have access to this information. \n* The evaluations are okay but not extensive (especially compared to the piBO paper). I think this is an important point, given that the utility of user-guided BO methods can only be judged empirically on a case-study basis. This connects with my next concern.\n* The paper lists a couple of different types of function priors in Section 3.1 but only seems to evaluate one type of such prior. It is, therefore, hard to judge how general this framework is for incorporating prior knowledge. Especially since the users state in the contributions that one can \"incorporate arbitrary user knowledge.\" \n\n### Major Comments\n* The contribution statement is too general except for item 2. For instance, 1 and 3 can be applied to any user-guided BO method. I expect some more technical details about what this method offers to the field of user-guided BO.\n* The derivation of importance sampling in Eq. (10) is unclear. $\\pi(f) p(f | D)$ is an unnormalized density, but the integral is taken over it. Then, the equality with Eq. (9), doesn't hold since the expectation is not normalized.\n* In Section 3.3, the authors introduce a tempering scheme based on the number of datapoints. Furthermore, they draw connections with generalized Bayesian inference (GBI). I think the authors should make it clear that the connection is very loose. A major characteristic of GBI is to find a statistically principled way to come up with the temperature, which is not the case here. It is particularly misleading as, in the last sentence of Section 3.3, the paper states that tempering is done so in a \"principled Bayesian manner.\" \n\n### Minor Comments\n* Couldn't the name collaborative Bayesian optimization be misleading as to making people to think this method involves multiple users?\n* Section 1 first sentence: Please cite classic papers that introduced BO for historical context.\n* Figure 9-12: The boundary of the error bands makes it hard to distinguish between the plots. Please consider improving the visibility here.\n* Section 2.1 equation: Generally, we aim to minimize the expectation of f or the regret. Saying that we minimize f, in the presence of noise, is too informal.\n* Section 2.2 second from the last sentence: I think replacing \"standard method\" with \"classic method\" would be better here since we have an influx of more efficient methods.\n* Eq (2): Has the \"equivalent in distribution\" sign been defined anywhere?\n* Section 3.1 second paragraph second sentence: the sentence is incomplete.\n* References: please consider making the reference more consistent and check the entries. Carl et al 2022b has a typo: joint entropy eearch -> joint entropy search; Jones et al. 2018 is missing the journal entry; Kingma and Welling 2013 was published in ICLR\u201914."
            },
            "questions": {
                "value": "* Given that the authors rely on GP posterior sampling, an obvious thing that could have been tried is Thompson sampling. Have the authors tried to include it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3781/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3781/Reviewer_ysEZ",
                    "ICLR.cc/2024/Conference/Submission3781/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3781/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608901166,
        "cdate": 1698608901166,
        "tmdate": 1700690621179,
        "mdate": 1700690621179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pSv5QxXVI6",
        "forum": "NjU0jtXcYn",
        "replyto": "NjU0jtXcYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_ZcGu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_ZcGu"
        ],
        "content": {
            "summary": {
                "value": "The paper considers a setting where BO is applied to tackle black-box optimization problems where good prior knowledge is available. However, the conventional GP prior might fall short in effectively incorporating this knowledge. Therefore, the authors propose a new approach to inject it, mainly based on reweighing the prior of the GP with the user-defined prior and deterministic update of the GP posterior. Empirically, two instances of the framework is tested in synthetic and hyperparameter tuning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper considers a novel black-box optimization setting where good prior knowledge exists and proposes a framework to handle this problem.\n2.\tBased on sampling, this framework is compatible with all Monte Carlo acquisition functions.\n3.\tWith acquisitions to be Log Expected Improvement and Max-Value Entropy Search, the proposed models work well in synthetic and hyperparameter tuning tasks when well-located prior to the optimal location is available, while the drop in performance is not obvious compared to the benchmark models."
            },
            "weaknesses": {
                "value": "1.\tAlthough the empirical performance of ColaBO looks promising in the synthetic task and hyperparameter tuning task, the theory developed in the work is limited. Therefore, how the method performs statistically is a concern, given that there are many approximations, such as the Monte Carlo acquisition and the RFF sampling.\n2.\tThe empirical section can be further enhanced by testing the algorithms on more challenging tasks, for example, higher-dimensional problems. The paper primarily focuses on the low-dimensional tasks, which might not provide a strong basis for demonstrating the algorithm\u2019s performance in realistic problems, where the dimensions might be much higher.\n3.\tThe presentation of the work can be improved. Initially, the authors define $\\pi(\\cdot)$ in Eq. 3 with input from $\\mathcal{X}$, but it later becomes $\\pi(f)$. Moreover, the notations become messier after Eq. 4, making it more challenging to follow. Also, while rejection sampling seems to be an important step in the model, the way it works is not introduced in the paper. Similar for RFF.\n4.\tGiven the authors\u2019 claim that this is a general framework for BO to incorporate prior, the paper should present concrete examples to demonstrate its generality. For example, the authors could show how the examples in the second paragraph of the introduction section can be solved effectively within the proposed framework."
            },
            "questions": {
                "value": "1.\tWhat is the difference between $f^*$ and $f_*$? What is $x_*$?\n2.\tIs there any reason the authors in favor of rejection sampling compared to other sampling methods? What is the efficiency of using rejection sampling here?\n3.\tHow $\\beta$ is determined? \n4.\tIn Figure 5, even though ColaBO-MES is injected with a poorly located priors, it still performs better than MES, how do the authors interpret that?\n5.\tThe authors show in Eq. 5 that $p(f|D, \\pi) \\propto \\pi(f)p(f|D) $. Does this proportionality still hold in Eq. 6? \n6.\tTo indicate where the optimum is located within the prior, couldn\u2019t we achieve it by simply defining the prior mean function in GP properly? For instance, assigning higher values to points where the users believe to be good and low values to the rest.\n7.\tWhy do the authors consider log EI instead of EI?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3781/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697806388,
        "cdate": 1698697806388,
        "tmdate": 1699636334562,
        "mdate": 1699636334562,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XGFEZrirR3",
        "forum": "NjU0jtXcYn",
        "replyto": "NjU0jtXcYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_tGCg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3781/Reviewer_tGCg"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces ColaBO, which allows domain experts to customize the BO optimization process by integrating prior beliefs, such as information about the probable location of the optimum or the optimal value. Through empirical experiments, it shows that ColaBO speeds up optimization when prior information is accurate and maintains reasonable performance even when the prior knowledge is misleading."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The framework's adaptability and flexibility to incorporate prior knowledge into the optimization process. The method maintains reasonable performance even when the prior knowledge is misleading, demonstrating its robustness in different scenarios."
            },
            "weaknesses": {
                "value": "Test functions used to evaluate the proposed framework were quite limited, only a restricted set of test functions was employed."
            },
            "questions": {
                "value": "Besides the likely location of the optimizer or the optimal value, what other forms of prior knowledge would generally be useful?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3781/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763770960,
        "cdate": 1698763770960,
        "tmdate": 1699636334476,
        "mdate": 1699636334476,
        "license": "CC BY 4.0",
        "version": 2
    }
]