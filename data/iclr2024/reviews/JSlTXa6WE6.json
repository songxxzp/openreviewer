[
    {
        "id": "diEYGxvQXa",
        "forum": "JSlTXa6WE6",
        "replyto": "JSlTXa6WE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_5KvE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_5KvE"
        ],
        "content": {
            "summary": {
                "value": "Propose certification for PINN, i.e., guarantees on the worst-case residual error of a PINN, which can be used for prediction of PINN's performance for trustworthy PINN."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-motivated as the theoretical guarantee is always a huge concern for PINN whose theory is not transparent, making it less practical than traditional finite element methods.\n\nThe theory is clearly proved together with supporting experiments."
            },
            "weaknesses": {
                "value": "The authors only provide theory for 1st and 2nd order derivates in PDEs. However, high-order PDEs are common, such as KdV, Kuramoto-Sivashinsky, and Boussinesq-Burger PDEs.\n\nI don't think the extension to general high-order PDEs is simple, although I noticed the author thought it was trivial, because the expression of high-order derivative is extremely complex, which may affect the tightness of the certification. How does the PDE order affect the tightness of your certification?\n\nIs this framework still valid when PINN encounters propagation failure? See https://arxiv.org/abs/2203.07404 Figure 2. The losses are very small but the error is huge.\n\nTo be more specific, there exists a case where PINN converges to a low residual loss but with a huge relative test l2 error, due to the propagation failure, or we can regard it as PINN converges to some spurious local minima corresponding to the trivial solution. In that case, the PINN model will be smooth and low loss. How can your theory deal with this case?\n\nBesides, although the author shows by a figure that the residual loss correlates with the test l2 relative error well, I would like to emphasize that residual loss only provides a loose upper bound for the test l2 relative error. In other words, low residual loss cannot guarantee low test error.\n\nThus, I suggest the author test their theoretical framework in some tough cases where PINN encounters propagation failure."
            },
            "questions": {
                "value": "Can this theoretical framework be extended to high-order nonlinear PDEs? High-order PDEs are common, such as KdV, Kuramoto-Sivashinsky, and Boussinesq-Burger PDEs. How does PDE order affect the tightness of your certification?\n\nIs this framework still valid when PINN encounters propagation failure? See https://arxiv.org/abs/2203.07404 Figure 2. The losses are very small but the error is huge."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698409032145,
        "cdate": 1698409032145,
        "tmdate": 1699636606187,
        "mdate": 1699636606187,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JAJT3ZodmG",
        "forum": "JSlTXa6WE6",
        "replyto": "JSlTXa6WE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a definition of global correctness conditions for PINNs that produces the solutions of PINNs. The authors propose CROWN based certification framework to bound first and second derivatives $\\mu_\\theta$ as well as the $f_\\theta$. \n\nWhile CROWN provides a theoretical bound, given the approximations used throughout the bounding process, such bound is still less tight to bound the actual $h$. The authors utilize a similar idea to BaB, and apply greedy input branching algorithm to the bounding process.\n\nThe authors have provided experiments to verify that (1) $\\partial$- CROWN are more tight compared to empirical errors computed with a large number of samples (2) residual-based certificates and the commonly reported solution errors are highly correlated. They have also provided an ablation study to check the importance of the branching algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper provides a good contribution to the research community. The authors are building the correctness conditions for PINNs over the input domain, which can improve the robustness and trustworthiness of PINN when applying it to the scientific domains.\n\n(2) While CROWN is not new, the authors make novel contributions to apply CROWN to solving PINN's correctness conditions. \n\n(3) In Section 5, the authors have provided detailed theoretical foundations for how to bound the correctness conditions. The proofs in appendix are long but easy to read.  The authors have also provided a theoretical analysis of the running time for $\\partial$-CROWM algorithm.\n\n(4) The authors have provided an improved version with greedy input branching, similar to the idea of original branch-and-bound (BaB) process."
            },
            "weaknesses": {
                "value": "(1) The major concern that reviewer has is that the certification procedure is the on PINN residual error, $f$, but not on the estimation to the true PDE solution. While the authors have provided Exp 2 and Appendix C to demonstrate that there is a strong relationship between the residual error and the PDE solution, it is only conducted on PDEs with a clear PDE solution. More experiments should be conducted with various PDE equations (with multiple solutions, with trivial solutions, etc.) to find out how bounding $f$ is effective. Especially for the recent papers that discuss the failure modes of PINN[1], how would bounding the residual errors help, both theoretically and empirically?\n\n(2) Figure 2 looks meaningless. The Figure 2(a)-2(d) only shows the visualization of the time evolution of $u$, versus the residual\nerrors for three different PDE equations. However, it is hard to observe any clear patterns of the residual errors, and the authors only briefly mention the residual errors without explaining any scientific insights from the Figure. The reviewer thinks it is better to put the Figure into Appendix, and instead briefly mention Appendix A and B in the main text.\n\n(3) Table 1 shows that  $\\partial$-CROWN approaches the empirical bounds obtained using Monte Carlo sampling while providing the guarantee that no point within the domain breaks those bounds. The reviewer think it will be nice to document some time it takes for MC sampling versus the $\\partial$-CROWN (at least for residual error), to demonstrate its efficiency.\n\n(4) Figure 1 is occupying too much spaces while the words on the upper right corner are hard to read. Maybe the authors should consider providing a more concise illustration that occupies less space. \n\n(5) In Section 6.3, the authors should consider also including the ablation study on $N_b$ into the main text. \n\n(6) In Section 5.3, the authors mention that \"exploring the areas... via sampling (SAMPLE, line 3)\", is there a typo and should be line 5 instead of line 3? \n\n\n[1]: Krishnapriyan, Aditi S., Amir Gholami, Shandian Zhe, Robert M. Kirby, and Michael W. Mahoney. 2021. \"Characterizing possible failure modes in physics-informed neural networks.\" arXiv preprint arXiv:2109.01050."
            },
            "questions": {
                "value": "(1) In the appendix A, the authors have mentioned to use Physics-informed Adversarial Training to reduce the errors of PINN and demonstrate that $\\partial$- CROWN can also bound the residual errors of PIAT. However, there are been various ways of reduce the errors of PINN, for example, scheduled training [1] or building a more complex architecture with transformer [2]. The reviewer is wondering if the $\\partial$-CROWN can adapt to NN training that involves more complex loss functions, or more complex architectures?\n\n(2) Similar to Question 1 in Weakness, how could $\\partial$-CROWN be used in solving the failures of PINN? If the residual error is large (or being stagnant for a long time), does that imply the PINN is failing to provide a good solution?\n\n(3) Have the authors study the relation between $\\partial$-CROWN's time complexity to the size of the PINN's neural network?\n\n(4) How does $\\partial$-CROWn behave when the amount of training data is low? Intuitively, the reviewer believes that the amount of training data needs to be evenly spreading across the domain to allow greedy input branching algorithm to work. But what happens when the amount of training data is not evenly spreading?\n\n \n[1]: Krishnapriyan, Aditi S., Amir Gholami, Shandian Zhe, Robert M. Kirby, and Michael W. Mahoney. 2021. \"Characterizing possible failure modes in physics-informed neural networks.\" arXiv preprint arXiv:2109.01050.\n\n[2]: Zhao, Zhiyuan, Xueying Ding, and B. Aditya Prakash. 2023. \"PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks.\" arXiv preprint arXiv:2307.11833."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5770/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5770/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681652904,
        "cdate": 1698681652904,
        "tmdate": 1699636606078,
        "mdate": 1699636606078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZYo5v4B09h",
        "forum": "JSlTXa6WE6",
        "replyto": "JSlTXa6WE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to establish correctness certification for PINN models via the proposed $\\partial-$CROWN post-training framework. Under several assumptions, the authors prove two theorems showing that the upper and lower bound of the solution can be computed in $\\mathcal O(L)$ time. The authors experimentally demonstrate the tightness of their bound via a few small PDE examples, showing that they can achieve excellent approximation (compared with MC samples) much more efficiently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Undoubtedly, bounding the error of ML solutions *globally*, as the authors try to tackle, is important and necessary for ML methods to be practically applicable. I am happy to see theoretical and empirical research on this avenue.\n- The improved Greedy Input Branching intuitively makes sense to me, although its direction comparison with the non-optimized algorithm seems to be lacking.\n- This paper is well-written, and the organization and flow are clear. I also pretty much appreciate the combination of theoretical proof and empirical demonstration."
            },
            "weaknesses": {
                "value": "While I like this paper, there are several non-negligible problems that have to be addressed before the paper reaches the bar of acceptance.\n\n- First of all, I am not sure why the authors naturally assume that the ground truth can be represented by a FNN. For instance, even a single electron wave function decays exponentially, which an FNN cannot represent. Since the goal here is to bound the global error, I do not think the gap between the best FNN versus the actual ground truth can be ignored. Similarly, assumption 1 limits the applicability of your theorems - what happens if I add some exponential layers, or what if I use other architecture?\n- Your theorems only prove the existence of bounds, while your algorithm is still greedy. If I understand correctly, this actually means that: the approximation error (say the numbers in Table 1) is not *guaranteed* to be an upper bound of the global error, right? If so, despite the acceleration you achieve, how can you demonstrate that in a large system, your error approximation is *accurate*?\n- I do think the paper should include more complex tasks, as PINN is known to be less accurate in large systems (or long simulation time intervals). Current experiments, while sufficient as a proof of concept, are insufficient to demonstrate the proposed algorithm can indeed provide a better approximation. For instance, you can try multi-body Schrodinger equations, like [1], or HJB equations [2]. These are of large scales and non-linear, making it challenging (IMO) to your method.\n\n[1] Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo\n\n[2]  Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?"
            },
            "questions": {
                "value": "- From my understanding, speed is one of your strengths. Can you more comprehensively compare with previous methods to demonstrate how fast your method is? A table or figure will be ideal.\n- I think the paper could be benefited by including more takeaways, especially for the theorems. I can sort of imagine how this theorem is proved, but it would be great if the authors could give a proof sketch and discuss how each assumption and condition works.\n- notation layer $k$: try to use $l$ since the total layer number is $L$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5770/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ",
                    "ICLR.cc/2024/Conference/Submission5770/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726439758,
        "cdate": 1698726439758,
        "tmdate": 1700623265412,
        "mdate": 1700623265412,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LTSydZDE33",
        "forum": "JSlTXa6WE6",
        "replyto": "JSlTXa6WE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_3Mdo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5770/Reviewer_3Mdo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to solve the question of \u201cefficient certification\u201d of PINNs which \u2013 as the authors state \u2013 is the question of trying to quickly estimate bounds on the supremum (over domain points) error made by the neural surrogate for the residuals. The authors give experiments to suggest that their methods give upperbounds almost as good as the estimations made via random sampling of domain points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The claims that have been proven in this paper do indeed seem a bit surprising \u2013 that upto second order of derivatives for the neural net, these functions can be linearly bounded bothways."
            },
            "weaknesses": {
                "value": "Prima facie the paper is very badly written \u2013 to the point of being very much devoid of motivations. Till one gets to the experiments, it's entirely unclear what is the \u201ccertification\u201d question that is even being solved. In fact even after reading everything in the main paper, it's not clear at all as to why the random point sampling way is not enough to implement a check of the inequalities as required in Definition 1. What exactly is being gained by this method (as demonstrated in the third column of Table 1) as opposed to the results in the first two columns?  \n\nEven the run-time reported to obtain some of these numbers in the third column of Table 1 seems to be in millions of seconds. \nThat is more than a day! So this method isn't hinged on an argument of speed either.  \n\nSecondly, I am not able to ascertain from the given descriptions if the presence of these linear two-way bounds is a special case for the first two derivatives of a neural net or does this happen for all derivatives. Is the method $\\partial-$CROWN hinged on the validity of these linear bounds? In that case, why is the method of interest to the general PINN formalism where arbitrarily high orders of derivatives can potentially occur.  \n\nThirdly, the theorems are very badly stated. It's not clear from their statements as to whether or not there is an explicit expression for these linear bounds and if yes (which it seems to be from the appendix!) then what information is required to be able to compute them.  \n\nFourthly, the choice of the PDE examples for testing are very skewed. If one is doing experiments on multiple PDEs then it would have been better to include PDEs that have more than 2 variables. It does not help to have all the examples be stuck to two variable PDEs!  \n\nAlso, the Schrodinger PDE example looks a bit strange. Which natural Hamiltonian function even leads to this? This seems to be coming from a system with a potential energy which is \u201c$-|u|^2$\u201d - and that is never a natural setting \u2013 to the best of my knowledge of quantum theory!"
            },
            "questions": {
                "value": "It would be great if the authors can give a precise answer to the first two weaknesses pointed out above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782955930,
        "cdate": 1698782955930,
        "tmdate": 1699636605838,
        "mdate": 1699636605838,
        "license": "CC BY 4.0",
        "version": 2
    }
]