[
    {
        "id": "455z5ZAsNK",
        "forum": "V01FPV3SNY",
        "replyto": "V01FPV3SNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_Lpzg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_Lpzg"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on defending current LLMs against adversarial attacks (i.e., jailbreak attacks). The authors propose a method that requires multiple times of model inference. \n\nThe first inference is the conventional inference that takes the original prompt (e.g., harmful instruction and jailbreak prompt) as the input and collects the output. Subsequently, this paper randomly drops the words in the original prompt, inferences to get the output, and detect the harmfulness of this output. Through multiple times of such procedure, the harmfulness of this original prompt is determined by collecting these detection results.\n\nThis paper shows experimental results on one dataset and two models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic of this paper is important in the field of LLM.\n2. The proposed method is intuitively reasonable that can defends adversarial attacks to an extent (e.g., the GCG attack)."
            },
            "weaknesses": {
                "value": "1. **Lack of baseline comparisons.** This paper did not compare with a highly related baseline, that is detecting harmfulness based on the model output [1]. This baseline requires roughly $L_{in} + (L_{in}+L_{out})$ input cost and $L_{out}$ output cost, where the overall cost could be much smaller than this paper's method (if the $L_{out}$ is not too large). Besides, this baseline has a simple variation, where we can instruct the LLM to revise the output of first stage, which could also potentially improve the helpfulness and reduce harmfulness.\n2. The experiments are not comprehensive. There are only two small tables. Only two relatively small models, one dataset, open-source models are considered. Since such method is more appropriate for proprietary models, experiments on proprietary models are needed.\n3. The claim of \"such alignment checking is not robust\" (page 4, Robust Alignment Check Function) is not well-supported. What is the relationship between adversarial prompts [2] and such claim? In think this point is critical. If the authors cannot fully clarify the drawbacks of existing alignment checking methods, the motivation of this paper will seem to be weak.\n4. The authors approximate $AC(\\cdot)$ by only inspecting the existence of prefix in a pre-collected prefix set (e.g., \u201cI can not\u201d, \u201cI\u2019m\nsorry\u201d). However, is the approximation robust? It is unclear. Such prefixes may vary across different models, for example, some models may output \"as a helpful and harmless chatbot, my job is to ....\". Since there are so many potential prefixes, I do not think enumerating to construct a prefix set is a robust solution.\n5. **Computational Cost**. The current calculation manner of computational cost is not convincing: authors seem to compare the per-token cost, however, this method requires much larger token length. Through a rough calculation, this method requires $n*(1-p)=20*(1-0.3)=14$ times of input cost, which has not been revealed by the authors.\n\n[1] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau. Llm self defense: By self\nexamination, llms know they are being tricked. arXiv preprint arXiv:2308.07308, 2023.\n\n[2] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial\nattacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698047921770,
        "cdate": 1698047921770,
        "tmdate": 1699636788517,
        "mdate": 1699636788517,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FIxSUanwUE",
        "forum": "V01FPV3SNY",
        "replyto": "V01FPV3SNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to defend alignment-breaking attack by perturbing the input prompt to see whether the request is rejected by an aligned LLM, which is interesting. Experiments on both attack dataset and QA datasets verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Defending the alignment-breaking attack for LLMs is a very important research direction to protect LLMs from being misused.\n\n2. The proposed method seems to be quite effective according to the reported experimental results.\n\n3. The proposed method is very easy to implement."
            },
            "weaknesses": {
                "value": "1. I wonder whether it is enough to have only one dataset for ASR and BAR evaluation.\n\n2. The size of the experimental dataset seems to be small.\n\n3. This paper does not consider the adaptive attack scenario."
            },
            "questions": {
                "value": "I wonder whether the proposed method can make some false positive errors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Reviewer_Nds5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840187529,
        "cdate": 1698840187529,
        "tmdate": 1699636788388,
        "mdate": 1699636788388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IcBdfRdtX9",
        "forum": "V01FPV3SNY",
        "replyto": "V01FPV3SNY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
        ],
        "content": {
            "summary": {
                "value": "The authors present a Robustly Aligned LLM (RA-LLM) as a countermeasure to jailbreaking attacks. The primary methodology involves randomly removing tokens from the prompt and assessing the failure rate under aligned LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The underlying principle of RA-LLM is evident: the strategic removal of tokens from the prompt has the potential to neutralize the adversarial prefix, thereby mitigating the effectiveness of the attack.\n\n2. The introduced methodology demonstrates substantial robustness when tested on Vicuna-7B and Guanaco-7B."
            },
            "weaknesses": {
                "value": "1. The concept of partially erasing the prompt as a defensive measure against jailbreak attacks has been previously explored, as evidenced by concurrent work [1]. It would be beneficial if the authors delved deeper into this method to enhance its defensive capabilities. Furthermore, it might be worth comparing the RA-LLM's performance with the perplexity-based defense [2], which has also demonstrated commendable robustness.\n\n2. The experimental evaluations appear to be limited to open-source LLMs. Is it feasible for the RA-LLM to be effective on GPT3.5/4? Comprehensive experimental results on GPT3.5/4 would enhance the study's credibility.\n\n3. In assessing computational costs, the authors have focused on financial implications rather than time expenses. The reviewer posits that time cost is of paramount importance, as it directly relates to the model's efficiency.\n\n\n[1] Aounon et al. Certifying llm safety against adversarial prompting.\n[2] Jain et al. Baseline defenses for adversarial attacks against aligned language models"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6818/Reviewer_8tCQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6818/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699198900203,
        "cdate": 1699198900203,
        "tmdate": 1699714715835,
        "mdate": 1699714715835,
        "license": "CC BY 4.0",
        "version": 2
    }
]