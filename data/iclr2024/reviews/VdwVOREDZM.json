[
    {
        "id": "4MmCUhibVi",
        "forum": "VdwVOREDZM",
        "replyto": "VdwVOREDZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_CqTg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_CqTg"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method that learns 3D generation from in-the-wild images, e.g., Imagenet. Specifically, this paper proposes to first learn a 3D-aware VAE that compresses images to latent space and decodes latent into 3D representations, i.e., triplane. To facilitate 3D learning from 2D images, especially unposed in-the-wild images, authors proposes to use RGB and depth discriminator for rendered novel views. After learning the 3D-aware latent features, authors uses a standard diffusion model to sample from the latent space. Experiments are performed on Imagenet, as well as three unimodal datasets. Some advantages over GAN-based methods are shown. Ablation study is also performed to show the effectiveness of different model parts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A new paradigm is used on the 3D-aware generation on in-the-wild images. The 3D learning happens in the first stage VAE training, where novel view RGB and depth discriminator is used. The second stage latent diffusion model samples from the trained latent space.\n2. Advantages of the new paradigm over the GAN-based methods, e.g., 3DGP and EG3D, are shown by quantitative evaluations."
            },
            "weaknesses": {
                "value": "1. For the first stage of 3D-aware VAE, there are previous methods that propose very similar method. For example, VQ3D and GINA-3D also encodes input images to 3D latents and decode them to the 3D representation of triplane. I would expect authors to give a more detailed explanation of how the proposed method differs from these methods.\n2. To continue on the above comment, I would also like to see the performance comparison with VQ3D on both VAE and generator, since the final generation performance largely depends on the performance of the first stage VAE. Therefore, it is important to understand if the performance improvement is from the first or the second stage.\n3. I would recommend using 50k, instead of 20k, samples to calculate FID, since it is the standard adopted by most of previous methods, e.g., StyleGAN, VQ3D. It would also allow easier comparison with previous or future work.\n4. The visual results shown in Fig. 9 is not very convincing. Flat structure can also be observed from the first two rows.\n5. Visual comparison with other methods is not enough. Firstly, I would like to see this comparison in the main paper instead of supplementary material, since it is a very important part for evaluation and can serve as reference when readers try to understand the advantage of the proposed method. Secondly, I would like to see more visual results for mode collapse of GAN-based methods, as mentioned in the third paragraph of section 4.2."
            },
            "questions": {
                "value": "IVID has shown the ability to generate 360 degree novel view synthesis. I understand that IVID uses a very different approach from this paper. But I would like to hear authors' opinion on learning 360 degree 3D generation with the current pipeline. From my point of view, there is no technical limitation, since you can freely change the novel view camera distribution sampling when training the VAE."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556558375,
        "cdate": 1698556558375,
        "tmdate": 1699636443924,
        "mdate": 1699636443924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2hNDGEuBQs",
        "forum": "VdwVOREDZM",
        "replyto": "VdwVOREDZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_D5rM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_D5rM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel approach to learning a 3D generative model from unposed images in view space.\n\nThe core idea of the paper is to first pre-train an auto-encoder that takes an image and its estimated depth map as input, and encodes it into a triplane NeRF. That triplane is then used to re-render the input image and depth as well an one (or several) additional views. The input image and depth are supervised straightforwardly, while the additional view and depth is supervised via an adversarial loss.\n\nSubsequently, the authors propose to train a latent diffusion model on the recovered latent space, enabling unconditional and conditional generative modeling."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Exposure is excellent, the method is exceedingly clear. The overview figure is great.\n- The paper is well-motivated and the shortcomings of prior work are clearly highlighted.\n- Design choices are clear.\n- Baselines are appropriate.\n- Ablations are detailed and insightful."
            },
            "weaknesses": {
                "value": "My core complaint with this paper is that I am not quite sure why you would use this method over a simple depth-warping plus inpainting baseline.\n\nThe generated images are of somewhat low quality - they are certainly far behind anything that can be generated with any SOTA 2D generative model.\n\nFor any generated image, I could always use the same monocular depth predictor used in this paper to estimate depth, and then warp the image to a novel view. The only challenge would then be holes - which, however, one could easily inpaint, as has been demonstrated in \"SceneScape\"  (https://arxiv.org/abs/2302.01133).\n\nI would really like to see the following simple baseline:\n1. Generate an image with a 2D image generative model.\n2. Predict monocular depth with the same model you are currently using.\n3. Warp the image to a novel view using the predicted depth.\n4. Use an inpainting method such as the one used in \"SceneScape\".\n\nThe only shortcoming here is that this requires warping and in-painting at test time, to render novel views. However, one could easily merge several such in-painted views into a single mesh, which could then be rendered from novel views, similar to what SceneScape does. \n\nEven so, I *do* believe that this paper adds significantly to the literature by clearly formulating the problem, showing how prior methods fail, and producing a method that significantly outperforms prior methods. I think this will spur follow-up work."
            },
            "questions": {
                "value": "I am overall OK with accepting this paper, as I believe that it is well-written, poses an important problem, and puts forth a reasonable baseline approach.\n\nI would be happy to increase my score if the authors could provide the baseline requested above.\n\n\n___\n\n\nI thank the authors for addressing my concerns. I think this additional baseline comparison adds to the paper! I increased my score to 8 and will happily argue for acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4639/Reviewer_D5rM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627177747,
        "cdate": 1698627177747,
        "tmdate": 1700874671458,
        "mdate": 1700874671458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ICOEFauqBv",
        "forum": "VdwVOREDZM",
        "replyto": "VdwVOREDZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_HTTZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_HTTZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method address the challenge of 3D-aware image synthesis for in-the-wild images. A key difference to previous works is that it developed upon a latent diffusion model, which demonstrates more sample diversity to GAN. To enable in-the-wild generation, unlike existing methods that rely on a shared canonical space, the authors propose to model instances in view space. To enable consistent 3D representation, the diffusion model predicts an implicit 3D representation, the triplane representation. The training also leverages monocular depth estimation to further boost 3D accuracy. The experiments show that the proposed work outperforms prior art by a large margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors leverages latent diffusion model to address the lack of sample diversity in 3D-aware GAN.\n- They propose to represent 3D-aware image by an efficient triplane representation.\n- The training loss avoids the necessity of multi-view images of the same instance, which makes it easier to train on a much larger amount of data. \n- An extensive ablation study to support design choices."
            },
            "weaknesses": {
                "value": "- 1. This paper only compare with GAN-based methods. It would be more convincing if a comparison to recent diffusion-based methods (GenVS, IVID, VQ3D) is presented."
            },
            "questions": {
                "value": "See weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815063808,
        "cdate": 1698815063808,
        "tmdate": 1699636443739,
        "mdate": 1699636443739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1aXGjhx17c",
        "forum": "VdwVOREDZM",
        "replyto": "VdwVOREDZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_US3i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4639/Reviewer_US3i"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a two-stage model for learning 3D-aware latent diffusion model in image view space. In the first stage, the authors learn a trasformation: (image, depth) -> latent -> triplane -> NVS images, by applying a GAN loss. Then in the second stage, the authors train a latent diffusion model, which can be directly decoded into a triplane plane NeRF. The learning of the pipeline does not require multi-view data and has shown better results than previous approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is nicely presented. Charts and tables are nicely made and I found the paper easy to read through.\n\n- The two-stage training is interesting. Each step of the pipeline looks reasonable to me.\n\n- The training of the method does not require 3D or multi-view image data."
            },
            "weaknesses": {
                "value": "- An important work is missing in discussion/comparison. \"VQ3D: Learning a 3D-Aware Generative Model on ImageNet\", ICCV 2023. The two works are very similar and both works adopt a two-stage learning scheme. The major difference is that VQ3D applies a GAN-based method for both stages.   \n\n- I am not sensitive to the quantitative number in the main paper but I saw many NVS results in the supplementary video are distorted. Also, I did not observe a significant visual improvement over the EG3D. I would resort to opinions from other reviewers.\n\n- As the second stage is trained on a latent space obtained by the first stage training, I am concerned that the diffusion generation quality (geometry correctness and image fidelity) is bounded by the GAN-based training. So what is the benefit of introducing the second stage? Easy sampling?\n\n- Strictly speaking, the training involves a large amount of 3D data, which comes from the pre-trained single view depth estimator."
            },
            "questions": {
                "value": "see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821257623,
        "cdate": 1698821257623,
        "tmdate": 1699636443652,
        "mdate": 1699636443652,
        "license": "CC BY 4.0",
        "version": 2
    }
]