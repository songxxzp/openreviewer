[
    {
        "id": "SprjJU1VCB",
        "forum": "C42FkKhAUC",
        "replyto": "C42FkKhAUC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_Sshb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_Sshb"
        ],
        "content": {
            "summary": {
                "value": "The paper focused on the sample importance in adversarial training (AT). The authors argue that adversarial examples (AEs) generated from inherently robust natural samples might not induce a sufficiently high loss to achieve optimal robustness, and thus crafting AEs with a fixed perturbation budget may limit the effectiveness of AT. They used the proximity of the natural samples to the class boundaries to distinguish important samples and proposed a reweighting function for assigning different perturbation budgets to individual AEs during AT. Experiments show the superiority over baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The whole logic of this paper is easy to follow. The objective function and the realization of the proposed method are clear.\n2. The authors proposed a realized measurement to compute the distance to the decision boundary, which is simple and intuitive.\n3. Experimental results show that, compared to the baseline approaches, this method can achieve a notable improvement in robustness and accuracy simultaneously on CIFAR-10 and Tiny Imagenet."
            },
            "weaknesses": {
                "value": "1. The paper has limited novelty at the philosophy level. [1] and [2] pointed out that from a geometric perspective, a natural data point closer/farther from the class boundary is less/more robust, and the weight of the adversarial data should be assigned accordingly. The proposed MWPB shares the same philosophy. The only difference is that this paper focuses on weighted perturbation budget while [1] and [2] focus on weighted robust loss. Besides, [3] and [4] have already proposed instance-wise AT methods of using weighted perturbation budgets 4 years ago. For now, the idea of using instance-wise weighted perturbation budgets in AT is no longer novel. At least, the authors should justify the necessity of using MWPB and explain why MWPB is better than the above-mentioned methods.\n2. The experiments are insufficient. Since the main idea of this paper is to justify the necessity of using adaptive $\\epsilon_i$ in AT, the authors should at least try different $\\epsilon$ in the setting of $\\ell_\\infty$-norm (e.g., 6/255, 10/255, 12/255) to see if MWPB can still work well. The authors should at least examine the range of $\\epsilon$ within which MWPB can outperform other methods. Furthermore, the authors should discuss why this paper only considers $\\ell_\\infty$-norm, instead of $\\ell_2$-norm. I would suggest the authors to try $\\epsilon$ in the setting of $\\ell_2$-norm (e.g., 128/255) as well.\n3. Besides, the authors should test the robustness of the proposed method under adaptive attacks [5]. Specifically, adaptive attacks assume the attackers know all the details of MWPB, including the proposed reweighting function. Since MWPB decreases the perturbation radii for vulnerable natural data, I wonder if the generated AEs from these vulnerable natural data are sufficiently robust. Therefore, I would suggest proposing an adaptive attack that assigns a larger perturbation radii to vulnerable natural data than robust ones during the generation of AEs at the test time. Then, see if MWPB can successfully defend against such attack and still outperform other methods.\n4. The paper has several typos and formatting inconsistencies. For example, in Section 4.4, it should be \u2018Large perturbation size \u2026 find sharper minima\u2019 instead of \u2018shaper minima\u2019, and \u2018Equation 4\u2019 should be \u2018Eq. 4\u2019 to ensure the consistency. In Table 4, \u201827.15\u2019 should be in **bold** instead of *italic*. Besides, Table 5 and Table 7 exceed the prescribed margins. I recommend a thorough review and correction of these errors to ensure the professionalism of the paper.\n\n[1] Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In *International Conference on Learning Representations*, 2020.\n\n[2] Huimin Zeng, Chen Zhu, Tom Goldstein, and Furong Huang. Are adversarial examples created equal? a learnable weighted minimax risk for robustness under non-uniform attacks. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 35, pp. 10815\u201310823, 2021.\n\n[3] Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct input space margin maximization through adversarial training. In *International Conference on Learning Representations*, 2019.\n\n[4] Yogesh Balaji, Tom Goldstein, and Judy Hoffman. Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets. *arXiv preprint arXiv:1910.08051*, 2019.\n\n[5] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In *International Conference on Machine Learning*, 2018."
            },
            "questions": {
                "value": "1. Theorem 1 points out inherently robust data contribute less to the inner maximization compared to inherently vulnerable data. Theorem 2 shows that increasing $\\epsilon$ will increase the inner maximization. Therefore, it is reasonable to give a larger $\\epsilon$ to those robust data. But is it necessary to decrease $\\epsilon$ to inherently vulnerable data? According to theorem 2, decreasing $\\epsilon$ will decrease the inner maximization. Could you provide a theoretical justification for why it is beneficial to decrease the perturbation radii on vulnerable natural examples?\n2. In Section 5.1 Hyperparameters, I noticed that the hyperparameter $\\alpha$ is different for every method (e.g., MWPB-AT, MWPB-TRADES and MWPB-MART) and dataset (e.g., CIFAR-10 and Tiny Imagenet). As a result, I assume the performance of MWPB heavily relies on the choice of $\\alpha$. If $\\alpha$ has to be manually chosen for every method and dataset, I believe it will bring some additional cost. Then it is probably not correct to say \u2018our proposed method reweights perturbation radii at no additional cost\u2019 in Section 2.3. Could you please further clarify how efficient it is to find an optimal hyperparameter $\\alpha$?\n\nRegarding the notable improvement in robustness and accuracy, I will consider increasing my score if the authors can address all my suggestions and questions well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7824/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7824/Reviewer_Sshb",
                    "ICLR.cc/2024/Conference/Submission7824/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698212793642,
        "cdate": 1698212793642,
        "tmdate": 1700605227872,
        "mdate": 1700605227872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lnbnqPWXHT",
        "forum": "C42FkKhAUC",
        "replyto": "C42FkKhAUC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_hkHv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_hkHv"
        ],
        "content": {
            "summary": {
                "value": "This paper employs a margin-weighted perturbation budget to generate adversarial examples for adversarial training, where the instance-wise weighting is estimated based on the degree of intrinsic vulnerabilities of natural examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Propose an adversarial training method with instance-wise perturbation budget;  \n2. Achieve robustness improvements across multiple baselines."
            },
            "weaknesses": {
                "value": "1. Writing needs improvement. There are many confused and hard-to-follow long sentences in the manuscript.  \n2. Novelty is limited: the motivation is not novel, as the unequal treatment of samples has been extensively discussed in previous research. Methodologically, it relies on a typical combination of reweighting and adaptive perturbation radii, lacking novelty.  \n3. It is strongly recommended that the authors provide empirical experimental data regarding Theorem 1. For instance, in the case of PGD-AT on the CIFAR-10 dataset, what proportion of samples align with Theorem 1?  \n4. Empirically, in the later stages of adversarial training, nearly all samples have a d_margin greater than 0. This implies that the perturbation budget for all adversarial samples exceeds 8/255. Given this scenario, achieving higher natural accuracy is quite perplexing. Therefore, it is hoped that the authors can offer detailed insights into the evolution of the perturbation budget during training.  \n5. Insufficient experiment: it is suggested that the authors conduct experimental comparisons with a broader range of baselines, such as AWP, rather than solely focusing on standard AT methods."
            },
            "questions": {
                "value": "Please refer to the questions in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698484049130,
        "cdate": 1698484049130,
        "tmdate": 1699636958292,
        "mdate": 1699636958292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IVZAJd1IJX",
        "forum": "C42FkKhAUC",
        "replyto": "C42FkKhAUC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_vNfc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_vNfc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new method to improve the adversarial robustness of DNNs. The authors notice the difference in robustness and their effects on adversarial training between different training samples. Due to the vulnerability of some samples, the attack cannot find the optimal solution for some robust samples. Therefore, they propose to assign larger perturbation budgets to robust samples to enhance the utility of adversarial training. Experiments have shown that the proposed method can improve the robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors novelly propose to assign different perturbation budgets to different training samples according to the vulnerability of samples, and they propose an efficient way to represent the vulnerability of samples.\n- Experiments show that the DNNs trained with the proposed MWPB exhibit high robustness under various attacks.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- How will the proposed method affect the natural accuracy in theory? If the perturbation budgets on robust samples are too large, training on such adversarial examples may hurt the discrimination power and generalization power of the DNN. For example, the natural accuracies are all reduced in Table 3. The authors are suggested also to discuss the theoretical effect of the proposed method on the natural accuracy of the DNN.\n- Is there a threshold for the radius in Equation (3)? If not, when an input is too far away from the decision boundary (misclassified or correctly classified), the radius will be extremely small or large, which may affect the stability of training. Besides, the magnitude of outputs $f_\\theta(x)$ may be different in different models, so the weight $\\alpha$ needs to be carefully adjusted for each model. \n- The hyperparameters in experiments are determined heuristically for different methods on different models. This setting hurts the reliability of the obtained results. I wonder whether the proposed method is stable to different hyperparameters.\n- In experiments, the proposed method is only used after 80 epochs, at which point the model may have learned a certain optimization direction. What about the performance when using the MWPB from the beginning of the training?"
            },
            "questions": {
                "value": "What is the distribution of the perturbation radius computed by equation (3)? How different is it from the traditional uniform perturbations? In other words, how many samples are assigned small radius and how many samples are assigned large radius?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651867388,
        "cdate": 1698651867388,
        "tmdate": 1699636958125,
        "mdate": 1699636958125,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aFC4au6FfP",
        "forum": "C42FkKhAUC",
        "replyto": "C42FkKhAUC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_jD4X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7824/Reviewer_jD4X"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a reweighting function for assigning perturbation bounds to adversarial examples used in adversarial training, and leads to a new approach named Margin-Weighted Perturbation Budget (MWPB). MWPB assigns perturbation radii to each adversarial sample based on its vulnerability of the corresponding natural example. Empirical evaluations are done on the CIFAR-10, SVHN, and TinyImageNet datasets, and applying ResNet-18/WRN-34-10 model architectures. The results show that combining MWPB can improve robustness under PGD-20, CW, and AutoAttack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of this paper include:\n- Clear writing with some intuitive explanations such as Figure 1.\n- The proposed MWPB is straightforward and easy to implement, and it is naturally compatible with different AT frameworks.\n- The proposed methods are evaluated under some standard attacking methods such as PGD and AutoAttack, and empirically MWPB can improve robustness compared to baselines."
            },
            "weaknesses": {
                "value": "The weaknesses of this paper include:\n- The construction of the reweighing function in Eq (3) seems heuristic. There should be ablation studies on the effects of different reweighing alternatives, and justify why the construction in Eq (3) is superior.\n- The considered baselines (AT, MART, TRADES) are not up-to-date, where the top-rank methods listed on RobustBench [1] can already achieve >70% robust accuracy under AutoAttack on CIFAR-10 ($\\ell_{\\infty}=8/255$). The authors are encouraged to validate the compatibility of MWPB with more advanced methods listed on RobustBench.\n- The values of hyperparameters in Section 5.1 seem to be deliberately selected (e.g., $\\alpha=0.58,0.42,0.55$). There should be ablation studies on the effects of different hyperparameters on the performance of MWPB (i.e., is MWPB sensitive to different values of hyperparameters?).\n\n[1] https://robustbench.github.io/"
            },
            "questions": {
                "value": "As stated in the Weaknesses sections, the baselines (AT, MART, TRADES) are not up-to-date (proposed no later than 2019). So I would like to know if MWPB can be compatible with more advanced methods listed on RobustBench. Besides, there should be ablation studies on the effects of different reweighing alternatives and sensitivity w.r.t. hyperparameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698660292210,
        "cdate": 1698660292210,
        "tmdate": 1699636957994,
        "mdate": 1699636957994,
        "license": "CC BY 4.0",
        "version": 2
    }
]