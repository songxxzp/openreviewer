[
    {
        "id": "d1756mxMvG",
        "forum": "dALYqPm9gW",
        "replyto": "dALYqPm9gW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_gTNz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_gTNz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a mechanism of positing transformer-like systems in the recurrent domain to alleviate the quadratic cost of inference in transformers with a linear complexity, as is observed in typical recurrent systems like RNNs. Prior work already achieves this by replacing the softmax operation of transformers with a kernel similarity mapping, but suffers from having to store a matrix as its state space, as well as an unconstrained addition of positive values which would lead to numerical instability. The authors augment this approach with a gating mechanism as well as an approximation to the kronecker delta function to alleviate both the problems, and evaluate their proposed mechanism in reinforcement learning regime where transformers cannot be naively used owing to large number of steps within each episode as well as the need for faster inference to enable fast data collection. The proposed mechanism, ReLiT and AReLiT show improved performance in resource constrained settings and can be seen as relevant substitutions in such an environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and clearly demonstrates the benefits of the proposed approach when faced with a memory / compute restriction.\n- The proposed mechanism (AReLiT) alleviates the need for storing a matrix in its recurrent state, thereby providing further savings in memory utilization."
            },
            "weaknesses": {
                "value": "- The approach in recurrent linear transformers (Katharopoulos et. al 2020) need not add just positive values at each iteration. In particular, it relies on a notion of similarity through kernel functions, where the kernel itself is positive but the mapping $\\phi$ need not be. While Katharopoulos et. al do use a mapping based on positive values, it can simply be extended to setups where $\\phi$ can also map to negative values; and hence this does not seem to be a limitation of their framework.\n- The motivation behind a learnable feature map for self attention is not clear. Why not just do a parameterized $\\phi$ represented through a small neural network, as opposed to the complex scheme of computing outer products for queries and keys? What is the motivation behind doing so?\n- It would be useful if the authors did ablations for each of their additions, not only about how much the addition contributes to the performance but also what are some of the other candidate options. For example, what is the impact of using the outer product based query-key computation or using a normal neural network based approach without outer products.\n- Given that an important benefit of the proposed approach is the savings obtained on time complexity, it would be nice to have some results highlighting the performance obtained by the different methods across wall clock time to really see the advantage of the method.\n- There are also baselines that are missing in a number of experiments. None of the experiments benchmark against Katharopoulos et. al and there are also baselines missing in Figure 2.\n- There are a few typos, eg. \u201cIn the RL\u201d \u2192 In RL, \u201caddresses\u201d \u2192 address in Section 3, etc."
            },
            "questions": {
                "value": "The authors claim that a positive feature map is required to ensure that the similarity scores produced by the underlying kernel function are positive. Technically a kernel function only requires mapping to some vector space where dot-product is defined, and thus defining a kernel function does not require positive feature maps. Could the authors clarify this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Reviewer_gTNz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698443427153,
        "cdate": 1698443427153,
        "tmdate": 1699636720646,
        "mdate": 1699636720646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3EF23EwBUA",
        "forum": "dALYqPm9gW",
        "replyto": "dALYqPm9gW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_EZRU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_EZRU"
        ],
        "content": {
            "summary": {
                "value": "The quadratic complexity on the sequence length characterizing the self-attention mechanism has led to a surge of works aiming at optimizing Transformers performances, mainly inspired by recurrent mechanisms. Building on the Linear Attention (Katharopoulos et al., 2020) and subsequent works (Peng et al., 2021) , this paper proposes two recurrent alternatives to self-attention. The former, **ReLit**, introduces a learned gating mechanism that decays each index of the recurrent state matrix  $C_t$, in addition to a learned feature map $\\phi$ instead of a fixed one (as in Katharopoulos et al., 2020 or Peng et al., 2021).  The latter proposed model, **AReLit**,  is a low-rank approximation of the former,  introduced to optimize the space complexity. The authors carried on an experimental analysis focused on Reinforcement Learning (RL) tasks, investigating the model capability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written, and the clarity and structure of the first sections help the reader in understanding the context. The model is an incremental improvement that builds upon Linear Attention (Katharopoulos et al., 2020) and subsequent works, especially (Peng et al., 2021). I believe that the relation and inspiration from (Peng et al., 2021) should be better highlighted, given that such paper already introduced a (scalar) gating mechanism in the recurrence of *causal* Linear Transformers. However, the approach is original and the topic is significant to the community."
            },
            "weaknesses": {
                "value": "The authors describe the Linear Transformer [1] as a recurrent approach. I believe that the recurrent view of transformers is feasible only in the *causal* modality, i.e. when **causal masking** is performed (causal attention, also referred to as \u201cdecoder self-attention\u201d or \u201cautoregressive attention\u201d -- see both [1, 2]). Moreover, the paper title refers to a general *Recurrent Linear Transformer* whereas the main contributions are an index-based gating mechanism and a learnable feature map (i.e., the recurrence was already available in the models they build upon) --  I believe the paper title could be adjusted to reflect this. \n\nDespite the very promising empirical evaluation on RL tasks, I believe that the model investigation could be improved. \nThe authors proposed two variants to surpass the limitations of Linear Transformers [1,2]. Hence, I would expect an in-depth investigation  and comparison with respect to such models and other similar competitors [3]. Apart from an ablation study  (relegated in the supplementary section K), there are not other intuitions. \nDirect comparison in terms of number of learnable parameters (additional parameters introduced in Section 3.1 and 3.2 seems a lot), execution times or memory footprint  (see Table 3 and Figure 2 of [2]) are needed. \nMoreover, why did the authors choose to tackle RL tasks instead of standard benchmark devised for Transformers/Long range dependecies [4]? (or the tasks performed by  [1,2,3]) \n\n\n\n[1] Katharopoulos, Angelos, et al. \"Transformers are rnns: Fast autoregressive transformers with linear attention.\" International conference on machine learning. PMLR, 2020.\n\n[2] Peng, Hao, et al. \"Random Feature Attention.\" International Conference on Learning Representations. 2022.\n\n[3] Peng, Bo, et al. \"RWKV: Reinventing RNNs for the Transformer Era.\" arXiv preprint arXiv:2305.13048 (2023).\n\n[4] Tay, Yi, et al. \"Long Range Arena: A Benchmark for Efficient Transformers.\" International Conference on Learning Representations. 2020."
            },
            "questions": {
                "value": "1) Section 3.1 and 3.2 seem to introduce a large amount of new learnable parameters.  Direct comparison in terms of number of learnable parameters, execution times or memory footprint (see Table 3 and Figure 2 of [2]) with respect to direct competitors [1,2,3] are needed (also, the tradeoff w.r.t performances).  The authors only refer to timings in the last paragraph of page 8 (and supplementary section J), but it seems that the considered competitor is a standard self-attention Transformer and not a recurrence-based one. \n\n2) The authors proposed **ReLit** and **AReLit** in order to solve the issue of positive value addition in Linear Transformers recurrence (that could grow arbitrarily large). It could be interesting to better investigate this issue (also empirically -- or at least make the reader understand where the Linear Transformer fails) and how the proposed model solve it. \n\n3) The relation with Peng et al. [2] should be better highlited in the paper text, given that a (scalar) gating mechanism was already introduced in such paper. \n\n4) I would expect an in-depth investigation  and comparison with respect to direct competitors [1,2, 3]. Apart from an ablation study  (relegated in the supplementary section K), there are not other intuitions. The central contribution of the paper is an improvement with respect to [1,2], hence the improvements should be the main investigation of the experimental evaluation. \n\n5)  Why did the authors choose to tackle RL tasks instead of standard benchmark devised for Transformers/Long range dependecies [4]? (or even the tasks performed by  [1,2,3])  What happens when the sequence scale is increased (ListOps)? \n\n\n\n[1] Katharopoulos, Angelos, et al. \"Transformers are rnns: Fast autoregressive transformers with linear attention.\" International conference on machine learning. PMLR, 2020.\n\n[2] Peng, Hao, et al. \"Random Feature Attention.\" International Conference on Learning Representations. 2022.\n\n[3] Peng, Bo, et al. \"RWKV: Reinventing RNNs for the Transformer Era.\" arXiv preprint arXiv:2305.13048 (2023).\n\n[4] Tay, Yi, et al. \"Long Range Arena: A Benchmark for Efficient Transformers.\" International Conference on Learning Representations. 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Reviewer_EZRU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698513510407,
        "cdate": 1698513510407,
        "tmdate": 1699636720509,
        "mdate": 1699636720509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KuZudcnJcJ",
        "forum": "dALYqPm9gW",
        "replyto": "dALYqPm9gW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_o1NU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_o1NU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents recurrent linear transformers -- an extension of Linear Transformer that involves the gating of state information. The authors posit that the linear transformers suffer due to (i) excessive positive additions while updating hidden states and (ii) sensitivity to the choice of kernel feature maps used to calculate the queries, keys, and values. To mitigate these mentioned shortcomings, the authors present ReLiT and its approximation called AReLiT. These use (i) a gating mechanism to limit the excess flow of state information and (ii) parameterized feature maps that replace fixed kernel feature maps. To evaluate the efficiency of ReLiT and AReLiT, long-horizon RL tasks are chosen where it is essential to remember past observations and actions to make an informed decision. Particularly, environments like T Maze, Noisy CartPole, MysteryPath, and Memory Maze are employed for empirical analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper aims at improving well-established transformer architecture for processing longer-length inputs. Especially in the case of deep sequential decision-making, having a sequence encoder that can process longer-length inputs is one of the coveted tools. The paper attempts a very relevant problem.\n\nWhile the authors describe an extension to linear transformers, they also suggest an approximate version that reduces the space complexity of the proposed improvement further, making it practically viable.\n\nI like the choice of environments, especially the T-Maze mentioned, where the agent has to remember the context of decision-making from the beginning of the episode. Plus, the Noisy CartPole environment is apt for testing the contextual abilities of different networks as the network has to model velocity using only displacement vectors."
            },
            "weaknesses": {
                "value": "The **main issue** with the current draft is it lacks a comparison of ReLiT with the linear transformer. From the introduction and the way the whole work is motivated, I was expecting to see empirical validation of the issues in the previous linear transformers, namely, the excessive positive feedback overburdening the hidden states in some way or the other, and the effect of fixed kernel functions. However, the draft falls short of underscoring these issues empirically. Unless these issues are substantiated, the motivation behind the work does not feel convincing.\n\n**Related Works**: Also, since the paper is about coming up with a better architecture for processing longer inputs, I feel the discussion on the efforts to increase context lengths in transformers should be included in the paper. For instance, authors could start with initial attempts like LongFormer. The Recurrent Memory Transformer (RMT) is another very relevant direction. Authors can qualitatively compare their approach to RMTs, too.\n\n**Evaluation metrics**: My understanding of the work is that the recurrent linear transformers would allow us to (i) have lower time and space complexity compared to traditional transformers and (ii) have effective hidden state usage. I am unsure whether focusing more on the expected returns of the RL agents will sufficiently capture the computational effectiveness or utility of gating. One of the reasons to raise this up is the paper, from its beginning, does not try to solve RL using transformers as a specialty. There are a lot of recent works, such as Decision Transformers, which try to attempt to solve RL using transformers. The paper, however, keeps their mention to minimal, which I am okay with as I evaluate the approach from the architectural superiority of ReLit against other contenders, and not from the perspective of return maximization.\n\n**The reported time and space complexities**: At the end of page 2, the paper describes the time complexity of the self-attention module in canonical transformers to be O(N d^2), which does not seem correct. I tried calculating the complexity on my own and also verified with a few other sources, too, and found the time complexity to be O(N^2 d + N d^2). I also found it hard to understand why Linear Transformer's self-attention complexity was independent of N."
            },
            "questions": {
                "value": "I have the following questions cum suggestions for the authors:\n1. The paper would really benefit from the addition of a comparison with linear transformers.\n2. Authors could revise the related works section by adding discussion on the aforementioned related directions.\n3. Authors could look into ways of highlighting the usefulness of the gating mechanism described in ReLit.\n4. How do different kernel functions affect the working of ReLiT or linear transformers in general? Can the authors please demonstrate an ablation study?\n5. Can authors please provide a step-by-step derivation of how time and space complexities are derived for different transformer variants?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Reviewer_o1NU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619075819,
        "cdate": 1698619075819,
        "tmdate": 1699636720392,
        "mdate": 1699636720392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r9SszqosTj",
        "forum": "dALYqPm9gW",
        "replyto": "dALYqPm9gW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_zjLV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6450/Reviewer_zjLV"
        ],
        "content": {
            "summary": {
                "value": "This paper was motivated by the heavy computational costs in vanilla Transformers and Linear Transformers in previous work. They provided two variants of Transformers, called recurrent linear Transformer and approximate recurrent linear transformer. They claim that with the first construction, the inference cost for each token is independent of the context length, and they used low-rank approximation in the second construction. The low-rank approximation is well theoretically motivated and is convincing.\n\nThey did experiments on RL problems which are known as computationally challenging to Transformers. They showed that in several RL environments, their models can compete with SOTA, but with 40-50 percent less inference costs. Generally speaking, this is a paper with obvious strengths and weakness. I vote for weak accept since despite some weakness, I think their idea is well motivated and the creative low-rank approximation is very worth investigating."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Their idea is very well motivated. They explain in detail where the inference costs in vanilla Transformers and Linear transformers arise.\n\n2. The performance on some RL environments show a great reduction on inference costs while their average rewards are basically the same as the previous models.\n\n3. The low-rank approximation in the approximate recurrent linear Transformer is well theoretically motivated. The theory seems good, and the approximation of Kronecker function by cosine functions are very creative and inspiring.\n\n4. The paper is generally well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. From the high level, I think the biggest issue is that the authors only did experiments in RL. Although they explained that this is because RL is particularly difficult for Transformers, this is not convincing enough. I still believe that the authors should include some experiments on language data / some other tasks which Transformer usually applies. The reason is, your idea of recurrent linear TF is not motivated by dealing RL problem with Transformers, and Transformers were not originally invented for RL problems, so it is a little bit strange to only conduct experiments on RL. \n\n2. The motivation part can be more precise and some of your motivation is not really important, from my perspective. In the third paragraph of the introduction section, you propose three motivations, while I think your methods do not really address the second one (the kernel function one). The authors claimed that the Transformers of linear Transformers have a manually-designed kernel function (softmax / Gaussian), but in your ReLiT model, although there are some learnable parameters in your kernel functions, the kernel function class is still manually-designed and fixed. Moreover, the kernel function class in ReLiT is not rich enough to make Transformers learn the 'globally optimal' kernel. You can only expect that the Transformers learn the 'optimal' kernel in the kernel class you specify.\n\nFrom my perspective, I think the tunable kernel function may not be the main reason for the experimental success of ReLiT on RL problems. The main advantage of your model is the low inference costs introduced by the low-rank approximation, and this learnable kernel is kind of far from your main contribution. This will make your paper more distracted.\n\nI am not sure how the authors think. My suggestion is, if you want to claim that the learnable parametric kernel is important and the kernel class you specify is good, you may need to do some ablation experiments. For example, what happen is you replace the softmax or fixed kernel function with this learnable kernel in the standard /linear Transformers?\n\n3. In the last paragraph in the section 2, you claim 'The Linear Transformer\u2019s self-attention has a context-independent inference cost, unlike the canonical self-attention mechanism', which I feel it kind of unfair. Although the inference cost for each token in linear Transformer is independent of the context length, the inference cost of a token matrix (a sequence) does depend on that, right? In sec 2.1, you showed the costs for standard TF for inferring a token matrix while in sec2.2, you showed the costs for one token. I believe you may modify your claim here."
            },
            "questions": {
                "value": "/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "/"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6450/Reviewer_zjLV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865915048,
        "cdate": 1698865915048,
        "tmdate": 1699636720266,
        "mdate": 1699636720266,
        "license": "CC BY 4.0",
        "version": 2
    }
]