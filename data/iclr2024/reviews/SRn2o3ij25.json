[
    {
        "id": "wekxEXQSr1",
        "forum": "SRn2o3ij25",
        "replyto": "SRn2o3ij25",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on addressing the long-tailed problem from the perspective of the training process uncertainty and model prediction correlation. it proposes an  Implicit Knowledge Learning method which consists of an Implicit Uncertainty Regularization (IUR) for mimicking the prediction behavior over adjacent epochs and an Implicit Correlation Labeling (ICL) to reduce the bias introduced by one-hot labels. Experiments are conducted on various long-tailed dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed idea is clear and easy to understand.\n2. The authors commit to open-sourcing the code to facilitate result reproduction.\n3. The authors discuss some potential limitations, such as computational costs.\n4. The IKL is a plug-and-play method which could be plugged into many existing long-tailed solutions and bringing performance improvement."
            },
            "weaknesses": {
                "value": "1. While some improvement can be observed in the tail classes, currently, there is no concrete evidence to support the claim that learning between two adjacent epochs can enhance the model's performance. The authors should provide more theoretical justification for this claim rather than relying solely on empirical observations from training experiments.\n2. IKL appears to be a regularization-based approach to network learning, and indeed, there are other regularization-based solutions for long-tail problems (e.g., WD[1]). It would be beneficial for the authors to provide a more detailed comparison between IKL and these existing solutions.\n3. In Table 7, IUR and ICL don't seem to have brought significant improvements to the results, especially given that this is on a smaller dataset CIFAR-100-LT. This raises concerns about the performance of IKL. The authors should address this concern by providing a more in-depth explanation or conducting additional experiments to demonstrate the effectiveness of IKL.\n\n[1] Alshammari, Shaden, et al. \"Long-tailed recognition via weight balancing.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "1. The authors propose IKL as a solution to address the issue of model prediction uncertainty. In fact, many expert methods like RIDE and SADE are also designed based on the same principle. The authors should provide a detailed explanation of why IKL results in greater improvements when combined with these expert methods compared to using Softmax (e.g., results in Table5, combined with Softmax only improves the performance by 0.5, but combined with RIDE improves 1.4)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_W2Ce"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698391931711,
        "cdate": 1698391931711,
        "tmdate": 1699637159151,
        "mdate": 1699637159151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3TvBPdb4Bv",
        "forum": "SRn2o3ij25",
        "replyto": "SRn2o3ij25",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_Zu8m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_Zu8m"
        ],
        "content": {
            "summary": {
                "value": "1. This paper aims to address long-tailed recognition via knowledge distillation.\n\n2. Considering the prediction uncertainty of models trained in adjacent epochs, the authors propose to use the model trained in the last epoch to guide the training in the current epoch. \n\n3. Inspired by this idea, an $L_{IUR}$ loss is proposed by directly distilling knowledge with KL divergence loss from the model trained in the last epoch.\n\n4. Moreover,  models trained by cross-entropy suffer from classifier bias. The paper proposes to use medium-feature to construct a new classifier. Based on the new classifier, it distills knowledge from the model trained in the last epoch again."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clear and easy to follow.\n2. The method is simple but effective. Improvements are observed when combining it with previous methods."
            },
            "weaknesses": {
                "value": "1. The proposed L_{IUR} loss uses KL loss to regularize the output from the current model to be similar to the output from the model of the last epoch.      \n    Because the last epoch model is fixed, the KL loss is actually equal to a cross-entropy loss with soft labels from the last epoch model.\n    The proposed L_{ICL} loss uses the pseudo-label from the last epoch model. The pseudo-label is calculated based on the medium-feature classifier.     \n    The difference between L_{IUR} and L_{ICL} is that L_{IUR} distills knowledge with a biased classifier while L_{ICL} distills knowledge with a medium-feature classifier without bias.    \n    Thus, from my point of view, the proposed method is a kind of ensemble of distillation with a rebalanced classifier (with L_{ICL}) and a biased classifier trained with cross-entropy (with L_{IUR}).     \n\n2. The authors claim that uncertainty of models trained in adjacent epochs exists. Is it really important?        \n    If we use only one well-trained teacher model through the training, it can also help models give consistent predictions.      \n    The paper should show the necessity of distillation with the model trained in the last epoch rather than a specific well-trained teacher model.    \n\n3. Comparison with previous distillation-based methods are missed.      \n\n4. What's the medium function in Eq. (7)? How to rank the features and find the medium?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646931307,
        "cdate": 1698646931307,
        "tmdate": 1699637158994,
        "mdate": 1699637158994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aG2CKAA5EM",
        "forum": "SRn2o3ij25",
        "replyto": "SRn2o3ij25",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework named Implicit Knowledge Learning (IKL) framework to tackle the long-tailed recognition problem. In detail, the IKL framework includes two techniques, so called the Implicit Uncertainty Regularization (IUR) and the Implicit Correlation Labeling (ICL). First, the main idea of IKL is to regularize the predictions of the current epoch using the ones from the previous epoch. It is especially shown to be effective to reduce uncertainty in the tail-class examples. Second, ICL constructs an additional label matrix based on the inter-class similarity, to improve the learning process. This can help to complement the typical one-hot labels. The full framework IKL can serve as a plug-and-play scheme, where it can be attached to existing long-tailed learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The necessity of IUR technique is quite clear and noticeable, as presented in Figure 1. Such discovery, where uncertainty values especially on the minor classes grow compared to the ones from the major categories is useful. The proposed scheme can properly address the problem. In addition, learning from correlations between different classes is reasonable. \n\n- The proposed framework is practical, since it can be built on existing methods to further improve long-tailed learning. \n\n- Experiments are extensive; it has been tested on more than three or four different types of baseline methods, while demonstrating consistent improvements."
            },
            "weaknesses": {
                "value": "- The critical downside of the proposed work is about technical novelty. The proposed IKL combines two components IUR and ICL, based on using previous predictions and inter-class correlations respectively, both of which are related to well-established literature. For example, as the elementary deep learning based semi-supervised learning methods, Temporal Ensembling [S. Laine et al., 2016] and MeanTeacher [A. Tarvainen et al., 2017] present the generalized version of IUR, where averaging past model predictions or model parameters to enforce consistency loss term. In that sense, despite the limited novelty, the proposed work needs to discuss the previous works in this direction and experimentally compare with those methods in the long-tailed learning scenario. Similarly, regarding the proposed ICL technique, it is common to learn from the dependencies among different class labels (also in the name of co-occurrence) [Z. M. Chen et al., 2019]. Authors also need to acknowledge previous attempts in this direction and provide sufficient discussions on what component is new. In summary, from the technical aspect, it provides limited innovation compared to previous literature that adopts similar approaches.\n\n- As an additional comment on ICL, the effect for applying ICL is currently unclear. To better understand the proposed component, it would be beneficial to quantitatively measure and qualitatively visualize the class-level dependencies (i.e., correlation matrix) on a specific dataset.\n\n- For calculating the class prototypes C, the verification for the superiority of median features compared to simple averaging is missing. Is there any reference or supporting experiments? In addition, it would be further helpful to provide a brief explanation about how to compute a median of features.\n\n- A proof-reading process, especially for referencing papers, is necessary. A lot of typos can be found, for example, missing a space (i.e., NCLLi et al.) and missing punctuation (i.e., offline process Peng et al.). Additionally, the reference section needs to be updated. For example, the paper \u2018Balanced meta-softmax\u2026\u2019 from Jiawei Ren et al., is presented in NeurIPS 2020, which is written as arXiv in the current version."
            },
            "questions": {
                "value": "- To further improve reproducibility, it is recommended to provide the set of parameters for each augmentation operation in RandAugment. \n\n- Related to the limitation provided in the conclusion, specific demonstration for the increase of space or memory by saving the previous epoch\u2019s predictions and computing class-wise similarity matrix, depending on the number of total samples and classes, would be helpful for better understanding this limitation.  \n\n- It needs to mention that the progressive scaling of $\\alpha$ is applied from section 3 for better clarity. It is confusing since it firstly appears in the experiment section.  \n\n---\n\n[Summary and guidance to rebuttal]\n\nOverall, the motivation for the proposed IUR and ICL is quite clear in the context of long-tailed learning and it conveys consistent improvements in experiments. However, as those techniques are not totally new, it is necessary to present connections with previous approaches in those directions and to emphasize the novel aspects of the proposed work. \n\n---\n\nReferences\n\n[S. Laine et al., 2016] Temporal ensembling for semi-supervised learning, in ICLR 2016.\n\n[A. Tarvainen et al., 2017] Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, in NIPS 2017. \n\n[Z. M. Chen et al., 2019] Multi-Label Image Recognition with Graph Convolutional Networks, in CVPR 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9210/Reviewer_6AeQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667913986,
        "cdate": 1698667913986,
        "tmdate": 1699637158873,
        "mdate": 1699637158873,
        "license": "CC BY 4.0",
        "version": 2
    }
]