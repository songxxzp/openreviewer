[
    {
        "id": "I18ys0EKo3",
        "forum": "7HfliVAtCG",
        "replyto": "7HfliVAtCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission313/Reviewer_Vm41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission313/Reviewer_Vm41"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use vision only backbone to represent categories as visual prototype to unify few-shot object detection and open vocabulary detection in cases of input and output. They propose DE-ViT, a combined approach that combine both DINO-v2 and RCNN together. They evaluate DE-ViT on open-vocabulary, few-shot, and one-shot object detection benchmark with COCO and LVIS, which achieves the new state-of-the-art results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The overall writing is good and easy to follow. \n\n- The proposed method achieves significant improvements on few-shot detection benchmarks. It also achieves strong performance on COCO/LVIS open vocabulary benchmark.\n\n- The proposed refined localization architecture is interesting."
            },
            "weaknesses": {
                "value": "- The idea of learning prototype to compare is not novel in few-shot learning (detection). In this work, authors adopt more recent strong DINO-v2 to extra visual features to enhance visual entity's comparison. \n\n- The idea is similar to OV-DETR since both uses the bypassing per-class inference procedure. \n\n- I double that the proposed approach may not be a real open vocabulary setting, since the authors assume the novel classes of COCO can be accessed during the inference. This means the vocabulary is still limited and pre-defined. For example, the proposed model is hard to extend into zero-shot setting where the novel prototypes are not available.\n\n- The visual novel objects are sampled from COCO or LVIS, which also may lead to data leakage. What about the classes sampled from ImageNet-21k to build prototypes?\n\n- The model needs extra Region Proposal Network along with backbone. What about the shared frozen DINOv2-ViT backbone design such as works [1][2]?\n\n- Missing parameters and GFLops analysis. \n\n- Presentation: What are the meanings of frozen or unfrozen? Better add the notations in the figure-2. \n\n\n[1]. Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n\n[2]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023"
            },
            "questions": {
                "value": "See the weakness part. I rate the current draft as boardline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656325968,
        "cdate": 1698656325968,
        "tmdate": 1699635957671,
        "mdate": 1699635957671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SCgljIm6IT",
        "forum": "7HfliVAtCG",
        "replyto": "7HfliVAtCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a new method is introduced for few-shot and open-vocabulary object detection. Using DINOv2, the method takes a small set of examples to create main patterns, called prototypes. With these prototypes, the authors describe a way to train a network that can tell apart one item from others, based on the proposals generated by the RPN. They also introduce a technique to improve the initial box predictions. Through detailed tests, the paper shows that their method works better than other current methods. The paper also includes a study that looks closely at how each part of their method contributes to its overall performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The utilization of DINOv2 embeddings to construct prototype for different classes is interesting.\n2. The introduction of the propagation procedure offers a novel approach to the problem.\n3. The method exhibits exemplary performance, surpassing preceding methodologies across diverse datasets and tasks.\n4. The manuscript provides an comprehensive ablation study evaluated the contribution from different modules."
            },
            "weaknesses": {
                "value": "The writing of Section 3.1 could be enhanced. Some portions might be presented in a more reader-friendly manner."
            },
            "questions": {
                "value": "1. Would it be possible to incorporate notations such as `f` and `h` into Figure 3 for easier referencing?\n2. Should the notation be \"[C]\\\\c_k\" rather than \"[C]/c_k\"?\n3. The upsampling process described in Eq.3 is somewhat unclear. Could you first elucidate the underlying objective or insight you aim to convey? The rationale behind upsampling on the channel dimension is not immediately evident. If the intent is to balance the number of prototypes and base categories, might a re-weighting approach similar to the focal loss be more appropriate?\n4. During the propagation procedure, what transpires if the expanded box encompasses another object of the same category?\n5. Could you provide a more detailed explanation of Eq. 4, particularly concerning the `w` and `h` components? An illustrative figure might enhance comprehension.\n6. Clarification on terms such as \"bAP\" and \"Split-n\" would be beneficial.\n7. Are there prior methods that employ DINO/DINOv2 embeddings to craft representations for detection/re-identification? If so, what's the difference between their methods and yours?\n8. For the definition of h, it might be better to use \"h=\\\\bar{f}\\\\dot p\" directly to create better reading experience.\n9. What are the limitations and failure cases of this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission313/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission313/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission313/Reviewer_bhpM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816691113,
        "cdate": 1698816691113,
        "tmdate": 1699635957581,
        "mdate": 1699635957581,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YCNhN9gCp1",
        "forum": "7HfliVAtCG",
        "replyto": "7HfliVAtCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission313/Reviewer_f8VP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission313/Reviewer_f8VP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an open-set object detector using vision-only DINOv2 backbone to learn new classes through images rather than language. The paper proposed a new region propagation technique for localization and evaluate the method on open-vocabulary, few-shot and one-shot benchmark. The experimental results show good improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and presented.\n2. The description of methods is clear."
            },
            "weaknesses": {
                "value": "1. The method lacks some novelty and doesn't look attractive to me. This is the main problem of the paper. So many parts ensembles together only cost more computational resources.\n2. On the few-shot benchmark, the good experiments are more shown on 10 or more shot rather than fewer-shot. There are so many different measures  shown in the experimental results, which are not convincing to me about its real performance.\n3. There is no visualization of the experimental results shown in the paper."
            },
            "questions": {
                "value": "please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889325471,
        "cdate": 1698889325471,
        "tmdate": 1699635957490,
        "mdate": 1699635957490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DaMBYOsU3q",
        "forum": "7HfliVAtCG",
        "replyto": "7HfliVAtCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission313/Reviewer_Jjnu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission313/Reviewer_Jjnu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a paradigm for few-shot object detection which does not rely on fine-tuning the base model or top layers for the few-shot classes. The authors compare the paradigm with few-shot detection, open-vocabulary detection, and one-shot detection. The authors propose to use a few samples from novel classes to create prototypes for objects which can be used to detect new objects in images. The proposed paradigm leads to improved performance compared to prior few-shot, one-shot, and open-vocabulary methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a different paradigm for few-shot object detection which alleviates the need for fine-tuning during the few-shot inference process. The paper also shows that this paradigm can lead to improved performance compared to prior methods."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper are based around a lack of clear motivations for some decisions and some un-fair comparisons. In particular, the authors should respond to the following:\n\n1. I believe that the comparisons to standard open-vocabulary object detection methods are unfair because the setting proposed in this paper is quite different - open-vocabulary (or zero-shot object detection) does not assume any knowledge of visual representation of novel classes, however, the proposed approach does. So, placing this method as an open-vocabulary object detection approach is unfair, and frankly, unnecessary. The authors can focus on few-shot/one-shot approaches - this is already a good paper for these settings.\n\n2. In figure 1, which objects come from based classes and which are the few-shot/novel categories? There are different colours used in the image but no legend. This makes it difficult to ascertain the performance of the proposed approach based on this image.\n\n3. If you are selecting the top-K classes in one step, why is there a need to up-sample to T? Can't we just keep using \"K\" because that is deterministic - just up/down-sample to K?\n\n4. How does the performance change with the number of samples used for prototype creation? From Table 3, it seems that the performance for 30-shot is lower than for 10-shot in some cases. Why? Is there an inflection point where adding more samples stops helping?\n\n5. Do prior one-shot object detection methods also follow the few-shot paradigm of fine-tuning the model? If not, what exactly is the difference between the proposed method and one-shot methods?\n\n6. Suggestion: It might be better to show equation 1 (and other similar things) as dot-products to make it more straight-forward to understand."
            },
            "questions": {
                "value": "Please see weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission313/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699223373417,
        "cdate": 1699223373417,
        "tmdate": 1699635957390,
        "mdate": 1699635957390,
        "license": "CC BY 4.0",
        "version": 2
    }
]