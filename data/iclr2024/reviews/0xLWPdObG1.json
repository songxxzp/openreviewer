[
    {
        "id": "RMHQg1wNrh",
        "forum": "0xLWPdObG1",
        "replyto": "0xLWPdObG1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_HXxH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_HXxH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a way to learn a deep neural network (DNN) with Poisson outcome distribution and Gamma-distributed random effects in one joint optimization approach using the h-likelihood framework. Apart from the derivation of a constant to obtain both maximum likelihood estimates for the fixed effects model part and best unbiased predictions for the random effects, the authors also using a method-of-moments approach to circumvent non-identifiability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Significance: Including random effects in neural networks is an important research topic that has not been fully explored\n- Originality: The paper seems to be the first to tackle Poisson DNNs with random effects in this way"
            },
            "weaknesses": {
                "value": "- Significance: \n    + It's not clear how much better the given idea works compared to other approaches\n    + Figure 4 shows that there is no true sparsity achieved, and separability between relevant and irrelavant features might depend on the size of the gap; using an actual l1-penalty via [overparametrization](https://arxiv.org/abs/2307.03571) could help in this case.\n    + Statements in 4.2\n        * Theorem 1: Maybe I have missed something, but AFAIU it's trivial that the likelihood will improve once you shift fixed information from random to fixed effects as both the likelihood and \"prior\" will increase (this is also common knowledge in mixed models). I would thus see this as part of Section 2.2. \n        * Online or post-hoc corrections like the one in (7) probably work, but why not directly encode the constraint by encoding the information in z (i.e., using the design matrix $\\tilde{Z} = Z - 1_n 1_n^\\top Z / n$ instead of Z where Z is the matrix stacking all observations $z_{ij}$)? \n    + Section 3: It's unclear to me why it is important to align $\\ell_e(\\theta,v)$ and $\\ell_e(\\theta, u)$ as both are simply two different model assumptions. The reason this might not be clear is that the same $f_\\theta$ is used, yet not explicitly defined. More specifically, if $u_i$ follows a Gamma distribution, then $f_\\theta$ should be the density of a Gamma distribution. But using **the same** Gamma density to evaluate the likelihood of a transformed random variable $v(u_i)$ does not make sense. If $v$ is $log(\\cdot)$, then $v_i$ yields values that can be negative while the Gamma distribution is only defined for positive values. If 2) the authors mean that one transforms $v_i$ and then plugs in $u_i$ into a Gamma density (or the other way around), then there should not be any additional Jacobian term as the log-Gamma distribution is exactly defined in a way such that $exp(v_i) \\sim Gamma$. \n- Clarity: The math is not always clear, e.g., \n    + $f_\\theta$ in (3) is never explicitly defined if I am not mistaken \n    + bold/non-bold symbols are not used in a consistent manner (e.g. $\\boldsymbol{\\mu}^m$ and $\\boldsymbol{\\mu}^c$ in Section 2)\n    + I assume $f_\\theta(y|u)$ does not actually depend on $\\theta$ directly? (in all fairness, this is also not very clear in many papers by Lee and authors which are cited for this)\n    + in Section 4.4 the typical sparsemax notation is adopted but not aligned with the previous notation (it does not get clear what $\\textbf{z}$ and $\\textbf{p}$ are in the context of the paper -- in particular as z is already used by the \"random slope features\", but the lines below suggest that the section talks about selection of fixed effects) and also contains typos (e.g., it should say \"genuine features $(K < 10)$\" I assume, not $k < 10$).\n- Limited empirical evidence: The simulation study is rather limited with\n    + fixed n, number of features, etc.\n    + no comparison against other hglm approaches (e.g. hglm package in R)\n    + no comparison with REML approaches (?)\n\n  and sometimes not very clear\n    +  why simulate $v_i \\sim N(0, \\lambda)$ if it's clear that $log (u_i)$ won't follow a Gamma distribution?\n    +  why does Table 1 say \"Distribution of random effects\" if the content is RMSPE values? Actually measuring the distribution quality would give more insights into the estimation performance.\n- Quality: the phrasing is a bit akward at several places (e.g. \"However, it could not yield MLE for the variance component $\\lambda$\")"
            },
            "questions": {
                "value": "1. Can you comment on my comment on Theorem 1?\n2. Can you clarify the idea behind Section 3 and the two different likelihoods?\n3. The authors talk about obtaining \"MLEs\" and \"BUPs\". How is the unbiasedness for random effects defined (as there are multiple ways to do this) and can the authors prove that unbiasedness is something that can be achieved, even in the case of a \"highly non-convex\" neural network likelihood (that influences the random effects at least indirectly)?\n4. Can z also contain information from x and if yes, how is identifiability ensured in this case (cf. [Ruegamer, 2023](https://proceedings.mlr.press/v202/rugamer23a.html))?\n5. How do REML approaches fit into the picture?  \n6. Would an EM-based approach for NNs like in [Xiong et al., 2019](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.pdf) be a meaningful alternative?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534588997,
        "cdate": 1698534588997,
        "tmdate": 1699637152177,
        "mdate": 1699637152177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F05flpNYQs",
        "forum": "0xLWPdObG1",
        "replyto": "0xLWPdObG1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_ZRFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_ZRFo"
        ],
        "content": {
            "summary": {
                "value": "This paper essentially extends subject-specific predictions to Poisson DNNs. It introduces Gamma random effects into Poisson DNNs and proposes a novel learning framework that can be applied when high-cardinality categorical features are present.  The experimental results across synthetic and real-world data suggest that in certain scenarios, employing this framework can yield advantageous results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The theory checks out and the introduction of a method that simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects for Poisson DNNs is a valuable contribution.\n2) The feature selection process for high cardinality categorical features is interesting and has valuable practical implications."
            },
            "weaknesses": {
                "value": "1) The synthetic experiments are rather limited. I would like to see how certain hyperparameters affect the experiments. For example, for any $x_{ij}$ only 5 features are chosen from an AR(1) process, is this choice standard? Why is the AR(1) process considered?\n2) The improvements are marginal across both synthetic and real-world experiments."
            },
            "questions": {
                "value": "1) Could you please provide more examples of why Poisson DNN modelling might be advantageous compared to previous methods, especially in real-world experiments that you have considered?\n2) At some point, after running a set of synthetic experiments, it is claimed that \u201cTherefore, the proposed method enhances subject-specific predictions as the cardinality of categorical features becomes high.\u201d However, this claim seems rather premature especially since only $q_{train} = 3$ and $q_{train} = 1$ have been considered. Could you please provide the RMSPE of PF-NN and PG-NN on a line chart where different values of $q_{train}$ are considered? This is needed to fully validate such a claim."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813291254,
        "cdate": 1698813291254,
        "tmdate": 1699637152045,
        "mdate": 1699637152045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mAACpDtX6p",
        "forum": "0xLWPdObG1",
        "replyto": "0xLWPdObG1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_z2gb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9157/Reviewer_z2gb"
        ],
        "content": {
            "summary": {
                "value": "Here the authors presents a novel framework that merges hierarchical likelihood learning with Poisson-gamma Deep Neural Networks. This approach aims to better handle clustered count data by incorporating both fixed and random effects into the model. A proposed two-phase training algorithm is aimed at improving model performance. The proposed method is evaluated on both simulated data and real data where it is compared to existing methods, various types of DNNs and GLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a novel Poisson-gamma Deep Neural Network (DNN) and supports it with a robust mathematical foundation. This includes deriving h-likelihood functions and addressing identifiability issues, which demonstrates a high level of theoretical rigour.\n- Incorporation of Random Effects: The model incorporates both fixed and random effects into a deep learning framework, aiming to capture both marginal and subject-specific variations in count data. This is particularly useful for applications where understanding both population and subject-specific trends is crucial such as in healthcare or social sciences.\n- Two-Phase Training: The paper proposes a comprehensive two-phase training algorithm that focuses on both pretraining and fine-tuning of the model parameters. This adds to the model's could potentially improve its generalisation to unseen data.\n- The motivation is well founded and I believe it would be of relevance to the ICLR community."
            },
            "weaknesses": {
                "value": "- The largest weakness is the unconvincing experimental result. They do not confirm fully confirm the advantages of the method (as claimed in the abstract). The distribution of the multiple runs on simulated data does not show obviously outperformance of this method compared to the others. It would have been interesting to see a statistical test of difference performed here.\n- Related to the previous point, results on real datasets are presented without any replications or multiple runs. No errors are shown in table  2 making it very hard to judge the importance of the values. It should not be terrible expensive to perform multiple runs on these relatively small datasets. It would also have been very interesting to see this method applied to much larger datasets.\n- The feature importance experiment (Figure 4) is valuable but I feel that it lacks context when not presented with more standard feature importance methods (e.g. permutation importance). I understand these are simulations with a pre-determined ground truth but contrasting this new method with a well-understood method would be a welcome addition to help better showcase the relative properties of this method.\n- Although the two-phase training can bring benefits, as set out in the paper, it is also important to understand the cost that comes with it. A clear comment from the authors on the absolute and relative efficiency (or better, computational cost per training compared with other methods) would be a very useful addition from practical standpoint. Without one, the reader is left wondering if this truly is practical."
            },
            "questions": {
                "value": "- Spelling: e.g. in 4.2 \"In contrast to HGLM and DNN\" should be HGLMs and DNNs\n- What is the early stopping method used for the experiments? Is it the same for pre-training and training?\n- The method does not perform as well as PG-GLM on the fruits dataset but better on the others. Do the authors have any hypotheses as to why that is the case? What is different about these datasets?\n- How do the authors envision the scalability of the proposed framework when dealing with extremely large datasets, particularly in medical and health applications?\n- Could this method be extended to a larger framework to include other types of random effects distributions beyond the gamma distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832419154,
        "cdate": 1698832419154,
        "tmdate": 1699637151941,
        "mdate": 1699637151941,
        "license": "CC BY 4.0",
        "version": 2
    }
]