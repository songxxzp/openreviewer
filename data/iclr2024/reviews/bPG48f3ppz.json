[
    {
        "id": "c8T428qaus",
        "forum": "bPG48f3ppz",
        "replyto": "bPG48f3ppz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
        ],
        "content": {
            "summary": {
                "value": "The author proposed a cross-modal SNN, named SpikeCLIP, which can perform feature extraction and alignment across multiple modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The author proposed a cross-modal model for SNN and explored some downstream tasks based on it."
            },
            "weaknesses": {
                "value": "1. The author mainly adopted the idea of knowledge distillation to train SpikeCLIP, however, a similar scheme [1] has been proposed previously. In addition, regarding the algorithm and neuron model design of SNN, I think the contribution of this paper is very limited. I think this paper is more about directly transferring the concepts related to CLIP to the field of SNN and lacks technical contributions related to SNN.\n\n2. The performance of SpikeCLIP on downstream datasets (CIFAR-10, CIFAR-100) is not superior, and the author did not fully list recent works about single-modality SNN in Table 1. For example, [2] can achieve higher performance (CIFAR-10: 95.58%, CIFAR-100: 78.71%, 4 time-steps) by directly training on ResNet-19 than SpikeCLIP, which means that the author's pre-trained multi-modality model based on ImageNet-1k is even inferior to a single-modality SNN through direct training on ResNet-19.\n\n3. The training dataset (ImageNet-1k) and model parameter size (56.87M) used by the author in this paper are too small. Although this paper is a preliminary exploration of SNN cross-modal learning, the performance achieved by the author and the number of downstream tasks attempted have serious deficiencies.\n\n4. In Figure 2, the text encoder section of SpikeCLIP uses the LayerNorm layer. However, the floating-point multiplication operations involved in LayerNorm are usually not allowed in the inference stage of SNN.\n\n[1] Xu, Qi, et al. \"Constructing deep spiking neural networks from artificial neural networks with knowledge distillation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2] Duan, Chaoteng, et al. \"Temporal effective batch normalization in spiking neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 34377-34390."
            },
            "questions": {
                "value": "See Weakness Section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Reviewer_EDPh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698136747518,
        "cdate": 1698136747518,
        "tmdate": 1699635983374,
        "mdate": 1699635983374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RuBhzuSJJB",
        "forum": "bPG48f3ppz",
        "replyto": "bPG48f3ppz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors introduce SpikeCLIP inspired by CLIP. To realize the SpikeCLIP, the authors provide an Alignment Pre-training + Dual-Loss Finetuning method. Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across various datasets commonly used for multimodal model evaluation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This is the first work to transfer the CLIP to the SNN field.\n2. The authors provide the code, which is good."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. The two steps can be seen as the KD method. So the work just uses a KD method to convert a CLIP as SpikeCLIP.\n2. The results are not good. For image classification, the accuracy is worse than other SOTA methods. For the zero-shot task, since SpikeCLIP is not trained on a really large dataset, it is much worse than CLIP, thus the value of SpikeCLIP is limited, considering that the greatest value of CLIP is suitable for zero-shot tasks."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission560/Reviewer_njnp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593528725,
        "cdate": 1698593528725,
        "tmdate": 1699795274714,
        "mdate": 1699795274714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QDCcHGpjcE",
        "forum": "bPG48f3ppz",
        "replyto": "bPG48f3ppz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission560/Reviewer_edZQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission560/Reviewer_edZQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes SpikeCLIP, an image-text multi-modal SNN based on CLIP, and a two-stage training method to fine-tune it on downstream tasks. The resulting model can achieve comparable performance on mainstream image datasets with reduced energy consumption and maintains robustness on zero-shot classifications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents the SNN image-text multi-modal model.\n\n2. The two-stage fine-tuning approach retains the performance of the original CLIP model on various tasks, including classification with undefined class labels."
            },
            "weaknesses": {
                "value": "1.The proposed model architecture for each modal separately is not innovative enough. The image encoder uses an existing SNN architecture (Spikingformer), while the text encoder is a simpler MLP structure, bypassing the difficulties of processing long sequences with SNNs. This design choice improves training efficiency but may limit the model's text-processing capabilities.\n\n2.The two-stage training process of distillation followed by task-specific fine-tuning lacks specific optimization for SNN computational characteristics.\n\n3.Due to the inaccessibility of CLIP's full pretraining dataset, this work uses the smaller ImageNet-1k for distillation pretraining. This restricts the model's generalization capability compared to the original CLIP, including both image and language modalities. More pretraining data would likely be necessary for the model to serve as a general-purpose multimodal foundation model, which may lead to more future challenges, such as convergence and training efficiency."
            },
            "questions": {
                "value": "1. Does the simple MLP text encoder sacrifice generalization ability in language understanding? For example, the paper does not describe the text templates used in zero-shot classification. If a fixed template like \"a photo of a {label}\" is used throughout, the text encoder's role may be oversimplified and insufficient to handle other plausible templates such as \u201ca picture of a {label}\u201d. More details should be provided on text settings.\n\n2. ImageNet-1k is used for alignment pretraining before fine-tuning on other datasets. However, test accuracy after further fine-tuning on ImageNet itself is not reported. Does SpikeCLIP have adequate representational capacity and scalability for such large-scale image classification tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646017572,
        "cdate": 1698646017572,
        "tmdate": 1699635983195,
        "mdate": 1699635983195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gq65g9FO9X",
        "forum": "bPG48f3ppz",
        "replyto": "bPG48f3ppz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission560/Reviewer_edTp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission560/Reviewer_edTp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed SpikeCLIP, a framework inspired by CLIP method to dually deal with both image and text input through spiking neural network. The SpikeCLIP is trained through a two-staged \u201calignment (CLIP) pre-training + Dual-Loss Fine-tuning\u201d, showing good results in classification tasks and zero-shot learning tasks. Compared to conventional CLIP, spikeCLIP shows theoretically high energy efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- SpikeCLIP is the first multimodal SNN architecture that shows the ability to deal with both text and image input. \n- The paper proposes an effective training framework to train SpikeCLIP. The alignment pre-training part finds a good way to transfer the representation learned in CLIP to SpikeCLIP.\n- The implementation of the framework and the experiments are solid. Moreover, the experiments results shows robust performance in image classification tasks (including zero-shot setting)."
            },
            "weaknesses": {
                "value": "- Although this is the first paper (as far as I know) to realize spiking version of CLIP, the paper itself is lack of enough novelty. Nowadays, as the surrogate gradient based SNN training methods have been greatly developed, transferring or reproducing a specific architecture in conventional ANN(artificial neural network) to SNN is never a significant issue. More important thing is actually to find the specifics of spikes in those architectures or settings, rather than claiming \u201cwe are the first spiking version of xx\u201d. Unfortunately, I did not find such highlights in this paper.\n- One may argue that this paper shows the energy efficiency of SpikeCLIP compared with its counterpart ScratchCLIP, however, such comparison is not fair enough. As shown in the appendix, the authors only calculate the SOPs corresponding to each spike, while the updating process of membrane potential, the BN/LN operations are not included. It is worth to note that these are only the computing energy. Moreover, the additional energy that needed to maintain the membrane potential for each neuron, which is actually more severe in reality, is not considered in their calculation. No need to mentioning the factual energy lies mostly in data/weight transferring, which is not mentioned either in the paper (the weight amount is the same, and the data input is in fact more than ANN, given one needs to repeat T times of input). I understand this is a paper focusing on algorithms, discussing the real implementation is somewhat out of the scope. However, if the only novelty or advantage actually is built on energy consuming, such discussion is then unable to be ignored.\n- Overall, the performance gap compared to ANN is still very big, and noticing this in only in CIFAR10/100, for larger dataset or more complex tasks, I do believe the gap will be larger."
            },
            "questions": {
                "value": "Please see above weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission560/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762059717,
        "cdate": 1698762059717,
        "tmdate": 1699635983115,
        "mdate": 1699635983115,
        "license": "CC BY 4.0",
        "version": 2
    }
]