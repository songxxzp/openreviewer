[
    {
        "id": "8ZjD5znK1N",
        "forum": "oMLQB4EZE1",
        "replyto": "oMLQB4EZE1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
        ],
        "content": {
            "summary": {
                "value": "The author proposes a tokenizer for DNA language model, namely, using BPE, in contrast to the k-mer approaches as used before. The authors also propose a large-scale benchmark called GUE to compare DNA language models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clear definition of motivations, challenges, and solutions\n- The use of BPE makes intuitive sense\n- Experiments look solid and extensive\n- Solving an important problem of DNA LM \n- A new large-scale benchmark"
            },
            "weaknesses": {
                "value": "- Novelty is questionable since BPE is a well-known technique. The use of FlashAttention, LoRA, and AliBi are also not new. So methodologically, it is hard to gauge its novelty."
            },
            "questions": {
                "value": "- Is there potential for cross-species information leakage? For instance, given the substantial overlap in genomes between humans and primates, the model might easily predict the masked token.\n\n- How does this compare to HyenaDNA?\n\n- On page 7, the authors note that they utilize LoRA for NT but opt for full fine-tuning for DNABERT/DNABERT-2. However, in the methods section, LoRA is described as an integral part of the approach. This is somewhat perplexing.\n\n- While the authors suggest further pre-training on GUE sequences, this might raise concerns regarding its ability to generalize to datasets with novel sequences. For a balanced comparison, it might be best if the authors refrain from additional pre-training on GUE sequences.\n\n- Did the authors evaluate the sequence statistics of the GUE sequences in relation to the sequences from the pre-training corpus?\n\n- The authors claim the method requires significantly less computational power and memory. Did they test the performance with a larger model size? If there wasn't a notable performance enhancement, it would be noteworthy to highlight this.\n\n- Have the authors assessed how the model's performance varies with different dataset sizes?\n\n- Have the authors conducted ablation on FlashAttention, AliBi, and LoRA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698033859427,
        "cdate": 1698033859427,
        "tmdate": 1700633681720,
        "mdate": 1700633681720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XFjTuzdeG7",
        "forum": "oMLQB4EZE1",
        "replyto": "oMLQB4EZE1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
        ],
        "content": {
            "summary": {
                "value": "The authors describe a foundation model for DNA sequences, improving on existing models (of which there are relatively few) in terms of computational requirements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "-Incorporation of more recent language model techniques into the modeling approach.\n\n- Foundation models have the potential to be a highly useful resource for the computational biology community.  There are very few options at the moment, and the significantly reduced computational requirements of DNABERT2 compared to the Nucleotide Transformer on the one hand, and improved accuracy over DNABERT, make it a welcome addition.\n\n- DNABERT2 is appropriately benchmarked against DNABERT and the Nucleotide Transformer.\n\n- The authors have curated a collection of datasets for benchmarking DNA language models.  The benchmark datasets are sufficiently challenging to provide good discrimination between the performance of the various methods, and indicate that there is still plenty of room for improvement."
            },
            "weaknesses": {
                "value": "- Deep learning models applied to one-hot encoded genomic sequences appear to have a much higher level of interpretability than those that utilize k-mer tokenization, and I expect this to be even worse for the BPE encoding used in this work.  Unlike other areas of application, in computational biology applications, interpretability is a key factor in choosing a model."
            },
            "questions": {
                "value": "- \"Despite having 30% more parameters than DNABERT, DNABERT-2 requires only one-third the number of FLOPs. This indicates the superiority of the Byte Pair Encoding (BPE)-based tokenization method over overlapping k-mer tokenization in terms of modeling efficiency.\"\nNot sure I agree with this statement - the increased efficiency might be the result of other differences between the models.\n\"This underscores the importance of providing the model with adequate data, particularly when the model size is scaled up, and further highlights the inefficiency of overlapping k-mer tokenization. The comparison between DNABERT and NT-2500M-1000g exposes the sample inefficiency of non- overlapping k-mer tokenization. Despite being trained on 2.5 times more tokens, NT-2500M-1000g achieves a performance similar to that of DNABERT.\"\nAgain, there are other differences between the models, so ascribing this to the difference in tokenization method is a stretch.  If you want to demonstrate the advantage of BPE tokenization, you will need to perform an experiment on two different versions of DNABERT2 - one with k-mer tokenization, and one with BPE tokenization.  **The authors have addressed this point with a thorough ablation study**.\n\n- Please compare your benchmark datasets with the recently published \"Genomic benchmarks\":\nGre\u0161ov\u00e1, K., Martinek, V., \u010cech\u00e1k, D. et al. Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC Genom Data 24, 25 (2023). https://doi.org/10.1186/s12863-023-01123-8\n\ntypos / grammar:\n\nBENCKMARK: GENOME UNDERSTANDING EVALUATION (GUE)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf",
                    "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722936544,
        "cdate": 1698722936544,
        "tmdate": 1700582601730,
        "mdate": 1700582601730,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K7LGsqTbp8",
        "forum": "oMLQB4EZE1",
        "replyto": "oMLQB4EZE1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
        ],
        "content": {
            "summary": {
                "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
            },
            "weaknesses": {
                "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
            },
            "questions": {
                "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794184380,
        "cdate": 1698794184380,
        "tmdate": 1699636344053,
        "mdate": 1699636344053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hiEwjLi49j",
        "forum": "oMLQB4EZE1",
        "replyto": "oMLQB4EZE1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces DNABERT-2, an advancement in genome foundation modeling, which aims to decode the linguistic intricacies of genomes. The authors assert that the computational and sample inefficiencies of k-mer tokenization, predominantly used in earlier models, act as barriers in the development of foundational models for large genomes. To address this, the paper introduces Byte Pair Encoding (BPE) as a replacement for k-mer tokenization. BPE is more efficient and overcomes the limitations of the k-mer approach. The authors also emphasize the need for a standardized benchmark for genome understanding and consequently introduce the Genome Understanding Evaluation (GUE) dataset. Experimental results reveal that DNABERT-2 performs on par with state-of-the-art models but with fewer parameters and less GPU time during pre-training. The model also shows significant improvements over the original DNABERT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. DNABERT-2 incorporates ALiBi and Flashattention mechanisms, enhancing speed and context length.\n2. The model successfully borrows several techniques from LLM (Large Language Models) and integrates them into DNABERT.\n3. The authors have collected a comprehensive dataset tailored for short sequence prediction.\n4. The research is detailed, with a focus on the nuances of the biology setting and the existing benchmarks, showcasing a holistic approach."
            },
            "weaknesses": {
                "value": "1. The input size for the proposed benchmark seems to be on the shorter side for genomics, potentially limiting its applicability to broader genomics problems.\n2. The benchmark's design appears constrained, lacking baseline models like CNNs and omits language model training from scratch, which could provide comparative insights.\n3. While the paper is apt for an ML conference, there is a discernible deficiency in the depth of biological insights. Better downstream tasks, such as CAGE-seq prediction and so on.... (longer sequence context)"
            },
            "questions": {
                "value": "1. In the introduction, can you clarify what you specifically mean by \"genome language modeling\"?\n2. Following up on the theme, why was there no citation or reference to models like deepbind/deepSEA? For instance, the TF-DNA binding prediction from Wang et al., 2022, seems not a great citation? Not a genomics language modeling. \n3. Given the unique structure and function of DNA, why is there a continued emphasis on tokenization in DNA language modeling?\n4. I recommend adding the count of sequences for each dataset in Table 1 to provide a clearer understanding of the dataset sizes.\n5. Why weren't tasks involving longer sequences incorporated after introducing DNABERT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814624012,
        "cdate": 1698814624012,
        "tmdate": 1700517891936,
        "mdate": 1700517891936,
        "license": "CC BY 4.0",
        "version": 2
    }
]