[
    {
        "id": "E85rsbg9Jy",
        "forum": "VP20ZB6DHL",
        "replyto": "VP20ZB6DHL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_gDTz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_gDTz"
        ],
        "content": {
            "summary": {
                "value": "This study introduces an approach aimed at mitigating hallucinations by harnessing the self-verification capabilities of Large Language Models (LLMs). The proposed method involves generating verification questions by an LLM to cross-check the accuracy of its initial responses, autonomously providing answers to these queries, and ultimately generating a refined response. The authors conducted a series of experiments to empirically establish the efficacy of this approach in addressing hallucination problems exhibited by the LLM across a range of tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The Chain-of-Verification concept is straightforward and can be practically implemented without necessitating adjustments to LLM's parameters.\n\n(2) The empirical evaluations conducted on a varity of tasks, including list-based questions (Wikidata), closed-book MultiSpanQA, and longform text generation, demonstrate the effectiveness of the proposed method in mitigating LLMs' hallucinations.\n\n(3) This paper is well-written and presents its ideas in a clear and comprehensible manner."
            },
            "weaknesses": {
                "value": "(1) The introduction of the proposed method does incur additional inference overhead. It would enhance the paper's rigor to compare these added computational costs with those associated with alternative methods that also target the reduction of LLM hallucinations.\n\n(2) The utilization of few-shot learning for enabling LLMs to perform planning, verification, and generation (3-shot as detailed in the Appendix) raises a potential concern that the results might be influenced by variations in the few-shot examples used."
            },
            "questions": {
                "value": "(1) What criteria were employed for the selection of few-shot examples, and what is the potential influence of employing different examples on the results?\n\n(2) Could you provide an estimate of the additional computational overhead that the proposed method would introduce?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698477034200,
        "cdate": 1698477034200,
        "tmdate": 1699636210550,
        "mdate": 1699636210550,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A8y72s9y8O",
        "forum": "VP20ZB6DHL",
        "replyto": "VP20ZB6DHL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
        ],
        "content": {
            "summary": {
                "value": "Large Language Models (LLMs) can sometimes exhibit \"hallucination,\" which refers to the generation of factually incorrect or misleading information. This work proposes the Chain-of-Verification (CoVe) method, a strategy for self-correcting LLM responses by asking and answering verification questions. The experimental results demonstrate that the CoVe approach reduces hallucinations in a variety of tasks, including Wikidata-based list problems, closed-book MultiSpanQA, and long-form generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The writing is well. \n2) The idea of correcting LLM responses by answering and answering verification questions from the model itself is valuable.\n3) The proposed method is effective in alleviating hallucination problems."
            },
            "weaknesses": {
                "value": "1) There is an absence of comparative analysis with other methods aimed at mitigating the hallucination issue. It remains to be clarified whether CoVe offers an enhancement in performance relative to other methods.\n2) More instances of prompts are required. For example, in Section 3.3, some examples of prompts need to be provided to distinguish between the several variants of verification variants.\n3) Needs to provide more examples of the use of CoVe in different tasks."
            },
            "questions": {
                "value": "1) How do we verify that the model can do plan verification, execution verification, and verified response well after a few shots? What happens if you use zero-shot CoVe?\n\n2) Is it possible to skip the step of generating a baseline response by directly generating questions similar to plan verification based on the query and generating the final response based on the response? Does CoVe have any advantages over this method?\n\n3) More details of prompts are needed in the supplemental materials, or it would be difficult for readers to follow the ideas.\n\n4) What is the time complexity of the proposed method and other competitors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2690/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2690/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761276830,
        "cdate": 1698761276830,
        "tmdate": 1699636210444,
        "mdate": 1699636210444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FfG5tlK3fj",
        "forum": "VP20ZB6DHL",
        "replyto": "VP20ZB6DHL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_r3aM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_r3aM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new prompting method to mitigate LLMs\u2019 hallucination issues, dubbed chain of verification (CoVE). The model first generates an initial output, and then, conditioning on it together with the input, it generates a series of questions targeting the facts stated in the output. The model then answers the verification questions and identifies the factual errors in the initial response, which is then revised to produce the final outputs. Various variants are explored, each answering the verification questions in different manners. CoVE is tested on several question-answering datasets on Wikipedia, including Wikidata and Wiki category list (both are created from templates), MultiSpanQA, as well biography generation. Results show that CoVE reduces hallucination, at a cost of slightly worse helpfulness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Mitigating hallucinations in LLMs is a timely and important topic\n- CoVE is simple and widely applicable\n- Various variants are tested out"
            },
            "weaknesses": {
                "value": "- The technical contribution is thin. I had a hard time justifying the paper\u2019s technical contribution since CoVE looks very similar to [1]\nThe two Wikipedia QA datasets (Wikidata and Wiki-category list) are created from simple templates and are rather toyish. I am not sure how much they add to the paper.\n- Hallucinations are especially tricky to address in generation; the only generation task considered is biography generation, which is way less challenging than most real-world applications\n- The paper aims to address hallucination problems. However, I do not find this reflected in the designs of the experiments: 3 out of 4 experiments are question answering, and the remaining one is a rather confined biography generation task. Therefore I find the experiments of this paper weak. Including a more diverse and challenging set of tasks can strengthen the results.\n- Wikipedia is a domain that most models have a lot of exposure to; it would be more interesting to see how CoVE performs in other domains such as scientific, legal, and medical domains.\n- It seems that CoVE can potentially negatively impact the model\u2019s helpfulness: in Table 1, the number of factual outputs reduces; similarly, in Table 3, CoVE produces fewer facts. Some discussion on this would be interesting.\n- I am concerned about making strong conclusions solely based on the FactScore metric: will a model receive higher FactScore by producing shorter outputs with less information? Complementing it with models\u2019 helpfulness, generation quality, and controlling for the number of facts might be necessary\n\n[1] https://arxiv.org/abs/2210.03350"
            },
            "questions": {
                "value": "- A key assumption behind CoVE is that even when the model is not able to generate factual outputs, it might still be able to identify nonfactual statements. A similar observation is mentioned by [2], but they argue that retrieval is necessary. Can the authors elaborate their views on whether or not the model needs external information to identify factual errors in its own outputs?\n\n[2] https://www.semanticscholar.org/paper/Language-Models-Hallucinate%2C-but-May-Excel-at-Fact-Guan-Dodge/45653ad43124f02dc2cf2db3357be1d1d78ddb18?utm_content=title&utm_medium=unfurl&utm_source=slackbot"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819024002,
        "cdate": 1698819024002,
        "tmdate": 1699636210367,
        "mdate": 1699636210367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wPzBAy9SSr",
        "forum": "VP20ZB6DHL",
        "replyto": "VP20ZB6DHL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_Lhor"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2690/Reviewer_Lhor"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles a hallucination problem of LLMs via a carefully designed framework of iterative prompting. Specifically, the authors propose chain-of-verification (CoVe), which revised the original response of LLMs by generating verification questions and then answering to them. All these processes are conducted via few-shot prompting. Through the experiments on various applications, the effectiveness of CoVe has been demonstrated as it successfully reduces a hallucination of applied LLM (LLaMA-65B) and outperforms the performance of carefully fine-tuned LLM to follow the instruction (LLaMA2-70B Chat)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Clarity**. Overall, the writing is clear and easy to follow. In addition, the organization of the main draft is well-established.\n2. **Well motivated problem**. Reducing the hallucination and improving the factuality of LLMs is an interesting and important problem. To this end, considering the improved prompting framework is a reasonable and well-motivated direction. \n3. **Simple and efficient method.** The proposed method is simple and can be applicable regardless of the types of LLMs. Also, it shows consistent improvement across the various applications."
            },
            "weaknesses": {
                "value": "1. **Absence of necessary baselines**. As the authors pointed out in the Related work sections, there are many relevant works based on prompting, to reduce the hallucination of LLMs [1,2,3] or improve LLMs\u2019 reasoning [4,5]. However, these baselines are never compared through the experiments now. Therefore, it\u2019s hard to verify the effectiveness of the proposed CoVe compared to them.  \n2. **Difficulty of direct comparison**. Currently, only zero-shot (or CoT) results are presented for LLaMA2 and few-shot results for LLaMA65B, respectively. It makes be hard to compare both models as there is no overlap. To ease their comparison, including the results of LLaMA2 few-shot and LLaMA zero-shot is strongly encouraged.  \n3. **Inconsistency across Tables**. While Factor+revise is presented in Table 3 and it achieves the best score, there are no such results in Tables 1 and 2. Does this method not perform well on the setups in Tables 1 and 2? To facilitate the understanding of the working mechanism of the proposed framework.  \n4. **More qualitative examples**. While the authors present some examples in Figures 1 and 3, it would be better to present more examples to help the understand of readers.\n\n[1] Manakul et al., Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models., arXiv:2303  \n[2] Cohen et al., Lm vs lm: Detecting factual errors via cross examination., arXiv:2305  \n[3] Varshney et al., A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation., arXiv:2307  \n[4] Miao et al., Selfcheck: Using llms to zero-shot check their own step-by-step reasoning., arXIv:2308  \n[5] Madaan et al., Self-Refine: Iterative Refinement with Self-Feedback., NeurIPS 23"
            },
            "questions": {
                "value": "Please address the concerns in above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699603755198,
        "cdate": 1699603755198,
        "tmdate": 1699636210271,
        "mdate": 1699636210271,
        "license": "CC BY 4.0",
        "version": 2
    }
]