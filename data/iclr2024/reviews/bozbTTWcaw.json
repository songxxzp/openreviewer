[
    {
        "id": "JkDQ30wWSC",
        "forum": "bozbTTWcaw",
        "replyto": "bozbTTWcaw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_Djj8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_Djj8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two simple gradient manipulation strategies to stabilize control learning via backpropagation through time in physics simulation. The approaches are motivated by the fact that naively executing back propagation through time along a long horizon in physics simulation with neural network feedback policy results in gradient exploding and vanishing problems. The first proposed approach cuts the gradients of the neural network inference step and the computed pseudo-gradients have the potential problem of non-rotational-free. The second proposed approach alleviates the issue of the first approach by zeroing out the gradient components with wrong signs. The proposed approaches are evaluated in three dynamics control problems and are shown to outperform the optimization using regular gradients."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed gradient modification approaches are simple and concise. The approach should be easy to implement.\n\n2. The proposed approach shows great and consistent performances on the designed problems."
            },
            "weaknesses": {
                "value": "1. Lack of theoretical guarantee for the proposed approach.\n\n2. The approach is only validated on the problems with simple dynamics. Its generalizability to more complex tasks is unknown."
            },
            "questions": {
                "value": "1. What does the color in Figure 1(b) and 1(c) mean? Does it mean the norm of the gradient?\n\n2. $\\theta$ in Eq. (1) is not defined.\n\n3. For the gradient stopping approach (section 3.1), does it guarantee the modified gradient is zero at global minimal?\n\n4. The underlining assumption of the approaches is \u201cmost physics simulators come with a well-behaved gradient flow\u201d, which is usually not true in most robotics problems especially when contacts happen [1, 2]. In those cases, the gradients from the simulation usually have much larger gradients than the gradients from the neural network. It would make the proposed approach much stronger if it could be evaluated on more complex robotic tasks such as the ones in [1, 2].\n\n5. Is there any theoretical guarantee of the correctness of the combination approach (section 3.3). It would be helpful to have two proofs: (1) the modified gradients keep the global optimality of the problem, (2) the modified gradient field is rotation-free.\n\n6. The complex version of the cart pole swing-up task is a bit strange in that the network is trained to control multiple poles with the same mechanism. Does it mean the network outputs 4x the number of actions to control 4-pole concurrently? The problem complexity does not indeed increase since basically, four copies of the single-pole policy should work well in this 4-pole task. A more meaningful task would be controlling a 4-linkage pole (i.e. multi-pendulum) which has much more complex dynamics than a single-linkage cart pole.\n\n\n[1] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax - a differentiable physics engine for large scale rigid body simulation.\n\n[2] Jie Xu,\u00a0Viktor Makoviychuk,\u00a0Yashraj Narang,\u00a0Fabio Ramos,\u00a0Wojciech Matusik,\u00a0Animesh Garg,\u00a0Miles Macklin. Accelerated Policy Learning with Parallel Differentiable Simulation"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2701/Reviewer_Djj8",
                    "ICLR.cc/2024/Conference/Submission2701/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698215805253,
        "cdate": 1698215805253,
        "tmdate": 1700769176833,
        "mdate": 1700769176833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mnDrzjw52u",
        "forum": "bozbTTWcaw",
        "replyto": "bozbTTWcaw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_BicD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_BicD"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to tackle the gradient explosion / vanishing problem in backpropagation through time. The authors first propose a method to stop the gradient of the feedback policy at each step with respect to the state while keeping the rest of the states active. Then, the authors point out that this update has problems due to rotation, and proposes to combine the original gradient and the modified gradient to tackle how rotation can slow down convergence of gradient-based methods. The authors compare their method against regular gradient descent and stopped gradient descent, and show that the method performs better."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The visualizations of the problems are well done and the problems that the authors are trying to convey are clearly communicated to the reader.\n2. The method shows convincing improvement over competitors on simple experiments."
            },
            "weaknesses": {
                "value": "(1) I have spent some time trying to understand the authors' argument on why it would be beneficial to stop the gradient of the policy with respect to the current state (i.e. $\\partial_x N$ in author's notation). But I still find that the motivations and the justifications for this is quite weak.\n \nFundamental theorem of calculus (generalized Stokes) tells us a nice connection about the loss and its gradient, so for smooth systems that the authors are considering, if we integrate back the gradient, we should get the loss. So when the authors set some of the terms to zero, there must be some surrogate loss that the modified gradient is considering. I'm willing to agree with the authors that this loss might have a better landscape compared to the original one; but how do we know that whether or not this landscape is completely unrepresentative or the original one?\n\nI think this is the most important section and contribution of the paper that is relatively not very well motivated in the paper, and am willing to give the paper a much better score if the authors can be more convincing about these points with some theory to back it up.\n\n(2) Related to above points, in the experiment results, the performance of combined (C) is much better than the Modifed (M). But it's not clear if this is because rotation is fixed, or if modified is simply not good because of the above issue.  \n\n(3) The authors are missing a large branch of relevant work on studying the efficacy of gradient-based methods in the setting of optimization through differentiable simulation [1,2,3,4], a lot of the issues that the authors have mentioned for Back Propagation Through Time (BPTT) could have benefited from citing these works. These works also have existing methods for improving the performance of BPTT (e.g. total propagation from [1], alpha-order gradients from [3]), which would have made stronger baselines. \n\n(4) It seems to me like the authors are mainly considering the difficulty of BPTT as considering complicating feedback from control actions, but previous works have mainly motivated the shortcomings of BPTT through the lens of characteristics in the dynamics such as chaos [1,2,3] or discontinuities through contact [3,4]. It is unclear if the author's method will improve performance for these difficult systems.\n\n(5) This is minor, but I wished the authors used a more standard notation from nonlinear control (where state-actions are (x,u), dynamics are f, and policy is k) or Reinforcement Learning (states-actions are (s,a) dynamics can still be f, policy is $\\pi$). \n\n\n[1] Parmas et al., \"PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos\", ICML 2018\n\n[2] Metz et al., \"Gradients are not all you need\"\n\n[3] Suh et al., \"Do Differentiable Simulators Give Better Policy Gradients?\" ICML 2022\n\n[4] Antonova et al., \"Rethinking Optimization with Differentiable Simulation from a Global Perspective\", CoRL 2022"
            },
            "questions": {
                "value": "(1) I think the biggest question is: why is stopping the gradient $\\partial_x N$ more beneficial, and how do we ensure it's taking more globally beneficial steps compared to the actual gradient? How do we know it's not completely wrong by ignoring these terms, or if there is some pathological system where throwing away these terms will completely make the optimization fail?\n\nA convincing case that the authors could consider is the Linear Quadratic Regulator problem where we have linear dynamics $S(x,c) = ax + bc$ and a linear policy $N(x,\\theta)=\\theta x$ over some horizon. It is known that gradient descent will converge to the optimal parameters $\\theta$ for this problem [5]. But if we assume that $\\partial_x N = 0$, can we actually converge to the minima of the original problem at all?\n\n[5] Fazel et al., \"Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator\", ICML 2018"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2701/Reviewer_BicD",
                    "ICLR.cc/2024/Conference/Submission2701/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698558097656,
        "cdate": 1698558097656,
        "tmdate": 1700702634370,
        "mdate": 1700702634370,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e9mQNZTl3C",
        "forum": "bozbTTWcaw",
        "replyto": "bozbTTWcaw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_Z3pv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_Z3pv"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the gradient exploding and vanishing problem due to recurrent operations in the context of control optimization with differentiable physics in the loop. It constructed a toy example to illustrate the problem and proposed a simple method to modify the gradient such that a better optimization landscape can be obtained. \n\nThe method is validated on three tasks and compared to alternative approaches (regular gradients, no combination, no long-range back-propagation)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written. The toy example and visuals are very helpful in terms of understanding the problem and solution. The choice of the example is well explained.\n- The proposed method is conceptually simple (gradient stopping and sign check) but sheds light on using gradient modification to stabilize optimization."
            },
            "weaknesses": {
                "value": "**Experiments**\n- The visualization of results could be improved. For example in Fig 3, multiple curves have the same color and overlap each other, making it difficult to draw a clear conclusion. It might be better to draw a mean-std plot for each method where the std (computed over trials) is shaded."
            },
            "questions": {
                "value": "1. The analysis assumes a simplified simulator (identity mapping + control). However, the transition between steps could be complex (e.g., non-linear contact) but the controller is simple (PD controller). How does this change the analysis? One concrete example is robot control.\n2. In the cart-pole experiment, Fig 4 suggests optimization is less stable with fewer poles. Is there an explanation for why this happens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614162583,
        "cdate": 1698614162583,
        "tmdate": 1699636211651,
        "mdate": 1699636211651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ob0JBNseDB",
        "forum": "bozbTTWcaw",
        "replyto": "bozbTTWcaw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_kKUm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2701/Reviewer_kKUm"
        ],
        "content": {
            "summary": {
                "value": "An learning algorithm is proposed for neural network systems that interact with a differentiable simulator. The idea is to train the neural network policy network with gradient descent, except to stop the gradient from backpropagating through the network multiple times. It is argued that backpropagating through time leads to optimization difficulties for gradient methods (e.g. the well-known gradient explosion in recurrent neural networks). Stopping the gradient in this way doesn't change the objective function, but can improve the dynamics of learning. The authors present nice simulated examples and a series of three experiments"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Very well written. Excellent presentation with nice examples.  \n- The toy examples and figures really helped with illustrating the points.\n- Experiments are clear and support the claims.\n- Differentiable control of simulators is a topic of interest. This could have high impact."
            },
            "weaknesses": {
                "value": "- I think the paper could have benefited from having more discussion of the chosen experiment applications. Knowing how these applications compare to potential real-world applications in terms of complexity would have been valuable context for the section on computational cost. It also would have given some context on how well this method might scale to complicated simulations."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840223948,
        "cdate": 1698840223948,
        "tmdate": 1699636211550,
        "mdate": 1699636211550,
        "license": "CC BY 4.0",
        "version": 2
    }
]