[
    {
        "id": "AxNmsRxLBH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission465/Reviewer_pC11"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission465/Reviewer_pC11"
        ],
        "forum": "cmAIfTK6fe",
        "replyto": "cmAIfTK6fe",
        "content": {
            "summary": {
                "value": "The paper proposes vision retention networks, an alternative to vision transformers, that offer a dual parallel and recurrent formulation. The paper discusses in detail the mechanisms behind this new network and what are necessary changes to apply the retention architecture to vision. Through ablation studies they evaluate their architecture compared to possible alternatives. The authors successfully demonstrate how this architecture scales with more data and parameters, showing promising results. Finally, they demonstrate how the retention mechanism can lead to faster inference in cases, especially when encoding longer sequences, a consequence of evaluating larger images with the same patch size."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Proposing a new architecture involves numerous challenges.\n1. The authors offer some ablations to demonstrate that the final architecture exhibits the best performance under some computational constraints. Experiments with models of different size and different datasets showcase how the architecture scales with the number of parameters and number of samples.\n2. The authors exhibit how a chunkwise retention can lead to competitive inference in terms of throughput, while allowing inference with larger sequence lengths. Being able to trade-off saturating the compute hardware (larger C) with computational complexity (smaller C) seems like a great outcome of your work.\n3. The code and models are going to be publicly available."
            },
            "weaknesses": {
                "value": "1. The main motivation is that retention networks can lead to faster inference. From experiments in Figure 2, Figure S1 and Table S1 this is not that evident. Keeping the patch size the same, the image size has to be increased substantially. For smaller -- typical -- images sizes, ViR models have smaller throughput compared to the regular ViT models.  \n2. The main advantage of retention networks is in the dual formulation. Since image encoding is not an autoregressive problem, it is not clear to me what are the benefits in this case (apart from chunkwise inference). Perhaps some relevant applications could include autoregressive image generation [1], or other decoder applications [2]. Still in most cases, having an encoder model for encoding image features makes the most sense.\n3. Retention networks introduce \"biases\" due to the decay factor. It is not clear if applications that require more global features may suffer from these restrictions. The authors could perhaps try object detection or semantic segmentation as other suitable downstream tasks.\n4. The retention mask introduces constraints due to the ordering of the tokens, i.e. patches. This is not discussed at all, apart from a short analysis on a multipass encoding as an alternative. Images are 2D, and it would make sense to decay the mask based on the 2D distance of the patches. This should be the main focus of the paper. Would be nice to also discuss connections with other \"soft\" inductive bias, e.g. see how ViTs can be adapted to consider distance between tokens at initialization in [3].\n\n[1] Chen, Mark, et al. \"Generative pretraining from pixels.\" International conference on machine learning. PMLR, 2020.\n\n[2] Beyer, Lucas, et al. \"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision.\" arXiv preprint arXiv:2303.17376 (2023).\n\n[3] d\u2019Ascoli, St\u00e9phane, et al. \"Convit: Improving vision transformers with soft convolutional inductive biases.\" International Conference on Machine Learning. PMLR, 2021."
            },
            "questions": {
                "value": "1. Are the baseline ViTs using flash attention [1] during inference? This can a big difference, especially for longer sequence lengths. In general, you could present only optimum (based on the best batch size) throughput for each image size, instead of showcasing results for multiple batch sizes. This would also improve the readability of Figure 2 (and Figure S1).\n2. A lot of (very successful) work has been built on top of ViT to makes them faster for inference. A lot of them rely on merging tokens at different levels of the Transformer architecture, e.g. [2]. Is it still possible to apply something like this with your new architecture?\n3. ViT numbers reported in Table 1 are suboptimal. Better ways to train ViT lead to higher accuracies [3, 4]. In general, models in Table 1 are trained for different number of samples and are difficult to compare with each other.\n4. Text could use some improvement. For example, I don't really understand the Setup section. How does the Hybrid architecture exactly look like?\n5. In [5] they also remove the [class] token but find significant computational benefits.\n6. How does chunkwise compare to other methods such as model partitioning and activation checkpointing? \n\n[1] Dao, Tri. \"Flashattention-2: Faster attention with better parallelism and work partitioning.\" arXiv preprint arXiv:2307.08691 (2023).\n\n[2] Bolya, Daniel, et al. \"Token merging: Your vit but faster.\" arXiv preprint arXiv:2210.09461 (2022).\n\n[3] Steiner, Andreas, et al. \"How to train your vit? data, augmentation, and regularization in vision transformers.\" arXiv preprint arXiv:2106.10270 (2021).\n\n[4] Touvron, Hugo, et al. \"Training data-efficient image transformers & distillation through attention.\" International conference on machine learning. PMLR, 2021.\n\n[5] Zhai, Xiaohua, et al. \"Scaling vision transformers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission465/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697220512089,
        "cdate": 1697220512089,
        "tmdate": 1699635973026,
        "mdate": 1699635973026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qVYbcqnCuz",
        "forum": "cmAIfTK6fe",
        "replyto": "cmAIfTK6fe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission465/Reviewer_98mC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission465/Reviewer_98mC"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an adaptation of the very recent retention network architecture to vision tasks, in particular for image classification. Retention Networks are a variant of transformer that is half way to RNN and pose some advantages to auto-regressive tasks. The proposed model adapts the architecture minimally (removing positional encoding / learnable gating function...) and trains models on ImageNet1k classification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper focuses on a new and promising line of models.\n\nThe paper narrative is clear: the goal is stated clearly, and the experiments are well targeted.\n\nThe proposed models have wide applicability."
            },
            "weaknesses": {
                "value": "One of the main appeals of RetNets is that it can handle the auto-regressive training and use training parallelization (one single fwd pass with n tokens leads to n loss terms). The auto-regressive nature of the task is however lost in vision applications. Thus I do not see how this is a good fit, and how the arguments about training parallelism are valid.\n\nPerformance seems to be ok when compared to ViT, but slightly more advanced architectures like PVT already surpass the proposed architecture by wide margins. The proposed architecture is also based on a pyramidal design, so the more direct comparison is PVT rather than ViT. \n\nClaims about throughput are not adequately tied to performance. We only see an isolated result in Table 1 with larger image sizes, but there are no comparisons to other methods with larger image size in terms of performance - only Fig. 2 compares throughput against ViT. However, ViT has been thoroughly improved in terms of throughput, and how it scales in terms of perf is not included in the paper.\n\nI was expecting experiments beyond ImageNet1k - after all there isn't a lot of technical novelty so the empirical evaluation has to be top notch.\n\nMinor:\n\"D_h is a scaling factor to balance the compute and parameter counts\" D_h in eq 7 cannot have this effect. Is D_h used elsewhere?"
            },
            "questions": {
                "value": "Performance against competition is a bit hard for the rebuttal - but maybe I misunderstood something that the authors would like to clarify. The throughput claims however have more margin to be expanded. And maybe the authors have some considerations on why this type of model is adequate for vision - and in particular why the \"training parallelism\" claim has an impact for a classification loss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission465/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681009620,
        "cdate": 1698681009620,
        "tmdate": 1699635972921,
        "mdate": 1699635972921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q62zdscmCn",
        "forum": "cmAIfTK6fe",
        "replyto": "cmAIfTK6fe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission465/Reviewer_A5wE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission465/Reviewer_A5wE"
        ],
        "content": {
            "summary": {
                "value": "This work (ViR) adopts the newly emerging RetNet architecture on the image classification task. The authors tested the architecture on the image classification task with the ImageNet dataset and achieved better accuracy numbers than the vanilla visual transformers (ViT) of similar size and computes. The authors also conducted throughput studies between ViRs and ViTs and claims ViR can scale favorably to larger image resolutions in terms of throughput and memory consumption."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is the first few works if not the very first, applying the newly emerging RetNet architecture to the image classification task.\n2. The accuracy numbers seem better than ViT with the controlled number of parameters and FLOPs."
            },
            "weaknesses": {
                "value": "1. The work partly focuses on inference performance, which is very worth studying since that\u2019s one of the biggest benefits the retention structure could bring. However, the comparisons on the throughput are not detailed enough. The comparison is only conducted on a very large high-power GPU with a large memory size. It would be nice to show how the throughput changes w.r.t. Input size on small platforms such as GPUs with small memory. In addition to that, the throughput numbers are not only dependent on the algorithm design but also on many other moving parts such as the underlying implementations [C1].\n2. The work is rather straightforward. The most advantages are inherited from the RetNet architecture. That is fine if enough reasonings and analysis are given for the specific task (here image classification) or studying the generality of multiple vision tasks are provided. At the moment they are lacking.\n3. The authors claim in the contributions that this can be used as a general vision backbone. However, without testing on at least certain relative position critical tasks such as detection, it is too early to draw this conclusion. And it is not straightforward for RetNet. \n\n[C1] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
            },
            "questions": {
                "value": "1. Could the authors provide an intuition despite the patches of an image are not treated equally, namely they are assigned with different decay values according to the ordering, ViR still outperforms ViT? In other words, if the patch with essential information is masked with a large decay, would the performance be worse? Artificially creating a test set might help to answer this question. \n2. What is the chunk size used in Figure 2 for ViR chunkwise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission465/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission465/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission465/Reviewer_A5wE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission465/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783624432,
        "cdate": 1698783624432,
        "tmdate": 1699635972813,
        "mdate": 1699635972813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tmbT3vWA8n",
        "forum": "cmAIfTK6fe",
        "replyto": "cmAIfTK6fe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission465/Reviewer_uppP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission465/Reviewer_uppP"
        ],
        "content": {
            "summary": {
                "value": "This paper is the pioneering work to apply the concept of retention with dual-form parallel and recurrent representations to Vision Transformers (ViTs). Specifically, ViTs suffer from the issue of extending to high-resolution images due to the quadratic complexity w.r.t the number of image tokens. So this work leverages the retention mechanism to enable fast and memory-efficient deployment while maintaining the training parallelism. Experiments show the competitive results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The target problem is of great interest to the Transformer community.\n\n* The pioneering work to apply the retention mechanism to ViTs and make it work."
            },
            "weaknesses": {
                "value": "1. Although the motivation is to make a larger resolution possible. However, I do not find large-resolution experiments to support the claim. The main result table is still conducted with an image resolution of 224x224. Also, how about the real latency and memory costs? FLOPs may be misleading metrics as it only counts the computation numbers.\n\n2. What are the difficulties when applying the retention mechanism to ViTs? Without sufficient discussion of the challenges and the corresponding opportunities, one can find it hard to identify the novelty of the proposed approach and think that it is a straightforward implementation of retention modules in ViTs.\n\n3. Why does the image-based Transformer have the nature of autoregressive? It is understandable that NLP needs such autoregressive settings to predict future tokens with decoder-based models. While most ViTs are still encoder-based models without decoding stages. So I don't get the intention here why we need to do this."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission465/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812849469,
        "cdate": 1698812849469,
        "tmdate": 1699635972686,
        "mdate": 1699635972686,
        "license": "CC BY 4.0",
        "version": 2
    }
]