[
    {
        "id": "K3vq3JhhDi",
        "forum": "H3UayAQWoE",
        "replyto": "H3UayAQWoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_N5Ty"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_N5Ty"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a framework called PPBench, which evaluates the psychological aspects of LLMs using thirteen scales from clinical psychology, categorized into personality traits, interpersonal relationships, motivational tests, and emotional abilities. The study examines five popular LLMs: text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, it employs a jailbreak approach to bypass safety alignment protocols and test the intrinsic natures of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The introduction of the PPBench provides a structured approach to evaluate the psychological aspects of LLMs, which has not been extensively explored in prior literature. \n2. The paper's interdisciplinary approach, bridging the fields of artificial intelligence, psychology, and social science, is significant."
            },
            "weaknesses": {
                "value": "1. Instructing LLMs to respond with Likert scale numbers oversimplifies their responses and may not capture the richness and nuance of their capabilities. Some psychological aspects are complex and may not be adequately represented by a single number.\n2. The prompt design appears overly simplistic. It raises questions about how the results might vary with the use of different prompts. Additionally, how will the result change based on the utilization of a more complex \"chain-of-thought\" prompt?"
            },
            "questions": {
                "value": "1. The paper's prompt design is straightforward. It would be beneficial to explore how different prompts, possibly more complex or nuanced ones, could affect the results. Can the authors provide insights into the impact of prompt variations on LLM behavior?\n 2. The paper shows variations in personality traits across different roles assigned to LLMs. Can the authors discuss the extent to which these variations reflect real-world applications and user interactions with LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2500/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751598564,
        "cdate": 1698751598564,
        "tmdate": 1699636186538,
        "mdate": 1699636186538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KPTUYDLoVb",
        "forum": "H3UayAQWoE",
        "replyto": "H3UayAQWoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_Rgnn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_Rgnn"
        ],
        "content": {
            "summary": {
                "value": "Large Language Models (LLM) have been recently investigated as artificial agents for their \"human-like\" emerging behaviors, which has led to consider various aspects, from their emotional competence to various forms of artificial personality.\nThis paper introduces  a framework, PPBench (Psychological Portrayal Benchmark), for evaluating the psychological portrayal of LLMs, containing thirteen widely recognized scales categorized into four distinct domains (Personality Traits, Interpersonal Relationships, Motivational Tests and Emotional Abilities).\nThe authors evaluate five variants of two major LLMs, one open source (LLaMA) and one proprietary (GPT), covering variations in model sizes and model updates, plus one safety variation using a 'jailbreak' to bypass internal control mechanisms. \nEach LLM variant is subjected to the questionnaires constituting the PPBench, under the specific constraints that they (LLM) can only respond as Likert scale values rather than text output. \nThe paper goes on to report on detailed experiments comparing LLM on those different domains (for instance, BFI for personality), reporting seven specific findings and discussing the properties of the various scales."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper addresses an important topic in the evaluation of LLMs' \"behavior\" through the use of psychometric techniques. It follows from previous technical work, but also important interdisciplinary discussions on the ability of LLM to demonstrate human-like behavior on a number of issues ranging from emotion recognition to empathy (affective or Theory of Mind). In that sense, the work is relevant to LLM research and, in view of the role of representational aspects of LLM in the emergence of such behavior, should also be relevant to ICLR.\nThe main originality of the paper is to try to integrate, and somehow extend, previous approaches into a comprehensive framework. \nThe rationale is appropriate and credible in suggesting an interest both for Computer Science researchers and Social Science researchers with a reminder in the latter case of the possible use of LLMs to emulate human subjects. \nThe paper is clear about its contributions, and a number of individual findings are indeed of interest:\n- the use of jailbreak methods to circumvent some inherent LLM mechanisms\n- the exploration of personality traits in roles\nThe choice of LLM is relatively limited, but covers one major proprietary LLM and one major Open source one. \nThe authors demonstrate a good awareness of previous work and the references are comprehensive and up to date."
            },
            "weaknesses": {
                "value": "1) The main weakness of the paper rests with its significance considering the amount of previous research in the field, of which the authors are aware as per Section 5 of the paper. This is of particular importance when comparing to work such as Safdari et al. [2023] (in the paper's references) whose methodology might appear more sophisticated than the direct application of questionnaires in this paper.\n2) There is no real discussion of the work limitations, either in terms of actual results and findings, or in terms of the overall framework. In particular considering the latter, it seems questionable that all psychometric aspects of LLM could be considered as equally relevant or justifiable. For instance, emotional competence of LLM (see e.g., Elyoseph et al. [2023]) can be attributed technically to their sentiment analysis ability and, in terms of training data, could be attributed to various sources (including fiction). In the specific case of empathy, it might be appropriate to distinguish between emotional competence and Theory of Mind. For the latter, it is mentioned in Bubeck et al. [2023] (in the paper's references) that GPT-4 has some ToM abilities in particular that of passing a modified version of the Sally Anne test (modified to ensure non inclusion in the training dataset). There was no real discussion on this aspect, not least in relation to the training base in case it includes fiction, following the hypothesized impact of fiction on empathic abilities [Kidd and Castano, 2013]. So, clearly a more in-depth discussion in 3.2.4 would have been welcome.\nAs far as personality is concerned, since this paper comes after previous work and claims to be providing a more consistent framework, one would have expected a more in-depth discussion on the relationship between personality and personas at the technical level, i.e. the assistant roles that might be activated under certain circumstances, in particular as the \"Likert Prompt\" of page 4 makes an explicit reference to the LLM being/acting as a \"helpful assistant\". Such embedded personas can be reflected in the high scores of Agreeableness throughout the LLM, which even jailbreaking fails to decrease below Crowd average. \n\nKidd, D.C. and Castano, E., 2013. Reading literary fiction improves theory of mind. Science, 342(6156), pp.377-380.\nElyoseph Z, Hadar-Shoval D, Asraf K and Lvovsky M (2023) ChatGPT outperforms humans in emotional awareness evaluations. Front. Psychol. 14:1199058."
            },
            "questions": {
                "value": "1) Have you considered the impact of alignment interventions such as RLHF, which might differ across models and implementations?\n(This point has been raised as a difference in behaviour between GPT-4 and GPT-3 on other LLM abilities)\n2) You mentioned the possibility of \"discovering the relation between psychometric results and the training data inputs.\": could you illustrate this potential by discussing the amount of fictional material (e.g. novels) reported, officially (developers) or unofficially (third parties) to form part of the training data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2500/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2500/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2500/Reviewer_Rgnn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2500/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767840640,
        "cdate": 1698767840640,
        "tmdate": 1699636186451,
        "mdate": 1699636186451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yO4CpRsBtF",
        "forum": "H3UayAQWoE",
        "replyto": "H3UayAQWoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_PbNV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2500/Reviewer_PbNV"
        ],
        "content": {
            "summary": {
                "value": "The main contribution of the work is an LLM benchmark dubbed PPBench, which aims to evaluate psychological portrayals (i.e., presenting traits/behaviors by an LLM that relate to mental/emotional states). The benchmark is built on 13 seminal scales across 4 domains: personality traits, interpersonal relationships, motivational traits, and emotional abilities. They then test 5 LLMs on this benchmark, using a recent jailbreak method to further uncover LLM abilities. They also compare to other downstream tasks to validate the benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The work presents a strong motivation for the research problem along with a solid case for psychometrics as \"the how\", gives a compelling case for the selected scales for the benchmark, and ultimately interesting empirical results testing several modern LLMs. In my opinion, this is an under-explored area of LLM research and having some evaluation metrics is a step in the right direction (though Goodhart's law beware).\n* The work performs robust statistical testing for the scales and also takes additional steps to account for model context for output reliability, e.g. randomization of question sequences."
            },
            "weaknesses": {
                "value": "* The conclusions drawn relative to any \"average human population\" performance is suspect. The human benchmarks in most cases are rather weak because of their locale/cultural biases and small sample sizes, e.g. \"six high schools in China\" for BFI, \"undergraduate psychology students from the United States\" for DTDD, or \"Hong Kong students\" in ICB. All of these are taken from seminal works (Table 6) are they aren't the only the studies that use each of the scales, so I think scales themselves are okay especially when considered as an array; however, the interpretation of experimental results then becomes much weaker because of these biases and small-N in the human benchmarks. I think the authors did make an honest attempt about being transparent about the demographic distributions in the Appendix, but in the main paper discussion I think the claims are still overreach and/or require additional caveats.\n* It would've been good to have some brief discussion on the prompt design impact, e.g. why was \"helpful assistant\" necessary? Do things break otherwise? Such a persona prompt may already implies certain caveats on psychological portrayal conclusions, e.g. only when an LLM is \"acting\" like a \"helpful persona\" is a \"empathic\" as defined/measured by the EIS, WLEIS, and ES."
            },
            "questions": {
                "value": "* My main question is whether this work is well-aligned with the ICLR venue. I generally liked the work's high-level framing and contributions, but it is a bit of a slight departure from other work that normally appear at ICLR. It crosses the boundaries between social sciences and technical domains\u2014and indeed such topics are important, but the question is more whether ICLR is the right venue for that to happen. I could see the work being of interest to the ICLR community, but I could also see critiques expecting additional technical rigor. I net out in favor, but I'm opening this line of inquiry because I'm curious to hear the authors articulate their own views.\n* temperature=0 is suspect. I get that this gives reproducibility, but given the paper was already doing good randomization and statistical testing. I don't feel like it would've been that farfetched to just use a temperate of 0.01 across the board (to align with LLaMA 2 experiments) or even higher and just do several replications. The presented results currently are already not comparable across LLMs because of the different temperature used for LLaMA 2, why was this not done?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think it's important that this paper include a statement that achieving certain levels of performance on the proposed benchmark does not imply *fitness* for related use cases. I think it's important to distinguish the (good) framing of the paper of psychometrics for LLMs\u2014which focuses on a scientific inquiry of understanding LLMs\u2014from the applicability of an LLM for say automated counseling or companionship use cases, simply because it clears a high score on the Emotional Abilities tests (Section 3.2.4. / Table 4). A high performance on the benchmark benchmark should not be seen as a *certification for use*."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2500/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802582143,
        "cdate": 1698802582143,
        "tmdate": 1699636186362,
        "mdate": 1699636186362,
        "license": "CC BY 4.0",
        "version": 2
    }
]