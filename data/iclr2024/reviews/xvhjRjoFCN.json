[
    {
        "id": "OGCjbbrCnq",
        "forum": "xvhjRjoFCN",
        "replyto": "xvhjRjoFCN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Bi-Directional cross-attention to model the interactions of the visual tokens. Experiments on ImageNet1K and ShapeNetPart are performed to evaluate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. Cross-attention has been widely used for years and the proposed method is simply some combination of cross-attention operation.\n2. The experimental results are not very impressive. The accuracy on ImageNet is only 82.0, which is not competitive."
            },
            "questions": {
                "value": "see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7336/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7336/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_bDbJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698567200893,
        "cdate": 1698567200893,
        "tmdate": 1699636877564,
        "mdate": 1699636877564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nucesrF9Ll",
        "forum": "xvhjRjoFCN",
        "replyto": "xvhjRjoFCN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_qx3u"
        ],
        "content": {
            "summary": {
                "value": "This research paper provides an enhancement to the Perceiver architecture that employs latent queries for the distillation of input tokens. The main novelty introduced is a bidirectional cross-attention module aimed at reducing computational demands. The authors analyze an architecture that iteratively stacks query-to-token and token-to-query cross-attention modules and find symmetry between these two attention values, suggesting that these two iteratively stacked modules can be merged into one. Therefore, a new bidirectional transformer architecture that only scales linearly with the input tokens as a means of dealing with general modal input data. This replacement results in a reduction of computational cost by approximately one-third, compared to iterative stacking cross-attentions, while also achieving higher accuracies.\n\nThe improved method demonstrates an impressive 82.0% accuracy for classification tasks on ImageNet-1K using compact models. These models require only a small fraction of the FLOPS compared to the original Perceiver. The paper also provides verification tests on more generalized input modalities, reinforcing the versatility and effectiveness of the proposed enhancements to the Perceiver architecture."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of introducing bidirectional attention to replace the iterative stacking cross attention is both innovative and simple. \n2. The way that the authors present their idea is also appreciated. An analog between the latent queries and \"what\" queries, and that between the input tokens and the \"where\" information, is first presented. Then, the symmetry between the \"what\" and \"where\" tokens are exemplified to suggest the improvement of the bidirectional attention. Overall, I enjoy reading this paper, and it is easy to follow.\n3. The bi-directional cross-attention is effective in reducing the computational cost of the Perceiver architecture and increasing its performance. It achieves the same performance with only a fraction of FLOPS."
            },
            "weaknesses": {
                "value": "- Despite its effectiveness, the motivation is more from an intuitive analogy of \"what\" and \"where\" tokens than a comprehensive theoretical or experimental conclusion. Only an image classification task is presented when analyzing the symmetry between iterative cross attentions between \"what\" and \"where\" tokens. There could also exist many others scenarios, where these cross attention value may violate the symmetry property. For example, the \u201cwhat\u201d tokens would attend to the context background tokens when detecting small object in the image, whereas these attended \u201cwhere\u201d tokens would more likely to attend to other \u201cwhat\u201d tokens in the next cross attention. Therefore, it is less convincing to conclude the \u201csymmetry\u201d behavior of the \u201cwhat\u201d and \u201cwhere\u201d tokens from a single illustration.\n    \n\n- It is surprising and strange in Table 1(a) that the most performance gain is brought by the naive approach that sequentially stacking two cross attentions with reverse orders by exchanging the query and key positions (+ 11 acc); whereas bidirectional only brings in an additional 0.8 acc. This result seems a bit contradictory with the emphasis of the paper on the bi-directional attention. In this regard, a more important part about the reason of the largely improved performance of the sequential cross attention deserves more detailed analysis. Specifically, this phenomenon would highlight the importance of refining the image tokens instead of fixing them as in Perceiver.\n    \n- The FLOPSs and Params reported in Table 1(a) is confusing. The FLOPSs and Params of bi-directional cross attention are even larger than that of sequential cross attention. However, \u00a0it is described that the implementation of bidirectional cross attention saves 1/3 parameters compared to naive sequential cross attention. Results in Table 1(a) contradicts this statement."
            },
            "questions": {
                "value": "See the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672820536,
        "cdate": 1698672820536,
        "tmdate": 1699636877405,
        "mdate": 1699636877405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sx6gfpIAds",
        "forum": "xvhjRjoFCN",
        "replyto": "xvhjRjoFCN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a bi-directional cross-attention Transformer (BiXT) that can process long sequences efficiently and effectively by using a small set of latent vectors to represent the \u2018what\u2019 and input tokens to represent the \u2018where\u2019 of the data. At the core of BiXT is the bi-directional cross-attention module that simultaneously refines latent vectors and input tokens. Compared to sequential cross-attention, the bi-directional cross-attention module leverages the symmetry of attention patterns between latent vectors and input tokens to reduce computational cost and memory consumption.  The authors evaluate BiXT on image classification, semantic image segmentation, and point cloud part segmentation.  They show that BiXT outperforms comparable methods in the low-FLOP regime and can easily integrate modality-specific components to improve performance further."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed bi-directional cross-attention has a simple and neat design\n2. Evaluations are conducted on two modalities, i.e., images and point clouds.\n3. The paper is well-written, hence easy to follow"
            },
            "weaknesses": {
                "value": "1. Despite the simple and neat design, the strength of the proposed method, bi-directional cross-attention, is unclear. Compared to using two uni-directional cross-attention modules sequentially, the system-level accuracy, FLOPs, and memory requirements are all similar (Table 1 on page 6). \n\n2. Insufficient comparison with some of the latest vision backbones. The methods in image classification (Table 2) and semantic segmentation (Table 3) are somewhat outdated. Many works were proposed to overcome the quadratic complexity of multi-head self-attention, such as MaxViT [1], BiFormer [2], and especially DualViT [3], which has a very similar design to BiXT. The performances of BiXT are not attractive if these approaches are included in comparison. Why are these methods not comparable with BiXT?\n\n3. Lack of experiments with larger models. It is unclear why the comparisons are positioned in a low-FLOP regime (Table 2). BiXT seems not to be specially designed for lightweight models, and the budgets of BiXT-Ti/8 and BiXT-Ti/4 in the final section of Table 2 are sufficient to cover training larger models with more parameters. It may be better to demonstrate the effect of model scaling.\n\n[1] Maxvit: Multi-axis vision transformer, ECCV 2022.   \n[2] BiFormer: Vision Transformer with Bi-Level Routing Attention, CVPR 2023.    \n[3] Dual Vision Transformer, TPAMI 2023."
            },
            "questions": {
                "value": "See the weaknesses part. Overall, I appreciate the simple and neat design of the bi-directional cross-attention. Still, I would like more clarification on its strengths and the experimental settings in the rebuttal. I will raise my rating if these concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7336/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7336/Reviewer_mPnx",
                    "ICLR.cc/2024/Conference/Submission7336/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827869954,
        "cdate": 1698827869954,
        "tmdate": 1700634642019,
        "mdate": 1700634642019,
        "license": "CC BY 4.0",
        "version": 2
    }
]