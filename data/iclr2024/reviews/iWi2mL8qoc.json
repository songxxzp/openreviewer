[
    {
        "id": "QC3kbBCRhO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_UsMm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_UsMm"
        ],
        "forum": "iWi2mL8qoc",
        "replyto": "iWi2mL8qoc",
        "content": {
            "summary": {
                "value": "This paper introduces a model for image inpainting. It identifies a gap in prior methods, noting that they often face challenges due to the lack of long-range dependencies and difficulty in capturing contextual information. To address these issues, the authors introduce a multi-scale window-based transformer designed to capture the effects of varying window sizes and gather essential contextual information. Additionally, they propose a selective mask update strategy that extracts crucial data from features processed by self-attention, leading to the generation of higher-quality results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The proposed framework outperforms the compared baselines in terms of FID.\n\n- Qualitatively, the proposed model surpasses the compared baselines."
            },
            "weaknesses": {
                "value": "- Marginal improvement: As shown in Table 1, the proposed method offers only a slight enhancement compared to MAT.\n- Limited baselines for comparison: The study does not compare with diffusion models, such as stable diffusion inpainting.\n- Limited novelty: Some existing works share similar insights with the proposed multi-scale window and selective mask update."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697233682774,
        "cdate": 1697233682774,
        "tmdate": 1699636970393,
        "mdate": 1699636970393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s3wBLwPoPK",
        "forum": "iWi2mL8qoc",
        "replyto": "iWi2mL8qoc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_Z4FZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_Z4FZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an image inpainting method based on the multi-scale window transformer. Moreover, Multi-Scale Window-based Polarized Self-Attention (MW-PSA) and multi-head contextual attention (MCA) are used to further improve the performance. The Selective Mask Update (SMU) is proposed to reflect critical information from the features that undergo self-attention. The proposed method enjoys good performance, but it suffers from obviously limited novelty."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The proposed method is easy to follow and achieve good performance in the comparison shown in the paper."
            },
            "weaknesses": {
                "value": "1. The main concern is the limited novelty. Both multi-scale training and Windows-based attention are widely used in computer vision. So the combination of them is naive and intuitive to achieve slightly better performance. However, the authors did not provide more in-depth discussions to make their claims clearer. For example, what is the key difference between multi-scale+window-based attention and multi-scale dilated convolution used in AOT-GAN[1]?\n2. Other techniques, such as MW-PSA and MCA, were all proposed in previous works. For the SMU, the authors miss enough discussions and clarifies about the other works of adaptive mask adjusting, such as Partial Conv[2], Gated Convolution[3], and so on.\n2. The writing of this paper is non-standard. The egregious citation issues are evident throughout the entire document, making it difficult to establish the credibility and reliability of the sources used. 1) Incomplete citations: a substantial portion of the references lack crucial information such as the name of the original publisher and the journal name.  2) Incorrect publication years: the paper \"High-fidelity pluralistic image completion with transformers\" was published in CVPR2021, but the authors marked that it was published in 2018 (without any publisher information). 3) Wrong citations: the authors said \"For these reasons, recent advancements in image inpainting have led to the proposal of Transformer-based models (Yan et al., 2018; Zhang et al., 2018a)\", but both (Yan et al., 2018) and  (Zhang et al., 2018a) are completely irrelevant to attention or transformer.\n\n[1] Zeng Y, Fu J, Chao H, et al. Aggregated contextual transformations for high-resolution image inpainting[J]. IEEE Transactions on Visualization and Computer Graphics, 2022.\n\n[2] Liu G, Reda F A, Shih K J, et al. Image inpainting for irregular holes using partial convolutions[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 85-100.\n\n[3] Yu J, Lin Z, Yang J, et al. Free-form image inpainting with gated convolution[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 4471-4480."
            },
            "questions": {
                "value": "It is strongly recommended that the authors revise and correct the citation and referencing issues. Furthermore, a comprehensive review of the paper's content for factual accuracy and relevance to the topic should be considered to ensure the overall quality of the research."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697966799897,
        "cdate": 1697966799897,
        "tmdate": 1699636970266,
        "mdate": 1699636970266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cT8rZoF8Zz",
        "forum": "iWi2mL8qoc",
        "replyto": "iWi2mL8qoc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_NdAo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_NdAo"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a Multi-Scale Window-based Transformer model dedicated to achieving high-quality image inpainting. This model is devised to effectively comprehend contextual information by incorporating a transformer network featuring multi-scale windows. To enhance this model's capabilities, a modified polarized self-attention network is employed to align with multi-window scales, optimizing the assimilation of significant contextual information. Moreover, the Selective Mask Update method is proposed to capture essential information from the output features, facilitating timely mask region updates, thus enhancing the utility efficiency of valid data. Experiments show that the proposed model generates superior results compared to other models on the benchmark dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strength:\n1.The paper robustly explicates the design rationale behind the Multi-Scale Window-based Polarized Self-Attention (MW-PSA) mechanism. It effectively validates the application's effectiveness through experiments in large mask inpainting tasks. This marks the first successful implementation of such technologies in inpainting tasks.\n2.The paper introduces an effective strategy for updating valid tokens within masks, which aids in enhancing task efficiency and the quality of generated results.\n3.The presentation of this paper is professional and fluent. It has almost no expression errors and clearly elucidates the authors' contributions."
            },
            "weaknesses": {
                "value": "Weaknesses:\n1. The complete omission of PSNR-oriented metrics in the comparative evaluation is an unreasonable approach. Typically, generative models tend to perform relatively poorly on PSNR-oriented metrics, and the exclusion of discussion around these metrics is perplexing. Transformer-based methods often excel, and for some downstream tasks in inpainting services, the completion results should demonstrate a certain fidelity. To comprehensively showcase the algorithm's performance, it is imperative to include experiments addressing these metrics.\n\n2. The conducted ablation experiments are exceedingly inadequate, providing only qualitative discussions. The scale, details, and ablation experiments related to the U-Net network settings should be included at the module level within the processing pipeline. Crucial details regarding hyperparameters, such as the k value in the Top-k selection of the Selective Mask Update (SMU) approach, merit comprehensive ablation studies.\n\n3.The motivation behind the 'strengthening local textures and enhancing fine details of the images' by simply concatenating U-Net networks is both perplexing and unconvincing. In practice, regression-based CNN methods of this nature are often considered to be the cause of over-smoothing and detail loss.\n\n4.The novelty and innovativeness of the paper are relatively low. Similar designs employing a Multi-Scale Window-based Transformer have been proposed in some articles. Although the algorithm's application tasks and design purposes might differ, the authors have neglected a thorough literature review, completely omitting references to highly similar works such as [1] and [2] . Furthermore, the MW-PSA appears to be a simple modification of the existing PSA under the input mechanism of parallel multi-windows, while the MCA is directly borrowed without substantial innovation.\n[1] Ren, P., Li, C., Wang, G., Xiao, Y., Du, Q., Liang, X., & Chang, X. (2022). Beyond fixation: Dynamic window visual transformer. In\u00a0Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\u00a0(pp. 11987-11997).\n[2] Cheng, R., Zhuang, Z., Zhuang, S., Xie, L., & Guo, J. (2023). MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for 12-Lead ECG Classification. arXiv preprint arXiv:2306.12098."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658102791,
        "cdate": 1698658102791,
        "tmdate": 1699636970163,
        "mdate": 1699636970163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j2uDmW4L1S",
        "forum": "iWi2mL8qoc",
        "replyto": "iWi2mL8qoc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_8FYW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7908/Reviewer_8FYW"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a multi-scale transformer architecture for image inpainting and a Selective Mask Update method for mask updating. Experiments show the effectivenss of the proposed method with a resolution of 256."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The proposed parallel self-attention through multi-dimensional windows to incorporate diverse contextual information, achieving superior performance compared to existing models.\n2. MW-PSA efficiently fuses features obtained through MCA in both channel and spatial dimensions.\n3. The experiments show the superiority compared to other models in both quantitative and qualitative evaluations."
            },
            "weaknesses": {
                "value": "1. This paper aims for high quality image inpainting, but it changed to high-quality video inpainting in conclusion. It is a resubmitted paper? The author should check carefully before submission.\n2. In my opinion, \"high-quality: means higher quality images with higher resoultions, but in this paper, all experiments are only conducted on 256x256. The author should add more experiments on higher resolution to show the effectiveness of their method. Besides, since some methods like CoModGAN are trained on 512x512, it is unfair to compared with them on 256x256.  \n3. Why not compare with diffusion models based methods which show superiority on large missing areas over GAN based methods? The experiments of this paper cannot convince me.\n4. The writing of this paper needs improving: \n\"Places368-standard\" in table 1\nThe mask color is different between row 1 and row2 in Figure 1"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736466656,
        "cdate": 1698736466656,
        "tmdate": 1699636970021,
        "mdate": 1699636970021,
        "license": "CC BY 4.0",
        "version": 2
    }
]