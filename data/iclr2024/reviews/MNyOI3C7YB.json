[
    {
        "id": "3RTKcBiSPq",
        "forum": "MNyOI3C7YB",
        "replyto": "MNyOI3C7YB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce an offline imitation learning algorithm called SEBO, which is centered on the task of learning a reward function from a dataset of expert demonstrations and applying it to unlabeled data to facilitate offline reinforcement learning.\n\nThe key innovation in SEBO lies in its use of a straightforward metric for generating the reward function, without the use of any neural networks. The algorithm employs search algorithms to determine the reward function. Specifically, SEBO constructs a KD-tree based on the expert demonstrations. For each unlabeled data point, the algorithm queries the KD-tree to identify its closest neighbor in the expert dataset and assesses the proximity between them. If the distance is minimal (indicating similarity to the expert trajectory), a high reward is assigned; conversely, if the distance is substantial (indicating deviation from the expert trajectory), a low reward is assigned.\n\nThe paper evaluates the SEBO algorithm across a range of MuJoCo environments and demonstrates its performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written, and the algorithm is studied well on different MuJoCo environments. The authors also conduct a sensitivity analysis with respect to the two parameters $\\alpha$ and $\\beta$\n2. SEBO is efficient, and easy to implement. It costs a minimal overhead over existing Offline RL algorithms, and does not involve training of a new neural network.  \n3. The authors also evaluate it in a scenario where there is only access to observations, a case that might be of real world importance."
            },
            "weaknesses": {
                "value": "**Evaluations restricted to deterministic environments**\n\nAll evaluations performed in this paper are conducted on deterministic environments (transitions in MuJoCo are completely deterministic), the claim that just one expert trajectory is sufficient might not be true if the environments are stochastic. For example, you may have a good (s,a,s\u2019) pair in the dataset, but you may assign a lower reward to it as its not present in the expert demonstration.  I think there is an inherent correlation between the number of expert trajectories needed and the stochasticity of the environment. \n\n**The paper lacks theoretical justifications, which makes understanding some parts a little difficult. For instance,**\n\nThis method might not work in situations where there are multiple ways to solve the same task. \nConsider the following example of a navigation problem, where the goal is to navigate to the destination from start position, and there are two ways to solve this task. One that goes left and reaches the goal, the other that goes right and reaches the goal.  Suppose the expert demonstrations takes the left path, and the unlabeled demonstrations (say from an expert policy as well) are from the right path, SEBO will assign low rewards to all transitions and not learn a good policy while BC will probably work."
            },
            "questions": {
                "value": "1. Is there an inherent assumption being made between the distribution of state-action pairs in the expert demonstrations and the unlabeled dataset? \n2. Why are the experiments on D4RL restricted to medium level task and not conducted on expert and random versions of it? \n3. Does the stochasticity of environments affect the performance of SEBO? \n4. When using point wise matching to determine the reward are you making some inherent assumptions on the transition kernel?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW",
                    "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698168636719,
        "cdate": 1698168636719,
        "tmdate": 1700499573227,
        "mdate": 1700499573227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QrE7JPWxW5",
        "forum": "MNyOI3C7YB",
        "replyto": "MNyOI3C7YB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the \"SEABO\" algorithm, which is an imitation learning algorithm that utilizes an expert dataset along with an unlabeled dataset. SEABO annotates the unlabeled dataset with rewards based on the distance between each state and the closest state in the expert dataset, and then runs an offline RL algorithm on the annotated dataset. The authors show improvements on the D4RL benchmark compared to prior baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes a very simple algorithm that achieves good performance relative to more complex methods. I think this type of work has good value to the community in that it introduces easy-to-reproduce results and discourages over-engineering of methods.\n\n- The paper is clear in presentation and mostly well written."
            },
            "weaknesses": {
                "value": "- There is only empirical analysis of the proposed method. I believe there are certain tasks where SEABO would perform poorly, such as a cliff-walking type of task where there is a precise boundary between what is accetable and what is a failure. \n\n- I hypothesize that the approach will also only work in lower dimensional control environments. This is because the method relies heavily on a  distance function, and this could suffer from the curse of dimensionality in more complex environments.\n\nMinor:\nSearch in the context of RL and planning typically has a slightly different connotation, which is using some type of tree-based or trajectory-shooting method that optimizes some cost function. In this work search is only used to find states close to an expert. I'm not sure what can be done with this in the writing, but I was expecting a method in the former category after reading the abstract. It may be better to replace \"search\" with \"nearest neighbors\" to be more specific."
            },
            "questions": {
                "value": "\"we hypothesize that the transition is near-optimal if it lies close to the expert trajectory\" -> Is this strategy always good or does it have weaknesses? I would imagine that certain environments with discontinuities or discrete events (e.g. a car crash) would not favor this strategy.\n\nThere is another simple baseline commonly used in offline RL, which is sometimes referred to as percent-BC, which is to imitate the top N% of trajectories in the offline dataset (such as N=10 or N=25). As it is somewhat similar in spirit to SEABO, it would be good to see comparisons to this approach.\n\nIs there any effect of the \"curse of dimensionality\" for higher dimensional states?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551986365,
        "cdate": 1698551986365,
        "tmdate": 1699636092658,
        "mdate": 1699636092658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qc7N4XfuXW",
        "forum": "MNyOI3C7YB",
        "replyto": "MNyOI3C7YB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
        ],
        "content": {
            "summary": {
                "value": "The paper proses an approach for imitation learning using nearest-neighbor-based reward computation and offline RL. Given a dataset of unlabeled environment interactions and a single demonstration, they use a KD-tree to compute the distance of each transition to it's nearest neighbor in the demo (euclidean distance) and use this as a pseudo-reward which they can optimize with offline RL. The method is evaluated extensively in D4RL locomotion (and few manipulation) tasks and shows strong performance over prior works."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea is simple and well-explained in the paper. The empirical validation is thorough, with results across many D4RL locomotion tasks and a small number of manipulation tasks. A representative set of baselines is used and the method shows strong empirical performance. The simple proposed method beats more complex alternatives that need to use optimal transport etc."
            },
            "weaknesses": {
                "value": "I don't have many issues with the paper. The method has some limitations (see below), but I don't think this invalidates the contributions of the current paper. \n\nOne minor weakness is that the approach is mostly evaluated on locomotion tasks for which precision is not the most critical. It could be great to evaluate it on a challenging, long-horizon manipulation task to test the limits of the method. For example the IKEA Furniture assembly benchmark could provide a nice test bed and I would be curious how well the proposed method performs.\n\nAnother minor point is that the paper lacks explanation how the euclidean distance is computed on the transition tuple. Do you compute euclidean distance on state, action and next state separately and then sum them? is there a weighting? Providing some details on this would be great!\n\n\n### Commentary on Limitations\n\nThe proposed method seems to have two limitations: \n\n(1) it's unclear how well it would work with visual observations where computing distances and nearest neighbors is a lot more challenging -- the authors mention this as a direction for future work and I agree that this is a limitation shared by many similar works so I wouldn't hold it against this paper. It should be mentioned though that the proposed approach with it's reliance on an analytic distance metric my be more troublesome in such scenarios than e.g. methods that use discriminator-based \"distances\".\n\n(2) it's unclear how well this would work in tasks that require very precise manipulation, since in such scenarios the proposed method would still assign high rewards to states that are only \"nearly\" demonstration states but may fail to perform the task in practice. It may be that methods that match trajectory segments instead of individual transitions fare better here?"
            },
            "questions": {
                "value": "--\n\n# Post Rebuttal Comments\n\nThank you for answering my review.\n\nI appreciate the new experimental results -- performance on Kitchen seems strong and should be included in the paper.\n\nRegarding the furniture assembly benchmark: if offline imitation algorithms struggle in this benchmark, would it be feasible to apply your algorithm in an online context? Also, note that there is a new version of the benchmark (IKEA Furniture Bench, https://clvrai.github.io/furniture-bench/) -- while its focus is on real-world manipulation, it also comes with a simulated counterpart and offline dataset -- you could check that one out as well!\n\nIn any case, thank you for adding the experiments and I maintain my recommendation of acceptance. I also skimmed the other reviews and it seems that all reviewers are in agreement that the paper should be accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "--"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625118704,
        "cdate": 1698625118704,
        "tmdate": 1701037312061,
        "mdate": 1701037312061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gmt4PrHTuS",
        "forum": "MNyOI3C7YB",
        "replyto": "MNyOI3C7YB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for Offline Imitation Learning (IL) that defines a reward function based on the Euclidean distance to the nearest neighbor expert state. The method, called SEABO, uses a KD-tree to efficiently query expert states and compute rewards for all transitions. The resulting problem can then be optimized using an arbitrary offline RL algorithm. The experimental results demonstrated improved performance in several tasks of the D4RL benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed approach is both novel and simple, and its implementation is efficient due to the use of a KD-tree, without the need for training an extra discriminator.\n2. This paper focuses on the context of single-expert-demonstration IL tasks, which is an area of growing interest in the field.\n3. I find the discussion on using different search algorithms in Section 5.4 and Appendix Section C interesting.\n4. The Limitations section in the appendix is highly appreciated, as it provides valuable guidance on tuning hyperparameters and applying SEABO on visual input."
            },
            "weaknesses": {
                "value": "1. I'm concerned about the use of Euclidean distance and would suggest that the authors include references justifying the use of this distance metric. This is crucial because there might be scenarios where states that are close in Euclidean distance are, in fact, far apart when accounting for the transitions within the Markov Decision Process (MDP). This particular challenge doesn't arise in discriminator-based methods, mainly due to the use of an additional neural network during training.\n2. I believe that \"(oracle)\" should be omitted from Table 1-3 and 6. For instance, consider \"IQL (oracle)\": it utilizes the ground truth reward but doesn't rely on expert demonstrations. Removing the ground truth reward and integrating an additional expert demonstration does not necessarily make the task more challenging.\n3. The experiments currently compare with only two Offline RL methods (IQL and TD3_BC). It would be better to include more recent baselines such as Trajectory Transformer [[1]], Diffuser [[2]], or other methods.\n\n[1]: https://arxiv.org/abs/2106.02039\n[2]: https://arxiv.org/abs/2205.09991"
            },
            "questions": {
                "value": "1. Can SEABO utilize alternative distance metrics in place of the Euclidean distance? If so, how much modification is required?\n2. The comparison between ground-truth rewards and the rewards obtained by SEABO (as in Figures 2 and 12) is intriguing. Have you also conducted similar reward comparisons in additional environments, such as AntMaze-v0 and Adroit-v0?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c",
                    "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669045245,
        "cdate": 1698669045245,
        "tmdate": 1700709353902,
        "mdate": 1700709353902,
        "license": "CC BY 4.0",
        "version": 2
    }
]