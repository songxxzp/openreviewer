[
    {
        "id": "Jy3AaMibCl",
        "forum": "ZuflmOaxb7",
        "replyto": "ZuflmOaxb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_GXb4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_GXb4"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the decentralized federated natural policy gradient method for multi-tasks in the infinite-horizon tabular setting. Precisely, all agents have the same transaction matrix but different rewards. Agents communicate with neighbors in a prescribed graph topology. Both federated vanilla and entropy-regularized NPG methods are analyzed with global convergence rates. With exact policy evaluation, non-asymptotic global convergence is guaranteed. With imperfect policy evaluation, convergence rates remain the same when the infinite norms of approximation errors are small."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tTo the best of my knowledge, this is the first work on decentralized FedNPG with convergence analysis.\n2.\tWithout trajectory transmission, the results show that the convergence rates do not show down a lot.\n3.\tWith function approximation, the communication complexity of vanilla FedNPG would be very high. But the natural policy gradient update in the tabular setting has a simple form. It is wise to choose this as no higher-order matrix is involved."
            },
            "weaknesses": {
                "value": "1.\tOnly the tabular setting is studied. The action and state space are discrete and finite.\n2.\tIt is good enough to give convergence performances for previously proposed algorithms (or with minor changes). However, the decentralized FedNPG algorithm is quite new. Some simulations are needed to verify the proposed algorithms.\n3.\tIn practice, it is very hard to be synchronous in each iteration with fully distributed settings. Especially, each agent randomly (categorical distribution) selects one agent to communicate."
            },
            "questions": {
                "value": "1.\tShould the mixing matrix $\\mathbf{W}$ be ergodic? I cannot find a related assumption or discussion, which confuses me with the statement \u201cilluminate the impacts of network size and connectivity\u201d. Can each agent compute independently and do a one-shot average? There is a connectivity rule in (Nedic & Ozdaglar, 2009), but not here.\n2.\tAs the local update is a key point in federated learning, is it possible to compute locally for several iterations without communication?\n3.\tIs it possible to show some simulation results?\n4.\t(Clarification) Are the reward functions deterministic?\n5.\t(Motivation) I personally like this topic, and would like to know more about the motivation. As each agent has its own reward function, why don\u2019t they simply use the local policies instead of the global policy? Does it make sense to force them to use the same policy?\n\nThis work is generally good. I promise to raise the score if questions 1 - 3 are fairly (or partially) addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6851/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6851/Reviewer_GXb4",
                    "ICLR.cc/2024/Conference/Submission6851/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6851/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697873483294,
        "cdate": 1697873483294,
        "tmdate": 1700695191856,
        "mdate": 1700695191856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "15YgIuJbOf",
        "forum": "ZuflmOaxb7",
        "replyto": "ZuflmOaxb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_tCM8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_tCM8"
        ],
        "content": {
            "summary": {
                "value": "The study employs the federated NPG method to address a multi-task reinforcement learning challenge. In their setup, there is reward heterogeneity among various agents, though they share identical transition kernels. Two algorithms, namely the vanilla and entropy-regularized FedNPG, are introduced to tackle the decentralized FRL issue within a graph topology. Additionally, the authors offer theoretical assurances for these algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The paper is well-written and clearly presented\n2. The authors provide a clear comparison of their findings with existing results.\n3. The analyses are solid."
            },
            "weaknesses": {
                "value": "* The study lacks simulation results that would validate the efficacy of the presented algorithms.\n\n* The framework presented is relatively simplistic, being limited to the tabular scenario with deterministic gradients. There's no consideration for function approximation or the presence of noise.\n\n* A notable omission is the lack of multiple local updates in the algorithms, which are the key features in Federated Learning (FL). Heterogeneity only exists  when there are more than one local updates. Consequently, the authors did not examine the influence of heterogeneity between agents, since their algorithms do not incorporate the multiple local update steps."
            },
            "questions": {
                "value": "* How would the algorithms behave if the transition kernels differ between agents?\n\n* Regarding agents' motivation to participate in the federation, prior studies [1][2][3][4] have explored the incentives in terms of linear or sublinear speedup. Do the proposed algorithms match this expected speedup in convergence rate as the number of agents increases?\n\n[1] Fan, Xiaofeng, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low. \"Fault-tolerant federated reinforcement learning with theoretical guarantee.\" Advances in Neural Information Processing Systems 34 (2021): 1007-1021.\n\n[2] Khodadadian, Sajad, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. \"Federated reinforcement learning: Linear speedup under markovian sampling.\" In International Conference on Machine Learning, pp. 10997-11057. PMLR, 2022.\n\n[3] Wang, Han, Aritra Mitra, Hamed Hassani, George J. Pappas, and James Anderson. \"Federated temporal difference learning with linear function approximation under environmental heterogeneity.\" arXiv preprint arXiv:2302.02212 (2023).\n\n[4] Shen, Han, Kaiqing Zhang, Mingyi Hong, and Tianyi Chen. \"Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup.\" IEEE Transactions on Signal Processing (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6851/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844481897,
        "cdate": 1698844481897,
        "tmdate": 1699636794175,
        "mdate": 1699636794175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IApxPWP3OC",
        "forum": "ZuflmOaxb7",
        "replyto": "ZuflmOaxb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_svj3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_svj3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes federated vanilla and entropy-regularized natural policy gradient methods under softmax parameterization. Some extensibility properties are given or proven, including global convergence and etc. Overall, this paper is well-written, and it has clearly expressed their work and the author's importance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Complete work with well-designed algorithms and theoretical analysis. The authors have considered a less common but easily thought of issue, i.e., multi-task RL."
            },
            "weaknesses": {
                "value": "It is easy to be considered as a combination of multiple existing works with not clearly discussed motivation. The most important issue is the lack of numercial experiments which could prove the efficiency of the proposed algorithms. The proposed theoretical results are overclaimed a bit, for the reason that there should be some assumpotions on the the structural form of the policy $pi$, like (107), in order to obtain the global covergence."
            },
            "questions": {
                "value": "1. More experiments to show the efficiency of the proposed algorithms;\n2. The convergence results should be improved, or the contributions should be properly clarified;\n3. More comparison with distributed optimization methods should be discussed, especially some convergence results. For the reason that maybe there are already some global convergence results for general distributed optimization problems (with multi-task RL as a special case)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "~"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6851/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904709053,
        "cdate": 1698904709053,
        "tmdate": 1699636794046,
        "mdate": 1699636794046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Vej4Q5alY",
        "forum": "ZuflmOaxb7",
        "replyto": "ZuflmOaxb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_P1H3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6851/Reviewer_P1H3"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the federated RL problem with multi-task objectives. It develops NPG based algorithms and provides non-asymptotic convergence guarantees under exact policy evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written. The problem setting and formulation are clearly presented, and the ideas are well explained."
            },
            "weaknesses": {
                "value": "- From the algorithm description, it seems that the agents would need to communicate and share their information with others. This seems to be different from the motivation of using a federated algorithm, where usually agents share parameters with a central entity for aggregation. Please elaborate. \n- The technical results need more explanation. Right now it is quite dry, in the sense that there is not much discussions."
            },
            "questions": {
                "value": "- From the algorithm description, it seems that the agents would need to communicate and share their information with others. This seems to be different from the motivation of using a federated algorithm, where usually agents share parameters with a central entity for aggregation. Please elaborate. \n- The technical results need more explanation. Right now it is quite dry, in the sense that there is not much discussions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6851/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699372106098,
        "cdate": 1699372106098,
        "tmdate": 1699636793919,
        "mdate": 1699636793919,
        "license": "CC BY 4.0",
        "version": 2
    }
]