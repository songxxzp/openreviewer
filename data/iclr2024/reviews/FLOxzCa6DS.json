[
    {
        "id": "QUHlgrD3Fc",
        "forum": "FLOxzCa6DS",
        "replyto": "FLOxzCa6DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_2m5o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_2m5o"
        ],
        "content": {
            "summary": {
                "value": "This paper studies applying fine-tuning techniques and public data in DP-SGD to privately train diffusion model. Resorting to public data for pre-train and applying DP-SGD to only fine-tune a small fraction of model parameters, the authors show the improvement both from training time and the performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and all the ideas are clearly presented. As an empirical paper, the authors detail the selections of hyper-parameters and model architecture."
            },
            "weaknesses": {
                "value": "Though the experiments are comprehensive and solid, the key ideas of this paper are relatively simple. The curse of dimensionality of DP is already a well-known problem, and for implementation of DP-SGD, especially in supervised learning tasks, pretrain with public data and fine-tuning have been extensively studied. It is interesting to see the application of those techniques in privately training diffusion model, but I am afraid that the new insights provided by this paper are not enough. For example, with the assistance of public data, one may also consider further improvement such as gradient embedding or low-rank adaptation in [1,2]. \n\nIn addition, since the authors propose fine-tuning only a small fraction of parameters, the produced efficiency improvement with a smaller noise required are clearly-expected consequences. I also note that fine-tuning may also come with a tradeoff. For example in Table 7, in low privacy regime, DP-DM with full training can out-performance proposed methods. But such tradeoff seems not being fully studied.  \n\n \nMinor issue: The main document should only contain a 9-page main body and references, while the authors also attach the appendix. \n\n[1] Differentially private fine-tuning of language models\n[2] Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning"
            },
            "questions": {
                "value": "I have several suggestions for the authors to further improve this paper. At a high-level, as an empirical paper, I would suggest a more comprehensive study on the influence of network architecture selection on DP-SGD; Also, to refine the fine-tuning results, the authors can also through searching to determine the fraction or layers of the diffusion model to fine-tune, as some general instruction for future work to design the fine-tuning strategy; Finally, given that public data is assumed, the author may consider how to fully exploit to refine the DP-SGD with less noise using embedding or low rank approximation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Reviewer_2m5o"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698445504387,
        "cdate": 1698445504387,
        "tmdate": 1699636410507,
        "mdate": 1699636410507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7oW7FWmxG2",
        "forum": "FLOxzCa6DS",
        "replyto": "FLOxzCa6DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_muGo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_muGo"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a privacy-preserving diffusion model achieved by fine-tuning the attention layer of the latent-diffusion model. The authors pre-trained the model using publicly available data to avoid consuming the privacy budget."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper demonstrates state-of-the-art results in image generation by allowing pre-training with publicly available data."
            },
            "weaknesses": {
                "value": "The most significant thing lack of novelty: The paper simply fine-tune the pre-trained public model with a similar technique in You & Zhao (2023). This is unremarkable because reducing the fine-tuning space is mentioned in You & Zhao (2023).\n\nHere are minor weaknesses:\nThe notations $\\Delta$ and $\\nabla$ used for the encoder and decoder might be confusing and could be clarified. There are some typographical errors, such as \"$B$\" instead of \"$B_p$\" in Algorithm 1. Furthermore, in Table 2, it's unclear how the privacy budget of synthetic data was handled when reporting 88.3% accuracy.\nFor the choice of public data, due to the unavailability of private data, you should not calculate FID between private and public data. Since EMNIST for MNIST dataset and ImageNet for other dataset are commonly used for public dataset, using them makes sense. However, calculating FID to choose public data may give readers the impression that private data was accessed."
            },
            "questions": {
                "value": "Correct me if I am wrong. From my understanding, the conditioning embedder appears to function effectively only for language prompts and not for class conditions.\nCan you explain how the conditioning embedder works when it has been pre-trained with public data with different labels than the private data? Does the model treat it as a class embedder, or does it treat it as random initialization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Reviewer_muGo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698475772536,
        "cdate": 1698475772536,
        "tmdate": 1700736222729,
        "mdate": 1700736222729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LsFZUgRBnq",
        "forum": "FLOxzCa6DS",
        "replyto": "FLOxzCa6DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_c8Le"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_c8Le"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to train differentially private latent diffusion models for generating DP synthetic images. Compared to training diffusion diffusions on the image spaces, training latent diffusion models reduces the number of parameters and therefore could be more friendly (in terms of computational cost and privacy-utility trade-off) in DP settings. To further reduce the number of training parameters, the paper proposes to only fine-tune the attention layers and the condition embedders. Experiments show that the proposed method achieves state-of-the-art privacy-utility trade-offs on several benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-written.\n* Given the widespread and successful use of latent diffusion models in non-DP settings, exploring whether they can help DP synthetic data generation is very important and timely. This paper demonstrates a practical pathway for doing it. The open-source code could be very useful to the community.\n* The results look promising."
            },
            "weaknesses": {
                "value": "* The proposed approach is a straightforward application of existing techniques."
            },
            "questions": {
                "value": "Although the proposed approach lacks novelty, DP latent diffusion models could be of great interest to the community and the results look promising. Therefore, I am leaning toward a positive score. However, it would be great if the authors could clarify the following questions and I will adjust the score accordingly.\n\n* The paper proposes to fine-tune only the attention layers and the condition embedders to reduce the number of fine-tuning parameters. One of the most commonly used approaches in both DP and non-DP communities is to do LoRA, adapter, or compacter fine-tuning (see [3] for an example). It would be better to comment on or experimentally compare with such approaches.\n\n* Table 7 in the appendix shows the GPU hours for training DP-LDM and DP-DM. Could you clarify if that includes the pre-training cost for both methods? If yes, it would be clearer to break down the time into pre-training and fine-tuning stages. If not, it would be better to include pre-training costs as well, as at least in the experiments of this paper, customized pre-training for each dataset has to be done.\n\n* Introduction claims that \"DPSGD ... does not scale well for large models that are necessary for learning complex distributions.\" It is not necessarily true. Prior work has demonstrated that DP-SGD works well with large language models. See [1,2] for some examples.\n\n* What does \"average\" and \"best\" mean in Table 4?\n\n* Section 5.3 discusses the process of selecting pre-training datasets. However, this selection process needs to use private data and therefore is NOT DP. Please refer to [4] for an example of how to select pre-training **dataset** in a DP fashion, and [5] for an example of how to select pre-training **samples** in a DP fashion. According to the results in the prior work, I guess that the selection between SVHN, KMNIST, and MNIST would only incur a small privacy cost. Still, the paper should at least discuss this issue (i.e., the privacy cost of this dataset selection step is ignored) and the related work, if not redoing the experiments.\n\n* Table 8 in the appendix: what does \"Best\" mean in \"Best CNN accuracy\"?\n\n* Table 10 shows that the results are sensitive to the selection of fine-tuning layers, especially in regimes with a high privacy budgets. It would be better to discuss hypotheses about why fine-tuning 9-16 layers is the best and provide recommendations for practitioners on how to choose this hyper-parameter for new datasets.\n\n* The line after Eq. 1: x_t is not defined.\n\n* Section 2.2 states that \"A single entry difference could come from either replacing or removing one entry from the dataset D.\" While both definitions (replacing vs. removing) are valid and used in practice, they result in different DP bounds as the sensitivity is different. The paper should be clear which definition is used in the experiments.\n\n* The paragraph after Eq. 3: in the definition of K and V, should \\phi(x) be \\phi(y)?\n\n* Step 4 in algorithm 1: N(0, \\sigma^2C^2I) should be  1/B N(0, \\sigma^2C^2I)\n\n* Related work: a space is missing in \"(GANS)(Goodfellow et al.,\" \n\n* Related work: \"Lin et al. (2023) do privatize\" should be \"Lin et al. (2023) do **not** privatize\"\n\n* Section 5: \"complexity : the\" should be \"complexity: the\"\n\n[1] Li, Xuechen, et al. \"When Does Differentially Private Learning Not Suffer in High Dimensions?.\" Advances in Neural Information Processing Systems 35 (2022): 28616-28630.\n\n[2] Anil, Rohan, et al. \"Large-scale differentially private BERT.\" arXiv preprint arXiv:2108.01624 (2021).\n\n[3] Yu, Da, et al. \"Differentially private fine-tuning of language models.\" arXiv preprint arXiv:2110.06500 (2021).\n\n[4] Hou, Charlie, et al. \"Privately Customizing Prefinetuning to Better Match User Data in Federated Learning.\" arXiv preprint arXiv:2302.09042 (2023).\n\n[5] Yu, Da, et al. \"Selective Pre-training for Private Fine-tuning.\" arXiv preprint arXiv:2305.13865 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819611763,
        "cdate": 1698819611763,
        "tmdate": 1699636410320,
        "mdate": 1699636410320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SH3Fslqpw7",
        "forum": "FLOxzCa6DS",
        "replyto": "FLOxzCa6DS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_Tmmk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4377/Reviewer_Tmmk"
        ],
        "content": {
            "summary": {
                "value": "The paper presents DP-LDM, a differentially private latent diffusion model for generating high-quality, high-dimensional images. The authors build upon the LDM model (Rombach et al., 2022), and propose to pre-train the LDM with public data and fine-tune part of the model on private data. They evaluate DP-LDP on several datasets and report promising results compared with the prior SOTAs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors identify the difficulty of scaling up DP DMs and propose a parameter-efficient approach targeting at the issue. \n- The authors claim new SOTA results for generating high-dimensional DP images, including those conditioned on text prompts, which is new."
            },
            "weaknesses": {
                "value": "I have no major complaints about the paper. The paper reads incremental, but it does a decent job in finding the correct hammer, which does lead to promising results. I list below some issues where the authors can improve on. I'll consider raising my score if the authors can properly address them.\n\n1. Missing references:\n  - On the model architecture: U-Net [1], transformers [2]. \n  - On the dataset: CelebA-HQ [3]\n  - On the properties of DP: [4]\n  - On privacy preserving data synthesis: [5]\n2. Evaluation\n  - The authors presented the accuracy results only on CIFAR-10 (in Table 2). All the remaining results on other datasets (Tables 3,4,5) are for FID. However, FID can at most be regarded as a fidelity metric, serving as a proxy for the utility of the synthetic data. It would be most straightforward to directly present the utility results, i.e., accuracy on the classification task. Can the authors add the accuracy results on CelebA for better interpretation of the results? (nit: \"we see a significant drop in accuracy as shown in Table 3.\" -- Table 3 is about FID.)\n  - Ghalebikesabi et al. [6] evaluated the high-dimensional medical dataset camelyon17, and so does the recent [7]. Have the authors considered performing evaluation on this dataset?\n  - The baseline methods do not come with a cite. Is DP-diffusion [6] or [8]? Appendix A.2 suggests [6], but the caption of Fig. 8 suggests [8], which is confusing.\n3. Clarity can be improved:\n - \"inserted into the layers of the underlying UNet backbone\": exactly where?\n - \"It modifies stochastic gradient descent (SGD) by adding an appropriate amount of noise by employing the Gaussian mechanism to the gradients\"\n - \"However, Lin et al. (2023) do privatize diffusion\" -> do not\n - \"encoder $\\Delta$ and decoder $\\nabla$\": non-standard notations\n\n\n\n**References**\n\n[1] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.\n\n[2] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n\n[3] Karras, Tero, et al. \"Progressive Growing of GANs for Improved Quality, Stability, and Variation.\" International Conference on Learning Representations. 2018.\n\n[4] Dwork, Cynthia, and Aaron Roth. \"The algorithmic foundations of differential privacy.\" Foundations and Trends\u00ae in Theoretical Computer Science 9.3\u20134 (2014): 211-407.\n\n[5] Y. Hu, et al., \"SoK: Privacy-Preserving Data Synthesis,\" in 2024 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 2024 pp. 2-2.\n\n[6] Ghalebikesabi, Sahra, et al. \"Differentially private diffusion models generate useful synthetic images.\" arXiv preprint arXiv:2302.13861 (2023).\n\n[7] Lin, Zinan, et al. \"Differentially Private Synthetic Data via Foundation Model APIs 1: Images.\" arXiv preprint arXiv:2305.15560 (2023).\n\n[8] Dockhorn, Tim, et al. \"Differentially private diffusion models.\" arXiv preprint arXiv:2210.09929 (2022)."
            },
            "questions": {
                "value": "See the evaluation part in weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4377/Reviewer_Tmmk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699256018889,
        "cdate": 1699256018889,
        "tmdate": 1700692629759,
        "mdate": 1700692629759,
        "license": "CC BY 4.0",
        "version": 2
    }
]