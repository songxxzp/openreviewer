[
    {
        "id": "QxURSq8xoZ",
        "forum": "gZRfDWLlGY",
        "replyto": "gZRfDWLlGY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_dZQY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_dZQY"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a generalized exact path kernel as well as its application in out-of-distribution detection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of the approach is intuitive and simple: the authors first establish the connection between the model prediction and the gradient of the loss function on the training data, then they perform spectral analysis on the subspace formed by the gradient vectors.\n2. The proposed generalized exact path kernel provides a natural interpretation of the prediction in terms of the loss and parameter gradients.\n3. Empirical results demonstrate the effectiveness of the method in identifying the signal manifold dimension."
            },
            "weaknesses": {
                "value": "1. Presentation of the manuscript can be improved: Some notations are not clearly defined. For example, \\epsilon in Theorem 3.1 is not introduced but only mentioned in the proof as a constant. Similarly, \\theta_s(t) is not defined properly and is not distinguished from \\theta_s. From the context, s denotes the iteration and t denotes the training data index, does \\theta_s(t) refer to a parameter specific to data point t?\n2. Theorem 3.1 is the foundation of the proposal but the proof does not seem to be rigorous: it states that \u03b8_{s+1} \u2261 \u03b8_s + d\u03b8_{s(t)} / dt, is a step size missing there?\n3. The OOD is obtained by analyzing the span of the linear subspace formed by the loss gradient vectors. However, the analysis may not take into account the nonlinearity."
            },
            "questions": {
                "value": "- What's the significance of \\epsilon in Theorem 3.1 and how is it determined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806700603,
        "cdate": 1698806700603,
        "tmdate": 1699636728178,
        "mdate": 1699636728178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XtgLUVqkci",
        "forum": "gZRfDWLlGY",
        "replyto": "gZRfDWLlGY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_HDfE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_HDfE"
        ],
        "content": {
            "summary": {
                "value": "The paper offers valuable insights into the concept of exact path kernels, enabling us to interprete prediction through the utilization of input gradients over training trajectories. This methodology was first introduced by Bell et al. 2023. In addition, the paper extends the approach to provide a more generalized representation of the output of a test point under general loss functions. Moreover, it establishes a connection between exact path kernels and out-of-distribution (OOD) methods such as GradNorm, GradOrth, ReAct, DICE, and others, all of which rely on the utilization of parameter gradients. The paper also delves into the exploration of using a modified Singular Value Decomposition (SVD) to estimate the signal manifold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper gives several interesting insights of exact path kernels in the setting of OOD detection.\n- Signal manifold estimation seems interesting."
            },
            "weaknesses": {
                "value": "- The paper writing can be improved, i.g. punctuation for equations. \n- I find that there is repetition like Bell et al. on Section 3, remark 1 and remark 2.That is, Bell et al. make the similar remarks on SGD and subsampling. Proving techniques resembles the approach in Bell et al., \n- While EPK gives a representation of test points in terms of a vector space where the basis is computed from gradients at every training step, most OOD methods consider the optimized parameter which is $\\theta_S$ as the notation in the paper. This means that the existing OOD does not consider the parameter trajectories of optimization."
            },
            "questions": {
                "value": "Minor points\n- $L_{CE}$ is not defined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838422854,
        "cdate": 1698838422854,
        "tmdate": 1699636728076,
        "mdate": 1699636728076,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2VlsS8qTGE",
        "forum": "gZRfDWLlGY",
        "replyto": "gZRfDWLlGY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_eT4X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6494/Reviewer_eT4X"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a decomposition of the prediction of differentiable models in terms of parameter gradients throughout training. It is shown that a subspace arising from this decomposition captures a significant portion of model prediction variations at a test point when perturbing training points. Some links are established with out-of-distribution detection methods and measuring the dimension of the signal manifold."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There are interesting connections between the provided decomposition and OOD/signal manifold dimension detection methods, which might be worth further exploration."
            },
            "weaknesses": {
                "value": "* My main concern is that the theoretical results seem to follow directly from the arguments of Bell et al., 2023. Perhaps it could help if the authors added a discussion on the technically novel aspects of the analysis, or alternatively emphasized more on the applications of this decomposition as the main contribution.\n\n* While I am not an expert in OOD detection and thus cannot assess the work based on its applied contributions, I am wondering if it is possible to have experiments where additional intuitions gained from the analysis of this work concretely improve the performance of some methods.\n\n* There is some notational ambiguity and mathematical imprecision in the current version of the manuscript. Specific examples are provided in the questions section below."
            },
            "questions": {
                "value": "* Some examples for improving the clarity and rigor of the mathematical statements:\n    * It might be more appropriate to write Eq. (1) in terms of the inner product notation $\\langle \\varphi_{s,t}(x), \\varphi_{s,0}(x_i)\\rangle$.\n    * It could help the readers better understand the setting if the authors clarify the meaning of \"$f$ has been trained by a series of discrete steps composed from a sum of loss gradients ...\". If this is just training via gradient descent, it might be easier to simply write down the GD equation.\n    * The input and output space of $f$ don't seem to be defined.\n    * Making sense of Eq. (8) is not straightforward as the LHS is a vector in $\\mathbb{R}^m$, while the RHS is a summation over $M$ scalars (if $\\theta^j$ is supposed to denote the $j$th index of $\\theta$).\n    * The notation $\\frac{dL(f(x_i,\\theta_s),y_i)}{d\\hat{y}_{\\theta_s(0)}}$ does not seem to be immediately interpretable. If my understanding of the meaning of this expression is correct, my suggestion is to define something like $L'(\\cdot,\\cdot)$ as the derivative of $L(\\cdot,\\cdot)$ with respect to its first argument, and use $L'(f(x_i,\\theta_s(0)),y_i)$ instead."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6494/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6494/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6494/Reviewer_eT4X"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855599856,
        "cdate": 1698855599856,
        "tmdate": 1699636727964,
        "mdate": 1699636727964,
        "license": "CC BY 4.0",
        "version": 2
    }
]