[
    {
        "id": "OHelJsaXz9",
        "forum": "J88EKENxyF",
        "replyto": "J88EKENxyF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_34Fi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_34Fi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method utilizing LLMs to tackle the Multi-Modal Contextual Image Retrieval (MMCIR) problem. The authors construct a Multi-Modal Captioning (MMC) dataset and introduce Context-Aware Captioning (CA-Cap) and Context-Aware Text Matching (CA-TM) objectives to train a frozen LLM for MMCIR. The proposed method has shown promising results on various benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Since LLMs are good at processing and integrating contextual information, utilizing LLMs rather than text encoders derived from image-text matching models is promising to address the Multi-Modal Contextual Image Retrieval (MMCIR) problem.\n2. The authors construct a Multi-Modal Captioning (MMC) dataset by enriching existing image captioning datasets from \u27e8image, caption\u27e9\nto \u27e8reference image,reference caption,text condition,target caption\u27e9, which can be helpful."
            },
            "weaknesses": {
                "value": "1. The description of the inference is too brief to understand. How are the fused CAT-LLM-(ret) and CAT-LLM-(cap) used to retrieve images?\n2. The authors mention that CLIP text encoder struggles with understanding objectrelations, word order and logic. However, the authors use the features of the target caption from CLIP text encoder to align with the ret token, and further utilize the representations of the generated caption from from CLIP text encoder for inference, which does not make sense to me."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "In my opinion, no ethics review are needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Reviewer_34Fi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580508983,
        "cdate": 1698580508983,
        "tmdate": 1699636258876,
        "mdate": 1699636258876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XXno3BzjXd",
        "forum": "J88EKENxyF",
        "replyto": "J88EKENxyF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_xXn6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_xXn6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that employs LLMs to address the Multi-Modal Contextual Image Retrieval (MMCIR) problem. Specifically, authors construct a Multi-Modal Captioning (MMC) dataset with CC3M and Llama2, and introduce two another objectives, including a Context-Aware Captioning (CA-Cap) and a Context-Aware Text Matching (CA-TM) objective. The CA-Cap aims to predict the next target token conditioned on the mapped visual vectors, text condition tokens and previous target tokens. The CA-TM is an info-NCE loss maximizing the similarity between the ret token and clip features of target caption. The trained frozen LLM achieve competitive results on several image-language tasks like Zero-Shot Composed Image Retrieval (ZS-CIR), Visual Storytelling and Visual Dialog."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Construct an MMC dataset based on the off the shelf CC3M dataset, with Llama2 and in context learning.\n2) Utilize the decoder-only LLM for Retrieval tasks with frozen CLIP image and text encoders, by introducing two tasks: Loss_cap for capion generation with LLM; Loss_itm for image-text matching with a retrieval token appended at the end of the input tokens.\n3) Design two new objectives (CA-TM and CA-Cap) for MMCIR tasks. The experiments show that these two objectives improve performances in zero-shot composed image retrieval and dense multi-modal contextual retrieval."
            },
            "weaknesses": {
                "value": "1) The quality of generated <T_con, T_tgtc> cannot be guaranteed, and a process for filtering and checking (manually or automatically) is necessary.\n2) Compared with standard Loss_cap and Loss_itm, the \u201ccontext-aware\u201d in CA-Cap and CA-TM only seems like an augmentation of data that enriches and extends the details of input texts. Do the improvements come from the more detailed text inputs from the new dataset or the two new objectives? Will baseline and competing methods perform better when trained with the newly proposed dataset in this paper?\n3) On the evaluation of CIRCO and GeneCIS, the metrics Recall@K and Avg R@1 have a performance decline when CAT-LLM-(ret) is added with CAT-LLM-(cap). This phenomenon is lack of analysis.\n4) Since this paper proposes to perform the MMCIR task using LLMs, it\u2019s necessary to compare the results of various LLMs with different sizes (e,g., OPT-2.7B, Llama-7B, (FLAN) T5-(X)XL).\n5) This paper only encodes the image into a set of prefix prompt tokens, just like what recently proposed Multimodal Large Language Models (MLLM, e.g., BLIP-2, LLaVA, mPLUG-Owl) do, and these MLLMs are also compatible with the methods in this paper. I think they may perform better when fine-tuning with Loss_cap and Loss_itm here since their poweful ability of image-langauge understanding."
            },
            "questions": {
                "value": "All questions are included in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Reviewer_xXn6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807116520,
        "cdate": 1698807116520,
        "tmdate": 1699636258780,
        "mdate": 1699636258780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3ZWa63tjrx",
        "forum": "J88EKENxyF",
        "replyto": "J88EKENxyF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_USW9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3122/Reviewer_USW9"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a solution for the task of Multi-modal composite image retrieval, leverage the potential of Large Language models. imoprtantly they cater to the requirement of multiple image and text queries at input for retrieval. Specifically, they introduce two objectives of Context-aware captioning and Contxt-aware text matching for context-aware training of an LLM for retrieval. Furthermore they also introduce a multi-modal captioning dataset to enhance training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The concept introduced here is really interesting, although the components used here carry less novelty.\n\n- The writing of introduction, and the overall paper is quite fluid and easy to understand.\n\n- The idea of using captioning as a task for context-awareness is well formulated.\n\n- Qualitative Figures are well portrayed."
            },
            "weaknesses": {
                "value": "- Given that sufficient experiments are conducted, little reasoning is provided as to why the methods perform (low/high) in the way they do. More analytical reasoning would be encouraged.\n\n- Does this retrieval include images containing multiple target objects for retrieval as well?\n\n- More ablations on design choices, and not just learning objectives would have been more insightful.\n\n- A time complexity analysis would have been helpful to understand the real-world adoption of such a method\n\n- How significant do the authors expect the newly introduced dataset to be for the community as it could be easily generated as shown in the paper? Maybe other researches may modify on this synthesising process to get the data they need instead of using the proposed dataset ?"
            },
            "questions": {
                "value": "- What about the time complexity of the proposed method against state-of-the-art methods ?\n- What are some of the limitations of this method ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3122/Reviewer_USW9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813639897,
        "cdate": 1698813639897,
        "tmdate": 1699636258689,
        "mdate": 1699636258689,
        "license": "CC BY 4.0",
        "version": 2
    }
]