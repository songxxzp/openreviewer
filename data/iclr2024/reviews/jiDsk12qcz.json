[
    {
        "id": "UlbMRW0ApA",
        "forum": "jiDsk12qcz",
        "replyto": "jiDsk12qcz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_jpJv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_jpJv"
        ],
        "content": {
            "summary": {
                "value": "The paper states that current method for training a LLM from scratch requires a significant cost and may result in redundant capabilities. While current model ensembling method and direct parameters pooling are either computational expensive or impractical due to the difference of model structure. Based on this the paper treats the probabilistic matrix generated by a LLM given a text as the knowledge of LLM, and proposed a framework named FuseLLM to minimize the discrepancy between the probabilistic matrix of the target LLM and the source LLMs. The empirical study of the proposed approach show the strong performance of the method. Besides, the paper also compare FuseLLM to knowledge distillation and the traditional model ensembling / merging methods to further justify the performance of the FuseLLM."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The propose approach which allows merging knowledge from multiple sources of LLMs is novel and straightforward.\n2. The empirical study shows strong performance of the proposed strategy."
            },
            "weaknesses": {
                "value": "The implementation part is not very well written, I cannot completely understand the specific details toward the mechanism of the alignment and fusion."
            },
            "questions": {
                "value": "1. For the detailed implementation part, I am a little bit confused. Can you explain in details how MinED works for token alignment, and how MinCE/AvgCE merge different matrices into P_t?\n\n2. As a combined objective is used in FuseLLM (5), what is the choice of $\\lambda$ in your experiment? I saw from Appendix E that for dynamic design $\\lambda$ is set from 0.7 - 1.0, while 1.0 reduces the (5) to the regular CLM objective. Could you provide a more detailed elaboration on this hyperparameter?\n\n3. In Table 1, interestingly FuseLLM receive significant improvement in several tasks (e.g., Hyperbaton, Web of Lies) from MPT and OpenLLaMA, which is actually expected. While there are still a lot of the tasks like Formal Fallacies, Causal Judgement, and most of the code generation tasks, where significant improvement are not observed. What if the strongest source model is used for continuing training, instead of the Llama-2? For example, OpenLLaMA for code generation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Reviewer_jpJv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698385398205,
        "cdate": 1698385398205,
        "tmdate": 1699636298090,
        "mdate": 1699636298090,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0TyPWbz99y",
        "forum": "jiDsk12qcz",
        "replyto": "jiDsk12qcz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_M2ne"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_M2ne"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenges and drawbacks of training large language models (LLMs) from scratch and proposes a cost-effective approach called knowledge fusion (FuseLLM). Knowledge fusion aims to merge existing pre-trained LLMs by leveraging their collective knowledge and strengths to create a more powerful single LLM. The study validates this approach using three different LLM architectures and demonstrates improved performance in areas like reasoning, commonsense, and code generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposed a novel fusion approach for LLMs from a probabilistic distribution perspective.\n- The experiments are relatively sufficient.\n- Then writing and presentation are clear and easy to read."
            },
            "weaknesses": {
                "value": "- The presented results could not support that the FuseLLM cuts the cost of initial training and has superiority to other fusion strategies. Lack of quantitative comparison (both results and the cost) between other strategies (continual training, ensemble, merging ...) and the proposed FuseLLM. I only find Table 7 that presents the perplexity.\n- The results (Table 1, 2 and 3) show that FuseLLM could not outperform original LLMs in some tasks (some in BBH and all the ME). Does it mean that the FuseLLM only work in some domains? Or in another word, does the FuseLLM could only inherit specific capabilities of LLMs? Have the authors studied this phenomenon? Overall, the improvements are not remarkable, will such fusion effort necessary (with a maybe large training cost)?\n- The experiments are only based on Llama-2, OpenLLama and MPT and on 7B scale. Why choosing these three types of models? Can the authors give some explanations? We know that the model will emerge powerful capabilities via scaling up. Maybe the fusion effectiveness will drop with the parameter scale grows. Authors may provide a discussion about this argue.\n- How do the Avg. values be calculated in the last lines of Table 1,2 and 3? Directly averaging all the values does not make sense due to the disparate data scales of each task.\n- Are the Q_t in Eq. 4 and the Q_t in Eq. 2 the same? I guess the Q_t in Eq. 4 should be used another notation."
            },
            "questions": {
                "value": "See in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3455/Reviewer_M2ne"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698473587174,
        "cdate": 1698473587174,
        "tmdate": 1699636298022,
        "mdate": 1699636298022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hf3cIMLXrK",
        "forum": "jiDsk12qcz",
        "replyto": "jiDsk12qcz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_CN5G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_CN5G"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel training method that utilizes prediction probabilities from LLMs as a teaching signal to tuning an LLM. Unlike conventional teacher-student distillation approaches, this work proposes token alignment to handle vocabulary differences and fusion methods to combine the predictions of multiple LLMs, which means the teaching signal is a mixed prediction distribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Training LLMs is expensive, making the motivation to merge existing LLMs valuable.\n\nThe paper is well-written and easy to follow, and the experiment setup and ablation studies are solid and insightful."
            },
            "weaknesses": {
                "value": "I think the proposed method presents a promising training algorithm for leveraging multiple LLMs as teachers. However, it's better to have insights about what the student LLM has learned. The best empirical fusion method, Min Cross-entropy, utilizes token-level probability maximization across multiple teachers as the teaching signal. This teaching signal is not intuitive. Considering Hinton's dark knowledge theory, why does token-level max pooling outperforms average pooling?"
            },
            "questions": {
                "value": "To my understanding, teacher models are frozen during the training process, allowing us to infer and store their prediction distributions before training. This enables the utilization of larger models, such as employing 70+B LLMs as teachers. Since teacher inference occurs only once, the additional cost should not be significant.\n\nRegarding the Token alignment, I am still unsure the method after reading Appendix A. Can you provide an example or some explanations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688907153,
        "cdate": 1698688907153,
        "tmdate": 1699636297938,
        "mdate": 1699636297938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ALdDl1wprn",
        "forum": "jiDsk12qcz",
        "replyto": "jiDsk12qcz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_fXfT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3455/Reviewer_fXfT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to fuse the power of multiple pretrained LLMs (LLaMA2-7B, MPT-7B, and OpenLLaMA-7B), via a continual pretraining on top of LLaMA2 with the 1 Million-document MiniPile corpus, where the training objective consists of a weighted combination of a normal causal LM loss and a multi-teacher knowledge distillation loss. The result model FuseLLM performs better than LLaMA2 on reasoning (BBH), commonsense, and code generation (MultiPL-E) benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper is well-written and easy-to-understand, and the details are clearly presented."
            },
            "weaknesses": {
                "value": "* The proposed approach relying on continual pretraining with a small pretraining corpus (MiniPile) looks limited to fusing the knowledge of the raw pretrained version of LLMs, but not applicable to \"-Chat\" or \"-Instruct\" models (e.g., Vicuna, Alpaca, MPT-instruct, ChatGPT, etc.)\n\n* All the benchmarks in this paper are classification tasks. It is unclear if the FuseLLM would practically produce better text outputs on instructions, such as real-world human instructions, e.g., Alpaca (https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json) and some generative tasks in Big-Bench, e.g., word_unscrambling.\n\n*  The performance improvement from the proposed fusion approach is not significant enough. Specifically,\\\n      on reasoning tasks: 39.70 (LLaMA2) -> 40.44 (LLaMA2 + MiniPile) -> 41.78 (LLaMA2 + MiniPile  + model fusion)\\\n      on commonsense tasks: 63.76 (LLaMA2) -> 63.86 (LLaMA2 + MiniPile) -> 64.56 (LLaMA2 + MiniPile + model fusion)\\\n      on code generation tasks: 14.63 (LLaMA2) -> 14.83 (LLaMA2 + MiniPile) -> 15.56 (LLaMA2 +MiniPile + model fusion)\\\nGiven the MiniPile continual pretraining is not a key contribution of this paper, the actual accuracy improvement brought by the proposed model fusion on commonsense and code generation is actually smaller than 1 percent. \n\n* The evaluation of this paper doesn't include the comparison of FuseLLM with any of the previous model fusion techniques, e.g., LLM-Blender (Jiang et al.) mentioned in the \"related work\" section. Notably, the LLM-blender also doesn't require heavy continual training on a 7B large model, which is not discussed."
            },
            "questions": {
                "value": "See \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816358773,
        "cdate": 1698816358773,
        "tmdate": 1699636297839,
        "mdate": 1699636297839,
        "license": "CC BY 4.0",
        "version": 2
    }
]