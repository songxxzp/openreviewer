[
    {
        "id": "LoN85FAvHI",
        "forum": "FlvtjAB0gl",
        "replyto": "FlvtjAB0gl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_54hM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_54hM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a vision-language framework with unified objectives for both language and image.\nThe crux of the proposed method is the design of a dynamic discrete visual tokenization module.\nIn particular, the proposed LaVIT leverages a dynamic token sparsification approach to embed the given image with fewer vision tokens.\nAfter that, the learned tokenizer is employed to generate discrete vision tokens.\nIn this way, the proposed framework can achieve a unified pre-training objective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method achieves very impressive performance on several vision-language tasks such as VQA and image captioning.\n- The strategy to pre-train the vision-language models with a unified objective seems interesting.\n- The method can generate high-quality images based on the given textual prompt."
            },
            "weaknesses": {
                "value": "- The structure of the writing, especially the method section, is very confusing to me.\nIn fact, the key to the proposed method is the learning of the vision token codebook.\nThis codebook and token selector are built on the first stage of learning.\nHowever, the authors' description of this part makes it look like an end-to-end training framework, which confuses me a lot.\nI would suggest the authors carefully revise the structure of this section.\n\n- The authors claim their method can achieve in-context learning capability. \nHowever, I cannot find any experiments pertaining to this merit.\n\n- The authors did not provide a good explanation of why inequitable treatment for language and vision harms multi-modal learning.\n\n- More details about the language-vision pre-training part are required."
            },
            "questions": {
                "value": "- Is it possible that the impressive performance of the proposed method is due to more training data, especially for the vision tokenization codebook?\n\n- Why do the authors still use the unselected tokens as keys and values? \nOne intuitive approach for these tokens is simply discarding them.\nMoreover, these tokens should contribute minorly to the final performance and are redundant as authors claimed.\n\n- For the causal attention, which tokens are seen as previous ones? \nAs there is actually no sequence for an image, using causal attention for image encoding is thus not convincing.\n\n- What is the function of the second half of Eqn. 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697531153898,
        "cdate": 1697531153898,
        "tmdate": 1699636274489,
        "mdate": 1699636274489,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7L7TnbitmB",
        "forum": "FlvtjAB0gl",
        "replyto": "FlvtjAB0gl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_L5Te"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the idea of treating images and text equally, transforming images into a series of discrete tokens to input into LLM, and training LLM from scratch to complete understanding and generation. The experimental part of the paper yielded so-called advanced results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes to convert the image into a series of discrete tokens and adjust the token length according to the image content.\n2. The visualization of discrete tokens is displayed in Experiments to help readers understand the semantic content carried by tokens.\n3. Better results than the baseline method are achieved in the experiment."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited and the ideas of the paper have already been explored in previous work. For example, end-to-end training LLM has been explored in Emu[MR1], and the transformation of visual information into discrete tokens have been explored in [MR2, MR3, MR4, MR5].\n\n2. The generation process relies on a stable diffusion model, which makes the verification of the visual generation capabilities of the proposed model limited. Do the image generation results rely on strong stable diffusion, or are they more attributable to the discrete labels generated by the model? It is unclear to what extent the good results achieved are due to stable diffusion.\n\n3. Visual token merger part is not well validated. How much additional complexity does this stage bring to the model? Are different codebooks needed for data in different domains (animation, painting, natural landscape, etc.)? Why is a complex merger mechanism needed? Can a simple attention map from a pre-trained model indicate the importance of individual image patches?\n\n3+. BTW, the processing of the decoder (using the stable diffusion model) is the same as Emu's [MR1] strategy. A reference to the corresponding location and some justifications are required.\n\n4. In the experimental results in Table 3, using a larger number of tokens (fixed) achieves worse results, and more explanations need to be added.\n\n\n\n\n[MR1] Generative Pretraining in Multimodality. 2023.\n[MR2] Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks. 2023.\n[MR3] Visual Transformers: Token-based Image Representation and Processing for Computer Vision. 2020.\n[MR4] Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training. 2023.\n[MR5] Beit v2: Masked image modeling with vector-quantized visual tokenizers. 20022."
            },
            "questions": {
                "value": "Refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697943584071,
        "cdate": 1697943584071,
        "tmdate": 1699636274399,
        "mdate": 1699636274399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r8DlgVJ0C5",
        "forum": "FlvtjAB0gl",
        "replyto": "FlvtjAB0gl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an image tokenization method for visual-language model (VLM) pretraining, called dynamic discrete visual tokenization. The proposed method has two key characteristics:\n* Discrete: Continuous feature vectors from each image patch are mapped to a learnable codebook, resulting in discrete visual tokens compatible with text tokens. The discrete visual tokens make it possible to pre-train VLMs using the same autoregressive language modeling paradigm as text-only LMs.\n* Dynamic: The number of visual tokens is dynamically adjusted depending on the image content using a token selector and token merger. The token selector chooses the most crucial patches in the image, and the token merger reacquires the visual semantics of the unimportant patches, respectively.\n\nThe proposed method is pre-trained in two steps:\n1. Tokenizer learning: The tokenizer is pre-trained to reconstruct the original image from the quantized compressed visual tokens.\n2. Vision-language model learning: The VLM is pre-trained to predict the next visual/text token in a sequence, using the same autoregressive language modeling paradigm as text-only LMs.\nThe proposed method achieves high performance on multimodal understanding and generation tasks, with the relatively small size of model parameters and training data requirements. The paper also shows that quantization and token compression are effective techniques for improving the efficiency of VLM pre-training.\n\nOverall, this paper presents a novel and effective method for VLM pretraining. The proposed method looks simple yet effective and achieves state-of-the-art results on various multimodal tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-organized and easy to read, and the advantages of the proposed tokenizer are clearly described and demonstrated through various experiments and evaluations. In particular, the method for improving the inefficiency of the conventional vision-text fusion method is effective, and this study can be considered primary research for further improvement."
            },
            "weaknesses": {
                "value": "There are a few suggestions for the ablation study.\n1. Compare the performance of text generation using merged continuous embeddings and quantized visual tokens. The loss of detailed information due to quantization is likely to affect the performance of text generation. However, it would be interesting to see how much of a difference there is.\n2. Show the performance difference with and without token merger. It would help to understand the importance of the token merger module."
            },
            "questions": {
                "value": "1. What is the computational (speed) improvement during inference using a dynamic token length? It may be important information because training and inference efficiency may vary.\n\n2. There are a few things that could be improved in the manuscript. Please see the following suggestions.\n\n- Figure 1 (a): `Adater-Style` -> `Adapter-Style`\n- The first line in Section 3: `the Large language model` -> `the large language model`\n- The last line in `Token Selector` of Section 3.1 : `the binary decision M` -> `the binary decision mask M`\n- Equation 3: $||l_{2}(\\hat{x_i})-l_{2}(c_{j})||$ -> $||\\hat{x_i}-c_{j}||_2$\n- The 2nd line in Section 4.4: `the token predictor` -> `the token selector`"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Reviewer_K5nK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765673845,
        "cdate": 1698765673845,
        "tmdate": 1699636274308,
        "mdate": 1699636274308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3XB3MN7KAB",
        "forum": "FlvtjAB0gl",
        "replyto": "FlvtjAB0gl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
        ],
        "content": {
            "summary": {
                "value": "This work proposed to unify the pre-training objectives for visual and textual tokens while adapting the LLM for vision-language tasks. To this end, they first train a visual codebook to quantize the visual embeddings and assign each visual token a discrete label. To reduce the length of visual tokens, a token selector is proposed to select important visual tokens dynamically, and the token merger is used to merge dropped tokens with selected ones. The visual tokens and text tokens are then concatenated and fed to the LLM, which is trained for classification-based language modeling tasks. The authors conduct extensive experiments to evaluate the proposed methods, and the experimental results show that the proposed methods achieve state-of-the-art performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experimental results demonstrate that the proposed method achieves a significant improvement compared to previous works.\n- The improvements from each proposed component is well supported by the ablation study results."
            },
            "weaknesses": {
                "value": "- The authors train a visual codebook to assign discrete labels to visual tokens, which can be computationally costly. In the context of efficiently adapting LLM for vision-language tasks, it would be interesting to explore whether existing codebooks [1, 2, 3] can be utilized to provide useful supervision for training LLM.\n\n- Hard to be completely fair when compared against other baselines. For instance, compared to BLIP-v2, this work adds additional English text corpus and further fine-tunes the LLM. The significant performance improvement over previous work may be attributed to the additional training data and trainable parameters.\n\n- The implementation details of the ablation studies could be clearer. It is confusing why the proposed method achieves different results in Tables 3, 6, and 7 when doing ablation studies. \n\n- The authors claim that the learned codebook encodes high-level semantic concepts. However, the visualization results of codebooks in Figure 5 may suggest that the same code groups image patches based on low-level visual patterns, such as color and textures. It would be interesting to see additional image-level visualization results, similar to those in [4], to demonstrate that the codebook encodes high-level semantic concepts.\n\n- Minor suggestions/typos: I noticed several typos while reading the paper: \n\n  - Figure 1 (a) caption: Adater -> adapter\n  - The symbols X and when calculating K and V of Equation (2) should be X_d.\n\n---\n- [1] Chen, Yuxiao, et al. \"Revisiting multimodal representation in contrastive learning: from patch and token embeddings to finite discrete tokens.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- [2] Esser, Patrick, Robin Rombach, and Bjorn Ommer. \"Taming transformers for high-resolution image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n- [3] Bao, Hangbo, et al. \"Beit: Bert pre-training of image transformers.\" arXiv preprint arXiv:2106.08254 (2021)\n- [4] Peng, Zhiliang, et al. \"Beit v2: Masked image modeling with vector-quantized visual tokenizers.\" arXiv preprint arXiv:2208.06366 (2022)."
            },
            "questions": {
                "value": "Refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3261/Reviewer_6o4w"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699231307497,
        "cdate": 1699231307497,
        "tmdate": 1699636274235,
        "mdate": 1699636274235,
        "license": "CC BY 4.0",
        "version": 2
    }
]