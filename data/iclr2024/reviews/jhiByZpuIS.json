[
    {
        "id": "Quy8TaaXKd",
        "forum": "jhiByZpuIS",
        "replyto": "jhiByZpuIS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_PcAY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_PcAY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents MSfusion, a collaborative learning framework that enables the training of large models on resource-constrained devices through model splitting. With its double shifting model splitting scheme, adaptive model overlapping, and contrastive loss functions, MSfusion maintains training effectiveness while significantly reducing computation and communication costs. The paper provides a mathematical reasoning for DSS and introduces the definition of an unbiased compressor. The authors also discuss the practical application of MSfusion in real-world scenarios where multiple companies with resource-limited servers and private data can collaborate to train high-performance large models. Overall, MSfusion offers a promising solution to the challenge of training large models on resource-constrained devices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper focuses on the significant task of collaborative training, which plays a vital role in various scenarios, particularly in training large models. The authors provide a clear motivation for their work, emphasizing the challenges faced when training resource-intensive models on devices with limited resources, and the necessity of collaborative learning frameworks. The practical application of MSfusion is also discussed, highlighting real-world scenarios where multiple companies with resource-limited servers and private data can collaborate to train high-performance large models.\n- A highly effective collaborative training framework, MSfusion, is proposed in the paper, offering both computational efficiency and high performance. MSfusion leverages model splitting to enable effective and efficient training of large models across participants with resource constraints. The paper introduces the double shifting model splitting scheme, adaptive model overlapping, and contrastive loss functions to maintain training effectiveness while significantly reducing computation and communication costs. The authors provide a detailed description of the MSfusion framework, including the training process, communication protocol, and model aggregation method.\n- The proposed approach is validated through experiments on various tasks. The authors conduct experiments on several datasets, such as CIFAR-10, CIFAR-100, and TinyImageNet, to evaluate the performance and efficiency of MSfusion. The results demonstrate that MSfusion achieves comparable or even superior performance compared to state-of-the-art methods, while substantially reducing computation and communication costs. The authors also perform ablation studies to analyze the contribution of each component of MSfusion to its overall performance.\n- The paper includes in-depth theoretical analysis to support the proposed framework. This analysis encompasses the mathematical reasoning for the double shifting model splitting scheme and the definition of an unbiased compressor. The authors provide mathematical reasoning for DSS, which serves as a crucial component of MSfusion. Additionally, they introduce the definition of an unbiased compressor, which is employed to compress the model updates before transmission, thereby reducing communication costs. Theoretical analysis is provided to support the effectiveness of these components and their contribution to the overall performance of MSfusion."
            },
            "weaknesses": {
                "value": "- The performance gap between the ablated models and the proposed model is not very large. This does not support the importance of the proposed contrastive learning objective and the dynamic overlapping method.\n- The association between the proposed collaborative training framework and the contrastive learning objective could be further discussed.  This additional loss seems independent from the proposed collaborative training method. Also, this contrastive learning method itself may benefit other distributed learning frameworks, or the backbone model itself."
            },
            "questions": {
                "value": "I would expect more discussion on the results of the ablation study, and the proposed cross-sub-model contrastive learning, as specified in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Reviewer_PcAY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3368/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698567749022,
        "cdate": 1698567749022,
        "tmdate": 1699636287314,
        "mdate": 1699636287314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rAuEqKkuHw",
        "forum": "jhiByZpuIS",
        "replyto": "jhiByZpuIS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_wET1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_wET1"
        ],
        "content": {
            "summary": {
                "value": "This work aims at a collaborative training framework of large models for resource-constrained participants. To solve this problem, model splitting and contrastive loss functions are adopted. The research problem is interesting and important, and technical solution is reasonable and easy to follow. However, there is a gap between the research background and the experimental setup. The most serious issue is that the models used in the experiments of the paper cannot be called as large models. Besides, both of the two adopted techniques, i.e., model splitting and contrastive loss have been widely explored in many existing federated learning approaches, leading to novelty concerns."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe research problem mentioned in Introduction is interesting and important.\n2.\tThe overall structure and writing of the paper are well-organized and clear, enhancing the readability and understanding of the content.\n3.\tThe adopted techniques are reasonable."
            },
            "weaknesses": {
                "value": "1.\tLacks of novel contribution. The proposed framework leverages adaptive model overlapping and contrastive loss function. These two proposed techniques, i.e., model splitting and contrastive loss have been investigated by many existing researches, while I do not find sufficient discussion on the related works. To have a clear view on the contribution, it is reasonable to have a comparison between the proposed approach and [1][2].\n2.\tThere is a gap between the experimental setup and the research background of the paper. From the title and abstract of the paper, I was looking forward to seeing collaborative training of large models, especially since the first sentence of the abstract mentioned models like GPT-3. However, I was surprised to find in the experimental setup that the visual tasks were performed using ResNet-18, and the model used for text-related tasks was not explicitly mentioned. But based on the calculated FLOPS in Table 1, I can infer that the parameter size of this model is far less than 1B. I am curious if a model with such parameter size can be considered a large model. If the authors decide to claim this framework is designed for Large Model, I highly recommend to conduct experiments on real large models such as models with at least billion-sized parameters.\n3.\tInsufficient baselines. The authors claim that the proposed framework is compared with partial training-based approaches, however, there are some well-recognized approaches missed, e.g., [1][3].\n4.\tLacks of theoretical analysis. Considering the there is a proposed loss function in the framework, it is better to have a theoretical convergence analysis on the framework.\n5.\tThere are some typos, e.g., \" for TinyImageNet experiments for WikiText2 experiments is 800\" in section A.2.2.\n\n\nMentioned Reference\n\n[1] Thapa, Chandra, et al. \"Splitfed: When federated learning meets split learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.\n\n[2] Li, Qinbin, Bingsheng He, and Dawn Song. \"Model-contrastive federated learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[3] Collins, Liam, et al. \"Exploiting shared representations for personalized federated learning.\" International conference on machine learning. PMLR, 2021."
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3368/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718007463,
        "cdate": 1698718007463,
        "tmdate": 1699636287240,
        "mdate": 1699636287240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E4LDzoBSnc",
        "forum": "jhiByZpuIS",
        "replyto": "jhiByZpuIS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_wTYd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3368/Reviewer_wTYd"
        ],
        "content": {
            "summary": {
                "value": "Summary*\nThis work focuses on collaborative learning of large models over resource-constrained\nparticipants. The authors propose a new model splitting strategy that assigns a submodel of the\nfull global model to each participant. They further introduce adaptive model overlapping and\ncontrastive loss functions, achieving effective and efficient training. The evaluation is conducted\nover 3 image datasets (i.e., CIFAR10, CIFAR 100, and TinyImageNet) and 1 natural language\ndataset (i.e., WikiText2) using ResNet18 and Transformer-based network."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The submodel splitting strategy is a natural way to reduce the computation cost and has\nbeen well studied in the federated learning context. The key contribution of this work is the\ndesign of adaptive model overlapping and contrastive loss functions to help maintain\ntraining effectiveness against model shift across participants.\n2. The convergence of the proposed algorithm is analyzed in smooth and strongly convex case."
            },
            "weaknesses": {
                "value": "1. The technical contribution of this paper is limited. Except introducing a double shifting\nmodel splitting scheme, the proposed design has no significant difference from existing\npartial training work as reviewed in Section 2.3.\n2. The convex assumption over the loss function (i.e., Assumption 1) for convergence analysis is\nquite strong.\n3. The experiments cannot support the previous design and analysis sections. While the\nprevious design sections are claimed to study the collaborative training of large language\nmodels (e.g., GPT-3 in Abstract), the evaluation diverges to computer vision tasks and only\ntake one natural language dataset for evaluation.\n4. Some important evaluation details are missing, including the detailed parameter size and the\nnumber of layers of the transformer model, as well as the size of participants and the\ndataset partition for the experiment over WikiText2."
            },
            "questions": {
                "value": "From Table 1, why the baselines of HeteroFL and FedRolex perform so badly over WikiText2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3368/Reviewer_wTYd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3368/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699060775547,
        "cdate": 1699060775547,
        "tmdate": 1699636287171,
        "mdate": 1699636287171,
        "license": "CC BY 4.0",
        "version": 2
    }
]