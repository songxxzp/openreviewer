[
    {
        "id": "WLE6qF9WVd",
        "forum": "5a79AqFr0c",
        "replyto": "5a79AqFr0c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a trainig-free framework to produce videos based on provided text prompts and motion sequences. An interleaved-frames smoother and a fully cross-frame interaction mechanism with a hierarchical sampler are proposed to enhance the quality of synthesized videos. The authors demonstrate that they achieve sota performance by entensive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy-to-follow. \n2. The proposed method is highly efficient and does not need training at all. Nevertheless, the quality of synthesized videos are not bad."
            },
            "weaknesses": {
                "value": "1. The proposed components (inter-frame interpolation, and cross-frame attention) are more-or-less explored in recent works. As such, I'm uncertain if this research will provide substantial insights to the community.\n\n2. The proposed metrics (frame consistency, and prompt consistency) leverage CLIP model. Given that the CLIP model primarily operates in a deeply semantic and abstract domain, it often misses finer image details. Consequently, I'm inclined to think that the suggested metric might not adequately assess temporal consistency (for example, critizing jittering and discontinuity). Thus, the assertion that the proposed methods attain improved temporal consistency seems to lack robust quantitative backing.\n\n3. This training-free framework cannot capture fine-grained motion pattern in video. Therefore, I believe it may not be the optimal approach for producing high-quality video content. Instead, I think finetuning the model on large-corpus video data might help improve the quality."
            },
            "questions": {
                "value": "Just as stated in the weakenss section, I am skeptical about the potential of training-free framework in video generation area. I'd be keen to hear the authors discuss potential future research directions in this direction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4289/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG",
                    "ICLR.cc/2024/Conference/Submission4289/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698558887480,
        "cdate": 1698558887480,
        "tmdate": 1700631149504,
        "mdate": 1700631149504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T1ClNll9KI",
        "forum": "5a79AqFr0c",
        "replyto": "5a79AqFr0c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_aLwG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_aLwG"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on training-free controllable text-to-video generation tasks. It introduces an interleaved-frame smoother method to generate smoother frames. Additionally, it modifies cross-frame interaction to better utilize Stablediffusion's weights, enhancing frame continuity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is clear and easy to follow.\n- It is a training-free method, not relying on large-scale training, and has low computational resource requirements.\n- The ablation experiments are well-designed and easy to understand."
            },
            "weaknesses": {
                "value": "- Overall, the innovation is average; applying ControlNet to video editing or generation is straightforward and easily thought of.\n- The experiments are not comprehensive; there are too few baseline comparisons, and the experimental validation is limited to just over 20 examples, making the results less convincing.\n- Limited by the absence of structure condition, this method can mainly edit videos with similar motion. Its effectiveness diminishes for videos with different motions or poses."
            },
            "questions": {
                "value": "As shown above, despite the method's average innovation and some shortcomings, I believe the exploration in this direction is worthwhile. \n- I hope the authors can complete more experiments and cases, preferably providing an analysis of failure cases. \n- Relying solely on the demo examples provided in the paper makes it challenging to be fully convinced.\n- If the authors can address my concerns, I will consider giving a higher score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no need"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656424122,
        "cdate": 1698656424122,
        "tmdate": 1699636396694,
        "mdate": 1699636396694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bM87LlRDtA",
        "forum": "5a79AqFr0c",
        "replyto": "5a79AqFr0c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_HJ3a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_HJ3a"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes ControlVideo, a training-free framework that can produce high-quality videos based on the provided text prompts and motion sequences (e.g., different modalities). ControlVideo adapts a pre-trained text-to-image model (i.e., ControlNet) for controllable text-to-video generation. The paper introduces an interleaved-frame smoother that alternately smooths out the latents of successive three-frame clips by updating the middle frame with the interpolation among the other two frames in the latent space, aiming to stabilize the temporal continuity of the generated videos. Besides, a fully cross-frame interaction mechanism is exploited to further enhance the frame\nconsistency, and a hierarchical sampler is employed to produce long videos more efficiently. Experimental results demonstrate that the proposed ControlVideo outperforms the state-of-the-art baselines both quantitatively and qualitatively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is clearly written, well organized, and easy to follow. The symbols, terms, and concepts are adequately defined and explained. The language usage is good.\n\n- The proposed method is simple and easy to understand. Sufficient details are provided for the readers.\n\n- The experiments are generally well-executed. The empirical results show the effectiveness of the proposed method, showing certain advantages over state-of-the-art baselines."
            },
            "weaknesses": {
                "value": "- The qualitative results showcase certain advantages of the proposed method over state-of-the-art baselines in controllable text-to-video generation. However, by checking the provided video results, the temporal consistency can still be improved. Also, in some cases, the background looks unchanged. Some visual details can still be improved. Providing more discussions on these could strengthen this paper further.\n\n- The fully cross-frame interaction mechanism considers all frames as the reference, which thus increases the computational burden. What is the intuition to consider all the frames as a large image? Why not select some key frames to reduce redundant information? It is interesting to provide more discussions and analysis on this.\n\n- The paper mentioned that the proposed interleaved-frame smoother is performed on predicted RGB frames at timesteps {30, 31} by default. It can be more interesting if more studies and analyses on different steps to apply such a mechanism are provided.\n\n- It seems the interleaved-frame smoother still brings more computational cost and affects the model efficiency due to the additional conversion and interpolation steps."
            },
            "questions": {
                "value": "- Why does the hierarchal sampler improve model efficiency? It seems all the frames still need to be generated, although it is a top-down generation from key frames.\n\n- It is suggested to remove some content about the background and preliminary since such information is well-known.\n\n- The reviewer is interested if the proposed ControlVideo can be extended to generate more challenging new information, such as a novel view/unseen part of an object.\n\n-  Will the authors release all the code, models, and data to ensure the reproducibility of this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813332784,
        "cdate": 1698813332784,
        "tmdate": 1699636396614,
        "mdate": 1699636396614,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aOtemRuSfO",
        "forum": "5a79AqFr0c",
        "replyto": "5a79AqFr0c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_XMqd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4289/Reviewer_XMqd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \"ControlVideo,\" a training-free framework that significantly improves text-driven video generation. It addresses issues like appearance inconsistency and flickers in long videos through innovative modules for frame interaction and smoothing. ControlVideo outperforms existing methods, efficiently generating high-quality videos within minutes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThe proposed method is straightforward, easily implementable, and reproducible, making it accessible for further research and application.\n\n\u2022\tThe paper introduces novel techniques for long video generation, and the \"interleaved-frame smoother\" effectively improves frame consistency.\n\n\u2022\tThe results demonstrate improvements over existing methods, substantiating the paper's claims."
            },
            "weaknesses": {
                "value": "\u2022\tWhile the full-attention mechanism and \"interleaved-frame smoother\" enhance frame consistency, they also significantly increase the computational time.\n\n\u2022\tThe background appears to flicker in relation to the foreground in some examples. For instance, in the \"James Bond moonwalk on the beach, animation style\" video on the provided website, the moon inconsistently appears and disappears.\n\n\u2022\tThe paper lacks quantitative comparisons with Text2Video-Zero in the context of pose conditions, which could be a significant oversight given the importance of pose in video generation."
            },
            "questions": {
                "value": "\u2022\tCould you provide additional results for long video generation to further validate the method's efficacy?\n\n\u2022\tIs there a potential solution to the flickering background issue mentioned in the second weakness?\n\n\u2022\tWould it be possible to employ a non-deterministic DDPM-style sampler as an alternative to DDIM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698946777899,
        "cdate": 1698946777899,
        "tmdate": 1699636396530,
        "mdate": 1699636396530,
        "license": "CC BY 4.0",
        "version": 2
    }
]