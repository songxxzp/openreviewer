[
    {
        "id": "geOOiN11eZ",
        "forum": "qPFsIbF3V6",
        "replyto": "qPFsIbF3V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_LBov"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_LBov"
        ],
        "content": {
            "summary": {
                "value": "this work presents a way of transpilation: turning one assembly code into another functionally equivalent assembly code.\n\nthe main techniques consists of: using a LM to generate candidate programs. from the internal values of the LM 1) alignment/attention and 2) uncertainty, generate localized sketches with holes for the candidate program. this localized sketch is then solved, with the holes resolved to values that are provably equivalent to the source code's corresponding fragments. the fragments are then stitched together, finishing the transpilation process.\n\nresults show the proposed method beats a reasonable set of baselines -- a heuristic based transpiler, and few-shot using gpt4."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## the good part of quality: that it worked\nThe presented method works, on a domain of highly structured translation task (i.e. highly stylized texts), something a language model should perform very well at, and it shows. The extra care taken to correct the translation locally is a reasonable yet good idea to complement the weakness of the language model.\n\nThe benchmark is thorough, and the evaluation (on what is being shown) is solid. \n\n## clarity\nI am very grateful how this work is able to encapsulate the domain specific aspects of compiler and assembly, so that the top level algorithm remains accessible to the ML audience. Thank you!\n\n## novelty : fair\nI think it is a straight forward paper, and it outlined reasonable decompositions of the transpiling tasks to LLM and a symbolic solver."
            },
            "weaknesses": {
                "value": "## the not so good part of quality:\n\n### evaluation set is small \nThis work can be significantly beefed up with a synthetic test set. Evaluation on mere 100s of programs is likely not sufficient. Since it is possible to compile C into both architectures, and since test generation / fuzzing is a well established approach, this work can benefit from an artificial/synthetic test set consists of about ~1k programs, to evaluate the correctness of the transpiler more thoroughly. \n\n### lack of statistic tests\nAt least we should see confidence intervals of the results, or some kind of t-test to make sure that the proposes method is better than the baseline not due to noise. Kindly ask your colleagues in statistics to look over your tables and give recommendations on how it could be made bullet proof.\n\nI would love to see this update in the rebuttal period.\n\n## fit of venue\nWhile I think this is a good paper, it might be a better fit at PLDI. As I am unsure what is the AI/ML lessons gained from this work, other than it is possible to build such a system, and some relatively detailed finding on how well language models are at learning over a highly stylized text (assembly code) when compared to English sentences.\n\nHowever, as other application papers of the compiler flavour has a precedence of appearing at ICLR, this is not a major concern."
            },
            "questions": {
                "value": "## program equivalence?\n\nAs I understand program equivalence is an undecidable problem. If I recall correctly, synthesis systems like sketch does not have a way to fully verify the SKETCH and the SPEC are identical over all input-outputs, but only over a bounded domain?\n\nIs this an issue for your work? Or is it because everything is bounded to begin with, as we're working over assembly and we only need to account for, for instance, 16 bits integers or similar ? Or is it some Z3 theory magic that allows for a DSL which programs can be reasoned for full equivalence?\n\nAt any rate, this should probably be clarified in the paper.\n\n## successfully compile = ?\n\nIf I read correctly, success is measured over a set of input-output test cases to see if the input code runs the same as the compiled output code. Is this related to the program equivalence problem above somehow? Is this comprehensive enough to make sure the transpiling is not mistaken?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Reviewer_LBov"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816391423,
        "cdate": 1698816391423,
        "tmdate": 1700720297950,
        "mdate": 1700720297950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z4w14pVdEj",
        "forum": "qPFsIbF3V6",
        "replyto": "qPFsIbF3V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_awtu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_awtu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach for translating low-level assembly programs into higher-level code for the purpose of analysis and human understanding. The approach is based on a combination of neural processing and symbolic program translation. The proposed approach,  called GUESS & SKETCH, extracts alignment information from features of the neural language model and passes it to a symbolic solver to perform \"transpilation\". The paper also presents experiments illustrating the benefits of GUESS & SKETCH as compared to GPT-4 and an engineered \"transpiler\"."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well written and presents a clear contribution. The combination of generative language models and program synthesis by sketching is new and it is shown to be effective as compared to state of the art techniques."
            },
            "weaknesses": {
                "value": "I could not understand the correctness guarantee provided by the approach. The authors say \"the correctness of GUESS & SKETCH is always lower-bounded by the correctness of the initial guess\" -- the authors should explain what they mean by lower bound here. If the translation is incorrect, how can it be useful in practice?\n\nThe scalability is unclear.  What s the largest program that has been translated using the approach presented here?"
            },
            "questions": {
                "value": "Please see above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4175/Reviewer_awtu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821551991,
        "cdate": 1698821551991,
        "tmdate": 1699636383507,
        "mdate": 1699636383507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PZAO8ki9Ov",
        "forum": "qPFsIbF3V6",
        "replyto": "qPFsIbF3V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_nDKt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4175/Reviewer_nDKt"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach for machine language translation. They attempt to utilise a generative language model with a confidence score to identify uncertain blocks or \"guesses\" which can then be symbolically solved using a neuro-symbolic solver. They rely on Sketch (Solar-Lexama  et al)  prior work for handling the expansion/completion of the uncertain tokens. The authors perform experiments on other three datasets (Unix, Euler, Benchmarks) outperforming or equal (in rare cases) in all test settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- While a simple concept, the method outperforms prior work\n- The concept of uncertainty is a good mapping to identify holes in the generated program\n- Evaluation is robust and thorough, providing analysis of failure cases\n- Authors identify a setting for Neuro-symbolic approaches to work stably and outperform prior works"
            },
            "weaknesses": {
                "value": "- Novelty within this approach is quite limited the translation is a standard approach the confidence is simple (see below), and they use an existing neuro-symbolic solver therefore, it is more on the sole idea of putting these together. This is the main criticism. However, they outperform prior work, and the idea is interesting and technically sound. \n- Confidence is very trivially explained. In general, deep models are very confident even when they are wrong. It isn't clear how this was implemented and is critical to the method. As the author's rely on this to identify potential errors for solving.\n- The parameter lambda is not ablated on as the threshold for identifying blocks. It is unclear if this is set low to allow more errors i.e. false negatives but to make the result more robust."
            },
            "questions": {
                "value": "- Explain more how the confidence is applied and used is it based on prior work as there is significant literature in this area\n- Does the Lamda hyper-parameter effect the output, was this ablated on, but not included?\n- Why do you choose only the region of error to be solved? Did you consider using a buffer before and after as well to increase the consistency across the section and provide context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4175/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828060512,
        "cdate": 1698828060512,
        "tmdate": 1699636383422,
        "mdate": 1699636383422,
        "license": "CC BY 4.0",
        "version": 2
    }
]