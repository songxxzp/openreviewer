[
    {
        "id": "HxwEADIVLe",
        "forum": "m3RRWWFaVe",
        "replyto": "m3RRWWFaVe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_cFWK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_cFWK"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a frameworks to evaluate ethical values of Large Language Models (LLMs). The authors introduce DeNEVILa framework, which addresses the challenge of finding prompts for language models that would provoke them to violate ethical values. The main idea it to use Variational Expectation Maximization algorithm to identify prompts that maximize the likelihood of language model violating a specified value, and it provides a method for generating optimal prompts through iterative adjustments to improve context connection and violation degree of completions. The authors also present VILMO, a method to improve LLMs' alignment with ethical values, offering a step towards understanding and enhancing their ethical behavior."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality\n- the paper proposed several novel methodologies for both prompt discovery and improving LLM generation.\n\nQuality\n- the paper evaluated a large number of LLMs\n- the paper included human evaluation"
            },
            "weaknesses": {
                "value": "Clarity\n- The paper is hard to read and follow, there is too much content in the paper, and most of it's in appendix.\n\nSignificance\n- Overall the results are interesting, but the data prompt discovery are extracted from a LLM and tested in another LLMs, which makes me wander how many time these miss alignment values will actually happen in real scenarios. To elaborate, how many real prompt from a user, using chatgpt, will ever trigger something as shown in figure 4 (d)."
            },
            "questions": {
                "value": "Check weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper might have change the line spacing to add more text and content in the paper. It's quite noticeable, but i don't have a way to properly verify it. I think this is unfair for other authors which respected the page limit."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698561936342,
        "cdate": 1698561936342,
        "tmdate": 1699636065641,
        "mdate": 1699636065641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BJ0ZzpO5xH",
        "forum": "m3RRWWFaVe",
        "replyto": "m3RRWWFaVe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_7uMF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_7uMF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes DeNEVIL which is a prompt generation algorithm to probe LLM\u2019s ethical values. Based on using this framework authors curate a benchmark dataset and do extensive studies to probe different LLMs. Through performing experiments on this dataset authors find that most models are not ethically aligned. To mitigate this issue, they propose VILMO which is an instruction based in-context learning to approach to generate instructions that can enhance models and make them more aligned."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper studies an important and timely problem.\n2. The paper probes various models from different models families and sizes."
            },
            "weaknesses": {
                "value": "1. The paper's writing needs significant improvement. The writing can be made more clear. This clarity can also highlight the motivation behind this work even better as for now it is not super clear to me how this way of probing models in a generative  manner does not have issues/challenges (e.g., reliability and faithfulness) that previous discriminative based probing approaches have. These things along with the overall writing of the paper needs to be improved.\n2. I am not sure how reliable the trained classifier introduced in section 3.1 is. I think more ablations needs to be done to validate and justify the use of this classifier.\n3. How diverse the generated prompts are? I think some ablations on this aspect needs to be done. In general, I feel like the ablation studies need to be strengthen to validate whether the generated prompts are indeed meaningful and diverse enough.\n4. VILMO is only tested on ChatGPT while previous studies were more comprehensive in terms of probing more models. I think it would be good to evaluate a larger pool of models to validate effectiveness of VILMO.\n5. PPL and SB metrics lack for the VILMO approach compared to the baseline it would be good to see if approaches can be implemented to better control this trade-off.\n6. For the human evaluations sample size is too small."
            },
            "questions": {
                "value": "Refer to the weaknesses for details of my questions. I will list them here once again:\n1. I am not sure how reliable the trained classifier introduced in section 3.1 is. I think more ablations needs to be done to validate and justify the use of this classifier.\n2. How diverse the generated prompts are? I think some ablations on this aspect needs to be done. In general, I feel like the ablation studies need to be strengthen to validate whether the generated prompts are indeed meaningful and diverse enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712008368,
        "cdate": 1698712008368,
        "tmdate": 1699636065520,
        "mdate": 1699636065520,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3cfg9Wjv21",
        "forum": "m3RRWWFaVe",
        "replyto": "m3RRWWFaVe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_AiAD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_AiAD"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the ethical values in LLMs based on moral foundation theory. Instead of just trying to \"know\" whether there is ethical issues in LLMs, they want to understand how LLMs deal with value conformity. They propose a DeNEVIL framework that dynamically generates and refines the prompts so that these prompts can induce LLMs to produce completions violating specified ethical values. They found most LLMs are not good at obeying ethical values under DeNEVIL. To improve LLMs' value conformity, they propose VILMO which generates value instructions to intervene in LLMs to generate output that follows the ethical values."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The authors have summarized two challenges in discriminative evaluations and tried to propose a new framework to address them. They have proposed new methodologies along with detailed analysis of different LLMs to support their claim. The research question is important and the authors did a great job to introduce their solution step by step."
            },
            "weaknesses": {
                "value": "Some details might be missing from the main paper which could potentially cause some unsmoothness in reading."
            },
            "questions": {
                "value": "- What is the model used in DeNEVIL? Additionally, for your results in Fig.2, ChatGPT has the lowest misalignment behavior, could it be because the moral prompt dataset is generated using it?\n- Related to the above question, for DeNEVIL, it seems we are generating the most \"aggressive\" prompt (to induce LLMs to generate harmful output as best as we can). I'm wondering if different LLMs should have different most \"aggressive\" prompts. \n- In Fig.3(c), since the goal of DeNEVIL is to probe the issues in LLMs, shouldn't we use LLaMA-70B model? And, what model is being evaluated for this figure?\n- For VILMO warning, have you considered a baseline as a templated prompt with certain values? For example, ``Please ensure that your completion does not violate \"[value]\".''"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1378/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1378/Reviewer_AiAD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698886277685,
        "cdate": 1698886277685,
        "tmdate": 1699636065441,
        "mdate": 1699636065441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "58seQhrun7",
        "forum": "m3RRWWFaVe",
        "replyto": "m3RRWWFaVe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_6fGR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1378/Reviewer_6fGR"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the DeNEVIL framework, which uses Moral Foundations Theory to evaluate the value alignment of LLMs. The framework generates MoralPrompt, an evaluative set that dynamically iterates to uncover the moral principles guiding LLM responses. Upon analyzing 27 LLMs, the authors find a lack of alignment with human ethical values, thus presenting their solution, VILMO (Value-Informed Language Model Optimization), an in-context alignment method that enhances the value conformity of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper addresses a critical aspect of AI safety and alignment, which is ethical behavior of LLMs.\n2.Introduces a new methodology for evaluating and enhancing the moral alignment of LLMs.\n3.Provides empirical evidence for the value misalignment in current LLMs.\n4.Some areas need further exploration, such as cross-cultural applicability and the method\u2019s robustness against diverse ethical dilemmas."
            },
            "weaknesses": {
                "value": "1.There may be potential biases in the selection of moral foundations and their interpretations.\n2.The scope of the ethical values considered may not be comprehensive or universally applicable.\n3.It\u2019s unclear how the VILMO method scales or its effectiveness across different LLMs and settings."
            },
            "questions": {
                "value": "1.How does DeNEVIL account for cultural and contextual variations in moral judgments?\n2.What measures are taken to ensure that MoralPrompt doesn't introduce its own biases?\n3.How does VILMO compare to other ethical alignment techniques in practical applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699195210760,
        "cdate": 1699195210760,
        "tmdate": 1699636065332,
        "mdate": 1699636065332,
        "license": "CC BY 4.0",
        "version": 2
    }
]