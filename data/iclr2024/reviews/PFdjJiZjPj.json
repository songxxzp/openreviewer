[
    {
        "id": "p2N6OOdREu",
        "forum": "PFdjJiZjPj",
        "replyto": "PFdjJiZjPj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_PLSn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_PLSn"
        ],
        "content": {
            "summary": {
                "value": "This work explores the ability of language language models (LLMs) for testing programs. Two benchmark datasets and 11 LLMs were evaluated. Results on pass rate and coverage were reported and discussed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper addresses an important problem that is the testing ability of LLMs. \nThe writing is overall good, though there are some sentences that I am not sure about the meaning. \nThe results, while being preliminary, might be still useful. \n10+ LLMs have been evaluated."
            },
            "weaknesses": {
                "value": "The significance and novelty of this work is unclear. The pass rate and test generation of the LLM has been already studied in existing work (Chen et al., 2021). Important relevant work was also missed in the paper. It was said, on Page 1, that \u201cIn this paper, we, for the first time, analyze the ability of recent LLMs in testing programs/code.\u201d This is at least over-claiming. Existing work like CodeMosa already applies LLMs for improving code coverage and it was not mentioned or compared in this paper.  \n \nCODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models (ICSE 2023) \n \nThis is an empirical paper, and its evaluation is neither systematic nor comprehensive. \n \nTwo datasets were used in the evaluation. The selection of datasets seems to be arbitrary. What are their characteristics? Are they sufficient? Why are they representative? What will happen to programming languages other than Python? \nThe metrics for evaluating the experimental results cannot be taken as granted. The pass rate indicates the ratio of tests that pass unit testing, but it cannot tell if a test is good or not and often those tests that do not pass are regarded as more important as the aim of testing is to find defects in the code.  \n \nCode coverage is definitely more interesting than pass rate. The code coverage in the results look mediocre. Also, the computation of code coverage in Eq. (2) seems to be wrong. Different tests can cover the same branches and they are aggregated with considering the redundancy in Eq. (2). As a result, the final coverage might be over-estimated. \n \nIf the focus of this paper is exploring the testing ability of LLMs, the use of LLMs to generate the program/code shifts the focus and unnecessarily complicates the evaluation. \n \nI do not understand what the following statement means: \u201cthe test cases generated by LLMs can show a descent pass rate, and this pass rate is even higher than the code pass rate on HumanEval+, which holds for both large and small LLMs.\u201d (Page 5)"
            },
            "questions": {
                "value": "1. Existing work already applies LLMs for improving code coverage. What\u2019s the advantage of the approach in this paper? Will it result in on par to even higher coverage? If the answer is no, does it mean that testing ability of LLM exercised in this paper is not the state of the art? \n\n2. What are the principles for dataset selection in the paper?\n\n3. How to evaluate the quality of tests generated by LLMs in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698506086841,
        "cdate": 1698506086841,
        "tmdate": 1699636266788,
        "mdate": 1699636266788,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4um7adSjiN",
        "forum": "PFdjJiZjPj",
        "replyto": "PFdjJiZjPj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_hLEw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_hLEw"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a thorough experimental evaluation of 11 LLMs for generating tests in 4 different settings:\n1. Oracle\n2. Self-generated\n3. All-generated\n4. Placeholder\n\nThe paper presents 6 observations from these experiments:\n1. Given a task from HumanEval+, the pass rate of generated tests is higher than the pass rate of code.\n2. The correctness of tests generated by an LLM is positively correlated with the LLMs ability to generate correct code.\n3. Using code generated by the LLM as part of the prompt for test generation leads to better results compared to using only a placeholder.\n4. Using Oracle code produces tests of higher quality (better coverage) compared to non-oracle solutions.\n5. Using correct code in the prompt produces tests of higher quality (better coverage) compared to using incorrect code.\n6. The correctness of the tests generated by an LLM decreases with the order of generation (first is best, last is worst).\n\nFinally, the paper presents a simple technique for improving program synthesis performance by:\n1. using self-generation of code instead of placeholder code\n2. weight-ranking tests based on their order\n\nUsing this technique improves the pass rate of HumanEval+ when using GPT3.5-turbo and CodeT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Impressive evaluation using 11 LLMs and different testing settings. The benchmarks could be improved, but are on par with what is being used by others in the field.\n\n- A solid set of observations, as outlined in my summary. I would argue that some of these are well-known (eg 1, 3)."
            },
            "weaknesses": {
                "value": "- The experimental evaluation and the observations are nice and make sense to share with the community, but the actual contribution of the paper beyond that is relatively minor.\n- Using self-generated code (instead of a placeholder) is not exactly a fair comparison when compared to previous work, as it samples the model 2x for each generation. Using the place-holder code was somewhat intended in the original technique so you only generate the code itself once."
            },
            "questions": {
                "value": "- How do you \"assign different weights to the generated test cases based on their order in each generation result\"?\n\n- Coverage rates are surprisingly high for a small number of tests, can you give some examples of program/branches that were hard to cover for all models? How many branches are to be covered for programs in these benchmark sets?\n\n- There is no discussion of the prompts used for each model (outside the mention at the end of 4.1). I expected different prompts (other than the Oracle/SG/AG/PH part) to be part of the evaluation.\n\n\n- Page 2 \"converge rate\"\n- Fig. 2 Caption: \"past rate\"\n- \"SG and WR\" -> RW, \"SR and/or RW\" -> SG"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675774709,
        "cdate": 1698675774709,
        "tmdate": 1699636266715,
        "mdate": 1699636266715,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U2jS6vgIya",
        "forum": "PFdjJiZjPj",
        "replyto": "PFdjJiZjPj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_7enN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_7enN"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the effectiveness of code models and LLMs in generating test cases for programs. The quality of the generated test cases is evaluated using the pass rate (percentage of generated tests that are valid, i.e., pass when testing a correct implementation) and coverage rate (percentage of branches in the program that are covered by the test suite). The authors evaluate four different types of prompts for generating tests and consider 11 different code models. In order to motivate the use of LLMs for generating tests, it is argued that additional tests can help improve the performance of LLMs for code synthesis tasks. To support this argument, empirical results for code synthesis tasks are presented when using the LLM-generated test cases. The empirical results are promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The ability of LLMs to generate valid and useful test cases is an interesting question worthy of study.\n\n2. The experimental design is largely sound (see some concerns below in **Questions**) and the experiments are comprehensive (in terms of the number of models and datasets considered).\n\n3. The experiments lead to interesting yet expected conclusions. For instance, larger models and models with better code synthesis capabilities also tend to be better at generating test cases. It also turns out to be helpful to provide a correct implementation of the code to be tested when generating useful test cases."
            },
            "weaknesses": {
                "value": "1. It is not clear to me why test case generation is restricted to three test cases. In practice, one would want to generate a much larger number of tests and therefore, it is important to evaluate the performance of the models in such settings. The authors already note that the first generated test case often is more likely to be correct than the third test. If the number of generated tests is increased, this problem is likely to become worse and this would suggest that LLMs should NOT be used for generating tests.\n\n2. I am not sure what the takeaway message is from the paper. It seems like if one actually intends to use the test cases as part of a test suite, it is perhaps not a good idea to use LLM-generated tests since they are likely to be incorrect. However, for more *fuzzy* use of tests such as providing guidance during LLM-generated code synthesis, such generated tests could be useful. It would help if the paper includes a discussion along these lines."
            },
            "questions": {
                "value": "I have a number of questions regarding the experiments. I list them below:\n1. (Section 2) I assume that the pass rate is calculated with respect to the oracle program. Is this correct? Is the coverage rate also calculated with respect to the oracle program?\n\n2. (Section 4.1) How is the *all-generated* code set created? I understand each LLM is used to generate 100 code completions but how are these combined? Also, why does all-generated code only include code from the smaller models? Finally, what does the all-generated prompt look like exactly?\n\n3. (Section 4.1) Why is the temperature set 0.2 for all code generation? Why not use the temperature setting that is specified in the papers for each model?\n\n4. (Section 4.3) Are the code comments used in the \"Placeholder\" setting also provided in the other settings?\n\n5. (Table 6) For calculating the Pass@1 rate, are the synthesized tests also considered?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714785537,
        "cdate": 1698714785537,
        "tmdate": 1699636266616,
        "mdate": 1699636266616,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eDMe9TcF2N",
        "forum": "PFdjJiZjPj",
        "replyto": "PFdjJiZjPj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_EZ26"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3190/Reviewer_EZ26"
        ],
        "content": {
            "summary": {
                "value": "Automated test case generation is an important field of research in software engineering. A piece of code designed to solve a certain problem may have some implementation errors. Given such code, we would like to:\n\n* generate inputs that cover most of the paths in the program (high coverage)\n* find the \u201ctrue\u201d outputs for these inputs that test the _intended_ functionality of the code. Note that this is not the same as simply running each input through the program under test. The program may have errors, and the model has to \u201csee through\u201d the implementation to find the intended functionality. This is called the oracle problem.\n\nBoth these problems have been extensively studied in the software engineering community. The first problem is typically solved with a combination of symbolic execution and evolutionary fuzzing. The second problem (the oracle problem) is much more challenging, and is perhaps an excellent domain to apply language models. The \u201cintended\u201d functionality of code is hard to infer by traditional analysis, but language models can a) use comments/documentation, and b) use semantic properties of the code itself.\n\nThis paper attempts to evaluate language models to solve __both__ of the above problems in a single pass. \n\nThe language model is shown a natural language description of a programming task, and potentially buggy code solution to that task. Then the model is asked to generate test cases consisting of _both_ inputs and oracle outputs.\n\nThe generated test cases are evaluated in two ways - a) code coverage on the presented buggy code solution, b) pass rate on the ground truth oracle solution.\n\nIn this paper, the buggy code to be tested is _also code generated by a language model_. The authors consider multiple settings for this:\n* one where the code is **self-generated**,\n* one where the code is generated by **other models**,\n* one where the code is actually oracle code, and\n* one where the code is just a placeholder stub and the model has to use only the problem description.\n\nThe authors analyze their results in these settings, and then use their insights to solve a related problem - **code synthesis**. Recent work (CodeT) has shown that generating tests along with code and using these tests to filter the generated code can improve generation accuracy. The authors use two insights to improve the performance of CodeT:\n* _Large models_ generate better tests (as measured by pass rate on the oracle) when shown their own possibly buggy generated code (self-generation setting), versus when not shown any code at all (placeholder setting).\n* If a model generates multiple tests in sequence, then earlier tests in the sequence are more likely to be correct than those that come later.\n\nFinally, the authors use these two insights to improve CodeT, and improve its code generation accuracy by up to 4.2 percentage points.\n\nIn summary, the paper has two main contributions - a) an analysis of how well language models can generate tests, and b) using insights from this analysis to improve the program generation accuracy of CodeT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. First analysis of the test case generation ability of large language models for competitive coding (algorithmic coding) problems.\n\n1. I like the takeaway that a model generates better test cases when you first ask it to generate code to solve the problem (analogous to Chain of Thought). This is a good empirical result backed by intuition."
            },
            "weaknesses": {
                "value": "I like the general premise of this work. However, it it my opinion that it is not ready for publication in its current form due to several drawbacks:\n\n1. There are two distinct problems that this paper is trying to solve. The first is test generation, and the second is test-guided program generation (analogous to CodeT). However in attempting to do both of these, the paper ends up not doing justice to either one.\n\n1. If the aim is to study test generation with language models, then:\n    - The paper needs to include, up-front, a discussion of code coverage, the oracle problem, other traditional software engineering approaches to solve this problem (test generation has seen a long line of work), and even other transformer-based approaches like Dinella et. al (2022) [1]. I understand that not all of these are relevant to competitive/algorithmic coding tasks, but there should be a thorough discussion and setting up of the problem to motivate your approach.\n    - Buggy code generated by language models can\u2019t be the _only_ setting in which test generation is evaluated. There are other examples of buggy code, for example, CodeNet includes several incorrect human solutions for each problem. It would be interesting to see how test generation works for these.\n    - The language model is never explicitly asked to or instructed to maximize coverage while generating tests. In fact, good coverage is almost an accidental by-product of the process. I could think of multiple ways to prompt the model to achieve better coverage, possibly with iterative refinement. And then there\u2019s the question of whether a language model is actually the best choice to maximize coverage at all, or whether a fuzzer could do better.\n\n1. If the aim is to improve test-guided program generation by generating better tests, then:\n    - The description of the methodology is so brief that it\u2019s difficult to fully understand how the authors build on CodeT. For example - how/where is the re-weighted score used? From my understanding, CodeT forms symmetry groups based on which solutions pass the _same sets_ of test cases (like a consensus). I don\u2019t understand how one would use re-weighting here. Similarly, when it comes to tests using self-generated code, how exactly do you incorporate this in the consensus algorithm of CodeT?\n    - The empirical gains aren\u2019t particularly striking unfortunately - the gain is limited to 4 percentage points for the largest model, and is minimal for smaller models. I wonder if some of your other insights could be used to further improve this performance (like you mentioned as a future direction).\n\n1. The writing and organization of this paper leave a lot to be desired. Here is a short list:\n    - The introduction is extremely brief and doesn\u2019t motivate the problem well. The part about \u201c4 test case generation settings\u201d comes out of nowhere, and terms like \u201cself-generated\u201d, etc. don\u2019t make sense until you mention that you\u2019re testing code that is produced by language models.\n    - In the Evaluation section, you didn\u2019t mention whether Coverage rate is evaluated on the code that\u2019s being tested (self-generated, all-generated, etc), or on the oracle code. I assume it\u2019s the former, but this should be specified.\n    - In the \u201cAll-generated\u201d setting, each problem is associated with multiple solutions. Do you give all of these simultaneously to the model and ask it to generate tests? Or one at a time, with one set of tests per solution?\n    - Typos and notation: a) What is the final subscript $k$ in $Q_{ij} = \\{(x_{ijk}, y_{ijk})\\}_{\\textbf{k}}$? b) Top of page 4 - \u201cHuamnEval\u201d, c) page 5 and page 9 - \u201cdescent pass rate\u201d, \u201cachieving descent performance\u201d, d) page 1 - \u201cquality of program synthesise\u201d.\n\n[1] Dinella, Elizabeth, et al. \"Toga: A neural method for test oracle generation.\" Proceedings of the 44th International Conference on Software Engineering. 2022."
            },
            "questions": {
                "value": "1. Can you provide some more details of how exactly you incorporate self-generation and re-weighting in the consensus algorithm of CodeT?\n\n1. Can you clarify the \u201cAll-generated\u201d setting - how exactly do you provide multiple solutions to the language model while generating tests?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698884106968,
        "cdate": 1698884106968,
        "tmdate": 1699636266529,
        "mdate": 1699636266529,
        "license": "CC BY 4.0",
        "version": 2
    }
]