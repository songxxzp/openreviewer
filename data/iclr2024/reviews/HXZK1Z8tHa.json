[
    {
        "id": "9tu2v2APSc",
        "forum": "HXZK1Z8tHa",
        "replyto": "HXZK1Z8tHa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
        ],
        "content": {
            "summary": {
                "value": "This paper tried to solve two problems: how to make Transformer faster and how to make Transformer's optimization faster.\nFor the first problem, they propose shared portion stripe attention (SPSA) to reduce the network latency up to 7x speedup.\nFor the second problem, they introduce residual connections to the value of SPSA.\nIn summary, they build a novel Transformer network: ShareFormer by SPSA with residual connections and gated united."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper reduced the computational complexity of stripe attention by Shared Portion Stripe Attention.\n\n2. They proposed Residual Connections on Value, which offset the obstruction of the information flow throughout the network.\n\n3. They reached similar or even better performance and fewer parameters compared with other methods."
            },
            "weaknesses": {
                "value": "1. In order to implement shared attention, they had to introduce residual connections on value and gated unit to control the extra complexity. I am concerned that the greater the complexity of the system, the higher the likelihood of training instability.\n\n2. In Tables 4, 5, and 6, DRUNet reached similar performance and much lower latency compared with ShareFormer. To be honest, I prefer DRUNet in real applications with nearly 5x speedup, even though there is a 0.1~0.2 performance loss."
            },
            "questions": {
                "value": "1. Could you please also list the number of parameters of each model in Tables 4 and 6?\n\n2. By adding V directly to the output, you're effectively giving more weight to the original values irrespective of the computed attention scores. This might dilute the effect of the attention mechanism, especially if the values in V dominate the weighted sum. Thus, what if introducing a trainable parameter to scale the residual connection instead of directly adding a residual connection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF",
                    "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555679116,
        "cdate": 1698555679116,
        "tmdate": 1700026043485,
        "mdate": 1700026043485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OT9Bb9KJMa",
        "forum": "HXZK1Z8tHa",
        "replyto": "HXZK1Z8tHa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new transformer architecture called ShareFormer for image restoration tasks like super-resolution and denoising. The key idea is to share attention maps between neighboring layers, which reduces computational cost and speeds up inference. Residual connections are added to preserve information flow. Experiments show ShareFormer achieves state-of-the-art accuracy with lower latency and better trainability than prior transformers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThe proposed ShareFormer delivers substantial improvements in efficiency, reducing latency by up to 2x compared to CNN models without compromising accuracy. This is achieved through an innovative technique of sharing attention maps between transformer layers to avoid redundant computations. \n\u2022\tThe method enhances trainability over other transformer architectures by introducing beneficial inductive biases, allowing faster convergence. \n\u2022\tAn additional strength is the generalizability of the approach, which is shown to be compatible with different attention mechanisms like shifted windows."
            },
            "weaknesses": {
                "value": "\u2022\tThe performance exhibited by the proposed ShareFormer is indeed commendable, adeptly striking a balance between quality and speed. Nonetheless, I must express some reservations regarding the motivation behind the proposed backbone. To put it candidly, certain elements of the core module in ShareFormer appear reminiscent of concepts present in existing methodologies. For instance, the notions of Residual on V and the Reuse of Attention map are echoed in methods like Restormer and ELAN. Similarly, the group split strategy bears similarities to the one found in EfficientViT [1]. Thus, at a glance, ShareFormer seems to be a thoughtful amalgamation of pre-existing techniques. I would strongly recommend emphasizing the unique aspects and contributions of ShareFormer to underscore its originality within the broader landscape.\n[1] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, Yixuan Yuan: EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. CVPR 2023: 14420-14430.\n\u2022\tThe gains in trainability from the residual connections could use more detailed analysis and intuition. The paper currently lacks insight into the underlying mechanisms enabling faster convergence.\n\u2022\tThe ablation study is limited and does not thoroughly evaluate the contribution of each component. More experiments could help tease apart the individual impact of techniques like SPSA and the gated units.\n\u2022\tImportant implementation details like dataset splits for training, validation, and testing are not provided. This makes reproducibility difficult.\n\u2022\tBesides the Image Super-Resolution, the overall improvements appear relatively incremental over strong prior work like SwinIR and Restormer. The advances are not radically transformative."
            },
            "questions": {
                "value": "Some concerns are raised in Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY",
                    "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679612271,
        "cdate": 1698679612271,
        "tmdate": 1700708564556,
        "mdate": 1700708564556,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DUB68LAQkv",
        "forum": "HXZK1Z8tHa",
        "replyto": "HXZK1Z8tHa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
        ],
        "content": {
            "summary": {
                "value": "This paper mainly proposes Shared Portion Stripe Attention (SPSA), which shares attention map in $(l-1)$-th and $l$-th layers and residually connects $\\textit{value}$ of attention to intermediate attention output. To show the importance of the proposed residually connected $\\textit{value}$, a Lesion study has been conducted. The authors also propose Combine SPSA with Gated Unit (CSGU) to enhance the existing GDFN module. ShareFormer shows comparable or improved performance when compared to state-of-the-art image restoration methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[S1] The references and related works cited by the authors are very recent and relevant.\n\n[S2] The Lesion study presented in Sec.4.1 is comprehensive for demonstrating the proposed shared attention is highly related to ensemble behavior of the sequentially placed attention mechanism. Deleting or permutating some self-attention layers could smoothly increase the reconstruction loss. This can be the evidence that the shared attention is not the ensembles of shallow networks.\n\n[S3] The performances of ShareFormer for large SR and lightweight SR are notably enhanced, compared to recent SOTA methods. These results were achieved with smaller model size, fewer computations, and faster speed than the others. (But an unfair issue remain. See question Q4.)"
            },
            "weaknesses": {
                "value": "[W1] The presentation of text and figures in this paper should become clearer and more understandable. See Q1, Q2, and Q3.\n\n[W2] The paper lacks some ablation studies proving the importance of the proposed components, such as CSAU. The authors should study architecture variants if they hope to clarify that CSAU comes not from insufficient considerations but from careful construction. And the efficiency and effectiveness differences between the original GDFN and the enhanced CSAU must be provided.\n\n[W3] Most importantly, the core parts of ShareFormer, shared attention, seem not novel. In ELAN, one of the sota methods, shared attention map mechanisms have been already proposed. Moreover, it has been already shown how sharing attention maps in more than one layers can impact on the performance and efficiency of the model (related to your Tab.8).\n\n[W4] A potential unfair issue is observed with respect to SR. See Q4.\n\n[W5] Despite the improved inference speed, the performance gains of grayscale denoising, color denoising, and JPEG CAR are not significant."
            },
            "questions": {
                "value": "[Q1] Where is the exact part that the shared attention is operated? Eq.(4), (5) apparently reveals that this operates in $(l-1)$-th and $l$-th SPSA blocks. However, Fig.2 illustrates attention map of SPSA is shared to CSAU, while Fig.3 depicts sharing attention map appears after residual connection on $\\textit{value}$. The reviewer thinks that the explanation of Eq.(4), (5) is the correct case the authors intended, while the phrase, \"**sharing attention map**\", in Fig.2 and Fig.3 is confusing. If it\u2019s right, \u201csharing\u201d can be omitted. Or not, please let me know what your first intention with respect to the exact parts of sharing attention map is.\n\n[Q2] This question is related to Sec.4.2. From ConViT, how can you draw the conclusion that the concentrated ERF implies an amplified locality bias of the network? The reviewer thinks that it is not sufficient for the authors to claim that this fact shows the locality bias of the residually connected $\\textit{value}$ in ShareFormer. I cannot find the acceptable evidence from ConViT paper. Don\u2019t you have any other evidence justifying your claim, such as visualization?\n\n[Q3] In Appendix D, why did you compare the attention maps of layers 1 and 3? ShareFormer shares attention map in layer-order-number pairs (0, 1), (2, 3), \u2026, (2n-2, 2n-1), respectively. Thus, comparison of attention maps in \u201clayers 0 and 1\u201d or \u201clayers 2 and 3\u201d is more compelling to demonstrate attention map redundancy. Additionally, I recommend the authors to compare attention redundancies of the cases where the shared attention is \u201cemployed\u201d and \u201cnot employed\u201d in the proposed ShareFormer, instead of SwinIR cases.\n\n[Q4] Did you apply the Progressive Learning to large and lightweight SR tasks with training patch size of from 128 to 384? However, the comparative methods, SwinIR, ELAN, and DLGSA, used smaller patch size, such as 48x48 and 64x64. I am concerned that this leads to a potential unfairness issue.\n\n\n\n**Minor issue (not necessary to be mentioned in author rebuttal, if the authors struggle to a limit on the number of characters of rebuttal.)**\n\n(1) Fig.2 omits (a) and (b) mark, while the caption uses (a) and (b).\n\n(2) In the last to second sentence of Tab.1 caption, Restormer never used window attention. Moreover, you should mention what is MHDA, which seems a typo. (In Restormer, MDTA was used, and MHDA was not shown in SwinIR.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc",
                    "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754643493,
        "cdate": 1698754643493,
        "tmdate": 1700791735844,
        "mdate": 1700791735844,
        "license": "CC BY 4.0",
        "version": 2
    }
]