[
    {
        "id": "hlQxh0mWSq",
        "forum": "Rnxam2SRgB",
        "replyto": "Rnxam2SRgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
        ],
        "content": {
            "summary": {
                "value": "This method introduces a post-hoc interpretability method based on recent advances in Neuron Identification and LLMs. Specifically, the authors aim to bypass the common limitation of a priori set concept sets using a captioning model, while at the same time trying to filter spurious correlations by generating synthetic images based on Stable Diffusion. Compared to other approaches like CLIP-Dissect, the method augments the set of input data by attention cropping to capture both global and local information thus modeling spatial information. Qualitative evaluation studies suggest that the model exhibits significant improvements compared to the alternatives."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This method builds upon recent advances in Neuron Identification and specifically CLIP-Dissect. Within the CLIP-Dissect framework, a probing dataset and a predefined concept are considered, aiming to uncover the individual functionality of each neuron. This is performed via a matching of the images-concept similarity vector and an activation summary of each neuron given the probing set. However, this method fails to take into account spatial information, while being restricted to the predefined concept set, usually comprising single word concepts.\n\nOn the other hand, Describe and Dissect (DnD) bypasses the first issue using attention crops of the image; this allows for capturing both global and local infomation. For the second issue, the authors introduce an approach based on the concept generation via a caption model and a summarizer based on LLMs. To select the best concepts and deal with spurious correlations in the original probing set, additional synthetic images are generated via Stable Diffusion and the selection is based on the considered scoring function. The authors introduce appropriate scoring functions and assess their impact.\n\nDespite the usage of several different methods, the paper is overall well written and easy to follow. The methods used are clear and the pipeline consistent."
            },
            "weaknesses": {
                "value": "However I find some issues with this approach:\n\n(i) Even though the pipeline intuitively makes sense, disentangling the contribution and the impact of each module is very difficult. The authors consider three distinct architectures: (i) BLIP, (ii) GPT and (iii) Stable Diffusion. Each one introduces a different bias to the model, rendering the interpretation of the results in various settings a bit demanding. Moreover, there are additional overheads arising from the attention cropping mechanism derived from contours of the most salient regions. With each introduced module, additional parameters and complications are introduced, such as the number of candidate concepts N,  the number  generated images Q, the number of lowest ranking images beta, and the construction of the prompt for the LLM.\n\n(ii) The effect of each one of the components is not addressed in this work. This applies to both the effect of the hyperparameters but also the modules themselves. This is a very important and missing element of the approach. What is the behavior when the attention crops are removed? What if an augmented concept set is used instead of image to text captioning? How impactful are the synthetic images generated via Stable Diffusion?\n\n(iii) Overall the experimental evaluation of the approach is only based on qualitative human studies with no apparent way to quantify the results. In the alternative CLIP-Dissect method, one could use the original labels as the concept set and assess the matching between the true labels and the neuron identification results. This could be performed either via similarity in the clip space (or using other text encoders). Evidently, this is not possible in the case of DnD. \n\n(iv) In this context, the fact that DnD uses an \"augmented\" concept set renders the comparison not straightforward. It's not clear to me what are the concept sets used for the other methods. Do the authors consider the respective concept sets used in the original publications? For example the concept set for ImageNet for CLIP-Dissect is the one found in the respective code implementation of said paper? Did the authors try using a different more expressive concept set with the other methods and compare the results? One could argue that by augmenting the concept set used in the CLIP-Dissect case, one can get more expressive concepts that could appear more relevant to a human evaluator.  \n\nDespite the fact that this constitutes a post hoc interpretability based on at least 4 black-box models (Attention Cropping, Caption Model, LLM, Stable Diffusion), it's still nevertheless an interesting approach. However, the absence of important ablation studies, as well as the inability to assess the performance of the method in a setting that does not solely rely on human studies, obstructs the thorough assessment and impact of the approach."
            },
            "questions": {
                "value": "Please see the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698081388107,
        "cdate": 1698081388107,
        "tmdate": 1699636803578,
        "mdate": 1699636803578,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K9zcWlyGdc",
        "forum": "Rnxam2SRgB",
        "replyto": "Rnxam2SRgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Describe-and-Dissect (DnD), a novel method for interpreting hidden neurons in vision networks. DnD stands out from existing neuron interpretation methodologies by eliminating the need for labeled training data or predefined concept sets, which enhances its broad applicability and generalization capabilities. The approach comprises three primary stages: 1) Enriching the probing image set with crops to encompass both global and local concepts; 2) Utilizing BLIP for generating natural language descriptions of highly activating images, followed by GPT-3.5 to condense these into candidate concepts; 3) Creating new images based on these candidates using Stable Diffusion, and re-evaluating the concepts via a scoring function that considers both original and generated images. The effectiveness of DnD is demonstrated through its application to ResNet-50 trained on ImageNet-1K and ResNet-18 trained on Places365, showcasing superior performance in human evaluations compared to established methods like CLIP-Dissect, MILAN, and Network Dissection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research addresses a crucial aspect of model interpretability by enabling natural language interpretation of neurons.\n\n- The proposed method DnD overcomes critical limitations of prior techniques, notably the dependency on large-scale annotated training data or a pre-defined concept bank. Its training-free nature, derived from the integration of various existing foundation models, ensures robust generalization across diverse neural network architectures and data distributions.\n\n- Human evaluations verify the method's effectiveness. DnD's interpretation for ResNet models trained on ImageNet and Places365 was preferred 2x and 3x times by humans compared to prior methods.\n\n- The paper is well-written and easy to understand. The paper has a clear logical flow and detailed experimental results."
            },
            "weaknesses": {
                "value": "- The approach depends on existing pre-trained foundation models. A more profound exploration of how different model choices impact outcomes would enhance the paper's depth. For instance, the efficacy of DnD highly relies on the captioning technique, as it derives candidate concepts from captions of top activating images. BLIP is used in this work, but its brief captions might limit the method. A comparative analysis with improved captioning systems like BLIP2 or dense captioning models such as LLAVA with prompting, could offer valuable insights into the method's current limitations and potential enhancements.\n\n- The paper mainly relies on human evaluation, making it hard to reproduce the results. Additionally, there is no mention of plans to release the data and human annotation files.\n\n- Section 3.1 on Probing Set Augmentations needs more detail and sufficient ablation studies. The specifics of how attention crops are implemented and their contribution to enhancing DnD's performance remain unclear. Furthermore, Section 4.3, including Figure 4, needs more elaboration on how the synthetic images from Stable Diffusion and the scoring functions enhance DnD's effectiveness."
            },
            "questions": {
                "value": "- In Table 4, the average human-annotated score for \"Top K Squared + Image Products\" (representing DnD's final design) is 3.33, notably lower than the 4.15 average in Table 2 and Table 3. Could you clarify the reason for this discrepancy?\n\n- Are there any plans to release the datasets and results for broader access and verification?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689722491,
        "cdate": 1698689722491,
        "tmdate": 1699636803456,
        "mdate": 1699636803456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vos5DPeMEn",
        "forum": "Rnxam2SRgB",
        "replyto": "Rnxam2SRgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_gXLM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_gXLM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Describe-and-Dissect, a new method to describe neurons of a neural network with natural language descriptions. The multi-stage procedure first observes the activation of a target neuron for a set of images and image crops of higher activated regions, then captions top activating images with BLIP and summarizes the generated text into a list of concept proposals with GPT-3.5. Finally, a text-to-image generation model is conditioned on the concept proposals to generate images which are used to obtain neuron activations and ultimately a score to select the best matching concept. The neuron descriptions generated by Describe-and-Dissect are preferred over existing methods based on human evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Describe-and-Dissect does not require training data and generates open-set concepts without human input or guidance.\n- The pipeline is reasonable, utilizing foundation models where appropriate, and its components are well explained.\n- The user study underlines the efficacy of the method compared to existing methods.\n- Ablation studies justify design choices to the most extend, i.e., the scoring function and using a generative model to score the concepts."
            },
            "weaknesses": {
                "value": "- While there has been precedence of other methods trying to interpret neurons and when they activate, the motivation of this problem is rather weak without a clear application. MILAN showcased an application where neurons were suppressed in order to remove spurious correlations. It would have helped the presentation of the paper if such an application was evaluated and compared to competitors.\n- At least a subset of neurons are considered to be polysemantic [1-7], that is, they do not activate on a single concept but rather activate on multiple seemingly unrelated concepts. This issue is not discussed at all in the paper and the proposed method makes the assumption that a single concept exists as step 3 filters a single concept from a list of candidate concepts. A recent method [8] instead tries to decode full representations of layers instead of individual neurons. At least some of these related works should be discussed, but ideally the issue of polysemanticity could have been analyzed as a potential limitation/failure case of the model.\n- Related to the previous point, it is unclear what the proposed method predicts for neurons that are unexplainable (e.g. due to polysemanticity or when the neuron activates on concepts not understandable by humans). How often does it happen that each of the candidate concepts describes a subset of the images the neuron activates on, but not a single concept describes all images? One limitation seems to be that in case of polysemantic neurons, Describe-and-Dissect can only extract one of the concepts and this should be clarified and possibly even evaluated. Qualitative examples of failure cases could make the picture more complete.\n- The crowdsourced experiment did not indicate the location of the neuron activation in each image. Based on the MILAN dataset it seems crucial to have this information to determine the semantic concept the neuron activates on. Why was this choice made? This type of evaluation could create a bias towards explanation methods that describe global image content. Since a captioning model was used (BLIP), part of the performance in the user study could be explained by this image captioning bias. The authors could have also used the MILAN dataset for evaluation, but this experiment is missing.\n- The authors mention that human raters have deemed some neurons to be unexplainable (caption of Table 4). This begs the question how we could tell whether a description of a neuron is trustworthy (neuron is easily explainable/monosemantic) or whether we should not rely on the generated description (neuron is not interpretable) without asking humans about every neuron.\n\n[1] Olah et al., Feature Visualization, Distill, 2017  \n[2] Olah et al., Zoom In: An Introduction to Circuits, Distill, 2020  \n[3] Mu et al., Compositional Explanations of Neurons, NeurIPS 2020  \n[4] O'Mahony et al., Disentangling Neuron Representations with Concept Vectors, CVPR Workshops, 2023  \n[5] Scherlis et al., Polysemanticity and Capacity in Neural Networks, arxiv, 2022  \n[6] Gurnee et al., Finding Neurons in a Haystack: Case Studies with Sparse Probing, arxiv, 2023  \n[7] Bricken et al., Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, Transformer Circuits Thread, 2023  \n[8] Dani et al., DeViL: Decoding Vision features into Language, GCPR, 2023"
            },
            "questions": {
                "value": "- All hyperparameters should be listed for reproducibility purposes, i.e. $\\alpha$ and $K$ (images used in step 1) do not seem to be specified.\n- What is the influence of important hyperparameters, e.g., $\\alpha$, $K$, $N$, $Q$? Why were the reported values chosen?\n- What function is used for $g$? Do you spatially average pool the activations of a neuron?\n- In Table 4, the captions mentions \"neurons deemed uninterpretable by raters were excluded\". How was this determined by the workers? Why was this data excluded? Apart from this not being too relevant for the comparison of scoring functions, this data is highly relevant and interesting for the overall evaluation of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691388311,
        "cdate": 1698691388311,
        "tmdate": 1699636803321,
        "mdate": 1699636803321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y8RyP0Meln",
        "forum": "Rnxam2SRgB",
        "replyto": "Rnxam2SRgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_YsCF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6902/Reviewer_YsCF"
        ],
        "content": {
            "summary": {
                "value": "This paper utilizes a captioning model and a large language model to provide descriptions for highly activated images for corresponding neurons."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The Describe-and-Dissect method presented in this paper addresses the limitations of previous research and is a model-agnostic, training-free approach.\n- The authors provide adequate ablation studies to study the difference between their chosen scoring function and conception selection.\n- The authors demonstrate that their proposed approach is qualitatively better than the baseline methods through crowdsourced experiments."
            },
            "weaknesses": {
                "value": "- The authors use crowdsourced experiments to compare with the baseline method, providing qualitative results but lacking quantitative ones, which falls short of demonstrating the effectiveness of the proposed approach.\n- The authors only use 2 datasets and 2 models  to check the efficacy of their approach. To make the paper stronger and for completeness, it will be beneficial to include more datasets and models."
            },
            "questions": {
                "value": "Please address the weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825450610,
        "cdate": 1698825450610,
        "tmdate": 1699636803201,
        "mdate": 1699636803201,
        "license": "CC BY 4.0",
        "version": 2
    }
]