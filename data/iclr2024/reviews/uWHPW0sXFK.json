[
    {
        "id": "JKmKCIRO4N",
        "forum": "uWHPW0sXFK",
        "replyto": "uWHPW0sXFK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_QF8F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_QF8F"
        ],
        "content": {
            "summary": {
                "value": "The paper intends to solve Fokker-Planck equations, especially in high dimensional space. The method proposed here is to leverage a normalizing flow model to satisfy a hard constraint on distribution function (integral equals 1) and the training objective is self-supervised. In experiment, the method is tested on problems of dimension=30 and 50, and it shows the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality:** The paper has several novelties including combining normalizing flow to Fokker-Planck equations, designing a self-supervised loss calculated by the MSE loss between neural ODE prediction and network prediction.\n\n**Quality:** The method is quite reasonable and exhibits some effectiveness in experiments.\n\n**Clarity:** The method is presented clearly in the paper.\n\n**Significance:** To applications of high-dimensional Fokker-Planck equations, this paper provides a useful tool."
            },
            "weaknesses": {
                "value": "The major weakness of the paper is insufficient experiment. For example, the authors could have compared their method with PINN with soft constraints and showed its advantage. Ablation study could be implemented on the combination of NF + SSL. What about NF + PINN loss? That is to use equation residual as loss."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Reviewer_QF8F"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3627/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698498249556,
        "cdate": 1698498249556,
        "tmdate": 1699636318292,
        "mdate": 1699636318292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vl8NTlC2Kv",
        "forum": "uWHPW0sXFK",
        "replyto": "uWHPW0sXFK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_RDE5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_RDE5"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method to solve Fokker-Planck equations using continuous normalizing flows, with known drift and diffusion functions. While one can resolve the dynamics using an ordinary differential equation (ODE), it necessitates knowing the score of the distribution at a specific time, a challenging task. To address this, the authors employ a neural network to learn $\\log p(x, t)$. The network is trained through a regression between i) $\\log p$ obtained by solving the ODE and ii) the network's output. Presumably, an exact solution to this regression would mean precisely learning $\\log p$. Additionally, the paper utilizes discrete normalizing flows, specifically RealNVP, to model an initial distribution that eventually diffuses into the steady-state solution of a Fokker-Planck equation. The proposed algorithms, both time-dependent and steady-state, have been validated on simple problems with known Gaussian distribution solutions, and the results showed good agreement.\n\nOverall, the learning method is novel and interesting, but the paper is lacking in clarity, rigor and convincing, non-trivial experiments. I do not feel the paper meets the standard for ICLR in its current form, but could meet the standard at ICLR or another venue after these points have been addressed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Framing the problem as regression is clever \u2013 reminiscent of score matching, the technique behind diffusion models. Thus the central contribution of the paper is novel and interesting\n- Most of the exposition of the background and theory is good, see weaknesses below for examples where this is not true\n- The experiments shown seem to have worked very well. However, they are too easy (see below)"
            },
            "weaknesses": {
                "value": "- Quite hard to understand some key points on a first reading. I only understood what was going on after looking at the algorithms. For example, it would have been very helpful to mention explicitly that by $\\log p_{net}$ you mean $\\phi$ (or just write $\\phi$ in the equation instead of $\\log p_{net}$)\n- Doesn\u2019t mention that the divergence of a network (trace of gradient) is only cheap (constant wrt data dimension) if you are happy with an approximation via the Hutchinson trace estimator. Otherwise the cost of the trace is the same as calculating the full Jacobian and scales linearly with data dimension. Seems to be the case in this work, but scaling behavior is not mentioned\n- No justification in the text of why the specific parameterization of $\\phi$ was chosen. It would be nice to get a rough picture without having to read the references. In addition, it is claimed that the design imposes hard constraints, but those constraints are not specified, nor is it clear why they are desirable\n- There should be a proof that solving the regression problem is equivalent to $\\phi = \\log p$. It is clear that if $\\phi = \\log p$ then the MSE loss is zero. Is the reverse also true? It is implicitly assumed, and an explicit demonstration would greatly improve the theoretical soundness of the paper\n- There is no mention of diffusion models or score matching, even though this is quite similar to this work. In both cases, you want to learn a simple function of $p(x, t)$ (here the log, in score matching the gradient of the log) which allows you to solve the dynamics of the system. Diffusion models are based on Fokker-Planck equations (see [1] for example)\n- The \u201cmethod of characteristics\u201d is not explained even though it is mentioned multiple times. At least a reference that explains it is a must\n- The PINN model is not explained or referenced, even though it is mentioned as a competitor\n- The experiments are disappointing: all problems are essentially toy problems with linear drift functions and known Gaussian solutions. Clearly $\\log p$ is just quadratic, so the form you have chosen for $\\phi$ easily fits this. In fact, the neural net part of $\\phi$ should probably just be 0. It\u2019s nice that the method works on these problems and keeps working for higher dimensions, but the problems need to be harder to be convincing. In the discussion you mention applications of Fokker-Planck equations such as in optimal control. Why don\u2019t you try one of these applications? If that\u2019s outside the scope of this work, you should have some problems where the drift and/or diffusion are nonlinear. You can solve the ground-truth dynamics with an SDE and approximate the agreement between the ground-truth and learned distribution via MMD or something similar\n- There should be some idea of how the method compares against competitors, e.g. a table or figure showing the difference\n\n[1] Song, Yang, et al. \"Score-Based Generative Modeling through Stochastic Differential Equations.\" International Conference on Learning Representations. 2020."
            },
            "questions": {
                "value": "- Why is $z$ transposed in eq (6)? \n- Can you change the $x\u2019$ and $t\u2019$ notation to $x_1$ and $t_1$ or similar? This would be more in line with other works that solve ODEs or SDEs\n- Can you avoid using $\\mathcal{N}$ to denote a network? This is usually used for normal distributions\n- What is MAPE?\n- Are you missing a minus in the exp in eq (31)?\n- Am I right that the toy example of 5.1 involves no learning? Maybe make this clear\n- What are $\\phi^f$ and $\\phi^b$ in algorithm 3? Please define them\n- In line 3 of algorithm 3, should it be $\\phi^b$ instead of $\\phi$?\n- Why do you need the discrete normalizing flow for the steady-state problem? Can\u2019t you just start with a standard normal for $x_0$ and evolve it to the steady state? Do you know anything about what $x_0$ is learned by the flow (in theory and in practice)? What does this distribution look like in your experiments?\n- What do you mean by \u201cIn future work, we may explore the application of PINF with diffusion for density estimation tasks to enhance model generalization and data generation quality.\u201d in the conclusion? Is your aim to enhance diffusion generative models? How would your method help?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3627/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672380348,
        "cdate": 1698672380348,
        "tmdate": 1699636318200,
        "mdate": 1699636318200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QAkdDAQVlm",
        "forum": "uWHPW0sXFK",
        "replyto": "uWHPW0sXFK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_QfEf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_QfEf"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of the normalization constraint in the Fokker-Planck (FP) equation, a vital equation describing the probability density function's (PDF) evolution in stochastic systems. Recognizing the potential of flow-based generative models, the authors introduce Physics-Informed Normalizing Flows (PINF), a novel extension of continuous normalizing flows (CNF). PINF integrates diffusion using the method of characteristics, reformulating the FP equation as ordinary differential equations (ODEs) to improve training efficiency and stability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tPINF addresses the normalization constraint effectively, a challenge often faced in high-dimensional FP problems\n\n2.\tThe model shows effectiveness over a few questions including high-dimensional cases."
            },
            "weaknesses": {
                "value": "1.\tThe problem formulation is not clear. There are 3 problem setups w./w.o. time varying/diffusion term. The drifting term seems to be a known function rather than to be learned. So what\u2019s the point of the 1st setup (algorithm 1) here solving an PDE with explicit knowledge of time derivative equation?\n\n2.\tThe paper motivation is not clear. As the data is generated by solving the ODEs, what are the benefits to use neural network to replace PDE solvers? Whether NN is faster/more robust/generalizable compared to direct solvers, it should be explicitly mentioned in the introduction and experimental comparisons should be provided.\n\n3.\tPhysics-constraint and probability density conservation are mentioned multiple times in the paper and the author seems to claim it is ensured by the design of the NN formulation. However, I didn\u2019t see any description or proof on this, along with experimental verification. Please correct me if I\u2019m wrong.\n\n4.\tLack of comparison baselines. There is no comparison baseline in the experiments, making it difficult to evaluate the difficulty of the task. Besides, there is no reproducibility check.\n\n5.\tThe paper claims itself as \u201cself-supervised training method\u201d (under equation 19). This is questionable as the paper just uses ODE solver to generate labeled data during training, so it is essentially supervised training."
            },
            "questions": {
                "value": "1.\tUnder equation 19, \u201cThis structured design can impose hard constraints that are strictly enforced,\u201d. can you describe which hard constraints are imposed? The initial condition match or the probability normalization constraint? Please elaborate if the latter.\n\n2.\tUnder equation 22, \u201cthe total density integral on the space domain remains conserved as the solution evolves under equation constraints or ODEs controls\u201d. Please elaborate, specifically what constraints the equations enforce.\n\n3.\tIn figure 2, what is the difference between ground truth and ODE solution? Just integral error?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3627/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688931851,
        "cdate": 1698688931851,
        "tmdate": 1699636318127,
        "mdate": 1699636318127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sSZ2dNvZmi",
        "forum": "uWHPW0sXFK",
        "replyto": "uWHPW0sXFK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_oz5q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3627/Reviewer_oz5q"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a physics informed normalizing flow model for solving Fokker-Planck equations by using normalizing flows to solve the normalizing constraint."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well grounded theoretically in a natural extension to CNFs which are applied specifically to the TFP and SFP problems."
            },
            "weaknesses": {
                "value": "Though the results are interesting, there seems to be a lack of any sort of baseline compared to say standard PINNs (which the authors mention as falling after d=3). Most of the results are evaluated qualitatively where the authors observe good agreements between the true solution and the nn solution. A way to perhaps solidify results could be to analyze some sort of loss metric (perhaps average MAPE or max MAPE) for different models (PINF, PINN) as d is increased, and see the differences in drop off.\n\nAlso, some exploration for comparisons as to how p_net does as compared to p_ode on the TFP problems might help better understand the performance of the model.\n\nFinally, the authors deal primarily with solutions for the Fokker Planck equation, which may be somewhat limited in scope in terms of applicability to the general conference."
            },
            "questions": {
                "value": "Why is only p_net returned for SFP as compared to both p_ode and p_net for TFP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3627/Reviewer_oz5q"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3627/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775562932,
        "cdate": 1698775562932,
        "tmdate": 1699636318046,
        "mdate": 1699636318046,
        "license": "CC BY 4.0",
        "version": 2
    }
]