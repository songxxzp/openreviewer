[
    {
        "id": "lUqhMFYA0Z",
        "forum": "rZpLOB9jYF",
        "replyto": "rZpLOB9jYF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_3JpF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_3JpF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduce a memory-modular learner for image classification that externalizes\nknowledge memorization from reasoning and thus effectively generalizes to new\nclasses by replacing memory contents.  Experimental results show the promising performance of their method\non diverse scenarios. \n\n\nIn conclusion, this paper develops a memory module for images and text, which can be seen as an augmentation method. \nIn my opinion, this method is simple and useful, but maybe not suitable for ICLR, because it does not provide a new sight or novel model. \nThe work is more likely an incremental work and I advised the author choose another conferance, such as CVPR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) the idea  is simple and useful;\n\n2) the experiments show the method is effective on many different datasets."
            },
            "weaknesses": {
                "value": "In my opinion, the work is incremental, and I dont find any novel module in this work apart from experiments."
            },
            "questions": {
                "value": "1. please talk about detailed difference between your work and other memory augment multi-models.ork."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698416986239,
        "cdate": 1698416986239,
        "tmdate": 1699636478886,
        "mdate": 1699636478886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CaRj3KFuX3",
        "forum": "rZpLOB9jYF",
        "replyto": "rZpLOB9jYF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_BjqH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_BjqH"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an external memory of texts and images from the Internet onto a fixed CLIP image encoder and meta-trains two cross-attention modules for image aggregation and text aggregation, respectively. After meta-training, the resulting model can be applied to a variety of classification tasks including general, few-shot and class incremental classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written. \nThe method is simple and effective.\nThe model seems flexible in terms of manipulating memory components.\nThe effectiveness of the proposed method is evaluated through many experiments, supporting the conclusions."
            },
            "weaknesses": {
                "value": "1) Lack of some implementation details. For example, after modifying either the text memory or image memory, will the image and text prototypes also be updated? The current description of the class-incremental learning experiment is not clear to me. Does the text memory and image memory only contain the exemplars from the current stage? Then how does the model perform well under class incremental classification as the whole system relies on the outputs of two aggregation modules to make predictions? Does that mean that the output features from the contextual modules are largely similar to the query features since one can regard this as CLIP zero-shot features which do not change with modified representations?\n\n2) Lack of discussion of related works in continual learning, especially a recent work that similarly contains an external memory [1]. It might make sense to include a subsection in the related work.\n\n3) Additional experiments of the aggregation module should be included. The current implementation uses cross-attention. It might make sense to compare to some baselines including self-attention (including the current query and the retrievals as a sequence of tokens to a self-attention block), linear attention (at the cost of fixed k), non-trainable cross-attention (without mapping linear layers), etc. These experiments help to fortify the current design choices.\n\n4) Experiments in Tab. 2 can be enhanced through changing text sources as well. I would be intrigued to see the outcome.\n\n[1] Zhen Zhu et al. Continual Learning in Open-vocabulary Classification with Complementary Memory Systems."
            },
            "questions": {
                "value": "Please see 1) in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper uses crawler to crawl data from the Internet which might cause issue of copyright."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631743044,
        "cdate": 1698631743044,
        "tmdate": 1699636478802,
        "mdate": 1699636478802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bgdkNdx1QW",
        "forum": "rZpLOB9jYF",
        "replyto": "rZpLOB9jYF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_ELpJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_ELpJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a memory-modular learner for image classification that performs adaptive reasoning using an external replaceable memory. The proposed model can generalize to unseen classes simply by replacing the memory contents without re-training. The experimental results show that the proposed method can apply to various scenarios, including zero-shot, few-shot, fine-grained, and class-incremental classification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper provides a versatile memory-based method for different classification tasks due to representing class weights as class prototypes that are generated form memory items.\n\n+ The proposed memory-modular learner has good generalization ability to unseen classes by replacing or updating the contents of the memory."
            },
            "weaknesses": {
                "value": "- On the whole, the table arrangement is very irregular and cannot clearly express the superiority of the results of the proposed method. To make matters worse, the table data contains errors, such as Table 9. For the figures: for the sake of standardization, the texts in Figure 1should be unified into \u201cTimes New Roman\u201d format; The pictures in Figure 2 are arranged misaligned; In Figure 3, it can use \u201ccite/citep\u201d to cite the corresponding article, which is more formal.\n- In Section 3, the proposed framework of learner looks similar to the [1] (reduced version). Are there any specific differences between them? \n[1] Ziniu Hu et al. Reveal: Retrieval-augmented visual-language pre-training with\nmulti-source multimodal knowledge memory, CVPR 2023.\n\n- Eq.1 exists errors. First, T calculates the average similarity rather than sum of features. Secondly, the image and text features should be explicitly stated whether L1 norm or L2 norm is used, and represented with the format \u201c||a||\u201d (the following equations are the same). Thirdly, some characters in the equation are not explained, such as \u2018\u03c3,d' in Eq.3.\n\n- In the experiments part, please provide the detailed analysis to the experimental results. Specifically, (1) In the zero-shot classification experiments (Table 1), why the results of Chen et al. method are higher than the proposed method on CUB-F? (2) In the memory replacement experiments, from the results of Table 2, it seems that replacing the memory content has no impact on the test results. Can you provide other datasets for testing? And provide detailed result analysis to illustrate the generalization performance. (3) In varying k experiments (Table 4), it does not indicate which dataset is used for this result. In addition, from the results, when k<32, the performance of the proposed method is not greatly affected; when k > 32, the performance drops significantly. Please provide a detailed explanation. (4) Table 7, \u2018mini-ImgNet\u2019 should indicate which dataset abbreviation it is in the text or caption. (5) In the class-incremental experiments, is the backbone used (i.e., CLIP-B/32) in each method is pre-trained or need to be re-train? Generally speaking, the results of DER are much better than of iCarL, but why is it opposite in Figure 3. And can you provide the results of using the Resnet model as backbone?"
            },
            "questions": {
                "value": "See my comments in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928376972,
        "cdate": 1698928376972,
        "tmdate": 1699636478686,
        "mdate": 1699636478686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DsS4ngNovP",
        "forum": "rZpLOB9jYF",
        "replyto": "rZpLOB9jYF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_sn6x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4930/Reviewer_sn6x"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a memory-modular learner for image classification, which dynamically selects appropriate information from memory containing previously learned knowledge. This study can contribute to zero-shot/few-shot learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Learning of images and texts together provides an effective setting for zero/few shot learning.\n2. Experiment results show significant improvement.\n3. Modular approach is effective for better generalization and continual learning."
            },
            "weaknesses": {
                "value": "1. Paper writing needs some improvement, e.g., please fix \"we make our model dynamically\naccesses to a memory\"\n2. In few-shot setting, please compare with more SOTA methods."
            },
            "questions": {
                "value": "1. In zero shot setting, competing methods \"Yu et al. (2020); Chen et al. (2022) are state-of-the-art zero-shot learners, which are trained with all the annotated training images and text attributes in the dataset on top of an ImageNet-trained ResNet101 (He et al., 2016).\" while the proposed study: \"The image memory is constructed based on images collected from the web with simple keyword search. For each target class c, images are collected using the class name c as the search keyword on Google and Flickr. These web-crawled images and texts may be noisy but consist in scalable memory contents that reflect the world knowledge. We follow a similar strategy for the text memory. We query Wikipedia with each target class c, and collect the relevant textual information.\" Is this a fair comparison? Is it possible that these crawled images/texts actually contain images in testing set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698967131432,
        "cdate": 1698967131432,
        "tmdate": 1699636478581,
        "mdate": 1699636478581,
        "license": "CC BY 4.0",
        "version": 2
    }
]