[
    {
        "id": "miMLXvdiku",
        "forum": "tqHgSxRwiK",
        "replyto": "tqHgSxRwiK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_oWWd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_oWWd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to compute the relative fairness between two sets of decisions. The paper presents a couple of metrics to compute the relative fairness of the decisions. If one of the decision sets is incomplete the paper proposes to leverage a ML model to regress the ground truth decisions onto the unlabeled set. The paper presents two approaches to perform the same using an unbiased and a biased bridge. Empirical results show that the proposed metrics are consistent with the ground truth scores."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is fairly easy to follow.\n2. The notion of relative fairness is novel."
            },
            "weaknesses": {
                "value": "1. The premise and contribution of the paper is unclear to me. Specifically, the utility and application of the proposed metrics need more clarification. As I understand relative fairness is useful when comparing two decision sets. But if both decision sets are biased and the relative fairness is near zero, what does that tell us?\n2. Relative fairness as presented in the paper is simply the difference between statistical parity (with some normalization) of each decision set. Why can't we just rely on the absolute statistical parity? It provides a more granular and absolute measure of fairness. Why do we need relative fairness or what extra information does that provide us?\n3. It is unclear to me why ML models are being used to approximate the decisions on an unlabeled set. If a small set of human decisions is available, why can't we just evaluate the absolute fairness of that set? The function f can be prone to a range of implicit biases of its own and the results of the proposed metrics can heavily rely on the parameterization of f.\n4. One of the use cases of relative fairness is when the fairness of the decisions suddenly changes over time. Even then it is unclear to me how relative fairness is useful over standard statistical parity."
            },
            "questions": {
                "value": "1. Table 3 and 4 are quite hard to read. What do the values in brackets indicate? Are those the p-values?\n2. Please respond to the comments in the weakness section as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1916/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697987572100,
        "cdate": 1697987572100,
        "tmdate": 1699636122172,
        "mdate": 1699636122172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KeTDVQ9ePp",
        "forum": "tqHgSxRwiK",
        "replyto": "tqHgSxRwiK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_XK1E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_XK1E"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a different perspective of measuring fair decision-making by computing the fairness difference between decision sets made by various decision-makers. The authors evaluate the proposed approach on UCI-adult income and SCUT-FB500 face beauty datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I like this perspective of measuring fair decision-making since there is potential to tap into often ignored aspects of decision-making: varied decision-maker biases and contextual differences."
            },
            "weaknesses": {
                "value": "While I liked the high-level idea, I think the problem modeling and presentation need some improvements. Below are the observed weaknesses (and respective suggestions) and some questions. \n\n- The examples and general presentation of the paper are not easy to follow. For example, the authors mention various non-complementary examples, which doesn't help drive the idea home. The annotator bias example points more towards the setting of different (ideologically/demographically) decision-makers and the same dataset (e.g., platform content) or different datasets (e.g., different sub-reddit data/ different parts of X). On the other hand, the motivating example looks at multiple experts (>2, as is in the methodology) who are similar (ideologically) on the same dataset (similar features selected by a hiring company.). In general, the examples don't support or complement the methodology. Authors should pick an intuitive running example that supports the main idea.  \n\n- There is lots of inconsistency in writing. For example, the authors mention that they use different and same datasets in various parts of the write-up, which makes reading the paper hard. From my understanding, the authors use the same (exact) dataset and use part of the dataset as X_{0} (training set) and the other as X_{1} (validation set). Authors should aim for consistency in writing to improve readership. \n\n- Although I liked the idea, it does look like theory and empirical designs don't support the goal. The definition of relative fairness would have been more robust and intuitive if the definition had constraints specific to decision-makers and data distributions. I believe varied constraint values significantly change the problem. The authors use historical datasets with no information on how/who collected the datasets, which is critical to the suggested framework.  Although individuals might be different by A/B, they might be ideologically/demographically or contextually (biasedly) similar, which changes the idea of the relative fairness subject to \"different\" decision-makers. Additionally, the authors use a beauty dataset which is somewhat not ethically a great decision, and mention that the adult dataset comes from truth and is, therefore, 100% fair, which is unrealistic and unverified.  \n\n  -  Authors should redefine (add constraints) relative fairness and respective terms and change datasets by using semi-synthetic datasets or conducting human subject studies to collect data. For example, authors could get content from different platforms or the same platform but different communities, and solicit for classifications from clearly different groups, e.g., 18-26 years and 27- 45 year-olds. \n  -  Lastly, the methodology seems flawed to me, since the ML model and X_{0} biases are propagated and possibly amplified to decisions on X_{1}. Currently, it looks like authors view {X_{0}, Y_{0}} as ground truth for desigining f(x) which is then used on {X_{1}, Y_{1}}. I think instead, there should be a benchmark, for example, assuming you have information {X, Y} on prospective students, and you want to measure relative fairness. Then curate a distinct group A to classify X to get Y_{A}, and another group B to classify X to get Y_{B}. Then measure the relative fairness of the two decision sets. \n\n- Although the authors cite some relevant literature, the literature review/related work section needs improvement. The section should be more cohesive and concise and clearly indicate the paper's strength and position in related works. \n\n- The way authors mention their contributions is not complementary to the idea and their work. The authors should restructure this section and make it stronger. Additionally, the authors mention that they show that a machine-learning model trained on human decisions can inherit bias/preference. This is not novel and is the motivation for most of the fairness in ML work. For example, several works on feedback loops and bail/justice systems show this. \n\n- Since authors define relative fairness as the statistical difference of the delta of two sets of decisions over a certain sensitive attribute, would it be reasonable to assume absolute fairness is particular to one decision set? Does it mean absolute fairness is a sub-routine of relative fairness?"
            },
            "questions": {
                "value": "I like the idea but the paper presentation and methodology (theory and experiments) need some improvement. I have outlined some of these issues and made suggestions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Reviewer_XK1E"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1916/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698358117794,
        "cdate": 1698358117794,
        "tmdate": 1699636122100,
        "mdate": 1699636122100,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yKsiwMC5Dz",
        "forum": "tqHgSxRwiK",
        "replyto": "tqHgSxRwiK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_kf4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1916/Reviewer_kf4f"
        ],
        "content": {
            "summary": {
                "value": "In fair classification, it is difficult or even ill-defined to have \"true labels\" for complex, subjective decision-making tasks. Hence, the paper proposes a relative notion of fairness. Operationally, for a pair of classifiers (people or algorithms), the paper tests for demographic parity (fraction of positives between two groups) between the two classifiers. The paper further considers a setting where the two classifiers make predictions (Y1 and Y2) on different datasets (X1 and X2). To apply the test, the paper trains a predictor on (X1, Y1), and use it to impute the first classifier's predictions on X2. The paper provides experimental results for the proposed methods on real datasets, in which the bias either exists in the data or is simulated."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. In fairness research, it is important to think about the fairness notions carefully. The paper proposes a natural notion based on comparing a pair of classifiers in a relative sense, surpassing the need to define \"true labels\" which can be ambiguous or even infeasible in many applications.\n\n2. The paper uses real data to evaluate the proposed method."
            },
            "weaknesses": {
                "value": "1. Methodology\nMany data statistics have been used as the notion of fairness. Prior literature suggests that choosing which one to use requires careful thoughts. The paper should justify the choice of demographic parity, and ideally, provides comments on whether the proposed algorithms can generalize to other data statistics.\n\n2. Novelty\nLiterature on individual fairness and label imputation should be overviewed:\n\n(a) While the proposed relative comparison is natural, this idea of relative comparison is similar in spirit to individual fairness, in which \"true labels\" are also unnecessary.\n\n(b) I appreciate simplicity in general, but I struggle to see the two proposed algorithms as \"novel\" as the paper has claimed. Essentially the paper trains a predictor to impute missing labels.\n\nTo these ends, I fail to see a clear main contribution of the paper: the formulation and ideas are not entirely novel; the theoretical result (Prop 4.5) is very weak; and the experimental results also need improvement (detailed in \"Experiments\").\n\n3. Mathematical formulation\nI find the mathematical formulation presentation quite loose:\n\n(a) Def 4.1 is based on \"equality in distribution\". What is the source of randomness? I suppose it's the distribution of data X, but it is not formally stated that X is random.\n\n(b) I find Assumption 4.2 quite problematic and unjustified. The assumption is on a complicated quantity (Y_\\Delta(A=a)), and I fail to understand what this assumption fundamentally means. I especially struggle with Eq. 6, where normality is claimed without having any assumption on the function f or the classifiers (Y0 and Y1).\n\nIs this the law of large numbers? Then the source of randomness needs to be defined. The assumption of large samples also in a certain sense defeats the purpose of label imputation in the \"unbiased bridge\" algorithm. One can just compute and compare the data statistics on (X1, Y1) and (X2, Y2) separately. \n\n(c) I am confused about the correctness of Prop 4.5. In order to show equality in Def 4.1, under Assumption 4.2, should both equal mean and equal variance be required? Does RBT=RBD=0 only implies equal mean? Since the assumptions are very restrictive, this result somewhat feels like a tautology.\n\n(d) Minor\n- In Def 4.3, the mean and variance should be called \"empirical mean\" and \"empirical variance\".\n- It should be made clear early in Sec 4 that Y is binary {0, 1}.\n- In Eq. 9, what is \\overline{f}?\n\n4. Experiments\n(a) I find that there is a non-trivial gap between the experimental setup, and the setting that the algorithms are introduced in. The problem is motivated by bypassing the need to define \"true labels\". However, the experiment still simulates settings that have these true labels. This also happens in Step 0 in in the motivating example of Section 3.\n\n(b) The experiment setup needs to be described in more detail. For example, I don't understand two equations for the injection procedure. What does \\overline{Y}_1 = \\overline{Y}_0 mean? How is the mean of the Gaussian computed, since there are values of \\lambda (separately for sex and race). What z(a) means also needs to be explained.\n\n(c) Rigor: The paper needs to describe if multiple hypothesis testing is accounted.\n\n(d) Presentation: The experimental results can be better presented by figures. The tables are very dense to parse.\n\n5. Grammar: \n- Title: \"test\" -> testing\n- Abstract: \"decisions sets\"\n- P2: \"what are absolutely fair decisions\" -> what absolutely fair decisions are\n- P2: \"present\" -> presence\n- P2: \"Homan and others\" -> refer to papers by the first author\n- P7: \"least beautify\""
            },
            "questions": {
                "value": "Clarifications on Weaknesses 3(a, b, c), 4(a, b, c) would be the most helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "While I appreciate that the paper discusses limitations of the proposed approaches. I think the paper can make a more in-depth discussion. For example, what are the limitations of using the demographic parity metric, and what subpopulations may suffer more under this approach?"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1916/Reviewer_kf4f"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1916/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793144600,
        "cdate": 1698793144600,
        "tmdate": 1699636122007,
        "mdate": 1699636122007,
        "license": "CC BY 4.0",
        "version": 2
    }
]