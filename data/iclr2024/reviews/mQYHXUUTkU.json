[
    {
        "id": "AJl5bBkYWU",
        "forum": "mQYHXUUTkU",
        "replyto": "mQYHXUUTkU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
        ],
        "content": {
            "summary": {
                "value": "The authors regressed fMRI neural activities onto CLIP embedding of images, after projecting the weight vector onto the \u201cnatural image distribution\u201d, they use the projected weights to generate natural language description and image visualizations.  Using this method, they validated many functional properties of the human visual cortices and defined visual properties for some less charted lands on human cortex."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This is a fast-paced field. The authors spend the time to add comprehensive references to previous literature, which form a strong foundation for evaluating this work.\n- Simple but principled method. Noticing and addressing the modality gap with embedding projection seems like a key advance to make this work.\n- The close comparison to BrainDiVE is interesting, which kind of suggests that the text caption captures the \u201cessence\u201d of high activation images, without the need for direct gradient ascent, i.e. the selectivity at least for the activation maximizing images is compressed in the words.\n- The authors showed additional applications for neuroscience discovery which is super cool. This visualization/caption tool will help define the visual property of many uncharted lands. Evaluating the separation of clusters with human subjects is convincing.\n- I can see this work\u2019s approach is broadly applicable to multiple neuroscience modalities, e.g. ephys or imaging data from animals. Though language caption may not be a proper medium for them."
            },
            "weaknesses": {
                "value": "- Not much a weakness but more like a c**omment.** I think it seems common in this domain (e.g. neuron guided image generation), to pipe together several large-scale pre-trained multi-modal models with brain data and then train linear adaptors between them and then it will work.  So not quite sure how technical challenging this work is comparing to previous ones."
            },
            "questions": {
                "value": "### Questions\n\n- In Eq. 4 why do you choose to use score to weighted average norm and direction separately, instead of averaging the vectors themselves? I can see arguments for both ways, but why do you choose this one?\n- In Fig. 5, the Word Voxel visualizations using BrainSCUBA is always some round cake like object with text on it \u2014\u2014 which is kind of strange, while the previous method (BrainDIVE) and NSD samples don\u2019t have this feature. Where do you think this bias could come from?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_G8Co"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813363392,
        "cdate": 1698813363392,
        "tmdate": 1699635956167,
        "mdate": 1699635956167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k8d2bt1mDC",
        "forum": "mQYHXUUTkU",
        "replyto": "mQYHXUUTkU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a data-driven method, BrainSCUBA, to generate natural language descriptions for images that are predicted to maximally activate individual voxels. This is achieved by first training a neural encoding model to predict the fMRI response from the image embeddings, then projecting the encoding weights into the representational space in a contrastive vision-language model, and finally producing interpretable captions for individual voxels. The results aligned with the previous findings about semantic selectivity in higher-order visual regions. The authors further demonstrated finer patterns that represent the \"person\" category in the brain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This method provides more interpretable neural encoding results.\n- The generated text and synthesized images for individual voxels in the higher visual regions align well with previous research findings."
            },
            "weaknesses": {
                "value": "- The scientific contribution and significance of this work are unclear. The paper didn't provide much new insights into the neuroscience findings. \n- The method of this paper is not sufficiently evaluated. There are multiple steps in the analysis, including training the encoding model, projecting weights, and transforming them into text or images, each of these steps (except encoding model accuracy) lacks a clear ground truth for comparison. Thus, we can hardly know how much the result deviated from the original voxel representation."
            },
            "questions": {
                "value": "- How does the encoding weight vector $W_i$ change regarding to the model's training procedure? For example, if different splits of training and testing datasets are used, to what extent does the $W_i$ fluctuate? This is concerned because all the following analyses depend on a robust and accurate $W_i$. And for a voxel that has poor prediction accuracy (e.g., correlation=0.2), we don't know how well $W_i$ can be trusted as the representative embedding of that voxel.\n- The projection method is intuitive and smart, but there's no direct validation of its effectiveness. Is there a specific reason for using cosine similarity as the distance metric?\n- The map in Fig.3 seems to be very categorical compared to the continuous and distributed brain map resulting from natural speech data (Huth et al. 2016). Does this imply the existence of more separable clusters in higher visual areas? Is there any finer organization within these clusters if you look at more UMAP dimensions, e.g., under the \"human-related\" group (blue voxels) or \"scene-related\" group (pink voxels)?\n- The generated words in Fig. 4 mostly align with expectations. However, certain words, like \"close\" in the \"Faces\" and \"Food\" categories, are strange. Do the authors have an explanation for that?\n- The findings in Fig.7 are interesting. However, I am concerned that the identified regions don't necessarily represent the concept of \"people\". A region that represents another semantic category (for example, \"home\") that often co-occurs with \"people\" words in natural language captions might also be highlighted with this method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_79Ny"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818636411,
        "cdate": 1698818636411,
        "tmdate": 1700330980355,
        "mdate": 1700330980355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k6704HH7eb",
        "forum": "mQYHXUUTkU",
        "replyto": "mQYHXUUTkU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission298/Reviewer_xxME"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to find natural language description that maximally activates fMRI response of a voxel  to assess semantic-selectivity of different brain regions.\n\nKey idea is to first train a linear encoder to predict voxel activations from a pretrained CLIP model. Then  Voxel weights are projected to CLIP embedding space to generate captions from a pretrained text decoder (CLIPCap).\n\nThe proposed approach is validated by comparing the selectivity obtained by the proposed method with brain regions which show selectivity for places, food, bodies, words and faces. The authors perform additional analysis to find a person specific region in body-selective areas demonstrating new scientific discovery from this approach."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Use of pre-trained modules (CLIP, CLIPCap) to generate captions which maximize a voxel\u2019s response (Section 3, Figure 1)\n2. Confirmation of BrainSCUBA\u2019s findings on well-known category selective brain regions (Figure 4,5)\n3. Demonstration of category selectivity through a text to image generation model. Figure 5 and Figure 6 show how this method can generate maximally activating images 10 times faster than gradient based maximization (BrainDIVE). The images generated using BrainSCUBA are also more human-interpretable as compared to BrainDIVE.\n4. Finding person and non-person specific cluster within EBA  (Table 3, Figure 8)\n5. Overall the paper is well written and easy to follow. The approach is presented in a simple yet comprehensive manner thus making it easier for new readers to follow.\n6. The approach is validated both qualitatively (figure 5) and quantitatively using different metrics (Figure 6, Table 1,2)\n7. The approach has potential to be extended to investigating more complex semantic specificity which are not feasible using category selective images only."
            },
            "weaknesses": {
                "value": "1. Minor: In Figure 3, color code used is not shown in the legend but is there in text in page 6. I recommend to add legend in the Figure also for clarity.\n2. I believe this paper does not fully leverage the potential of BrainSCUBA. The captions generated are currently restricted to Nouns. Semantic selectivity using images is limiting as we have to find natural images that consist of only one concept without confounds. BrainSCUBA can allow a deeper investigation of social interaction through verbs , subject-object pairs and finding which regions are selective for specific interactions/emotions. I emphasize that I mentioned this in the weakness section as I would have loved to see more new results of semantic specificity (other than confirmations)."
            },
            "questions": {
                "value": "1. Is the semantic selectivity found here limited by training set (NSD)? Can we expect to see semantic specificity that is present in CLIP training data but not in NSD images?\n2. What was the intuition behind the choice  \u201cA photo of a/an [NOUN]\u201d. Did you consider other prompts ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840916411,
        "cdate": 1698840916411,
        "tmdate": 1699635955954,
        "mdate": 1699635955954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U6D1oTXM6W",
        "forum": "mQYHXUUTkU",
        "replyto": "mQYHXUUTkU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to combine fMRI encoding models with pretrained vision-language models to find captions for different voxels in visual cortex. They use this method to perform exploratory data analysis on an existing fMRI data (Natural Scenes Dataset, NSD). They find that the method can reconstruct existing known distinctions in preferences in visual cortex (places, faces, bodies, food, words), and can uncover heretofore unrecognized distinctions in the extrastriate body area and in the social network of the brain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, this is an interesting application of pretrained vision-language models for better understanding how the brain is organized. The exploration of fine-grained distinctions in the social network of the brain (section 4.4) is quite convincing, especially given the human evaluation results. The paper is clearly written and the demonstration of the use case would, I believe, be of substantial interest to neuroscientists and cognitive scientists."
            },
            "weaknesses": {
                "value": "I don't believe that this submission is well-suited for a machine-learning-focused conference such as ICLR. It uses off-the-shelf, pretrained models to find information about visual cortex, which would be primarily of interest to neuroscientists and cognitive scientists. I cannot find substantial methodological advances here that would be of general interest to a conference aimed at machine learning researchers."
            },
            "questions": {
                "value": "My concerns are not about the soundness of the work\u2013which is fine\u2013but about the appropriateness for publication in a machine learning conference. I don't think that there is much extra work the authors could do to convince me otherwise. I'm open to re-evaluation of the paper's merits if the other reviewers deem it appropriate for this conference. \n\nNevertheless, I do have some minor comments:\n\nPage 2: acitvations -> activations\nPage 3-4: I found Figure 2b and its accompanying justification inscrutable. If the point is that there are no images close to unit sphere of captions, and hence blending (eq. 4) must be used to find something closer to the manifold of natural images, this does a poor job of conveying that, and text would be a better way of communicating that. If there is a different point they're trying to make, the authors should take a few sentences to explain what it is. \nPage 7-8: I found the attempt at quantification conveyed in Figures 6 and Table 1 of dubious relevance. If the point of the method is to find visually coherent images that are easy for a human to understand the gist of, using sample images from the NSD itself would do just as well (e.g. Borowski et al. 2021). If the point is to get a better view of what an entire area is selective to, then it seems BrainDiVE works better. The authors should clearly state what they're trying to convey in these figures and tables, they take up a lot of space in the paper but don't add much, in my opinion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission298/Reviewer_aznk",
                    "ICLR.cc/2024/Conference/Submission298/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission298/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871626399,
        "cdate": 1698871626399,
        "tmdate": 1700285288390,
        "mdate": 1700285288390,
        "license": "CC BY 4.0",
        "version": 2
    }
]