[
    {
        "id": "aKawfMQhnD",
        "forum": "e2YOVTenU9",
        "replyto": "e2YOVTenU9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a methodology to degrade NN performance on new tasks. Specifically, the paper mentions that adversaries may want to adapt a pretrained NN to a new task while violating its terms of use. To mitigate this issue, the presented methods performs a form of neural architecture search to find NN architectures that degrade performance on the tasks for which the NN was not trained."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Demonstrates results on transnasbench.\n- Leverages zero-cost proxies in a new way."
            },
            "weaknesses": {
                "value": "While this paper is interesting, I am not quite convinced of the motivation behind it. If you want to prevent others from fine-tuning a NN, why even release its parameters to begin with? perhaps in this use-case, the model should not even be released? Also, the definition of \"task\" is very broad. Would additional data be considered a new task? for example, more 32x32 images to be classified into cifar 10 classes for a cifar-10 NN? Or is it just when the classification head is modified?\n\nOther weaknesses include:\n- a limited evaluation on NAS benchmarks, making it harder to appreciate the motivation of the paper. If evaluation was done on some NN that someone wants to protect, then it would've helped.\n- 2% performance degradation on CIFAR-leve NNs is actually quite large.\n- Zero-cost proxies are meant to guage accuracy in general and have not been verified to work for this  task of minimizing out-of-training-distribution accuracy."
            },
            "questions": {
                "value": "Can you please provide responses to the weaknesses above. The work is interesting but not fully convincing in its current form."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689035030,
        "cdate": 1698689035030,
        "tmdate": 1699637107129,
        "mdate": 1699637107129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PD4dX4zhRs",
        "forum": "e2YOVTenU9",
        "replyto": "e2YOVTenU9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_umc9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_umc9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a cross-task NAS framework to find an architecture to mitigate unauthorized DNN transferability.  A binary predictor using multiple zero-cost proxies is proposed to accelerate the NAS procedure. The results and the ablations demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The whole formulation of reducing transferability at the architecture level involved with architecture search is solid.\n2. The proposed nas method based on binary predictor is efficient and effective in designing model architectures with low transferability."
            },
            "weaknesses": {
                "value": "1. Since the proposed method is based on a predictor, maybe it is better to cite a series of predictor-based NAS work. For example, PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor AAAI 2023 TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework NeurIPS 2021 and so on.\n2. It seems that there is no training details about the binary predictor, What is the training cost for this predictor? Is one pre-trained predictor suitable for processing the architectures from different search spaces?"
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859741340,
        "cdate": 1698859741340,
        "tmdate": 1699637107023,
        "mdate": 1699637107023,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VKtwvyoThI",
        "forum": "e2YOVTenU9",
        "replyto": "e2YOVTenU9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
        ],
        "content": {
            "summary": {
                "value": "This paper prposes ArchLok to mitigate unauthorizaed DNN transfer. ArchLock first encodes the NN architecture to evaluate rank two architectures with task embeddings, then perform neural architecture search to find archs that are good on source tasks but bad on target tasks. Evaluation on NAS-Bench-201 and Trans-Bench-101 demonstrate that ARchLock significantly reduces the transferbility."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "By addressing security at the architecture level, ArchLock potentially fills a gap left by other security measures that focus on the model parameter level. This provides a more holistic defense strategy for DNN models.\n\nArchLock focuses more on the architecture rankings rather than the actual performance numbers, and utilize efficient zero-cost proxies as supervision. This approach can be scaled to any size of architecture pool and reduces the cost of training several architectures from scratch.\n\nExperiments on NAS-Bench-201 and TransNAS-Bench-101 demonstrate the effectiveness of ArchLock. It can effectively degrade the performance on target tasks by up to 30% while preserving the performance on source tasks."
            },
            "weaknesses": {
                "value": "The details of S / TU / TK are not clearly described. Algorithm 1 shows the cross-task search when the target task is known, but it does not discuss how the other two baselines are performed. Additionally, it is still unclear how the GraphEncoder (Figure.1) is executed and how task embedding is extracted. How much overhead does  task embedding take for each new task?\n\nAre the numbers in Tab 1 and 2 real measurements, or are they directly taken from NAS-Bench/TransNAS-Bench? The zero-cost proxies/predictors are trained on the same set of datasets, which may lead to potential overfitting. Evaluating on unseen and large-scale datasets (e.g., ImageNet [can be a subset with ful 224x224 resolution], Miniplaces) is necessary to demonstrate effectiveness.\n\nArchLock aims to design architectures that show less transferability on new tasks, but it does not discuss what kind of architecture leads to poor transferability. For example, do different datasets dislike different architecture designs, or there is an type of architecture that transfers bad generally on all tasks? It would be beneficial to provide visualizations and discussions of general un-transferable architectures so that further work can gain inspiration and insights."
            },
            "questions": {
                "value": "See weakness above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699259390677,
        "cdate": 1699259390677,
        "tmdate": 1699637106916,
        "mdate": 1699637106916,
        "license": "CC BY 4.0",
        "version": 2
    }
]