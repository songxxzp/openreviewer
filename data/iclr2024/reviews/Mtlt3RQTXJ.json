[
    {
        "id": "whLl3cDMpn",
        "forum": "Mtlt3RQTXJ",
        "replyto": "Mtlt3RQTXJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_1pz2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_1pz2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Gode, a novel approach that integrates graph representations of individual molecules with multi-domain biomedical data from knowledge graphs. By pre-training two graph neural networks on different graph structures and employing contrastive learning, Gode effectively combines molecular structures with corresponding knowledge graph substructures. It achieves SOTA performance on 11 chemical property tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. \n\nThe authors conducted extensive experiments to investigate the proposed method under various settings and tasks."
            },
            "weaknesses": {
                "value": "The novelty of this paper may be limited as Gode\u2019s modules, in my perspective, appear to originate from KANO. Gode reorganizes the knowledge graph (KG) and contrastive learning modules while incorporating a typical graph neural network (GNN) module. Additionally, the performance of Gode on most datasets is slightly better but comparable to that of KANO.\n\nRegarding Figure 2, the ablation study results are interesting as even a minor adjustment in Gode can result in inferior performance compared to KANO.\n\nLastly, I believe the authors should conduct additional experiments to directly compare the proposed method with KANO. It appears that the authors may have overlooked this aspect, as many experiments in the current draft only compare alternative methods within Gode. For instance, enriching Figures 3-5 with the results of KANO would significantly enhance the persuasiveness of the findings."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Reviewer_1pz2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698204684866,
        "cdate": 1698204684866,
        "tmdate": 1700449693878,
        "mdate": 1700449693878,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qOh4RRYBns",
        "forum": "Mtlt3RQTXJ",
        "replyto": "Mtlt3RQTXJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_C94D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_C94D"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors proposed GODE that integrates graph representations of individual molecules with multi-domain biochemical data from knowledge graphs. Graph neural network (GNN) was used to learn representation of molecular graphs and knowledge graphs. Contrastive learning was used to improve its predictive capabilities. Improved molecular property prediction results were demonstrated in experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The clear and coherent structure help comprehension. The authors provide good insights into potential research directions."
            },
            "weaknesses": {
                "value": "The originality is light based on the following observations:\n1.\tThe preliminary steps for pretraining the original molecule graph representation have previously been explored in the paper titled \"Does GNN Pretraining Help Molecular Representation?\" (Sun et al, 2022).\n2.\tSimilarly, the pretraining strategies for bio Knowledge graph representation were also discussed in \"Pre-training Graph Neural Networks for Molecular Representations: Retrospect and Prospect\" (Xia et al, 2022).\n3.\tThe construction of a Biomedical Knowledge Graph was prevously detailed in the paper \"A unified drug\u2013target interaction prediction framework based on knowledge graph and recommendation system\" (Ye et al, 2021).\n4.\tFurthermore, the concept of contrastive learning between the molecular graph and an augmented knowledge graph for molecules was previously addressed in \"Molecular Contrastive Learning with Chemical Element Knowledge Graph\" (Fang et al, 2022).\n\nThis work is a combination of the previously mentioned research, with some customization. Additionally, =the performance of GODE appears to be close to that of \"Molecular Contrastive Learning with Chemical Element Knowledge Graph\" (Fang et al, 2022) and \"Knowledge graph-enhanced molecular contrastive learning with functional prompt\" (Fang et al, 2023)."
            },
            "questions": {
                "value": "The author states the embedding produced by GODE is robust, but I do not see the corresponding discussion about robustness learning in the later sections. Could authors elaborate more?\n\nExplain more about what the positive/negative classes are in contrast learning.\n\nIn page 4 \"Embedding Initialization\", four methods were cited. Which method was used in the experiments? If all of them were used, did the authors observe any differences in performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545315738,
        "cdate": 1698545315738,
        "tmdate": 1699636080388,
        "mdate": 1699636080388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HlMV6ljRim",
        "forum": "Mtlt3RQTXJ",
        "replyto": "Mtlt3RQTXJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_ar3U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_ar3U"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a molecular representation learning method, GODE, based on knowledge graphs. Specifically, GODE trains two graph neural networks, one for the molecule and another for its corresponding knowledge graph subgraph encoders. The molecule encoder employs the pretraining approach inspired by GROVER, while the knowledge subgraph encoder follows a GINE-like strategy. After pretraining, both are further trained through contrastive learning, and the learned representations are subsequently applied to downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of the paper is clear and reasonable. This paper considers applying knowledge graphs to molecular representation learning, which differs from the majority of methods that rely solely on information within molecules. This strategy allows for the incorporation of more domain knowledge, potentially leading to better representations.\n\n2. The paper is well-structured, and the steps of the method are easy to follow, enhancing the accessibility and understanding of the proposed approach."
            },
            "weaknesses": {
                "value": "1. My primary concern stems from the experimental setup's consistency. The baseline methods used for comparison seem to follow different settings, as the authors mention in the appendix that they have reproduced these baselines using their official implementations. Some of these baselines follow the settings from [1], while others have their own network structures. To validate the effectiveness of their representation learning, experiments should ideally be conducted under the same settings, including the same backbone model, node/edge feature selection, and other relevant parameters. Different representation learning methods can yield different results when applied to different settings. For instance, the outcome of using Grover on GIN might differ from using it on GTransformer[2]. If each method is used with different settings, it becomes challenging to assess the effectiveness of the proposed method. The final results may be influenced by the quality of the chosen backbone models, making it difficult to draw meaningful comparisons and conclusions about the proposed method's performance. Therefore, it is essential to standardize the experimental settings to ensure a fair and consistent evaluation of all methods.\n\n2. Furthermore, the experimental results for GROVER appear to exhibit significant deviations from what is reported in the original paper. This raises concerns about whether the authors may have overlooked or omitted some critical details during the reproduction process. It is imperative for the authors to thoroughly investigate and address any potential discrepancies in the results, ensuring that the experiments align closely with the findings presented in the GROVER paper.\n\n3. This paper introduces a highly promising direction for using knowledge graphs in representation learning. However, the method section appears to be overcomplicated, and its technical contributions seem rather limited. Both the molecular and knowledge graph encoders undergo separate pretraining, followed by contrastive learning, which makes the pretraining process redundant. It is recommended that the authors poform more ablation studies on whether all these steps are neccessary. \n\n[1] Hu W, Liu B, Gomes J, et al. Strategies for pre-training graph neural networks[J]. arXiv preprint arXiv:1905.12265, 2019.\n\n[2] Zhang, Zaixi, et al. \"Motif-based graph self-supervised learning for molecular property prediction.\" Advances in Neural Information Processing Systems 34 (2021): 15870-15882."
            },
            "questions": {
                "value": "1. See the major concern above.\n2. Why is the GROVER results in the paper signigicantly lower than the reported results in the original paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Reviewer_ar3U"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654416080,
        "cdate": 1698654416080,
        "tmdate": 1699636080316,
        "mdate": 1699636080316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x77EaG2ybq",
        "forum": "Mtlt3RQTXJ",
        "replyto": "Mtlt3RQTXJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_afSU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1521/Reviewer_afSU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a molecule representation learning method, GODE,  that incorporates information from the molecular graph structure as well as domain specific biochemical information from knowledge graphs. The method leverages two pre-trained graph neural networks (GNN); one trained on the molecular graph and another trained on the relevant biochemical knowledge (sub)graphs. The GNNs are trained using contrastive learning to fuse the two complementary graph information. Downstream finetuning experiments suggest that the proposed method exhibits competitive performance in 11 chemical property prediction tasks compared to other molecule property pre-training methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "*Proposed method shows strong performance in 11 molecule property prediction tasks from moleculenet\n\n*Some nice comprehensive ablation studies, eg figure 3 and performance across different knowledge graph sizes\n\n*Paper is well structured and well written"
            },
            "weaknesses": {
                "value": "*Number of seeds (3) in experiments seems pretty low\n\n*Missing some of the larger benchmark datasets. (eg HIV, PCBA). Any explanations why? (I\u2019m not suggesting those are particularly great benchmark datasets)"
            },
            "questions": {
                "value": "*Amount of relevant information in a knowledge graph likely varies significantly for a particular molecule. Eg aspirin is very well studied compared to other molecules. Any thoughts on when the knowledge graph information is useful or even harmful for a particular molecule property prediction task?\n\n*In Table 2 and Table 3: would be clearer if there is some visual grouping of the methods that leverage molecule information only vs methods that additionally leverage knowledge graph information\n\n*Figure 3: any thoughts on what affects the steepness of the improvement as knowledge graph size is increased? Eg size of the fine tuning dataset, chemical similarity of the fine tuning dataset compared to the knowledge graph, etc?\n\n*Why wasn\u2019t the Ye et al, 2021 model benchmarked?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1521/Reviewer_afSU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699418824124,
        "cdate": 1699418824124,
        "tmdate": 1700682872419,
        "mdate": 1700682872419,
        "license": "CC BY 4.0",
        "version": 2
    }
]