[
    {
        "id": "RipZsvwAHi",
        "forum": "VhQUwxIHER",
        "replyto": "VhQUwxIHER",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
        ],
        "content": {
            "summary": {
                "value": "The authors consider the problem of training a fair model with comparable accuracy on all protected groups. The catch is that the model doesn't know which observations belong to which protected groups during the training process. The main idea of the paper is to achieve low loss for each observation during training. This means minimizing the expected loss across the observations *and* the variance of the loss across the observations. The authors formulate this update as a Lagrangian and describe some heuristics for updating in a \"harmless\" way where the variance can decrease without impacting the expectation too much. They run experiments on four different data sets and show that their method achieves comparable accuracy to other approaches but can achieve lower variance and fairness. They also perform some ablation experiments to show their heuristic updates improve performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The idea of minimizing the variance of losses is interesting.\n\n* I like Proposition 2 that losses have to be independent of protected groups. I would like to see it in the main body and the conclusion that, since protected groups aren't known, a very natural thing to do is minimize the variance in all losses.\n\n* The way of choosing the harmless update shows substantial thought but I would've liked more explanation.\n\n* The experiments are comprehensive: several data sets, baselines, and metrics."
            },
            "weaknesses": {
                "value": "* I found the motivation for why they want to minimize the variance of loss weak a little weak in the main body. Moving Proposition 2 to the main body and adding more explanation could help.\n\n* I found Proposition 1 unpersuasive because the squareroot of the variance is a *very* loose upper bound on the maximum accuracy difference loss. At one point in the proof, they blow up one side by a factor of n. I would only find a result like this persuasive if there was a (somewhat) tight upper and lower bound.\n\n* Honestly, the results in Table 1 aren't impressive to me. Most of the metrics are quite similar except for variance which, to be fair, only their algorithm has been optimized for."
            },
            "questions": {
                "value": "Is my assessment of Proposition 1 as loose correct?\n\nIs my assessment of the algorithms having comparable performance in the experiments correct?\n\nCan the problem be sold in the context of federated learning? It seems like a natural fit there where you're training on a private distribution and you want your model to perform well on other distributions that you don't have access to.\n\nDo you have results for the time it took to train each model (sorry if I missed this)? It seems like your training method uses more information which might be an unfair advantage over other methods if it takes significantly longer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_DML4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697569993410,
        "cdate": 1697569993410,
        "tmdate": 1699636501053,
        "mdate": 1699636501053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "STIpuYSyEs",
        "forum": "VhQUwxIHER",
        "replyto": "VhQUwxIHER",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of deploying a fair machine learning model without using user demographic information. This problem emerges when demographic information is sensitive or private and, therefore, cannot be collected and stored by the model deployer. The paper proposes minimizing the variance of losses during training while roughly maintaining the average loss as a solution to improve fairness without demographics. Experiments are performed to demonstrate that this approach (variance minimization) improves fairness without demographics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022\tThe paper is well written, and the related work is introduced in a very reader-friendly manner.   \n\n\u2022\tThe idea of minimizing the loss variance as a surrogate for improving fairness is exciting. Specifically, it makes practitioners avoid learning group information, increasing privacy. I would appreciate it if the paper gave more intuition about why we should expect that variance minimization improves fairness.   \n\n\u2022\tThis reviewer appreciates the detailed discussion and intuition about how to solve the proposed optimization problem. It was very clearly explained.  \n\n\u2022\tThe results in the paper (Table 1) indicate that the simple proposed approach of decreasing variance may outperform existing methods \u2013 see weaknesses for comments."
            },
            "weaknesses": {
                "value": "\u2022\tThe paper only considers accuracy disparities as fairness metrics. At the beginning of the paper, the authors mention fairness metrics such as equalized odds; however, such metrics are not considered. The authors focus on accuracy disparities and do not provide results for other fairness metrics of major interest in the literature. This reviewer strongly suggests the authors replace two of the metrics in Table 1 with other fairness metrics (e.g., equal odds and statistical parity).  \n\n\u2022\tTable 1 provides results for the performance of different fairness improvement techniques. However, the reported values are very close. Therefore, it is hard to get any conclusion out of the reported values. Can the authors please add confidence intervals?"
            },
            "questions": {
                "value": "\u2022\tHow does this paper relate with works in machine learning that argue that a flat loss landscape implies better generalization [1]? Does variance minimization imply a flat minimum in the loss landscape?  \n\n\u2022\tThe problem of minimizing some model performance constrained to the loss being small enough has been studied in the Rashomon effect literature. The paper may benefit from related work in the field, such as [2]. Moreover, the authors argue that if $\\delta$ (Eq. 1) is small enough, then the \"fair\" model and the ERM are equivalent. A recent paper showed how small $\\delta$ needs to be to ensure that the models are equivalent [3].  \n\n\n[1] Hao Li et al. Visualizing the Loss Landscape of Neural Nets. 2018.  \n\n[2] Amanda Coston et al. Characterizing Fairness Over the Set of Good Models Under Selective Labels. 2021.  \n\n[3] Lucas Monteiro Paes. On the Inevitability of the Rashomon Effect. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5096/Reviewer_6MVk"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709627769,
        "cdate": 1698709627769,
        "tmdate": 1699636500957,
        "mdate": 1699636500957,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qn9HcgmNWt",
        "forum": "VhQUwxIHER",
        "replyto": "VhQUwxIHER",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_NHS2"
        ],
        "content": {
            "summary": {
                "value": "To enhance fairness in situations where sensitive attributes are unavailable, this paper proposes the minimization of the variance of the loss function, specifically minimizing loss distribution\u2019s second moment while not increasing its first moment, dubbed as VFair in this paper. \n\nTo enhance fairness without compromising the model's utility, this paper builds upon dynamic barrier gradient descent [1] and introduces further improvements. During the training process, the neural network's parameters are updated harmlessly, thereby ensuring the model's utility is preserved."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThis paper proposes a simple yet effective method to improve fairness without demographic information, namely by minimizing the variance of the training loss. \n\n\u2022\tFurthermore, to ensure the enhancement of fairness without compromising the model's utility, the harmless update method is used to update the model's parameters. This method ensures that gradient updates align with the primary objective's gradient without causing conflicts.\n\n\u2022\tPoint out the relationship between MAD and the variance of the loss function."
            },
            "weaknesses": {
                "value": "\u2022\tAlthough this is the first time introducing the idea of reducing the variance of the loss function into the fairness domain, similar concepts have been explored in earlier works [2, 3, 4]. It may be advisable to include a more comprehensive discussion in the related work section.\n\n\u2022\tIt is advisable to supplement additional experimental details. For example, a more comprehensive elaboration on the configuration of the ERM model for various datasets, particularly those not discussed in the DRO, would be beneficial. Additionally, it is better to specify the optimizer and the learning rate decay strategy, as these factors may have a impact on the stability and convergence of the loss function. While the paper briefly mentions the setting of the hyperparameter $\\epsilon$, it may appear somewhat ambiguous. It is also necessary to clarify the robustness of the hyperparameter $\\epsilon$.\n\n\u2022\tThe explanation of the optimization objective in Appendix B.2 seems to contradict the main body of the paper. The $\\hat{\\sigma}$ in the main body refers to a different object than the $\\hat{\\sigma}$ mentioned in the appendix.\n\n\u2022\tWhile the gains over baselines are moderate to low, it is advisable to provide standard deviations to better illustrate the stability of the outcomes.\n\n[1] Gong, Chengyue, and Xingchao Liu. \"Bi-objective trade-off with dynamic barrier gradient descent.\" NeurIPS 2021 (2021).\n\n[2] Balaban, Valeriu, Hoda Bidkhori, and Paul Bogdan. \"Improving Robustness: When and How to Minimize or Maximize the Loss Variance.\" 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE, 2022.\n\n[3] Li, Tian, et al. \"Tilted empirical risk minimization.\" arXiv preprint arXiv:2007.01162 (2020).\n\n[4] Lin, Yexiong, et al. \"Do We Need to Penalize Variance of Losses for Learning with Label Noise?.\" arXiv preprint arXiv:2201.12739 (2022)."
            },
            "questions": {
                "value": "\u2022\tIs the \"the sample will be filtered out from updating in this iteration.\" in the second-to-last paragraph on page five meaning not using this portion of the sample for training?\n\n\u2022\tHow is it derived from equation (6) that $\\lambda = 0$ in certain cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737384965,
        "cdate": 1698737384965,
        "tmdate": 1699636500834,
        "mdate": 1699636500834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pUPPrVXSl7",
        "forum": "VhQUwxIHER",
        "replyto": "VhQUwxIHER",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5096/Reviewer_6VBP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes minimizing the standard deviation of classification loss among training samples as a way to promote fairness without access to sensitive attributes (by promoting worst-off group accuracy)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Ensuring worst-off group fairness for unknown group partitions has strong real-world significance.\n- Using variance as a proxy for worst-off group fairness looks novel.\n- The paper is well written."
            },
            "weaknesses": {
                "value": "- Experimental results look underwhelming and not accompanied by confidence intervals or any form of statistical significance tests.\n  - Differences between different methods or to unconstrained ERM do not seem large; it's difficult to attribute the proper importance to the very small metric differences without this context.\n  - Plus, all results correspond to averages over 10 runs, so these details should be easy to compute.\n\n- Code or experimental artifacts are not shared.\n  - Properly reviewing the paper would require access to code and empirical results, since the main claims are quite empirical in nature (e.g., lower loss variance equates to higher fairness).\n\n- It would be important to have a comparison with methods from the constrained optimization literature (standard variance minimization subject to constraint on the empirical risk) or from the standard fairness literature (e.g., with access to sensitive attributes).\n  - Of course it is not expected for VFair to surpass methods that have access to sensitive attributes, but it would be useful to properly contextualize VFair's results.\n\n- Some important details are missing (see `Questions`)."
            },
            "questions": {
                "value": "- Was a standard constrained optimization approach to the problem attempted before constructing the proposed algorithm?\n  - Although the intuition behind VFair is well explained, it seems that to motivate a new algorithm it would make sense to compare to standard CO algorithms or justify in some other way why they are not used.\n  - Why is the standard Lagrangian dual ascent formulation not used? e.g., see [A], and the [TensorFlow CO framework](https://github.com/google-research/tensorflow_constrained_optimization/blob/master/README.md)\n\n\n- > \"Lagrangian function, which however employs a fixed and finite multiplier and thus may not fully satisfy the constraint\"\n  - The Lagrangian function does not employ a fixed multiplier, the $\\lambda$ would have to be optimized jointly with the $\\theta$ until a stable saddle point is reached.\n\n- What ML algorithm is actually used to fit the data? The paper is written w.r.t. generic model parameters $\\theta$, which is fine, but what actual model was used in the experiments? A neural network?\n- Possibly I missed it, but I can't seem to find an actual definition for the loss function $\\ell$ used throughout the paper. Is it BCE?\n  - Multiple plots show the distribution of sample-wise loss for different models; are all algorithms evaluated on the same loss function?\n\n- Is there any explanation for why some (greyed out) rows of Table 2 always result in a constant/uniform classifier over the 10 runs?\n  - Perhaps plotting $\\lambda_1$ and $\\lambda_2$ as well as loss as training progresses could shed some light.\n\n---\n[A]: Cotter, Andrew, Heinrich Jiang, and Karthik Sridharan. \"Two-player games for efficient non-convex constrained optimization.\" Algorithmic Learning Theory. PMLR, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754048637,
        "cdate": 1698754048637,
        "tmdate": 1699636500735,
        "mdate": 1699636500735,
        "license": "CC BY 4.0",
        "version": 2
    }
]