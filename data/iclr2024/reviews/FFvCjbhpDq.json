[
    {
        "id": "Hh7kgch9si",
        "forum": "FFvCjbhpDq",
        "replyto": "FFvCjbhpDq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_5ZRC"
        ],
        "content": {
            "summary": {
                "value": "This work investigates catastrophic forgetting in fine-tuning pre-trained reinforcement learning (RL) policies on subsequent tasks sequentially in a stationary environment while data distribution shifts. It first shows how fine-tuned policies would deteriorate in performance for previous tasks. Then, the paper identifies two conditions in which forgetting occurs, namely, state coverage gap and imperfect cloning gap. Experimentally, the work further shows how existing knowledge retention methods like elastic weight consolidation (EWC) mitigate forgetting during the fine-tuning process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This is an important research problem for both the understanding of deep RL training and potential practical deployments. We have seen extensive studies on fine-tuning of supervised learning. The same aspect in RL is relatively less studied. As deep RL moves towards large-scale pretraining, understanding the best practices of fine-tuning with downstream tasks is crucial.\n2. The paper shows strong empirical analysis in understanding the problem, accompanied with extensive experimental results. I find the identification of the two conditions to be informative to researchers of this subfield\n3. The paper in general is clearly written with key results elaborately explained.\n4. Experimental results are comprehensively displayed. I particularly find figure 4 to be intuitive and helpful in visualizing the forgetting phenomenon."
            },
            "weaknesses": {
                "value": "1. The choice of benchmarking algorithms for knowledge retention, although somewhat representative of existing methods, does not quite match with state-of-the-art approaches. Newer methods like [1], if added, can strengthen the conclusions of the paper.\n2. It is unclear to me how is this setting different from continual/lifelong RL\n3. [Minor] the term \u2018realistic RL algorithms\u2019 is confusing \n\n[1] Ben-Iwhiwhu, E., Nath, S., Pilly, P. K., Kolouri, S., & Soltoggio, A. (2022). Lifelong reinforcement learning with modulating masks. arXiv preprint arXiv:2212.11110."
            },
            "questions": {
                "value": "1. How is non-stationary enironment different from data shifts in stationary environment? Is it not the same underlying data shift problem?\n2. What if we pretrain \u2018CLOSE\u2019 states first instead? Do we see better forward transfer?\n3. Can the authors provide their views on why pre-trained models (counterintuitively) do not seem to exhibit any signs of positive transfer? Existing methods do seem insufficient for RL to leverage pretraining\n4. Why is EWC missing in some of the subsequent experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698677420,
        "cdate": 1698698677420,
        "tmdate": 1699636848034,
        "mdate": 1699636848034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aqwaOeg5UQ",
        "forum": "FFvCjbhpDq",
        "replyto": "FFvCjbhpDq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_bshN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_bshN"
        ],
        "content": {
            "summary": {
                "value": "This is an experimental paper that studies the forgetting issue in finetuning pre-trained models with RL. The paper focuses on two special cases of the problem: state coverage gap and imperfect cloning gap. To study the two problems respectively, the paper compares several existing methods in Meta-World, Montezuma's Revenge, and NetHack. Results shows that RL with behavior cloning on the pre-training dataset outperforms other methods, maintaining the pre-trained capabilities better during RL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Forgetting of the previously learned skills is a problem worth studying in RL. \n\n2. The paper refines this problem into two cases and conducts appropriate experimental evaluation."
            },
            "weaknesses": {
                "value": "1. As an experimental paper studying forgetting, it lacks evaluation on many related methods. The paper only evaluates two kinds of  methods: parameter regularization and behavior cloning. But there exists many other methods addressing the forgetting issue in the literature of continual RL and finetuning with RL, like using offline RL over previous data [1], adding KL-divergence loss to the pre-trained policy on the online data [2], and a lot of methods in sharing representations and structures [3].\n\n2. The experimental results do not provide different insights into the two problems. All the results demonstrate that Finetuning+BC outperforms other methods and the vanilla Finetuning method suffers from forgetting on the FAR states. But beyond that, the results lack further analysis of the two problems and do not reflect the significance of dividing the forgetting problem into the two types.\n\n3. The paper has no novel contributions in methods and techniques. It also cannot provide insights in how to better address forgetting in the future work.\n\n[1] Modular Lifelong Reinforcement Learning via Neural Composition.\n\n[2] Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos.\n\n[3] Towards continual reinforcement learning: A review and perspectives."
            },
            "questions": {
                "value": "1. Can the experimental results provide different insights into the two problems? In addition to these two problems, does the problem of forgetting include other cases?\n\n2. Based on the experimental results, are there any insights in improving the existing methods or further addressing the forgetting problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754355693,
        "cdate": 1698754355693,
        "tmdate": 1699636847907,
        "mdate": 1699636847907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X1vFgAJU2b",
        "forum": "FFvCjbhpDq",
        "replyto": "FFvCjbhpDq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_BuFP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_BuFP"
        ],
        "content": {
            "summary": {
                "value": "Summary:\n\nThis paper studies fine-tuning in RL, and specifically the issue of forgetting and potential mitigation strategies. They demonstrate in several settings (simulated robot manipulation, Montezuma's Revenge and NetHack) that if a policy is pretrained on some part of the state space which is far from the initial state distribution, the knowledge is often forgotten and there are little to no improvements over training from scratch. They furthermore investigate different knowledge retention strategies (such as L2 penalties between pretrained and fine-tuned policy weights, possibly weighted by fisher information, as well as simple BC regularization on the pretraining data). They find that BC regularization helps the most, and can help prevent forgetting the behaviors encoded in the pretrained policy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper's main takeaway message, that adding BC regularization helps avoid forgetting previous behaviors during fine-tuning, is well supported by the experiments. This is demonstrated in 3 environment, including continuous control (MetaWorld), a pixel-based Atari game (Montezuma's Revenge) and a procedurally generated, long-horizon game with complex dynamics (NetHack). \n\n- The paper does a nice job with their analysis and visualizations illustrating the forgetting behavior."
            },
            "weaknesses": {
                "value": "- The main takeaway, which is essentially that co-training on the old tasks prevents forgetting when learning a new task, is pretty unsurprising and has been demonstrated before in previous works in continual learning both for the supervised case and the RL case. It's not clear what the contribution of this work adds. \n- An obvious downside of co-training on previous tasks is that the memory requirement increases linearly with the number of tasks and the computation increases quadratically - this is not adequately discussed. \n- It would have been nice to include result for the L2 and EWC on Montezuma and NetHack."
            },
            "questions": {
                "value": "Some suggestions on the writing:\n\n- In the intro, it would be helpful to give a bit more details on the \"knowledge retention techniques\" used to mitigate the forgetting problems. Currently, the ready does not have much idea on the methodological aspects going into the paper. \n\n- Example in 2-state MDP: the notation here is confusing. Both $theta$ and $f_\\theta$ are used before being defined. Please add the definitions in the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699121669647,
        "cdate": 1699121669647,
        "tmdate": 1699636847788,
        "mdate": 1699636847788,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EhQYXXxdfR",
        "forum": "FFvCjbhpDq",
        "replyto": "FFvCjbhpDq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
        ],
        "content": {
            "summary": {
                "value": "This paper examines finetuning of pretrained RL agents in a single environment. Two problematic mechanisms are identified. A state coverage gap occurs when the agent is pretrained on a part of the state space but, in the fintetuning phase, has to first learn a policy on a different part. Then, the policy on the first part of the state space is lost during finetuning and must be relearned. The second, the imperfect cloning gap, occurs when the agent is pretrained through imitation learning. As the policy is finetuned, the performance on states later in trajectories also degrades. \nThe use of behavioru cloning on states from the first task and other forgetting mitigation techniques are shown to solve these issues. A variety of environments are considered including toy tasks, metaworld and Nethack to demonstrate the problem and the utility of the solutions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- There are extensive experiments on a variety of environments. The sequence of metaworld tasks was an interesting custom addition. \n\n- The identified problem could be relevant in a variety of practical settings. The imperfect cloning gap seems to be particularly applicable since we may often want to start with imitation learning from previous policies if possible. \n\n- There was sufficient detail in the text to understand the experiments and the figures were clear in general."
            },
            "weaknesses": {
                "value": "- The clarity of certain sections could be improved with more details. For example, for the initial toy example, it would help if the main text explained the motivation behind the MDPs design a little more: why that particular choice of transitions and rewards was made. Also, $f_0$ is not described in the main text and it looks like subfigures b) and c) are interchanged for this example.\n\n- The main proposed solution, behaviour cloning from the pretrained policy, seems to be somewhat limited. Behaviour cloning is inherently limited by the quality of the pretrained policy. The experiments show that it's possible to retain the pretrained policy's performance but not exceed it.\n\n- While novelty is difficult to judge, it seems like the identified problematic phenomena are facets of catastrophic forgetting i.e. the idea that neural networks will forget on certain parts of the input space after being trained on others. In this view, it's not too surprising that pretraining on one part of the state space will lead to a detoriation of performance in another. I can appreciate that there's value in demonstrating this in an RL setting though."
            },
            "questions": {
                "value": "- Are the benefits of pretraining purely from learning a good policy? Are there benefits due to the representations learned in the pretraining phase? \n\n- Have you experimented with the agent only learning a decent, but not great, policy on the FAR tasks? Could we expect to surpass the performance of the pretrained policy? Using behaviour cloning would seem to be limited by the pretrained policy.\n\n- Have you considered off-policy methods? It seems like there may be an advantage since these methods could simply keep around samples (or trajectories) from previous tasks in the replay buffer to learn from without having to necessarily imitate the previous behaviours.\n\n- In the robotic sequence task, have you tried pretraining on the second and third tasks? Are there any learning benefits for the 4th task if you do so? \n\n- In Montezuma's revenge, how far is the agent able to reach without pretraining? Does it get past room 7 consistently? It would be nice to see the overall learning curves of the agent that has been pretrained vs. the one that has not.\n\n- Nethack levels are generated procedurally. How are the sequence of levels chosen for these experiments? When the agent is reinitialized, is it to a fixed level with the same seed?\n\n- For the Nethack experiment, Fig.6, how come the learning curve for finetuning only matches that of the original agent after 2 billion steps? It looks like, if training continued further, the pretrained agent would even do worse.\n\n- The Sokoban results (fig.7) seem to be fairly poor for both agents since they can only fill less than 1.5 pits on average---not close to a solution. Is this to be expected?\n\nMinor points:\n- I would consider moving some more of the results from the appendix to the main text since it looks like there's still space remaining.\n\n- In Fig. 4, I would consider changing the text \"pre-trained optimal policy\" to \"pre-trained expert policy\" since we don't necessarily have the optimal policy in those environments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7158/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7158/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7158/Reviewer_dWac"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699158908238,
        "cdate": 1699158908238,
        "tmdate": 1700687189346,
        "mdate": 1700687189346,
        "license": "CC BY 4.0",
        "version": 2
    }
]