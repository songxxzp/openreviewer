[
    {
        "id": "eKPBjNgbpn",
        "forum": "zEOnlJaRKp",
        "replyto": "zEOnlJaRKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission167/Reviewer_4zRf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission167/Reviewer_4zRf"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a systematic study of the adversarial training of neural network solvers for TSP and CVRP. The methodology part extends the adversarial training pipeline by ensembling multiple models, leading to the so-called CNF approach in this paper. Experiments are conducted on TSP and CVRP, with adversarial samples mainly generated by (Zhang et al., 2022)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper is well-written and easy to follow. I could see the efforts of the authors in the organization and the figures.\n* Robustness in solving routing problems is an important measure and worth studying.\n* The experiment study seems extensive and in general seems sound."
            },
            "weaknesses": {
                "value": "* The authors believe that model degeneration on clean data is an important issue, and propose the Collaborative Neural Framework to solve it. However, the motivation for developing such a collaborative framework is not clear. The authors only mentioned that collaboration will mitigate existing issues in adversarial training, but do not explain why. From my side, the reason for the improvements brought by \"collaboration\" is quite straightforward and trivial: \"collaboration\" in this paper means training multiple models in adversarial training and using all of them to predict on the same problem instance. Since we are studying an optimization problem where solution evaluation is very fast, we can easily generate multiple predictions using multiple models and pick the best one. Therefore, it is not surprising that \"collaborated\" models are way better than standalone AT models. \n* One step further on the previous point, it is not surprising to see that the performance of Collaborative Neural Framework is kept on clean instances after adversarial training. One can achieve similar results in Figure 1 by simply freezing one model on the clean data and doing adversarial training on the other models. The \"collaborated\" model will never degenerate on the clean data.\n* The improvements brought by the interesting neural router module seem only marginal. As read from Table 1, there are significant performance improvements when 1) from POMO to POMO_AT (add adversarial training), and 2) from POMO_AT (1) to POMO_AT (3) (use 3 models together, i.e., collaboration). Adversarial training seems not an original contribution in this paper; Collaboration, as discussed above, is not surprising to bring a significant improvement. While with the only technically sound module, the improvement from POMO_AT (3) to CNF (3) is not that significant.\n* The authors made the claim on Page 2 that \"simply increasing the model capacity\" will not help Adversarial Training to generalize better, but I do not find any experimental evidence to support that in the main paper.\n### Minor Points\n* In the title: in combinatorial optimization's convention, VRP is usually considered different from TSP. However, in this paper, the authors seem to call both TSP and CVRP \"subsets of VRP\". I believe it will cause confusion and would suggest changing the naming to \"routing problems\".\n* Please explain \"OOD\" in Section 4.3 to make it self-contained.\n* Not sure if it is proper to call all Reinforcement Learning methods as \"REINFORCE\" in Eq (1) because the RL algorithms deployed nowadays usually integrate many more tricks than the vanilla form in Eq (1)."
            },
            "questions": {
                "value": "* The authors mentioned three papers on the adversarial robustness of CO, and based the major experiment study in the main paper on (Zhang et al. 2022). From my understanding, the other two papers (Geisler et al., 2022) and (Lu et al., 2023) considered the \"hardness\" of the CO problem itself (i.e., there are some guarantees on the optimal objective), while (Zhang et al., 2022) only considered the objective score solved by an existing solver. Can the author justify the reason of selecting (Zhang et al., 2022) as the main experiment protocol?\n* Do the neural network know the behavior of the attacker during training? I.e., are the adversarial data points generated by the attacker in the test dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission167/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission167/Reviewer_4zRf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726633349,
        "cdate": 1698726633349,
        "tmdate": 1700725165287,
        "mdate": 1700725165287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "quGm1jhpLI",
        "forum": "zEOnlJaRKp",
        "replyto": "zEOnlJaRKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission167/Reviewer_APh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission167/Reviewer_APh6"
        ],
        "content": {
            "summary": {
                "value": "To solve vehicle routing problems\u2019 (VRPs) vulnerability to adversarial examples and existing adversarial training methods do not strike a good balance between generalization (on clean instances) and robustness (on adversarial instances). The Collaborative Neural Framework (CNF) is proposed by the authors, but some descriptions in the paper are not very accurate and clear. The novelty is also limited. The following are some of my doubts and suggestions for the paper, hoping to improve the quality of the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The content of this paper is practical and valuable. The experiment of the paper is relatively sufficient."
            },
            "weaknesses": {
                "value": "(1) This paper describes adversarial examples in the field of image processing, but does not give a formal definition of adversarial examples in the field of VRPS, and suggests adding a formal definition of adversarial examples in the field of VRPS.\n\n(2) When the number of trained models is listed in Figure 1, it is only increased to 5. Perhaps when the number of models increases, the traditional adversarial training method can exceed the method proposed in the paper. Therefore, it is suggested to increase the number of trained models to demonstrate the effectiveness of the method. \n\n(3) The basis for selecting an attacker is not elaborated, and it is suggested to prove the generality of the selected attacker.\n\n(4) At the end of the paper, experiments are conducted on the out of distribution data, but the relationship between OOD data\u2019s performance and the adversarial robustness is not explained in detail, so it is suggested to elaborate."
            },
            "questions": {
                "value": "(1) During the Outer Minimization of CNF, there are three instance types, namely \"ori\", \"local attack\" and \"global attack\". How to prove that the instance of \"global attack\" improves the effectiveness of the method? It is suggested to add experiments in this part or give theoretical explanations.\n(2) In the Outer Minimization stage of CNF, a neural router is trained. Will training a neural router seriously increase the training time? It is suggested to clarify in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806483129,
        "cdate": 1698806483129,
        "tmdate": 1699635942254,
        "mdate": 1699635942254,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s2kYbD9hLr",
        "forum": "zEOnlJaRKp",
        "replyto": "zEOnlJaRKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission167/Reviewer_8oTe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission167/Reviewer_8oTe"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors focused on enchance the robustness and generalization of vehicle routing problems. The proposed Collaborative Neural Framework (CNF) enhances robustness by adversarially training multiple models to work together, thus improving defense against attacks and potentially increasing generalization on clean instances. This approach is supported by experimental evidence showing that CNF effectively defends against a range of attacks and also performs well on real-world benchmark instances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The exploration of robustness within the context of vehicle routing problems represents a critical area of research that has received limited attention in prior studies. This paper makes a commendable contribution to the field by addressing this gap.\n\n2. The proposed method CNF stands out for its novelty. It builds upon the established principles of min-max optimization, fundamental to adversarial training, which ostensibly enhances the robustness of the model. The efficacy of CNF is further substantiated by the empirical results presented within the study."
            },
            "weaknesses": {
                "value": "1. It seems that the adversarial attack introduced in this model has no attack budget. While acknowledging the distinctions between Vehicle Routing Problems (VRPs) and image-based tasks, it is important to note that the intrinsic discreteness of VRPs does not preclude the assignment of an attack budget, as demonstrated in adversarial settings pertinent to GNNs. The reviewer posits that evaluating the model's performance across various attack budgets is crucial for addressing the balance between clean accuracy and robust accuracy.\n\n2. In the context of graph-based tasks, the absence of constraints on an adversary typically facilitates a significant degradation in performance, often to levels below random chance. Nevertheless, according to Table 1, the vanilla model's performance does not seem to be significantly compromised. Could the authors elucidate the factors that might be contributing to this unexpected resilience?\n\n3. A considerable volume of literature [1-5] suggests that adversarial data augmentation can bolster generalization capabilities, particularly when the adversarial perturbations involved are small. Should the CNF enhance generalization as well, would this imply that the adversarial attack delineated within this study is potentially suboptimal in terms of its strength?\n\n[1] Xie et al. Adversarial examples improve image recognition. CVPR 2020.\n\n[2] Herrmann et al. Pyramid adversarial training improves vit performance. CVPR 2022.\n\n[3] Wen et al. Adversarial cross-view disentangled graph contrastive learning. 2022\n\n[4] Kong et al. Robust optimization as data augmentation for large-scale graphs. CVPR 2022.\n\n[5] Cheng et al. Advaug: Robust adversarial augmentation for neural machine translation. 2020"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699464036344,
        "cdate": 1699464036344,
        "tmdate": 1699635942134,
        "mdate": 1699635942134,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GaUTgo18OH",
        "forum": "zEOnlJaRKp",
        "replyto": "zEOnlJaRKp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission167/Reviewer_ko2G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission167/Reviewer_ko2G"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel adversarial training based framework (CNF) on VRP tasks by generating global adversarial examples on a set of collaborative formed models, and distributing these samples through a well-designed attention-based neural router to perform effective joint adversarial training. The proposed method, CNF, is claimed to achieve better performance on both accuracy and robustness perspective. Experiments has been conducted by comparing CNF with various baselines on TSP and CVRP tasks. Results looks promising - CNF (3) achieves much better vanilla accuracy and robustness with large gap compared with existing baselines. Extensive ablation studies and analysis has been well presented to show the effectiveness of CNF. The whole paper contains enough experimental details and the overall writing is clear."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The overall novelty is okay but it is interesting to see when it is applied to VRP tasks.\n- Impressive experimental results - proposed CNF is better than other AT baslines by a quite large margin on TSP task.\n- Detailed experimental setup and solid experimental analysis. I really love reading the Section 4.3 OOD generalization part - most of the robust training algorithm do not consider such scenario.\n- The whole paper is well-written. Especially for Section 3, it is well-organized for reader to follow the exact CNF pipeline."
            },
            "weaknesses": {
                "value": "- No training efficiency was discussed while compared with other baselines. According to Algorithm 1, the training cost is largely relied on n, k and for neural router, its inference time is also heavily relied on K. We should have a column showing the exact training time for CNF and other baselines to achieve the table numbers.\n- Notation is quite unclear: in Algorithm 1, n refers to the iterating variable from 1 to B and for later sections, n refers to the total number of generated samples. In Section 3.2, the captital N was introduced to refer the total number of instances. Also k refers to the attack steps while the capital K refers to the topK samples selected for neural router. However, in Section 4.2, the calligraphic K is also used to refer the topK parameter. This makes reader get confused while checking the experimental details."
            },
            "questions": {
                "value": "- (Included in Weakness part) Can you provide the total training time for both CNF and other baselines shown in Table 2? \n- You include the diversity-enhanced ensemble training methods (GAL) as one of baseline methods. However, it has been proved to be not that strong compared with other recent robust ensemble training methods, such as ADP, DVERGE, TRS. Especially for DVERGE, it also claimed to have well-balance between benign accuracy and robust accuracy by crafting adversarial examples on each submodels vulnerability region and reducing adversarial transferability between submodels. It would be interesting to see how DVERGE would perform on the VRP tasks.\n- I'm still quite confused about your inference setup. Given M models you have, it looks unfair to just report the best gap among all models on each instance as the actual robustness: attacker should have the prior information of each models' vulnerability to the attack instance so you should report the worst gap instead. From your side, you do not have the information about the ground-truth so you cannot always choose the best performed model against the unseen attack instances.\n\n\n=============================================================\n\nUpdates:\n\nI thank authors for conducting additional experiments and further clarifying paper notations. These new results and experimental details largely addressed my major concerns. However, I'm still quite confused about the inference reporting metric (Q.3): considering the white-box attack setting, attacker should have all information about your routing strategy and generate adv instances for the whole system instead of the best-performed model. Author should elaborate more on this. I will keep my score but raise my confidence to 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission167/Reviewer_ko2G",
                    "ICLR.cc/2024/Conference/Submission167/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699594966148,
        "cdate": 1699594966148,
        "tmdate": 1700712381908,
        "mdate": 1700712381908,
        "license": "CC BY 4.0",
        "version": 2
    }
]