[
    {
        "id": "lSWl1eYZmF",
        "forum": "MNShbDSxKH",
        "replyto": "MNShbDSxKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of prior visual programming methods exhaustively generating entire code snippets for each new task. Instead, they proposed GNSVR to grow and re-use a library of modules. GNSVR first initializes new modules based on a train set, then evaluates the new modules based on additional few-shot training examples, and finally it executes the modules on the full test set. It shows comparable performance on visual reasoning tasks, can transfer modules to new tasks, and adapt to new visual reasoning tasks with few training examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I am appreciative of the idea of generating more modular and composable modules that are verified, to be used in a library of skills for future tasks. I think the idea of continuously growing this library is innovative, and the method simple and elegant."
            },
            "weaknesses": {
                "value": "W1. Is there anything to prevent overfitting to the small train set of a given task, and only creating specific modules tailored for that domain? I can imagine that these overfitted modules may not be very useful in new tasks. \n\nW2. Isn\u2019t it possible that the method creates a broad function signature, but during stage 2 verification with the small set of train examples, it overfits to a bad implementation that only works for those examples, and therefore actually harms performance from there on out? I\u2019m mainly concerned about the above two overfitting challenges.\n\nW3. There seems to be an assumption that the queries in the train set actually all require similar modules, for example, to verify the new modules, the selected set of test cases from the train set must actually use those modules. I\u2019m not sure if this is a reasonable assumption, and also, how is this sampling of train examples (both in stage 1 and stage 2) done?\n\nW4. In the experiments, are Visprog/Viper also given the same few-shot training set? For example, I believe you can use the same method of error correction and correct both Visprog/Viper when it is incorrect. In this way, you can include Visprog/Viper comparison in Tables 3 and 4. Would be great to better disentangle what drives this improvement of performance -- modularity in the proposed modules, or the training examples given in the loop, etc."
            },
            "questions": {
                "value": "Q1. Please clarify the selection of training examples used in stage 1 and 2. \n\nQ2.  \u201cIf a program encounters errors during execution, we incorporate the error information into the LLM\u2019s prompt and instruct it to rectify these issues.\u201d How is this done?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_JpcA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698285393394,
        "cdate": 1698285393394,
        "tmdate": 1699636792932,
        "mdate": 1699636792932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4iIhr5iJO1",
        "forum": "MNShbDSxKH",
        "replyto": "MNShbDSxKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes GNSVR, a neuro-symbolic system that grows a library of modules for visual reasoning tasks through the use of LLMs. It consumes a small set of example training tasks, and generates new modules with a two-step process. In step 1, a LLM is queried to see if an example task can be solved given functions from a base API, if the LLM responds negatively, then it is instructed to propose a new module to add into the API that would help solve this task, as well as to produce an I/O example for how this module would be used to solve the task. In step 2, given doc-string descriptions of these would-be useful modules, an LLM is prompted to author a python function, potentially referencing base-API logic, that matches the description and corresponds to its previously generated I/O example. At test-time, these modules are integrated into the API, and a LLM uses API calls to produce a program that can reason over visual input matching the logic of the input question. The system is primarily evaluated against other neuro-symbolic systems for visual reasoning that operate with a fixed API (VisProg, ViperGPT). Compared to the previous systems, GNSVR offers competitive performance on standard benchmarks (GQA, RefCOCO), and demonstrates promising results that its discovered modules can help generalize to related tasks, like image editing and knowledge tagging."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, I think the direction of the proposed method is sound and compelling; systems like VisProg and ViperGPT provide promising ways to push neuro-symbolic reasoning past toy-domains, but they are limited by fixed APIs. Using LLMs to augment these APIs in a task-specific manner is an interesting idea with many potential avenues for future exploration.  \n\nFrom a methodological stand-point, I think the division of labor between the module initialization step and the module generation step is a neat insight. This design decision allows the LLM to create it's own tests, that can then be used to help guarantee that LLM generated functions 'do what they are supposed to'. \n\nThe experimental results are largely positive for the proposed method, although I have some concerns about their design that I'll detail below. That said, I think the experimental results do support the claim that the proposed system offers improvements over both VisProg and ViperGPT across tasks. Perhaps the most compelling claim is that the modules that GNSVR finds from the GQA and RefCOCO tasks can \"transfer\" for related tasks of image editing and knowledge tagging, supported by the fact that GNSVR significantly outperforms VisProg in this setting. The evaluations against fully-supervised methods on the Raven and MEWL tasks is also fairly impressive, considering GNSVR is operating in a few-shot learning paradigm."
            },
            "weaknesses": {
                "value": "# Main Concern\n\nFrom my perspective, the biggest current weakness of the paper is that from the experimental design its hard to parse out exactly how the discovered modules affect the system's performance. Ostensibly, this can be gleaned from comparisons between GNSVR and VisProg/ViperGPT, but there are more differences between these systems beyond merging in discovered modules. Specifically, GNSVR uses a \"base API\" that is a combination of VisProg and ViperGPT, so the \"fair\" comparison would be against an ablated version of GNSVR that removes steps 1 and 2, and just tries to solve test-problems with the original API functions. This condition is considered in the ablation experiment (GNSVR w/o ML), but only a subset of the RefCOCO test-set. To solidify the claim that the improvement GNSVR observes stems from its discovered modules, this base condition should be added to all of the experimental set-ups (tables 1-5), for example, from Table 2 its unclear how much of the delta improvement between VisProg and GNSVR can be attributed to improvements in the base API versus improvements to the API from the new modules. \n\nBeyond this, I'm also slightly concerned about the design of the GNSVR w/o ML baseline. At inference time, is this baseline allowed to invoke arbitrary python logic in the style of ViperGPT (e.g. standard control flow constructs) or is it restricted to *only* using API function calls in the style of VisProg. I would imagine that the first condition would be more fair to evaluate GNSVR. Solving some tasks might require simple logic that the LLM knows how to express in python, but might not be directly expressible with a series of API calls. In GNSVR, this logic is incorporated into modules, but in the baseline the LLM should still have the opportunity to invoke similar logic in its test-time solutions (otherwise its impossible to properly evaluate the usefulness of the discovered modules). Please clarify which of these modes the baseline is operating in. \n\n# Minor\n\nCompared to VisProg and ViperGPT this system seems to require more training data, as the I/O pairs are not only used to populate in-context examples, but also impact (i) what module concepts are proposed and (ii) how the correctness of each module concept is evaluated. This point about reliance on training data is touched on in the ablation section, but it would be good to make this distinction explicit when comparing the pros/cons of the proposed system against past work."
            },
            "questions": {
                "value": "While the broad-stroke description of the method is clearly presented, many of the important details were left unspecified (making reproducibility challenging without a reference implementation). To improve clarity, and help understanding of the results, I think the below questions would be important to answer in future revisions:\n\n# 1. Module Generation \n\n(a) Can new modules be hierarchical (e.g. can one proposed module referenced a previously proposed module), or can they only call out to the original API functions?\n\n(b) The error-correction and pass-rate logic for the module generation step are not clear. What is the \"pass-rate\" at which a function is accepted as a new module, from the listed examples it seems like a single input/output pair is used, so is the pass rate 1? \n\n(c) What exactly is the error-correcting mechanism when one of the authored functions produces an error -- is it sent back through the LLM with a modified prompt? How many times? What constitutes an error? This seems like a potentially important part of the contribution, so I would encourage the authors to even consider adding an ablation condition demonstrating this is helpful for the performance of the system.\n\n(d) It would be informative to provide additional details on the generated modules for each task: how many are created for each task? How often does each one get used in solving test-cases? Do they each capture distinct concepts, or are some modules duplicates?\n\n# 2. In-context examples\n\nI don't believe there is any information into how in-context examples are chosen, which can greatly affect LLM performance.\n\n(a)  From the example prompts, it looks like a single function is given as an in-context example for step 1 and step 2, are these always COMPARE_SIZE and LOC, as shown in figures 15 and 16? \n\n(b) For the inference in-context examples are these chosen randomly from the training tasks that found a successful program? \n\n# 3. MISC \n\n(a) For the Raven task, my understanding is that the input is just a series of images. If this understanding is correct, how do you turn these images into a question? I am also quite curious to see the internals of the SOLVER generated module, is this module shared between Raven and MEWL, or does it employ distinct logic?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_vHZ5",
                    "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767372221,
        "cdate": 1698767372221,
        "tmdate": 1700508778236,
        "mdate": 1700508778236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vlzGzJslie",
        "forum": "MNShbDSxKH",
        "replyto": "MNShbDSxKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_QVVg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_QVVg"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a Generative Neuro-symbolic Visual Reasoning Model (GNSVR) for vision-language tasks that involves creating, testing, and re-using neural \"modules\" i.e. neural network components that are steerable and solve well-defined abstraction and reasoning tasks. To evaluate these modules, a pre-trained LLM model is used to first determine whethere a new module is needed given a reasoning training set. Then, the LLM is asked to generate the function signature of the new module as well as an execution (reasoning) script to solve the given reasoning problem. The signature is used to generate a new module, which is evaluated against few samples to ascertain whether it is the right component. Once a generated module satisfies this criteria, it is added to the library of modules and the process is repeated. The empirical results show that GNSVR is able to develop novel reasoning modules, and utilize them successfully for a variety of vision language reasoning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Firstly, the paper is very well written and the GNSVR method is very well explained, with a healthy split between using the main paper and supplementary for splitting the key information versus additional information.\n\nThe empirical performance of GNSVR for transfer tasks, as well as the examples of the modules generated is quite impressive - it would justify the overall approach to growing modules for reasoning and their effectiveness for few-shot  generalization."
            },
            "weaknesses": {
                "value": "There are several components to the GNSVR framework but the paper provides no detailed analysis in the main paper of the importance of each of the components of the GNSVR framework.  While I think the framework overall is useful, the components are not all equally important to solve the reasoning problem and hence it is important to understand for future research on modular reasoning to understand what works and what doesn't, and if so why not.\n\nHow big of a role does \"good initlialization\" of the neural module operators plays?How important is defining the correct input and output format for a new module? How important is the selection of the few shot samples to evaluate a new module? (the authors say \"We extracted 300 examples from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL based on experimental experience.\" - is the experience here just cherry picking for results or something else?) How important is the LLM capability to learn new modules? What role does the prompt play for the LLM in evaluating existing modules and creating new ones? Without detailed analysis to support answers to all these questions, the paper is limited in terms of explaining the method beyong just presenting a new method and showing empirical results.\n\nLastly, I would not suggest the authors not report results on the RAVEN dataset. As shown independently in [1] and [2] the dataset contains flaws in choice design which enables models to learn shortcuts to solve the RPM reasoning task. I would recommend the authors use the i-RAVEN dataset inroduced in [1] instead.\n\n**References** \n\n1. Hu, S., Ma, Y., Liu, X., Wei, Y. and Bai, S., 2021, May. Stratified rule-aware network for abstract visual reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 2, pp. 1567-1574).\n2. Spratley, S., Ehinger, K. and Miller, T., 2020. A closer look at generalisation in raven. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVII 16 (pp. 601-616). Springer International Publishing."
            },
            "questions": {
                "value": "* Could the authors comment on why they chose to go with a neuro-symbolic approach versus a purely neural approach for defining the neural module scripts? Is it only for controllability, and if so can they comment on how a neural script (e.g. an LLM that abstracts the problem of generating the script and executing it) would compare? There have been approaches towards learning neural scripts and executing them dynamically at inference time for reasoning tasks in prior work e.g. [1]\n\n* I think the module initialization and evaluation during generation in GNSVR is closely related to the research on automatic group discovery and using it for model design paradigm introduced in computer vision previously e.g. [2, 3]. It would make the related works section more comprehensive to include discussion on how GNSVR relates to this growing subfield of automatic evaluation and model design.\n\n\n**References**  \n\n1. Rahaman, N., Gondal, M.W., Joshi, S., Gehler, P., Bengio, Y., Locatello, F. and Sch\u00f6lkopf, B., 2021. Dynamic inference with neural interpreters. Advances in Neural Information Processing Systems, 34, pp.10985-10998.\n2. Vendrow, J., Jain, S., Engstrom, L. and Madry, A., 2023. Dataset interfaces: Diagnosing model failures using controllable counterfactual generation. arXiv preprint arXiv:2302.07865.\n3. Gao, I., Ilharco, G., Lundberg, S. and Ribeiro, M.T., 2023. Adaptive testing of computer vision models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4003-4014)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699290393149,
        "cdate": 1699290393149,
        "tmdate": 1699636792677,
        "mdate": 1699636792677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3Js79px0vh",
        "forum": "MNShbDSxKH",
        "replyto": "MNShbDSxKH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
        ],
        "content": {
            "summary": {
                "value": "This paper relies on the abilities of Large Language Models, but in effect extends them by attempting to enable reuse of code, as opposed to generating code from scratch.  They focus on the visual domain, where tasks include asking questions which require both visual analysis and symbolic reasoning.  They demonstrate state-of-the-art performance on several tasks"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper makes a clear case for it's own contribution, and that contribution does appear to be valuable - visual learning is an important task, and the prohibitive cost of using SOTA large language models makes reuse of code appealing (although they explicitly don't use ChatGPT4 \"due to the prohibitive cost\", so maybe this is less of an argument than it would be otherwise).  The fact that they can show it's use on several domains and types of tasks is also appealing."
            },
            "weaknesses": {
                "value": "My main concern is that, based on the presentation, it seems that the authors took a lot of highly intricate API's for LLM's that large teams may have worked on and cobbled them together to solve a new task.  I refer to this section: \"The success of our GNSVR relies on a set of pre-defined modules and APIs as the starting point. We utilize handcrafted modules from VisProg (Gupta & Kembhavi, 2022) as our initial components. Additionally, we incorporate several new APIs from ViperGPT to enhance module creation. We also include some new APIs from ViperGPT (Sur\u00b4\u0131s et al., 2023) for making new modules.\"  I appreciate their novelty in how they use these API's, but the ratio of insights of these authors vs of the authors of the API's appears insignificant.\n\nI'm also not entirely convinced of the novelty of this paper.  I refer to \"Iterative Disambiguation: Towards LLM-Supported Programming and System Design\" (Pereira and Hartmann) and \"Self-planning Code Generation with Large Language Models\" (Jiang et al).  I don't think the fact that this is in the visual domain is enough to call it \"novel\", because there is virtually no engagement with visual modalities by the authors - as stated above, according to my understanding, they are using predefined modules which handle the interface between vision and language.\n\nIf I was given evidence against either of the two above claims - that all of the work (particularly the \"visual reasoning\" work) is being done by existing tools, or that the paper is not fundamentally using LLM's in a novel way - I would be happy to increase my score."
            },
            "questions": {
                "value": "Please clarify exactly where you created novel algorithms or ideas.  If any of those ideas require more than iterative prompting, please state so explicitly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6843/Reviewer_CDRY"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6843/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699541650515,
        "cdate": 1699541650515,
        "tmdate": 1699636792549,
        "mdate": 1699636792549,
        "license": "CC BY 4.0",
        "version": 2
    }
]