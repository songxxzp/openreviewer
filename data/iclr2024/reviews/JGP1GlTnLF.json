[
    {
        "id": "HqA3h78wXc",
        "forum": "JGP1GlTnLF",
        "replyto": "JGP1GlTnLF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_5Q8T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_5Q8T"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple and effective training-time backdoor defense method called Learning from Distinction (LfD). Specifically, LfD first trains a low-capacity model as the teacher model to guide the learning of a clean student model. During the training process of student model, the losses of teacher can be uses to discriminate the poisoned samples and clean samples. The objective of student model is to maximize losses of poisoned samples by weighting training data. Extensitive experiments demonstrate the effectiveness of proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is simple and effective. The losses of trained low-capacity teacher model provides the information to weight the training data. With these weights, the student model can defend backdoor attack. Extensitive experiments on three datasets demonstrate the effectiveness of proposed method. Also, ablation studies including the analysis of hyperparameters \\alpha and \\beta and capacity of teacher model, are conducted to demonstrate the effectiveness of proposed method."
            },
            "weaknesses": {
                "value": "Experiments are not sufficient. Though the paper uses seven dirty-label backdoor attacks and two clean-label attacks, these attacks are not recent works. There are some recent backdoor attacks e.g. WaNet [1] to be deffend. \n\nSince the proposed method is a training-time backdoor defense, the paper should compare results with more other training-time defenses e.g. DBD[2].\n\n\n[1] Nguyen, Tuan Anh, and Anh Tuan Tran. \"WaNet-Imperceptible Warping-based Backdoor Attack.\" International Conference on Learning Representations. 2020.\n[2] Huang, Kunzhe, et al. \"Backdoor Defense via Decoupling the Training Process.\" International Conference on Learning Representations. 2021."
            },
            "questions": {
                "value": "When the paper says \"fine-tuning\" a teacher model, does \"fine-tuning\" mean that the teacher model is trained from a \"pre-trained\" model?\n\nIs the hyperparameter \\gamma in equation (5) and (6) (or \\alpha in equation (4)) dataset-dependent? Is this hyperparameter related to the networl architecture of teacher and student models? It is better to provide more ablation studies."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Reviewer_5Q8T"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706450780,
        "cdate": 1698706450780,
        "tmdate": 1699636856415,
        "mdate": 1699636856415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3vAOGxjts3",
        "forum": "JGP1GlTnLF",
        "replyto": "JGP1GlTnLF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_8dBF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_8dBF"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel training-time defense method called Learning from Distinction (LfD) to protect deep neural networks (DNNs) from backdoor attacks. LfD uses a low-capacity model (losses of compromised samples and benign samples can be more distinctive) as a teacher to guide the training of a backdoor-free student model. Results on different datasets demonstrate that LfD reduces ASR while having a minimal impact on benign accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper finds that the losses of compromised samples are more distinguishable from benign samples on low capacity models.\n- Extensive experiments including those on Cifar, GTRSB and ImageNet subsets."
            },
            "weaknesses": {
                "value": "- Intuitively, findings make sense but need more theoretical proof or performance guarantees. Although this paper briefly analyzes that smaller models will have higher error rates, it is difficult to infer that smaller models are, therefore, better at distinguishing the loss values of compromised and benign samples (so it is an assumption, not a conclusion). Also, empirically, both Figure 1 and Figure 2 need to include more attack methods, as well as more models and datasets.\n\n- Lack of comparison with state-of-the-art attacks/defenses. The most recent baseline is from 2021. This paper could include more advanced attacks and defenses. Specifically, it could include some stealthy attacks that barely change the distribution of model weights (intuitively, such attacks will not cause significant loss differences) and some defense methods based on weight analysis. Meanwhile, this paper can explore adaptive attacks. \n\n- Lack of evaluation on non-attack settings. We usually cannot guarantee that the dataset must have compromised data. Therefore, it would be helpful to evaluate how benign accuracy is impacted when there is no attack data. \n\nMinors:\n\nThe baseline results on the same dataset look different from the existing work. For example, the original ABL paper has an ASR of 1%-20% on ImageNet. But ABL's ASR in this paper can reach 70%. Please indicate the differences in settings from the original paper or follow its original settings.\n\nAnother concern is that it needs to be clarified why the baseline settings is not consistent across different datasets (more baselines for smaller datasets)."
            },
            "questions": {
                "value": "See comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7207/Reviewer_8dBF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713274948,
        "cdate": 1698713274948,
        "tmdate": 1699636856282,
        "mdate": 1699636856282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rnDzyAMZjy",
        "forum": "JGP1GlTnLF",
        "replyto": "JGP1GlTnLF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_as8q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_as8q"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author proposed using a low-capacity model to defend against backdoor learning. The key idea is to use the task difficulty difference between the original task and the backdoor task to identify the potential poisoned samples. The author then proposed a weighted training pipeline to mitigate the impact of poisoned samples to defend against backdoor learning. The author shows that the proposed method achieved better defense performance compared to the baseline method and had minimal impact on the original task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized and easy to follow.\n2. The proposed method achieved better performance in terms of low ASR and high CA"
            },
            "weaknesses": {
                "value": "1. The paper's key observation that low-capacity models can easily learn backdoor functions is not particularly novel. This finding has already been discussed in a few prior studies.\n2. The authors have not addressed the potential for adaptive attacks in their methodology. Specifically, it is unclear whether their proposed defense mechanism would remain effective if the backdoor samples were made more complex and thus harder to learn.\n3. One noteworthy adaptive attack is the \"all-to-all attack,\" which requires the model to initially learn the primary task before adapting to the backdoor function. Theoretically, the defense mechanism proposed in the paper may not be adequate for countering this particular type of attack. While it is unreasonable to expect a defense method to cover all possible adaptive attacks, the paper falls short in addressing this obvious example. Consequently, the study presented lacks comprehensiveness in evaluating its robustness against different types of adaptive attacks."
            },
            "questions": {
                "value": "Could you provide the precision and recall metrics for the detection of poisoned samples using the proposed method? Additionally, it would be valuable to compare these results with those achieved by previous methods to offer a comprehensive evaluation of the new approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721975601,
        "cdate": 1698721975601,
        "tmdate": 1699636856154,
        "mdate": 1699636856154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "buMVAzjDVi",
        "forum": "JGP1GlTnLF",
        "replyto": "JGP1GlTnLF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_7h85"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7207/Reviewer_7h85"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an idea of training time backdoor mitigation. The key idea is\nto leverage a low capacity model as the teacher to distillate a student model.\nExperiments on CIFAR, GTSRB, and a subset of ImageNet showed the efficiency of\nremoving backdoors."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using a low capacity model is interesting. It will be useful if we\ncan understand the relationship between the model capacity and the training\ndataset.\n\nThis is a timely and important topic."
            },
            "weaknesses": {
                "value": "The fundamental assumption of the paper is consistent with ABL (Li et al.,\n2021), which is the losses of poisoning data and clean data are distinct. This\npaper aims to amplify the differences, as the difference may not be able to\ndistinguish. I would like to point out recent work has discussed the observation\nof ABL, and many proposed counterexamples. Moreover, Wang et al., 2022 also\nmentioned the existence of natural trojans, which are backdoors learned from\nnatural and benign datasets. Khaddaj et al. 2023 aruge that backdoor features\ncan be natural features of a dataset. In other words, the assumption is wrong\nand does not hold.\n\nAlso, I would like to mention that ABL surely has done more comprehensive study\non the loss of different attacks and settings before drawing the conclusion. The\nexperiments done in this paper is relatively weak, making it less convincing.\n\nI also have questions regarding the capacity of the model. Training low-capacity\nmodel is hard, as discussed by the lottery ticket theory. But if a method can\nsuccessfully find the winning ticket, it is relatively easy to train a model\nthat is tiny while maintaining high accuracy. Existing work indicates the size\ncan be more than 10x smaller while still having great accuracy. I wonder what\nsize the teacher models are in this method? \n\nA broader question is that is it possible that the pruning method used in\nfinding the teacher model can significantly affect the final result? It is also\nalmost impossible to know what capacity is needed, and what is learned by the\nmodel, so how do you verify that the features are not in the teacher model\n(namely, it is the low capacity rather than distillation that is the main\nfector)?"
            },
            "questions": {
                "value": "See comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772164517,
        "cdate": 1698772164517,
        "tmdate": 1699636856041,
        "mdate": 1699636856041,
        "license": "CC BY 4.0",
        "version": 2
    }
]