[
    {
        "id": "kHapeueync",
        "forum": "PcBJ4pA6bF",
        "replyto": "PcBJ4pA6bF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission545/Reviewer_phME"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the decentralized federated learning with both data and model heterogeneity. To solve this problem, the authors introduced a novel DESA method, which generated global synthetic anchors to guide the local model training. For each client, in addition to standard supervised classification loss, it would also consider the classification loss over synthetic anchors and cross-client knowledge distillation losses for improving the model's generalization performance. Experimental results validated the effectiveness of DESA over baselines with respect to both inter- and intra-client prediction performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**(1) Originality:** This paper handled both data and model heterogeneity in decentralized federated learning problems without public data. Technically, it proposed to generate synthetic anchor data from each client. Besides, supervised contrastive loss between client data and synthetic data was introduced to mitigate the data heterogeneity among clients. The knowledge distillation loss over synthetic data was designed to mitigate the model heterogeneity among clients. The generalization performance of the proposed DESA method was theoretically analyzed.\n\n**(2) Quality:** DESA used synthetic anchors to solve the issues of decentralized federated learning without public data. It further leveraged contrastive loss and knowledge distillation loss to handle data and model heterogeneity. The hyperparameter analysis in the experiments also validated the impact of those two losses on the proposed DESA method.\n\n**(3) Clarity:** Overall, the presentation of this paper is easy to follow. This paper illustrated the three crucial components of DESA in different subsections. The effectiveness of DESA was evaluated on a variety of benchmarks, including both heterogeneous and homogeneous model settings.\n\n**(4) Significance:** The studied decentralized federated learning is practical in real-world applications, especially when no public data is available among local clients. Thus, the developed method without leveraging public data in this paper can be applied to more general FL problems compared to previous works relying on public data."
            },
            "weaknesses": {
                "value": "The weaknesses of this paper are summarized below.\n\n(1) The research question is not well motivated. This paper studied decentralized federated learning regarding the performance of every client model on other clients. Traditional FL settings might focus only on the performance of every client model on its own client domain. Thus, it might be more convincing to provide some practical examples to illustrate why inter-client test accuracy should be emphasized in real FL scenarios.\n\n(2) The introduction shows that the proposed approach aims to generate minimal synthetic anchor data to enhance client-model generalization. However, this \"minimal\" property of generated data is not discussed in the experiments. The ablation study in subsection 5.4 shows that the size of synthetic data can significantly affect the inter-accuracy. Thus, there might exist a trade-off between the model performance and the size of synthetic data. More explanations can be provided here.\n\n(3)  Subsection 3.1 shows that synthetic anchor data is shared amongst the client\u2019s neighbors. However, it is unclear how the neighbor information is defined in the experiments. In addition, it seems that the synthetic anchor data $D^{Syn}$ in Subsection 3.2 simply combines all the anchors $D^{Syn}_i$ within each client.\n\n(4) The definition of distribution matching in Eq. (3) is confusing. First, it is unclear why this term can guarantee the class-imbalanced anchors. How is the function $\\psi^{rand}(x|y)$ affected by the class labels? Second, it is defined over all clients $i=1,\\cdots, N$. Then why does the generation of anchors within client $i$ rely on the data on other clients? Third, it is not explained whether the minimization of MMD between true data and anchor data would increase the risk of privacy leakage. That is, when anchor data becomes more similar to the true data, it is more likely to include the private domain information.\n\n(5) In the derived generalization in Theorem 1, it assumes (i)  real labeling and synthetic data labeling are similar, and (ii) real labeling and distillation data labeling are also similar. It is confusing how both assumptions can always be guaranteed in real scenarios.\n\n(6) The experimental settings show that for heterogeneous model experiments, multiple baselines are compared, including FedMD, FedDF, FCCL, FedGen, and VHL. But Table 2 only lists the results of FedHe, FedDF, and FCCL."
            },
            "questions": {
                "value": "(1) The client structure information is not provided in the experiments. Does it imply that all the clients are connected with each other in all experiments?\n\n(2) In subsection 3.3., \"$P(\\cdot)$ index class category\" is confusing. Where is $P(\\cdot)$ used in this section?\n\n(3) The communication costs of DESA can be analyzed, because it might include additional anchor sharing and logits sharing compared to baselines.\n\n(4) Some notations used in Theorem 1 are undefined, e.g., $d_{H\\Delta H}$, $\\lambda(P_i)$, etc. In addition, what does \"if $\\psi_i \\circ P^{Syn} \\to \\psi_i \\circ P^T$ for any $\\psi_i$\" imply?\n\n(5) Below Proposition 2, it is shown that when the local data heterogeneity is severe, the model learning should rely more on the centralized data, e.g., synthetic data and the extended KD data. This can be verified in the experiments, e.g., how the hyperparameters $\\lambda_{REG}$ and $\\lambda_{KD}$ can be changed with respect to the data heterogeneity.\n\n(6) \"FedSAB\" in subsection 5.3 is undefined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698020567923,
        "cdate": 1698020567923,
        "tmdate": 1699635981647,
        "mdate": 1699635981647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hh6lbWywV1",
        "forum": "PcBJ4pA6bF",
        "replyto": "PcBJ4pA6bF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
        ],
        "content": {
            "summary": {
                "value": "The paper studied data and model heterogeneities in decentralized federated learning (FL), which is a serverless FL setting. In particular, the paper focused on the generalization, beyond personalization, of client models. The proposed method, DeSA, leverages synthetic anchors using data generation techniques to introduce two effective regularization terms for local training: anchor loss that matches the distribution of the client\u2019s latent embedding with an anchor, and KD loss that enables clients learning from others. Experiments demonstrated the effectiveness of DeSA on intra- and inter-domain tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper considered a complex setting where both data and model heterogeneities are present, which can be hard to tackle in general. New loss terms are introduced to deal with the heterogeneities, and data synthesis technique are used to avoid sharing real data. The approach is reasonable and justified.\n\n2. The paper provided extensive experimental results to demonstrate the effectiveness of DeSA, which is compared against methods from both model heterogenous and homogeneous settings."
            },
            "weaknesses": {
                "value": "1. The motivation of considering generalization ability of client models on inter-domain tasks is not clear. In the model heterogenous setting, each client may process a model with a different architecture,  which is compatible with its own configuration. While the client can benefit from other clients\u2019s data to train a personalized model, why does this model have to perform well on other clients\u2019 tasks too? Other clients may not be able to acquire or deploy the model.\n\n2. In experiments, from Table 2 it seems that data heterogeneity and model heterogeneity are correlated. That is, each dataset is assigned one model architecture. The results would be more interesting if both different datasets and models are assigned independently (by dividing a dataset into multiple clients)."
            },
            "questions": {
                "value": "1. In DIGITS and OFFICE experiments, what is the number of clients? Is a client identified as a dataset?\n\n2. How does DeSA perform in cases where each client has limited training data, e.g., a few samples per class? Can each model benefit more from the global synthetic dataset and KD?\n\n3. Minor issues:\n- In Equation (3), the definition of D_i^{Syn} involves summation over i.\n- In Equation (6), L_{CE} is not used (but introduced right after the equation).\n- First line of Section 5.3: FedSAB?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission545/Reviewer_xb5U"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734439237,
        "cdate": 1698734439237,
        "tmdate": 1699635981571,
        "mdate": 1699635981571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9wzohjNdAY",
        "forum": "PcBJ4pA6bF",
        "replyto": "PcBJ4pA6bF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission545/Reviewer_VrA4"
        ],
        "content": {
            "summary": {
                "value": "The paper explores decentralized federated learning, focusing on both data and model heterogeneity\u2014a notably challenging context where traditional FedAVG and its derivatives fall short. The authors introduce a novel approach, DESA (Decentralized FL with Synthetic Anchors), which employs synthetic anchors to act as class-specific feature centers. To generate these synthetic anchors, the authors utilize randomly sampled feature extractors and optimize data points using the empirical maximum mean discrepancy (MMD) loss. Subsequently, each client is trained using anchor loss and knowledge distillation loss to combat data and model heterogeneity, respectively. Experimental validation is conducted on domain-shifted datasets: DIGITS, OFFICE, and CIFAR10c, where DESA shows superior performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem formulation is both rigorous and practically relevant.\n\n2. Experimental evidence substantiates the efficacy of DESA on DIGITS, OFFICE, and CIFAR10c datasets."
            },
            "weaknesses": {
                "value": "1. Certain aspects of the paper remain ambiguous.\n1-1. Equation (2) introduces an objective that encompasses all clients for defining inter-client loss. However, the decentralized nature of the problem implies that each client can communicate only with adjacent nodes, raising questions about the feasibility of this objective.\n1-2 Equation (3) suffers from unclear terminology; specifically, the meaning of the representation (x\u2223y) is not explained. Additionally, the methodology for generating \"randomly sampled feature extractors\" is also unclear.\n1-3. Equation (4) contains undefined notations, requiring clarification.\n2. The paper touches upon privacy concerns arising from the sharing of synthetic data but fails to delve deep enough into this critical issue. Given the importance of privacy in federated learning algorithms, the authors should offer a more comprehensive discussion, preferably in the main text rather than relegating it to the appendix.\n3. The theoretical results are primarily based on Ben-David et al. (2010), a fact highlighted in the appendix but missing from the main text. This could potentially weaken the paper\u2019s contribution.\n4. The paper struggles to bridge the gap between theoretical claims and empirical results. The presented theorems are contingent upon strong assumptions, and their relevance to the experimental findings is not intuitively obvious."
            },
            "questions": {
                "value": "ould you please clarify the issues listed under \"Cons\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846497709,
        "cdate": 1698846497709,
        "tmdate": 1699635981498,
        "mdate": 1699635981498,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nAD3hcHU1C",
        "forum": "PcBJ4pA6bF",
        "replyto": "PcBJ4pA6bF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission545/Reviewer_2BZe"
        ],
        "content": {
            "summary": {
                "value": "Decentralized FL enables clients to own different local models and separately optimize local data. How can every client's local model learn generalizable representation is unknown. To address this question, This paper proposes a Decentralized FL technique by introducing Synthetic Anchors, as DESA. Authors leverage the synthetic anchors to implement 1) anchor loss that matches the distribution of the client's latent embedding with an anchor and 2) KD loss that enables clients learning from others. The proposed method doesn't presume access to real public or a global data generator."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The studied problem is novel and well motivated.\n2. Distilling local synthetic anchor is interesting.\n3. There are theoretical analysis of the proposed methods, in which the new generalization bound is better.\n4. Figure 3 is interesting, jointly considering worst local accuracy and global accuracy. Experiment results show significant improvements of the proposed method."
            },
            "weaknesses": {
                "value": "1. The local synthetic anchor dataset iss shared. Thus, the privacy of the synthesized anchor should be considerred. Although the DP is used to protect synthetic anchor. But could this defend against recovering the raw data?\n2. It would be better to conduct a more ablation study to decouple the effect of the sythetic anchor and the KD loss."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847465472,
        "cdate": 1698847465472,
        "tmdate": 1699635981435,
        "mdate": 1699635981435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yk8h7pEK1k",
        "forum": "PcBJ4pA6bF",
        "replyto": "PcBJ4pA6bF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission545/Reviewer_R93g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission545/Reviewer_R93g"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of decentralized mutual learning. The challenges of decentralized mutual learning, other than the ordinary data non-iid issue, include model-heterogeneity and no server-coordination. This paper tackles this problem via constructing synthetic anchor data, whose information is shared across clients to bridge the large gap among data distributions. The paper further designs novel losses including regularization loss for representations of both anchor and true data; and a knowledge distillation loss to tackle model heterogeneity issue. Some theoretical insight is provided and numerical experiments on several benchmarks show convincing results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Disclaimer: the reviewer is not very familiar with the anchor data generation in federated learning. Thus, I may not accurately assess the novelty of the technique proposed by this paper.\n\n- the problem this paper considers is interesting and important. Features like no central server coordination and model heterogeneity make practical sense.\n\n- the proposed algorithm is intuitive, has theoretical insight. And it seems to be also communication-efficient since only logits of anchor data require to be transmitted across clients.\n\n- the overal presentation is very good, and I find enjoyable to read the paper.\n\n- experimental results seem to be convincing."
            },
            "weaknesses": {
                "value": "- overall I find the designed model contains a lot of subtlety, as it is quite complex and contains many components. So it appears a bit difficult to probe what really works and what does not.\n\nFor example, \n\n(a) how difficult is the data synthesis process (i.e. eq. 3) when the data is highly non-iid across clients. Since it basically minimize discrepancy between representations of local data and global data, does this process always successfully generate satisfactory anchor regardless of how data is partitioned? there is some visualization of synthetic anchor in appendix, but the quality of synthetic anchor still seems to be mysterious.\n\n(b)  the losses are not dissected well enough so that readers can make sure each loss is orthogonal, and plays its desirable role. the losses are designed based on intuitive heuristics. However, what role does each loss exactly play is not clear enough. For example, the anchor loss defined in eq 4, is that a bit overlapping with what eq 3 (i.e. anchor data synthesis)? basically, if data is generated from eq 3, will eq 4 automatically be relatively small? \n\nBasically, whether these losses are overlapping, and whether these losses have monotonic correlation, is difficult to determine.\n\n- following up on the subtlety of the model components, the ablation studies for DESA is not comprehensive enough to help. the hyperparameters (e.g. $\\lambda_{KD}$, $\\lambda_{REG}$, and IPC) are not searched comprehensively. For example, the inter accuracy vs. $\\lambda_{KD}$ is still monotonic with the three data points, and readers cannot grasp a full picture of the role of $\\lambda_{KD}$ or KD loss."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699303998009,
        "cdate": 1699303998009,
        "tmdate": 1699635981360,
        "mdate": 1699635981360,
        "license": "CC BY 4.0",
        "version": 2
    }
]