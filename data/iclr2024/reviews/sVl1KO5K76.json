[
    {
        "id": "HSk0ykmoIh",
        "forum": "sVl1KO5K76",
        "replyto": "sVl1KO5K76",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_R3HM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_R3HM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the use of momentum in Sharpness Aware Minimization (SAM) is investigated and then a speed-up method is proposed. The main idea is to use the capability of momentum on lookahead to reduce one backward calculation in each iteration."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The computational time of SAM is almost twice of that of SGD, which is due to twice backward  calculation used in SAM. Thus, the idea of using momentum to estimate the gradient and to reduce one backward calculation is interesting. \n\n+ The paper is generally well organized and some explanation, e.g., Fig. 1, is clear."
            },
            "weaknesses": {
                "value": "- the mechanism of momentum has not been clearly presented.If we look two successive iterations together, we could find that using a momentum in the next iteration almost equals to use a larger learning rate in the previous iteration. Then I can understand that why the author claim that  \"the loss is expected to increase\" when \"a well-chosen learning rate\" is used, which has similar effect on attack used in SAM. However, if this is the reason, then there will be some other unclear points, see question 1 and 2.\n\n- the numerical experiments need improvements: (1) the general performance is not very good, comparing with the performance reported in recent papers. (2) as a method that focusing on speeding up SAM by cutting one backward  calculation, it is should be compared with random perturbation based methods, which have almost the same speed-up performance and mechanism."
            },
            "questions": {
                "value": "- Using momentum in the next iteration has similar performance as taking a larger learning rate in the previous iteration. But it is based on the fact that the \"a well-chosen learning rate\" is used. In practice, the learning rate is not necessarily optimal, thus I do not think increasing it can always increase the loss value, as attack procedure in SAM. In Section 4.1, the authors \"empirically show that the momentum descent step actually results in an ascent on the per-batch loss\", however, by showing the cos similarity (which is not directly related to value, due to the heavy non-linearity of DNN's training landscape). Can the author report the value change directly, also for different learning rate?\n\n- In a  paper, https://arxiv.org/pdf/2309.15639v2.pdf, recently accepted in NeurIPS2023, an opposite conclusion is given: one need to add momentum to enhance the attack in SAM. Following their idea, even when we do not want to perform backward calculation, we need to add a momentum to increase not to decrease the value. So can the author discuss the link to that paper? (generally I do not want to ask the authors to compare with recent papers, but they two are strongly linked, so I think it is better to discuss here)\n\n- Could the author provide a formulation similarly to NAG on which lookahead direction is calculated. \n\n- Could the author include comparison with other speed-up method for SAM and also please notice that the current reported performance for other method also the baseline SAM is not very good."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631323459,
        "cdate": 1698631323459,
        "tmdate": 1699636500034,
        "mdate": 1699636500034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9go4ookWou",
        "forum": "sVl1KO5K76",
        "replyto": "sVl1KO5K76",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_BYgZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_BYgZ"
        ],
        "content": {
            "summary": {
                "value": "The authors are striving to reduce the computational overhead incurred in the SAM algorithms, where they propose Momentum-SAM (MSAM). The proposed method utilizes the momentum direction to approximate sharpness computations. In such a manner, nearly no extra computation cost would be needed compared to the vanilla SAM. The authors empirically show the effectiveness of their method via experiments on CIFAR100 and ImageNet. The results show that the proposed method can reduce the computational overhead, but may harm the model performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Strengths**\n1. The paper is clearly written and easy to follow. Also, the presentation of the paper is good.\n2. The paper present some interesting results, where it not only gives empirical results on the conventional benchmarks, also presents detailed study in regards to its properties."
            },
            "weaknesses": {
                "value": "**Weakness**\n\nThe paper is an empirical paper aiming to reducing the computational overhead incurred by the vanilla SAM algorithm. The proposed method is a quite heuristic method that substitutes the accent direction in SAM which is the positive gradient direction of the current weight model by the history gradient direction. In this way, the computational overhead could indeed be reduced. I would discuss the paper from several perspectives.\n\n1. I think the novelty of the paper is moderate. There are quite a number of papers targeting on the very topic. The core idea is somewhat quite similar to LookSAM. And the improvement of the proposed method is marginal. In fact, it is quite difficult to contribute further on this very topic, given there are a number of efficient methods proposed. And it may be more important to ensure the accuracy first. Given the results of the proposed method is good, I think the proposed method is acceptable. But the authors should present discussions in regards to the following paper, which contribute the topic from another perspective.\n\n    [1] Bahri, Dara, Hossein Mobahi, and Yi Tay. \"Sharpness-aware minimization improves language model generalization.\" arXiv preprint arXiv:2110.08529 (2021).\n\n    [2] Ni, Renkun, et al. \"K-SAM: Sharpness-Aware Minimization at the Speed of SGD.\" arXiv preprint arXiv:2210.12864 (2022).\n\n2. The significance of this paper is limited. To my understanding, the core of NAG is to accelerate convergence. It should be noted that here accelerating convergence does not mean to reduce the computation cost at each training batch. The authors have not provided any demonstration regarding the convergence of the proposed heuristic method. Given that the support of the proposed method is too weak, in my opinion, the minimum requirement of ICLR acceptance here is to discuss to what extent the proposed method can convergence from a theoretical perspective. See [3] and other related papers for reference.\n\n    [3] Andriushchenko, Maksym, and Nicolas Flammarion. \"Towards understanding sharpness-aware minimization.\" International Conference on Machine Learning. PMLR, 2022.\n\n3. The proposed method is not well supported. I could understand the authors' intention. However, the rationality of the proposed method is not clearly demonstrated. Regarding SAM, the ascent direction is solved explicitly and has a clear meaning. Minimization following the ascent direction can force the loss within the neighbourhood region to be low (i.e. flat). This is why SAM works. However, in my opinion, the proposed method cannot provide such a guarantee, given that the ascent direction could not characterize the maximum loss within the neighbourhood region. I notice that the authors give some simple empirical demonstration regarding the similarity between the SAM and MSAM. However, such a empirical demonstration could not give enough support and moreover, a similarity in direction could not give direct connection that the corresponding loss of the two models are close. It is not quite appropriate that one just give some other directions and claim that such directions could lead training to flat minimum."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have not found any discussions about the limitations and potential negative societal impact. But in my opinion, this may not be a problem, since the work only focuses on the learning method in machine learning. Still, it is highly encouraged to add corresponding discussions."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Reviewer_BYgZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673927197,
        "cdate": 1698673927197,
        "tmdate": 1699636499938,
        "mdate": 1699636499938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qg7qKDWlvt",
        "forum": "sVl1KO5K76",
        "replyto": "sVl1KO5K76",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_kkxn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_kkxn"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Momentum-SAM (MSAM) that removes the computational overhead. The method utilizes momentum inspired by NAG instead of gradient ascent step, and therefore has almost the same speed as conventional optimizers, such as SGD and Adam. The experimental results show that the method yields comparable performance within the same computational budget."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is easy to follow and provides clear motivation.\n- The proposed method does not introduce additional hyperparameter except the momentum coefficient.\n- The paper provides experimental results from various perspectives."
            },
            "weaknesses": {
                "value": "- From the results in Table 1, 2, 3, it can be seen that MSAM achieves relatively low performance that SAM except several cases. When the number of epochs of MSAM is doubled to make the same budget just like in A.9, can MSAM achieve higher performance than SAM? More generally, if MSAM yields higher final performance than that of SAM, the method could be more convincing. It would be nice to add more results to Figure A.8, for example, other methods like ESAM or other models/datasets.\n- The claims in chapter 4 are not convincing. Is there any theoretical reasons why positive momentum direction doesn't make the performance better? Also, it is unclear why $\\rho_0$ is close to $\\rho^{\\textrm{opt}}$.\n- Is the momentum applied to SAM as well in the paper? I understand that SAM has no momentum term.\n- \"efficient implementation\" of the method described in page 4 is quite straightforward, it seems to be unnecessary to describe it separately."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5090/Reviewer_kkxn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763062838,
        "cdate": 1698763062838,
        "tmdate": 1699636499853,
        "mdate": 1699636499853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VFz7PVssmn",
        "forum": "sVl1KO5K76",
        "replyto": "sVl1KO5K76",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_auP2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5090/Reviewer_auP2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed an efficient sharpness-aware-minimization optimizer that does not need to calculate the inner optimization during the training which accelerates the training progress while keeping the model performance close to the original SAM. In order to obtain a good estimator of the gradient ascent direction in SAM, this paper uses the negative momentum direction as the estimation and shows that the model reaches a low sharpness and high performance. Since for most optimizers such as SGD and ADAM, momentum is used during training, there will be no computation overhead for the proposed method. MSAM achieves good empirical results on both cifar10 and ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The problem is important and the phenomenon that the direction of momentum is negatively correlated to the gradient direction of the current batch is very interesting. The empirical results are good at both cifar10 and ImageNet."
            },
            "weaknesses": {
                "value": "To me, the intuition of why the negative momentum direction is a good estimate of the gradient ascent direction is still not clear.  To be honest, the phenomenon why this happens is more important and interesting to me since there are quite a lot of methods that can efficiently simulate SAM. The paper does have some analysis showing that MSAM actually minimizes the sharpness, but I would love to have more intuitions or hypotheses. For example, is this dataset related? Is this task related, like only for classification tasks or only for vision problems?"
            },
            "questions": {
                "value": "Can we have more experiments on finetuning results as the original SAM? \nCan we have some language model related experiments? \nAny hypotheses or intuitions of why the momentum direction is so different from the current batch? \nFor Section 4.3, can you make it clear why it is related to m-sharpness?\nCan we have some ImageNet NAG results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803855285,
        "cdate": 1698803855285,
        "tmdate": 1699636499758,
        "mdate": 1699636499758,
        "license": "CC BY 4.0",
        "version": 2
    }
]