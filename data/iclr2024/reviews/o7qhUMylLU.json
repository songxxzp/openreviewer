[
    {
        "id": "XlFgEPfgtq",
        "forum": "o7qhUMylLU",
        "replyto": "o7qhUMylLU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_FdZN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_FdZN"
        ],
        "content": {
            "summary": {
                "value": "This submission introduces a new algorithm dubbed MAMEX which enjoys regret bounds for the Nash Regret in General Sum Markov Games with low MADC. A metric introduced in this work that measures the complexity of a certain Markov Games.\n\nVia an online to offline conversion the regret bounds can be converted into sample complexity guarantees for learning an $\\epsilon$-approximate Nash Equilibrium. These guarantees hold in expectation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think that the paper's idea is well explained in the main text. \nUnderstanding which are the lighter assumptions to enable sample efficient learning in Markov Games is an important theoretical problem which matches the interests of the ICLR community."
            },
            "weaknesses": {
                "value": "I think that the paper should make it clearer that MAMEX is not computationally efficient.\nThis is because of (i) the equilibrium computation over the set of pure policies in step 4 of Algorithm 1 is often over combinatorial sets like for example the set of deterministic policies in the tabular setting and (ii) the sup over the function class in the policy evaluation routine."
            },
            "questions": {
                "value": "Q1) In the tabular case or in the Linear MDP case would there exist an efficient way to implement MAMEX ?\n\nQ2) Do you think that the MADC are necessary quantities that should appear in lower bounds too ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Reviewer_FdZN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698185918535,
        "cdate": 1698185918535,
        "tmdate": 1699636268095,
        "mdate": 1699636268095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PS2Lf4efXn",
        "forum": "o7qhUMylLU",
        "replyto": "o7qhUMylLU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_hiyJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_hiyJ"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a general recipe for solving multi-agent reinforcement learning through the notion of decoupling coefficient. The framework allows for various function approximations and encompasses both model-based and model-free approaches. Theoretical results show that if a game enjoys finite MADC then there exists a centralized algorithm for $\\sqrt{K}$ type regret."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The framework proposed provides a general recipe for solving multi-agent reinforcement learning under various function approximations, and the bounds obtained under this framework are sample efficient. I believe this is the first framework that includes both model-based and model-free methods. \n2. Compared to the previous works (MA - DMSO), the framework only requires solving a single objective optimization, which makes it much more feasible."
            },
            "weaknesses": {
                "value": "While I like the results presented in this paper, it is not very clear to me what the relationship between MADC and the existing measures (such as MADMSO, the single agent decoupling coefficient). I believe the paper would benefit from more discussion and comparison on that. Please see the question raised below for more details on this point."
            },
            "questions": {
                "value": "1. When $n = 1$, the problems are reduced to a single agent RL. In this case, is MADC reduced to DC, or is it more general than DC? (since it is not restricted to greedy policy)\n2. If MADC reduces to DC when $n=1$, does an RL instance with finite DC imply the MG version of the RL instance also has finite MADC (and thus can be solved efficiently?)\n3. How should one compare MADC and MADMSO? We know that tabular MG and linear mixture MG have finite MADMSO and MADC. Are there any classes of games that have finite MADC but not finite MADMSO or vice versa? \n4. It seems that the current framework does not distinguish NE/CCE/CE. It seems that as long as the NE/CCE/CE equilibrium oracle is efficient, it takes the same amount of samples to find NE/CCE/CE?  As NE is a much stricter notion of equilibrium than CCE, for example, on might expect more samples to be needed (or in other words, harder) to learn NE. So are the results tight for each of NE/CCE/CE or is it an upper bound for the worst-case regret needed for finding any equilibrium?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Reviewer_hiyJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698523343504,
        "cdate": 1698523343504,
        "tmdate": 1699636267985,
        "mdate": 1699636267985,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K0FyKQRlgr",
        "forum": "o7qhUMylLU",
        "replyto": "o7qhUMylLU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_JQUp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_JQUp"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the decoupling coefficient from the single agent setting to the multi-agent setting for general-sum Markov games, termed MADC. The authors then propose an extension of the Maximize-to-Explore family of algorithms from the single agent setting to general-sum Markov games to solve for a variety of equilibrium concepts. By doing so, they construct an approach that relies only on solving an optimization problem and computing the equilibrium of a normal-form game. The authors prove their approach achieves sublinear regret (i.e., exploitability of the equilibrium) where the MADC appears as a constant."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors propose a general approach for learning a variety of equilibrium concepts across Markov games. They also define the multi-agent decoupling coefficient (MADC), and show how it relates theoretically to convergence rates to equilibria. All of this is done assuming the difficult setting with function approximation (either the transition kernel or the action-value functions are directly modelled)."
            },
            "weaknesses": {
                "value": "I would like to see the authors compare / contrast their approach with PSRO [1, 2]. PSRO also consists of two components (single agent optimization to compute a best-response and computing the equilibrium of a normal-form game), applies to Markov games, leverages function approximation, and can learn CCE, CE, and NE.\n\n[1] Lanctot, Marc, et al. \"A unified game-theoretic approach to multiagent reinforcement learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] Marris, Luke, et al. \"Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers.\" International Conference on Machine Learning. PMLR, 2021."
            },
            "questions": {
                "value": "**Note I am increasing my score conditioned on some discussion of the limitations of scaling this approach to practical sized games (see my last follow-up comment) as well as including a discussion of a comparison with PSRO in related work**\n\n- Pure Policy: I was confused by the definition of pure policies. You state that the set of pure policies is a subset of each agent's local policies, but this is still to vague. I assume a pure policy is a deterministic mapping from states to actions, but can you clarify / confirm this?\n- Definition 2.4: It might help to state an intuitive definition in words as well, i.e., the $\\delta$-covering number is the minimum number of balls of radius $\\delta$ required to cover a set.\n- Equation 3.1: Can you state somewhere that $\\eta > 0$ is a hyperparameter that you are introducing?\n- Can you please bring Algorithm 1 into the main body?\n- You say \"We prove that MAMEX achieves a sublinear regret for learning NE/CCE/CE in classes with small MADCs\", but haven't you proven sublinear regret regardless of how large the MADC is (as long as it is finite, Assumption 2.7)? Do you have an interesting example in which MADC is infinite?\n- Practicality: If you are going to consider all pure joint policies in a corresponding NFG (called meta-game in PSRO), why bother with function approximation of a value function? Why is it important to have the value function when you're already going to compute an equilibrium of this enormous game (assuming computing an equilibrium is more expensive than reading all the payoff entries)? Why not just deploy the equilibrium policy and be done rather than continue to iterate to learn the best approximation to the value function? In other words, if I have an oracle to solve an NFG of the size you suggest, I can just ask it to return the equilibrium policy assuming the entries in the payoff tensor are the *actual* Monte-Carlo returns of the pure-policies in the Markov game. Is there some setting you have in mind where it is cheaper to approximate a value function and avoid calculating returns for every joint pure policy? Sorry if I'm missing something, but it feels like something big is being swept under the rug here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3198/Reviewer_JQUp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683786380,
        "cdate": 1698683786380,
        "tmdate": 1700830413546,
        "mdate": 1700830413546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUKfSgFQa9",
        "forum": "o7qhUMylLU",
        "replyto": "o7qhUMylLU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_cnZv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_cnZv"
        ],
        "content": {
            "summary": {
                "value": "This work studies online learning in general-sum Markov games with general function approximation, and focuses on the centralized learning paradigm. They extend the regret decoupling idea that has been used for the single-player case, and give a new notion, called multi-agent decoupling coefficient (MADC) to quantify the sample complexity. They propose a (conceptually) simple centralized algorithm called multi-agent maximize-to-explore (MAMEX) and show that it is able to achieve a regret upper bound related to MADC."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The work provides a very general and conceptually simple algorithm to deal with centralized multi-agent reinforcement learning. It nicely extends the prior work on the single-player case. \n- The writing is quite good."
            },
            "weaknesses": {
                "value": "- To me, there lacks motivation to study equilibrium learning in a centralized manner, particularly when it does not consider any global value optimization.  Equilibrium seems to be a concept under which selfish players cannot make unilateral move, and is usually used to characterize the steady state when every player plays independently and selfishly. However, if the players are in coordination, perhaps they can aim higher, such as higher social welfare. Can you give more motivations on centralized learning NE/CE/CCE without considering social welfare?  A related questions is: why is the Reg_{NE/CE/CCE} a meaningful measure for the quality of a centralized MARL algorithm? \n- There are some previous works also on quantifying sample complexity of multi-agent RL, including Wang et al. 2023, Foster et al., 2023, Chen et al. 2022b. In this work, the comparisons to these previous works are all about the \"simplicity\" of the objective. The comparisons on \"sample complexity\", however, are missing.  How to compare the new notion MADC with the complexity measure proposed in these works? Do they have any relations?"
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699192185036,
        "cdate": 1699192185036,
        "tmdate": 1699636267820,
        "mdate": 1699636267820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DAxYGX8Hlj",
        "forum": "o7qhUMylLU",
        "replyto": "o7qhUMylLU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_pi75"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3198/Reviewer_pi75"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenges and solutions in the context of Multi-Agent Reinforcement Learning (MARL) for general-sum Markov Games (MGs) with a focus on efficient learning of equilibrium concepts such as Nash equilibrium (NE), correlated equilibrium (CE), and coarse correlated equilibrium (CCE). The authors address the complexities of using function approximation in general-sum MGs, which is different from single-agent RL or two-agent zero-sum MGs.\n\nThe paper introduces a unified algorithmic framework called Multi-Agent Maximize-to-EXplore (MAMEX) for general-sum MGs with general function approximation. MAMEX extends the Maximize-to-Explore (MEX) framework to MGs by optimizing over the policy space, and updating the joint policy of all agents to achieve a desired equilibrium. It combines an equilibrium solver for normal-form games defined over the policy space and a regularized optimization problem for payoff functions over the hypothesis space.\n\nThe paper defines a complexity measure called the Multi-Agent Decoupling Coefficient (MADC) to capture the exploration-exploitation tradeoff in MARL. MAMEX is shown to be sample-efficient in finding NE/CCE/CE in MGs with small MADCs, covering a wide range of MG instances, including those with low Bellman eluder dimensions, bilinear classes, and low witness ranks.\n\nThe paper's contributions include the development of MAMEX, a unified algorithmic framework for model-free and model-based MARL, and the introduction of the MADC complexity measure to quantify the exploration difficulty in MGs with function approximation.\n\nThe paper aims to provide a foundation for addressing the challenges of efficient MARL in general-sum MGs with a unified approach, covering both model-free and model-based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally clear.\n\n- The authors provide a novel efficient framework to compute CCE, CE, and NE in general-sum MGs.\n\n- The authors propose a novel interesting complexity measure MADC, which captures the exploration-exploitation tradeoff for general-sum MGs.\n\n- The final regret of the algorithm depends on the introduced MADC measure."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is how to perform the policy evaluation step. It seems to me that it will be computationally expensive to construct an estimator for each pure strategy for each player. Can the authors explain this more in detail? How big is the policy space of the pure strategy? If we are replacing the set with a 1/K-cover how much are we losing?\n\n- There are some typos in the paper (e.g. page 8 solveing )\n   \n- It is not easy to understand the paper without looking at the appendix. For example, it would be better to include the definition of $\\{l^{i,s}\\}_{i \\in [n]}$ in the main paper. \n\n- How do the authors compare the regret results with previous SotA algorithms?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699313374138,
        "cdate": 1699313374138,
        "tmdate": 1699636267751,
        "mdate": 1699636267751,
        "license": "CC BY 4.0",
        "version": 2
    }
]