[
    {
        "id": "W1wJBU5CRH",
        "forum": "FbRWdSxTPY",
        "replyto": "FbRWdSxTPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_115n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_115n"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on studying the factors that may affect data annotation for audio signals. A small-scale dataset is collected (1,020 audio samples, each audio sample has a duration of 10 seconds). Then a quality estimation model is trained by transfer learning and the collected data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The finding between transcription time and SQS, WER is interesting."
            },
            "weaknesses": {
                "value": "1) The presentation is not very clear, I\u2019m not sure what is the main difference between the proposed Speech Quality Score (SQS) and the common MOS.\n2) The novelty of this paper is very limited. In fact, there is no novelty in terms of the machine learning perspective. This paper is more suitable to be submitted to speech-related conferences (e.g., Interspeech, Icassp, etc.). Specifically, this work simply employs a pre-trained speech quality estimator (NISQA) and finetunes on its own dataset.\n3) The comparison to other models (e.g., NISQA, DNSMOS) is also unfair, because of different training data and the label scale. In DNSMOS, the MOS scale is from 1 to 5, however, in the collected dataset, the label scale is from 1 to 4. Although it may be okay for Pearson correlation, metrics such as Mean Square Error (MSE) will be significantly affected by this scale mismatch."
            },
            "questions": {
                "value": "Although each audio sample is 10 seconds, do they contain the same number of words? I believe it will also affect the transcription time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697687869748,
        "cdate": 1697687869748,
        "tmdate": 1699636553360,
        "mdate": 1699636553360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QrqrcAyHWe",
        "forum": "FbRWdSxTPY",
        "replyto": "FbRWdSxTPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_SKsa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_SKsa"
        ],
        "content": {
            "summary": {
                "value": "This paper describes experiments to predict a new rating of the difficulty with which a speech utterance can be transcribed directly from the audio. It uses an in-house 1000-utterance dataset with this new annotation. The new annotation is correlated with NISQA predictions, DNSMOS predictions, accuracy of a single human transcriber compared to an exhaustive human transcription panel, and speed of annotating an utterance. The NISQA model can be fine tuned to predict this annotation to achieve r = 86% correlation with the ground truth."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is an interesting application: triaging utterances for transcription by annotators of different skill level.\n\nThis seems like a well defined task that can be solved accurately with fine tuning of an existing model.\n\nThe paper is relatively easy to follow."
            },
            "weaknesses": {
                "value": "The clarity of the paper is hindered by the use of common technical terms in non-standard ways. For example, there is a frequent conflation between speech quality and speech intelligibility. The name of the proposed metric is the Speech Quality Score, although it predicts intelligibility (how easy it will be to transcribe for a human listener). While these are often related, they are not always, despite the first sentence's unsupported claim (\"Speech intelligibility is directly associated with audio quality\"). Similarly, MOS (Mean Opinion Score) is described as a metric even though it is merely a scale upon which many different metrics are measured (speech quality, noise suppression, overall quality, etc). Word error rate is mentioned as a measure of speech quality, but it is explicitly a measure of intelligibility. Furthermore its use in this paper is to compare one human transcription against a consensus human transcription, but this is not explained until page 5 after being mentioned several times.\n\nThe relevance of the paper to ICLR is not clear. It is a paper about speech intelligibility prediction and while of interest to the speech community, I don't think is general enough in terms of machine learning approaches or applicability to warrant publication at ICLR, which focuses on machine learning.\n\nThe reproducibility of the results is quite low without the release of either the dataset or the model. This is a new task with guidelines that could be interpreted differently from different readings of the paper, so without some aspect of the work being released, it is not clear exactly what a reader of the paper is meant to take away from it. I don't think the fact that it is possible to perform this general task is sufficiently interesting to warrant publication on its own.\n\nIt is not clear why this subjective measure of \"Speech Quality Score\" is necessary as opposed to a more objective measure like the actual time that it took to annotate a given utterance or the inter-rater (dis)agreement. No justification is provided, nor is any quantitative evaluation undertaken. Such an objective score would need to account for differences in utterance duration and different overall speeds between raters, so could normalize within each rater and by utterance duration. It seems that even without these normalizations, SQS is still correlated with annotation time (r=0.38). Presumably with them it would be even more correlated."
            },
            "questions": {
                "value": "Results are only provided in the paper for fine tuning the pretrained NISQA model. What are the results of training the model from scratch to predict SQS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807115751,
        "cdate": 1698807115751,
        "tmdate": 1699636553226,
        "mdate": 1699636553226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M0gWDGxIrK",
        "forum": "FbRWdSxTPY",
        "replyto": "FbRWdSxTPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_PsYv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_PsYv"
        ],
        "content": {
            "summary": {
                "value": "This paper creates a new subjective test (Speech Quality Score, SQS) for annotating speech clips. SQS is shown to correlate moderately with annotation time (r=0.38) and it is suggested to use the SQS value to determine whether to apply a speech enhancement method before transcription or not. NISQA is fine-tuned with a SQS dataset and shown to have good performance (r=0.86)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This is a novel idea that could help improve speech annotation, which is a challenging problem. \nThe results of the NISQA-based model are good."
            },
            "weaknesses": {
                "value": "The basic premise of the paper is SQS and the NISQA-based model can improve speech annotation (see Figure 1), but this was never done. That is, SQS and the NISQA-based model have not been shown to have any end-to-end utility.\n\nIn addition, it isn't clear SQS is even needed. Why not just do speech enhancement for all speech clips, or play both speech enhanced and originals to the annotators?\n\nMinor issues:\nSome references have errors, e.g., \n\tITUT Rec. Itu-t rec. p. 800.1 => ITU-T Rec. P.800.1\n\tpesq => PESQ\n\tDnsmos => DNSMOS\n\nIn the introduction, these are not SOTA SE methods for denoising and dereverberation: MetricGAN (Fu et al., 2019), Denoiser (Defossezet al., 2020), SepFormer (Subakan et al., 2021). I suggest citing winners of the ICASSP DNS 2022 challenge as better examples."
            },
            "questions": {
                "value": "What is a Sigma's proprietary tool (Section 2.1)? Add a reference\nWhy does Table 1 not have 5: Excellent? \nWhy are only N=2 ratings done for the dataset in 2.1? That makes your training data fairly noisy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901634954,
        "cdate": 1698901634954,
        "tmdate": 1699636553123,
        "mdate": 1699636553123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HZG6sk0KVd",
        "forum": "FbRWdSxTPY",
        "replyto": "FbRWdSxTPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_YGuK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5441/Reviewer_YGuK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel subjective speech quality measure, known as Speech Quality Score (SQS), within the audio data annotation framework. It argues that existing objective and subjective measures do not effectively consider factors that may affect the annotation process, leading to poor correlation with the audio quality perceived by the annotator. The proposed SQS measure takes into account the most relevant characteristics impacting transcription performance and, thus, annotation quality. Additionally, the authors propose a Deep Neural Network (DNN)-based model to predict the SQS measure. The experiments conducted on a dataset of 1,020 audio samples with SQS annotations show promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses an important (and open) issue in the field of audio data annotation, highlighting the need for a more effective measure of audio quality.\n- Introducing the SQS measure is innovative, considering factors directly impacting transcription performance and annotation quality.\n- The use of a DNN-based model for predicting SQS metrics demonstrates a strong correlation between ground-truth and predicted SQS values, indicating the reliability of the proposed model."
            },
            "weaknesses": {
                "value": "- The paper could have expanded on the specific characteristics encompassed by the SQS measure to provide a more comprehensive understanding of its composition.\n- The research relies heavily on the RTVE2020 Database for experimentation. The results might be limited and may not generalize well to other databases or real-world scenarios. For e.g., the paper does not speak much about the data collection framework setting (whether clean references provided, quality of those clean references, raters qualifications, what type of questions asked...)\n- An idea of how noisy the recordings were (e.g., using a spectrogram) would have conveyed the point on how inherently noisy the recordings were (based on Fig 4(a), looks like most NISQA scores are below 3ish, so that says about the relationship b/w intelligibility and quality esp for low quality scenarios."
            },
            "questions": {
                "value": "- How can the SQS measure be validated against other subjective measures like MOS (or is MOS even the right framework for this)?\n- What are the specific characteristics considered by SQS that make it more effective than existing measures? (Some ablations on combining WER and NISQA to build this hybrid metric compared to SQS might have been useful)\n- Can the proposed model generalize well to other datasets or real-world scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699105729050,
        "cdate": 1699105729050,
        "tmdate": 1699636553023,
        "mdate": 1699636553023,
        "license": "CC BY 4.0",
        "version": 2
    }
]