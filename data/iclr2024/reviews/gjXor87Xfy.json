[
    {
        "id": "VKcl4CjTIQ",
        "forum": "gjXor87Xfy",
        "replyto": "gjXor87Xfy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2962/Reviewer_pqbr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2962/Reviewer_pqbr"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to offer a scalable training method for Memory-Based Dynamic Graph Neural Networks (MDGNNs) by mitigating the temporal discontinuity issue, thus training MDGNNs with large temporal batch sizes. It consists of two main contributions: 1) conducting a theoretical study on the impact of temporal batch size on the convergence of MDGNN training, and 2) proposing PRES based on the theoretical study, an iterative prediction-correction scheme combined with a memory coherence learning objective to mitigate the effect of temporal discontinuity. The evaluation shows that the proposed approach enables up to 4X larger temporal batch sizes and achieves up to 3.4X speedup during MDGNN training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ It targets an emerging and important GNN, MDGNNs, and the proposed designs generally make sense. \n+ The problem definition is easy-to-follow.\n+ The introduced concept of memory coherence is interesting.\n+ The code is publicly available."
            },
            "weaknesses": {
                "value": "- The trained graphs seem small. It is unclear how PRES performs on large-scale graphs.\n- No absolute execution time is reported.\n- It is tested on four GPUs. Its scalability to multi-nodes (with more GPUs) is somewhat unclear.\n- In many cases, PRES still sacrifices some precision for performance gains."
            },
            "questions": {
                "value": "Overall, this is a solid study with clear innovations. The theoretical study on the impact of temporal batch size on the convergence of MDGNN training is extensive and helpful. My major concerns focus on the evaluation aspects. It would be extremely helpful if the authors could offer more information about these questions:\n\n1. PRES is mainly evaluated on four graph datasets (Reddit, Wiki, Mooc, and LastFM). It seems these graphs are not very large with around 1K to 10K vertices and 400K to 1.3M edges. It would be helpful to justify that these graphs are large enough or PRES\u2019s performance is not affected by the graph size.\n\n2. It would be helpful to report the absolute execution time as well rather than relative speedup only.\n\n3. It would be helpful to discuss if this method can be extended to multi-nodes with more GPUs.\n\n4. It seems Table 1 shows that PRES still sacrifices some precision for performance gains in many cases. Please correct me if I have any misunderstanding here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722421508,
        "cdate": 1698722421508,
        "tmdate": 1699636239998,
        "mdate": 1699636239998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e3DXJynxkS",
        "forum": "gjXor87Xfy",
        "replyto": "gjXor87Xfy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2962/Reviewer_dnLR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2962/Reviewer_dnLR"
        ],
        "content": {
            "summary": {
                "value": "This work conducts a theoretical study on the impact of the temporal batch size in MDGNN training. This shows that there can be a significant gradient variance using a small temporal batch, which in turn sheds light on an unexpected benefit of large batch sizes. Next, the authors define memory coherence, which represents the similarity of gradient directions within a temporal batch. Memory coherence is then used to model the upper boundary of gradient. \nWith these theoretical insights, the authors present PRES with two main components: 1) iterative prediction-correction scheme 2) memory coherence smoothing. The former uses a GMM (updated with MLE) to predict newest memory states and fuses it with the calculated memory state to obtain the final \u2018corrected\u2019 state. The latter uses a new learning objective to promote larger memory coherence. \nUsing PRES, the authors were able to increase the temporal batch size without compromising overall accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe authors provide theoretical results on the influence of temporal batch size on MDGNN training.\n-\tWith memory coherence, the authors effectively define new methods to compensate for the accuracy drop of naively increasing the temporal batch size."
            },
            "weaknesses": {
                "value": "- The tradeoff of improved speed at the cost of lower accuracy does not seem to be appealing.\n- Comparison with prior work on increasing temporal batch size is insufficient. \n-\tIn a similar manner, there are only a small number of baselines in the experimental results. \n-\tThe specific results of a dataset (LASTFM) is excluded."
            },
            "questions": {
                "value": "The paper is overall well written. The introduction on MDGNN was easy to follow. Insights from theoretical analyses were well presented. It was also evident how these insights became the main building blocks of PRES. However, my main concern comes from the experiment section.\n\n-\t**Is it really useful to gain speed at the cost of accuracy?**\n\nSo far this is my main concern. At first I thought the authors were trying to achieve SOTA accuracy.\nHowever, what the authors are doing is gaining speedup of around 2x to 3x, at the cost of decreased accuracy (~1.0%).\nI am not so sure about this, considering the effort the community is putting to gain higher accuracy.\nEspecially on the tested datasets, the number of vertices is only around few thousands, which wouldn't take terribly long to train.\nI believe this partially comes from not reporting the training time (only the speedup is reported) and there are not enough baselines to compare. But the bottomline is that a strong justification is needed for this.\n\n-\t**What is the consensus on the \u2018optimal\u2019 temporal batch size?**\n\n\tIn Figure 3, the authors show the performance of baselines by increasing the batch size up to 100. In the figure the \u2018small batch size\u2019 seems to be ~50. My question is do the majority of MDGNNs use a batch size smaller than 50, or are they already using approximately 500 (which seems to be the optimal size in Figure 4)? If the latter is the case, then personally the insight from theorem 1 (variance of the gradient for the entire epoch can be detrimental when the temporal batch size is small) loses some of its shine. Thus, the authors should try to first do a comprehensive overview on the currently used batch sizes.\n\n-\t**How does PRES differ from other baselines?**\n\tTwo related works came to my mind which are missing in the current paper. \u201cEfficient Dynamic Graph Representation Learning at Scale\u201d (arXiv preprint, 2021, https://arxiv.org/abs/2112.07768) and \u201cDistTGL: Distributed Memory-Based Temporal Graph Neural Network Training\u201d (arXiv preprint, 2023, https://arxiv.org/abs/2307.07649). Both try to increase the temporal batch size without harming the accuracy. The former also uses prediction to utilize data-parallelism, while the latter tries to push the temporal batch size to the extreme for distributed GPU clusters. In my opinion, both (and any other baseline that shares the same goal with this work) should be compared methodologically and speedup-wise (in the current setting). Also, it would be interesting to see if these can also benefit from PRES. \n\n-\t**Why is performance with/without PRES not shown with the LASTFM dataset?**\nLASTFM stands out in that 1) the AP is the lowest 2) the speedup of PRES is the lowest. However, I was unable to find a figure like figure 4 for this dataset. Is there a reason for only leaving this dataset out?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2962/Reviewer_dnLR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835737211,
        "cdate": 1698835737211,
        "tmdate": 1700813778940,
        "mdate": 1700813778940,
        "license": "CC BY 4.0",
        "version": 2
    }
]