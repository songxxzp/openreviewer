[
    {
        "id": "pxDFPJ4eBm",
        "forum": "Wg3kLZ6pII",
        "replyto": "Wg3kLZ6pII",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission905/Reviewer_oGQD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission905/Reviewer_oGQD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes observation-guided diffusion probabilistic model (OGDM), which essentially incorporates an additional GAN component that tries to further match the simulated (via ODE) distribution and true (noise injection) distribution. The proposed method accelerates inference speed while maintaining high sample quality by altering only the training procedures of diffusion models. Empirically, it demonstrates benefits across datasets and diffusion baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors' proposed method OGDM is novel to my knowledge. There were existing works that also explored utilizing a discriminator to help diffusion model training/sampling, but this paper's proposal is different from those.\n* Empirically, the proposed method demonstrates nontrivial benefits when incorporated into various baselines, especially with few NFEs."
            },
            "weaknesses": {
                "value": "1. There is no empirical comparison between the proposed method and the existing diffusion works that utilize a discriminator. Currently, the paper only compares with the \"vanilla\" diffusion training baselines. As we have already seen from prior works, incorporating a GAN component into diffusion models could improve the empirical results by a lot, I would highly suggest the authors do so, e.g. [1], [2], to give a clear picture of where this method stands against its peers.\n2. The presentation of the paper feels unnecessarily complicated at times. In my opinion, for example, Fig.2, Sec.3.1, and 3.2 are quite distant from the presented method in Sec.3.4. $y$ which at first is defined to be some observation of the underlying diffusion process, but only to materialize as true or fake label. I suggest the authors make the story more straightforward, like the related work [1] and [2].\n3. I find Sec.3.5 lacks motivation and clarity, and does not contribute to the overall paper. Essentially the whole subsection is built upon an unreal assumption: for any $\\beta$, $p_{u|v}$ can be approximated by some weighted geometric mean of two distributions. The authors claim to provide validity of the approximation, but only to analyze a toy example where the base distribution is Gaussian.\n\n\n[1] Xiao et al. \"Tackling the generative learning trilemma with denoising diffusion gans.\" ICLR 2022.\n\n[2] Kim et al. \"Refining generative process with discriminator guidance in score-based diffusion models.\" ICML 2023."
            },
            "questions": {
                "value": "1. In the introduction, the paper talks about diffusion models with DDPM's formulation with \"thousands of NFEs\" and \"reverse Gaussian assumption\", but only presents experiments with fast deterministic samplers, where these issues do not arise. Related to this: in Sec.3.5, are you suggesting you are working with a stochastic sampling process that is fast? (You mentioned fast sampling, and also each reverse step is defined as a Gaussian so I assume so.) This is not standard practice nor what is considered in the experiments.\n2. In Eq.18, how do you get $x_{t-s}$? Just add a smaller noise to data, which corresponds to one with time step $t-s$?\n3. For Eq.25 and the associated explanation, I do not think your method considers the second KL term, but rather it considers the KL between the marginals of $p$ and $q$ at time $\\tau_{i-1}$. In other words, the discriminator is not aware of $x_{\\tau_i}$ in your formulation.\n4. Could you explain why the discriminator needs both $t$ and $s$ as input, rather than just $t-s$? If $t_1-s_1 = t_2-s_2$, then $q(x_{t_1-s_1})$ and $q(x_{t_2-s_2})$ should have the same marginal, no?\n5. I suggest the authors provide training overhead (if trained from scratch) of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Reviewer_oGQD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744909689,
        "cdate": 1698744909689,
        "tmdate": 1699636017340,
        "mdate": 1699636017340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Be6GwxVJhd",
        "forum": "Wg3kLZ6pII",
        "replyto": "Wg3kLZ6pII",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission905/Reviewer_RSLy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission905/Reviewer_RSLy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new training objective for diffusion models. Compared to the original DDPM framework, an additional term is added into the loss function, with the goal of fooling a discriminator to predict the image after one-step denoising as real image. The proposed objective is grounded in a framework where a sequence of observations are added into the original forward and backward process of DDPM. And each observed variable $y_t$ is defined as whether the associated image $x_t$ is from the real image distribution or not. Experiments show that models trained with the proposed objective (both trained from scratch and fine-tuned from pre-trained diffusion models) outperform the baselines in terms of FID and recall, especially when the number of denoising steps is small."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a new training objective for diffusion models that is theoretically grounded.\n2. The paper did comprehensive experiments on three datasets of various resolutions, using various sampling algorithms."
            },
            "weaknesses": {
                "value": "1. The proposed training pipeline is coupled with a specific sampling algorithm. At inference time, when the sampling algorithm is changed to another one that is different from the one used during training, the proposed method has limited improvements compared to baselines, as demonstrated in Table 3. This limits the applicability of the method, since if new sampling algorithm is proposed, the diffusion model also needs to be re-trained.\n2. Comparison with important baselines are missing. Specifically, the discriminator guidance [1] should also be compared, since it also utilizes a discriminator, and it can be applied at inference time without re-training diffusion models. In particular, the reported performance in Table 2 seems to be worse than the number reported in [1].\n3. The cost for training diffusion models should also be reported.\n\n[1] Kim et al., 2023. Refining generative process with discriminator guidance in score-based diffusion models."
            },
            "questions": {
                "value": "1. Why is the projection function $f(\\cdot)$ defined as the one-step denoising of the diffusion model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission905/Reviewer_RSLy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818271977,
        "cdate": 1698818271977,
        "tmdate": 1699636017248,
        "mdate": 1699636017248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wbHayPddsh",
        "forum": "Wg3kLZ6pII",
        "replyto": "Wg3kLZ6pII",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission905/Reviewer_D5aC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission905/Reviewer_D5aC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new training method for the diffusion models, where a discriminative loss is designed to reduce the sampling steps for inference. Experiments on several datasets show the proposed method performs better than baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and clear.\n2. The paper presents detailed theoretic analysis for the proposed method.\n3. Experiments on several datasets show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The paper introduces additional cost for training, but there is no additional training cost analysis.\n2. The advantage of diffusion models compared to GAN is the training stability, it introduce GAN training again, which may harm the training stability.\n3. The experiments are conducted on unconditional generation, leaving its performance on the mainstream text-to-image generation models unclear."
            },
            "questions": {
                "value": "Please provide the training memory and time cost, and better to provide its performance for Pretrained conditional text-to-image models such as Stable Diffusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698978763815,
        "cdate": 1698978763815,
        "tmdate": 1699636017157,
        "mdate": 1699636017157,
        "license": "CC BY 4.0",
        "version": 2
    }
]