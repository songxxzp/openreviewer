[
    {
        "id": "eXfGexV40S",
        "forum": "ezBH9WE9s2",
        "replyto": "ezBH9WE9s2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_LoJV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_LoJV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed AnyText, a diffusion-based multilingual visual text generation and editing model. It combines auxiliary latent model which is a control net for text condition, and a text embedding module which injects text visual information in the prompt latent space. Text-control diffusion loss and text perceptual loss are using in training. A large-scale multilingual text images dataset, AnyWord-3M, is introduced."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- extended control net for text input condition\n- new visual text token embedded in the prompt \n- introduced OCR related perceptual loss\n- new dataset\n- new state-of-the-art under proposed new evaluation benchmark"
            },
            "weaknesses": {
                "value": "- using models trained own dataset to compare with previous baselines is not so fair\n- the requirement of user given text mask is not always easy in practice."
            },
            "questions": {
                "value": "- It is not clear whether the improved results come from better training data or the proposed model. It would be best to compare the baseline models trained on the same dataset, or train the proposed model on the previous LAION glyph subset.\n- in the experiments, the ground truth text mask is used as conditional input. It would be interesting to see what if random text position and mask is used, can it still generate reasonable image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615646938,
        "cdate": 1698615646938,
        "tmdate": 1699636275521,
        "mdate": 1699636275521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1TX4efgb0f",
        "forum": "ezBH9WE9s2",
        "replyto": "ezBH9WE9s2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_5GTp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_5GTp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces modules to enhance the text-drawing capabilities of text-to-image diffusion models. The auxiliary latent module embeds glyph and position information obtained from an off-the-shelf OCR module and fuses these latents as diffusion-step invariant conditions through ControlNet. Additionally, the text embedding module encodes a tokenized prompt, replacing the tokens of rendered glyphs with special tokens. Since these special tokens, the components of the auxiliary latent module, and ControlNet are the only trainable parts in the entire model, this method can be readily applied to existing diffusion models without retraining the diffusion UNet. The modules are trained on the AnyWord-3M dataset, also proposed in this paper. The performance of the proposed method surpasses that of previous text-generation-focused text-to-image diffusion models and also offers multilingual text generation capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Generating text glyphs properly in images produced by text-to-image diffusion models has been a longstanding issue. Research has shown that this capability can be improved by increasing data and model size, but this is somewhat obvious or expected. Following ControlNet, which proposes controllability for Diffusion UNet, the text glyph generation problem can be solved; however, as shown in this paper, it would result in a monotonous style. One of the paper's strengths is that the generated glyphs harmonize with the content of the generated images and are not monotonous. Additionally, the paper's ability to handle multilingual text glyph generation with relatively less data is another notable strength."
            },
            "weaknesses": {
                "value": "As revealed in the ablation study, the most significant performance improvement of this method occurs upon the introduction of text embedding. This is attributed to the performance of PP-OCRv3. If one were not to use the OCR module's embedding and instead employ a general image encoder like the CLIP visual encoder, it is questionable whether the same level of performance improvement would have been achieved. Additionally, many modules are added to the vanilla text-to-image diffusion model, but the paper fails to mention the computational overhead that arises as a result. Although the paper highlights multilingual capabilities and provides qualitative results for Korean and Japanese in the figures, it is disappointing that these two languages are excluded from the quantitative results, falling under the \"others\" category. Furthermore, it is regrettable that the results of the ablation study are listed only in terms of Chinese sentence accuracy and NED, without any FID measurements. The lack of qualitative results corresponding to each ablation experiment is also a drawback."
            },
            "questions": {
                "value": "I'd like to pose questions that can address the weaknesses discussed.\n\n1. Could you elaborate on how the PP-OCRv3's performance specifically influences the results?\n2. Have you considered measuring the computational overhead when additional modules are integrated into the vanilla text-to-image diffusion model?\n3. Why were Korean and Japanese languages included in the qualitative results but not in the quantitative ones?\n4. Is there a reason why FID measurements were not included in the ablation study?\n5. Why were qualitative results not provided for each individual ablation experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778987919,
        "cdate": 1698778987919,
        "tmdate": 1699636275436,
        "mdate": 1699636275436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xh2KJUPdjy",
        "forum": "ezBH9WE9s2",
        "replyto": "ezBH9WE9s2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_xCSM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_xCSM"
        ],
        "content": {
            "summary": {
                "value": "The manuscript unfolds AnyText, a profound diffusion-based multilingual visual text generation and editing model. It meticulously tackles the intricacies involved in precise text portrayal within generated images, deploying auxiliary latent modules and text embedding modules as strategic tools. To augment the training phase, the introduction of text-control diffusion loss and text perceptual loss is articulated, which serves to bolster text generation quality. A formidable performer, AnyText triumphs over existing paradigms, championing improved accuracy and quality in text generation. Furthermore, the introduction of a novel dataset, AnyWord-3M, enriches the existing reservoir of multilingual image-text pairs, reflecting a thoughtful contribution to the scholarly community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1) A notable innovation lies in the paper's strategic approach to circumvent challenges, ensuring precise text portrayal within generated images.\n\n(2) The infusion of auxiliary latent modules coupled with text embedding modules acts as a catalyst, promoting enhanced accuracy and coherence in the text generation process.\n\n(3) Strategic incorporation of text-control diffusion loss and text perceptual loss during training heralds improvement in the overall text generation quality.\n\n(4) A commendable addition is the introduction of AnyWord-3M, a robust dataset enriched with 3 million image-text pairs, elaborately annotated with OCR in multiple languages, signifying a valuable asset to the research fraternity."
            },
            "weaknesses": {
                "value": "(1) The architecture seems somewhat reliant on pre-established technologies such as Latent/Stable Diffusion and ControlNet, which slightly shadows its novelty.\n\n(2) Encumbered by a complex array of components and a multi-stage training regimen, the model\u2019s re-implementation emerges as a challenging task, compounded further by numerous critical hyperparameters requiring manual assignment.\n\n(3) Certain aspects, such as token replacement, require a more elaborate discourse for clearer comprehension, primarily concerning the identification of corresponding tokens and their subsequent utility in text-image generation.\n\n(4) There exists a potential ambiguity concerning the intermediate generative results (x'_0), where the possible presence of noise or blur could compromise the precision of perceptual loss computation.\n\n(5) A clearer depiction of computational resource demands (GPU Hours), beyond the ambiguity of epochs, would enhance the paper\u2019s practicability and replicability.\n\n(6) A more explicit elucidation on the operational synergy between Z_a from the Auxiliary Latent Module and Z_0 from VAE, as depicted in Fig. 2, alongside their application within Text-ControlNet, would augment the manuscript's clarity."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780632842,
        "cdate": 1698780632842,
        "tmdate": 1699636275356,
        "mdate": 1699636275356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YFDe9WHpKA",
        "forum": "ezBH9WE9s2",
        "replyto": "ezBH9WE9s2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_TuoG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3269/Reviewer_TuoG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an adapter-based module that can be plugged into existing diffusion models to perform multilingual visual text generation and editing. It contains a control net to control the text location and text content and a special text embedding module to improve the multilingual text generation ability. This paper also presents a large-scale multilingual text image dataset, AnyWord-3M, with image-text pairs and OCR annotations in multiple languages."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed adapter-based module is a plug-and-play module that can guide visual text generation of many existing pre-trained diffusion models and can apply to multiple languages, which was not achieved in previous works.\n2. The proposed text adapter and text encoder are proven to be effective in improving the OCR performance.\n3. The proposed AnyWord-3M dataset is the first large-scale text image dataset with multilingual OCR annotations and is useful for future study."
            },
            "weaknesses": {
                "value": "1. The method and dataset collection miss a lot of details. For example, is the linear projection layer trained or fixed for ocr encoder in the text embedding module? \n2. A lot of information is not presented in the examples shown in the paper, for example, the layouts for images in Figure 1, and the captions for images in Figure 5."
            },
            "questions": {
                "value": "I have some questions about the model architecture, dataset construction, and experiment design.\n\nFor the model architecture:\n1. Which feature is extracted from PP-OCR to serve as text embedding?\n2. Is the linear projection layer for the OCR encoder trained?\n3. What is the configuration for the fuse layer? A single convolution layer or a stacked convotion? \n4. What is the input to text controlnet? Based on Figure 2, it seems to be the concatenation of $z_a$ and $z_t$.\n5. Why is the image resolution for glyph image 1024x1024 instead of 512x512? This does not align with the final image resolution.\n\nFor the dataset construction:\n1. How is the tight position mask $l_p$ annotated? As far as I know, the PP-OCR model does not support arbitrary shape text detection.\n2. The glyph image $l_g$ contains rotated text; how is the bounding box annotated from the position mask?\n3. The captions are generated using BLIP-2 model instead of the original captions. Could authors provide some statistics like the length of the captions and show some example captions for images in AnyWord-3M? How does this difference affect the model performance?\n\nFor the experiment:\n1. Could authors provide more information about the input for the visual examples? For example, the layouts for Figures 1, 6, 8, 12, the captions for Figures 5, 10, 11.\n2. The Sen. ACC and NED metric is measured using the same OCR model as the encoder in model training, which might be unsuitable. Could authors evaluate the OCR performance using another OCR model?\n3. Table 3 shows the improvement brought by visual text embedding in Chinese text generation. I wonder if this also improves the English word generation.\n4. In the experiment, the text diffuser model is not trained on the same dataset as GlyphControl and AnyText Model. Is it possible that authors fine-tune them on the same data and compare final performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3269/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3269/Reviewer_TuoG",
                    "ICLR.cc/2024/Conference/Submission3269/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3269/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808001655,
        "cdate": 1698808001655,
        "tmdate": 1700676321502,
        "mdate": 1700676321502,
        "license": "CC BY 4.0",
        "version": 2
    }
]