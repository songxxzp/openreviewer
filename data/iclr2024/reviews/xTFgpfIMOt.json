[
    {
        "id": "ScsaZTBPK6",
        "forum": "xTFgpfIMOt",
        "replyto": "xTFgpfIMOt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach for selecting a skill from a fixed repertoire of pre-trained behaviors. Though the general approach is mainly studied in the light of test-time distribution shift, the selection scheme is general and could potentially be applied to other problems, e.g., task selection in long-horizon tasks. At a high level, the methodology adds a regularization function to the advantages of the task repertoire to increase the tasks's value function according to the state visitation frequency (the more a policy observes that state, the higher the value is going to be). At test time, the action with the highest value is selected. Experiments in simulation and real world show the effectiveness of the approach over a set of baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem this paper is trying to solve is important: skill selection in previously unseen scenarios is challenging. Using values for selection is not novel (See, for example, Chen et al., Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation), but the way the overall selection method is novel to me.\n2. While the case study is focused on adaptation to distribution shifts, the approach could be generalized to other skill selection problems, e.g., long-horizon task execution.\n3. The experiments show a good margin over other baselines, e.g. a naive high-level controller for skill selection trained with supervised learning."
            },
            "weaknesses": {
                "value": "The major methodological weakness in the problem formulation is the bias induced by the proposed cross-entropy term. As proven by Theoreom 4.2, the increase in the value function is proportional to the state visitation frequency. This is a problem because the high-level policy will select the low-level policy, which mostly visited a state, not necessarily the best available policy. For example, a policy that resets almost immediately will visit a low neighborhood of the initial state and, therefore, will be pushed up and, possibly, preferred to a policy with a higher value but visits the same region of the state space much more infrequently. I don't see how this can be prevented without an extra normalization term on the state visitation frequency.\nAnother problem, though less structural than the previous one, is that there might be multiple policies with similar values and the high-level policy switching between them at random. This could lead to suboptimal behavior and possibly lead to failures. I don't see that there is any measure preventing this in the current approach.\n\nI am also not convinced by the experimental setup. I don't think I understood why policies trained in the real world are used for evaluation. This seems to be very interesting but orthogonal to the paper's contribution. This is a problem, in my opinion, because the policies can barely walk and keep balance, even without any pull forces (this can be seen at the beginning of the first video). This confounds the current experiments since the evaluation metrics are speed and stability. The gait and stability just can't be compared to the policies obtained via sim2real. It would be important to look into this and check whether this gap is still there after upgrading to a better policy. In addition, it is challenging to see whether a similar effect is happening in simulation without any visualization."
            },
            "questions": {
                "value": "Is there any difference in the policies on their original MDP after fine-tuning? Or, in other words, does the cross entropy have any effect on the policy performance?\nIs there a way to quantify whether none of the available skills are good enough? (e.g., thresholding values)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697671796646,
        "cdate": 1697671796646,
        "tmdate": 1699636746882,
        "mdate": 1699636746882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uT5RfknCBL",
        "forum": "xTFgpfIMOt",
        "replyto": "xTFgpfIMOt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for policy adaptation to different tasks. Instead of relying on a high level-controller to select the task appropriate behavior, they sample from the softmax distribution derived from the behavior policies\u2019 value functions. Specifically, they add a regularization cross-entropy term (equation 1) to artificially raise the value function in the encountered states of a behavior policy while lowering the value of other behavior policies (section 4.1). In this way, they assert that the propensity for the value function to over-estimate out-of-distribution states is reduced. This facilitates selection of the appropriate or closest in-distribution behavior policy to the encountered state at run-time.\n\nIn section 4.2, theoretical analysis is provided. They present a modified Bellman operator and show that it is a contraction (lemma 4.1). Theorem 4.2 also asserts that - with appropriately selected hyperparameters \u2013 the value function of the in-distribution behavior policy should be lower.\n\nEvaluation is done on a legged quadruped robot in both simulation and the real world where their proposed method outperforms baselines in data efficiency (Figure 3 for simulation) and general performance (Table 1 for real world results). They further validate that their approach selects the appropriate behavior for the current situation with high accuracy (Figure 4) and show how fine-tuning with the additional cross-entropy loss causes the gap in value functions of in-distribution versus out-of-distribution policies to become more apparent (Figure 5)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- State-of-the-art performance compared to recent baseline methods.\n- Theoretical analysis included.\n- Simulation and real-world experiments conducted. \n- Ablation study included.\n- The work is well written and clear."
            },
            "weaknesses": {
                "value": "- The approach introduces an additional hyperparameter $\\beta$ that must be tuned. I am also unaware of how sensitive the approach is to this hyperparameter (whether most selected values will work well and beat baselines or whether only a small handful are appropriate).\n- The approach somewhat changes the definition of the Bellman operator such that it also contains a notion of the policies propensity to have encountered a given state instead of being based solely on the expected cumulative reward. Moreover, some values of $\\beta$ appear to have potentially odd behaviors. For example, selecting $\\beta=1$ appears to make the Bellman operator no longer depend on the reward signal?\n- There is perhaps some issues of fairness compared to RMA and HLC baselines. The authors use a state-of-the-art RLPD actor-citric method for their base learning approach. If RMA and HLC baseline methods also use actor-critic agents, were they also updated to use RLPD? If not, I would perhaps be concerned that the performance benefits reported may be in part due to RLPD instead of the author\u2019s proposed method.\n- Only a small number of real-world trials (3) are done and no confidence interval / variance is reported with the results (Table 1)."
            },
            "questions": {
                "value": "Questions are in part copied from the weaknesses section: \n- How sensitive is the approach is to the $\\beta$ hyperparameter (whether most selected values will work well and beat baselines or whether only a small handful are appropriate)?\n- Some values of $\\beta$ appear to have potentially odd behaviors. For example, selecting $\\beta=1$ appears to make the Bellman operator no longer depend on the reward signal. Can the authors clarify this?\n- There is perhaps some issues of fairness compared to RMA and HLC baselines. The authors use a state-of-the-art RLPD actor-citric method for their base learning approach. If RMA and HLC baseline methods also use actor-critic agents, were they also updated to use RLPD? If not, I would perhaps be concerned that the performance benefits reported may be in part due to RLPD instead of the author\u2019s proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828089332,
        "cdate": 1698828089332,
        "tmdate": 1699636746733,
        "mdate": 1699636746733,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OOCVa7x72b",
        "forum": "xTFgpfIMOt",
        "replyto": "xTFgpfIMOt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_K9rY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6577/Reviewer_K9rY"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of robot adaptation on the fly to unfamiliar scenarios and proposes a method for robust autonomous modulation (ROAM) that dynamically selects and adapts pre-trained behaviors to the situation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The problem of adaptation on the fly is important for robotics.\n\n+ The proposed approach seems novel and is well-justified to address on-the-fly robot adaptation.\n\n+ Experiments using real robots are a strength and well demonstrate the proposed method.\n\n+ Comparison with existing methods is clear in the related work section."
            },
            "weaknesses": {
                "value": "- Figure 1 motivates the problem using examples of facing various terrain and robot failure (e.g., damaged leg), but no experiments were performed on real robots in these scenarios.\n\n- Showing on-the-fly adaptation across different scenarios (beyond dynamic payloads) could make the experiments more convincing, for example, in a scenario when a robot with a heavy payload suddenly steps on icy terrain."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698886023346,
        "cdate": 1698886023346,
        "tmdate": 1699636746586,
        "mdate": 1699636746586,
        "license": "CC BY 4.0",
        "version": 2
    }
]