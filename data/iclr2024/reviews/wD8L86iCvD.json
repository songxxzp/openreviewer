[
    {
        "id": "GpJvEs5eIz",
        "forum": "wD8L86iCvD",
        "replyto": "wD8L86iCvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_QdMV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_QdMV"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to introduce an architecture which leverages large language models to solve multi-modal tasks. The authors do this by using 3 modalities: visual, audio and text. They introduce a a Temporal synchronization module that synchronize the visual and audio modalities. Moreover, they introduce a variant of Q-Former which uses causal attention."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well organised and it is very easy to be read and understood.\n- The method obtains significant improvements over the previous systems."
            },
            "weaknesses": {
                "value": "- Very personal opinion for Fig 1: I think the trainable modules should have different colors to be even easier to understand the figure. All causal Q-former should have the same color, as they are applied on multiple windows with the same parameters. But then, the LLM should have a different color and each encoder should also have a different color. In this way you are making sure no one thinks you are using the same parameters across all these. Also a lot of papers use a special symbol (the fire symbol) for the trainable layers, as in video-llama figure and other papers which I think would help if you also used it. \n- The novelty is somewhat limited as I will argue below:\n    1. Regarding the benchmarks, what is the novelty? As I understand, the paper does not provide a newly created dataset, but only puts together previously established datasets and calls it a new benchmark. Correct me if I am wrong.\n     2.  As I could see, the only difference between the proposed method and the video-LLAMA is using causal attention in the q-former, and the synchronization.\n- Why some datasets were chosen only for training and some only for testing? In the paper, there are some explanations for some of them, but there are still datasets such as llava-150k, text caps, ego4d or videochat, that were chosen for either testing or training without mentioning why.\n- From section 4.2 :\u201cfair comparisons, InstructBLIP is further fine-tuned on the same image and video training data as FAVOR.\u201d Was the same done for Whisper? If not, why? If yes, mention it.\n- Does Video-LLAMA use the same exact visual/audio encoders? If not, I am afraid the results will not reflect the true capabilities of this model.\n- Moreover, Video-LLAMA does not finetune the LLM, while this paper uses LoRA to finetune it. While one can argue that LoRA does not actually change the original weights, there is still some training happening there. Thus, I would also ask the authors to do an experiment where they keep the LLM frozen, as in video-llama and just train the other components. In this way it will be clear if the benefits come from the causal attention/synchronization model or if they come just from the fact that there are many more trainable parameters in the proposed model.\n- In Table 2 and 3, some results are missing for the Video-LLAMA and are replaced with \u201c-\u201d. Why? As far as I understand, Video-LLAMA could be applied on all the tasks that the proposed method can be applied to by just removing the missing modality. One can just remove the audio or the visual branch in the Video-LLAMA and turn it into a uni-modal model. One would not even need to re-train it as the LLM is not trained at all, so it does not \u201ccare\u201d if there are 2 modalities as input or only one. Please, correct me if I am wrong."
            },
            "questions": {
                "value": "- My main issue is the lack of novelty and the issue that the results may be good due to \u201cunfair decisions\u201d. \n- Mainly, the authors need to clarify if both the proposed model and the video-llama use the same encoders or not. If they don\u2019t use the same encoders, the experiments should be rerun to have a consistent setting with the same encoders.\n- Moreover, the authors need to provide evidence that even when not fine-tuning the LLM and using consistent encoders with video-llama, their causal Q-former and synchronization module helps. Right now, the boost in performance could be attributed entirely to fine tuning the LLM.\n- The authors should run the Video-LLAMA also on the task marked \u201c-\u201d, as mentioned in my previou comments. However, as I may be wrong here, I ask for clarifications for why this can not be done, if it can really not be done.\n- If these issues are addressed (mainly the fairness comparison), I think this paper is a good one, and even if it does not have a lot of novelty, it proposes a fairly simple extension that can obtain a significant boost in performance. But as it stands right now, there seems to be an unfair comparison and also lack of novelty and the paper is not good enough for being accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Reviewer_QdMV",
                    "ICLR.cc/2024/Conference/Submission5289/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697910522902,
        "cdate": 1697910522902,
        "tmdate": 1700845982812,
        "mdate": 1700845982812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1m8aSNdp4u",
        "forum": "wD8L86iCvD",
        "replyto": "wD8L86iCvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_gnXu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_gnXu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes FAVOR to transform an LLM into a multi-modal large model that is capable of understanding audio, speech, images, and videos. By using a causal mask in the Q-Former, the model learns temporal causal relationships of video frames and employs temporal synchronization to align visual and audio input features. In order to evaluate the model's comprehension abilities across audio, visual, and audio-visual domains, this paper introduces AVEB, which includes six single-modality tasks and five multi-modality tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is the first method to incorporate not only video and audio but also speech into a unified large model. The proposed causal Q-Former and the designs of temporal synchronization and sliding windows are proved to be effective.\n- The proposed multi-modal tasks ISQA and AVM are two significant tasks that can genuinely reflect the model's ability to align and comprehend audio/speech and visual information."
            },
            "weaknesses": {
                "value": "Overall, I find the novelty and contributions of this paper limited. Whether it's the model framework design, the design and effectiveness of the novel loss function, the proposed AVEB, or the models compared in the experiments, I see some flaws or shortcomings. Therefore, I would rate it 5 out of 10.\n- The novelty of FAVOR is limited. Overall, it bears similarities to Video-LLaMA. The method of integrating speech is not uniquely designed, and the use of sliding windows is not particularly novel.\n- The audio-visual benchmark makes a limited contribution since the majority of the tasks and datasets are adopted from other sources, and there is only one evaluation dataset for each task.\n- As for Tables 2 and 3, more models can be added in comparison, and it is not clear whether each model is evaluated in a zero-shot setting in each task, as the training datasets are only mentioned in the text. Furthermore, the SOTA performance in a fair comparison setting for each task is not specified in the tables, making it challenging to assess the effectiveness of FAVOR.\n- The diversity loss does not introduce a novel concept as it essentially constitutes a similarity loss applied across features. In addition, I don't understand why, in the second point of the main contributions, the diversity training loss can enable the causal Q-Former to efficiently handle audio-visual sequences **from a small number of training examples**. I cannot see from the experimental results either. On the other hand, from the experimental results in Table 9, it can be observed that the diversity loss does not necessarily lead to improvements on every task; in some cases, there are even performance degradations. To prove that diversity loss enhances model learning, applying it to other Q-Former-based models should also yield improvements."
            },
            "questions": {
                "value": "- Regarding the tasks in AVEB, there are several alternative datasets that have been employed for each specific task, such as Clotho[1] for audio captioning, COCO Captions[2] for image captioning, and OK-VQA[3] and ScienceQA[4] for visual question answering\n    - I'm curious about the criteria that guided the authors' selection of datasets for each task.\n    - Would it not be advantageous to consider the inclusion of multiple datasets for each task, given the potential biases present in individual datasets? This approach could offer a more robust assessment of the model's capabilities.\n    - Audio-visual question answering, e.g. MUSIC-AVQA[5] and AVQA[6], is a well-established task for evaluating models' cross-modal comprehension abilities, yet it is absent from AVEB.\n    - I'd like to suggest the inclusion of the recently introduced VALOR-32K[7] dataset, which centers around audio-visual captioning. Integrating this dataset and associated task into AVEB could substantially enhance the comprehensiveness of the benchmark, thereby facilitating a more thorough evaluation of audio-visual models.\n- More multi-modal models should be included in the experiments, such as VALOR[7] and VAST[8].\n- The importance and impact of the storytelling fine-tuning set are not explicitly explained.\n    - If fine-tuning on this dataset does not affect the benchmark performance, why should it be fine-tuned?\n    - Could you provide some examples to illustrate the differences in the model before and after this fine-tuning?\n- How many parameters need to be trained in FAVOR? What are the computational resources (GPUs) used for training the model? What is the training time with these computational resources?\n- How is InstructBLIP evaluated on the cross-modal tasks AVSD and AVSSD based on the fact that it cannot handle audio input? Does it mean it had no access to any audio input information during evaluation?\n- Upon encountering \u201cin order to handle variable-length inputs,\u201d I expect the sliding window method can convert input data of varying lengths into a uniform length. However, the number of sliding windows $W$ still depends on the input length $T$.\n\n[1] Konstantinos Drossos, Samuel Lipping, Tuomas Virtanen. Clotho: An Audio Captioning Dataset. ICASSP 2020.  \n[2] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C. Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv:1504.00325.  \n[3] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi. OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge. CVPR 2019.  \n[4] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. NeurIPS 2022.  \n[5] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, Di Hu. Learning to Answer Questions in Dynamic Audio-Visual Scenarios. CVPR 2022.  \n[6] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, Wenwu Zhu. AVQA: A Dataset for Audio-Visual Question Answering on Videos. ACM MM 2022.  \n[7] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, Jing Liu. VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset. arXiv:2304.08345.  \n[8] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu. VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Reviewer_gnXu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698044478630,
        "cdate": 1698044478630,
        "tmdate": 1699636529245,
        "mdate": 1699636529245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Djldum0Wm8",
        "forum": "wD8L86iCvD",
        "replyto": "wD8L86iCvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_AkuS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_AkuS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an audio-visual joint representation learning framework, specifically, a causal Q-former structure is proposed. The paper also proposes an audio-visual evaluation benchmark (AVEB) for evaluation. Experiments show that the designed model has promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. While there has been significant progress on LLM, large-scale multimodal models are still unexploited. The paper timely focuses on an important and interesting problem.\n\n2. The paper is overall well-written and easy to follow. \n\n3. Experimental results look promising."
            },
            "weaknesses": {
                "value": "1. The overall contribution and technical novelty do not meet the bar of ICLR.  Audio-visual large model is not a new idea. Using feature syncing and concat of different modalities, casual SA, sliding windows are well-known technologies. \n\n2. The design of the model architecture is questionable and needs more clarification: \n\n(1) The original self-attention unit also has the ability to attend to the contextual information (including previous frames), what is the additional gain to have another causal self-attention unit in the network? The motivation of such a design is unconvincing to me from the paper. Where does the claim \"what happens next questions are sometimes difficult to learn using only the positional embeddings\" come from? Are there solid studies/experiments/data points supporting this claim? Can you please clarify?\n\n(2) For the diversity loss, isn't the current equation also pushing the same feature far away (when i = j)? I don't feel the design is correct if there are no additional constraints on it. Isn't such design encouraging random features? \n\n3. The paper uses \"fine-grained\" in the title / abstract, but there isn't anything really related to \"fine-grained\" in the main paper. What does \"fine-grained\" here stand for? \n\n4. The experimental results need to be more solid and better analyzed. \n\n(1) From the results, FAVOR yields over 20% improvement, but where the improvement comes from is not discussed in the paper (a simple claim that \"the fine-grained causal modelling of video\" is not sufficient to answer the question, more details should be discussed). \n\n(2) \".... and to capture the correlation between what it \u201chears\u201d and \u201csees\u201d. Such tasks were almost infeasible for any other audio-visual models so far, since they were unable to understand both speech and non-speech sounds and did not model the audio-visual correlations in fine-grain.\" - first, solid datapoints on the failure of other audio-visual models should be provided (add citations); second, why FAVOR can handle this task should be better discussed.\n\n(3) The paper provides certain examples of FAVOR including acoustic audios / music rather than speeches only, but it's unclear to me how FAVOR models then. Do we need to model them separately from speech or we just send them to the model as is together with speeches? What about the tones, rhythms, etc.?"
            },
            "questions": {
                "value": "1. Fig 2 is unclear. Is input query representing the \"q1,...qn\" in Eq(2), and the feature sequence representing the \"h_t^AV, ...h_(t+k)^AV\" in Eq(2)?\n\n2. What is the storytelling fine-tuning set designed for? Why do we need such a set? Is it for training or benchmarking? If for benchmarking, what is it benchmarking for?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "LLM models are often with certain ethics issues. The paper is currently missing a discussion on discrimination / bias / fairness concerns of the proposed model."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5289/Reviewer_AkuS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698133376795,
        "cdate": 1698133376795,
        "tmdate": 1700733713880,
        "mdate": 1700733713880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lrfQ4ZDctY",
        "forum": "wD8L86iCvD",
        "replyto": "wD8L86iCvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_jiVL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5289/Reviewer_jiVL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework to learn a joint representation for audio and visual data in a fine-grained manner. The framework is designed to help multimodal large language models. A causal Q-Former structure is also introduced to capture the causal relationship between audio-visual frames over time. Additionally, the authors have created an evaluation benchmark for audio-visual learning that focuses on question-answering tasks to assess the effectiveness of our proposed framework."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper includes an interactive demo, allowing readers to experience the performance."
            },
            "weaknesses": {
                "value": "1. The present study's primary contribution pertains to the audio-visual joint representation, which lacks a clear definition of the term \"fine-grained.\" It is uncertain how this term is used in the context of this paper, and further clarification is required to better understand the audio-visual joint representation in question.\n\n2. According to the network design, the term \"fine-grained\" may denote fine-grained temporal resolution, where the temporal synchronization module is employed to synchronize visual and audio signals with a resolution of 0.5 seconds. However, the ablation of the temporal resolution as well as the rationale behind this synchronization have not been clearly explicated. It is essential to understand the underlying reasons for the setting of the temporal resolution and the motivation behind the synchronization, as it serves as a crucial aspect of the network design. Hence, further elaboration is required to provide a comprehensive understanding of the fine-grained temporal resolution and its relevance to the proposed network design.\n\n3. I am seeking clarification about the contribution of the proposed framework design. What sets it apart from previous methods? The act of incorporating additional modalities into a multimodal LLM appears to be a trivial implementation. Furthermore, the Q-former has previously been proposed for multimodal fusion. Thus, I am curious to know what novel aspects this framework design offers.\n\n4. The proposed audio-visual evaluation benchmark appears to be a composite of existing multimodal benchmarks. It is suggested that the term \"propose\" be replaced with a more appropriate term. Moreover, the benchmark in question, as a fine-grained multimodal benchmark, lacks some existing fine-grained audio-visual benchmarks, such as FAVDBench: Fine-grained Audible Video Description (CVPR 23).\n\n5. The author of the paper expounds on the utilization of speech and speech-video interactions. In speech-videos, two options exist for inputting speech audios to the network. These options include direct inputting of speech audios into the network or converting speech to text before the multi-modal fusion process. However, the author does not provide a comparative analysis of the two strategies. Furthermore, the ASR in audio-visual inputs is worse than audio-only, as demonstrated in Table 2. This result suggests that using the latter option may be more favorable. Lastly, the size of Whisper Large-v2 is not available in the paper."
            },
            "questions": {
                "value": "As above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5289/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669210355,
        "cdate": 1698669210355,
        "tmdate": 1699636529020,
        "mdate": 1699636529020,
        "license": "CC BY 4.0",
        "version": 2
    }
]