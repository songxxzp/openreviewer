[
    {
        "id": "zq1rjdahJk",
        "forum": "YZrg56G0JV",
        "replyto": "YZrg56G0JV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_GSze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_GSze"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the use of multitask training to enable myopic (overly simplistic) exploration methods to discover solutions to difficult MDPs which would otherwise not be tractable to learn directly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The core argument of this paper is salient and interesting, and seems to be well supported by the theoretical arguments. The experimental validation in a deep RL context is also appreciated in what is otherwise a theory paper."
            },
            "weaknesses": {
                "value": "I'm not well versed in recent theoretical/tabular RL exploration literature, so I can't speak very well to the novelty and significance there, but in a deep RL context this work seems relevant, but also very closely related to existing approaches such as goal-conditioned RL (Hindsight Experience Replay in particular) and to some extent automatic curriculum generation methods. \n\nI think these connections are very interesting, and this theoretical analysis isn't redundant with that work, but it does leave me feeling like this paper would me more interesting/have a stronger contribution if that connection were explored more. As it is I'm left feeling that while this specific argument is to my knowledge novel, it overlaps a lot with prior work. \n\nIn addition, I also felt like the presentation of the paper needs some significant polish. There's some issues with grammar and odd phrasing throughout the paper, and I while I appreciate the intuition provided for various definitions/theorems I felt like I frequently lost the thread on those.\n\nOverall, I think this is solid work that could be high impact, but it needs a little more polish to really shine. As such I'm inclined to recommend rejection, but I also admit that I don't have a good sense of the impact on the tabular RL exploration literature, so I will caveat that I can't properly evaluate that aspect and I will defer to other reviewers there."
            },
            "questions": {
                "value": "-While I generally follow the argument, this paper has some rough grammar and odd word choice in places. I'd recommend a thorough editing pass to improve the language.\n\n-The explanation of the multitask setup in Section 2.1 confused me as to how the tasks are getting selected. Is there a structure or order in which tasks are chosen among M?\n\n-Likewise Definition 1 is a little confusing. C is a function of beta and delta? I'm confused as to why sample complexity doesn't depend on either the algorithm itself or the task(s) being learned. The following paragraph seems to think C is a function of the MDP, but this isn't part of the definition.\n\n-How does algorithm 1 differ from the cited Zhumabekov 2023 policy ensemble method? It seems like algorithm 1 is essentially an ensemble of policies, of which one is sampled for each episode?\n\n-How does this idea of multitask myopic exploration differ from normal goal-directed RL methods like hindsight experience replay? The motivating example in Figure 1 seems roughly in line with such methods, and seems like it should share their limitations (e.g. large state spaces and low-dimensional manifolds of interesting/human-relevant tasks).\n\n -I find the term \"myopic exploration gap\" a little confusing. I understand the intuition (how much could a myopic exploration method improve upon the current best policy at any given time), but I wouldn't call that a gap. Maybe something like \"myopic exploration potential?\" A gap would imply it is comparing myopic exploration to another (optimal?) exploration algorithm. I know this term is coming from previous literature, but it seems confusing unless I'm misunderstanding the definition here.\n\n-Doesn't PPO (like all on-policy policy gradient methods) have issues with epsilon greedy optimization due to it's off-policyness? How did you resolve this issue for the experiments?\n\n-In the tabular case, multitask myopic exploration relies on coverage assumptions in the space of possible tasks (if I understand correctly), but it's not tractable to assume this in the deep RL case. Did this factor come up in the BipedalWalker experiments at all? Do the assumptions (mostly, at least) hold?\n\n-Some more details/analysis on the deep RL experiments in the main paper would be appreciated, such as performance/training curves. I realize the focus of the paper is tabular/theoretical, but this topic has a lot of connections to methods used in deep RL (such as goal-directed RL and automatic curricula, as noted), and in my opinion exploring that connection further would be very interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4126/Reviewer_GSze",
                    "ICLR.cc/2024/Conference/Submission4126/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774743365,
        "cdate": 1698774743365,
        "tmdate": 1700619816799,
        "mdate": 1700619816799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zi5ZaIKCF8",
        "forum": "YZrg56G0JV",
        "replyto": "YZrg56G0JV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_ZHzf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_ZHzf"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the statistical efficiency of exploration in Multitask Reinforcement Learning (MTRL). The authors show that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like \u03f5-greedy that are inefficient in general can be sample-efficient for MTRL. To validate the role of diversity, the authors conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper shows that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like \u03f5-greedy that are inefficient in general can be sample-efficient for MTRL.\n2.\tThe paper is well-written and easy to follow.\n3.\tTo the best of my knowledge, this is the first theoretical demonstration of the \"exploration benefits\" of MTRL, which is insightful for future research on efficient exploration in deep RL."
            },
            "weaknesses": {
                "value": "1.\tThe assumption that the task set is adequately diverse may be too strong in deep RL. Although the authors discuss implications of diversity in deep RL, it remains unclear to me. The authors may want to provide more insight into how to define and design a diverse task set for efficient exploration in deep RL."
            },
            "questions": {
                "value": "Please refer to Weaknesses for my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806598934,
        "cdate": 1698806598934,
        "tmdate": 1699636377729,
        "mdate": 1699636377729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OCZJWj4pOX",
        "forum": "YZrg56G0JV",
        "replyto": "YZrg56G0JV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_q9zq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_q9zq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the potential exploration benefits of multitask reinforcement learning from a theoretical perspective. This paper shows that when the set of tasks is diverse enough (measured by multitask MEG), a generic policy-sharing algorithm with myopic exploration is sample-efficient. Importantly, such myopic exploration is common in practice, and computationally efficient (unlike GOLF which requires solving nested optimization oracles). The paper also gives concrete examples of tabular/linear MDPs such that the diversity condition is satisfied. In the end, the paper validates the proposed theory with experiments and builds connections with curriculum learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The general idea of this paper is novel and natural. Sample efficiency of myopic exploration is an important topic.\n\n- The paper is very well-written.\n\n- The theoretical results are sound.\n\n- Discussion on limitations and comparison with prior works are adequate."
            },
            "weaknesses": {
                "value": "- The only weakness of this paper in my opinion is that examples where multitask MEG is bounded are too restrictive. Definition 7 is a very strong requirement, and intuitively, diverse tasks can be defined more general. Moreover, the feature coverage assumption is additional since it is not needed for learning linear MDPs with strategic exploration."
            },
            "questions": {
                "value": "- Is it possible to relax Definition 7?\n\n- The offline learning oracle solves  $f_1,...,f_h$ simultaneously. Can you do them sequentially and have similar guarantees?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4126/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4126/Reviewer_q9zq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947605180,
        "cdate": 1698947605180,
        "tmdate": 1699636377627,
        "mdate": 1699636377627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IeDtRxOaXv",
        "forum": "YZrg56G0JV",
        "replyto": "YZrg56G0JV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_E4JK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4126/Reviewer_E4JK"
        ],
        "content": {
            "summary": {
                "value": "This paper claims that in the scenario of multitask-RL, a naive exploration strategy is enough. It formalizes their intuition in Def 3 and provide the theoretical guarantee in Theorem 1."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proof seems to be sound."
            },
            "weaknesses": {
                "value": "1. It provides a possible explanation to explain the success of naive exploration in the case of multitask RL. However, it is hard to validate such explanation.\n\n2. The intuition of the proof is that, if we have a base policy class with good coverage, we are able to find the optimal policy by combining the base policies with naive exploration. However, it have been well known that exploration is simple when we have good coverage. Therefore, their contribution seems not novel enough. \n\n[1]. Xie, Tengyang, Dylan J. Foster, Yu Bai, Nan Jiang, and Sham M. Kakade. \"The role of coverage in online reinforcement learning.\" arXiv preprint arXiv:2210.04157 (2022)."
            },
            "questions": {
                "value": "1. What does Def 3 mean in linear MDP?\n\n2. Can you provide an example where Def 3 holds and the coverage of the base policies is poor?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699591881486,
        "cdate": 1699591881486,
        "tmdate": 1699636377529,
        "mdate": 1699636377529,
        "license": "CC BY 4.0",
        "version": 2
    }
]