[
    {
        "id": "xz9PRmxJGx",
        "forum": "0uUASYeXav",
        "replyto": "0uUASYeXav",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_rSLu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_rSLu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an object-centric actor-critic algorithm, where first object-centric representations are extracted from pixels using SLATE, and then a GNN is used to train the policy and Q-value prediction. The method was evaluated on object reaching, pushing, and navigation tasks, and compared with Dreamerv3 and a transformer-based OCRL baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper combines a number of elements (SAC, SLATE, GNN transition model) and demonstrates that it works."
            },
            "weaknesses": {
                "value": "- The method claims to outperform the baselines, but it feels like the particular environments are somehow crafted / cherry-picked / designed to have some failure cases for the baselines. i.e. it is striking that for Navigation and Object reaching the OCRL method seems on par or even better, but it completely fails on this PushingNoAgent and Object reaching with different color scheme environment, which are less standard benchmarks.\n\n- It is unclear from the paper and method description exactly which parts are novel for this paper. i.e. 4.1 simply rehearses SLATE and 4.2-4.5 utilize the GNN architecture as described by Kipf et al 2020? I assume that the main novelty is in the combination + training end-to-end with SAC?"
            },
            "questions": {
                "value": "- Any idea on why the OCRL method fails on particularly the PushingNoAgent and Object Reaching with the VOYC color scheme tasks, but is on par or even better on the others? Also the fact that the OCLR method completely fails on a task seems like rather a bug or wrongly tuned hyperparameters, since there shouldn't be any significant difference in the problem structure...\n\n- As a model-based baseline DreamerV3 has no structured latent space as opposed to the proposed method. An interesting comparison would be to have a model-based baseline that uses structured world models, e.g. like the one proposed in Kipf et al."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Reviewer_rSLu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698398724187,
        "cdate": 1698398724187,
        "tmdate": 1699636870590,
        "mdate": 1699636870590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UPuG4aTIto",
        "forum": "0uUASYeXav",
        "replyto": "0uUASYeXav",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_aEan"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_aEan"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an RL method based on object-centric representations and world models. The object-centric representation is based on SLATE, the components built on top of the representation (transition, reward, value and actor model) use graph neural networks and the RL algorithm is based on SAC (continuous and discrete). The work is evaluated on 5 variations of 2 environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* **Ideas**: there are several promising ideas combined together in this work, such as the idea of using object-centric representations together with world models, and the idea of using graph neural networks for the transition dynamics."
            },
            "weaknesses": {
                "value": "* **Model-based approach?**: it is not clear to me how this is a model-based approach for RL. Are the world model components (ie. transition and reward models) only used to improve the representation? Or is the actor-critic actually learned in imagination and this is not detailed in the paper?\n* **Novelty**: the work combines several ideas from the literature (SLATE, the GNN from CSWM, SAC in continuous and discrete settings) into one method. While it is interesting to see that this works, in the tested environments, a strong novel contribution seems to be missing from the paper.\n* **Evaluation**: the experiments in the paper are mainly limited to two tasks/environments (with variations). It is unclear what the authors are trying to show, as they mention `efficiency' but in the standard versions of the tasks the \"OCRL\" baseline seems to be more efficient, as it converges faster in the 5x5 Navigation task and it starts learning faster and converges to a similar result in the BGYR Object Reaching task. I also find the ablation study unclear: if the approach is model-based, how are SAC-based approaches ablations? How does the tuning of the entropy term cause such a large difference in performance?"
            },
            "questions": {
                "value": "* Why is the OCRL baseline affected by the changes in colors and GOCA is not? Aren't they both based on SLATE for the encoding?\n* I think the evaluation is unfair to DreamerV3 as this baseline's representation is not pre-trained on any data from the environment, while GOCA (and possibly OCRL too?) representation is pre-trained. I would ask the authors to find a way to make this comparison fair (e.g. pre-training the DreamerV3 world model too)\n* How many random steps from the environment are used for pretraining SLATE?\n\nTypos/writing suggestions:\n* In Section 4.2, \"We approximate THE transition function\" --> (missing THE)\n* In the Conclusion, \"We presented GOCA, an object-centric off-policy ~~value-based~~ \" -> I wouldn't call the approach value-based as it uses an actor-critic"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Reviewer_aEan"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620642440,
        "cdate": 1698620642440,
        "tmdate": 1699636870331,
        "mdate": 1699636870331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ObF9yzhqKs",
        "forum": "0uUASYeXav",
        "replyto": "0uUASYeXav",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_HvnL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_HvnL"
        ],
        "content": {
            "summary": {
                "value": "This manuscript integrates object-centric representation learning with object-relational dynamics, aiming to construct a dynamic model directly from pixel data for use in model-based reinforcement learning. On a conceptual and motivational level, the paper addresses a pivotal issue in RL, specifically in scenarios involving multiple objects or entities.\n\nWhile the underlying idea is straightforward, the reported results suggest that the proposed solution is not only simple but also effective. Nonetheless, there are aspects of the technical implementation that require additional clarification, and there is room for improvement in the manuscript\u2019s overall clarity and writing style. Given these considerations, my initial assessment leans towards a borderline rating, contingent upon further revision and clarification in the further phases."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**[About the idea and motivation]** While the conceptual framework of object-centric reinforcement learning has been explored and analyzed in recent literature, notably by Yoon et al. in 2023, the current manuscript stands out by presenting a harmonious synthesis of object-centric learning and dynamic modeling. The framework demonstrates impressive efficacy, particularly in the realm of pixel-based reinforcement learning control tasks. Consequently, the paper\u2019s core idea holds significant merit and is poised to make a positive impact within the fields of reinforcement learning and dynamic modeling.\n\n**[About the evaluation]** While the existing evaluations substantiate the effectiveness of the proposed methods, there is a need for additional assessments to verify these findings further, details of which I will give in the subsequent sections. Nonetheless, the design and evaluation components of the paper validate the efficacy of the approach, and the conducted ablation studies indicate the significance and impact of each aspect of the model\u2019s design."
            },
            "weaknesses": {
                "value": "I listed both the weaknesses and questions here.\n\n1. **[About model design]**\n\n- I recognize the potential of employing patch-based object-centric learning for extracting meaningful features for each object. However, there is a concern regarding the sufficiency of these learned factors in accurately modeling the dynamics and policy. Alternatives like deep latent particles [1] could potentially offer more straightforward features, facilitating more effective dynamics modeling. While it is not necessary to conduct experiments with these alternatives during the rebuttal phase, any additional clarification from the authors on how they ensure that the patch-based model can learn representations that are sufficient and robust enough for modeling the dynamics would be highly appreciated and beneficial.\n\n- In Fig. 2 and Section 4.2, it appears that the authors have adopted an assumption of dense interactions among objects, modeled using complete graphs. However, it is a well-acknowledged fact that interactions in real-world scenarios tend to occur sparsely rather than densely. While I know that Graph Neural Networks (GNNs) have the capability to emulate these naturally sparse interactions through their neural representations, there remains a question to me is why the authors chose not to explicitly model these interactions using a sparse graph, which could potentially provide a more direct and accurate representation of the interactions. One reference is [2]. \n\n2. Does the proposed method extend its applicability to more realistic scenarios, particularly in terms of generalization capabilities? Specifically, it would be insightful to understand if the method can generalize to scenarios involving a greater number of objects, novel objects, or additional object attributes (such as color) during the inference stage. One reference is [3]. \n\n3. **[About presentation]**\n\nThe paper demonstrates a clear presentation and logical flow throughout. However, there are several areas that could be enhanced for an even better presentation. Here are some suggestions for improvement:\n\n- I recommend making sections 3.1 and 3.2 to be shorter, given that they cover fundamental concepts in RL that the target audience is likely to be familiar with. By doing so, you can maintain the reader's focus on the core contributions of your work. You might consider moving the more detailed explanations to an appendix.\n\n- For all equations, I suggest employing a different text operation for \"node\" and \"edge\", maybe \\text{} or \\texttt{}, to clearly distinguish these terms from other variables and operations.\n\n\n- Check the citation formats, use \\citep and \\citet correctly. \n\n4. The current title, \"Graphical Actor Critic,  captures the aspect of the methodology, I recommend incorporating terms like  \"Relational\" to provide a clearer and more precise indication of the paper's focus. Given that the method does not directly learn or estimate graphical models, this adjustment would help set accurate expectations for the reader and align the title more closely with the paper\u2019s core contributions.\n\n*References*\n\n[1] Daniel, Tal, and Aviv Tamar. \"Unsupervised image representation learning with deep latent particles.\" ICML 2022.\n\n[2] Zadaianchuk, Andrii, Georg Martius, and Fanny Yang. \"Self-supervised reinforcement learning with independently controllable subgoals.\" CoRL 2022.\n\n[3] Zhou, Allan, et al. \"Policy architectures for compositional generalization in control.\" arXiv preprint arXiv:2203.05960 (2022)."
            },
            "questions": {
                "value": "I listed the questions together with weaknesses in the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7287/Reviewer_HvnL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831034404,
        "cdate": 1698831034404,
        "tmdate": 1699636870168,
        "mdate": 1699636870168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "at28jm51cx",
        "forum": "0uUASYeXav",
        "replyto": "0uUASYeXav",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_6gyQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7287/Reviewer_6gyQ"
        ],
        "content": {
            "summary": {
                "value": "The authors presented an object-centric off-policy value-based model-based reinforcement learning approach that uses a pre-trained SLATE model as an object-centric feature extractor; and outperforms object-centric model-free and model-based baselines. Their work shows GNN works well for this type of tasks. If properly presented, the framework presented can become a general purpose method for object-centric downstream tasks in terms of representation.  However, the paper is hard to read and follow and the contributions are not evident."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper lists down limitations and future scope of work.\nThe philosophy of using GNN along with Actor Critic is known in other setups, however, the authors have worked hard in the formulations.\nIt's good to see the details of setup in Appendix."
            },
            "weaknesses": {
                "value": "The abstract needs to be to the point, crisp, mentioning the the gaps in SOTA and what was done with numbers in support.\nThe introduction section should contain an illustration of the problem statement and use case.\nI believe that this model needs to be tested in embodied ai challenge tasks and their benchmarks to derive the benefits of this in downstream tasks. The work is mostly based on principles of SLATE.\nTuning plays a significant role in GOCA (Fig 6), so the practicality of efficient deployment in a use case is doubtful.\nI find the results not substantial.\nSupplementary material is not well utilized, no reference to code or videos."
            },
            "questions": {
                "value": "How does the message passing happen and how does it scale with large graphs?\nHow are the embedding dimensions come into?\nShould the losses be equally weighed? dVAE and cross entropy - after eqn 1.\nHow do you plan to overcome the listed limitations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699295172958,
        "cdate": 1699295172958,
        "tmdate": 1699636870059,
        "mdate": 1699636870059,
        "license": "CC BY 4.0",
        "version": 2
    }
]