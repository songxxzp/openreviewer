[
    {
        "id": "rujgMo8exR",
        "forum": "pFOoOdaiue",
        "replyto": "pFOoOdaiue",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission198/Reviewer_cFcL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission198/Reviewer_cFcL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a robust RL framework on top of RARL, considering the entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE). With this extension, solving zero-sum games will not always face complex saddle point optimization problems. This paper is with theoretical support and numerous impressive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The algorithm is novel in the robust learning community.\n2. The empirical results demonstrate that the proposed method outperforms the existing baselines.\n3. Proposed methods are well motivated with good explainability."
            },
            "weaknesses": {
                "value": "1. Overall, how the proposed method is adapted to the real world with different action spaces between protagonist and adversary is still unclear. The detailed questions and concerns please refer to the questions section."
            },
            "questions": {
                "value": "1. Your framework is built on the top of RARL, which means that the adversarial action spaces are specifically chosen to be different from those of the protagonist agent in order to exploit domain knowledge. Could you explain more about how you set up the adversarial action spaces with the additional environments/tasks compared with the ones (e.g., halfcheetah, swimmer,  hopper ) RARL provides? \n\n2. Is your proposed method also compatible with Noisy Robust Markov Decision Process (NR-MDP), which MixedNE-LD builds on top of? If so, what the role of $\\alpha$ will be in NR-MDP? Is it just similar to the concept of $\\delta$ mentioned in MixedNE-LD paper for defining the limit of the adversary?\n\n3. How can we elaborate the concept of rationality in the experiment? Does the most rational adversary mean the strongest adversary (severity strength)? In that case, I am not sure if irrationality at the beginning represents the less strength of attack. \n\n4. On page 2, a statement: \"Conversely, in a setting where the protagonist is completely rational and the adversary is completely irrational, i.e.,\nit plays only random actions...\". Do you mean that the protagonist is playing only random actions? Then what action of the adversary take?\n\n5. On page 5, a statement: \"We propose to initially solve an adversarial problem with a completely irrational adversary, i.e., $\\alpha \\rightarrow \\infty$, which results in a simpler plain maximization of the performance of the protagonist, neglecting robustness\". Does it mean that we do not have any attack now?\n\n6. Could you provide the hyperparameter tuning or how you select $\\xi$ and $\\epsilon$? I think they will also influence the adversarial learning process.\n\n7. Could you please point out which script.py under mujoco_experiments is your proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698451289222,
        "cdate": 1698451289222,
        "tmdate": 1699635945731,
        "mdate": 1699635945731,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WKXtuBdpLk",
        "forum": "pFOoOdaiue",
        "replyto": "pFOoOdaiue",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission198/Reviewer_S4Ho"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission198/Reviewer_S4Ho"
        ],
        "content": {
            "summary": {
                "value": "This paper builds on robust adversarial reinforcement learning (RARL) and add entropy regularization into the players' objectives. By adding regularization, the authors bound the rationality of the adversary (and protagonist) making the problem slightly easier to solve. Over training, the regularization is annealed, creating a curriculum, such that the ultimately trained protagonist is robust against a strong adversary. The authors show this approach outperforms several baselines empirically across a variety of tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I found this paper generally easy to follow and well written. It proposes a simple modification to RARL that appears to greatly improve the empirical performance and make sense from a theoretical perspective as well."
            },
            "weaknesses": {
                "value": "Once the temperature had been split between the two players ($\\alpha$ and $\\beta$), I found the connection to QRE a bit tenuous. This is okay, but I think it would be better to present the QRE with one temperature and then state that you find better performance by using two.\n\nI would have liked to see results over the course of training. Do you see monotonic improvement? How challenging is the saddle point problem (adversary vs protagonist) experimentally? Can you plot $(\\alpha, \\beta)$ over training?"
            },
            "questions": {
                "value": "**I have upgraded my score after review of the authors' feedback**\n\n- Note McKelvey and Palfrey defined the QRE along with a more specific QRE, called the limiting logit equilibrium, that is obtained by annealing the temperature from infinity to zero (homotopy approach).\n- Equation 4: You describe this as a maximum entropy formulation, but this looks like an entropy regularized approach rather than selecting the equilibrium with maximum entropy (which is different).\n- Irrational: I understand why you've chosen to pair \"irrational\" against \"rational\", but I think it's inaccurate. I think you mean \"random\". Note that a random policy is not necessarily irrational (e.g., random is Nash in rock-paper-scissors).\n- Section 4.1: You say \"In Markov games... QRE can be computed in closed form...\". This is not true. If we could compute QREs in closed-form (at any temperature), we could compute nearly exact Nash equilibria in closed-form. I think you just mean that computing the denominator of equation 5 is difficult due to the integral. The integral becomes a sum with finite actions, but you still have to solve a fixed-point problem to compute a QRE.\n- Why do you define a probability distribution over $\\alpha$ instead of just controlling $\\alpha$ point-wise?\n- Equation 11: Do you minimize this for a fixed $\\lambda$ and $\\eta$?\n- Do you have an easily-accesible reference for QREs with different temperature values per player (i.e., heterogeneous QREs)? I'm familiar with QREs, but never seen this and I couldn't track down the precise Goeree reference you cite. To my knowledge, this is a deviation from the QRE concept, but still makes for an interesting story as inspiration for your approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Reviewer_S4Ho"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687008711,
        "cdate": 1698687008711,
        "tmdate": 1700829690873,
        "mdate": 1700829690873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xiYktdrGjp",
        "forum": "pFOoOdaiue",
        "replyto": "pFOoOdaiue",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission198/Reviewer_DPhM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission198/Reviewer_DPhM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel entropy regularization algorithm called Quantal Adversarial RL for adversarial reinforcement learning \u00a0which \u00a0modulates adversarial rationality to ease the complexity of solving saddle point optimization problem in robust adversarial RL.They connect entropy regularization in RL to bounded rationality and Quantal Response Equilibrium in game theory and show how temperature parameter in entropy regularization can control rationality and helps to train against a rational adversary. They provide extensive experiments showing QARL outperforms RARL and other baselines in several MuJoCo problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Connection between entropy regularization and bounded rationality is novel.\n2. Proposes a novel constraint optimization problem to design curriculum for updating temperature coefficient that slowly changes an irrational adversary to a fully rational one.\n3. Extensive empirical experiments to demonstrate the effective of their algorithm."
            },
            "weaknesses": {
                "value": "1. Relies on heuristic approach to design curriculum schedule with no theoretical guarantee on convergence behavior.\n2. Certain parts of the paper and notations can be improved, see questions section."
            },
            "questions": {
                "value": "1. Where is policy $\\pi$ used in the equation 6,7,8? What is variable $\\mathcal P$ as the conditioning parameter?\n2. Definition of $Q^\\*$ and $\\pi^\\*$ is not very clear. Both $Q^\\*$ and $\\pi^\\*$ depend on each other in eqn 6 and 8?\n3. Is there any insight into why the curriculum approach helps optimization compared to direct adversarial training?\n4. How well does the computation overhead of sampling different rationality level scale?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784041768,
        "cdate": 1698784041768,
        "tmdate": 1699635945569,
        "mdate": 1699635945569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YJOkB10lOt",
        "forum": "pFOoOdaiue",
        "replyto": "pFOoOdaiue",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission198/Reviewer_puRh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission198/Reviewer_puRh"
        ],
        "content": {
            "summary": {
                "value": "Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against an adversary in a competitive zero-sum Markov game. However, in a high-dimensional control setting, finding the Nash equilibria faces complex saddle points. This method eases the complexity of the saddle point in optimization problems based on entropy regularization. They show the solution of an entropy-regularized problem corresponds to a Quantal Response Equilibrium, in which agents may be irrational with a certain probability. Based on this fact, this paper proposes an algorithm named Quantal Adversarial Reinforcement Learning (QARL). This algorithm first trains the protagonist against an irrational adversary and gradually increases the irrationality of the adversary until it is fully rational. This paper shows that QARL achieves stronger performance and robustness compared with other RARL algorithms in a wide range of reinforcement learning settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is novel in the sense that it proposes a new method achieving stronger performance and robustness in adversarial reinforcement learning by gradually increasing the rationality of the adversary during training.\n\n2. This paper draws an interesting connection between Quantal Response Equilibrium and an entropy-regularized problem and proposes a hyperparameter to smoothly control the rationality of the agent.\n\n3. The experimental results are detailed and convincing. QARL outperforms other RARL algorithms in both performance and robustness. The experiments are conducted in a wide range of reinforcement learning settings including MuJoCo locomotion and navigation problems."
            },
            "weaknesses": {
                "value": "1. To train the protagonist by QARL, this paper requires that the rationality of the adversary can be controlled. This is a strong assumption, and its motivation is not well-justified. It would be better if this paper could show the performance of QARL even if the rationality of the adversary is not tuned but just increasing. Most importantly, this weakness limits the possible application of this algorithm to scenarios where a reliable simulator is available because the control over the rationality of the adversary may only be possible in a simulator.\n\n2. Estimation of (12) consumes non-negligible online trajectories. The way to tune the rationality hyperparameter $\\alpha$ is expensive."
            },
            "questions": {
                "value": "Have you tried any other heuristics for tuning $\\alpha$? If so, could you briefly discuss them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission198/Reviewer_puRh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788526939,
        "cdate": 1698788526939,
        "tmdate": 1699635945496,
        "mdate": 1699635945496,
        "license": "CC BY 4.0",
        "version": 2
    }
]