[
    {
        "id": "yA202pP2ou",
        "forum": "1JbsdayvhO",
        "replyto": "1JbsdayvhO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission403/Reviewer_a83P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission403/Reviewer_a83P"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new diffusion based framework that for 3D scene generation from 2D images.\nThe proposed method includes a new neural representation for 3D scenes, based on lifting image-based features from multiple cameras into 3D space. A denoising diffusion framework that ensures the consistency of the 3D scene throughout the generation process by first generating neural scene representations and then conduct volumetric rendering. Experiments on multiple datasets shown the proposed method could generate 3D scene with better quality than previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The overall pipeline is clear. The idea of generating a 3D scene representation first and then render it to 2D images is reasonable for preserving 3D consistency.\n- The writing is overall clear.\n- The result quality looks plausible."
            },
            "weaknesses": {
                "value": "- The proposed representation of 3D scenes is not totally new. Using projected image features is common in MVS field for constructing cost volumes [W1]. Using cross-attention to fuse information from arbitrary number of views during network forward is interesting; yet in [W2], a similar network structure was also proposed to fuse features from arbitrary number of views, the only difference is they uses RNN rather than cross-attention layers. An in-depth discussion of key differences between the proposed method and these related works should be included. \n- Two type of baselines is lacked for 3D generation / reconstruction: (1) Optimization-based pipeline with diffusion model as priors, e.g., [W3]; (2) Diffusion models trained on purely 3D data, e.g., [W4].\n- For the visual result in supp. materials: generated results do not have comparisons. inference results do not provide input images. Lacking these materials make it hard to evaluate the quality.\n\n[W1] Yao, Yao, et al. \"Mvsnet: Depth inference for unstructured multi-view stereo.\" Proceedings of the European conference on computer vision (ECCV). 2018.\n\n[W2] Riegler, Gernot, and Vladlen Koltun. \"Free view synthesis.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIX 16. Springer International Publishing, 2020.\n\n[W3] Liu, Ruoshi, et al. \"Zero-1-to-3: Zero-shot one image to 3d object.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[W4] Wang, Tengfei, et al. \"Rodin: A generative model for sculpting 3d digital avatars using diffusion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "- In 3.1: \"To ensure the scene geometry is well-defined even outside the union of the camera frustra, for each camera we also calculate a polar representation of the displacement of p relative to the camera\u2019s center, and use the resulting angles to interpolate into a second feature map (with an equirectangular projection)\" - Does the result feature map $f_v'$ is shared amount all views? How to use the angles to interpolate this feature map?\n- The proposed method supports arbitrary views. How does the performance w.r.t number of views? \n- Real multi-view datasets usually contain camera calibration errors (as they are usually estimated via some algorithm). How does the robustness of proposed method w.r.t camera calibrations?\n- Regarding the ablation study experiment on using triplanes: It seems the experiment directly using 2D U-Net to output tri-plane features. Yet, as argued in [Q1], there exists an incompatibility between the tri-plane representation and the naive 2D U-Net method. In [Q1] a 3D-aware convolution for output tri-plane features from 2D U-Net is proposed and achieved good quality. How would this variant fit into the proposed method in this paper?\n\n[Q1] Wang, Tengfei, et al. \"Rodin: A generative model for sculpting 3d digital avatars using diffusion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Reviewer_a83P",
                    "ICLR.cc/2024/Conference/Submission403/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697967210075,
        "cdate": 1697967210075,
        "tmdate": 1700304489778,
        "mdate": 1700304489778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RF9x7Kd6fW",
        "forum": "1JbsdayvhO",
        "replyto": "1JbsdayvhO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission403/Reviewer_Wz4z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission403/Reviewer_Wz4z"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use a multi-view version of a diffusion model for 3D reconstruction, which would be suitable for estimating occluded parts from a relatively small number of viewpoints. The proposed method involves several technical novelty, in which the key technical contribution may be in the methods of 1) NeRF-like 3D representation using image features and 2) exploiting multi-view consistency to diffusion models. The experiments show that the proposed method achieves better novel-view synthesis accuracies compared with, e.g., RenderDiffusion and PixelNeRF."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ For the context of usual multi-view 3D reconstruction, in which the camera views are not structured (i.e., allowing free movement), this paper may be the first attempt to use modern image generation methods like diffusion models.\n+ The performance of the proposed method outperforms SOTA diffusion-based 3D reconstruction.\n+ The simple regularization used in this work, dropping out the features from a view when rendering to that same viewpoint, would be helpful in a broader context."
            },
            "weaknesses": {
                "value": "- There are some unclear technical details and contributions (see detailed comments).\n\n- As a single-view 3D reconstruction method, I agree with the practical value of using diffusion models or similar stuff. However, the proposed method basically targets the multi-view contexts. For multi-view 3D reconstruction, usual MVS-based methods (or neural-MVS-based methods, perhaps) may still achieve much better performance for large-scale and detailed reconstruction. \nA fundamental drawback of using multi-view-consistent generative models for multi-view 3D reconstruction may be in the limitation of input image resolution (due to the size of graphics memory), as well as the generative models change the image content that can degrade the faithfulness of the 3D reconstruction.\n\n- Related to the above, a potential merit of using generative models for 3D reconstruction is the occlusion recovery, as written in the introduction. In such senses, I could see experiments assessing the results of 3D reconstruction under severe occlusions.\n\n- Also related to the above, a potential drawback of using diffusion models for 3D reconstruction may be that they change the content of given multi-view images. I could see some discussions (and potential failure cases, if any)."
            },
            "questions": {
                "value": "- The contribution statement is not clear: \"first denoising diffusion model that can generate and reconstruct large-scale and detailed 3D scenes.\" Which part is actually the \"first\"? Which adjective corresponds to which technical part? For example, readers may think RenderDiffusion met the above stuff. \n\n- Related to the above, it is pretty helpful to state the technical difference and practical merit compared with recent methods like RenderDiffusion and PixelNeRF.\n\n- As the paper mentions, the proposed 3D representation is similar to PixelNeRF. To the 3D representation, which part is the key technical difference from PixelNeRF?\n\n- The multi-view version of diffusion models inputs the camera poses as embeddings. If allowing the arbitrary viewpoints to reconstruction, I thought it is not enough just to input (and map) camera matrices to achieve reasonable multi-view conditioning on diffusion models. The multi-view image features are just concatenated (i.e., pixel-aligned), so there should be a part that computes correspondence among the views. Attention layers may do it, but I am wondering if it is really capable of finding correct correspondences from the given information (without, e.g., geometrically projecting the pixels to 3D scenes during the training of diffusion models). Do they use any strong assumptions, e.g., camera locations are almost the same or less variety for training datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Reviewer_Wz4z"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556450721,
        "cdate": 1698556450721,
        "tmdate": 1700625516268,
        "mdate": 1700625516268,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qy3iwhNRKR",
        "forum": "1JbsdayvhO",
        "replyto": "1JbsdayvhO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission403/Reviewer_C6XK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission403/Reviewer_C6XK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a diffusion model for generating new perspectives of reconstructed 3D scene using 2D inputs from various viewpoints. The authors employ a set of techniques, such as a 3D feature representation and feature dropping during training, to enhance the consistency of the 2D diffusion model in view synthesis. They also extend the model's capability to accommodate different numbers of input views, making it more versatile. The experimental results presented by the authors demonstrate superior performance compared to previous approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a diffusion-based model for handling the novel-view generation problem given several inputs of viewpoints. This model is able to generate new content in areas where the given viewpoints are not covered."
            },
            "weaknesses": {
                "value": "I have two main concerns about this paper:\n\n- 1. After reviewing the supplementary material, I noticed that the generated images from different viewpoints don't seem very consistent. The videos display noticeable bouncing. Have the authors conducted both qualitative and quantitative assessments to verify if the proposed representation approach truly maintains consistency among the outputs of the diffusion model for the same 3D scene?\n\n- 2. The paper presentation could be enhanced. While Figure 1 demonstrates the adaptive neural scene representation, there is no figure of the structure or pipeline of the multi-view diffusion model. Given that this diffusion model plays a crucial role in this paper and contains modules like the setwise multi-view encoder, it deserve a figure. Otherwise, readers might find it difficult to quickly grasp the authors' intention."
            },
            "questions": {
                "value": "As mentioned in the weaknesses section, have the authors undertaken both qualitative and quantitative evaluations to figure out if their method indeed preserves consistency? (or at least an ablation study) \n\nWhile I acknowledge that perfection might not be attainable due to the presence of the black-box diffusion model, an examination in this regard is necessary to substantiate the authors' assertion that *their method ensure 3D consistency while retaining expressiveness*."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Reviewer_C6XK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683935681,
        "cdate": 1698683935681,
        "tmdate": 1699635966867,
        "mdate": 1699635966867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ptP1oQQpLP",
        "forum": "1JbsdayvhO",
        "replyto": "1JbsdayvhO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission403/Reviewer_MQQ6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission403/Reviewer_MQQ6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a diffusion model for fast and detailed reconstruction and generation of realistic 3D scenes. It introduces a new neural scene representation, a denoising-diffusion framework for learning a prior over the representation, and a unified architecture for conditioning on varying numbers of images. This method achieves SOTA performance in 3D reconstruction and unconditional generation according to the experiments part."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This method achieves state-of-the-art performance in both 3D reconstruction and unconditional generation tasks. Notably, in Figure 2, impressive scene-level 3D generation results are observed, surpassing the capabilities of prior works.\n2. The designed method is practical as it can accommodate arbitrary numbers of input views, making it versatile and applicable in various scenarios.\n3. The methodology section is characterized by its clarity and accessibility, facilitating a comprehensive understanding of the entire design."
            },
            "weaknesses": {
                "value": "1. No qualitative comparison was made with baselines, particularly VSD. While this paper includes numerous quantitative comparisons with three baselines, the figures only display the results of this paper in recontruction and generation task. The visual quality of VSD is also good, and it would be valuable to include its results for comparison.\n2. No quantitative and qualitative comparison was made with VSD in the task of single view reconstruction.\n3. The experiments section lacks clarity, as the pixelnerf++ and renderdiff++ are not explained in the main paper but rather in the appendix. Additionally, although the authors revised VSD, the table still refers to it as VSD instead of VSD*, and this should be explicitly noted."
            },
            "questions": {
                "value": "The experiments section requires improvement, specifically in the reconstruction task where it is important to include a comparison with VSD. Additionally, qualitative comparisons with all three baselines in both tasks should be provided. It is crucial to include the ablation study in the main paper rather than relegating it to the appendix. Overall, the experiments section needs reorganization to better incorporate these essential components."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission403/Reviewer_MQQ6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699220702099,
        "cdate": 1699220702099,
        "tmdate": 1699635966783,
        "mdate": 1699635966783,
        "license": "CC BY 4.0",
        "version": 2
    }
]