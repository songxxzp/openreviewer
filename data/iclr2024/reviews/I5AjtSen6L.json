[
    {
        "id": "1Hd98kPaac",
        "forum": "I5AjtSen6L",
        "replyto": "I5AjtSen6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_wEZx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_wEZx"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to certify the fairness of GNN models. The authors specify an indicator function that returns 1 when a given fairness metric (e.g. SP or EO) is below a treshold. This essentially reduces the problem to certifying a generic binary function, a problem that can be tackled with existing randomized smoothing certificates. They adopt the randomized smoothing framework and consider perturbations w.r.t. both the node attributes ($l_2$ norm) and the graph structure ($l_0$ norm) using Gaussian and Bernoulli noise respectively."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In my opinion the main contribution of the paper is the introduction of the novel and highly relevant problem of certifying the fairness of GNNs. \n\nConsidering the joint perturbation of both continious attributes and discrete structure is relevant and novel."
            },
            "weaknesses": {
                "value": "One of the biggest weaknesses of the paper is that it does not properly place its results in the context of the existing literature. Once the indicator function w.r.t. the threshold has been defined the problem of certifying the output of the resulting function $g$ is a trivial application of previous results. The result from Theorem 1 has been know since 2019, since Theorem 1 (specifically Eq. 3) in [1] directly applies. However, the authors spend considerable effort re-proving these known results (e.g. via the intermediate Lemmas A1, A2 and A3 which are also known). Similarly, the results from Theorem 2 and Theorem 3 are already known, see [2] and the generalization in [3].\n\nThe paper also ignores the existence of collective certificates (see e.g. [4] and follow up work). Since for such collective certificates the predictions of all nodes are simultaneously certified this automatically implies a certificate on any function of those predictions and in particular any fairness metric. Therefore, a comparison with collective certificates is warranted. \n\nAs a minor point: While the authors are the first to consider certficates w.r.t. both features and structure when the features are continious, joint certificates for discrete features have already been studuied in [3]. Moreover, the certificate in [3] is provably tight, while there is no such proof for the joint certificate presented in this paper. In addition, the presented joint certificate depends on the order: whether the features or the structure is certified first. Since the examined datatset can easily be modeled with discrete features (or at least approximated with a large number of categories) a reasonable baseline would be to use the joint certificate from [3] -- now applied to certify the function $g$ rather than the underlying classifier.\n\nAs a minor point: The authors claim that there is a high computational cost of calculating $\\epsilon_A$. While the cost is higher compared to Gaussian smoothing it is definitely not prohibitevely high since as shown by [3] the certificate can be computed in linear time w.r.t. the certified radius, and thus the maximum certified redius can be also efficiently computed.\n\nIn addition to the above I have strong doubts in the correctness of Proposition 1 and the discussion in the preceeding \"Obtaining Fair Classification Result\" paragraph. Namely, the only thing that the randomized smoothing certificate asserts is that the output of the smoothed $g$ is the same for an observed input and any perturbation withing the certified radius. However, it does not imply that any particular $n$ dimensional output (for each of the $n$ test nodes) is certified. This is the same reason why we cannot simply certify a function $h = I$(accuracy of all nodes > threshold) and we need dedicated collective certificates as in [3] which certifies the collective prediction of all nodes. I'm happy to be proven wrong and reconsider if the authors provide a more in depth explanation. Note, that it is also not valid to consider the average vector of predictions. This also highlihts another big weakness -- certifying g is informative but any particular vector of predictions (for each test node) is not cerified, so in practice it's not clear which prediction the model should return.\n\nMoreover, it seems that the guarantee in Proposition 1 only applies to perturbations w.r.t. the features since the authors take the best (smallest) value over all structure perturbation. If this is the case this should be clearly mentioned, if this is not the case it is not clear at all why the certificate holds when selecting the argmin as proposed.\n\nAnother weakness is the evaluation. While the 3 datasets (German Credit, Recidivism, Credit Defaulter) are often used in the fairness literature they are not ideal since the graph structure is not given but derived from the features (see NIFTY (Agarwal et al., 2021)). This means that there is a high correlation between the features and the structure and the redunancy in information leads to overly optimistic results. The two Pokec datasets which are also often used in the fairness literature would be more suitable.\n\nSince every entry is flipped with the same probability $1-\\beta$ even for very small values of $1-\\beta$ we will introduced many new ones in the adjacency matrix destroying the sparsity which makes scaling to larger graphs more difficult.\n\nFinally, I think the motivation behind the Fairness Certification Rate (FCR) metric is questionable. A more informative metric would be e.g. the largest certified threshold $\\eta$ for different bugets.\n\nReferences:\n1. Cohen et al. \"Certified adversarial robustness via randomized smoothing\"\n2. Lee et al. \"Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers\"\n3. Bojchevski et al. \"Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more\""
            },
            "questions": {
                "value": "1. Assuming that only the features are perturbed which final vector of test predictions is returned and why exactly is this vector certified?\n2. Does Proposition 1 apply to both feature and structure perturbations? \n3. How exactly are Theorems 1, 2 and 3 different from existing results (See weaknesses)? \n4. How does the approach compare to collective certificate (w.r.t. only structure perturbations for example)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3771/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3771/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3771/Reviewer_wEZx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863375234,
        "cdate": 1698863375234,
        "tmdate": 1700742060446,
        "mdate": 1700742060446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7GwNhfTKec",
        "forum": "I5AjtSen6L",
        "replyto": "I5AjtSen6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_NACu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_NACu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a fairness defense framework for certain attacks (adding Gaussian noise to certain nodal attributes and adding/deleting edges in adjacency), ELEGANT, which guarantees node label predictions that achieve a certain level of fairness with high probabilities under certain perturbation budgets. The perturbation budget on the adjacency for a given fairness level is computed by formulating an optimization problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Paper is fluent.\n- Certification of fairness is an important and novel problem, which is considered by the paper.\n- This work systematically builds an optimization framework and solves it to find the corruption budget on the adjacency matrix for a certain level of fairness."
            },
            "weaknesses": {
                "value": "- I could not follow how the proposed scheme can be used to certify fairness metrics requiring the ground-truth label information (e.g., equal opportunity) over the test set. The paper claims that equal opportunity is a metric for which they can provide verification.\n- The bias mitigation part of ELEGANT stems from the data augmentations (applied as attacks), as the Authors also mentioned in their paper. By applying multiple augmentations during the Monte Carlo process, they just blindly find the ones that help with algorithmic bias (which is costly). There are already existing works searching over such augmentations for bias mitigation [1, 2, 3], which utilize theoretical findings and systematic designs to automatize augmentation designs. Thus, the bias mitigation part of this work is not of sufficient contribution (Figure 2 would be fairer, if fairness-aware methods' results are obtained over the same set of corrupted graphs used for ELEGANT).\n- While the initial research problem is an interesting and important one, to the best of my understanding, this paper applies well-known corruptions to the input graphs multiple times and utilizes Monte Carlo to estimate if the predictions achieve a certain level of fairness. The proposed approach is computationally costly, and lacks an inventive approach. \n\n[1] Ling, Hongyi, et al. \"Learning fair graph representations via automated data augmentations.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Kose, O. Deniz, and Yanning Shen. \"Demystifying and Mitigating Bias for Node Representation Learning.\" IEEE Transactions on Neural Networks and Learning Systems (2023).\n\n[3] Dong, Yushun, et al. \"Edits: Modeling and mitigating data bias for graph neural networks.\" Proceedings of the ACM Web Conference 2022. 2022."
            },
            "questions": {
                "value": "- How can one utilize ELEGANT for fairness certification for a fairness metric requiring the ground-truth labels (like equal opportunity)?\n- Can you additionally provide the results for fairness-aware baselines over the same corrupted graphs as ELEGANT uses for the results in Figure 2? Accordingly, I suggest re-framing the discussion about bias mitigation in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3771/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3771/Reviewer_NACu",
                    "ICLR.cc/2024/Conference/Submission3771/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698881938405,
        "cdate": 1698881938405,
        "tmdate": 1700768909022,
        "mdate": 1700768909022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VYmNf02lgl",
        "forum": "I5AjtSen6L",
        "replyto": "I5AjtSen6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_dksM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_dksM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel framework for certifying the fairness of graph neural networks (GNNs), which are models that can learn from graph-structured data. The framework, called ELEGANT, aims to protect the GNNs from malicious attacks that can corrupt the fairness of their predictions by adding perturbations to the input graph data. The framework uses a theoretical analysis to certify that the GNNs are robust to certain perturbation budgets, such that the attackers cannot degrade the fairness level within those budgets. The framework can be applied to any existing GNN model without re-training or making any assumptions. The paper evaluates the framework on real-world datasets and shows that it achieves effective and efficient certification, as well as debiasing benefits."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper addresses a novel and important problem of certifying the fairness of GNNs, which can enhance reliability while maintaining the fairness of GNNs in various applications.\n* The authors introduce a novel and flexible framework that can certify the robustness of any GNN model to perturbation attacks by using a principled analysis and a plug-and-play design. The framework does not rely on any assumptions about the GNN structure or parameters and does not require re-training the GNNs.\n* Extensive experiments and analysis are conducted to demonstrate the advantages of the proposed framework in defending fairness-aware attacks."
            },
            "weaknesses": {
                "value": "* The problem setting as well as the assumptions need further clarification in my opinion. 1) Why the authors choose to certify a classifier on top of an optimized GNN is unclear, given the existing works on robustness certification of GNNs on regular attacks mainly choose to directly target GNNs. 2) The difference between the attacking performance of GNNs and the fairness of GNNs should be clarified, especially how this difference affects the assumptions and theoretical results. It seems like the certification approach could also be applied without considering the binary sensitive attribute. 3) How the main theoretical findings differ from existing works on robustness certification of GNNs on regular attacks could be explicitly discussed for ease of understanding.\n\n* Some recent works tackling graph robustness are not covered in the related works, especially spectral-based methods such as [1-4]. \n \n\n* For experiments, do there exist other defense methods that also defend the fairness-aware attack on graphs that could be included as baselines?\n\n* Minor: in Theorem 4, should \u201cCertified Defense Budget for Attribute Perturbations\u201d be \u201cCertified Defense Budget for Structure Perturbations\u201d?\n\n[1] Adversarial Attacks on Node Embeddings via Graph Poisoning, ICML 2019\n\n[2] A Restricted Black-box Adversarial Framework Towards Attacking Graph Embedding Models, AAAI 2020\n\n[3] Not All Low-Pass Filters are Robust in Graph Convolutional Networks, NeurIPS 2021\n\n[4] Robust Graph Representation Learning for Local Corruption Recovery, WWW 2023"
            },
            "questions": {
                "value": "Please kindly refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699002363572,
        "cdate": 1699002363572,
        "tmdate": 1699636333380,
        "mdate": 1699636333380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WtzagoDcjo",
        "forum": "I5AjtSen6L",
        "replyto": "I5AjtSen6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_zsXF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3771/Reviewer_zsXF"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to investigate the certified defense in group fairness based on randomized smoothing. Specifically, the method of analyzing certified robustness with randomized smoothing is transferred to the analysis on fairness. In their analysis the problem is reformulated for the fairness problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The attempt to build certified fairness defense is interesting. There is an interesting direction to further investigate.\n2. The presentation is quite clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The technical contribution of this paper is a bit weak in that they mostly followed [1]. The main contributions merely lie in reframing the problems into the robustness on fairness.\n2. There is a major concern about how the proposed method can improve the fairness of graph neural networks. In particular, according to the theorem 1, the smoothed version of the classifier can only guarantee the discrimination wouldn't change a lot after any perturbations. However, if the given GNN is biased, how can this method improve the fairness? Can you clarify how the proposed method can improve the fairness of models?\n3. More analysis on the certified robustness in fairness should be given.\n\n[1] Cohen, Jeremy, Elan Rosenfeld, and Zico Kolter. \"Certified adversarial robustness via randomized smoothing.\" international conference on machine learning. PMLR, 2019."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699298787017,
        "cdate": 1699298787017,
        "tmdate": 1699636333260,
        "mdate": 1699636333260,
        "license": "CC BY 4.0",
        "version": 2
    }
]