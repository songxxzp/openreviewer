[
    {
        "id": "QsQnEWYLE8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_eKQs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_eKQs"
        ],
        "forum": "HiClR4rwJf",
        "replyto": "HiClR4rwJf",
        "content": {
            "summary": {
                "value": "The paper introduces value-factorization for settings with temporally-extended actions (macro-actions). The Individual-Global Maximum (IGM) property is first adapted to such setting, and then different algorithms (based on those already existing in literature for the primitive-action case) are derived. Experimental results show the improvements of value-factorization is such setting over existing decentralized solutions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The investigated setting is interesting: indeed, macro-actions are a more general and accurate way to represent real-world multi-agent problems, and can thus lead to a wider application of these. With value-factorization being one of the most prominent technique investigated in MARL recently, it seems natural to derive such an extension. Also, the way in which it is derived, as well and the whole discussion about the use of macro-states and the updates only on terminated macros is logical and sound. Most of the experimental results are clear and really show the benefits of value-factorization methods over non-factored ones."
            },
            "weaknesses": {
                "value": "While the paper is generally clear in its notation and easy to read, some passages appear to be less readable and a bit more messy. Also, some of the experimental results, although going in the right direction, are not sound enough to actually credit the proposed theoretical claims. Please see the Questions below for a more detailed explanation."
            },
            "questions": {
                "value": "- The background section is a bit rushed. For example, you mention the Q-function in Section 2.1, but you are never actually defining it formally or even explaining what that is intuitively. Although probably every reader of this paper will already know this, the point of the Background section is exactly to provide a brief recap of such basic notions, and thus I think it is important to be as accurate and comprehensive as possible here.\n- Why not requiring the values not to be non-positive in your proposed Adv-IGM is a more general condition? The non-positive nature of the advantage function is not a superimposed limitation, but rather an intrinsic property of this function, and as such I do not clearly see how this can be defined as a limitation. Also, it is not entirely clear to me how you can avoid the decomposition in Equation (4) with your proposed formulation. Please argument a bit more in this respect to make your claims clearer.\n- Why not comparing also to Cen-MADDRQN? It would have been interesting to compare the obtained results not only against a decentralized solution (to assess the improvements of the proposed technique w.r.t. that), but also against a centralized solution, to compare how well is your decentralized technique doing against a (more expensive) centralized one, as well as assessing how such a decentralized technique can scale better when problems become more complex.\n- The comparison results of different mixers seems a bit expected: these are indeed what one would expect from the standard literature in the primitive-action setting. Indeed, QMIX is a generalization of VDN (additivity is a sub-case of monotonicity), and QPLEX in a generalization of both (as monotonicity is a sufficient but not necessary condition for IGM). What is the added value of such a comparison here? What is the additional bit of knowledge that the reader can gain from your proposed results?\n- The comparison between using the true state vs. using the macro-state in AVF is interesting, but only reporting results on a single problem is not convincing enough to claim that the true state is not a good conditioning signal. Could you please add similar results on another benchmark to prove that your claim generalizes beyond the specific WTD-S setting?\n- Table 2 is a bit confusing: I appreciate the idea of comparing primitive-action algorithms with their macro-action counterparts, but the proposed results are not actually clear in this sense. You propose results for BP-10 for the primitive-action algorithms, but then you move to BP-30 and WTD-T1 for your AVF-QPLEX (and the proposed macro-action PG method). How can we make a good comparison out of that? Would not it have been better to compare these on the same set of benchmarks?\n- Also, results of the primitive-action algorithms and their macro-action counterparts on BP-10 does not seem to match: as you claimed in Section 3, your proposed methods should reduce to the primitive case when macro-actions have a single-step duration. So what is the difference here? Why are their performances not matching with those shown in Figure 4? If you are comparing these primitive-action algorithms on setting with a temporally-extended actions, that would be an unfair comparison. If not, however, I would expect their performance to be on-par with your AVF versions. Could you explain this a bit more in details?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Reviewer_eKQs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697287996988,
        "cdate": 1697287996988,
        "tmdate": 1699636537802,
        "mdate": 1699636537802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "afvuR4k7xh",
        "forum": "HiClR4rwJf",
        "replyto": "HiClR4rwJf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_mc14"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_mc14"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors delve into the domain of value factorization in asynchronous MARL. They extend the concept of IGM to accommodate macro-action settings and integrate it with pre-existing value factorization techniques. Furthermore, the paper offers insights and strategies on leveraging centralized information. Empirical experiments conducted on simple scenarios underscore the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem of value decomposition for asynchronous MARL is interesting and open.\nThe paper is well organized and easy to read."
            },
            "weaknesses": {
                "value": "The novelty and contributions of this paper appear somewhat limited. \nThe paper introduces the concept of IGM for macro-actions. However, it's worth noting that this introduction may seem less impactful, as existing value factorization algorithms can inherently fulfill this requirement as long as state and action representations are adjusted in accordance with prior research, which is precisely what the paper demonstrates.\nThe experimental scenarios employed in this study are simplistic."
            },
            "questions": {
                "value": "1. The paper emphasizes that defining Mac-IGM and MacAdv-IGM is not trivial. But it seems that the primary distinction lies in considering only terminated macro-actions. Are there other complexities that merit attention?\n\n2. It is apparent that extending existing value factorization methods to macro-action settings would inherently adhere to Mac-IGM. What is the significance of introducing Mac-IGM, and does it offer advantages in the development of algorithms, especially in cases where a direct extension might violate Mac-IGM? Further clarification on this aspect would enhance the paper's contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698221203712,
        "cdate": 1698221203712,
        "tmdate": 1699636537707,
        "mdate": 1699636537707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "USfuHDaxvQ",
        "forum": "HiClR4rwJf",
        "replyto": "HiClR4rwJf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_JQAT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_JQAT"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a value decomposition method for asynchronous multi-agent reinforcement learning (MARL). The authors define conditions called Mac-IGM and MacAdv-IGM, extending the well-known IGM condition to the asynchronous MARL setting. The proposed method, named AVF, combines local value functions with temporally consistent macro-states to alleviate noise from others' macro-action terminations. AVF can be integrated with various value decomposition methods, such as VDN, QMIX, and QPLEX. AVF combined with these algorithms, outperforms other asynchronous MARL algorithms that do not utilize value decomposition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written with good readability and comprehensibility.\n2. This paper introduces the conditions for value decomposition in an asynchronous MARL setting for the first time.\n3. This paper presents a simple yet effective method to enhance performance and stability, which can be easily applied to other value decomposition methods with minimal modifications."
            },
            "weaknesses": {
                "value": "1. It appears that Mac-IGM and MacAdv-IGM are natural extensions of IGM, and the following propositions hold in a straightforward manner.\n2. Additionally, while temporal consistency is essential for successful and stable learning, the method of utilizing temporally consistent states may lack novelty."
            },
            "questions": {
                "value": "1. [About Weakness 2] Have you experimented with other methods that utilize temporally consistent states? If so, could you provide the results and explain why these approaches have an advantage over others?\n2. Could you please provide more results of Mac-IAICC on other environments that have already been considered? It would be greatly appreciated if you could include learning curves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Reviewer_JQAT",
                    "ICLR.cc/2024/Conference/Submission5344/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766263976,
        "cdate": 1698766263976,
        "tmdate": 1700576842378,
        "mdate": 1700576842378,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aZXLgrdzQ6",
        "forum": "HiClR4rwJf",
        "replyto": "HiClR4rwJf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_E8n9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5344/Reviewer_E8n9"
        ],
        "content": {
            "summary": {
                "value": "Authors introduce value factorization methods which were designed for synchronous framework to the asynchronous framework. Firstly, they formalize the new IGM principles for the asynchronous framework, proving they generalize the primitive cases. Secondly, they propose AVF algorithms which extend previous factorization methods to use macro-actions. Finally, authors verifies the performance of AVF algorithms through experiments in standard benchmark environments in macro-action literature, and explores the influence of using different extra information in the mixer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, the content of the paper is well structured and organized.\n\n1. Authors extend current popular principles to macro-action based settings: IGM and Advantage-based IGM, and provide extensive theoretical analysis of the proposed principles.\n\n2. Authors provide empirical results in different environments.\n\n3. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. Dec-MADDRQN is a fully-decentralized value-based method, I don't think its performance is good enough to act as a baseline to compare with CTDE-based algorithm, perhaps you can compare with Mac-IAICC in more environments and show the results.\n\n2. The paper lacks the pseudo-code of the training process of AVF algorithms.\n\n3. For fairness, authors compare AVF algorithms with primitive value factorization methods in the primitive action version of BP-10, but they didn\u2019t provide any explanations about what changes have been made in the primitive action version of BP-10 from the original version.\n\n4. Regarding the use of extra information in the mixer, the authors list three different mechanisms, but the second mechanism that using the macro-state of the agent whose macro-action has terminated is missing in the experiment."
            },
            "questions": {
                "value": "**Major questions**\n\n1. As shown in Figure 2, if the macro-action of agent at the current timestep is not terminated, its macro-observation will not change. But I think the reality is that a change in macro-action of any other agent will lead to a change in the macro-state, and ultimately lead to a change in the macro-observation of agent i. Can you explain your ideas about this?\n\n2. In the field of macro-action based MARL, are there other CTDE value-based algorithms?\n\n3. The authors mention low-level policy in the Preliminaries, but they do not mention how to select actions for low-level policy in AVF algorithms. In my opinion, if low-level policies do not work during the duration of a macro-action, then the macro-action can be simulated by executing a specific micro-action continuously over a period of time. Is it right?\n\n4. Different agents\u2019 macro-actions start and end at different time steps, how to choose the duration time $\\tau$ of the joint macro-action?\n\n5. As far as I know, in the value factorization algorithms, in addition to the IGM and Adv-IGM principles, [1] proposes DIGM principle and [2] proposes RIGM principle. Can you discuss whether and how these two principles can be applied to the asynchronous framework?\n\n**Minor questions**\n\n1. Formula (5) is missing $,$ between $m^{-}$ and $ \\hat{o}^{\\prime}$.\n\n2. In the tuple of the Dec-POMDPs, the last element in the tuple should be $\\gamma$, not $z$.\n\n3. What is the measure compared in Table 1? Average win rate or average return?\n\n**References**\n\n[1] Wei-Fang Sun, Cheng-Kuang Lee, and Chun-Yi Lee. DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning. ICML 2021.\n\n[2] Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei, Xinwang Liu, and Cheng Wang. RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5344/Reviewer_E8n9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825619614,
        "cdate": 1698825619614,
        "tmdate": 1699636537451,
        "mdate": 1699636537451,
        "license": "CC BY 4.0",
        "version": 2
    }
]