[
    {
        "id": "JyD2HSVZpb",
        "forum": "s2HF7VCU8P",
        "replyto": "s2HF7VCU8P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission881/Reviewer_xLb4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission881/Reviewer_xLb4"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework to predict text-embedding based on the generated image from a Stable Diffusion. More precisely, it involves multiple components: a prompt-embedding and word classification training objective, a curriculum learning schedule, and a domain-adaptive kernel learning method to boost the performance. The paper also shows that the learned prompt-embeddings along with the images can be fed back for generative model training, which improves the model's text-image alignment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper aims for an interesting task of predicting prompt embedding from Stable Diffusion generated images, and proposes multiple novel components to achieve the task.\n2. The generated prompt-embeddings can later be paired with the images to fine-tune the generative model, which enhances the text-image alignment of the model."
            },
            "weaknesses": {
                "value": "1. I am generally questionable about the portion of domain-adaptive kernel learning:\n    - Is it really applicable? From the setup, it seems to require  knowledge about the test domain. However, in real world task, the test domain could be unknown or infinitely large due to the generative power of a diffusion model. How should we define the test domain for this learning task?\n    - It requires a hyper-parameter $r$ to define the number of k-means centroids. However, this $r$ is not studied and how should we choose it?\n\n2. The paper has some typos. For example, on the top paragraph of Page 6, there's a \"Figure ??\" typo."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726196941,
        "cdate": 1698726196941,
        "tmdate": 1699636014756,
        "mdate": 1699636014756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MlWjJlNTme",
        "forum": "s2HF7VCU8P",
        "replyto": "s2HF7VCU8P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission881/Reviewer_zz1f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission881/Reviewer_zz1f"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework to predict the original text-prompt embedding given the generated images by a text-conditioned diffusion model, in purpose of tuning the diffusion model toward better semantic alignment. The proposed framework employs an ensemble of four pre-trained visual-encoder backbones and trains a head on top of them to predict the original text-prompt embedding of BERT. One of the visual-encoder backbone is the U-Net of the pre-trained diffusion model, and as a by-product of the fine-tuning for the prompt-embedding prediction, it is claimed that the updated U-Net is better at generating the image with semantic alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The considered topic of fine-tuning the text-to-image diffusion model toward better semantic alignment is important and of great interest. The proposed method seems to be simple."
            },
            "weaknesses": {
                "value": "- The main objective is unclear.\n    - A large body of the manuscript and the title itself focus on predicting the original text embedding. However, as long as the actual caption is not generated but only the text embedding is predicted, it is unclear what the point is. As for evaluating the text-alignment, most of work already report CLIP similarity score, and the motive for making a new prediction model for text embedding seems to be weak. As for improving the U-net backbone via fine-tuning, one could also consult CLIP similarity loss first.\n- The proposed method seems to be rather hand-wavy.\n    - Why it has to be four encoder backbones, and why this specific combination (U-Net, ViT, Swin-T, CLIP) is needed? If this is solely for the performance, proper ablation study (using a subset of the backbones) is needed.\n    - Why is it designed to predict the BERT embedding? CLIP-text encoder seems more welcoming, considering CLIP-image encoder is already used as a backbone, and another backbone, the diffusion U-Net, is highly related to it.\n    - Does making a better prediction via extra techniques (Multi-label classification, Curriculum learning, DAKL; Table 1) necessarily lead to a better performance in image generation with the diffusion model? Although Fig. 4 shows the overall improvement, it is unclear if this is closely correlated to cosine-similarity score in Table 1.\n- Comparison with other work is missing.\n    - There are some recent studies proposing other fine-tuning methods for diffusion models (e.g., [1], [2]). The paper lacks comparison with these approaches.\n        - [1] Aligning Text-to-Image Models using Human Feedback, Lee et al. 2023\n        - [2] Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack, Dai et al., 2023\n    - While predicting the text-embedding proposed in this paper can be new, it should be compared with the captioning models as well. For example, by captioning the generated images with BLIP-2 and embedding the generated caption with BERT, the performance can be compared. \n    ****\n- Minor:\n    - \u201cFigure ??\u201d on Page 6, the number is missing.\n    - It is not entirely sure, but the Latex style feels a little off from the standard. The authors might want to double-check this.\n\n---\n\nThe authors' rebuttal partly resolved my concerns and I updated the scores."
            },
            "questions": {
                "value": "- In vocabulary classification, it seems the training would mislead the model if there are synonyms or paraphrase. How is this handled?\n- If the text-embedding cosine-similarity is evaluated with the newly generated images by the updated U-Net, do the scores get better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission881/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission881/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission881/Reviewer_zz1f"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698867722902,
        "cdate": 1698867722902,
        "tmdate": 1700639971176,
        "mdate": 1700639971176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VxCBOa8bAY",
        "forum": "s2HF7VCU8P",
        "replyto": "s2HF7VCU8P",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission881/Reviewer_2mcu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission881/Reviewer_2mcu"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to enhance the understanding of stable diffusion by leveraging predictions on prompt embeddings, while simultaneously improving the capability of stable diffusion. The paper's completeness is fair, and the experiments are comprehensive. However, the motivation behind the paper is not clear, the method lacks novelty, and the overall contribution is limited."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is reasonably well-rounded in terms of completeness, the experiments are generally sufficient, and the figures and charts are quite clear."
            },
            "weaknesses": {
                "value": "The motivation in this paper is very weak, making it difficult to identify significant innovation or contributions. The motivation as I understand it in the paper is to enhance the understanding of the diffusion model through embeddings. However, the method section only discusses how to obtain embeddings, without addressing how this enhances the understanding of the diffusion model. If the motivation is just predict the text embeddings, Image Caption + CLIP can already be achieved. Furthermore, I see significant contradictions in its methodology. In the paper, it is mentioned that the model itself has semantic issues. However, it suggests that training a model using these problematic images generated by the model can actually improve the issue, which, in my opinion, is contradictory."
            },
            "questions": {
                "value": "Question:\n\n1. It seems that your method might not fulfill your motivation. Your motivation aims to enhance the understanding of the diffusion model, but your method primarily focuses on predicting prompt embeddings. How does this directly contribute to a better understanding of the diffusion model?\n\n2. Regarding the method, you later mention using the model you trained to participate in generating and claim that this can address some issues in the original model's prompt understanding, such as object omissions. However, the data used for training your model consists of images with issues generated by the original model. How does a model trained on problematic data contribute to resolving the issues in the original model?\n\n3. In terms of novelty, what are the distinct advantages of computing embeddings using your method compared to directly using an image caption model to predict prompts and then calculating embeddings using an image encoder?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission881/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698923787467,
        "cdate": 1698923787467,
        "tmdate": 1699636014590,
        "mdate": 1699636014590,
        "license": "CC BY 4.0",
        "version": 2
    }
]