[
    {
        "id": "0ucjtbuUGG",
        "forum": "vM1xZ9kDUj",
        "replyto": "vM1xZ9kDUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_xrjY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_xrjY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the C3 approach, or \"Capture Concept through Comparison\", which is a multi-modal pretraining framework focusing on aligning abstract concepts between the visual and language modalities. \n\nSpecifically, the framework consists of two main components: a concept mining procedure, and a set of training objectives. During concept mining, concepts are extracted as n-grams from text annotations of images. This results in an augmented image-text dataset where each image is accompanied by not only a text description, but a set of \"concepts\" (n-grams). The augmented dataset is used to pretrain the model using two objectives, namely the Matched Concept Prediction (MCP) loss and the Matched Concept Contrastive (MCC) loss. The MCP loss aims at predicting whether a concept C is shared between two images. The MCC loss encourages the concept-conditioned visual features of two images to be close, should they share the same concept.\n\nThe proposed C3 framework is assessed on four image-caption datasets, under the continual pretraining and pretraining-from-scratch configurations, with various widely used pair-centric objectives. Evaluation using the VL-Checklist benchmark shows that model trained with C3 achieves improved performance. An ablation study and a visualisation are provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed C3 framework is conceptually correct, in the way that a concept can be better emphasised by comparing images sharing that concept.\n- The extensive experimental results clearly showed its efficacy under different configurations for visual-language pretraining.\u00a0\n- The appendix provides an interesting study on the abstraction degree of concepts, using the \"dispersion degree\" and the \"salient density\" metrics."
            },
            "weaknesses": {
                "value": "- The intuition of this paper, which is existing methods only focus on tangible concepts and are therefore less effective when processing abstract concepts like \"side by side\", is not well reflected by the proposed method. Mining n-grams does make it possible to cluster images with a certain concept, however, how does this reinforce learning of abstract concepts?\n- The paper lacks details on how n-grams are collected and processed. For example, it is apparent that not all n-grams are meaningful. How does the framework select useful n-grams?\n- The paper claims that C3 achieves better results than baseline models with fewer iterations. While the count of iterations has been reduced, the training using C3 involves an augmented triplet dataset, where the n-grams serve as extra signals for comparison between images. Therefore, C3's computation burden per iteration could be much higher than the compared methods. It is therefore preferable to compare the wall time or FLOPS per epoch if the paper is trying to highlight the efficiency.\n- The visualisation provided in Figure 4 is not convincing enough. For example, Figure 4(c) shows \"looking at a bird\", why is the bird not highlighted? Figure 4(g) is about \"ready to hit a ball\", but why are hands the only highlighted region while this preparation posture obviously involves legs? The interpretation of concepts can be highly subjective and therefore I suggest the paper provides some quantitative studies about this. For example, it can provide the classification of model on a subset of abstract concepts trained with and without C3."
            },
            "questions": {
                "value": "Please see the weakness points above. As a summary here, I expect some answers for these questions:\n1. How does the n-gram based learning reinforce modelling of abstract concepts?\n2. How are n-grams selected? Can the authors provide more statistics about the collected n-grams?\n3. How much time does C3 take to train a model? Alternatively, how much computation (measured by, for example, FLOPS) is needed? Is this still less than the baseline models?\n4. Can the authors provide quantitative evidence that modelling of abstract concepts is indeed improved by C3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Reviewer_xrjY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698577261533,
        "cdate": 1698577261533,
        "tmdate": 1699636330142,
        "mdate": 1699636330142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yuJ7o9fvf8",
        "forum": "vM1xZ9kDUj",
        "replyto": "vM1xZ9kDUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_J1vN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_J1vN"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a concept-centric pre-training method for improving the text-image alignment properties of vision-language models. It introduces a data mining formulation using n-grams to find image-text pairs containing text with similar concepts and uses this to formulate loss functions for image-text alignment. The framework is primarily evaluated on the VL-Checklist, where it shows quantitative gains over the baselines, and is also evaluated on 2 other datasets where it performs competitively with other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper introduces concept-based pre-training, which is an interesting way of improving the alignment of VLMs.\n+ The idea is simple, intuitive, and easy to follow.\n+ The evaluation of the image-text alignment is performed on the VL-Checklist, which is a standardized benchmark for the task.\n+ The quantitative gains over considered baselines is clear on the VL-Checklist, which shows the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "- The main contribution is the concept mining framework and the resulting MCP/MCC loss functions. While the ablation study focuses on the effect of not using the loss functions, there is no evaluation of the actual triplets mined. Some qualitative examples are given in the supplementary, but there is no evaluation of the effect of this mining framework. For example, K1 and K2 are set to 5 and 80 (page 6, paragraph 5)). Why these numbers? What is their significance? How were they selected? \n- While the experimental results show improvement, the considered tasks are still somewhat related, i.e., falling within the same umbrella as text-image matching. Does the pre-training/finetuning strategy improve the performance on downstream tasks such as VQA, zero-shot classification, etc.? It would be good to show additional gains on tasks other than the ITM task.\n- The gains over METER, the main source of comparison, seem to be limited on tasks other than the ones in the VL-Checklist. Is there any reason why? Not much qualitative discussion is present, so it is hard to understand why the gains do not transfer to other tasks.\n- Why n-grams and not semantic similarity between captions/sentences/noun phrases? It is an ablation study that can show the impact of choices in the framework, considering the concept mining paradigm is the core contribution.\n- Algorithm 1 seems incomplete. What is C_k? I assume it is an empty set like I_k. Need to show the role of K1 and K2 in the algorithm. K1 and K2 are mentioned in passing in the text without detail. This would be a good place to introduce it. \n- Minor: writing could be improved by denoting what the abbreviations are when they are introduced. For example, the term VLP is thrown in Section 3 without any prior references to this. Given the many acronyms in the vision/ML community, expanding it would help readability. For the record, I think it refers to vision-language pre-training, but I am not entirely sure."
            },
            "questions": {
                "value": "My major concerns are based on the ablation studies and the choice of evaluation benchmarks beyond image-text matching. There is no evidence of generalization beyond the ITM tasks that somewhat limits its contributions. Please see the weaknesses section for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686646957,
        "cdate": 1698686646957,
        "tmdate": 1699636330069,
        "mdate": 1699636330069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YfjOj2umGb",
        "forum": "vM1xZ9kDUj",
        "replyto": "vM1xZ9kDUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_snHQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_snHQ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents 3 main contributions -- (a) a data mining procedure to create <image, image, text> triplets such that the two images are conceptually related to the text, (b) concept centric learning objectives created for the training dataset containing the mined triplets, (c) empirical results to show the benefits of (a) and (b).\n\nThe empirical results are shown under the settings of pretraining and parameter-efficient finetuning using LoRA. Additionally, ablation studies are also included to study various aspects of the learning objectives."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-written and organized.\n\n* The data mining method is graceful and can be scaled to other domains / large-scale datasets.\n\n* The concept centric pretraining objectives -- MCP and MCC -- to maximize the mutual information between the training triplets are intuitive and easy to implement.\n\n* The results in Table 1 are impressive -- especially continual pre-training results on COCO and pretraining from scratch."
            },
            "weaknesses": {
                "value": "* It is unclear why the authors choose to train C3 in Table 2 for half the number of iterations? Are the other hparams (learning rate, batch size) the same between METER and C3? Is the number of iters halved to account for using two images (due to <image, image, text>) in a single example in C3 compared to one image in METER?\n\n* Also, the improvements in Table 2 for C3 compared to METER seem very small, especially so on test splits of SNLI-VE and NLVR.\n\n* The ablation study in Table 3 does not show very clear patterns, e.g., (a) row 2 for Flickr30k-ZS seems comparable to row 4, (b) all the rows in SNLI-VE seem very close\n\n* Unclear what to make of the visualizations in 4.6 as there is a risk of cherry-picking. Is there a better way to quantify the claim \"C3 exhibits the ability to focus on specific regions in accordance with the text fragments\"?"
            },
            "questions": {
                "value": "* Is it possible to show an ablation study where <image, image, text> triplets are constructed using an alternate data mining strategy. E.g., instead of looking for n-gram match, create triplets {<image, image, text_1>, <image, image, text_2>} where (text_1, text_2) are semantically similar (but do not necessarily have n-gram overlap)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731751155,
        "cdate": 1698731751155,
        "tmdate": 1699636329990,
        "mdate": 1699636329990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IOz4eu3kK9",
        "forum": "vM1xZ9kDUj",
        "replyto": "vM1xZ9kDUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_pYtZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3739/Reviewer_pYtZ"
        ],
        "content": {
            "summary": {
                "value": "Aligning the semantics of images and text is a challenging task. The conventional approaches have tried to improve alignment by adding extra information (tags, bounding boxes) as linkages between the two modalities. However, the existing methods mainly focus on aligning concrete objects and ignore other crucial abstract concepts that are hard to see (e.g., \"side by side\" and \"upside down\").\n\nTo address this limitation, the paper proposes a new method called \"Capture various Concepts through data Comparison (C3)\" for learning cross-modal representations. C3 incorporates a novel n-gram-based mining procedure to discover the concepts intrinsic to the database. It also frames the model inputs as triplets (image-image-concept) to capture abstract semantics in images better.\n\nBased on this setup, the paper proposes two concept-centric pre-training objectives, Matched Concept Prediction (MCP) and Matched Concept Contrastive (MCC), to learn concept-level alignment better. Extensive experiments show that models trained with C3 consistently improve performance on various comprehension and reasoning benchmarks, whether starting from scratch or fine-tuning from an existing model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to understand, particularly with the help of Figure 2 and Algorithm 1, which clearly elucidate the core of the proposed mining strategy. The benefits of the proposed method are clearly explained and demonstrated through a variety of rigorous experiments and evaluations. The experimental results show consistent performance improvement. It reveals that the proposed mining strategy is simple but effective, and it has the potential to be leveraged in other studies well."
            },
            "weaknesses": {
                "value": "1. The paper has omitted a significant benchmark from the evaluation. VQAv2 is a widely used and crucial benchmark in VLM research, and its performance should be evaluated.\n\n2. With the overload of VLM studies, it is impractical to compare and validate all of them. However, the performance of some well-known methodologies may be added to the results section. BEIT3, for example, is a known high-performing model with a parameter size of about 1.9B, making it a suitable comparison.\n\n3. While the volume of data and the number of iterations in the training process are essential, so too is the parameter size of the model. Adding the parameter size to Table 2 would facilitate the evaluation of the proposed method. C3, for instance, has a parameter size of 400M, which is relatively lightweight. It could be another advantage of the proposed method."
            },
            "questions": {
                "value": "1. It is difficult to find the ablation result with or without the MCC objective. Is there any difficulty in the ablation process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3739/Reviewer_pYtZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770045462,
        "cdate": 1698770045462,
        "tmdate": 1699636329810,
        "mdate": 1699636329810,
        "license": "CC BY 4.0",
        "version": 2
    }
]