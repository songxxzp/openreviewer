[
    {
        "id": "RbpwVJ5FMO",
        "forum": "02f3mUtqnM",
        "replyto": "02f3mUtqnM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_ZnPu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_ZnPu"
        ],
        "content": {
            "summary": {
                "value": "Deployment of large language models is costly whereas smaller models can be deployed on edge devices but tend to lag behind in response quality. This work proposes a hybrid inference approach to save cost and maintain response quality. A query router is employed for assigning queries to large or small language model depending upon the predicted query difficulty and the desired quality level. This desired quality level is dynamically tunable at test time for trading quality for cost. The proposed design achieves 40% fewer calls to large model with no drop in response quality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper addresses and interesting problem considering that currently available smaller language models can fairly perform well. Depending upon the predicted difficulty level of the query, it is interesting to use a router to pass the relatively easier queries to smaller model. This approach can be cost effective.\n\nMultiple router score designs are proposed.\n\nThere is thorough empirical analysis with good discussion."
            },
            "weaknesses": {
                "value": "The proposed design requires that for each LLM pair, a router is required to be trained which might be a costly undertaking in a production environment. \n\nThis paper discusses the cost/quality analysis in context of a language model pair. In real world scenarios, there might be multiple LLMs available and several competing factors to be optimized or traded-off. \n\nFigure 1 is not properly aligned and some inconsistent border is visible (Fig 1 (C))."
            },
            "questions": {
                "value": "This Paper states that \u201cwe expect that using the router to route queries to the small model will not detract significantly from the realizable cost advantage.\u201d Is this an assumption or empirically verified conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698587887532,
        "cdate": 1698587887532,
        "tmdate": 1699636874627,
        "mdate": 1699636874627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MzQ8XSRmUG",
        "forum": "02f3mUtqnM",
        "replyto": "02f3mUtqnM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel inference paradigm called hybrid inference, which utilizes two models of different sizes to handle queries. This approach aims to balance infference cost and response quality by routing easy queries to a smaller model while directing more complex queries to a larger model. The authors propose an orchestration framework that involves a router trained on a dataset of representative queries. The router dynamically routes queries to the appropriate model, thus reducing overall costs while maintaining response quality. They present three variations of the router: a deterministic router, a probabilistic router, and a probabilistic router with data transformation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper sets the problem in the context of LLM inference and focuses on the evaluation of response quality and cost advantage. It defines metrics for measuring the effectiveness of the routing strategy, considering the intrinsic uncertainties in natural language processing tasks. The evaluation is conducted on the MixInstruct dataset, which comprises a diverse range of tasks such as question answering, summarization, and information extraction. The experimntal results demonstrate the efficacy of the proposed routing strategies, especially in scenarios where the performance gap between the small and large models is minimal. The deterministic router achieves good cost advantages with negligible drops in response quality, while the probabilistic router further improves the cost advantage without compromising response quality. The probabilistic router with data transformation exhibits even more promising results, achieving significant cost advantages with no quality drop."
            },
            "weaknesses": {
                "value": "The main limitation of the paper seems to be its reliance on the assumptions about the quality gaps and the routiing mechanisms. These assumptions could potentially affect the overall effectiveness and efficiency of the routing process. Additionally, the reliance on specific models and the need for manual intervention in setting the threshold for routing may limit the scalability and generalizability of the proposed framework."
            },
            "questions": {
                "value": "The approach might encounter problems in accurately distinguishing between easy and hard queries, especially when dealing with a large performance gap between different models. How do you elaborate on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Reviewer_VA7E"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616980620,
        "cdate": 1698616980620,
        "tmdate": 1700841210138,
        "mdate": 1700841210138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U4s76jN4Rg",
        "forum": "02f3mUtqnM",
        "replyto": "02f3mUtqnM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_uyT6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_uyT6"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a hybrid inference strategy designed to minimize the computational expense by limiting the number of queries to the larger model and utilizing smaller models to function as decision-making routers.\nInitially, the approach assesses if the user's input query is easy or hard by evaluating the anticipated response quality from both the small and large models.\nTo evaluate the complexity of a query, the paper describes three distinct methodologies, each utilizing the same classification model but differing in their training and inference schemes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Paper presents a novel hybrid inference strategy designed to minimize the computational expense by limiting the number of queries to the larger model and utilizing smaller models to function as decision-making routers.\nMoreover, paper presents multiple different approaches to training the decision making routers and its effectiveness."
            },
            "weaknesses": {
                "value": "I have following major concerns.\n\n1. **Reliability of BART scores for routing**\nI am uncertain about the efficiency of training the router model to decide whether the BART scores of the smaller model is similar to those of the larger one. BARTScore has demonstrated strong performance in extractive QA; however, its correlation may diminish in abstractive QA contexts [1], suggesting that the metric might not be suitable for assessing open-ended generation tasks. Establishing a correlation between evaluations of routing using BARTScore and human assessments would be beneficial to verify the reliability of BARTScore for routing evaluation purposes.\n\n2. **The Impact of Training Data Versus Model Size**\nI am of the opinion that the size of the model is not as critical as the differences in the training data used for each model in determining quality. For instance, consider evaluating the performance disparities between models like (Llama-2 7B and Llama-2 13B) versus those between (Llama-2-7B and the more recent Zephyr-7B [2]). Would the performance gap trend similar to the reported trend in Figure6?\n\n[1] G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment., Liu et al., 2023 \\\n[2] https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha"
            },
            "questions": {
                "value": "Same as weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7318/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699478040270,
        "cdate": 1699478040270,
        "tmdate": 1699636874389,
        "mdate": 1699636874389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "01SJ6h5IHh",
        "forum": "02f3mUtqnM",
        "replyto": "02f3mUtqnM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a router that assigns queries to differently sized models. Their method results in 40% fewer calls to the large model, with no drop in response quality. They introduce two main techniques to improve performance:\n\n- using soft probabilities instead of hard probabilities\n- using a data transformation with a relaxation $t$ to provide stronger training signal."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Paper is well written, and authors do a good job of building upon concepts used in final technique.\n- Ablations and analysis are extensive and well-thought out, giving researchers ample inspiration to build upon this technique.\n- The analysis of performance on different model size pairs is interesting to me."
            },
            "weaknesses": {
                "value": "Please cite these works:\n- https://arxiv.org/abs/2305.05176 - routing on a query level\n- https://arxiv.org/abs/2211.17192, https://arxiv.org/abs/2302.07863 - latency reduction using small and big models\n\nI believe writing a discussion of the tradeoffs of these approaches would improve the current draft."
            },
            "questions": {
                "value": "- In this method, we are able to reduce cost and latency, but not as much latency reduction as methods such as speculative decoding (https://arxiv.org/abs/2211.17192). While there is added cost with speculative decoding, do you think there's any possibility of closing this gap?\n- Do you think this might be because of scoring query wise vs token-wise? Why not use this method token wise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7318/Reviewer_9pHQ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7318/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699664186882,
        "cdate": 1699664186882,
        "tmdate": 1699664251880,
        "mdate": 1699664251880,
        "license": "CC BY 4.0",
        "version": 2
    }
]