[
    {
        "id": "ctNCr6d6cN",
        "forum": "xnUIMz5u2s",
        "replyto": "xnUIMz5u2s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_6i9F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_6i9F"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a benchmark to test for social reasoning abilities in LLMs. The benchmark they propose is programatically adapted from the existing ToMi benchmark. The novelty of this work over previous work is that the benchmark does not only test for the ability of LLMs to infer unobserved mental states of agents, but tests LLM's abilities to choose an appropriate action for a given scenario based on such an inference. To do this task well, the LLM needs to both do the inference and connect the inference to the appropriate action. The authors show that models can do the ToM inferences well, but struggle on the task of selecting an appropriate action. Humans, by contrast, get near-perfect accuracy. \nWith further analysis, the paper shows that the problem for models is connecting the inference to the right action; when provided with oracle inferences accuracy goes up significantly. Inspired by this, the work proposes a prompting technique called Foresee and Reflect (FaR). FaR is a zero-shot prompting technique that asks models to reason about the future following from a scenario and use that to reflect on who might need help in the scenario. FaR prompting improves performance for all models tested, but most of all for GPT-4. In further experiments, the authors show that FaR prompting generalises to other datasets, and through ablations they show that both the foreseeing aspect as well as the reflecting aspect are crucial for performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written and easy to understand.\n\nThe authors show convincingly that their prompting method is better than baselines; they use a set of baselines and show it works on multiple datasets.\n\nThe analysis by adding information to the prompt to find when models start to perform better on the action selection is insightful.\n\nThe ablations they do throughout the work are helpful for understanding the importance of all parts of the prompting method. \n\nThe results are interesting; LLMs can do ToM inferences but seemingly cannot connect them to appropriate actions without structured prompting or explicit mentioning of the inferences."
            },
            "weaknesses": {
                "value": "The authors mention that the main contribution of this work is evaluating the ability of LLMs to take actions based on ToM inferences, but do not mention a paper that does the same: \"Understanding Social Reasoning in Language Models with Language Models\", Gandhi et al. 2023. This work also tests the ability of LLMs to take appropriate actions in Sally-and-Anne-type scenarios, evaluate humans on it as well, and find similar results (models struggle more with the actions than with the inferences).\n\nThe authors apply no control conditions, like true belief situations, to further investigate whether models are actually doing ToM-reasoning (which is common in research investigating ToM in LLMs).\n\nThe authors compare models zero-shot to humans who are given three examples of the task. Even if the examples have no label, prior work shows that few-shot prompting without labels often works as well as with labels.\n\nNot necessarily a weakness but I'd like to see some discussion; could there be spurious correlations in the examples? E.g. in Figure 2 Owen is mentioned more often than the others and is also the answer.\n\nTo summarise; my main concern with this paper is contribution in light of another work doing the same."
            },
            "questions": {
                "value": "1. Could you discuss your contribution in light of Gandhi et al. 2023?\n2. Why don't you apply control conditions like true belief situations?\n3. Why do you give humans 3 examples and models none?\n4. Did you test for spurious correlations between the observations and the answer?\n\nSome less important style bits that I'm adding here because they are not weaknesses:\n1. The use of italic is confusing and distracting, you sometimes use it for stressing things, other times for spelling out acronyms, other times for no apparent reason\n2. Typo on page 4 s/stroy/story\n3. Personal preference: figure 3 is confusing and makes it hard to compare models because it depends on initial performance (cannot increase a lot if it was already high); would consider making a different type of plot where models are still comparable, e.g. just highlight the improvement part of a bar in a normal absolute performance plot\n4. I don't understand the reason for putting a paragraph about the connection to A*\n5. Figure 5 has a confusing title and I would add more details in the caption about what is going on in this figure"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8449/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8449/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8449/Reviewer_6i9F"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697642044042,
        "cdate": 1697642044042,
        "tmdate": 1700502649241,
        "mdate": 1700502649241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vuIISmhsbh",
        "forum": "xnUIMz5u2s",
        "replyto": "xnUIMz5u2s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_WCD3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_WCD3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new benchmark, Thinking for Doing (T4D), to test theory of mind capabilities in large language models (LLMs). It modifies the existing ToMi benchmark to require models to not just infer mental states but act based on those inferences. Experiments are conducted with GPT-4 and PaLM-2.  To improve model performance on T4D, the authors introduce a new prompting method called Foresee and Reflect (FaR) which guides the model through two steps: predicting future events and reflecting on actions to address potential challenges."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- I found the paper to be very clear, with sections and methods well motivated.\n- The paper collects representative data from human participants to measure performance.\n- The proposed method improves performance on the given task."
            },
            "weaknesses": {
                "value": "The paper\u2019s contributions can be split into two parts: A benchmark for testing theory of mind, and a prompting method to achieve. In terms of research, the paper proposes a new problem and a new method. I talk about their weaknesses separately.\n\n### Weakness: T4D\n\n- The formal reasoning steps and inferences required for T4D are not clearly specified. For a model to succeed at T4D, it likely needs to make inferences about agents' utilities, beliefs, perceptual access and how helping might alter beliefs to increase utilities. However, the paper does not delineate the full reasoning process of inferring and comparing utilities across agents that would lead to selecting the correct action. Clearer formulation of the multi-step inference process needed could better highlight the reasoning challenges in T4D.\n- Theory of mind relates to having a causal model for other agents\u2019 mental states, how they are formed and how they affect actions (that differ from the self). T4D which is based on ToMi suffers from the same limitations as ToMi:\n    - lack of diversity semantic: beliefs are only about locations of objects. Variations in app D. are still very limited.\n    - lack of diversity syntactic: all examples have the same structure in the sentences\n    - load on memory tracking, rather than testing for Theory of Mind:\n    - ToMi does not have clear percepts\n    - It is unsurprising that adding these additional \u201chops\u201c to reasoning over ToMi and planning leads to lower zero-shot performance.\n- Lack of probing methods: the authors only try a single method of probing LLMs; Asking a direct question. The work would improve if alternate probing methods were also tried. For example asking the model to complete a sentence; ie actually do instead of what it thinks it would do.\n- The paper ignores work that relates to using theory of mind for planning, ie, in strategic settings [6, 7, 8]; where thinking is for doing. This includes work that relates to collaboration and competition in strategic multi-agent settings. A discussion on this is needed. A paper from human ToM reasoning [5] could be insightful to include in the discussion.\n- Similarly, a justification of how this benchmark builds over other Theory of mind benchmarks [ones in the paper and 1, 2, 3] is needed.\n- Lack of evaluations: The probing method that they use is restricted to be questions about helping others by providing information.\n- Only test with GPT-4 and Palm-2, both closed source models.\n\n### Weakness in methods\n\n- Not a fair comparison across methods. The prompt for FaR is much longer and is much more task specific. It is relatively unsurprising that such scaffolding helps the model\n- No justification as to why this method is better and not another alternative. For example asking the tomi question first, where does the <agent> believe object is and then asking the model about the helping questions are simpler scaffolds that the authors fail to test\n    - A 1-shot baseline is needed\n    - Methods need to be compared based on # of tokens in the prompt\n- Other prompting methods, that are generic to reasoning need to be tested: Eg: a 0-shot detailed tree of thought prompt, detailed self-critique prompt etc.\n- As a method being contributed, there need to be  a broader set of evaluations across domains.\n- Similar to the results on T4D, there is a lack of results with other language models with FaR.\n\nCitations:\n\n[1] Gandhi, K., Fr\u00e4nken, J. P., Gerstenberg, T., & Goodman, N. D.  (2023). Understanding social reasoning in language models with language  models. *arXiv preprint arXiv:2306.15448*.\n\n[2] Moghaddam, Shima Rahimi, and Christopher J. Honey. \"Boosting Theory-of-Mind Performance in Large Language Models via Prompting.\"\u00a0*arXiv preprint arXiv:2304.11490*\u00a0(2023).\n\n[3] Trott, Sean, et al. \"Do Large Language Models know what humans know?.\" *Cognitive Science* 47.7 (2023): e13309.\n\n[4] Jones, Cameron Robert, Sean Trott, and Ben Bergen. \"EPITOME: Experimental Protocol Inventory for Theory Of Mind Evaluation.\" *First Workshop on Theory of Mind in Communicating Agents*. 2023.\n\n[5] Ho, Mark K., Rebecca Saxe, and Fiery Cushman. \"Planning with theory of mind.\" *Trends in Cognitive Sciences* 26.11 (2022): 959-971.\n\n[6] Meta Fundamental AI Research Diplomacy Team (FAIR)\u2020, et al. \"Human-level\n play in the game of Diplomacy by combining language models with \nstrategic reasoning.\" *Science* 378.6624 (2022): 1067-1074.\n\n[7] Gandhi, Kanishk, Dorsa Sadigh, and Noah D. Goodman. \"Strategic Reasoning with Language Models.\" *arXiv preprint arXiv:2305.19165* (2023).\n\n[8] Dasgupta, Ishita, et al. \"Collaborating with language models for embodied reasoning.\" *arXiv preprint arXiv:2302.00763* (2023).\n\n[9] Hu, Jennifer, and Roger Levy. \"Prompt-based methods may underestimate large language models' linguistic generalizations.\" *arXiv preprint arXiv:2305.13264* (2023)."
            },
            "questions": {
                "value": "Please see the suggestions specified in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738413751,
        "cdate": 1698738413751,
        "tmdate": 1699637053651,
        "mdate": 1699637053651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ahXacLbijB",
        "forum": "xnUIMz5u2s",
        "replyto": "xnUIMz5u2s",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_pfbS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8449/Reviewer_pfbS"
        ],
        "content": {
            "summary": {
                "value": "This paper first proposes a new evaluation paradigm for LLM and also proposes a new prompting framework for LLMs called FaR, which provides a structure to encourage LLMs to anticipate future challenges and reason about potential actions. Compared with other prompting methods such as Chain-of-Thought and Self-Ask, FaR boosts the LLM (e.g., GPT-4) performance significantly. Besides, it also outperforms other baseline methods in few-shot in-context learning, showing the effectiveness of the new prompting method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The new prompting framework that encourages LLMs to anticipate future challenges and reason about potential actions is novel and impressive. \n\n2. The paper is well-organized and well-written with clear motivations, detailed discussion, nice figures, and sufficient comparison experiments, making it easy to follow and understand.\n\n3. This work performs comprehensive experiments over benchmark data to show the effectiveness of Far in several in-context settings."
            },
            "weaknesses": {
                "value": "1. This work illustrates the differences among FaR and GPT-4 in real-world scenarios. I am also curious about what it will look like with different prompting methods, such as Chain-of-Thought and Tree-of-Though. This work does not introduce existing prompting methods in detail, which will confuse the audience to some degree.\n\n2. FaR breaks the T4D process into three steps, question decomposition, theory-of-mind inference, and common sense assumption. However, the motivations for the breakdown have not been introduced in this work. I would like to suggest that this work explains the motivations clearly."
            },
            "questions": {
                "value": "1. What are the differences between existing prompting methods and FaR in LLMS?  What are the advantages of FaR?\n2. What is the motivation for the three steps in the T4D process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814504757,
        "cdate": 1698814504757,
        "tmdate": 1699637053506,
        "mdate": 1699637053506,
        "license": "CC BY 4.0",
        "version": 2
    }
]