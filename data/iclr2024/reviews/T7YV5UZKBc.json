[
    {
        "id": "STuqdDJf8j",
        "forum": "T7YV5UZKBc",
        "replyto": "T7YV5UZKBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_pojm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_pojm"
        ],
        "content": {
            "summary": {
                "value": "- The authors propose the optimal adaptation method through the lens of neural architecture search (NAS) in few-shot recognition.\n- Given a pre-trained neural network, the proposed algorithm discovers the optimal arrangement of adapters, which layers to keep frozen, and which to fine-tune.\n- The authors demonstrate the generality of our NAS method by applying it to both residual networks and vision transformers and report state-of-the-art performance on Meta-Dataset and Meta-Album."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) The proposed methods find some interpretable trends using layer-wise adaptations, which include the early/late layers of ResNet and ViT."
            },
            "weaknesses": {
                "value": "- (-) The authors stated the superior performances in various experimental settings. However, the author didn\u2019t specify the structure and the number of parameters.\n- (-) There is no ablation study on the two-stage search for optimal path (sec. 2.4): the best-performing path during training time, the searching path at test time, and the proposed hybrid one."
            },
            "questions": {
                "value": "- What is the most differentiating point from the prior NAS structures?\n- How many parameters increased by adapting?\n- Could authors provide parameter tables comparing NFTS (ResNet18) with others? The detailed architectural layout could be helpful to understand better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2290/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2290/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2290/Reviewer_pojm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563688319,
        "cdate": 1698563688319,
        "tmdate": 1700686223750,
        "mdate": 1700686223750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AWVZ9su2qx",
        "forum": "T7YV5UZKBc",
        "replyto": "T7YV5UZKBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_eX3h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_eX3h"
        ],
        "content": {
            "summary": {
                "value": "1. This paper provides the first systematic Auto-ML approach for finding the optimal adaptation strategy in few-shot learning.\n2. This method designs a novel strategy for defining the search space.\n3. The proposed method, namely NFTS, outperforms state-of-the-art methods in both Meta-Dataset and Meta-Album benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation for introducing NAS into FSL is good, as mentioned in this work: current FSL works have started to understand the trade-off between frozen weights and trained parameters. It makes sense to automatically search for the best configuration instead of manual search or \"carefully tuning learning rates.\"\n\n2. The experimental results present the superiority of NFTS; it achieves a significant performance gain on the Meta-Dataset.\n\n3. The analysis is interesting as it shows the trend that the best-searched configuration does perform the best in the unseen downstream."
            },
            "weaknesses": {
                "value": "The results lack significance compared to the additional training required to obtain NFTs. The method requires training a supernet, performing an evaluation to find the best subnet. As NFTs achieve only a less than 1% accuracy gain on the Meta-Dataset in a multi-domain setting, the method is excessively computationally expensive and inefficient when compared to the actual performance gain."
            },
            "questions": {
                "value": "Could you offer some insights about their consistent adaptation of (\u03b1) block 14 and their lack of adaptation for block 9 in the 'Discovered Architectures' paragraph?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639046181,
        "cdate": 1698639046181,
        "tmdate": 1699636161525,
        "mdate": 1699636161525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vsgr4JAbRG",
        "forum": "T7YV5UZKBc",
        "replyto": "T7YV5UZKBc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_mc8g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2290/Reviewer_mc8g"
        ],
        "content": {
            "summary": {
                "value": "The paper presents NFTS, a hierarchical method for neural architecture search in the few-shot image classification domain. The proposed framework engages various ways to adapt ResNet and ViT architectures to the support set including fine-tuning and adaptation parameters and then performs a search to identify the best-performing combination amongst each search path. The total number of paths is limited to both address computational limitations and prevent overfitting. Experiments on Meta-Dataset and Meta-Album demonstrate the strong efficacy of the approach when adapted inside a prototypical classifier with ResNet and ViT backbones. Ablation studies provide insights into various aspects of the method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is very well-written.\n- NFTS is empirically effective and demonstrates a good balance between enabling adaptation using the support set while preventing overfitting. Clear empirical evidence shows the model's ability to select a more optimal architectural combination than previous baselines.\n- Experiments are extensively performed on large-scale datasets that demonstrate the efficacy of NFTS across both ResNet and ViTs.\n- Ablation studies justify various architectural choices made such the total number of search paths and the granularity of options."
            },
            "weaknesses": {
                "value": "- Empirical results reported lack confidence intervals. Although I suspect this is due to space limitations, they should be included to verify the statistical significance of the results report and for better comparison with baselines. If some results do not meet statistical significance, they must be modified when presented in results tables to reflect so accordingly and the claims made need to be adjusted.\n- Ablation study on search paths shows that N=3 not only provides better computational efficiency but additionally prevent overfitting on the support set. How does it compare with N=2 or N=4? I believe that further insights here would be useful as to how this hyperparameter is set. Furthermore, how does performance vary depending on N across ViT and ResNet?\n- Meta-dataset baselines that are compared to omit some recent methods that can be included for completeness of comparison [1, 2, 3].\n\n[1] Improved Few-Shot Visual Classification\n[2] CrossTransformers: spatially-aware few-shot transfer\n[3] Enhancing Few-Shot Image Classification with Unlabelled Examples\n[4] Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning"
            },
            "questions": {
                "value": "Please address the questions and limitations noted above. Overall, I believe that this is a strong submission, and the broader research community can benefit from it. I believe that the empirical results of the paper need to be verified in terms of statistical significance by providing the appropriate confidence intervals across the reported numbers. This is the only major weakness in the submission, and once addressed with the other limitations noted, I would be more than happy to recommend the paper for acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807151057,
        "cdate": 1698807151057,
        "tmdate": 1699636161459,
        "mdate": 1699636161459,
        "license": "CC BY 4.0",
        "version": 2
    }
]