[
    {
        "id": "ERGOKlFual",
        "forum": "H98CVcX1eh",
        "replyto": "H98CVcX1eh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_KGfv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_KGfv"
        ],
        "content": {
            "summary": {
                "value": "The authors provide a theoretical analysis of the compositional generalization capabilities of linear hypernetworks under a teacher-student setup where the teacher network is the ground-truth data-generating model, and is itself a linear hypernetwork. The key result is that under 3 assumptions (compositional support, connected support, and no over-parameterization), the student can provably _identify_ the modules of the teacher. This identification is further demonstrated to be a necessary and sufficient condition for compositional generalization. The authors provide a series of experiments to validate their theoretical findings and test how sensitive the empirical results are to various violations of the assumptions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "######## Strengths ########\n- The problem of compositional generalization is of great interest to the ML/AI communities, and there are very few theoretical treatments of the problem\n- The combination of theoretical results and empirical evidence that tests beyond the bounds of the theory is well balanced"
            },
            "weaknesses": {
                "value": "######## Weaknesses ########\n- The experimental setting is unclear, especially the sequential decision-making portion, which makes it hard to assess its impact\n- It is unclear whether the three assumptions (compositional support, connected support, and no over-parameterization) are necessary conditions or just sufficient\n- The choice of linear hypernetworks as the base modular model somewhat limits the intepretability of the compositionality\n\n######## Recommendation ########\n\nI perceive this work as borderline acceptable. The balance of strengths and weaknesses, I believe, works out in favor of the strengths slightly. That being said, I encourage the authors to address my comments below to improve their manuscript.\n\n######## Arguments ########\n\nThere are very few theoretical treatments of compositionality in the context of neural nets in the literature. In particular, I am only aware of [1] (which the authors actually fail to mention). The two works study very distinct settings, and so I do believe that this submission proposes a novel piece of work. Moreover the authors execute a fairly comprehensive experimental evaluation of their ideas, which is rare for theoretical works. To me, this merits acceptance of the work.\n\nHowever, there are a few shortcomings that the authors should address.\n- Clarity of the experimental setting\n    - Sec 4.1\n        - How is compositional generalization measured? Does the agent train the task-specific coefficients on the new task without training the basis vectors? This isn't really described anywhere.\n        - \"certain overparameterization is necessary for meta-learning to succeed\" -- what does this mean? Doesn't this contradict the theory? Why is it necessary? Haven't all previous results used the \"right\" number of modules? Also, all results in Fig. 3F seem to show that more module overparameterization is consistently better. It isn't clear how this connects to the theory or the remaining results and claims. \n    - Sec 4.2\n        - Am I reading right that the agents are trained on _tens of millions_ of tasks? Though it seems that convergence occurs much earlier on. Could the authors zoom in to the left portion of 4E?\n        - The training setting for 4.2 is very unclear. Is there a teacher? If not, what is the training process? Why is \"accuracy\" used as the metric in a sequential decision making problem? How many total tasks are there (training on 10M out of 1B maybe isn't that bad, but 10M out of 30M isn't super encouraging)? Does accuracy correlate with actual performance on the task (i.e. does the agent actually compositionally generalize to new RL tasks)? How are we measuring \"OOD generalization\" in this setting?\n- Necessary or sufficient conditions \n    - The authors' aim, as stated in the introduction, is to understand \"what properties of the data generating process allow to discover modular solutions enabling compositional generalization\"\n    - However, the phrasing of the first lines of page 4 suggests that assumpions i-iii are _sufficient_ conditions, but not _necessary_ conditions for identification\n    - This means that what we really get is an understandong of _some_ of the properties that allow to discover modular solutions that compositionally generalize\n    - The authors do take steps to understand empirically what the \"cost\" is of violating those assumptions, and the assumptions themselves are fairly intuitive, so that's a positive\n- Interpretability of linear hypernetworks\n    - Other modular solutions (such as the neural module networks of [2]) have very intuitive explanations of what each module does _and how that composes with other modules_\n    - Linear parameter combinations within a hypernet don't seem to have such a straightforward intuition\n    - The authors' attempt at understanding what kinds of composition the model can learn is in the experiments of Sec 4.2, but since those are not clearly explained, it's hard to get any intuition from them. I encourage the authors to include an intuitive description of what the compositionality is in those tasks and how linear hypernets should capture that compositionality.\n\n\n[1] Ghazi et al., \"Recursive Sketches for Modular Deep Learning.\" ICML, 2019.\n\n[2] Andreas et al., \"Neural Module Networks.\" CVPR, 2016"
            },
            "questions": {
                "value": "######## Additional feedback ########\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nAbstract\n- Throughout the abstract and much of the text, it's unclear what types of problems the authors are tackling. There's a mention of \"demonstrations\" and \"action-value functions\", which somewhat hints at a solution geared toward RL. Much later, it becomes clear that a \"demonstration\" is the output of the teacher network, and that action-value prediction is just one application of the linear hypernets.\n\nSec 3\n- Up to this point, it was still unclear to me what the authors meant by identification of modules\n    - Is it that the student can determine which modules to use, or that it can find the right set of modules?\n    - Is this in a setting where the student knows the latent codes?\n    - These are later clarified, but it might be worth doing so earlier on\n\nSec 5\n- It's odd that the authors combined a related work section with the discussion, but I think it works okay. \n\nTypos/style/grammar\n- Intro, page 1 first paragraph: series of tasks --> set of tasks [a series is sequential]"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3724/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698369836697,
        "cdate": 1698369836697,
        "tmdate": 1699636328760,
        "mdate": 1699636328760,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7zKiTW0Px3",
        "forum": "H98CVcX1eh",
        "replyto": "H98CVcX1eh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_9UrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_9UrZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-task teacher-student approach with modular architecture for compositional generalization.\nIt uses hypernetworks to convert the generalization to the identification of modules and theoretically show the result up to linear transformation.\nExperiments also support the ability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It focuses on an important question of how to learn modular structure for compositional generalization.\n- The hypernetwork and modularity approach cast the generalization problem into an identification\nproblem.\n- Both theory and experiments support the result.\n- It proposes connected support to address permutation invariance."
            },
            "weaknesses": {
                "value": "My largest concern is that the framework is very constrained, and some constraints, e.g., linearity, may be essential to deriving the results.\nIt indicates that there may be difficulty when generalizing the result to more complex situations.\n\n(1) Linear assumption in hypernetwork.\n\n(2) It uses two two-layer neural networks.\n\n(3) It assumes knowing the correct (teacher) architecture.\n\n(4) The theory assumes knowing the number of modules (M) and hidden units (h)."
            },
            "questions": {
                "value": "Please respond to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3724/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764298288,
        "cdate": 1698764298288,
        "tmdate": 1699636328689,
        "mdate": 1699636328689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WV2t2Q0rMB",
        "forum": "H98CVcX1eh",
        "replyto": "H98CVcX1eh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_bgW6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_bgW6"
        ],
        "content": {
            "summary": {
                "value": "This work explores the ability of hyper-networks to detect the ground-truth task generating functions and have modules (rows of the linear hyper-network weights) specialise to these underlying factors of variation. Moreover these modules can then be composed in previously unused combinations to generalise to new tasks. Experiments demonstrating the benefit of modular meta-learning algorithms over monolithic meta-learners are also shown."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Originality\nThis paper uses an established technique, the teacher-student setup, to understand the necessary conditions for a dataset to promote systematic generalisation. Moreover, a connection between the teach-student setup and meta-learners is drawn, which I have not seen been made frequently before. Also considering ANIL, MAML and hyper-networks as the class of monolithic and modular meta-learners is interesting a new. Thus, the combination of concepts, theoretical technique and models considered in this work is  new.\n\n## Quality\nThe theoretical setup of this work seems appropriate for addressing the main concerns of this work - under what dataset conditions do meta-learners naturally generalise compositionally. The overall structure and logical setup of the work is good and sections naturally lead from one to the next. Section 3.3 directly tests the theory which is presented and in the case of Discrete task distributions the experiments support the theoretical findings. As far as I can tell the assumptions of the setup are clearly stated and the setup is clearly defined.\n\n## Clarity\nFigures are visually clear and well designed.\n\n## Significance\nI reiterate here that I think this work uses a very interesting combination of previous ideas. As a result the reported findings are interesting and I could certainly see the results leading to future work and guiding research on meta learning. Thus, I think this work does have the potential for high significance. I would say this is contingent on some of the weaknesses discussed below being mitigated."
            },
            "weaknesses": {
                "value": "I have two broad concerns for this work. One is on clarity and the other is on quality. I will begin with clarity as I think this may be a factor in the concerns on quality.\n\n## Clarity\nOverall I found this work to be relatively unclear. Notation is used but not properly introduced, for example $(U_k)$ where it is not mentioned what the double subscript $k$ refers to, or why the second subscript is necessary. Another example is where it is said $\\forall k,l$ where it is necessary to infer from context what $k,l$ is referring to. Also where it says $U_{k_i}$ what does the $i$ or $k$ refer to and why is this necessary. Similarly in Theorem 3.1 where it says \"The if $\\mathcal{L}(W_2,\\Theta)$...\", what is this new $\\mathcal{L}$ referring to? Is it the loss and this is just a mistake in the font? If so, why does this loss function not accept the same  parameters as previous loss functions. Finally, the notation introduced in the paragraph beginning \"We can now present our main result\" is particularly confusing because the superscript $(i)$ is overloaded three times, once referring to a row, the other referring to a column and the third for a full matrix sliced out from a tensor.\n\nSecondly, Definitions 3.1,3.2 and 3.3 are not clear and no intuition or interpretation is provided. For example, where defining irreducibility, the fact that all rows of $W_1$ are pairwise different means that each hidden neuron will be activated for a different feature - and extremely important point for a work concerned with whether meta-learners can extract ground-truth features from a \"metateacher\". This is not stated. Similarly, how this interacts with nonlinearities on the hidden neurons is ignored and what it means for no columns of $W_2$ to be $0$ is also not mentioned. Why would a full column of $W_2$ be $0$? Similarly, \"Compositional Support\" seems to just be saying that $U$ is a basis of $\\mathbf{R}^M$ and so I am not certain of how this new terminology is necessary. Is compositional support a weaker case of having a basis as $U$ does not need to be the **minimal** spanning set? Would Definition 3.1 and 3.2 together then imply that $U$ is a basis? Definition 3.3 is just extremely difficult to parse in general and so is Theorem 3.1. This is due to a lot of terminology being mentioned without definition and needing to be inferred based on context. For example, what is $\\hat{M}$ and why is Theorem 3.1 making a distinction between even and odd values of $n$? The difficulty in following these definitions alone makes the rest of the work difficult to follow.\n\nMy final point on clarity is that the figures, while visually neat and well done, are vague and their captions unhelpful. This is particularly bad when the figures are relied on heavily to explain concepts. For example, where it is said \"See Figure 2B for an illustration of a connected support\" and then the caption does not explain what a connected support is or how the connected vs disconnected task families connects to the actual Definition 3.3. Essentially, every figure caption should be elaborated on and potentially more information be placed in the figures to depict what is actually going on. For example, in Figure 1B, the tiling of the $x,y$ space is not connected at all to the rest of the work beyond the notation of $p(\\tau)$.\n\n## Quality\nMy concerns on quality are likely due to misunderstanding from the above. I would like to reiterate that I do believe this work has potential significance. I am, however, struggling to connect this work to the general literature on compositional generalisation. For example, assuming that $P_x$ has full support over the input space. I see how the learned weights of the linear hyper-network are compositional, in the sense that they operate similar to a set of basis vectors, which in the case of a linear mapping is also features for the network. However, this is then more similar to feature learning rather than exact modules. Or is the idea here that the features being learned which align with ground truth is the same as identifying a composable module? How would this then tie in with disentanglement, which implies compositionality. While I would be open to such claims - the most obvious on to me here being that feature learning and module learning in the limited setting you study are the same thing - I think that argument needs to be made explicit. This would be a different take on modularity in general though and systematic generalisation which tends to focus more on how separate pieces are learned to be separate in spite of covariances and this makes the problem easier [1]. In your case it seems more like a claim that if the input space is sufficiently explored then the network will learn the ground truth features but just because this is the only way to learn the task (to learn a full-rank basis set) and is not in fact learning to identify or solve a smaller problem which is then composable.\n\nThe experiments of Section 4 seem to be more in line with the standard notions of compositionality [2], however due to the above issues grounding the theory to larger scale models of compositionality I also struggle to see how Section 4 fits in with the rest of the work, beyond just demonstrating that hyper-networks are better in compositional domains than ANIL and MAML. Is there a greater connection beyond this (I do think this result is important in its own right though)?\n\nI would be open to increasing my score quite substantially if it is shown that I have indeed missed something crucial. Alternatively if the clarity issues are addressed and the connection to prior work made more explicit I would also increase my score. Likely to a 7. I would certainly prioritise improving the clarity, with the figures being particularly low hanging fruit which would make quite a big difference if improved upon.\n\n## Minor Points\n1. \"with each row representing one parameter module\" - I believe I understood what you were saying, but this was not an easily understandable sentence.\n\n[1] Hadley, Robert F. \"Systematicity in connectionist language learning.\" Mind & Language 9.3 (1994): 247-272.\n[2] Ruis, Laura, et al. \"A benchmark for systematic generalization in grounded language understanding.\" Advances in neural information processing systems 33 (2020): 19861-19872."
            },
            "questions": {
                "value": "I have raised a number of questions in my discussion above. I think the only outstanding question or point of clarification I have at this point is the following:\nCould the authors please explain what Figure 2A is aiming to show with the permutation invariance? It is only referenced in condition (ii) but then not explained in the figure. I think I would benefit from understanding this point better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3724/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3724/Reviewer_bgW6",
                    "ICLR.cc/2024/Conference/Submission3724/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3724/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774008687,
        "cdate": 1698774008687,
        "tmdate": 1700686364039,
        "mdate": 1700686364039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8hqkGa1Q2A",
        "forum": "H98CVcX1eh",
        "replyto": "H98CVcX1eh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_4knX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3724/Reviewer_4knX"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of compositional generalization in modular architectures. The authors show that in the teacher-student setting, it is possible to identify the underlying modules up to linear transformations purely from demonstrations. They further show that meta-learning from finite data can discover modular solutions that generalize compositionally in modular but not monolithic architectures. The authors also demonstrate how modularity implemented by hypernetworks allows discovering compositional behavior policies and action-value functions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper theoretically shows that students can learn the underlying modules from the teachers under certain conditions.\n2. The results are supported by empirical experiments."
            },
            "weaknesses": {
                "value": "1. The paper is not very well written and is hard to follow. There are no simple examples explaining the problems the authors are trying to solve.\n2. There is no section that explicitly discusses related work."
            },
            "questions": {
                "value": "1. Could you rearrange the paper to have a background section explaining things like MAML, hypernetworks, etc, for readers who are not familiar with these concepts?\n2. Could you explain why the network modules have to be hypernetworks in your setting?\n\n\n-----------\nupdate: raised to 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3724/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3724/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3724/Reviewer_4knX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3724/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798746474,
        "cdate": 1698798746474,
        "tmdate": 1700444113076,
        "mdate": 1700444113076,
        "license": "CC BY 4.0",
        "version": 2
    }
]