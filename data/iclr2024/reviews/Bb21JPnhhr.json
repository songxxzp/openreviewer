[
    {
        "id": "t7zg5367lW",
        "forum": "Bb21JPnhhr",
        "replyto": "Bb21JPnhhr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_R7fa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_R7fa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method named AntGPT to investigate if large language models encode useful prior knowledge on bottom-up and top-down long-term action anticipation. Experimental results on Ego4D LTA, EPIC-Kitchens-55, and EGTEA GAZE+ benchmarks indicate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe motivation is clear, and the usage of large pre-trained models is interesting.\n-\tExperimental results on multiple benchmarks indicate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "-  The novelty is limited, AntGPT is a straightforward application of large language models to the action anticipation problem.\n- In Table 1, the improvement of the proposed method is marginal.\n- The proposed method relies on the classification results of previous fragments, and classification errors may lead to the accumulation of errors and affect the prediction results."
            },
            "questions": {
                "value": "Please refer to the Weaknesses for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698414145765,
        "cdate": 1698414145765,
        "tmdate": 1699636612902,
        "mdate": 1699636612902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e6GJX1Dkka",
        "forum": "Bb21JPnhhr",
        "replyto": "Bb21JPnhhr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_jEKR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_jEKR"
        ],
        "content": {
            "summary": {
                "value": "The paper studies whether and how large language models can assist long-term action anticipation (LTA) from videos. To do so, the paper introduces AntGPT, an approach to perform LTA based on a mixture of a bottom up (direct prediction from observations) and top down (goal prediction, then action sequence prediction conditioned on goals) approaches. The paper also shows that the large language models used in the approach can be effectively distilled to smaller models for convenience. Extensive evaluations assess the influence of each component on final performance. The proposed approach is shown to outperform state-of-the-art approaches on major datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The question, whether LLMs can assist action anticipation, is surely interesting.\n\nI found the paper informative and the study well designed to answer the research questions stated in the introduction.\n\nExperiments are extensive and designed to be informative for the reader, beyond showcasing the abilities of the proposed approach.\n\nThe final model is shown to outperform previous approaches on the main datasets.\n\nI appreciated the \"limitations\" section."
            },
            "weaknesses": {
                "value": "While I believe the paper is sound and an interesting contribution, one point is not clear to me:\n\nIn-context (few-shot) learning is used to anticipate future actions with LLMs. The idea of using few-shot seems to be motivated by constraints due to LLMs rather than due to the task. Indeed, while one may use the entire training set as a set of examples, this would lead to a large prompt, which could be both impractical and detrimental to performance. However, in principle, it would make perfect sense to avoid few-shot learning altogether and resort to some form of supervised learning. While I understand the constraints, I feel this part is not well discussed in the paper. Also, how are few-shot examples selected in practice?\n\nMinor comments:\n- the \"visual procedure planning\" paragraph in the related works section is not very easy to follow. It seems like the paragraph states what visual procedure planning is related to, but it does not give a clear definition of what it is intended by the term \"visual procedure planning\". I'd suggest revising.\n- in the same section, the reference (Mascaro et al.) is missing the publication year\n- \"an few-shot high-level goal predictor\" -> \"a few shot ...\"\n- at page 5, the paper states \"we either use the video metadata as pseudo goals when it is available, or design the goals manually\". It is not clear to me what \"design the goals manually\" means, and this does not seem to be well described in the main paper.\n- the paper relies on CLIP, but it would have been interesting to see how the use of other ego-specific video-language models such as EgoNLP, LaViLa or EgoNLPv2 would perform in this context\n- while experiments are performed on EPIC-KITCHENS-55, no experiments are performed on EPIC-KITCHENS-100. I believe experiments on EK55 are relevant enough, but EK100 is a larger dataset (of which EK55 is a subset) and a much denser and cleaner one, so it would seem a perfect test bed for LTA. I'd encourage the authors to consider it for future experiments.\n- In Table 1, there is a star (*) next to \"oracle goals\", but I could not find the explanation of this symbol anywhere\n- In table 5, \"from-scratch\" is reported for the second model as setting. Does this mean that the model was randomly initialized? From the text it seems that it was still pre-trained on some text corpus. Can the authors clarify (or fix) this?"
            },
            "questions": {
                "value": "The authors could discuss on few-shot learning as referenced above and comment on the most relevant minor comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698474674829,
        "cdate": 1698474674829,
        "tmdate": 1699636612788,
        "mdate": 1699636612788,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V12hfK9bn0",
        "forum": "Bb21JPnhhr",
        "replyto": "Bb21JPnhhr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_v3Zh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_v3Zh"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates using large language models to help action anticipation in bottom-up or top-down manners. For the bottom-up approach, future actions are predicted using observed videos or actions. For the top-down approach, the prediction of future actions is additionally conditioned on the goal. The LLMs are used by prompting or fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The formulation of decomposing to bottom-up and top-down approaches is interesting and intuitive.\n2. Performance gains are observed on several datasets and ablation studies are good in general.\n3. The writing and presentation is good in general."
            },
            "weaknesses": {
                "value": "1. The interpretation of the results should be cautious. The performance gain is evident but not surprising since additional labels of goals and additional knowledge from LLMs are introduced. Although the boundary of using additional information has become vague since the popularity of LLMs, I still believe we should be cautious when comparing a new method with LLMs and previous methods without LLMs.\n\n2. Using LLMs for action anticipation makes me raises the question about the fundamental positioning of this task. In this sense, the action anticipation is mainly a language-understanding task rather than a vision task. If we look at Table 2, it is better to use actions as inputs than using visual features. If we still treat action anticipation as a vision task, what is the role and the importance of the visual modality? If we treat it as a language task, the contribution of this paper will be limited since it mainly utilizes prompting and fintuning of LLMs. If we treat it as a multi-modal task, we need to see more insights and experiments about the interaction between modalities."
            },
            "questions": {
                "value": "I don't have too many questions about the technical part of this paper. I would appreciate comments on how should we position the task of action anticipation in the following research. Should this task actually be a compound task of action recognition (for observed video) and language understanding for anticipation? Or should we still study this task as a standalone task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Reviewer_v3Zh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698543811264,
        "cdate": 1698543811264,
        "tmdate": 1700694320231,
        "mdate": 1700694320231,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YOuXoamgnX",
        "forum": "Bb21JPnhhr",
        "replyto": "Bb21JPnhhr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_Utid"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5811/Reviewer_Utid"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to utilize large language models for long-term action anticipation. CLIP visual features are extracted from each video segment and a sequence of action labels are predicted, which are input to the LLM. In-context learning is used to predict the final goal, which is embedded with the CLIP text encoder. Experiments on three datasets show the efficacy of the proposed method. The authors also show the distilled smaller models can be effective as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of using LLMs for long-term action anticipation is interesting and the intuition makes sense.\n2. Experiments are thorough. The authors compare multiple usages of LLMs for the LTA task, including in-context learning and fine-tuning.\n3. The proposed method achieves SOTA on three benchmarks."
            },
            "weaknesses": {
                "value": "1. The paper writing/organization can be improved. The method description part of AntGPT is quite confusing. For example, an overview of how the separate parts of Figure 1 (b-e) are connected to each other will help. Also, for the experiment section, putting \u201ccomparison with state-of-the-art\u201d to the front might be better IMO.\nOther minor presentation issues: Some text in Figure 1 is too small to read. \n2. There is no error analysis. When does the proposed method fail and why?"
            },
            "questions": {
                "value": "1. The goals in Figure 2 are sometimes simply a repetition of the last action labels. Can the authors elaborate on how the goals are defined (manually annotated)? Should they be the last action of the videos?\n2. In Table 5, the distilled model is even better than the original model. How is that possible? Can the authors provide more explanation on that?\n\n------------Post rebuttal comments: the authors have addressed my questions and concerns. I have increased my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5811/Reviewer_Utid"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735628346,
        "cdate": 1698735628346,
        "tmdate": 1700968321179,
        "mdate": 1700968321179,
        "license": "CC BY 4.0",
        "version": 2
    }
]