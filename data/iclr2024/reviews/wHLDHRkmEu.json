[
    {
        "id": "Hz7uW4Mef1",
        "forum": "wHLDHRkmEu",
        "replyto": "wHLDHRkmEu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission920/Reviewer_8Qir"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission920/Reviewer_8Qir"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel Parameter-Efficient Tuning framework for Referring Image Segmentation. By designing a bi-directional intertwined vision-language adapter, this frame exploits the frozen power of pre-trained vision-language models. In addition, this paper introduces a Global Prior Module and a Global Shortcut Tuning Network to extract global prior from text input to regularize the visual feature.\nCombining these contributions, this paper outperforms previous parameter-efficient tuning methods on RIS benchmarks and even surpasses SOTA full fine-tuning approaches on several tasks of RIS benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Strong performance on three referring image segmentation benchmarks.\n* The well-designed Local Intertwined Module: according to the Table 4: Local Intertwined Module improves can outperform ETRIS with very small amount parameters."
            },
            "weaknesses": {
                "value": "* The limited improvement brought by Global Shortcut Tuning.  According to Table 5, Compared with No Global, BarLeRIa achieves 72.2 on RefCOCO using 2.21M, while without GST, the model achieves 71.4 using 0.39M. It seems that the model without GST owns a better balance between performance and parameters."
            },
            "questions": {
                "value": "* See in Weaknesses.\n* Is any possible to provide the training time comparison between ETRIS and BarLeRIa?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission920/Reviewer_8Qir",
                    "ICLR.cc/2024/Conference/Submission920/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744507171,
        "cdate": 1698744507171,
        "tmdate": 1700750385928,
        "mdate": 1700750385928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PHqittbfZo",
        "forum": "wHLDHRkmEu",
        "replyto": "wHLDHRkmEu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission920/Reviewer_eHeC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission920/Reviewer_eHeC"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of using parameter efficient fine-tuning in referring image segmentation (RIS). A sophisticated PEFT paradigm is proposed, consisting of Adapting Shortcut with Normalizing Flow (SNF),  Local Intertwined Module (LIM), and Global Shortcut Tuning (GST). Results are presented on commonly used RIS benchmarks of RefCOCO, RefCOCO+, and G-Ref. A detailed evaluation shows superior results of the proposed method over previous PEFT method in RIS."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written. Adequate background is provided, the literature review covers very recent approaches, and the paper provides necessary background to the required concepts.\n- It seems novel to me to use a small side network that takes text+image features with attention and output side features containing global priors. And the ablation study shows a significant improvement with this module.\n- The proposed approach gets strong results that beats the previous parameter efficient fine-tuning approach in RIS.\n- The paper conducts a thorough ablation study to show the effectiveness of each proposed module."
            },
            "weaknesses": {
                "value": "- Writing in Section 3.3 and 3.4 is not clear enough. It should be possible to write in one equation for (2)(3)(4) with a better subscription. Also it's a bit unclear whether $F_v^i$ is the same as (the $i^{th}$) $[cls, embed]$.\n- (typo): it should be intertwine not interwine in Figure 2."
            },
            "questions": {
                "value": "Does the side network (Global Shortcut Tuning Network) not operate in parallel with the vision/language encoder? And is it required to cache each $F_v^i$ for the side network? If so, what is the extra time and memory cost for this approximately?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797303355,
        "cdate": 1698797303355,
        "tmdate": 1699636018802,
        "mdate": 1699636018802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AQqC35jLQF",
        "forum": "wHLDHRkmEu",
        "replyto": "wHLDHRkmEu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission920/Reviewer_iAN9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission920/Reviewer_iAN9"
        ],
        "content": {
            "summary": {
                "value": "The authors have proposed a novel intertwined vision language efficient tuning algorithm based on the large-scale CLIP model. They claim that the previous methods overlook adapting the biased feature from pre-trained models and global prior regularization. The proposed method achieves state-of-the-art performance on RefCOCO, RefCOCO+, and G-Ref."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method brings a few trainable parameters into CLIP for both feature adaptation and modal fusion, which achieves the best performance on several RIS datasets.\n2. The proposed method utilizes a novel intertwined structure to assist modal feature fusion."
            },
            "weaknesses": {
                "value": "1. The global prior is predicted after the vision-language blocks. Then, the global prior is input to the global shortcut tuning module. However, such a design is quite a long path. The inference speed may be very slow.\n2. The comparison with other methods may be unfair. The proposed method utilizes ViT-Large as its backbone.\n3. The dimensions and meanings of different symbols are not introduced in the paper, such as [Fl, Fv].\n4. The decoder is not introduced in this paper. Although the authors have claimed they follow previous works, it is not detailed enough."
            },
            "questions": {
                "value": "Could you provide any results of the inference time compared with other parameter tuning methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860489107,
        "cdate": 1698860489107,
        "tmdate": 1699636018722,
        "mdate": 1699636018722,
        "license": "CC BY 4.0",
        "version": 2
    }
]