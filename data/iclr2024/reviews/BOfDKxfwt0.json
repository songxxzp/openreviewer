[
    {
        "id": "Fz1gr7tyL1",
        "forum": "BOfDKxfwt0",
        "replyto": "BOfDKxfwt0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_M7pJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_M7pJ"
        ],
        "content": {
            "summary": {
                "value": "This work introduces RealChat-1M, a large-scale dataset containing 1M human-LLM conversations.  The authors presented a comprehensive exploration of the dataset through four distinct use cases: the development of content moderation models, establishment of a robust safety benchmark, training of instruction-following models, and the creation of challenging benchmark questions, highlighting the multifaceted utility of the dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- A large-scale real-world conversational dataset with diverse range of applications, including the development of content moderation models, the creation of a robust safety benchmark (jailbreak conversations), the training of instruction-following models, and the collection of challenging benchmark questions (Arena-Hard-200). \n- This work offers a comparative analysis across all use cases, leveraging both open-source LLMs like Vicuna and proprietary models such as GPT-4. The experimental results reveals 1) large performance gaps between open and proprietary models, 2) safety concerns of all the models especially the models developed by open source community (e.g., Vicuna, Alpaca)."
            },
            "weaknesses": {
                "value": "While the presented dataset significantly surpasses the scale of other datasets, I find myself questioning the unique value it brings. OpenAssistant, SharedGPT, Anthropic HH, and Chatbot Arena have already proven their worth in fine-tuning LLMs, assessing LLM safety and performance. What compelling reasons can be put forth to encourage researchers to opt for a subset of RealChat-1M for these very purposes?"
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616039370,
        "cdate": 1698616039370,
        "tmdate": 1699636249891,
        "mdate": 1699636249891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VsEqEyMPab",
        "forum": "BOfDKxfwt0",
        "replyto": "BOfDKxfwt0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces RealChat-1M, a dataset contains 1 million real-world conversations with 25 large language models collected from 210,000 unique IP addresses in the wild. The authors demonstrate the usefulness of the dataset on multiple use cases like content moderation models, safety benchmarks, or training instruction-following models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The dataset introduced by this paper is a large-scale dataset containing interactive logs of 210,000 unique IP addresses with 25 large language models, which is both extremely valuable and meaningful for future LLM development, given most datasets that were used to train LLMs are not publicly available.\n- Part of the data that can jailbreak the safeguards of leading LLMs is repurposed by the authors to be a benchmark for safety and robustness study.\n- The authors also curate a benchmark that contains challenging and high-quality user prompts for identifying the gap between open-sourced and proprietary models.\n- Demonstration for the use cases of the dataset were nicely done."
            },
            "weaknesses": {
                "value": "- Figure 1 and 2 could be redrawn by leaving vicuna/English out because the distribution is left-skewed and therefore the number for the rest of the models/languages are hard to interpret.\n- Although in the limitation section there is a paragraph about the data quality, deeper analysis could be done. For example, how many of them is from MMLU and MT-Bench, is there some human annotation done for duplicate data?"
            },
            "questions": {
                "value": "How many prompts are there after filtering for section 3.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638791163,
        "cdate": 1698638791163,
        "tmdate": 1699636249812,
        "mdate": 1699636249812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DsbywKamiz",
        "forum": "BOfDKxfwt0",
        "replyto": "BOfDKxfwt0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
        ],
        "content": {
            "summary": {
                "value": "This work presents a dataset of 1 million real user chats with 25 popular LLMs. The main contribution of the paper is the data itself, which was collected over 5 months and includes a diverse array of user interactions. The authors present some analysis of the contents of the dataset, then show 4 use cases: building a moderation model, a safety benchmark, instruction tuning, and a challenging prompt benchmark."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The data itself is quite valuable. As the authors note, much of the data actually used in training models is proprietary and private\n- The 4 use cases are all creative and well designed. They demonstrate the potential of the dataset as a strong resource \n- The analysis is quite interesting, for example the cluster analysis in figure 3. It also supports/is validated by past work, e.g. the observation that many users now are interested in LLMs for help with coding\n- The size and diversity of the dataset promises many future uses"
            },
            "weaknesses": {
                "value": "- The dataset does not seem quite as diverse as the abstract suggests. Although 25 LLMs are used, Vicuna-13B is by far the most frequently used one. Similarly, while there may be a few instances of many languages, the vast majority is still in English\n- As the authors point out, there are no human preference values which is one weakness compared to related datasets (see table 1)\n- It is not clear whether the use format (single model vs side-by-side) and IP address have similar balance issues to model and language above. Although there are 210K unique IP addresses, is some large portion of the dataset covered by only a few of these? And do most users access single models rather than the perhaps more interesting side-by-side mode? Having the answers to these questions would help evaluate this paper further."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "It would be useful to ensure that the real user inputs do not put any of the users at risk of deanonymization or harm. The authors make a best effort to prevent this, but having some review of this may be necessary."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788787352,
        "cdate": 1698788787352,
        "tmdate": 1699636249737,
        "mdate": 1699636249737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fr4883U4KC",
        "forum": "BOfDKxfwt0",
        "replyto": "BOfDKxfwt0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
        ],
        "content": {
            "summary": {
                "value": "Through the introduction of the RealChat-1M dataset, this paper provides a comprehensive resource for studying user interactions with LLMs in real-world settings. They leverage RealChat-1M for 4 use cases: developing content moderation models, building a safety benchmark, training instruction-following models, and creating challenging benchmark questions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. With a size of 1 million, this dataset stands out as unparalleled among its peers.\n2. This dataset offers authentic user prompts and highlights potential safety concerns, paving the way for future research.\n3. This paper presents studies that highlight the four distinct applications of this novel dataset.\n4. The authors have a plan to regularly update the dataset in the future, which is a crucial step given the frequent release of newer and more advanced LLMs from the community."
            },
            "weaknesses": {
                "value": "While the user prompts in this dataset are genuine, the responses are synthetic and do not have quality ratings. Thus, additional processing is necessary before use."
            },
            "questions": {
                "value": "Are there plans to enhance the website's interface design to increase its accessibility for a broader audience? Such improvements could lead to a more diverse user base and a richer dataset distribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836656916,
        "cdate": 1698836656916,
        "tmdate": 1699636249649,
        "mdate": 1699636249649,
        "license": "CC BY 4.0",
        "version": 2
    }
]