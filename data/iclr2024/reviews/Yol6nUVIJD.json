[
    {
        "id": "FWhnALkOYd",
        "forum": "Yol6nUVIJD",
        "replyto": "Yol6nUVIJD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a multi-model, multi-agent framework called ReConcile, which is inspired by roundtable conference discussions. In each round, every agent must produce an answer, an explanation, and the corresponding confidence level. Following multiple rounds of discussion, ReConcile generates a collective team answer. Experiments on various benchmarks (StrategyQA, ECQA, GSM8K, AQuA) demonstrate the effectiveness of ReConcile."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.It is an interesting idea to extend previous work from multi-agent debate to roundtable discussions.\n2.Experiments on various benchmarks show that the proposed method outperforms previous work."
            },
            "weaknesses": {
                "value": "1.The experiments are conducted on relatively simple reasoning tasks, which seem inconsistent with the claim of \"solving complex reasoning problems\" made in the introduction. Does ReConcile enhance performance on more complex reasoning tasks, such as logical reasoning, the MATH dataset, CommonsenseQA?\n2.The proposed method consists of multi-model predictions. An ablation study (Table 5) shows that the performance significantly decreases without the multi-model approach. It is unclear whether the performance gain is caused by the multi-model ensemble. A comparison with an ensemble baseline should be considered.\n3.There are many hyperparameters in ReConcile (number of discussion rounds, voting weight). It would be interesting to explore if these hyperparameters can be dynamically generated during the roundtable discussions and if they are robust across tasks.\n4.It is unclear whether a weak model could negatively impact the performance. \n5.There is a line of work improving the reasoning ability of LLMs [1].   It would be interesting to investigate whether ReConcile outperforms these methods both theoretically and empirically."
            },
            "questions": {
                "value": "1.How to calculate the performance of ChatGPT? The performance reported in Table 2 seems lower than previous work [2]\n\n[1] WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\n[2] Making Large Language Models Better Reasoners with Step-Aware Verifier"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737019450,
        "cdate": 1698737019450,
        "tmdate": 1699636684881,
        "mdate": 1699636684881,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5tCxJbTGfx",
        "forum": "Yol6nUVIJD",
        "replyto": "Yol6nUVIJD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel algorithm (ReConcile) for leveraging multiple LLM calls (like self-refine) to improve results over single LLM calls. ReConcile is similar to Multi-Agent Debate, but with 3 additional innovations:  (1) using distinct model families (GPT, Bard, & Claude), (2) leveraging confidence estimates from the models, and (3) encouraging the models to attempt to convince each other. ReConcile is compared against several relevant baseline algorithms on four standard benchmarks (StrategyQA, ECQA, GSM8K, AQuA). ReConcile obtains clear improvements over baselines on most of these experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is well motivated and clearly explained, which is a challenge for a framework of this complexity. The baselines and benchmarks are well chosen, and the experimental methodology appears sound. \n\nAblation studies, summarized in Table 5, quantify the contribution of each of the architectural components to accuracy. The lift is largest for the multi-model dimension, which is particularly interesting. \n\nInterestingly, it is noted (perhaps unsurprisingly) that when a stronger model (such as GPT-4) is included in multi-model, multi-agent debate, it tends to dominate the decision, which causes the results to converge on the accuracy of the stronger model acting alone."
            },
            "weaknesses": {
                "value": "While GPT-4 itself is shown to benefit from ReConcile, GPT-4 is not included in most of the baseline algorithms, such as Multi-Agent Debate. So we are left to wonder whether GPT-4 would benefit as much or more from those other baseline. \n\nWhile relative efficiency is discussed (in terms of number of rounds for a given accuracy), token counts are not discussed. This makes it hard for readers to determine how much of the benefit of ReConcile might simply be due to increased tokens (and cost). \n\nThere are clearly many more important evaluations that could be performed. Fortunately the code for the framework is provided for review, so I assume it will be made available as OSS later. This will allow the community to run many more evaluations with new generations of models."
            },
            "questions": {
                "value": "How often do the agents settle on a single answer? Maybe I missed this detail somewhere."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698972980042,
        "cdate": 1698972980042,
        "tmdate": 1699636684770,
        "mdate": 1699636684770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rlrVJRll4C",
        "forum": "Yol6nUVIJD",
        "replyto": "Yol6nUVIJD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-agent framework to improve the reasoning capabilities of LLMs by leveraging multiple rounds of discussion, demonstrations of answer-rectifying human explanations used to convincing other agents, and a confidence-weighted voting mechanism. The discussion phase also includes uncertainties in predictions with recalibrated confidence values. The evaluations of the proposed approach with three diverse agents (Claude, GPT-3.5 turbo and Bard) show significant performance improvements in some of the reasoning benchmarks where majority of the gains come from diversity in agent outputs and the use of convincing samples."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The problem statement of leveraging multiple agents' diversity to improve reasoning is very timely.\n- The paper presents a clear concise definition of the proposed approach as well as ablations of the individual components of the proposed approach in providing gains for reasoning benchmarks. The detailed ablations provide useful insights for the reader in building atop the proposed work.\n- The gains on the StrategyQA benchmark are quite impressive from diversity of different model responses."
            },
            "weaknesses": {
                "value": "- The recalibration scale used to determine uncertainties in predictions is not backed by experiments"
            },
            "questions": {
                "value": "- It would be useful to provide individual ablations for demonstrations of answer-rectifying human explanations on each of the models and more qualitative examples of the same to disentangle its effect from diverse model responses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6257/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6257/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699386270037,
        "cdate": 1699386270037,
        "tmdate": 1699636684623,
        "mdate": 1699636684623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ud38VGoqUn",
        "forum": "Yol6nUVIJD",
        "replyto": "Yol6nUVIJD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a RECONCILE framework which leverages different LLM agents to collectively generates answers to reasoning tasks. Specifically, in multiple rounds, LLM agents generate responses and corresponding confidence using specific prompts, discuss in groups and generate explanations that may convince other agents. Experiments conducted on ChatGPT, Bard, and Claude2 achieve better performance on QA (StrategyQA and ECQA) and math problems (GSM8K and AQuA), compared to single-agent baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes an interesting multi-agent multi-round collection method to improve model performance on math and commonsense reasoning tasks. This indicates some complementary information from different trained LLMs and suggest how different LLMs can be applied to solve challenging tasks.\n2. Some ablation studies and experiments (such as the importance of using confidence estimation) are informative and motivating."
            },
            "weaknesses": {
                "value": "1. There are many moving pieces in the propose method, especially the convincing samples and team answer generation heuristics with confidence estimation. In particular, the method requires choosing samples \"from the training set for which the agent's initial answer is wrong\" brings a very strong bias compared to the baseline methods. Although there are ablation studies, it is still not very convincing what \"complementary benefits\" are provided across different LLMs, and how exactly the multi-agent method is fundamentally better than previous debate or self-consistency. This is more concerning because in Table 3, it seems that although Weighted Vote saturates with 3 rounds, majority vote results keep improving. Furthermore, the proposed confidence rescaling heuristics seem brittle and not clear the impact of this from tasks to tasks.\n2. This paper is mostly evaluated on two major tasks, QA/reasoning (StrategyQA and ECQA) and math (GSM8K and AQuA). Previous reasoning papers (such as self-consistency) were evaluated on a much wider range of tasks to illustrate how generalizable the proposed approach is."
            },
            "questions": {
                "value": "1. Why is the margin over baselines much larger for StrategyQA and ECQA (while being almost the same on GSM8K and AQuA)?\n2. Are the results in Table 2 and Table 3 consistent (on StrategyQA)? Why is the weighted vote score 79.0 in Table 2 and 78.7 in Table 3?\n3. Can you clarify how grouping works? What are \"distinct categories\"?\n4. From Table 4, why do you think GPT4 + Bard/Claude2 hurt the performance (of GPT-4 + GPT-4) significantly? i.e., why would the powerful model be dramatically impacted by smaller models?\n5. In Section 5.1, how is 3 rounds of Debate with GPT-4 different from RECONCILE with GPT-4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699601106540,
        "cdate": 1699601106540,
        "tmdate": 1699636684489,
        "mdate": 1699636684489,
        "license": "CC BY 4.0",
        "version": 2
    }
]