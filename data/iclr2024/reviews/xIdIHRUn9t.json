[
    {
        "id": "JVAbmvqhqT",
        "forum": "xIdIHRUn9t",
        "replyto": "xIdIHRUn9t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_LtG6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_LtG6"
        ],
        "content": {
            "summary": {
                "value": "The task of Motion Customization involves adapting these models to produce videos showcasing specific motions using reference video clips. However, conventional adaptation methods often entangle motion concepts with appearances, limiting customization. To address this, \"MotionDirector\" is introduced, employing a dual-path Low-Rank Adaptions (LoRAs) architecture and an appearance-debiased temporal loss, which effectively decouples appearance and motion, enabling more versatile video generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper identifies the challenge in generalizing customized motions across diverse appearances. The integration of motion in the video appears great, and this effect can be attributed to the decoupling treatment of the temporal module.\n- It proposes a dual-path architecture designed to separate the learning of appearance and motion.\n- The visual results show that the proposed method outperforms multiple baseline methods on motion control and video object replacement."
            },
            "weaknesses": {
                "value": "- The explanation of the decentralized temporal loss is not very clear. It might be beneficial to verify the effect of this loss through more ablation experiments, especially in the context of video visualization.\n- Training LoRA does not appear to be computationally intensive, but it's advisable to specify the training cost in the article.\n- There are concerns about the generalizability of this method for video motion extraction. It's worth considering the possibility of developing a unified video motion extraction module to address this issue.\n- There are no supplementary videos, which makes the paper less convincing."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The generated content may contain some concept bias."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698293828219,
        "cdate": 1698293828219,
        "tmdate": 1699636307780,
        "mdate": 1699636307780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0FiMGMJI9R",
        "forum": "xIdIHRUn9t",
        "replyto": "xIdIHRUn9t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_8uoP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_8uoP"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the concept of Motion Customization in text-to-video diffusion models and proposes a method called MotionDirector, which applies dual-path inserted LoRAs to decouple the learning of content and motion. It also incorporates an appearance-debiased temporal loss to refine the learning process further. The paper validates the approach through experiments on two benchmarks, demonstrating its superiority in terms of motion fidelity and appearance diversity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- The proposed appearance-debiased temporal loss sounds reasonable.\n- I appreciate Fig 4, which shows the denoising paths of different conditions.\n- The demo quality is good."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty**: \n   - The concept of decoupling the learning of content and motion is not new and has been explored in works like \"Align your Latents\" [1] by NVIDIA. The dual-path architecture seems to be a reiteration of this idea.\n   - The methodology largely builds upon existing techniques like Low-Rank Adaptions (LoRAs).\n\n2. **Lack of Justification for Appearance-Debiased Temporal Loss**: \n   - The paper introduces an appearance-debiased temporal loss but does not provide a thorough explanation or justification for its effectiveness.\n   - The introduction of a hyperparameter $\\beta$ is not accompanied by a sensitivity analysis, leaving its impact on the model's performance unclear.\n\n3. The video length is too small (number of frames), it seems only experiments on video length equal to 16 are conducted. Considering that it requires 8 minutes to fit 16 frames, it becomes very time-consuming and even inapplicable for longer videos.\n\n4. Technical contribution is weak. \n\n[1] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Reviewer_8uoP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698456080524,
        "cdate": 1698456080524,
        "tmdate": 1699636307697,
        "mdate": 1699636307697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OfPSVkZQiB",
        "forum": "xIdIHRUn9t",
        "replyto": "xIdIHRUn9t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_QKrd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_QKrd"
        ],
        "content": {
            "summary": {
                "value": "This paper presented MotionDirector, a diffusion-based pipeline for text-to-video video editing. The paper emphasized the design of motion-appearance decoupling dual-path architecture and a special appearance-de-biased temporal loss. The experiments are conducted on UCF Sports and LOVEU-TGVE-2023."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The way that authors decouple the motion and appearance when using LoRA is novel and smart.  \n2. The qualitative experiment results are convincing. \n3. The paper generally reads well."
            },
            "weaknesses": {
                "value": "1. I personally do not see a necessity that especially formulates the task of motion customization. It is a subset of video editing tasks. Meanwhile, the motion pattern is not generated from scratch nor adjustable. \n2. There is no discussion of failure cases, which can provide important insights for the video editing field."
            },
            "questions": {
                "value": "1. I really love the motivation of appearance-debiased temporal loss. Especially, the illustration in Figure 4 is intriguing and meaningful. However, I expect the authors to provide more discussion and analysis for this part. Including but not limited to answering the following questions:\n\n\t* Is there a more theoretical and/or experimental proof for the hypothesis: motion primarily impacts the underlying dependencies between these point sets, whereas the distances between different sets of points are more influenced by appearance?\n\n\t* Is there a better way to evaluate the effectiveness of AD loss? In the paper, there are only two sample videos showcasing the impact of adding AD loss to the training.\n\n2. Do the authors test how many frames can be consistently generated using the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3534/Reviewer_QKrd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698516778828,
        "cdate": 1698516778828,
        "tmdate": 1699636307610,
        "mdate": 1699636307610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RjTY7VHyzp",
        "forum": "xIdIHRUn9t",
        "replyto": "xIdIHRUn9t",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_E7V1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3534/Reviewer_E7V1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MotionDirector, a dual-path LoRA architecture to decouple the learning of appearance and motion within videos for transferring the motion. It also proposes a new loss function for debiasing appearance bias in temporal information. Experiments show that the proposed method can generate diverse videos with desired motion concepts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow \n2. The idea of dual-path model combining LoRA is intersting, \n3. Experimental results show the effectiveness of the proposed method to achieve the transfer of target actions."
            },
            "weaknesses": {
                "value": "1. In this paper, the LoRA technique is used to decouple the learning of appearance and dynamics in reference videos. Does this method require separate training for each specific set of videos for a particular motion to generate videos? How does the video quality fare beyond the distribution?\n\n2. The authors have designed two branches for learning the appearances and dynamics of videos. In the temporal branch, the authors have included spatial LoRA and share parameters with the spatial branch. Does such inclusion of spatial LoRA in the temporal path interferes with learning temporal information?\n\n3. In Figure 4, video 1 & video 2, and video 2 & video 3 are relatively close, but they do not seem to have much in common (e.g., the three videos do not share the same motion or appearance). On the contrary, video 3 and video 4 are farther apart. Therefore, I am skeptical about the claim that the distance between clusters is determined by the appearance of the videos. It's important to base such claims on statistical results from a larger sample rather than a few videos, as the latter can lead to a higher degree of randomness. I suggest gathering more video data to support the argument.\n\n4. In Figure 4, Part D, the authors mention that it represents the visualization of appearance-debiased latent codes, but it's not clear how it relates to Part C. How does Part D reflect appearance debiasing?\n\n5. In the section \"Temporal LoRAs Training\" on page 6, why does inserting spatial LoRA into the temporal path allow the temporal LoRA to ignore the appearance information in the training dataset? Why wont it affect spatial LoRA during the training of the temporal path?\n\n6. How was Equation 6 derived? Why was this form of loss function with sampling used to eliminate appearance bias in the temporal information? Epsilon_anchor is used as an anchor, so why is there another epsilon_i later on? What is the purpose of having two anchors?"
            },
            "questions": {
                "value": "see weaknessess"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857997349,
        "cdate": 1698857997349,
        "tmdate": 1699636307517,
        "mdate": 1699636307517,
        "license": "CC BY 4.0",
        "version": 2
    }
]