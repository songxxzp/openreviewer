[
    {
        "id": "wsCl3MNgCR",
        "forum": "XM7INBbvwT",
        "replyto": "XM7INBbvwT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission981/Reviewer_NQcN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission981/Reviewer_NQcN"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the link between calibration and human trust in an AI model.\nThe authors state that according to prospect theory, humans consume probabilities based on a reference point to their current situation.\nBased on this the authors develop a calibration approach (PT-calibrated) that re-weighs probabilities based on how humans subjectively consume them. \nTo evaluate their approach they develop a human survey where participants are, among other aspects, queried about their trust in the model based on confidence. \nThe authors show that the model whose calibrated confidences were further treated with the weighting function (based on prospect theory) is best trusted by the participants and its predictions best correlate with human decisions.\nThe authors also find the largest increase in human trust between the first and last questions of the survey by PT-calibrated, however these results are not statistically significant."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper examines a very important problem: the link between confidence calibration and how humans make judgments using these confidence scores\n- The paper shows how a reweighting function (with ideas from decision theory) that can reweight confidences elicits more trust from humans than a simple calibrated model\n- The paper's ideas and results are crucial to creating trustable ML systems and would be very interesting to these communities"
            },
            "weaknesses": {
                "value": "- I think the paper's experimentation is lacking. \n    - The current experimental setup is much too simplistic: 1. Just asking the users how much they trust the system can result in a lot of noise especially as users have no reason to be faithful. It seems that prior works usually measure some proxy for trust [1], or simulate an environment where where participant's trust is linked to some monetary risk/reward [2.3]\n    - The authors show experiments on a single task, also the authors ignore the temporal effects of changing trust as the participant interacts with the system. \n- A lot of experimental design choices (eg Likert scale to quantify trust) seem to differ from prior works that examine human trust. Perhaps the authors could spend more time justifying them \n\nMinor:\n- Table 1, why not round down to some number of significant digits?\n\nOverall, I think the paper has some very interesting ideas but is still not mature enough for acceptance owing to the lack of thorough experimentation. \n\n[1] Zhang, Yunfeng, Q. Vera Liao, and Rachel KE Bellamy. \"Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making.\" Proceedings of the 2020 conference on fairness, accountability, and transparency. 2020.\n[2] Vodrahalli, Kailas, Tobias Gerstenberg, and James Y. Zou. \"Uncalibrated models can improve human-ai collaboration.\" Advances in Neural Information Processing Systems 35 (2022): 4004-4016.\n[3] Gonzalez, Ana Valeria, et al. \"Human evaluation of spoken vs. visual explanations for open-domain qa.\" arXiv preprint arXiv:2012.15075 (2020)."
            },
            "questions": {
                "value": "- How much time did each participant take to complete the survey on average? \n- How do the authors ensure that the participant responses were faithful?\n- Did the participants receive $1 per question or for 30 questions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699113363160,
        "cdate": 1699113363160,
        "tmdate": 1699636024129,
        "mdate": 1699636024129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aiwRUcGiTz",
        "forum": "XM7INBbvwT",
        "replyto": "XM7INBbvwT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission981/Reviewer_Yu3U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission981/Reviewer_Yu3U"
        ],
        "content": {
            "summary": {
                "value": "The paper examines the effectiveness of the probabilistic calibration as to how it affects the human decision making. In particular, the paper studies if the humans (decision makers) are willing to change their decision making depending on how (and what kind of) forecast is revealed to them for the relevant event. Along with standard calibrated and uncalibrated forecasts, the paper also employs post-hoc corrections to the forecasts based on the prospect theory in behavioural economics. Overall, the paper finds that there are no significant differences in the reported users' trust for different forecasts, but forecasts involving prospect theory correction shows better correlation to users decisions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper asks a relevant question. Traditional calibration is usually considered as the de-facto measure of reliability in popular machine learning literature. However, machine learning prediction systems are not built in isolation and have major implications how they affect human decision systems. Thus, studying the usability of calibration to actual human subjects is an insightful research question.\n2. The introduction of prospect theory based post-hoc correction is also interesting to make the forecasts better aligned to human interpretations."
            },
            "weaknesses": {
                "value": "1. One of the crucial limitations of the paper is lack of thorough description of human study conducted. The paper claims that \"there is no reported difference in the level of trust reported by the participants\". However, without further information on the nature of instructions / guidelines provided to the human subjects, it could very well be the case that the subjects of this study behaved randomly (which is not an uncommon phenomenon, and is usually controlled for in user studies by designing good incentive mechanisms). The paper (in the current form) does not delve much deeper whether the measures were taken to control random behaviour.  My opinion is also informed by correlation in Figure 7, where the difference between random and calibrated / uncalibrated is not that different. \n\nOverall, I think the paper is interesting. However, due to the above concern, I'm hesitant to fully rely on the user study.  I'm happy to hear more from the authors."
            },
            "questions": {
                "value": "1. The paper misses some of the relevant literature on the implications of calibration to decision making [1,2].  \n\n\n\n[1] Benz et al. Human-Aligned Calibration for AI-Assisted Decision Making. \n[2] Rothblum et al. Decision-Making under Miscalibration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Reviewer_Yu3U"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699185501101,
        "cdate": 1699185501101,
        "tmdate": 1699636024060,
        "mdate": 1699636024060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6fMi25Sc8u",
        "forum": "XM7INBbvwT",
        "replyto": "XM7INBbvwT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission981/Reviewer_7xFz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission981/Reviewer_7xFz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes correcting calibrated confidence scores based on Kahneman and Tversky's prospect theory as to adjust confidence scores in line with how people perceive probabilities. For example, reporting a 80% confidence score as 90%, as per prospect theory, a 90% probability would be perceived as 80%. In a study with human participants, they compare the impact of their proposed approach against a calibrated model and 3 other baselines. While there is no significant difference in terms of reported trust between the models, the correlation between decisions and predictions increases for their approach compared to the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed idea of using prospect theory on top of calibration to help align human perception with the model's predictions is nice and seems novel.\n- The experimental results support the claim of the paper that using prospect theory together with calibration increases correlation of individuals decisions with the model's prediction."
            },
            "weaknesses": {
                "value": "- The methodological contribution itself is relatively small, the application of prospect theory to the problem is quite straightforward.\n- The study setting is somewhat limited in that the participants have to make decisions based on the predictions of the model only and have no other information available. This doesn't seem to be realistic in most assisted decision making scenarios, where the individual could ignore the model if they do not trust it and base the decision on their own knowledge (e.g., the tasks in Vodrahalli et al. 2022). It would be interesting to know if we can expect that calibration+prospect theory to also lead to higher correlation in such tasks where the individual has the same (or other/additional) information available as the model.\n- Some parts of the study design and the evaluation were unclear to me (see questions)."
            },
            "questions": {
                "value": "- For some parts of the evaluation it is unclear which data was used: Are the results of Table1 and Figure 2 from the data in the validation set? Was the test set used in the survey with the human participants?\n- It would be nice if the authors could point out earlier that the $\\gamma$ value chosen is not specific for this task. This was unclear when described in page 6 and only discussed much later in the conclusion.\n- It is interesting that, even though individuals reported to trust the random model less, the correlation of the random model's prediction with the individuals' decisions is higher than the calibrated and uncalibrated model's correlation (Figure 6a and 7). Do the authors have an intuition why this is the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission981/Reviewer_7xFz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699630338749,
        "cdate": 1699630338749,
        "tmdate": 1699636024004,
        "mdate": 1699636024004,
        "license": "CC BY 4.0",
        "version": 2
    }
]