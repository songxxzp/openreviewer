[
    {
        "id": "aeB1ZvGJAG",
        "forum": "ehSQZa4vuk",
        "replyto": "ehSQZa4vuk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_kJx4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_kJx4"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the issue of spurious correlations in reinforcement learning (RL), introducing the concept of policy confounding to formalize and analyze this problem. The authors provide theoretical insights, examples, and experimental results to demonstrate the effect of policy confounding, comparing different solutions and highlighting its impact on state representations in RL. The work aims to shed light on how an agent's policy can induce spurious correlations, potentially leading to representations that do not generalize well outside the trajectory distribution induced by the agent's policy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces and formalizes the concept of policy confounding, shedding light on a previously underexplored aspect of spurious correlations in RL. The theoretical framework, complemented by some examples, enhances our understanding of how policy confounding impacts state representations in RL. The manuscript is well-structured, providing a logical flow of ideas, and the writing is clear, making complex concepts accessible."
            },
            "weaknesses": {
                "value": "Clearly articulates the problem of policy confounding and relates it to issues like spurious correlations and generalization.\n\nProvides a formal theoretical characterization grounded in state abstraction frameworks and MDPs.\n\nIncludes pedagogical examples that effectively demonstrate policy confounding.\n\nExperiments confirm the theory and show when policy confounding is most problematic.\n\nHighlights an important generalization challenge in RL that has been overlooked."
            },
            "questions": {
                "value": "The paper argues policy confounding poses a distinct challenge from general RL generalization, but does not extensively benchmark against recent generalization methods like invariant risk minimization, data augmentation, dynamics randomization, robust policy learning, and meta RL. These approaches could be highly relevant given the claims about out-of-trajectory generalization. While theory and simple experiments demonstrate policy confounding, how confident are you this poses a problem not already addressed by the latest techniques for improving generalization in RL? Comparisons to some of these state-of-the-art methods could better situate your claims within the broader context of research on robustness and generalization.\n\nYou demonstrate policy confounding in simple domains. Do you have evidence this manifests in more complex, high-dimensional problems? Testing on complex benchmarks could better showcase significance.\n\nThe theoretical analysis introduces useful formalisms but is very dense. For readers less familiar with this notation, could you provide more intuitive explanations of key results like Proposition 1 and Theorem 1?\n\nWhile you propose some basic mitigation strategies, the paper does not offer concrete solutions. What directions seem most promising for future work to address policy confounding?\n\nThe distinction between out-of-trajectory and out-of-distribution generalization is somewhat unclear. Could you clarify this difference with explicit examples?\n\nHow does policy confounding relate to prior work on spurious correlations and generalization in RL? Are there clear differences in causes and solutions?\n\nYou cite causal representation learning as a promising direction for future work, but do not provide specifics on how these techniques could be applied to address policy confounding. Could you expand on how invariant risk minimization or other causal inference tools could help mitigate the effects you demonstrate? Are there any concrete steps you propose for integrating causal representations into solving this problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Reviewer_kJx4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762782503,
        "cdate": 1698762782503,
        "tmdate": 1699636269272,
        "mdate": 1699636269272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k5Fw6CwMzi",
        "forum": "ehSQZa4vuk",
        "replyto": "ehSQZa4vuk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_besR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_besR"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate if RL agents develop myopic, suboptimal habits that rely on spuriously correlated inputs to make decisions. They formally define policy confounding by considering minimal representations of a policy's marginal state distribution, and then showing that these minimal representations do not generalize, like if the policy gets teleported to a state outside of it. They evaluate on toy gridworld settings where confounding happens, and show that some strategies that increase the diversity of data experienced by the agent (off-policy, exploration, etc.) reduces confounding."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The study of generalization for RL is quite important.\nThe study of the training dynamics of deep RL is also quite important. \nThis paper addresses both such problems, since policy confounding can happen both during training and evaluation.\n\nOverall, I appreciated the theoretical definition of policy confounding, and find the way the authors choose to define it by considering the representations induced by a policy to be intuitive. However, there are several writing issues (see below) that I fear will make this paper's message miss the mark.\n\nThe experiments the author chose, while toy, highlight the policy confounding problem well. The proposed improvements, while not novel, are simple and intuitive for tackling confounding."
            },
            "weaknesses": {
                "value": "## Presentation\nOverall, the paper is very awkward for reading. The authors propose many definitions and setup notations in the early pages before finally arriving to the definition of policy confounding (Sec. 6). I would suggest for the authors to reformat the paper to first give a high level sketch or intuition of the definition, perhaps aided by a visual figure, and then explaining the numerous notations and definitions required for defining policy confounding. \n\nIn section 2, the authors provide a very lengthy (almost page long) textual description of policy confounding. This is very difficult to read, I would advise the authors to move explanations into visual aids, split up into paragraphs, highlight key steps, etc. \n\nIndeed, the authors have a tendency of writing long paragraphs and describing things step by step at a low level, rather than summarizing high level points, splitting points into multiple paragraphs, etc.  Section 6, Example 1 is almost impossible to read. I suspect most readers will not even bother reading this. \n\n## Experimentation\nThe authors can improve in two ways. First, they can connect their theory more to their toy experiments. For example, is it possible to come up with a toy environment where all possible representations can be enumerated and tracked? Then it could be interesting to see what kinds of representations RL algorithms learn, and if they indeed are minimal and do not generalize.\n\nNext, it would be interesting to investigate confounding in existing deep RL tasks, especially in well-known benchmarks (Atari, DMC, etc.), and to see if some RL algorithms are better than others. For example, would MBRL algorithms fare better or worse than model-free RL algorithms?"
            },
            "questions": {
                "value": "Can the authors improve the presentation? \n\nCan the authors think of experiments that analyze and connect to their theory more? \n\nCan the authors showcase policy confounding in non-toy tasks in deep RL (continuous states, actions, well-known benchmark, etc.)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790138040,
        "cdate": 1698790138040,
        "tmdate": 1699636269188,
        "mdate": 1699636269188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eVR02qePnz",
        "forum": "ehSQZa4vuk",
        "replyto": "ehSQZa4vuk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_aFjn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_aFjn"
        ],
        "content": {
            "summary": {
                "value": "The authors discuss policy confounding, the phenomenon where RL agents rely on spurious correlations for their policies.  They define and study this problem from a theory standpoint, and empirically evaluate several proposed solutions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is an interesting, significant topic that is worthy of study, and I think that, issues below aside, authors have a solid approach to the problem."
            },
            "weaknesses": {
                "value": "See questions below for a couple of clarity weaknesses.\n\nIn Example 1 (Section 6), X_t and x_t seem to not be defined when they are first used.  Later, it is stated that X_0 is the signal received at t=0, but even then, it is not clear how X differs from G.  These should be better defined.\n\nThe do operator is not well-defined.  Based on the informal description (just below Definition 9), I thought that it meant that there exists an s_t, such that we consider all possible values of Phi(s_t), and adversarially pick one to try to meet one of the conditions in Definition 9 if possible (even if this is the correct interpretation, it is not clear).  However, based on a few readings for Section 6.0, I suspect the correct interpretation is something different.\n\nIt is not clear to me how exactly Example 1 corresponds to the ideas before and after it (perhaps this confusion would be resolved by resolving the confusions above).  The implication appears to be that the two different L_8 positions under the optimal policy (i.e., the green and purple positions in Figure 1) are equivalent under some Phi.  But, per Definition 3, this is not how state representations work: we can discard some subset of the state to get our state representation, but I do not see how the two different L_8 positions can have the same state representation per that definition.  So either I am misunderstanding the implication (possibly due to the clarity problems mentioned above), or else there might be a fundamental error in the example or the definitions.\n\nAnother possibility that may explain some of my confusion is that L_t and l_t do not encode the full position, but instead only the timestep or the \u201chorizontal\u201d position.  If this is the case, this confusion is caused by more imprecise/incomplete definitions.\n\nJustifying/motivating the work: The examples given focus on the idea that we train on one MDP, and then evaluate on another.  This approach illustrates the issue well, but is too contrived to provide motivating examples that show why we should care about policy confounding in practice (and the experiments are all set up this way as well).  6.2 attempts to address this by discussing when we should worry in practice: function approximation and narrow trajectory distributions.  However, these two paragraphs are far too short and high-level to truly justify this work.  I believe that this is an interesting topic worthy of study, but I think this paper in its current form does a poor job of showing the reader that this is an interesting topic worthy of study.  Perhaps showing that policy confounding can be a problem in less contrived settings (for example, when the MDP or distribution of MDPs does not shift between training and evaluation) would help address this weakness.  Alternatively, another approach could be to focus a future version of the paper on a setting where the training and evaluation MDPs are inherently different, such as \"sim-to-real\" robotics.\n\nThe main theorem seems almost trivial, and the empirical work is based on extremely simple toy gridworlds.  So even aside from the issues above, the contribution may be a bit light."
            },
            "questions": {
                "value": "Starting at definition 2, the paper became difficult to follow.  Are \\Theta^1, \\Theta^2 random variables (RVs)?  For a while, I couldn\u2019t figure out what they were (I was thinking that they were sets like \\Theta, but that didn\u2019t make complete sense, and then I was thinking that there must be typos in the definition, before I realized that they might be RVs).  If they are RVs, a statement that they are RVs, as opposed to \\Theta, which is a set, would be helpful.\n\nThe \\cross_i notation was extremely confusing (I almost gave up on trying to understand the paper over this).  The interpretation I settled on for \\cross_i dom(\\Theta^i) is dom(\\Theta^1) \\cross dom(\\Theta^2) \\cross \u2026  Is this interpretation correct?  A definition would help clarify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3210/Reviewer_aFjn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818187004,
        "cdate": 1698818187004,
        "tmdate": 1700505377436,
        "mdate": 1700505377436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UDVYzwkxGD",
        "forum": "ehSQZa4vuk",
        "replyto": "ehSQZa4vuk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_BRdu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3210/Reviewer_BRdu"
        ],
        "content": {
            "summary": {
                "value": "The phenomenon of \"policy confounding\" is identified, where an RL agent may learn to discard important information in the state due its policy focusing only on some small subset of states. Some theory is developed to understand conditions where the effect may arise and experiments in toy domains illustrate the identified phenomenon."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The phenomenon of \"policy confounding\" is intriguing and novel. The presentation of the paper is great, the text is easy to follow and the figures are clear. Explanations are given in sufficient detail. \nThere are interesting connections made to a causality perspective and the theoretical framework used (considering a factored MDP) is well-chosen. The experiments clearly demonstrate the effect in a few illustrative environments.  \n\nOverall, I find the main idea of paper to be very interesting and potentially relevant in many situations where RL agents are trained."
            },
            "weaknesses": {
                "value": "The primary weakness is that the experiments focus on toy examples specifically designed to elicit the problem. It's not entirely clear if this problem is relevant in more realistic settings. Appendix C does contain examples of failures from previous works which may be related to policy confounding though.\n\nA slight weakness is that no mechanisms outside of conventional strategies (e.g. experience replay) are proposed to address policy confounding."
            },
            "questions": {
                "value": "I'd be willing to increase my score based on the responses to the following questions. \n\n- T-Maze, sec2. It could be more clear to mention why the avg return is around 0.2. Is this around the level that is to be expected? Why is there a little bump around step 30k for the eval env? \n\n- As an alternate demonstration, instead of adding the ice to the environment, why not simply place the agent directly on the state above (or below) the original starting location and see how it fares on the train env? Then, it would clear to see that if the agent always follows the trajectory it expects instead of using the color signal. We wouldn't need to introduce this additional mechanism of the ice or modify the transition function.\n\n- About the formal definition (def 9) of policy confounding:\n\tIt would be helpful to understand this definition by explaining how it applies to the T-maze.\n\tCould you clarify the meaning of the do() operator here from a mathematical point of view? \n\tWould it be the same as writing that the equality has to hold for all $s_t$ s.t. $\\phi(s_t)$ rather than only the ones visited by $\\pi$? \n\n- About \"Narrow trajectory distributions\". If the environment transitons leads to a diverse set of states, would we still observe policy confounding? What if the agent is incentivized to explore through exploration bonuses? Could this avoid the issue?\n\n- The paper makes a link between policy confounding and causality (or lack of it). Are there any tools from causality that could be used to address the problem?\n\n-  Is policy confounding inevitable to some extent? It seems to be a consequence of having policies focus on certain parts of the state space."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699055803121,
        "cdate": 1699055803121,
        "tmdate": 1699636269002,
        "mdate": 1699636269002,
        "license": "CC BY 4.0",
        "version": 2
    }
]