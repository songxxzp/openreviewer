[
    {
        "id": "tCSpwF0hRp",
        "forum": "w327zcRpYn",
        "replyto": "w327zcRpYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_7Rhx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_7Rhx"
        ],
        "content": {
            "summary": {
                "value": "The paper propose SUBER, a RL environment that applies LLM to simulate user behaviors. SUBER consists of several components including memory, preprocessing,  postprocessing, and LLM modules. The user history is sent to the RL module, and it returns an item. Both user history and the recommended item are processed as a prompt, then the LLM outputs the simulated user rating over the item. SUBER is built on two public datasets. The paper does experiments to validate its effectiveness. Finally, the paper shows that how A2C is trained in this environment."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.Using LLM to simulate user behaviors is interesting.\n2.The paper is easy to follow.\n3.The paper does detailed ablation study."
            },
            "weaknesses": {
                "value": "1.The paper does not discuss the limitation of SUBER. In my opinion, I think the input feature is quite simple, and miss numerical and sequential informations.\n2.The paper does not compare SUBER and other RL-based simulators, such as VirtualTaobao, RecoGym. Thus it is quite hard to evaluate the significance of SUBER in the RL4RS area.\n3.The paper does not cite two recent papers about RL4RS simulators, \"RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System\" and KuaiSim: A Comprehensive Simulator for Recommender Systems.\n4.As an RL environment, I would suggest that the author evaluate more RL algorithms besides A2C, such as SA2C(Supervised Advantage Actor-Critic for Recommender Systems), HAC(Exploration and Regularization of the Latent Action Space in Recommendation) and off-policy top-k(Top-K Off-Policy Correction for a REINFORCE Recommender System)."
            },
            "questions": {
                "value": "See the above question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698493359469,
        "cdate": 1698493359469,
        "tmdate": 1699636179552,
        "mdate": 1699636179552,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SApUPqx47g",
        "forum": "w327zcRpYn",
        "replyto": "w327zcRpYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed SUBER, a framework designed to address common challenges in RL-based recommender systems, such as issues related to data availability and the design of reward functions. The paper conducted several ablation studies on movie and book recommendations to demonstrate the effectiveness of the method and examine the effect of each component in the framework."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe motivation of this paper is clear and the challenges it aims to address are significant to the RL-based recommender system.\n-\tThe method is presented clearly. Each component is well explained, and the flow of the entire framework is well presented in the figure.\n-\tThe paper conducted a series of ablation studies to scrutinize the effect of different components within the framework."
            },
            "weaknesses": {
                "value": "-\tThe proposed  framework is to tackle key challenges in RL4Rec, such as data accessibility, the uncertainty of the user model, and the assessment of models. However, the originality of this research is ambiguous to me. It appears to predominantly integrate components of Reinforcement Learning (RL) with Large Language Models (LLMs). Furthermore, due to the absence of a thorough comparison with existing RL simulators and state-of-the-art techniques including RL4Rec and LLM-integrated RecSys, it's challenging to position the precise significance of the contributions claimed in this study.\n-\tThe paper has only made comparisons between different settings of SUBER on the metrics proposed in this paper. The comparisons with other methods on some commonly used metrics such as MAP/R^2/Personalization are missing. These comparisons would be essential to understand the benefits of using this method.\n-\tThe prompts used in the pre-processing module and the user description generation step require hand-crafted templates, which may limit the generalizability of the method in other scenarios.\n-\tThis method may require more computational resources than other methods due to the usage of LLM. More analysis and evaluations should be done. \n-\tThe authors failed to monitor significant existing literature on RL4Rec, including various methods and simulators, as well as RecSys integrated with LLMs. This oversight renders the paper's scope and credibility questionable."
            },
            "questions": {
                "value": "-\tThe paper claims that it addresses the challenge of model evaluation, is it referring to the evaluation metrics mentioned in Section 4.2 and Table 1-2? How do these metrics outperform the existing evaluation methods?\n-\tAs addressed in the weakness section, I think the comparisons between this method and other methods on metrics such as MAP/R^2/Personalization are essential to verify the effectiveness of the method. Could the authors provide these results?\n-\tI\u2019m not sure about the purpose of generating the user descriptions. The paper mentioned that the Age / Job / Hobbies of the users are randomly sampled from external distributions, how does this random information affect the outcome? And what\u2019s the motivation for doing so? An ablation study to compare the results with/without this information would be helpful.\n-\tIt\u2019s known that different prompt methods could affect the response of LLMs. How does the prompt template affect the outcomes in this framework? I suggest the authors try several different prompt templates in the pre-processing module and the user description generation step, then report the range of the results.\n-\tIn Fig 4a, I observed a significant performance drop somewhere between 1.6M - 1.7M steps, why does this happen? It seems not due to the randomness because this pattern is consistent across all embedding dimensions. Or more generally, I found the pattern of these three lines seems to be extremely similar, I\u2019m surprised by this because I suppose these are three independent experiments with different dimension settings. Is there any particular reason for the similarity between these three lines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698544110088,
        "cdate": 1698544110088,
        "tmdate": 1699636179480,
        "mdate": 1699636179480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KskYvuK3qX",
        "forum": "w327zcRpYn",
        "replyto": "w327zcRpYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_K35w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_K35w"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for training and evaluating RL-based recommender systems, which uses large language models (LLMs) to simulate human behavior and rate recommended items."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper proposes a user simulation framework based on large language models, which can alleviate the problems of data scarcity and model evaluation for reinforcement learning based recommender system.\n+ The paper designs a flexible and extensible environment based on reinforcement learning principles, which can interact with different LLMs and recommendation strategies.\n+ The paper provides a modular framework and open-source code, which is a valuable tool for the recommender system domain. It helps researchers and developers to train and evaluate reinforcement learning based recommender systems without real user interactions."
            },
            "weaknesses": {
                "value": "- The manuscript could benefit from more robust experimental support. The absence of comparisons with other simulation algorithms may impact the persuasiveness of the paper.\n\n- The paper appears to lack some key experiments that validate the effectiveness and advantages of RL training in the proposed environment. While the paper presents ablation studies on different components of the environment, it does not compare the RL-based recommender system with other baselines or state-of-the-art methods on real user data or benchmark datasets. It would be advantageous if the authors could include such experimental validations in future research.\n\n- The use of LLMs to generate synthetic users is an innovative approach that leverages the powerful capabilities of LLMs to simulate human behavior and preferences. However, the paper does not evaluate the quality and diversity of the user generation, nor does it compare it with real user data. This could potentially lead to biases and inaccuracies in the simulation. It would be beneficial if the authors could delve deeper into this aspect in future research.\n\n- The related work section of the paper seems too general and lacks precision. It does not adequately highlight the differences and connections between their work and existing research. It would be advantageous if the authors could elaborate more on the relationship and uniqueness of their work in relation to existing research, to enhance the depth and breadth of the paper."
            },
            "questions": {
                "value": "1.\tCan the author provide a distribution chart for scores predicted by LLMs and actual scores? The numbers reported in the table do not provide an intuitive image. On the other hand, it is more important to focus not on the overall score distribution, but on the differences in scoring for each item (it is possible for the overall score distribution to be the same, but with significant differences in individual item scores).\n2.\tWhat is the difference between the last two rows in Table 1? Is it a type error? \n3.\tComparing rows 3-8 in Table 1 with the 9th row, it can be observed that the model is very good at distinguishing between High and Low Ratings on a scale of 0-9. However, when the score scale changes to 1-10, there is a significant decrease in performance. Does this indicate that the model can only identify very poor items? (0 score)\n4.\tI suggest the author showcases the performance of several traditional models and conventional RL methods, under your reward metric (Figure .4a)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699108927660,
        "cdate": 1699108927660,
        "tmdate": 1699636179401,
        "mdate": 1699636179401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nCUH7K1COq",
        "forum": "w327zcRpYn",
        "replyto": "w327zcRpYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
        ],
        "content": {
            "summary": {
                "value": "The authors present a promising solution to the challenge of training recommender systems when real user interactions are not available. They propose SUBER, a novel Reinforcement Learning (RL) simulated environment tailored for recommender system training, which leverages recent advancements in Large Language Models (LLMs) to simulate human behavior within the training setting. A series of ablation studies and experiments demonstrate the effectiveness of their approach. This research represents a significant step towards creating more realistic and practical training environments for recommender systems, even in the absence of direct user interactions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe concept of employing Large Language Models (LLMs) to mimic synthetic users is intriguing. SUBER offers a multifaceted approach, generating synthetic data while harnessing the potential of LLMs to accurately emulate the behavior of users with undisclosed patterns.\n2.\tThe authors meticulously conduct comprehensive ablation studies to dissect the various components of LLMs, demonstrating the scalability and versatility of SUBER in the process.\n3.\tThe article is impeccably articulated, presenting its ideas with clarity and maintaining a logical flow throughout."
            },
            "weaknesses": {
                "value": "1.\tHow do the authors assess the accuracy of their simulated environment? The paper primarily showcases the training curve of the RL model within this environment but lacks a comparative analysis against other environment simulation methods. The absence of online experiments further challenges the validity of the simulated environment.\n\n2.\tThe authors should expound on their rationale for using LLMs to simulate users, clarify why this approach is effective, and outline the advantages it offers over alternative environment simulation methods.\n\n3.\tThe authors claim that this dynamic environment can serve as a model evaluation tool for recommender systems, but it lacks empirical evidence to support this claim. A clear methodology for measuring the accuracy of the proposed evaluation method is needed.\n\n4.\tThe absence of t-tests or error bars in the results section raises concerns about the reliability and reproducibility of the experimental findings.\n\n5.\tThe paper mentions the limitation of context length for providing a list of all possible items to an LLM. Further details are required regarding how the author addressed this particular issue."
            },
            "questions": {
                "value": "1.\tI suggest the authors evaluate the simulated environment from more aspects. To establish the accuracy of the simulated environment, consider conducting comparative experiments. Compare the performance of your proposed environment with existing methods for simulating user interactions. Additionally, performing online experiments where applicable, could help validate the authenticity of your simulated environment.\n\n2.\tI suggest the authors provide a more in-depth explanation of why LLMs are chosen to simulate users. Elaborate on the effectiveness of this approach by highlighting its advantages over alternative simulation methods. This could include discussing how LLMs can capture complex user behavior or adapt to changing patterns more effectively.\n\n3.\tTo substantiate the claim that your dynamic environment serves as a model evaluation tool, conduct experiments that demonstrate its utility in evaluating recommender systems. Present a clear experimental setup and results that support this assertion.\n\n4.\tEnhance the reliability and reproducibility of your experimental results by including t-tests or error bars. \n\n5.\tExplain in detail how you addressed the limitation of limited context length. What techniques or strategies did you employ to mitigate this constraint when using LLMs in your environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2439/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699677528249,
        "cdate": 1699677528249,
        "tmdate": 1699677528249,
        "mdate": 1699677528249,
        "license": "CC BY 4.0",
        "version": 2
    }
]