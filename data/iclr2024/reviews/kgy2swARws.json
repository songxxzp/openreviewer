[
    {
        "id": "3Glf6tJZLS",
        "forum": "kgy2swARws",
        "replyto": "kgy2swARws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_wDjU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_wDjU"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses a limitation in certain diffusion models, like DDIM, which use selective sampling during generation, potentially compromising sample quality by missing information from unselected steps. They introduce S2-DMs, which incorporate a $L_{skip}$ method to reintegrate this omitted information. S2-DMs improve sample quality, are easy to implement, require minimal code changes, and are compatible with various sampling algorithms. Experimental results on CIFAR10 and CelebA datasets demonstrate superior performance compared to DDIMs and PNDMs, with FID scores of 8.01/6.41 in 10 steps."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors propose to leverage the skip information to train the diffusion model due to the asymmetric property of the DDIM. \n2. The proposed method achieves comparable results in different datasets."
            },
            "weaknesses": {
                "value": "1. I cannot understand what does Figure 1 means in the paper. Does it represent for the trajectory of an individual sample $x_t$? What does *real* mean? What is the dashed line? What is the solid line? Is it a two-dimensional dataset? What is the x-axis and what is the y-axis?\n\n2. The additional training objective is scaled by *0.01*. I strongly doubt the effectiveness of the novel part of this paper.\n\n3. Many other fast sampling techniques [1,2,3] have been proposed in the last years. The author should consider to compare with them.\n\n[1] Qinsheng Zhang et al. \"Exponential Integrator\"\n[2] Bao Fan et al. 'Analytic-DPM'\n[3] Qinsheng Zhang et al. 'generalized DDIM'"
            },
            "questions": {
                "value": "1. Can authors compare with the DDIM which is only trained on the sampling time steps $t_i \\in [t_0, t_1,\\cdots, t_N]$. I am not convinced by this additional loss $L_{skip}$.\n\n2. Why does the performance get worse when the step increases as shown in Figure 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3479/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3479/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3479/Reviewer_wDjU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697840973410,
        "cdate": 1697840973410,
        "tmdate": 1700494626649,
        "mdate": 1700494626649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IQGKF5Ywc9",
        "forum": "kgy2swARws",
        "replyto": "kgy2swARws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_uLTx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_uLTx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Skip-Step Diffusion Models (S2DM), a technique to train discrete-time diffusion models which aims to alleviate the train-test mismatch seen by DDIM-style samplers. The authors propose a new loss function (Equation 12) which is the standard DDPM loss, except that the model output has been re-weighted. The authors take a linear combination of this new loss and the standard loss to train S2DM models. The authors study their proposed method empirically on CIFAR10 and CelebA, measuring FID scores and comparing to DDIM and PNDMs. Generally, Table 1 and Table 2 indicate lower FID scores for the proposed method than the baselines."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper studies an important and well-motivated problem, namely that of retaining sample quality in diffusion models while reducing the sampling time.\n- The proposed method is a straightforward modification of standard DDPM training and easy to implement"
            },
            "weaknesses": {
                "value": "- There are several inconsistencies and minor details which make the derivation in Section 3.2 hard to follow.\n     - The loss in Equation 7 should be an expectation over $x_0, \\epsilon$, and $t$.\n     - Should $p(x_{t-skip} | x_t)$ be $p_\\theta(x_{t-skip} | x_t)$? And should $q_\\theta(x_t | x_{t - skip}$ be $p_\\theta(x_t | x_{t - skip})$?\n- It was not clear to me how aligning the *forward* processes $q_\\theta(x_t | x_{t - skip})$ and $q(x_t | x_{t-1})$ would result in the model incorporating the \"skip\" information as claimed. For instance, these conditional densities rely on some *fixed* $x_{t-1}$ and $x_{t-skip}$ -- which themselves are random variables depending on an actual datapoint $x_0$, and so may be significantly different from one another (e.g. in Equation 8 and Equation 9).\n- The derivation of the loss (Equation 10) is not sound. The original DDPM loss (Equation 7) is *not* obtained by matching the forward transitions and matching terms as claimed, but rather by minimizing the KL between the reverse transitions (see Equation 5 in the DDPM paper). Moreover, the authors take their proposed derivation and modify it with another heuristic in Equation 12.\n- I am quite skeptical of the results in Table 1 and Table 2, given that the proposed change is such a small modification of the standard DDIM training procedure. These results would be significantly more convincing if the authors presented error bars with their FID scores. I also will note that the authors obtained significantly lower FID scores in Table 1 and Table 2 for DDIM than the original DDIM paper [2] (see Table 1 in [2]). If one compares the results for S2DM in the submission and the FID scores from Table 1 in [2], DDIM consistently outperforms the reported S2DM scores.\n\n### Minor Comments (that do not affect my score)\n- The terms $\\tau$ and $(1 - \\tau)$ should likely be swapped in Equation 13 to match Algorithm 1.\n- The reference to Watson et al. (2021) appears twice at the start of Section 2\n- Section 5, \"Relate Work\" should be \"Related Work\"\n- Section 5 lists several relevant pieces of work \"other innovative methods have been introduced to further refine DDPMs sampling, such as reverse SDEs with unique coefficients, \u201dcorrector\u201d steps, and probability flow ODEs\" but does not include a citation for these methods\n- Equation 3 should use \"\\log\"\n- Figure 3 \"stpe\" should be \"step\"\n\n\n\n### References\n[1] [Ho et al., Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n\n[2] [Song et al., Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2010.02502)"
            },
            "questions": {
                "value": "- Is there any clear way to interpret the proposed loss (Equation 12)? For instance, Equation 13 essentially says that we are training a model such that its output should match $\\epsilon$ and also its output scaled by $\\sqrt{1 - \\alpha_{skip}}/\\sqrt{\\alpha_{skip}}$ should simultaneously match $\\epsilon$ -- this is highly unintuitive to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807761667,
        "cdate": 1698807761667,
        "tmdate": 1699636300944,
        "mdate": 1699636300944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IKMyoAO10g",
        "forum": "kgy2swARws",
        "replyto": "kgy2swARws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_4Hkm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_4Hkm"
        ],
        "content": {
            "summary": {
                "value": "The authors suggests a skip-step loss function for training the versions of DDIM, which allows to considerably improve over the baseline training procedure on a standard generative modeling benchmarks at a price of small modifications of the training procedure. The proposed procedure also allows to enhance the sampling quality with the fixed number of sampling steps."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The suggested modification of the skip-step loss function is simple and appealing, and the experimental results shows considerable improvement over the baseline."
            },
            "weaknesses": {
                "value": "I can not point out any significant weaknesses of the submission."
            },
            "questions": {
                "value": "Is it possible to come up with a counterpart of the suggested skip-step loss function for the continuous-time models, not the ones, based on DDPM formalism?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699454590106,
        "cdate": 1699454590106,
        "tmdate": 1699636300847,
        "mdate": 1699636300847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "maFYNptOKt",
        "forum": "kgy2swARws",
        "replyto": "kgy2swARws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_2AL9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3479/Reviewer_2AL9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach to the training of diffusion models while using DDIM for sampling, that addresses the issue of asymmetry between the training process and the sampling process. It incorporates a new skip-connection loss, $L_{skip}$, which acts as a regularizing complement to the traditional score-matching loss. This method has been empirically validated, achieving impressive results on CIFAR10 and CelebA datasets and outstripping the performance of other leading algorithms, including DDIM and PNDM, in terms of FID score. The authors have also performed ablation studies to explore how varying the skip steps impacts performance. Remarkably, this method is straightforward, requiring minimal code alterations, and is versatile enough to be integrated with other sampling techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The algorithm is novel, simple in design, and achieves superior performance when sampling using few steps."
            },
            "weaknesses": {
                "value": "The formulations and notations have typos that might lead to confusion on the methodology. Furthermore, the performance enhancements requires extra training due to the added regularization on the original objective. Consequently, it remains ambiguous whether the improvements seen in DDIM are a result of rectified asymmetry between training and sampling or if they stem from a more precisely trained score function owing to the constraints imposed on the relationships between steps."
            },
            "questions": {
                "value": "- In (9), why is $q(x_t | x_{t - 1}) = \\sqrt{\\alpha_t} x_{t - 1} + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$? The original process for adding noise shoud have $\\sqrt{1 - \\alpha_t}$ instead of $\\sqrt{1 - \\bar{\\alpha}_t}$ for the noise term?\n- Moreover, why are we matching $q(x_t | x_{t - skip})$ with $q(x_t | x_{t-1}$, but not $q(x_t | x_{t-1}$? The derivatives of $L_{skip}$ between (8) - (12) is confusing.\n- Do you have any intuition on why the relationship between skip-step information and the quality of the sampling step is not strictly symmetric? As described in Figure 3.\n- How the balance between $L_0$ and $L_{skip}$ (the value of $\\tau$) affect the performance?\n- With the regularized term, is the training cost larger than training original DDPM? \nI am curious whether the trained score also perform better when sampling using DDPM?\n\nMinor comments:\n1. The average value of $L_0$ is approximately 80-100 times larger than that of $L_{skip}$, however in (13) when setting $\\tau = 0.99$, $L_0$ would be 10000 larger than $(1 - \\tau)L_{skip}$.\n2. Caption in Figure 2, line 3, additional \"the\" in \"the the\"\n3. Caption in Figure 3, 'stpe'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699581370968,
        "cdate": 1699581370968,
        "tmdate": 1699636300788,
        "mdate": 1699636300788,
        "license": "CC BY 4.0",
        "version": 2
    }
]