[
    {
        "id": "DdECDcxoL9",
        "forum": "SNGANmQPLv",
        "replyto": "SNGANmQPLv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_y2C2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_y2C2"
        ],
        "content": {
            "summary": {
                "value": "This work explores the impact of different instruction formats on the in-context learning ability of MLLM. The authors propose a UMIT method, which introduces task definition and transfers the multimodal instructions of diverse tasks in a unified style. A retrieval-based approach is also proposed to enhance exemplars sampling."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is the first to investigate the impact of different instruction formats on the in-context learning ability of MLLM.\n2. The proposed UMIT outperforms OpenFlamingo and Otter in terms of performance on the MME Benchmark and SEED-Bench."
            },
            "weaknesses": {
                "value": "1. This work mainly focuses on instruction formats, lacks innovation, and fixed instruction formats are difficult to generalize to open-domain tasks.\n2. When testing, does UMIT require using ChatGPT to obtain the task definition for each new sample? This can result in significant inference costs for the unseen task.\n3. The description of the details of UMIT, especially the use of symbols, is somewhat confusing. For example, what does the text encoder encode in exemplar sampling? And where does X_{instruct}^{i} come from in section 2.5?\n4. The experimental results on SEED-Bench show a significant improvement when training OpenFlamingo and Otter on the data collected by the authors. In contrast, the gains from changing the format seem less pronounced. This raises a question of whether the role of data diversity is much greater than the task definition proposed by the author.\n5. Some experimental results on VizWiz contradict the author's conclusions and should be analyzed in more detail. For example, in Table 4, \"DEIQA (F3) mixed\" performs worse than \"DEIQA (F3) random\". And in Table 5, \"vqa+diff\" performs worse than \"vqa+same\"."
            },
            "questions": {
                "value": "1. In Otter / MMICL, what is the difference between \"instance-level instruction\" and \"question\"?\n2. Do other ICL methods harm zero-shot performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645179936,
        "cdate": 1698645179936,
        "tmdate": 1699637128595,
        "mdate": 1699637128595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F8hptlKGIQ",
        "forum": "SNGANmQPLv",
        "replyto": "SNGANmQPLv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_srth"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_srth"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to find a better vision-language instruction format for in-context learning. Based on the existing components of [examples, image, instruction, question, answer] in instruction format, the authors further introduce the task definition as the prefix of the instruction. During the test, the authors also explored different types of exemplar sampling methods. The models are compared with the previous OpenFlamingo and Otter models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem of enhancing the in-context learning capabilities of the vision-language model is important.\n- The proposed method of adding a task definition is very simple."
            },
            "weaknesses": {
                "value": "- The contribution and novelty of this work is insufficient. Adding a carefully designed task definition with minor improvements (0.3 in Tab3) is not that significant in terms of technical contributions or scientific findings.\n- Although the paper uses a mix of many existing datasets for training, the evaluation is limited to a few benchmarks.\n- The writing is not that good. For example, the name Octopus is not introduced in the paper, which is confusing. And some typos need to be revised.\n- The literature review in Sec2.1 is not correct, e.g., \"Moreover, Otter (Li et al., 2023b) and MMICL (Zhao et al., 2023b) further introduced\". Otter was before Qwen-VL."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676744506,
        "cdate": 1698676744506,
        "tmdate": 1699637128459,
        "mdate": 1699637128459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WTCcwjbk1I",
        "forum": "SNGANmQPLv",
        "replyto": "SNGANmQPLv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Unified Multimodal Instruction Tuning (UMIT), a framework to suggest how to construct a text-image interleaved instruction dataset by merging diverse visual instruction datasets in a unified multimodal instruction format. The experiments are based on OpenFlamingo. This paper also studies the impact of different components in multimodal instruction formats."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed approach seems reasonable for certain tasks; studying how the format of the multimodal instruction will affect the in-context learning performance is also an interesting topic. The experiments show positive results on the benchmarked datasets and tasks."
            },
            "weaknesses": {
                "value": "One concern the reviewer has is that: the strategy of using uniform instruction styles and defining tasks clearly helps with tasks we already know well. However, this method assumes we can list and describe every possible task type, which might not be practical. The real world is full of unexpected and varied tasks, and a model that's too focused on a set of specific tasks might struggle to adapt to new or different ones. This over-specialization could limit the model's usefulness in a wider range of real-world situations.\nAlso, if the reviewer understands it correctly, the task definition for each instruction-exemplar pair is manually crafted, which might introduce empirical errors/bias, and might not be scalable enough."
            },
            "questions": {
                "value": "How will the quality of the manually crafted task definition affect the overall performance? Have the authors tried efforts to automate this process to make it maybe more scalable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698903051564,
        "cdate": 1698903051564,
        "tmdate": 1699637128356,
        "mdate": 1699637128356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PBUex302yD",
        "forum": "SNGANmQPLv",
        "replyto": "SNGANmQPLv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_4eXe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8964/Reviewer_4eXe"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new format for multimodal instruction tuning that includes a task definition prompt into the input context for multimodal models that can do in-context learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The contribution of retrieval to select the best exemplars for in-context learning is interesting, and the results in Table 4 show the benefit of retrieving relevant in-context examples.\n\n- The main contribution of \"task definition\" does not seem to be particularly novel, but the results in Tables 3 and 4 do seem to indicate the benefit of using task descriptions."
            },
            "weaknesses": {
                "value": "- The main contribution here seems to be the new \"task definition\" component of the prompt that precedes the in-context examples, which does not seem particularly novel, and looking at the examples in Figure 1, I don't really see what information they provide that is not provided in the instance instruction itself -- the task definition just seems like a more verbose form of the instance-level instruction.\n\n- The retrieval augmentation in Section 2.4 requires us to have a reasonably large database of training examples to choose from, which may enhance task performance on benchmarks but defeats the purpose of in-context learning in the real world.\n\nOne ablation experiment I would suggest is to use the task-definition prompt without the instance-level instruction (DEQA)."
            },
            "questions": {
                "value": "- In equation 5, is the text embedding E_text computed using the question X_q, answer X_a or both? this is not made clear"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699048487283,
        "cdate": 1699048487283,
        "tmdate": 1699637128249,
        "mdate": 1699637128249,
        "license": "CC BY 4.0",
        "version": 2
    }
]