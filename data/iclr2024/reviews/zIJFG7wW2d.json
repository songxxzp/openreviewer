[
    {
        "id": "mmhOh99ijk",
        "forum": "zIJFG7wW2d",
        "replyto": "zIJFG7wW2d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_tm2h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_tm2h"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach where Large Language Models (LLMs) are repurposed as agents during reasoning, enhancing the autonomy of the question-answer process. Specifically, the proposed agent-based reasoner autonomously generates task-specific instructions without the need for training, which subsequently aids the LLM in reasoning over the question. The methodology's efficacy is showcased across various tasks (generation, classification, and reasoning) and on models of differing scales, such as Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. Notably, the results are commendable."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The innovative AgentInstruct autonomously streamlines prompt engineering. It significantly reduces the human effort required in designing chain-of-thought exemplars while maintaining high reasoning prowess.\n \n2. AgentInstruct exhibits versatility, as evidenced by its robust performance across LLMs of various scales (13b, 70b, and ~200b), indicating that it isn't limited to a specific scale.\n\n3. The method's adaptability extends to different tasks, including generation, classification, and reasoning, suggesting it isn't merely task-specific.\n\n4. The community stands to benefit immensely from the authors' decision to release the code, as it paves the way for effortless result reproduction and broader application of the method."
            },
            "weaknesses": {
                "value": "1. Although the methodology promotes a higher degree of autonomy in the question-answer process, its implementation might be deemed intricate by some, which could be a potential shortcoming (on the contrary, the implementation of simple methods such as CoT is easier).\n\n2. Implementing the method could demand higher computational throughput from the LLM, leading to increased computational costs or API fees.\n\n3. While there's a notable enhancement in the performance of cutting-edge LLMs \u2014 Vicuna-13b by 13.3%, Llama-2-70b-chat by 23.2%, and GPT-3.5 Turbo by 17.0% \u2014 the performance increment isn't as pronounced with larger models like GPT-3.5 Turbo as it is with Llama-2-70b-chat. This raises concerns about the method's scalability with even larger models. The absence of data on its application to GPT-4, which is larger than GPT-3.5, further intensifies this curiosity about the method's scalability."
            },
            "questions": {
                "value": "Please see weaknesses. I would like to update my evaluation after the discussion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698462240811,
        "cdate": 1698462240811,
        "tmdate": 1699636626004,
        "mdate": 1699636626004,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n8qyAXygfu",
        "forum": "zIJFG7wW2d",
        "replyto": "zIJFG7wW2d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_LvbF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_LvbF"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces AgentInstruct which generalizes zero-shot reasoning abilities of LLMs. Here, the agent generates instructions which optimize the reasoning process of LLMs. The proposed approach is compared with zero-shot and zero-shot-CoT as baselines and reports 17.8%  and 6.5% average improvement across the tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The originality of the paper is mainly rooted in its  proposed approach being a cost-effective alternative to zero-shot CoT as the instructions can be generated using a bigger more costly model while the reasoning LLM can be a cheaper alternative. This is a valid argument highlighted by the authors."
            },
            "weaknesses": {
                "value": "The agents used a much bigger and powerful LLM, GPT-4, as the default agent to generate the instructions. However, the models which were evaluated on the proposed approach were limited to Vicuna, Llama-2-chat, and GPT-3.5 Turbo. As GPT-4 is missing from the list of models, the evaluation against baseline approaches (zero-shot and zero-shot-CoT) is not a fair comparison because they have not utilized the power of GPT-4. This flaw in experiment design, unfortunately, invalidates the reported results.\n\nSimilarly, the agent is given access to search engines to get the \"top releant web pages containing information about the dataset\". This added information provided to the agent (GPT-4 model) is an extra context which the baseline approaches (zero-shot and zero-shot-CoT) don't have access to. Consequently, the comparison between the proposed AgentInstruct and the baseline methods is not a fair comparison.\n\nIn other words while the approach decouples the \"instruction generation\" from \"reasoning\", the \"instruction generation\" step is utilizing extra context and more powerful model which reasonably contributes to the higher end-to-end performance of the approach. \n\nIn the \"Ablation Study\" the authors have evaluated the impact of removing each of the components of the Zero-Shot AgentInstruct to assert that they are all effective. At the same time this ablation study further highlights the impact of GPT4 in the overall performance of the method (Table 1). \n\nFurthermore as demonstrated in Figure 6, the ZeroShot GPT-4 only marginally lacks behind the Zero-Shot Agent-Instruct (79.5 vs 88.1 or 0.6% improvement) with lower cost ($1 vs. $2). This is a very important observation which further invalidates the sanity and effectiveness of the proposed method because a much simpler approach (i.e., ZeroShot which is the simplest of approaches) is demonstrating almost similar results as the very complicated Zero-Shot AgentInstruct. In the same section the authors mention that \"Though ReAct narrowly outperforms\nzero-shot AgentInstruct\" where narrow corresponds to 0.8% improvement which is larger than the 0.6% improvement of Zero-Shot AgentInstruct over the Zero-Shot GPT-4."
            },
            "questions": {
                "value": "In table 1, please elaborate on the first 3 settings: w/o Agent Instructions, w/o Input Examples and w/o Labels. Reading the manuscript and the paragraph that follows Table 1, it is not fully clear what each of these components represent. Particularly the \"Labels\" have not been discussed in the manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5899/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5899/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5899/Reviewer_LvbF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698516910954,
        "cdate": 1698516910954,
        "tmdate": 1700544061186,
        "mdate": 1700544061186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L6zz0nqKPD",
        "forum": "zIJFG7wW2d",
        "replyto": "zIJFG7wW2d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_JX3F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5899/Reviewer_JX3F"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to improve the zero-shot performance of LLMs on language tasks. The proposed approach involves automatically generating task-specific instructions (using an LLM based agent) that are provided to the reasoning LLM to improve its performance. The task-specific instructions might include steps to break down the task into simpler steps, information about the format of the expected answer. To generate these task-specific instructions, the agent takes as input information about the dataset (sourced from the web) and a few examples from that task. Using this extra context, the authors show that LLMs can reason better about the task and show improvement across several generation and classification based benchmarks. On average they show 6.5% improvement over a chain-of-thought baseline, and 17.5% average improvement over state-of-the-art LLMs like Vicuna, LLaMa2, and GPT-3.5-Turbo."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper demonstrates the effectiveness of their approach on a fairly exhaustive evaluation. The approach is evaluated on 29 datasets spanning generation, classification and reasoning. The evaluation is run using three LLM baselines.\n- The authors provide exhaustive details about the prompts, evaluation dataset and experiments in the supplementary for easily replicating their experimental setup. The authors also provide code to run the experiments done in the paper.\n- The authors also attempt to do error analysis to better understand failure modes of their approach. I specially enjoyed reading Section 3.6 and Appendix C.1"
            },
            "weaknesses": {
                "value": "- Evaluation vs \u201cIn-the-wild\u201d generaliztion: While the paper shows strong improvements across various benchmarks, I am not convinced of the fundamental assumption made in the paper. To improve a performance on the task, the authors assume that the task is one of the \u201cstandard\u201d tasks LLMs are typically evaluated on. While this is great for evaluating, its unclear how the proposed approach will work \u201cin-the-wild\u201d. In realistic usecases, the task might be completely different than one of the standard usecases. Additionally, we might not know beforehand which benchmark does the task belong to? In that case, it\u2019s not going to always be possible to generate task-specific instructions.\n- Second, while the authors claim that the performance of the method is zero-shot, I believe that showing a few labeled examples in the prompt to the agent responsible for generating task-specific instruction is a bit unfair. A fair approach would be to make these examples be available even to other zero-shot baselines. Interestingly, without Input Examples, the approach takes a severe hit (see Table 1, Row 1 vs Row 3).\n- Finally, I also believe that cost comparisons done in Figure 6 are a bit misleading. Here, the authors show that Zero-shot AgentInstruct cost ~2$ vs 20$ (ZS-COT GPT4). This is true when running evaluation on standard benchmarks because we have to generate task-specific instructions once per benchmark. But like I said before, this is not going to be true in-the-wild. In that setup, a new instruction will have to be generated for every unique user query. For completeness, and to be consistent with a realistic use-case, it would be more accurate to include the costs of creating task-specific instruction to each instance of the task, and then show the overall cost."
            },
            "questions": {
                "value": "Related to points raised in the weaknesses section: \n\n- How will the approach work in-the-wild, when the source of task is not known apriori. That is we don\u2019t know which dataset does the task belong to, or if the task belongs to any dataset at all.\n- What is the cost of creating the task-specific instruction. Can you add a column to Figure 6, which adds this cost multiplied by number of instances in the dataset plus the additional cost of reasoning using GPT-3.5 Turbo?\n- I also didn\u2019t understand the point of experiments in Section 3.3? What is the insight from that experiment?\n- I also didn\u2019t fully understand how the agent uses the question-answering API (Section A.3.1) to add context about the task? I think it\u2019s one of the most crucial steps of the pipeline, and it\u2019d be great to explain how it\u2019s implemented in more detail (and perhaps in the main manuscript). Concretely, how are the retrieved documents to generate the instruction? Are the retrieved documents added as prompt to the agent to generate task-specific instruction? Is that in addition to the name of the dataset, task information and few input examples? Why are they added to a vector database?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5899/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956341988,
        "cdate": 1698956341988,
        "tmdate": 1699636625773,
        "mdate": 1699636625773,
        "license": "CC BY 4.0",
        "version": 2
    }
]