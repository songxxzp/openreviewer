[
    {
        "id": "j0UoH8rTLH",
        "forum": "5HpZZbgdeK",
        "replyto": "5HpZZbgdeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_75D8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_75D8"
        ],
        "content": {
            "summary": {
                "value": "This paper propose tot improve the current calibration methods by convert it into a binary case under the one-versus-all setting and demonstrate that reformulating the confidence calibration of multiclass classifiers as a single binary problem significantly improves the performance of baseline calibration techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It study the shortcome of many post hoc calibration problem and provide a better loss to improve post hoc calibration method."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. This work provide a good and easy to improve most post hoc method, however it seems too simple. I would say it is more like a part of a post hoc method paper although the author give comprehensive experiments.\n2. It would be better to include more metrics other than ECE.\n3. I would suggest the author to include the TvA into training time calibration to see if it works."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5749/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5749/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5749/Reviewer_75D8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697783033159,
        "cdate": 1697783033159,
        "tmdate": 1699636603492,
        "mdate": 1699636603492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ct2hucSB7b",
        "forum": "5HpZZbgdeK",
        "replyto": "5HpZZbgdeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_TXd7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_TXd7"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the calibration of multi-class classifiers trained to discriminate many classes. The proposal consists in using a binary top-versus-all approach: the calibration problem is transformed into providing a confidence estimate regarding whether the prediction made by the classifier is correct. The authors first provide a succinct state-of-the-art on calibration approaches, then present the classifier calibration problem, and present their contribution. Experimental results are provided, before the paper briefly concludes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is overall written in a clear and understandable manner, and is pleasant to read. \n\nThe results displayed in the Experiments section are good and show that the proposal is interesting."
            },
            "weaknesses": {
                "value": "The contributions of the paper seem somehow rather restricted: the proposal consists in recasting the calibration problem into a binary problem (i.e., adjusting the level of confidence in the prediction issued by the classifier); there is no theoretical study. The proposal is not really formalized. \n\nThe state-of-the-art does not include a number of works on classifier calibration, which may have been interesting to include in the discussions and in the experiments (see, e.g., Venn predictors). \n\nSome parts in the paper are redundant\u2014for instance, Sections 2 (related work) and 3 (problem setting) are tightly connected and may have been merged into a single one. Section 4 also mentions some related work which could have been presented and discussed previously. The notations are sometimes inconsistent (e.g., the authors use small x's and y's as well as capital ones interchangeably; as well, they indistinctly use \"one-versus-all\", \"one-vs-the-rest\", etc.)"
            },
            "questions": {
                "value": "In Section 2, page 3, you mention \"more advanced methods\": can you be more specific ? As well, when referring to Gupta and Ramdas (2022) which first defines the top-label calibrator, their work should be presented with more details (here or in the \"Problem setting\" section) as it is highly connected to the proposal. \n\nIn Section 3.2: \"This discretizes the probability.\" This sentence is a bit clumsy; can you clarify ? \n\nIn Section 3.3, you may also mention that the ECE is not a proper scoring rule. This also questions its use as a metric for assessing calibration performance. Could you provide any insight regarding this ? \n\nSection 4 should be improved. In its present state, it is hard to see what is exactly the proposal. In particular, I think that the proposed approach should be clearly and formally stated (and not only via Algorithm 1), notably by explicitly formulating the criterion used to replace Equation (1)\u2014this would clarify the difference with the former top-vs-all proposal by Gupta and Ramdas (2022). \n\nIn Section 4.1, could you elaborate on \"minimizing the cross-entropy loss increases the probability of\nthe correct class (thus only indirectly decreasing the confidence), but minimizing the binary cross-entropy\nloss directly decreases the confidence\" ? \n\nIn its current state, Section 4.2 is short, which is regrettable since it addresses the more important part in the paper\u2014the properties of the proposal. The argument that \"the [proposed] reformulation [of the top-vs-all approach uses] the full calibration dataset\" could be discussed: then, the positive and negative classes are imbalanced (and heavily imbalanced in the case of numerous classes), which may degrade performances. Can you discuss this porential issue ? As well, the sentence \"the classifier's prediction and accuracy are unaffected\" is unclear; in addition, if the classifier's predictions are left unchanged compared to the one-vs-all case, it also means that your approach cannot improve the classifier's accuracy by righting erroneous decisions: can you elaborate on that ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5749/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5749/Reviewer_TXd7",
                    "ICLR.cc/2024/Conference/Submission5749/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687813139,
        "cdate": 1698687813139,
        "tmdate": 1700665748255,
        "mdate": 1700665748255,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0k9kN6VMVo",
        "forum": "5HpZZbgdeK",
        "replyto": "5HpZZbgdeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_fZjD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_fZjD"
        ],
        "content": {
            "summary": {
                "value": "Paper proposes confidence calibration for multi-class classification as a single binary classification problem using top-vs-all approach. This gives ability to calibrate large number of classes with scarce per-class data, and the usage of binary cross-entropy loss with regularisation term. Benchmark image datasets are utilised to evaluate the proposed approach, showing stability in classification accuracy and calibration improvements against existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Paper proposes sound yet simple idea which improves the existing calibration approaches. It includes good set of experiments and evaluations to show the usefulness of proposed techniques. As model-agnostic approach, it would be possible to apply the algorithm to different existing neural network models and post-processing calibration techniques. This is an interesting idea that could bring some new knowledge to the field, especially from the practical view of uncertainty calibration."
            },
            "weaknesses": {
                "value": "Background and literature review could be in a more compact form. For now, it is repeated in many sections making the follow of the presentation a bit hard: Otherwise it is clearly written and structured. Paper lacks some of the analysis and discussion of the proposed approach and results in a broader sense. Also, it has limited discussion of the results in relation to practical utilisation of approaches, i.e., which of the proposed combination of algorithms should be selected in different scenarios from practitioners' perspectives. From empirical point of view, it would strengthen the paper, if additional dataset from other than image domain would be considered."
            },
            "questions": {
                "value": "- References lacks some details, please add all the relevant information to cited work (also for ArXiv pre-prints)\n- What would be your conclusions or \"rule of thumb\" of selecting particular algorithm (i.e., calibration method with TvA) from the practitioners' point of view for certain applications or classification problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764381855,
        "cdate": 1698764381855,
        "tmdate": 1699636603251,
        "mdate": 1699636603251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WlXvXtBAIi",
        "forum": "5HpZZbgdeK",
        "replyto": "5HpZZbgdeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_JkUP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5749/Reviewer_JkUP"
        ],
        "content": {
            "summary": {
                "value": "The authors provide a conceptually simple and practical technique for the post calibration of trained models that could scale to the case of\nmany classes: they reduce the problem to one binary calibration problem (and not many calibration sub problems, such as often is the case in prior work). The paper contains a good discussion of prior work, and presents many empirical experiments and comparisons on vision data sets with up to 1000s of classes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Calibration, or obtaining good or reliable probabilities, is an important task in many areas of machine learning.  Classification into many classes is challenging and occurs often in practice. The authors present a simple problem formulation and reduction that can be practical: that of calibrating the probability assigned to the highest scoring class (the 'confidence'). The paper is fairly clear, and many experiments\nand in particular comparisons with other techniques are presented. The authors motivate their approach well (in particular, efficiency\nconsiderations)."
            },
            "weaknesses": {
                "value": "A major issue is weak novelty or contribution.  Another important issue (but somewhat secondary) is that the paper clarity is\nsomewhat poor too. I'll give a  summary below and then expand on these in the 'Questions' section.\n\nMain issue with contribution: one would think in any practical application of calibration, one would be interested in good\nprobabilities assigned to top candidates, not just the very top (to make good decisions based on the classifications), but the authors\nonly focus on the very top in their development of the technique and evaluations (if I am not mistaken, and to keep the solution and the\nevaluation simple..). Using the other scores should improve the calibration too.  I believe this severely limits the current contribution, and more research and development of the approach is required to make the paper a technically strong contribution."
            },
            "questions": {
                "value": "[roughly in order of importance]\n\nWith many candidate classes given an instance (eg 100s to 1000s), it\nis understandable that one may not want to assign good probabilities\n(waste time/resources on) on all the candidates, and focusing on the\ntop is well motivated (the issue of sparsity of data, for training or\ncalibrating per class, is understandable as well). However, it is also\nnot advisable to throw out all the information (all the scores\nassigned to the classes), except the top (or the winning) class. For\ninstance, the spread (closeness) of the scores can be very\ninformative. Furthermore, in any plausible application of calibration\nin this setting, for instance in subsequent decision theoretic\nactions, plausibly one wants to know the probabilities assigned to the\nother, top few, classes as well.\n\n\n- not clear how binary methods (such as HB or Iso) are used alone, without TvA for\n calibration.. (eg in Table 1) TvA is used on the top score.. but use of the binary\n methods to this multiclass setting is not clear to me in the experiments.. I don't think the authors explained this.. and then\n the authors explain that the binary methods perturb the decision of the original classifier, etc. \n\n- What is a reference for \"I-Max binning\".. I believe it is first mentioned on page 7 (from my searching the paper..), and it scores\n very well.. (Table 1, on Imagenet dataset/models) Also: why include it if the probabilities can sum to more than 1.0 with this method ?\n (for some evaluation scores such as log loss, perhaps for ECE too, this could lead to cheating by a method...)\n\n- the authors use 'confidence' (eg on page 4 when they say 'beyond\njust considering confidence'), but they define it in passing on page 3\nas 'the confidence is the top class probability' (top probability as\nopposed to the probability assigned to other, non-top,\nclasses)... promote it or highlight this technical definition better\n(because confidence is a generic term, but here in this paper, at\nleast from this point on, it has a more technical meaning... at least\nafter page 3!). For example, the use of 'confidence' in the abstract\n(used 3 times) reflects the more generic meaning ...\n\nother clarity comments:\n\n- Intro is vague, for instance, in \".. to predict the true\nprobability of a good decision, i.e., their accuracy.\"  What is a\n\"good decision\"? (is it committing to one class or label, for a given\ntest instance, and the label turning out to be correct? 'accuracy'\noften has a technical term in machine learning, which is one minus\nzero-one error, or the proportion of test instances correctly\nclassified.. so if the proportion is 80%, do we want the model to\nalso always assign a fixed 80% to its classifications? or a\nprobability that is more fine-grained than that (not fixed at\n80%.. which can simply be obtained from cross-validation!) ).. I am\nguessing the latter .. perhaps quick/short examples would clarify the statements.  Also the\ndistinction between 'uncertainty quantification' (at the beginning of\nintro) and providing good probabilities or calibration is not clear\neither (the techniques are mentioned with citations, but more\nexplanations would be useful).\n\n-  could drop 'in our work' in \"We are interested in our work in..\"\n\n- drop 'process' in 'a complementary post-processing\n calibration process'..\n\n- In Related Work section, not sure what 'less complex than the other\n ones' means in the long sentence: 'the problem of confidence calibration, less complex\n than the other ones' (in what ways were the aforementioned citations\n more complex?)\n\n- page 5: \"We notice \" to \"We note \" (the former implies you have\n observed something, in your work/experiments, etc. while the latter\n means you want the reader to note or observe something, and that's\n what you mean)\n\n- change \"one-vs-the-rest approach\" to  \"one-vs-rest approach\"?\n\n- the semantics of probability P() in equation 1 of 3.1 is not clear\n (in the sense of how it is computed, ie in what way or on what\n probability space, or how is it empirically estimated.. ) ... although\n the example you give afterwards helps. Perhaps insert \"(when computed\n on unseen or test instances)\" in \"the probability of being correct\n when the confidence is ..\", so it becomes \".. the probability of\n being correct (when computed on unseen or test instances) when the\n confidence is ..\"\n\n\n- Top-versus-Rest (instead of Top-vs-All) ? (I understand one-vs-all\n is commonly used instead of one-vs-rest, and this follows a similar\n pattern)\n\n\n- 3.3, page 4: the presentation/description of ECE should be\n    improved, perhaps by a quick example ..\n\n-  replace 'size' (in 'equal mass or equal size') with 'width'\n perhaps? as 'size' is ambiguous: it could mean bin extent or width,\n or number of points or instances (whose score fall) in the bin (what\n is meant by bin mass, I believe, is number of instances in the bin).."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773278162,
        "cdate": 1698773278162,
        "tmdate": 1699636603104,
        "mdate": 1699636603104,
        "license": "CC BY 4.0",
        "version": 2
    }
]