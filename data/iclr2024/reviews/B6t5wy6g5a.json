[
    {
        "id": "88nadVhIwW",
        "forum": "B6t5wy6g5a",
        "replyto": "B6t5wy6g5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_XL3H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_XL3H"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an innovative approach to address multimodal misalignment in Large Multimodal Models (LMM) by adapting Reinforcement Learning from Human Feedback (RLHF) to vision-language alignment. The proposed Factually Augmented RLHF method enhances the reward model with factual information, improving performance and mitigating reward hacking issues."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Multimodal Misalignment Addressed: This study effectively tackles the issue of multimodal misalignment in Large Multimodal Models by adapting Reinforcement Learning from Human Feedback to vision-language alignment, improving model performance.\n2. Factually Augmented Reward Model: The proposed Factually Augmented RLHF method enhances the reward model with factual information, which not only improves performance but also mitigates reward hacking issues in RLHF.\n3\u3002 Performance Improvement: By training the model with augmented data and utilizing RLHF, this approach achieves remarkable performance gains."
            },
            "weaknesses": {
                "value": "1. The authors proposed that the evaluation indicates that this benchmark dataset aligns well with human evaluations, especially when scores are adjusted for anti-hallucinations. I didn\u2019t find how to get this conclusion in the experimental section or other sections.\n2. The method section requires an overall overview of the relationships between the various parts of the method.\n3. Are answers and questions in the MMHAL-BENCH dataset written by humans or constructed by automatic methods?"
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Reviewer_XL3H"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697992531908,
        "cdate": 1697992531908,
        "tmdate": 1699636397228,
        "mdate": 1699636397228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "67RxSVHbqi",
        "forum": "B6t5wy6g5a",
        "replyto": "B6t5wy6g5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_Y7nK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_Y7nK"
        ],
        "content": {
            "summary": {
                "value": "This paper is a vision-language paper with three contributions:\n- introducing LLaVA-RLHF, a vision-language model trained via reinforcement learning from human feedback (RLHF) to improve multimodal alignment\n- developing Factually Augmented RLHF to mitigate the problem of reward hacking in RLHF by leveraging additional factual information, such as image captions\n- proposing an evaluation benchmark for hallucination, MMHAL-BENCH   \n\nConcretely, the paper starts with tuning the LLaVA model using existing vision-language datasets (e.g., VQA-v2 and Flickr30k). Then, the 10k preference data was collected by human annotators, selecting the preferred response among two responses. Next, the reward model for RLHF is trained on the collected preference data. Unlike the vanilla reward model that utilizes the preference data, the proposed method (i.e., factually augmented RLHF; Fact-RLHF for short) injects additional information (e.g., image captions) into the reward model. So, Fact-RLHF requires additional information in both training and inference. Finally, the instruct-tuned LLaVA model is optimized to generate the responses having maximum rewards.\n\nThe paper verifies the effectiveness of the proposed method on three existing benchmarks (MMBench, POPE, and LLaVA-B) and a new benchmark, MMHAL-B."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(S1) Application of RLHF to the vision-language domain is definitely a novel direction.   \n  \n(S2) The paper conducts a wide range of experiments to demonstrate the usefulness of Fact-RLHF on diverse benchmarks.\n\n(S3) The paper presents a new benchmark dataset, MMHAL-BENCH. \n\n(S4) The paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "(W1) I could NOT find direct evidence that the proposed method reduces hallucinations and mitigates reward hacking. As shown in Table 6, LLaVA-RLHF did not significantly differ from LLaVA-SFT in the hallucination rate. LLaVA-RLHF even falls behind LLaVA-SFT in the 13B model. More convincing evidence or a detailed analysis is required that Fact-RLHF truly improves hallucinations. I am not expecting the response that LLaVA-RLHF just outperforms LLaVA-SFT on human alignment benchmarks (i.e., LLaVA-Bench and MMHAL-BENCH).\n\n(W2) Fact-RLHF works when additional human-annotation information (e.g., image captions and OK-VQA rationale) is available. It implies that we can\u2019t use Fact-RLHF when such additional information does not exist. The paper did not study more realistic scenarios where additional information is unavailable or noisy. \n\n(W3) The paper did NOT elaborate on experimental setup (e.g., descriptions of each dataset and task & evaluation metrics), assuming that all readers are familiar with such setup. I could find some details only for MMHAL-BENCH in the appendix.\n- Descriptions of each benchmark and task (e.g., basic statistics, data format, what should the model predict?)\n- Evaluation protocol for each benchmark (e.g., which evaluation metric is used and how to compute the results)"
            },
            "questions": {
                "value": "Please see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4292/Reviewer_Y7nK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698209044898,
        "cdate": 1698209044898,
        "tmdate": 1699636397152,
        "mdate": 1699636397152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FnjIOKGEyk",
        "forum": "B6t5wy6g5a",
        "replyto": "B6t5wy6g5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_vv7w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_vv7w"
        ],
        "content": {
            "summary": {
                "value": "This work performs RLHF for the vision-language alignment of LMM models. This work proposes a new alignment algorithm called Factually Augmented RLHF to reduce the hallucination in vision-language tasks. It also contributes a new benchmark named MMHAL-BENCH to evaluate LMM\u2019s performance with a focus on hallucinations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work is one of the first applications of RLHF to improve the multimodal alignment of LMMs. This is an inevitable next step for the LMM research.  \n\n2. It proposes a new hallucination benchmark named MMHAL-Bench.\n\n3. This work shows that RLHF indeed improves the performance of the LLaVA model in LLaVA-Bench and MMHAL-Bench."
            },
            "weaknesses": {
                "value": "1. The key issue of this work may be lack of details, as the method section only covers high-level procedures.\n- RLHF is notoriously difficult to reproduce.\n- No information is given about labelers, such as how many annotators participate in, who they are, and how they are recruited, etc. \n\n2. More experimental results are desired, as this work reports the final performance only.\n- Is there any quantitative evaluation that focus on how much hallucination reduces? \n- It would be better to include some experimental results about the performance of the reward model. \n\n3. The supplementary file is useless for reviewing. Only a github link is provided, but it is not accessible."
            },
            "questions": {
                "value": "1. FACT-RLHF is sourced from QA pairs in the LLaVA, A-OKVQA and VQA-v2. Are they any other datasets to get more samples? \n\n2. How much does it cost to collect FACT-RLHF annotations? How long does it task? How many labelers are involved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work exploits large-scale human feedback to learn AI models, but much details are not shown in the draft. Thus, it is not possible for reviewers to understand any ethical issues that this work may involve."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490484030,
        "cdate": 1698490484030,
        "tmdate": 1699636397053,
        "mdate": 1699636397053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zDLEXmwK8s",
        "forum": "B6t5wy6g5a",
        "replyto": "B6t5wy6g5a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_Kjps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4292/Reviewer_Kjps"
        ],
        "content": {
            "summary": {
                "value": "## Summary \n- Introduces a new multimodal large language model - LLaVA-RLHF for improved multimodal alignment. Make a variety of enhancements at various stages of the pipeline.\n\t- Instruct Tuning stage:\n\t\t- Augments the vision synthetic instruction tuning dataset (LLaVA dataset 98k conversations + 60k conversations for preference modeling) with human annotated multi-modal data in conversation format by using the VQA-v2, A-OKVQA dataset (converted to multi-round QA task) and Flickr30K dataset (converted to spotting captioning task) to train LLaVA-SFT+ model\n\t- Preference Modeling:\n\t\t- They collect preference dataset by creating a set of varied questions covering 12 main object categories from COCO and 8 different task types. This is released as MMHAL BENCH\n\t\t- This human preference dataset is collected for 10K held out examples from LLaVA\n\t- RL Tuning:\n\t\t- Finally they using RL tuning where they propose a new alignment algorithm called Factually Augmented RLHF for improved multimodal alignment.\n\t\t\t- Reward model training\n\t\t\t\t-  The key idea in Fact-RLHF is to use additional information from existing datasets (e.g. image captions from the coco dataset/rationales from A-OKVQA dataset). This additional factual data is provided to the reward model both at training and inference time.\n\t\t\t- RL Tuning\n\t\t\t\t- RL tune using PPO on the 10K held out examples collected from LLaVA\n- The model improves by 96% LLaVA-Bench dataset  compared to text-only GPT-4 which is better than 87% improvement of previous methods.\n- The model improves by 60% on the MMHAL-BENCH benchmark developed in the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## Strengths/Weakness\n\n- This is the first LMM trained with RLHF\n- Gets SOTA results for LLaVA-Bench and MMHAL-BENCH \n- The RLHF model degrades slightly on the capability benchmarks. Can you please cite the reference to the scaling law of LLaVA-RLHF mentioned in the paper?"
            },
            "weaknesses": {
                "value": "## Strengths/Weakness\n\n- This is the first LMM trained with RLHF\n- Gets SOTA results for LLaVA-Bench and MMHAL-BENCH \n- The RLHF model degrades slightly on the capability benchmarks. Can you please cite the reference to the scaling law of LLaVA-RLHF mentioned in the paper?"
            },
            "questions": {
                "value": "## Questions/Clarifications\n\n- Slightly confused by the data used for RLHF-RL Tuning. AFAIU the authors collect human preferences on 10k hold out LLaVA dataset. But how do you get the factual information for these examples (captions or rationals) for these required by the reward model?\n- Can you please clarify how is the image captioning data from COCO converted to instruction tuning data and is it also used for RL Tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764528963,
        "cdate": 1698764528963,
        "tmdate": 1699636396981,
        "mdate": 1699636396981,
        "license": "CC BY 4.0",
        "version": 2
    }
]