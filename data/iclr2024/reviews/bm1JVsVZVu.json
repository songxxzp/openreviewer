[
    {
        "id": "LoQjZaXWCN",
        "forum": "bm1JVsVZVu",
        "replyto": "bm1JVsVZVu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_w58H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_w58H"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a gradient-free black-box multi-objective optimization (MOO) method with dynamic weighting of objectives, leveraging concepts developed in the gradient based multi-objective optimization literature. The proposed method achieves this by deriving a gradient-free estimates for the objective gradients with respect to the parameters of the search distribution, and solving for dynamic weights for each objective calculated based on the aforementioned estimates. The authors provide relationship between the solutions to the original MOO problem and the solutions of the Gaussian smoothed MOO problem, which the proposed method find. The authors further provide theoretical convergence guarantees for the proposed method, and provide empirical validation for the validity of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper provides a black-box multi-objective optimization algorithm in the stochastic setting with theoretical convergence guarantees, which is a novel contribution to my knowledge.\n\n* The paper provide empirical evidence for the efficacy of the proposed method over existing methods using synthetic and real world benchmarks.\n\n* The paper is fairly easy to read, and the ideas are presented well."
            },
            "weaknesses": {
                "value": "* The proposed method assumes full access to the objective function, while in most applications (like in multi-task learning setup used in the experiment setup), full evaluation of objectives might not be possible or might be costly. The theoretical guarantees provided does not seem to cover this. However, the authors provide empirical evidence for the proposed method in this setting.\n\n* The setting considered might be not suitable for very large parameter spaces (i.e. when d is very large), which is the case when the decision variable is represented by a deep neural network.\n\nMinor comments:\n\n* The Pareto stationarity defined in Proposition 2.3. may not be a necessary condition for Pareto optimality when $\\mathcal{X} \\subset \\mathbb{R}^d$ (Consider when there is a global minimum for all the objectives at a boundary of  $\\mathcal{X}$)"
            },
            "questions": {
                "value": "* How doe the proposed method scale with the size of the parameter to be learned?\n\n* How sensitive is the proposed method to the stochasticity of the objectives (i.e. when the average over all data-points is estimated by the average of over a mini batch)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Reviewer_w58H"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770838935,
        "cdate": 1698770838935,
        "tmdate": 1699636840533,
        "mdate": 1699636840533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RrTWWWJ2Ow",
        "forum": "bm1JVsVZVu",
        "replyto": "bm1JVsVZVu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_ZfhH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_ZfhH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a MGDA-type gradient-based algorithm for solving the problem of black-box multi-objective optimization. Inspired by the stochastic gradient approximation method in black-box optimization, the objective of MOO is replaced by the expected loss where the decision is sampled under some parametric search distribution. Then the MOO problem is transformed into the problem of finding the optimal search distribution with minimal expected loss. When assuming the search distribution to be Gaussian, the gradient of the distribution parameters (mean and variance) is accessible, making it possible to apply gradient-based MOO algorithms. The relation between the two problems and the convergence rate are analyzed. Numerical experiments are conducted to verify the effectiveness of the proposed algorithm compared to evolutionary algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of applying stochastic gradient approximation method to black-box MOO problem sounds novel.\n\n2. This paper is technically sound. The analysis on the relation between the original black-box MOO problem and the smoothed version in convex/non-convex cases is interesting. \n\n3. This paper is well written in general and easy to follow."
            },
            "weaknesses": {
                "value": "1. The related work section only includes evolutionary methods and BO for multi-objective black-box optimization. The relevance to previous works on MOO/stochastic gradient approximation should also be clarified. A detailed discussion of technical difficulties in applying stochastic gradient approximation to black-box MOO may help better highlight the significance and technical novelty of this paper. \n\n2. The experiment considers evolutionary algorithm as the only baseline, which seems insufficient. There are also other major branches of black-box MOO algorithms, e.g., multi-objective Bayesian optimization [Zhang & Golovin, 2020] or multi-objective bandits, that should be compared."
            },
            "questions": {
                "value": "1. The proposed method assumes that the covariance matrix of the search distribution is a diagonal matrix. As I understand, this assumption indicates that the different dimensions of x are independent. Can you justify this assumption? I wonder if there is any performance gap with/without using this assumption.\n\n2. The Monte-Carlo sampling approach for calculating the expectation may bring additional computational complexity to the proposed algorithm. Can you analyze the complexity or provide some empirical results on the real running time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833377531,
        "cdate": 1698833377531,
        "tmdate": 1699636840397,
        "mdate": 1699636840397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BMM9mgA2Iv",
        "forum": "bm1JVsVZVu",
        "replyto": "bm1JVsVZVu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_J6Rt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_J6Rt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a ASMG algorithm for black-box multi-objective optimization. It is the first to design a stochastic gradient algorithm for black-box MOO with a theoretical convergence guarantee. The authors also prove the convergence rate for the proposed ASMG algorithm in both convex and non-convex cases. Empirically, the proposed ASMG algorithm achieves competitive performances on both toy examples and CLIP models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think the studied topic of black-box MOO problem is very important, especially with the emergence of  large language models, multi-modality models. This is a good timing to investigate on this topic given that the most popular APIs we use (ChatGPT, etc.) are only accessible for inference.\n\nTo the best of my knowledge, this paper is the first one that designs a stochastic gradient algorithm for black-box MOO. Moreover, it comes up with a theoretical convergence guarantee. For convex case, it possesses a convergence rate $\\mathcal{O}\\left(\\frac{\\log T}{T}\\right)$. For non-convex case, it has a $\\mathcal{O}\\left(T^{-\\frac{1}{2}}\\right)$ convergence rate to reach a Pareto stationary solution.\n\nI also think the CLIP example is very important. This shows that the authors do not only want to test the proposed algorithms on toy examples. They actually would like to use it on those state of art algorithms, which I really appreciate it."
            },
            "weaknesses": {
                "value": "think this paper can be improved in several ways:\n\n1. More discussions about more related works, such as recent KDD papers about one important application of multi-objective optimization in learning to rank https://dl.acm.org/doi/abs/10.1145/3580305.3599870 and https://dl.acm.org/doi/abs/10.1145/3580305.3599482\n\nOne more example is the lack of discussion about the seminal work in MOO for Multi-task learning https://arxiv.org/pdf/1810.04650.pdf\n\n2. How likely does the assumption that the covariance matrix \u03a3 is a diagonal matrix hold?"
            },
            "questions": {
                "value": "1. How likely does the assumption that the covariance matrix \u03a3 is a diagonal matrix hold?\n\n\n2. I'd like to see how likely the assumptions in Section 4, especially the convex case, hold? For example, in 6.1 SYNTHETIC PROBLEMS, the"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863088168,
        "cdate": 1698863088168,
        "tmdate": 1699636840289,
        "mdate": 1699636840289,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JSNJGGOfHK",
        "forum": "bm1JVsVZVu",
        "replyto": "bm1JVsVZVu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_hUW7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7111/Reviewer_hUW7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel algorithm called Adaptive Stochastic Gradient Algorithm for Black-Box Multi-Objective Learning (ASMG) to optimize multiple potentially conflicting objectives with function queries only. The authors provide the detailed discussion of the original MOO and the corresponding Gaussian-smoothed MOO and provide theoretical proofs for the convergence rate of the algorithm. The authors also demonstrate the effectiveness of the ASMG algorithm on various numerical benchmark problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Originality: The authors provide a novel approach to black-box multi-objective optimization with the new concept of \"adaptive\" stochastic gradient approximation that has a theoretical convergence guarantee.\n\n2. Quality: The paper provides a rigorous theoretical analysis of the proposed algorithm, such as proofs of convergence rate. The authors also demonstrate the effectiveness of the algorithm on multiple numerical benchmark problems, showing that it outperforms the ES method in terms of convergence speed and solution quality.\n\n3. Clarity: The paper is well-written and easy to follow, with clear explanations of the algorithm and its theoretical properties.\n\n4. Significance: The proposed method has the ability to optimize multiple objectives simultaneously with function queries only, which is the scenario of many MOO-based learning problems, such as tasks involving large language models that are only allowed for access with APIs."
            },
            "weaknesses": {
                "value": "1. In the introduction, lack of discussion on advantages of the Gaussian-smoothed MOO, compared with the traditional method. \n2. Lack of comparison with more recent state-of-the-art methods. The authors only compare their algorithm with the ES-based method. \n3. Limited discussion of the algorithm's computational complexity and runtime."
            },
            "questions": {
                "value": "1. How does the proposed algorithm compare to other black-box multi-objective optimization methods? Since only comparison with the ES-based method is discussed in the paper. \n2. What is the computational complexity and runtime of the proposed algorithm compared with other black-box multi-objective optimization methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7111/Reviewer_hUW7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699194163694,
        "cdate": 1699194163694,
        "tmdate": 1699636840192,
        "mdate": 1699636840192,
        "license": "CC BY 4.0",
        "version": 2
    }
]