[
    {
        "id": "7VFODJBsY0",
        "forum": "1vCnDyQkjg",
        "replyto": "1vCnDyQkjg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_JJQ3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_JJQ3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents UniHSI, a unified framework for Human-Scene Interaction (HSI). The framework aims to support versatile interaction control through language commands, providing a user-friendly interface. The authors propose a definition of interaction as a Chain of Contacts (CoC), which represents the steps of human joint-object part pairs. UniHSI consists of a Large Language Model (LLM) Planner that translates language prompts into CoC task plans and a Unified Controller that executes the plans. The framework enables diverse interactions, fine-granularity control, and long-horizon transitions. The authors collect a dataset ScenePlan for training and evaluation, and experiments demonstrate the effectiveness and generalizability of the framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The proposed method is the first to generate long-horizon and open-domain human-scene interactions.\n- The proposed method outperforms existing baselines in both success rate and contact error.\n- The method proposed to use an LLM Planner to generate a sequence of contact pairs (Chain of Contacts) as an intermediate representation, and parse that into a reward function for learning a unified controller. I find this solution both intuitive and elegant."
            },
            "weaknesses": {
                "value": "The paper does not provide a thorough explanation of the method. Specifically, it is unclear how different cases are handled by the LLM Planner, and how each type of interaction pair is translated to the reward. See questions."
            },
            "questions": {
                "value": "- In supp tbl 1, example 1 step 2 pair 2: The OBJECT is chair and PART is left knee. Is this a typo or a design choice?\n- what does {none, none, none, none, front} and {bed, none, none, none, front} mean, and how does the UniHSI controller use this information?\n- In supp tbl 1 example 2, why is the direction between mattress and body parts \u201cdown\u201d and the direction between pillow and head \u201cup\u201d?\n- Can the LLM Planner, or a quick extension of the LLM Planner support fine-grained spatial relationships such as distances and directions between objects? For example, can this method handle cases where 1) a chair is placed too closed to a table so that it is impossible to sit on it,  and 2) two chairs are placed facing opposite directions, and the agent must choose the correct one to watch tv?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Reviewer_JJQ3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2155/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652789098,
        "cdate": 1698652789098,
        "tmdate": 1699636148736,
        "mdate": 1699636148736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sKHerBeAjr",
        "forum": "1vCnDyQkjg",
        "replyto": "1vCnDyQkjg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_YGEs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_YGEs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to interact with virtual scene interfaces via language commands. Its proposed UniHSI prompts an LLM to translate user commands into (human part, touched object) sequences. An example is \"play video games\" -> (chair, seat surface, pelvis, contact, down)  \n-> (table, keyboard, left hand, contact, down) -> (table, keyboard,\nright hand, contact, down). They are then executed by a second model.  \nThis modularity allows modeling more interactions than before, and doing to automatically. The authors also propose a new dataset, Sceneplan, which combines PartNet and ScanNet into the above sequences of multiple interactions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The new Sceneplan dataset could be useful for the community, and the different difficulty levels can help drive the community\n* The UniHCI method is the first that generalizes to much more interaction combinations than previous methods (this is claimed by the authors, I am not inside the field enough to judge it) ## Weaknesses\n* The evaluation of the UniHSI method would benefit from more ablations on the design choices (loss, which module leads to the error, used motion style datasets) and comparison to more SOTA baselines (here, the authors blame code unavailability and too different training setups)\n* The proposed new method (UniHSI) is only evaluated on the proposed new dataset (Sceneplan) which both use the LLM Planner module. This may give an unfair advantage.\n* The Sceneplan dataset was automatically generated, although the involved LLM Planner in UniHSI gives much lower performance than humans (57.3 vs 73.2, \"Failures typically involved incomplete planning and out-of-distribution interactions\"). This may lower the dataset quality severly.\n* Most sentences in the paper are hard to parse, mostly because of over-complicated words (see Questions). This can be fixed in a (major) revision.\n* The reference section is inconsistent: Six different spellings for CVPR; Harvey 2020 misses where it was published; Zhang 2022a misses the arxiv identifier; in Tevet 2023 ICLR should not be referenced to as only \"ICLR, 2023\". This can be fixed in a revision.\n* The paper may be more applicable to computer vision conferences, judging by the references and by the fact that it is heavily practical/conceptual. This potential weakness can be better judged by the AC and Senior AC."
            },
            "weaknesses": {
                "value": "See Questions below."
            },
            "questions": {
                "value": "In Table 3, is there no other SOTA method that allows combinations of Sit, Lie Down, and Reach (other than the AMP vanilla baseline)? Maybe other reviewers from the field could also comment.\n\nExamples of hard-to-parse sentences:\n* \"The framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions.\"  -> These are three sentences in one.  \nHow about \"Interactions intuitively describe contacts between human joints and object parts. Thus, we define sequences of such contacts as Chain of Contacts (CoC).\"\n* \"the initial effort involves the uniform definition of different interactions. We propose that interaction itself contains a strong prior in the form of human-object contact regions.\" -> It is confusing to use \"uniform\" in a non-statistical way when the surrounding text contains priors etc.. What do you mean by uniform throughout the text?\n* \"To facilitate the unified execution, we devise the TaskParser as the core of the Unified Controller.\" -> Please use simpler language.\n* Section 3.2 is easier to read. It would be great to write more like in this section.\n\nIn how far is the discriminator $D$ adversarial and what effect does this have?\n\nIn Equation 7, instead of using exp to ensure positivity, you could consider using Softplus, which is sometimes more numerically stable.\n\nPlease analyse the dataset quality in more detail.\n\nPlease format the reference section coherently.\n\nPlease add the Limitations from the Appendix to the main paper\n\nIt would be an option to split the dataset from the method. This way, you could focus more on either of them. E.g., a big multi-round dataset with high quality would be a great contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2155/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674953621,
        "cdate": 1698674953621,
        "tmdate": 1699636148661,
        "mdate": 1699636148661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RlSbNZQTwQ",
        "forum": "1vCnDyQkjg",
        "replyto": "1vCnDyQkjg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_tgmd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_tgmd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to generate physical human-scene interactions from language inputs. The method is capable of generating multiple tasks with a single model. \n\nA large language model (LLM) is used to translate the language prompt (e.g go sit on the chair) into a plan of multiple sequential steps (e.g. go closer to the chair, make contact between hip and seating area in the chair). Each step is defined as the contact between a human joint and an object part. The paper calls this: chain of contact.\n\nThe authors derive observation and reward based on the contact pair. A humanoid agent is trained using AMP (Peng et al. 2021) to maximize the sum of rewards for all the steps specified by the LLM. \n\nTo train the humanoid agent, the authors collected a dataset of plans by prompting the LLM in different scenarios. These scenarios require access 3D scene data (i.e. object meshes, scene layout, object parts)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is novel, tackles an important problem, provides interesting results, opens the door for exciting future directions, is easy to read, and is technically sound.\n- The main novelties of the paper are:\n\n1. Using LLM as a task planner. i.e. using LLM to convert textual prompts into smaller executable steps.\n2. Representing all the steps as contact pairs. i.e chain of contact\n3. Unifying the design of the observation and reward for all tasks.\n\n- The method is capable of generating multiple interactions with the same model."
            },
            "weaknesses": {
                "value": "- The motion quality looks worse than previous works like InterPhys, AMP, or NSM. \n- The paper mentions the use of prompt engineering to generate the plan data. It is not entirely clear how was this done. How difficult was this? How sensitive is the model to changes in the prompt? I think this process involves several steps which were not discussed in detail. Thus I think it will be hard to reproduce this part.\n- The LLM expects examples of a task plan with each prompt. It is unclear how many examples were needed in total. How were these examples generated? manually?\n- The comparison in Table 3 seems weak. I understand the difficulty since the code for InterPhys is unavailable. However, I still have many questions about the comparison. How many test objects were used and how many trails were run? Did you use similar numbers as the ones used by InterPhys Hassan et al 2023? In addition, the performance of AMP seems quite low. InterPhys is an extension of AMP so I expect AMP to perform better than the provided numbers. I suspect the model has not been trained properly.\n- The method does not handle interaction with moving objects.\n\nMinor weaknesses\\comments for improvements\n- There are three recent(possibly concurrent) works that use LLM as an agent planner. It would be helpful to the reader to discuss the differences between the proposed methods and their approaches. \n1. Athanasiou et al.\"SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation\"\n2. Brohan et al.\"RT-2: New model translates vision and language into action\"\n3. Rocamonde\u2020 et al. \"VISION-LANGUAGE MODELS ARE ZERO-SHOT REWARD MODELS FOR REINFORCEMENT LEARNING\"\n- The authors argue for the value of language input. Language is useful for some applications but it is not necessarily a better or more user-friendly interface. For many users, a joystick, brush, and fingers might be more comfortable. Especially in games or digital design.\n- The contact pair contains 5 elements. Then it is not a pair. Maybe it is better to be called a contact tuple or a set.\n- It would be nice to see an example of when the human plan succeeded but the LLM one. It would be useful to put both plans side by side"
            },
            "questions": {
                "value": "- Can this approach be applied to kinematic models? What would need to change?\n- What is the importance of having a reward for the \"not care\" contact?\n- What is the difference between $\\hat{d_k}$ and $\\bar{d_k}$? I don't see how the directional part of the reward in Eq.7 enforces the desired behavior.\n- In Table 2. How are the weights chosen without adaptive weights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Reviewer_tgmd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2155/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686799295,
        "cdate": 1698686799295,
        "tmdate": 1699636148591,
        "mdate": 1699636148591,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UshyGbF7B6",
        "forum": "1vCnDyQkjg",
        "replyto": "1vCnDyQkjg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_gSqi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2155/Reviewer_gSqi"
        ],
        "content": {
            "summary": {
                "value": "The paper contributes the first unified framework that can generate physically plausible HSI motions in static scenes given compounded language commands. To examine the method's effectiveness, the paper constructs a new dataset containing thousands of interaction plans on existing 3D scenes. Experiments show that the proposed framework outperforms baselines on different single-step tasks, and also exhibits its ability to handle hard task plans and good generalizability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The proposed framework is unified without task-specific designs. It can directly support language instruction input without manual task decompositions.\n\n(2) The method achieves impressive performance on unseen real-world scenarios, indicating its good generalizability.\n\n(3) The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "(1) The evaluation metrics aim to examine whether a method can complete the task. However, both the Success Rate and the Contact Error are defined on $c_k,d_k$ that are generated by the LLM Planner. If the LLM Planner fails to generate reasonable chains of contacts or contact pairs, the evaluation may be misleading.\n\n(2) The current evaluation focuses on contact information that only relates to local human body parts. I think it is also important to examine the global reality of the generated motion. The evaluation for motion reality may include whether the motion is human-like and whether it is semantically faithful to the language command.\n\n(3) In the definition of task observations, the human joint $v_k^j$ corresponds to the nearest object surface point $v_k^{np}$. However, $v_k^{np}$ may not be a reasonable contact point for $v_k^j$. Understanding object geometries and affordances is important to finding ideal contact areas that human prefers.\n\n(4) The method is limited to static scenes."
            },
            "questions": {
                "value": "* In Figure 5, there are some interactions among different human joints such as \"cross the leg\" and \"lean forward in meditating\". How to model such interactions in the method?\n\n* In the method design, the style reward examines the reality of every two adjacent frames. I am curious whether it is sufficient without evaluating longer frame ranges that include more integral human motion semantics."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2155/Reviewer_gSqi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2155/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817362590,
        "cdate": 1698817362590,
        "tmdate": 1699636148503,
        "mdate": 1699636148503,
        "license": "CC BY 4.0",
        "version": 2
    }
]