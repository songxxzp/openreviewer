[
    {
        "id": "oJTHjEBYPY",
        "forum": "AAxIs3D2ZZ",
        "replyto": "AAxIs3D2ZZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_LzbJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_LzbJ"
        ],
        "content": {
            "summary": {
                "value": "This paper compares Reinforcement Learning from AI-generated intermediate Feedback (RLAIF) with RLHF in summarization and dialog generation tasks. It also investigates techniques to improve AI-generated preference alignment."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is well-organized, making it easy to follow.\n2. It demonstrates RLAIF\u2019s comparability to RLHF in specific tasks and provides optimal settings, offering a more cost-effective solution for AI alignment\u2014a significant contribution given the experiment\u2019s high cost and urgency."
            },
            "weaknesses": {
                "value": "1. The study only uses non-public \"palm2\" models, reducing its credibility. Including open-source models could strengthen its validity.\n2. The tasks are confined to summarization and dialog generation. Exploring additional areas like QA, code generation, or translation could provide a more comprehensive understanding of AI and human feedback interactions."
            },
            "questions": {
                "value": "Incorporating an exploration of widely used algorithms like Proximal Policy Optimization (PPO) could enrich the study\u2019s findings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762185880,
        "cdate": 1698762185880,
        "tmdate": 1699636332495,
        "mdate": 1699636332495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iNMt9fqGS4",
        "forum": "AAxIs3D2ZZ",
        "replyto": "AAxIs3D2ZZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_RhkW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_RhkW"
        ],
        "content": {
            "summary": {
                "value": "This paper compares the efficacy of reinforcement from human feedback (RLHF) and reinforcement from AI feedback (RLAIF) on the PaLM 2 XS model, for the tasks of summarization and \"helpful dialogue generation\" (Bai, et al. 2022). The paper found that when using a PaLM 2 Large model as the AI labeler, the performance of RLHF and RLAIF were similar. The paper also includes a number of experiments on the use of chain-of-thought in the AI labeler, the size of the AI labeler, and the number of feedback examples in RLHF/RLAIF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is generally clear and well-written, although see below for some framing suggestions. The analysis of the amount of human/AI feedback needed to reach maximum performance is interesting, as is the effect of chain-of-thought and self-consistency on the alignment between AI and human annotations. The qualitative analysis also hints at an interesting topic, namely that RLHF and RLAIF-trained models may be optimizing slightly different objectives; however, it does not give a full treatment to this topic (again, see below for more comments)."
            },
            "weaknesses": {
                "value": "It should be unsurprising that a weaker model (PaLM 2 XS) can be improved based on feedback from a larger model (PaLM 2 L). As noted by the authors in Section 2.2, this can be viewed as a distillation result. While I believe the paper still has some valuable insights, I wish it would be more clear upfront (e.g., in the abstract) that the RLAIF setting described within is one where the target model and the labeling model differ in size.\n\nThe qualitative analysis in Section 5 is relatively shallow and would benefit from some additional justification: (1) can you provide more conclusive evidence that these trends exist, e.g,. with human labelers?; and (2) can you provide a hypotheses as to why these trends occur? For example, given that the paper notes only 78% agreement between human and AI labelers, further effort could be put into distinguishing between the labels used to train RLHF and RLAIF, which could elucidate downstream differences in trained model behavior.\n\nThe paper also runs a number of experiments to determine whether chain-of-thought reasoning and self-consistency improve alignment between human and AI labelers, finding that self-consistency does not lead to improvements. However, given that human ratings are (1) subjective and (2) subject to noise, the paper should more seriously consider the possibility that lower \"AI Labeler Alignment\" may not necessarily lead to worse downstream performance. This is partially discussed in Section 4.6, but the authors consider only a single comparison and do not directly compare the two model outputs, instead comparing both of them individually to a supervised fine-tuned baseline.\n\nA minor note: I find it strange that this paper is titled \"RLAIF\" when it is not the first work to use or introduce the term. See for example Bai, et al. 2022 (\"Constitutional AI\") for earlier usage of this term. I would recommend the authors remove the title and simply use the subtitle"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804068983,
        "cdate": 1698804068983,
        "tmdate": 1699636332408,
        "mdate": 1699636332408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s9RjoMSC4X",
        "forum": "AAxIs3D2ZZ",
        "replyto": "AAxIs3D2ZZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_BDUS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_BDUS"
        ],
        "content": {
            "summary": {
                "value": "This paper presents and studies RLAIF, an alternate to RLHF wherein the preference data is synthetically produced by an LLM. \n\nThe preference labeling is done by prompting a Palm2-L model, with a prompt that consists of (i) a base/detailed preamble, (ii) optional exemplars, (iii) sample (context + 2 responses) and (iv) ending string. The label is obtained by considering the logprobs for 1 and 2 (after \"Preferred Response=\"). When generating preference labels, the paper also considers (i) CoT reasoning, (ii) self consistency, (iii) mitigating position bias by doing two inference passes. \n\nAn RM is trained on the generated preference data, and used to train an LM via RL. There are 3 evaluations: (1) AI labeler alignment which measures the accuracy of the synthetic AI-generated preference data against human preferences, (2) pairwise accuracy of the RM and (3) the human winrate of the RLAIF-trained LMs.\n\nExperiments are conducted on two domains: summarization (tldr) and helpful dialog (anthropic hh-rlhf). There are several takeaways, listed below:\n\n(1) For preference labelling: CoT helps across both domains, inconsistent results between detailed and base preamble, size of the AI labeler helps. Self-consistency (higher temperature) and few-shot exemplars hurt performance. \n\n(2) The RM converges faster (relative to human preferences) with AI generated feedback\n\n(3) RLAIF and RLHF both outperform an SFT model with a winrate of 70% (summarization) and 60% (helpful dialog). RLAIF vs RLHF has a winrate of 50%, suggestion equal performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper presents a comprehensive study of the role of LLM-generated feedback in RLHF. By performing sound experiments at each stage of the RLHF process, this paper shows RLAIF to be a reliable alternative to human preferences: (1) the agreement between the human preference data vs AI preference data (with multiple approaches), (2) the performance of the RM trained on different data and (2) the human evaluation performance of the LLMs trained with RLHF vs RLAIF. Though the methodology may not be novel, the comprehensive experiments are insightful and valuable to the community.\n\nThis paper presents several valuable and insightful results (1) impact of CoT/self consistency during preference labeling, (2) impact of the AI labeler size on accuracy, (3) RM performance as a function of amount of preference data, (4) studying position bias."
            },
            "weaknesses": {
                "value": "Since the comprehensive experimentation is a strength of this paper, is it imperative that the experiments and analysis is sound. The following points would benefit from additional experimentation or discussion:\n\n(1) In Figure4, it's not clear to me why adding exemplars hurts performance. There is a one sentence justification for this on page6, but I think it's insufficient, since exemplars hurt performance even without CoT. Is it the case that your exemplars are low quality? Could this be mitigated by using the 0-shot generations as exemplars? It would also be valuable to understand the role of exemplars at different labeler sizes (e.g. P2-S using exemplars produced by P2-L). \n\n(2) Again, regarding Figure4: Since human annotators have a 60-75% agreement rate on these datasets, I wonder if small differences in accuracy in Table4 are meaningful. Is it possible that the AI labels are more correct than the human preferences? Analyzing the disagreements may shed light on this. If so, what does it imply about the results/takeaway in Figure4.\n\n(3: suggestion) This is not a weakness, but more of a suggestion. It would be interesting to see the relationship between Figure4 and Figure5b. Does a higher quality AI-preference dataset necessarily lead to a more accurate RM? \n\n(4) The human evaluators used to assess the final RLHF/RLAIF/SFT-trained LLMs are distinct from the preference data/RM. How do we know if these annotators are modeling the same preference policy expressed by the datasets/RMs, and not (for example) just picking the longest response? To this end, it would be good to either show RM scores for the final LLMs or to measure the agreement of the human evaluators against the original preference data or the RMs."
            },
            "questions": {
                "value": "Questions in the weakness section:\n\n1. Why do exemplars hurt performance of the preference labeler?\n2. Is it possible that the AI labels are more correct than the human preferences? If so, what does it imply about the results/takeaway in Figure4?\n3. Does a higher quality AI-preference dataset necessarily lead to a more accurate RM? \n4. Does a higher quality/more accurate RM necessarily lead to a better/more preferred LLM?\n5. How do we know if the human annotators are modeling the same preference policy expressed by the datasets/RMs, and not (for example) just picking the longest response?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806500998,
        "cdate": 1698806500998,
        "tmdate": 1699636332290,
        "mdate": 1699636332290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7agBoMOLny",
        "forum": "AAxIs3D2ZZ",
        "replyto": "AAxIs3D2ZZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_4rk2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3762/Reviewer_4rk2"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to analyze the performance of reinforcement learning with AI feedback (RLAIF). RLAIF is similar to RLHF but instead of collecting expensive human annotations, the preferences are generated by another LLM. Under the setup of this paper, the RLAIF achieves a similar win rate as RLHF in human evaluation, showing that RLAIF can potentially mitigate the scalability issue of RLHF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Investigating RLAIF\u2019s performance, especially comparing it to RLHF, is important and timely.\n* The achievement of RLAIF in this paper\u2019s setup is interesting. It can achieve the same level of performance as RLHF.\n* The writing is really clear."
            },
            "weaknesses": {
                "value": "* If I understand correctly, both RLAIF and RLHF are based on the SFT baseline (fine-tuned PaLM2 XS) and REINFORCE. In this case, the key difference among RLAIF and RLHF in the experiments is the used reward models and the training data for the reward models. Therefore questions raise:\n  * What is the accuracy of the finally trained Human Feedback RM? I only see in Appendix E that the RM \u201cis trained until the training loss and accuracy curves plateau\u201d, but in Figure 5(b) it is still not plateau. Having this number can help readers understand (1) if the on-par performance of RLAIF and RLHF is due to using RMs with similar accuracy.\n  * To further dig into the above question, analysis of how RM accuracy affects the RLHF results can also be helpful.\n  * The AI feedback quality is the base and will first affect the trained AI feedback RM and then the RM will affect the result. However, the analysis in Section 4.6 entangles the two steps. It causes confusion if the performance difference comes from the trained RM or the AI feedback with different AI Labeler Alignment?\n  * Moreover, only the AI feedback with 76.1% and 78% AI Labeler Alignments are compared, if having a wider range analysis, it will be easier to understand the impact of the AI Labeler Alignments to the trained RM.\n* About human evaluation. Since the reported number is only the total human rating, I\u2019m curious about other statistics. How many input-outputs examples are used? How many evaluators rate the same input-outputs example? What\u2019s the inter-annotator agreement of the results?\n* The experimental results after controlling the length in Appendix F only shows the comparison of RLAIF vs SFT and RLHF vs SFT. Is there also a comparison between RLAIF vs RLHF?"
            },
            "questions": {
                "value": "* About the experimental setup,\n  * Are the SFT baselines using only the preferred responses in the datasets? I have checked section 3.3, appendix A.1 and E, but haven\u2019t seen the answer to this question.\n  * Is there a reason why the reward models are also initialized from the SFT models (described in Section 3.3)? Their output spaces are different. Is it a random try or based on some statistics?\n  * What is the baseline used for REINFORCE (mentioned as \u201cwe use REINFORCE with a baseline\u201d in Section 3.3)? Is it the output of the value model in the authors\u2019 setup?\n* Presentation suggestion:\n  * The paper describes in-context learning with exemplars (section 2.1) and self-consistency (section 2.1.3) as they are a part of the used methodology. However, in experiments (Figure4), they turn out useless and not applied in the end. In this case, I would suggest not to put much emphasis on them but only mention them and say the authors also study their effects. \n  * Add that Figure 5 (a)(b) are results on summarization task in the caption.\n* In section 4.2, \u201cwe observe that the optimal configuration employs chain-of-thought reasoning and no in-context learning (\u201cDetailed + CoT 0-shot\u201d)\u201d Should here be \u201cDetailed / Base + CoT 0-shot\u201d, since for summarization the best is detailed and for helpfulness the best is base?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3762/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816826674,
        "cdate": 1698816826674,
        "tmdate": 1699636332175,
        "mdate": 1699636332175,
        "license": "CC BY 4.0",
        "version": 2
    }
]