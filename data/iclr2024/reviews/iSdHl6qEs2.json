[
    {
        "id": "BCle05Nq0w",
        "forum": "iSdHl6qEs2",
        "replyto": "iSdHl6qEs2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_dqhy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_dqhy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces S4, a novel strategy for exploring the search space in size-aware transformer architecture design. S4 strategically guides the updating process by adhering to predefined size ranges, effectively eliminating the need for unnecessary training iterations and sampling efforts on models that do not conform to size specifications"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The illustration of using gradients to estimate the search space is exceptionally clear and visually engaging. It effectively aids in understanding a complex concept. The paper's clarity and ease of follow are commendable."
            },
            "weaknesses": {
                "value": "- **Larger datasets**: The experiments in the paper were carried out on small scale datasets, which may make the results less convincing. It would be better to conduct experiments on larger datasets to validate the results.\n\n- **Variable step size**: The paper claims that fixed step size is better, but it may prohibit the progress of searching for a better search space. It would be better to use variable step size to allow for more exploration of the search space.\n\n- **Clear experimental details**: The experimental details in the paper are not clear. For example, the estimated gradient by 40 sampled subnet is generated by a trained supernet. It would be better to provide more details about how this was done.\n\n- **Ablation study**: The ablation study is missing. As you claimed in the paper, your initial search space is already based on state of the art architectures. I wonder if we started from a bad initial point can still lead us to a better search space? An ablation study would help answer this question.\n\n- **Less human expert experience**: The settings of search space require too much human expert experience. It would be better to automate this process as much as possible.\n\n- **More significant improvements**: The improvements are trivial as shown in table 2. From my understanding, to estimate gradient to guide the search space, you need to train a supernet on current search space, which requires a large amount of computation resources already. However, the final results are almost the same with baseline (do not apply S4), which makes me think the initial search space is already a good choice and there is no need to explore the search space. If there is anything that I misunderstand, please tell me.\nI understand. Here are my suggestions:\n\n- **Importance of \"Size-aware\"**: Limiting the size during the searching process may indeed prohibit further performance improvements. A slightly larger search space that exceeds the required size might be helpful.\n\n- **Code Missing**: Unfortunately, I could not find any information about whether the code will be available."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7492/Reviewer_dqhy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717447310,
        "cdate": 1698717447310,
        "tmdate": 1699636904500,
        "mdate": 1699636904500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cAgGPRAz50",
        "forum": "iSdHl6qEs2",
        "replyto": "iSdHl6qEs2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_kmdm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_kmdm"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a constrained optimization framework to seek the search space for size-aware transformer architecture. This method combines accuracy gradient ascent with discrete neighboring search space evolution. Extensive experiments have demonstrated the plug-and-play nature and superior performance of this method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is written in a clear and accessible manner, making it easy to comprehend.\n2. It's great to see that the figures in this paper are simple and reader-friendly, making it accessible to a wider audience.\n3. The method presented exhibits a plug-and-play characteristic, thus delivering exceptional performance for downstream tasks."
            },
            "weaknesses": {
                "value": "1. This paper introduces a search space exploration strategy in a simplistic manner. Considering the extensive literature on search space exploration, this paper lacks innovation.\n2. In Table 2, the method is compared only on the Tiny ImageNet dataset, and there is no experimental comparison conducted on the ImageNet dataset. As a result, the genuine efficacy of this approach remains unverified.\n3. The predetermined step size of the method may require manual adjustment for different downstream tasks, which, to some extent, still relies on the manual design of the structure."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744101069,
        "cdate": 1698744101069,
        "tmdate": 1699636904308,
        "mdate": 1699636904308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "09LAYF74xw",
        "forum": "iSdHl6qEs2",
        "replyto": "iSdHl6qEs2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_U6go"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7492/Reviewer_U6go"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a constrained optimization framework for transformer architecture search, named S4, that allows the search space to evolve to neighbor search spaces under user-specified constraints, such as model size. The authors demonstrate the effectiveness of S4 through experiments on various benchmarks, including Cifar10, Cifar100, Tiny ImageNet, and SUN397."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to follow with concise expressions and clear logic.\n2. The idea of size-aware search introducing a more effective way to explore architectures is interesting."
            },
            "weaknesses": {
                "value": "1. The paper does not compare the search costs. This makes it hard to know if the method is efficient.\n2. The results reported in the paper are after a retraining process from scratch. I am interested in knowing how the performance of the entire search space's subnet changes with each Evolutionary step of the search space. Specifically, how does the inherited accuracy from the trained supernet change? The paper proposes a rough approximation search algorithm in a highly discretized search space, but such an algorithm does not have a theoretical guarantee. Therefore, rigorous experiments are necessary to validate the effectiveness of the method. Relying solely on final accuracy is not sufficient.\n3. Compared to S3, the contribution of this work appears to be insufficient, and the improvements are also small. While the idea of using size-aware techniques to reduce unnecessary searches is interesting, the solutions provided in the paper are somewhat crude. Additionally, the experiments are conducted on small datasets, whereas S3 includes results from mainstream large datasets such as ImageNet, COCO, ADE20K, and VQA2.0. As a result, the experiments in this paper are not so solid or reliable. Since the S3 paper provides ImageNet results, I would like to see S4's performance on ImageNet for a fairer comparison. There is also the possibility of transferring network structures found in small datasets to larger ones."
            },
            "questions": {
                "value": "After each change in the search space, what is the relationship between the supernet weights at times t and t-1? Are they inherited in a way similar to OnceForAll?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761473874,
        "cdate": 1698761473874,
        "tmdate": 1699636904172,
        "mdate": 1699636904172,
        "license": "CC BY 4.0",
        "version": 2
    }
]