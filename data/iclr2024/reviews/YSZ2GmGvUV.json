[
    {
        "id": "PSgKYGvOUO",
        "forum": "YSZ2GmGvUV",
        "replyto": "YSZ2GmGvUV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_5Wz3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_5Wz3"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors observe that trigger features, often referred to as backdoor features, present a distinct concentrated behavior within the spectral space, especially around the top singular values. Based on these insights, they introduce \"EigenGuard\", a method designed to mitigate the impact of trigger features by scaling the top k spectral features during the training process. Experiments show that, models with EigenGuard show better performance on both backdoor and natural examples compared with other defense algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors propose a method that does not require extra clean data and an unlearning process but can still remove the impact of triggers and enable training clean models on untrusted datasets.\n\n* The proposed method surpasses the previous works on some attacks.\n\n* The paper provides an ablation study to investigate the effect of the singular value $k$ and layers."
            },
            "weaknesses": {
                "value": "* The findings are not novel. SM Moosavi-Dezfooli et al.[Ref-1] present that universal perturbations exhibit a concentrated behavior within the spectral space. Backdoor triggers, as a special type of universal perturbation, exhibit a similar property, which appears to lack significance. \n\n[Ref-1] Moosavi-Dezfooli, Seyed-Mohsen, et al. \"Universal adversarial perturbations.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n* Some arguments lack validation. For instance, the authors claim that existing detection-unlearning  based mitigations may induce a decrease in accuracy as the neural network may forget many useful features for classifying clean samples. However, in this paper, the authors do not support this argument with references or experiments. Contradictorily, as shown in Tab. 3, fine-tuning maintains better ACC than EigenGuard. How do these experimental results support the aforementioned argument? In addition, I do not believe that modifying the dimension of the feature space does not degrade the prediction accuracy for clean or natural images. The authors should justify why their manipulation of the dimension of feature space does not lead to ACC drops.\n\n* Design choices are unclear. \n    * After progressively eliminating the dimension of the feature space by setting the top singular values of features SVD decomposition to 0, why is it necessary to lift the original small singular values and generate new features? Doesn't this manipulation lead to a degradation of the prediction accuracy for clean or natural images?\n    * What is the rationale for scaling $\\sigma_k$ by 0.001 in Algorithm 1, and how is this scale factor determined?\n\n* The paper lacks theoretical proof of the proposed method.\n\n* Writing needs improvement.\n\n* The evaluation lacks comprehensiveness and does not include comparisons with some related works:\n\t\n   * For defenses: \n\n       * Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, and  Shiqing Ma. Rethinking the reverse-engineering of trojan triggers. In Advances in Neural Information Processing Systems, 2022.\n\n      * Jonathan Hayase and Weihao Kong. Spectre: Defending  against backdoor attacks using robust covariance estimation.  In International Conference on Machine Learning, 2020\n\n    * For attacks:\n\n      * Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. Deep feature space trojan attack of neural networks by controlled detoxification. In AAAI, 2021\n\n      * Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. Advances in Neural Information Processing Systems, 33:3454\u20133464, 2020\n\n      * Li, Shaofeng, et al. \"Invisible backdoor attacks on deep neural networks via steganography and regularization.\" IEEE Transactions on Dependable and Secure Computing 18.5 (2020): 2088-2105.\n\n      * Nguyen, Anh, and Anh Tran. \"Wanet--imperceptible warping-based backdoor attack.\" arXiv preprint arXiv:2102.10369 (2021)."
            },
            "questions": {
                "value": "* Is the proposed method able to purify more complex attacks? (See weakness missing literatures)\n\n* What could be an adaptive attack and will the proposed method still be effective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698329506829,
        "cdate": 1698329506829,
        "tmdate": 1699636415271,
        "mdate": 1699636415271,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tn53Nnzfxk",
        "forum": "YSZ2GmGvUV",
        "replyto": "YSZ2GmGvUV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_Zcuo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_Zcuo"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the vulnerability of Deep Neural Networks (DNNs) to backdoor attacks from poisoned training data. It introduces a novel module called EigenGuard, which helps DNNs neglect backdoor triggers while maintaining performance on legitimate data. Through experiments, the authors show that models equipped with EigenGuard outperform other defense algorithms in handling both poisoned and clean data. This work contributes to enhancing the security of DNNs against backdoor attacks in scenarios with potentially unreliable training datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of this seems to be novel. The authors find that forcing a high-dimensional feature space will make backdoor images fail to attack.\n- This paper provides a theoretical understanding of the proposed method.\n- The paper is easy to follow.\n- The experimental results seem to be good. It outperforms other defense methods."
            },
            "weaknesses": {
                "value": "- The paper could be strengthened by addressing the potential of adaptive attacks, especially when attackers know the EigenGuard defense.\n- Why is CLR only evaluated on CIFAR10? The authors should justify it. Also, for the defense choices, the authors should justify why these methods are considered. As far as I know, many other SOTA defenses are not considered [1,2,5].\n- The paper would benefit from discussing the effectiveness of EigenGuard against self-supervised learning backdoor attacks [3-5], as the proposed defense operates on the output of the encoder (i.e., f). This exploration could significantly enhance the robustness and applicability of the proposed method.\n- To bolster the generalizability of the findings, it would be advantageous to evaluate the effectiveness of EigenGuard across a variety of model architectures. \n- Minor issues, such as the newline problem in Section 4.3, should be rectified for improved readability and professionalism.\n\n[1] Chen, Bryant, et al. \"Detecting backdoor attacks on deep neural networks by activation clustering.\" arXiv preprint arXiv:1811.03728 (2018).\n\n[2] Tang, Di, et al. \"Demon in the variant: Statistical analysis of {DNNs} for robust backdoor contamination detection.\" 30th USENIX Security Symposium (USENIX Security 21). 2021.\n\n[3] Saha A, Tejankar A, Koohpayegani S A, et al. Backdoor attacks on self-supervised learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 13337-13346.\n\n[4] Li, Changjiang, et al. \"Demystifying Self-supervised Trojan Attacks.\" arXiv preprint arXiv:2210.07346 (2022).\n\n[5] Feng, Shiwei, et al. \"Detecting Backdoors in Pre-trained Encoders.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698358102091,
        "cdate": 1698358102091,
        "tmdate": 1699636415123,
        "mdate": 1699636415123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pXIvvIukCi",
        "forum": "YSZ2GmGvUV",
        "replyto": "YSZ2GmGvUV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_6rWf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_6rWf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel defense method called EigenGuard to mitigate backdoor attacks on deep neural networks. The authors first analyze the spectral behavior of features in neural networks and observe that backdoor features tend to exhibit a concentrated behavior within the spectral space, while natural features are distributed in a high-dimensional space. Based on these observations, the authors propose the EigenGuard module, which forces the top k spectral features to share the same scale during training. This module effectively neutralizes the impact of backdoor connections while preserving the natural performance of the model. Experimental results demonstrate that EigenGuard outperforms three existing defense methods in terms of both backdoor attack success rate and natural accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a novel defense method, EigenGuard, which leverages the spectral behavior of features in neural networks to mitigate backdoor attacks during model training.\n\n2. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The theoretical foundation of the paper seems derivative, lacking novelty.\n\n2. The paper does not provide a comprehensive review of related works and omits comparisons with well-established baselines."
            },
            "questions": {
                "value": "I have a few concerns regarding the proposed method and experiments\uff1a\n\n- The defense strategy hinges on the premise that backdoor features and genuine features are distributed in distinct spectral spaces. This foundational idea has already been explored by prior works such as [1, 2] for backdoor mitigation. While this paper implements the theory to devise algorithms that defense backdoor attacks during model training, the theoretical underpinning raises questions regarding its novelty.\n\n- The paper overlooks some of the more recent developments in backdoor attacks and defenses. As a result, the experiments lack a comprehensive scope. Omissions include backdoor attacks like [3-6] and backdoor defenses during training such as those presented in [7, 8]. For a more holistic overview, the paper may refer to existing work [9].\n\nOther questions: \n\n- I feel the details of the threat model are missing.\n\n- Some terms such as **$A$** in formula 1 and $k, \\sigma_k$ in algorithm 1 are unclear.\n\n- It is confusing why the CLB is not considered for experiments (e.g, Cifar100 and GTSRB).\n\n- From Table 3, it's observed that some models, when defended with EigenGuard, exhibit improved accuracy compared to when no defense is applied. Is there a rationale behind this outcome?\n\n[1] Tran, B., Li, J. and Madry, A., 2018. Spectral signatures in backdoor attacks. *Advances in neural information processing systems*, *31*.\n\n[2] Karim, N., Arafat, A.A., Khalid, U., Guo, Z. and Rahnavard, N., 2023. Efficient Backdoor Removal Through Natural Gradient Fine-tuning. *arXiv preprint arXiv:2306.17441*.\n\n[3] Wang, Z., Zhai, J. and Ma, S., 2022. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 15074-15084).\n\n[4] Li, Y., Li, Y., Wu, B., Li, L., He, R. and Lyu, S., 2021. Invisible backdoor attack with sample-specific triggers. In *Proceedings of the IEEE/CVF international conference on computer vision* (pp. 16463-16472).\n\n[5] Nguyen, A. and Tran, A., 2021. Wanet--imperceptible warping-based backdoor attack. *arXiv preprint arXiv:2102.10369*.\n\n[6] Cheng, S., Liu, Y., Ma, S. and Zhang, X., 2021, May. Deep feature space trojan attack of neural networks by controlled detoxification. In *Proceedings of the AAAI Conference on Artificial Intelligence* (Vol. 35, No. 2, pp. 1148-1156).\n\n[7] Wang, Z., Ding, H., Zhai, J. and Ma, S., 2022. Training with more confidence: Mitigating injected and natural backdoors during training. *Advances in Neural Information Processing Systems*, *35*, pp.36396-36410.\n\n[8] Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B. and Ma, X., 2021. Neural attention distillation: Erasing backdoor triggers from deep neural networks. *arXiv preprint arXiv:2101.05930*.\n\n[9] Li, Y., Zhang, S., Wang, W. and Song, H., 2023. Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey. *IEEE Open Journal of the Computer Society*."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4413/Reviewer_6rWf",
                    "ICLR.cc/2024/Conference/Submission4413/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800903533,
        "cdate": 1698800903533,
        "tmdate": 1700663177617,
        "mdate": 1700663177617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8KK2BWlbpt",
        "forum": "YSZ2GmGvUV",
        "replyto": "YSZ2GmGvUV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_fVxx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4413/Reviewer_fVxx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new defense mechanism against backdoor attacks. The paper first investigates the singular value decomposition of the of the activation layer of neural networks. These investigations reveal that the dominant singular values of the activation layer preserve relative information of the clean vs. backdoor data, while the low-energy singular values mix them together. Motivated by this observation, this paper proposes EigenGuard. In short, EigenGuard uses a spectral filter to lower the significance of the low-energy singular values to combat backdoor attacks. Experimental results over CIFAR-10, CIFAR-100, and GTSRB shows that the proposed method effectively combat backdoor attacks such as BadNets, Blend, SIG, and Clean Label attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work presents interesting observations regarding the influence of backdoor attacks on the singular value decomposition of neural network activation layers.\n\n- The experimental results demonstrate that this approach can be useful in combating some existing backdoor attacks."
            },
            "weaknesses": {
                "value": "- The paper starts its discussions in the introduction by stating inaccurate facts about the state of existing backdoor defenses. In particular, the paper says: \"_When attempting to train a clean model on unauthorized datasets, existing methods typically try to\nfine-tune the neural networks on some additional datasets..._\" While this was the case for older backdoor defense methods, recently, there has been quite a good progress in proposing methods that do not necessarily require clean held-out validation set to mitigate backdoor attack. For example, see [1-4]. Additionally, saying that \"_With the uncontaminated datasets split after the detection, we can train the model to unlearn backdoor triggers with designed unlearning loss._\" about Spectral Signatures (Tran et al. [1]) is inaccurate. We know that this method has a two-step training process, where after filtering the poisoned data, it re-trains the entire network and, as such, has less effect on the benign accuracy. These ambiguities in the presence of the past literature has led the paper to claim in Table 1 that it is the only work that doesn't require clean data AND doesn't do unlearning AND uses the natural training. There are other works within the literature that satisfy this criteria. For example, see [2-4].\n\n- There are several major issues with the current method:\n\n   1. The paper emphasizes over and over about the relationship of the backdoor triggers with a low-rank space. For instance, it reads: \"_One plausible explanation for this observation is the limited effective subspace associated with the trigger. This suggests that the trigger features are distributed in a low-dimensional subspace._\" However, these explanations are just a restatement of the actual method used in the paper, not based on step-by-step intuitive reasonings. I highly encourage the authors to re-write these statements and try not to rely on the observations in the figures but on the intuitions and explain why this method should work.\n\n   2. More importantly, the observations made in Figure 2, the explanation of the paper about this figure, and the actual methodology seem different. In particular, after plotting the t-SNE of different singular values in Figure 2, the paper says: \"_However, from the middle t-SNE figure, one can see that the pink dots represent backdoor images distributed uniformly in the space and overlap with other color dots. Thereby, the network cannot classify these samples as trigger classes since they are similar to samples belonging to different natural classes._\" So, from this explanation it seems that the natural way of dealing with backdoors is to remove the dominant singular values. However, as shown in Algorithm 1 in the Spectral Filter, the proposed method actually preserves those singular values and dampens the effects of the remaining ones. Perhaps there is a misunderstanding here that needs to be resolved.\n\n  3. There are two important related works that this paper needs to discuss its relationship with them in more detail. First, the method of Spectral Signatures [1] also uses SVD in the feature space of neural nets to filter samples that are poisoned. Second, Collider [2] uses local intrinsic dimensionality (LID) to argue that backdoors reside in a locally high-dimension manifold. The current work argues that backdoors reside in a low-dimensional sub-space (even though it does it rather informally) and as such, it is vital to clarify its stance with prior work.\n\n  4. The theoretical contributions seem too abstract. In other words, it is unclear how the provided theory is supporting the proposed method, as having a set of segregated feature vectors for backdoored vs. clean data seems too artificial and not directly related to EigenGuard. Please re-write this section to make its connections with the proposed method clearer.\n\n- The experimental results are limited. The paper tests its approach against BadNets, Blend, SIG, and Clean Label attacks that are published before 2020s and declares that they are state-of-the-art. There are newer backdoor attacks that exist in the literature today, including but not limited to: Dynamic Attacks [6], WaNet [7], ISSBA [8], Refool [9], etc. The same also applies to the baseline defenses used in the paper, where many recent works, such as [1-4], are left out. To provide a comprehensive evaluation, incorporating all these newer baselines is necessary. Preferably, large-scale experiments on ImageNet dataset is also needed.\n\n[1] Tran et al. \"Spectral signatures in backdoor attacks.\" _NeurIPS_, 2018.\n\n[2] Dolatabadi et al. \"Collider: A robust training framework for backdoor data.\" _ACCV_, 2022.\n\n[3] Hayase et al. \"Spectre: Defending against backdoor attacks using robust statistics.\" _ICML_, 2021.\n\n[4] Liu et al. \"Beating Backdoor Attack at Its Own Game.\" _ICCV_, 2023.\n\n[5] Huang et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" _ICLR_, 2023.\n\n[6] Nguyen et al. \"Input-aware dynamic backdoor attack.\" _NeurIPS_, 2020.\n\n[7] Nguyen et al. \"Wanet\u2013imperceptible warping-based backdoor attack.\" _ICLR_, 2021.\n\n[8] Li et al. \"Invisible backdoor attack with sample-specific triggers.\" _ICCV_, 2021.\n\n[9] Liu et al. \"Reflection backdoor: A natural backdoor attack on deep neural networks.\" _ECCV_, 2020."
            },
            "questions": {
                "value": "Apart from the questions raised above, here are some additional questions/suggestions:\n\n- What settings are used for the empirical evaluations of Section 3? Some figures, such as Figure 2, only present the result without mentioning the dataset, backdoor, model architecture, etc.\n\n- Does the same empirical analysis (those in Section 3) also hold for ALL the above-mentioned attacks [6-9]? Does it also hold for clean-label attacks?\n\n- What is the significance of the theoretical analysis, and how does it relate to the rest of the paper? Can you verify its statements in a realistic setting with quantitative analysis?\n\n- Using the term \"head\" for the first layers of the ResNet model is confusing. Usually, head refers to the last classification layer and is a short-term for \"classification head\". Consider using feature extractor or other alternatives to avoid confusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699082070156,
        "cdate": 1699082070156,
        "tmdate": 1699636414989,
        "mdate": 1699636414989,
        "license": "CC BY 4.0",
        "version": 2
    }
]