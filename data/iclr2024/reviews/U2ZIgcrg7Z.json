[
    {
        "id": "Nnhw0DRMWw",
        "forum": "U2ZIgcrg7Z",
        "replyto": "U2ZIgcrg7Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_RdGi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_RdGi"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses challenges in personalized federated learning with large foundation models and limited resources, including data, computation, and model access. The proposed method, ZOOPFL (Zeroth-Order Optimization for Personalized Federated Learning), adapts inputs through zeroth-order optimization and uses linear projections for personalization. Input surgery is introduced to reduce computation costs and enhance personalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experiments are comprehensive, covering multiple datasets in both computer vision (CV) and natural language processing (NLP) applications.\n- The paper's focus on federated learning settings that address both data privacy and model privacy is intriguing."
            },
            "weaknesses": {
                "value": "My main concerns are the validity and privacy risks of this FL setting. \n- First, the black-box FL setting lacks practicality. The paper assumes the existence of large foundation models on clients in the form of encrypted assets, and it does not require the uploading of transformed inputs. However, this does not align with the most common scenarios in machine learning model services, such as the access of various black-box large language models like ChatGPT. In practical scenarios, local data needs to be uploaded to the model service provider.  \n- Second, the motivation for deploying zeroth-order optimization methods based on the local encrypted black-box model setup is not well-motivated. This setting implies that it is entirely possible to train an white-box emulator [1] as a proxy for the black-box model and directly perform first-order optimization based on the white-box emulator. However, the authors do not provide relevant discussions and experimental comparisons.\n- In terms of model privacy, the privacy leakage of a black-box model is closely related to the number of queries [2], but the authors do not provide theoretical or empirical studies on this. \n- The experimental section lacks ablation experiments with varying levels of noise added on the transformed data and visualizations of transformed data.\n\n[1] Xiao, Guangxuan, Ji Lin, and Song Han. \"Offsite-tuning: Transfer learning without full model.\" arXiv preprint arXiv:2302.04870 (2023).  \n[2] Tsai, Yun-Yun, Pin-Yu Chen, and Tsung-Yi Ho. \"Transfer learning without knowing: Reprogramming black-box machine learning models with scarce data and limited resources.\" International Conference on Machine Learning. PMLR, 2020."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Reviewer_RdGi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701788892,
        "cdate": 1698701788892,
        "tmdate": 1699636892383,
        "mdate": 1699636892383,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9beNi98vOi",
        "forum": "U2ZIgcrg7Z",
        "replyto": "U2ZIgcrg7Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_hxrH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_hxrH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed ZOOPFL, a zeroth-order optimization system for the black-box local model under a federated learning setup. Instead of directly fine-tune the black-box foundational model, ZOOPFL learns input surgery and semantic re-mapping for black-box large foundation models in federated learning. ZOOPFL aims to adapt inputs to models and project outputs to meaningful semantic space. The experiment shows that ZOOPFL performs better than the ZS setup in both NLP and CV benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. In the current foundational model era, the black-box foundational model is becoming popular. It is important to propose some ideas to efficiently personalize the foundational model without direct interference with it. Compared to other existing works related to foundational model with FL, the proposed ZOOPFL is the first to achieve federated learning with large black-box models, which is very relative to the current challenges.\n\n2. The paper is well-written and clearly structured. The author selects two different data modalities to validate the soundness of the proposed ZooPFL."
            },
            "weaknesses": {
                "value": "1. The idea seems very similar to the soft-prompt training [1], which is also working on the input surgery without directly inference the foundational model. What is the benefit of the auto-encoder pre-training in your paper?\n\n2. What are the benefits of personalization? In the experiment part, it mainly focused on the overall accuracy boost compared to ZS, which does not reflect anything regarding to personalization.\n\n3. I suggest the paper should be more clear about the only baseline ZS. I checked several times in the paper, and I could not understand what ZS stands for and why it is a suitable baseline for ZooPFL.\n\n\n[1]. Wang, Zifeng et al. \u201cLearning to Prompt for Continual Learning.\u201d 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 139-149."
            },
            "questions": {
                "value": "1. What does ZS stand for in the paper? Does it stand for zero-shot training?\n\n2. I am curious why the author selected the personalized FL as a topic to discuss. Even without step 3, this paper still makes its point regarding how to efficiently use the black-box model under FL setup.\n\n3. I am not very clear why ZooPFL needs Semantic re-mapping. Could the author elaborate more on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707536510,
        "cdate": 1698707536510,
        "tmdate": 1699636892238,
        "mdate": 1699636892238,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vsb4g6PFuN",
        "forum": "U2ZIgcrg7Z",
        "replyto": "U2ZIgcrg7Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_1Crz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_1Crz"
        ],
        "content": {
            "summary": {
                "value": "The authors designed a method that treats foundation models as black boxes. The idea is to use zeoth-order optimisation to  make sure that fine-tuning can be done efficiently on-device (e.g., to train through federated learning)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Being able to fine-tune Foundation Models (FM) in a privacy-preserving way is an important problem\n- Overall this paper helps us understand the scenario of incorporating FM in FL settings. \n- The use of zeroth-order optimization, input surgery and semantic re-mapping are interesting contributions here"
            },
            "weaknesses": {
                "value": "Some areas to improve:\n\n- While the idea of using FM as black box is interesting, there might be some privacy implications. It is unclear if the input to the FM reveals any information about the private input that is used both for training and during inference. I assume that the FM --being a black box and too big to be hosted on-device-- is run externally). As a result, this method might limit the ability of FL to offer privacy-preserving training. \nIn other words,  If we assume that the \"black box\" in figure 2.b runs externally, what are the privacy implications wrt to its input and output crossing the device boundary. If it runs on-device then what are the assumptions wrt to its size and the fact that it is a black box. \n\n- The authors assume that the FM is a black box. With more and more FM being open-sourced, it would be great if the authors can further motivate their approach and what might be the main advantages of incorporating a black box. \n\n- The evaluation is mostly done on rather simple benchmarks. I was wondering if the proposed approach (to train just parts of the model) would carry enough capacity to tackle larger tasks. Maybe some discussion or even evaluation on a more complex task would be great. \n\n- The paper might benefit from some understanding of the memory footprint and computation complexity of this method. Overall, the main target of this method is to make FM training possible with FL (on-device). As a result, we should have a good understanding on the memory/computation overhead."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Reviewer_1Crz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772952418,
        "cdate": 1698772952418,
        "tmdate": 1699636892116,
        "mdate": 1699636892116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RSKdKdiypw",
        "forum": "U2ZIgcrg7Z",
        "replyto": "U2ZIgcrg7Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_MDQp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7431/Reviewer_MDQp"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for personalized federated learning (FL) while relying on the existence of foundation models at clients. The main idea is to train some additional components (auto-encoder and semantic re-mapping) that are applied before or after the foundation model. Zeroth-order optimization has been applied due to the assumption that the foundation model cannot be accessed for purposes other than inference. Experimental results confirm the advantage of the proposed method compared to some baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The consideration of foundation models in FL is an important research direction."
            },
            "weaknesses": {
                "value": "- The paper assumes that foundation models are located at FL clients, but the clients cannot perform back-propagation on these models. It is not clear in what practical scenario such an assumption would hold. It is worth noting that most large language models (LLMs) nowadays are hosted in the cloud. Obviously, transmitting data to the cloud, even for inference, violates the privacy promise provided by FL. It seems that the authors of this paper try to overcome this privacy violation by assuming that the foundation model is hosted on each client. However, this has several issues. First, many types of LLMs are not feasible to run on mobile devices, which means that the proposed approach may only be possible in the case of cross-silo FL but not cross-device FL. Second, and more importantly, if the foundation model is hosted at the client, it is unclear why gradients cannot be computed, since each client has full access to its model in this case. \n- Overall, the proposed approach is a combination of several known techniques, including zeroth-order optimization, so the novelty seems limited. \n- The method requires additional components to be added to an existing foundation model, which appears to be a patch instead of a long-term solution. These additional components will cause additional computational overhead, which has not been studied in the paper."
            },
            "questions": {
                "value": "My questions are related to the weaknesses mentioned above, which are summarized as follows:\n- In what practical scenario would a FL client host a foundation model, but does not have full access to it?\n- What are the key technical challenges and novel solution in this work?\n- What is the additional computational overhead of the additional components (auto-encoder and semantic re-mapping) in the proposed method, when the full combined model is used for inference? It would be helpful to measure and compare the inference time with and without these additional components on a real device."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7431/Reviewer_MDQp"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699233190023,
        "cdate": 1699233190023,
        "tmdate": 1700584671956,
        "mdate": 1700584671956,
        "license": "CC BY 4.0",
        "version": 2
    }
]