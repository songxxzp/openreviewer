[
    {
        "id": "Kq4smy9un0",
        "forum": "ZCFKXwBTsw",
        "replyto": "ZCFKXwBTsw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission826/Reviewer_cmps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission826/Reviewer_cmps"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a data filtering method using an image captioning model. First, it trains an image captioning model on a small and high-quality dataset. Second, it uses the model to generate captions for the images to be filtered. Third, it computes the similarities between the generated captions and the images' original alt-texts, and filters out the low-score image-text pairs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "It is an interesting attempt to go beyond CLIPScore based filtering and explore other score based filtering methods."
            },
            "weaknesses": {
                "value": "1. The proposed method is not well motivated. The authors argue CLIPScore has three limitations, and propose a caption-text score, where caption is generated by a captioning model on an image, and text is alt-text. However, the caption-text score could also have similar limitations - the proposed method could also miss hard samples and the captioning model could also be biased, which are not discussed in the paper.\n2. Even if the hypotheses in \"Section 3 - Image-Captioning\" are true, they do not necessarily result in a better filtering metric than CLIPScore. We can hypothesize the similar statements for CLIPScore. The large-scale results in Table 1 supports this - SIEVE's avg performance 52.3 is worse than CLIPScore's avg performance 52.9.\n3. As mentioned in the paper (Sec 4.2), the results between medium and large scales are not consistent. The reviewer suspects result variance could partially explain that. How many models are trained for each filtering setting to get the quantitative results, and what is the variance?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697937270109,
        "cdate": 1697937270109,
        "tmdate": 1699636010102,
        "mdate": 1699636010102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UN6ilhIduV",
        "forum": "ZCFKXwBTsw",
        "replyto": "ZCFKXwBTsw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission826/Reviewer_pF34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission826/Reviewer_pF34"
        ],
        "content": {
            "summary": {
                "value": "This paper studies dataset curation in the context of training CLIP moddels (which contrast images and text). Such models are typically trained on large datasets containing pairs of images and captions collected from the web, and curating such datasets has been an open research problem. The authors propose a new method for curating large scale image-text datasets, called SIEVE. The method works as follows. First, the authors use an image captioning model to generate captions for the images in the dataset. Next, the generated captions are compared to the original captions using a sentence similarity model. This score, optionally combined with other techniques (e.g. CLIPScore, the alignment between the image and original caption according to a trained CLIP model), shows to be a useful signal for selecting samples from a dataset. The authors present several experiments on DataComp, a dataset filtering benchmark, and achieve state of the art performance on both the medium and large scales of this competition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper has several strengths.\n\n1. Firstly, the results are quite strong, providing substantial gains over strong data curation baselines. The authors obtain state of the art on a the challenging DataComp benchmark.\n2. The proposed method is quite simple and easy to implement. It is also not very computationally expensive.\n3. Both CLIP models and better methods for designing datasets are vibrant research directions, and this paper present solid advances in both. Therefore I believe it would be of interest to many in the community"
            },
            "weaknesses": {
                "value": "1. It is unclear how scalable the method is. The fact that the improvements seen at medium scale are larger than the improvements seen at large scale of DataComp are somewhat concerning for the scalability of the method. The paper would be stronger if these gains were consistent or grew as scale increased.\n2. While the authors present several ablations, I believe the captioning model is a central part of their pipeline. As such, I think the paper would be stronger if the authors explored more diverse captioning models."
            },
            "questions": {
                "value": "Two of the main downsides listed by the authors for using CLIPScore models (false positives and false negatives) seem to be mitigated by better CLIP models. In the future where we have better CLIP models, do you expect CLIP models to be enough? Also, if these were the main causes of the shortcomings of using CLIPScore, shouldn't we expect that using better CLIP models would lead to better filtering? This is in contrast to experiments in previous literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698419123233,
        "cdate": 1698419123233,
        "tmdate": 1699636010027,
        "mdate": 1699636010027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7VfrnFHiHe",
        "forum": "ZCFKXwBTsw",
        "replyto": "ZCFKXwBTsw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission826/Reviewer_sFAh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission826/Reviewer_sFAh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for pruning large text-image datasets for pretraining models. Given a dataset with uncurated text-image samples, the proposed method consists of generating multiple captions from an image, matching the generated captions with the original associated text, and discarding samples with low text cosine similarity score. The method is evaluated on the DataComp benchmark on the \u201cmedium\u201d and \u201clarge\u201d settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed idea is simple and seems to remove miss-aligned text-image samples well enough to increase the performance of the downstream tasks with respect to a set of baselines."
            },
            "weaknesses": {
                "value": "1. First, the proposed model was submitted as part of the DataComp competition, where other proposed models from other teams were submitted as well. On the medium-scale experiment, SIEVE is currently ranked in the 6th position (5th by the ICLR submission time). However, the paper fails to acknowledge the other competitor models and does not report their results. This lack of transparency regarding how the model performs with respect to other submissions (and only reporting results on the baselines) is concerning and provides a partial view of the state of the art to potential readers.\n\n2. I wonder how much the pruned model can be claimed to be trained on a 24 million dataset while, at the same time, the captioner model used to prune the model has been trained on 14 million image-text pairs and the text matching network on billions of text pairs. Clearly, the final model is leveraging the information from the captioner and text matching network, so I don\u2019t see a clear justification to not count the data used on those two subcomponents of the proposed method as training data. I think a clear and honest discussion about the data used for the whole process is necessary.\n\n3. The paper claims repeatedly that the dataset used for training the captioner model is \u201ccurated\u201d, \u201chigher data quality\u201d, or \u201cwell-aligned\u201d. However, the datasets used for training the captioner are GCC3M, GCC12M, SBU, COCO, and Visual Genome. From the total of 14 million images, about 12 million may come from GCC3M or GCC12M. Both of these datasets are neither curated nor well-aligned and they suffer from the same problems as LAION. In fact, the collection processes between GCC and LAION are very similar."
            },
            "questions": {
                "value": "It would be interesting to evaluate how bias (e.g. gender bias) behaves with the pruned dataset with respect to the original dataset: i.e. can bias be reduced by pruning, or is bias increased/unchanged?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724269952,
        "cdate": 1698724269952,
        "tmdate": 1699636009955,
        "mdate": 1699636009955,
        "license": "CC BY 4.0",
        "version": 2
    }
]