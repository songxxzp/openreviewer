[
    {
        "id": "kqYgQ0FqhV",
        "forum": "iMRhuFS0Uz",
        "replyto": "iMRhuFS0Uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the trade-off between value estimation stability and performance improvement caused by in offline RL. The authors propose a new algorithm by introducing a new mildly constrained policy to obtain both stable value estimation and good evaluation performance. The authors test their method on D4RL mujoco tasks to verify its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow.\n- The trade-off between stability of value learning and policy improvement is important and not well-studied by previous work.\n- The experiment part adequately explains how the policy constraint influences the evaluation and policy evaluations, which validates the motivation of this paper."
            },
            "weaknesses": {
                "value": "- The authors claim that a mild-constrained evaluation policy improves the final performances, but its effectiveness is questionable.\n    - The improvement may be attributed to the policy constraint strengths of original method are not well selected. E.g., in fig.4, the performance of TD3BC on hopper-m can achieve >80 with $\\alpha=10$. If we use this value as the baseline, then the improvement of the proposed method is actually limited. This also happens on other two settings plotted in fig.4. Meanwhile, if the original policy constraint strengths are suitable, a milder constraint in MCEP may actually degrade the performances (e.g., TD3+BC on medium-expert tasks).\n    - For DQL and DQL-MCEP, there are no remarkable differences on most tasks. So why MCEP is effective on some baselines but helps little on others?\n    - Based on the above analysis, we can find whether the additional evaluation policy improves the performances heavily depends on the strength of policy constraint in original baseline. MCEP can achieve better with better hyper-parameter, which is actually infeasible in offline RL setting, however.\n    - Although the authors give an ablation study in sec. 5.4, the improvement of MCEP is not very significant and the results seem to be inconsistent with previous figures and tables (See questions).\n- The authors only test their methods on mujoco tasks. To comprehensive verify the advantages of proposed method, more experiment results (e.g., on maze/kitchen/adroit) are needed. \n\nMinor issues:\n- There are two `3)` in the first paragraph of sec.5.\n- In the first paragraph in page 9, $\\\\tilde{\\\\alpha}, \\\\tilde{\\\\lambda}$ instead of $\\\\tilde{alpha}, \\\\tilde{lambda}$."
            },
            "questions": {
                "value": "- Which level are you using in fig.7? If it is \"-medium\", why are the performances of TD3BC with $\\alpha=2.5$ and TD3BC-MCEP very different from the values reported in table 1? The hyper-parameters should be the same."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_WVL4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697773189166,
        "cdate": 1697773189166,
        "tmdate": 1699636388833,
        "mdate": 1699636388833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ixL6hcGCio",
        "forum": "iMRhuFS0Uz",
        "replyto": "iMRhuFS0Uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the policy constraint methods in offline reinforcement learning. It takes an interesting idea: decoupling the constraint strength for stable value estimation and for policy learning. Specifically, they find that while we need a restrictive policy constraint to mitigate extrapolation error in value estimation, a milder constraint is allowed for policy learning. Thus, apart from the target policy used in actor-critic learning of standard offline RL, a mildly constrained evaluation policy (MCEP) is proposed to be separately learned with a more relaxed policy constraint. This paper instantiates MCEP with existing offline RL method TD3+BC, AWAC, and DQL and demonstrates improved performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. An ingenious idea of decoupling constraint strengths for stable value estimation and for policy learning. Two kinds of distributional shifts, namely OOD actions during Bellman bootstrapping and deployment, to my knowledge, are rarely distinguished in offline RL literature. \n2. The design of MCEP is simple and general for policy constraint methods.\n3. Challenging humanoid tasks are introduced in the experiments, and MCEP demonstrates good performance.\n4. The visualization of the toy example in Figure 2 illustrates the motivation well."
            },
            "weaknesses": {
                "value": "1. The most important finding of this paper, i.e. the difference between policy constraint strengths for stable value learning and for a performant evaluation policy, is validated empirically but lacks a theoretical analysis.\n2. The improvement of MCEP upon DQL is limited, which doubts the benefits of MCEP for modern offline RL methods with better designs, such as ReBRAC.\n3. The proposed MCEP is only applicable to policy constraint methods and does not outperform other kinds of sota offline RL methods, such as MCQ and EDAC."
            },
            "questions": {
                "value": "1. What do you mean by 'While the target policy may recover its performance by iterative policy improvement and policy evaluation, we observe that the evaluation policy may fail to do so.'\n2. How does MCEP 'overcome this drawback' (state-agnostic constraint) discussed in Singh et al.?\n3. As the authors claim in the introduction that the toy maze experiments 'validate the finding of (Czarnecki et al., 2019)', I recommend also mentioning Czarnecki et al. in Section 5.1.\n\nTypos: \n\n- Section 4.3 DQL WITH MCEP should be a paragraph in parallel with TD3BC with MCEP and AWAC with MCEP. \n- $C\\left(\\pi_{\\beta, \\pi^E}\\right)$ should be $C\\left(\\pi_{\\beta}, \\pi^E\\right)$ in the Equation (10)\n- There seem to be some explanation sentences missing after 'We next introduce the policy improvement step for the evaluation policy' and Equation (10).\n- The caption of Figure 6 is incorrectly the same as that of Figure 7."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_px9k"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697814195193,
        "cdate": 1697814195193,
        "tmdate": 1699636388758,
        "mdate": 1699636388758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rkSdiFMbNT",
        "forum": "iMRhuFS0Uz",
        "replyto": "iMRhuFS0Uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
        ],
        "content": {
            "summary": {
                "value": "In policy constraint offline reinforcement learning (RL) algorithms, it is a common practice that the constraints for both value learning and test time inference are the same. This paper argues that such a paradigm may hinder the performance of the agent during test time inference. To address this issue, they propose the Mildly Constrained Evaluation Policy (MCEP) for test time inference. The idea is quite simple and the implementation is also easy. MCEP has the same objective function as the policy trained during the offline phase, but it does not participate in the policy evaluation phase. The authors show that by doing so, the performance of the offline RL agents can be improved."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "# Strengths\n\nThis paper is generally well-written, and the logical flow is clear. I would say this paper is also well-motivated and proposes an interesting test-time inference algorithm. The resulting method is very simple, and the authors provide some figures and a toy example to illustrate to the readers the key idea behind their method, which I personally like very much. The authors combine their method with three off-the-shelf offline RL algorithms, and conduct some experiments on the D4RL locomotion datasets. The authors also conduct experiments on the Humanoid datasets, where the authors collect the corresponding static datasets by themselves. One can observe performance improvement by building MCEP upon numerous base algorithms. To summarize, the strengths and the advantages of this manuscript are\n\n- this paper is well-written with a clear logic flow\n\n- the core idea and the resulting method of this paper is quite simple and easy to implement\n\n- the improvements from the proposed method are significant on many base algorithms\n\n- the authors provide source codes, and I believe that the results presented in this paper are reproducible"
            },
            "weaknesses": {
                "value": "# Weaknesses\n\nI think the submission has the following potential flaws\n\n- (major) limited evaluation. Though the authors combine their proposed MCEP method with three offline RL algorithms, they only evaluate them on locomotion tasks, which are actually simple and easy to get a high return. So, my question is, can the proposed method benefit other domains like antmaze, kitchen, and adroit? These domains are known to be more challenging than the MuJoCo tasks. I strongly believe that the empirical evaluations on these domains are critical to show the effectiveness and advantages of the proposed methods. If the proposed methods fail in these domains, I also expect possible explanations from the authors. This paper feels quite incomplete without the experiments on these domains.\n\n- (major) It turns out that the hyperparameter selection counts in MCEP. Based on the empirical results in Section 5.4, TD3BC-MCEP and AWAC-MCEP are slightly sensitive to the hyperparameters. This may cause issues when using the MCEP in practice. Can the authors further explain this phenomenon and are there any ways that we can get rid of it?\n\n- (minor) inconsistent abbreviation for some of the algorithms, e.g., the authors write TD3-BC in the first few paragraphs while using TD3BC later. This is not a big issue and can be easily fixed, please check your submission for potential similar issues.\n\nI will be happy to update my score if the concerns are addressed during the rebuttal or in the revised manuscript."
            },
            "questions": {
                "value": "It seems your method is not restricted to the policy constraints offline RL methods, can your method be applied to value-based offline RL algorithms like CQL? I would expect explanations from the authors if CQL-MCEP fails and underperforms vanilla CQL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4218/Reviewer_VtJK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698385370926,
        "cdate": 1698385370926,
        "tmdate": 1700527930769,
        "mdate": 1700527930769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i5dSpH7TfK",
        "forum": "iMRhuFS0Uz",
        "replyto": "iMRhuFS0Uz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4218/Reviewer_r8JF"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new approach to address the problems in offline policy learning (e.g. extrapolation). The idea is to train an extra policy (called evaluation policy) based on the $Q$ function learned from the critic of a standard constrained actor-critic offline method. The idea is that using different constraint weights for the critic and evaluation policy should addresses the trade-off between evaluation performance and stable value estimate."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow and well written. As far as I know the idea is novel.\n\nExperiments seem to be well conducted and results are fairly explained."
            },
            "weaknesses": {
                "value": "While the idea of the approach is interesting, I think the paper needs a bit more work. My main concern is that the contribution is limited and the results are not super clear and convincing to me.\n\n- For example, given that $\\pi_e$ is not involved in the optimization of $Q$, why are you training $\\pi_e$ at each step and not only at the end? Training $\\pi_e$ at the end will allow to do an analysis of the impact of the constraint. \n- Have you tried to train a greedy policy starting from the recovered Q, similarly to what done in the grid experiment?\n- Why haven't you tested other environments in D4RL, eg Antmaze-v0?\n- Figures are not readable when printed out. The font is too small.\n\n\nTypos:\n\nacheive -> achieve\n\npriority. i.e. \n\nwrong latex commands in page 9"
            },
            "questions": {
                "value": "See Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698915701879,
        "cdate": 1698915701879,
        "tmdate": 1699636388599,
        "mdate": 1699636388599,
        "license": "CC BY 4.0",
        "version": 2
    }
]