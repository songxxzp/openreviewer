[
    {
        "id": "fc2bgmWBvC",
        "forum": "CYmF38ysDa",
        "replyto": "CYmF38ysDa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_aEcx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_aEcx"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an evaluation framework FLASK that includes 12 fine-grained metrics over 4 ability dimensions (logical thinking, background knowledge, problem handling and user alignment) to evaluate the skill set of current large language models (LLMs). Its evaluation results on dataset constructed from MMLU, BBH, FinQA and Haiku Generation dataset provides a more detailed picture of the abilities of LLM that benefits the model choices. For example, Vicuna and Alpaca fall behind GPT-3.5 on logical thinking and background knowledge abilities, and not all abilities receive consistent benefit from scaling. It also splits a hard subset FLASK-hard and finds that proprietary models like GPT4 cannot handle it well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- A new framework for evaluating the skillset of LLMs focused on dimensions like logical thinking and background knowledge that would benefits various reasoning tasks and user alignment that benefits the alignment research by filling the blank of detailed evaluations.\n- The difficulty level annotation is informative in differentiating the instances into different levels of ability requirement. And the findings on FLASK-hard reveals the even the strongest proprietary models so far like GPT-4 fall short on solving expert-level problems and highlights the future research directions in enhancing specific models.\n- The experimental results are rich and include patterns and biases observed that can be overlooked in the overall metrics."
            },
            "weaknesses": {
                "value": "- The annotator recruited are people with at least senior undergraduate students, therefore I am not sure whether the difficulty annotations for simple lifestyle knowledge (easy to answer without formal education) and advanced lifestyle knowledge (no formal education but explaining a well-known concept) are fair. Though it may not be a perfect solution for this, I would like to hear the thoughts from the authors about designing such pipeline in an affordable manner.\n- The Expert-level knowledge set (FLASK-hard) is not a large set."
            },
            "questions": {
                "value": "See weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3099/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662046913,
        "cdate": 1698662046913,
        "tmdate": 1699636256176,
        "mdate": 1699636256176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Oo2NeyyvOk",
        "forum": "CYmF38ysDa",
        "replyto": "CYmF38ysDa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_Qvzt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_Qvzt"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a new benchmark for evaluating large language models (LLMs) in a fine-grained fashion, namely FLASK (Fine-grained Language Model Evaluation Based on Alignment Skill Sets). FLASK contains 1,700 example instances compiled from 120 NLP datasets such ash MMLU and GSM8K. It creates a set of four primarily abilities (logical thinking, background knowledge, problem handling, and user alignment), which are further divided into 12 fine-grained skills. Each of the 1,700 instances in FLASK is labeled with regard to the top-3 required skills among the twelve. The authors demonstrate that using fine-grained evaluation of LLM outputs enhance the agreement between GPT-4's automatic rating results and human-rater results. The authors applied FLASK on closed-source LLMs such as GPT-3.5 and Bard, as well as open-source LLMs including Alpaca-13B and Vicuna-13B. The results indicate that open-source LLMs currently lag behind closed-source ones in all the skill categories, but especially in logical thinking and background knowledge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. Defining a novel, skill-based, fine-grained method for evaluating the quality of LLMs that is applicable to problems in many domains. The skill breakdown among the four primary abilities and 12 finer-grained abilities capture the most important qualities that users desire in an LLM.\nS2. Demonstrating through empirical results that evaluation results show increased human-machine agreement under FLASK's fine-grained approach compared to traditional approaches such as those based on metrics like ROGUE and skill-agnostic evaluation. This advances the state of art for LLM evaluation."
            },
            "weaknesses": {
                "value": "W1. There seems to be some arbitrariness in creating the list of 12 fine-grained skills. Some of the skills seem to be not clearly motivated or crisply applicable to all problem instances. For instance, the fine-grained skill \"readability\" applies to *all* problems, because readability of generated text is desirable to the human users regardless of the domain or difficulty of the problem. So this begs the question for what sort of problems would \"readability\" be singled out as an \"essential skill\". As another example, the fine-grained skill \"metacognition\" is related to whether the model is aware of the limits of its own knowledge. Therefore, whether a problem instance requires metacognition seems to depend on whether the problem is beyond the ability of the model, which is of course dependent on the model itself. So it's unclear to me how metacognition could be labeled as the essential skill of a problem instance in a model-independent way. In summary I think there is room for improvement in the details of the four abilities and 12 fine-grained skills, but this doesn't take away from the primary strength of this paper (S1 and S2).\nW2. Likewise, there seems to be some arbitrariness in the ten problem domains, in two aspects. First, there are some obvious missing domains such as laws, finance, and travel. It is unclear which of the ten domains those would fall into. Second, it's unclear how this system handles problem instances that involve multiple domains.\nW3. The conclusion in the comparison of open- and closed-source LLMs is somewhat dubious because of a lack of public knowledge of the closed-source LLMs' parameter count. The open-source LLMs shown in Figure 2 and Figure 4 are all 13B in parameter count, while there is no published parameter count for GPT-3.5 or Bard. Therefore it is possible that the difference seen here is attributable to differences in model size rather than details in the model's development (e.g., OpenAI and Google's undisclosed training recipes). \nW4. The English of this manuscript could use some improvement. E.g., some phrases were not idiomatic, which hinders comprehension (e.g., \"major-level theorems\"). I suggest the authors request editing help from a native English-speaking colleague."
            },
            "questions": {
                "value": "Q1. From each of the 120 datasets, how did the authors select the particular examples to be included in FLASK? This question is important because it affects whether the instances in FLASK are representative or tend to be outliers in certain aspects.\nQ2. Do the authors plan to make the FLASK dataset open source? This is not addressed in the manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3099/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813437968,
        "cdate": 1698813437968,
        "tmdate": 1699636256072,
        "mdate": 1699636256072,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R8Qu5rr2RG",
        "forum": "CYmF38ysDa",
        "replyto": "CYmF38ysDa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_BpKX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3099/Reviewer_BpKX"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a fine-grained evaluation protocol called FLASK, aimed at assessing the alignment capabilities of language models (LMs). FLASK emphasizes four primary aspects: Logical Thinking, Background Knowledge, Problem Solving, and User Alignment, that are further divided into 12 fine-grained skills to facilitate comprehensive LM evaluation. The authors design specific scoring rubrics for each task, providing guidance to human evaluators or model-based evaluators in accurately measuring the skill level of LMs.\n\nMoreover, this research proposes an approach that utilizes GPT-4 as an evaluation model to automate the evaluation process. The authors conduct extensive experiments to demonstrate the effectiveness of their proposed method, evaluating its robustness, reliability, and the correlation between human-based and model-based evaluations, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces FLASK, a new evaluation framework designed to assess the performance of language models in a fine-grained manner. FLASK encompasses a diverse range of essential skills for language models, such as logical correctness, commonsense understanding, comprehension, harmlessness, and more.\n\n2. By employing task-specific scoring rubrics, both human evaluators and evaluation models can accurately assess the performance of language models and provide qualitative analyses based on the rules outlined in the rubric descriptions.\n\n3. The authors conducted comprehensive experiments to showcase the superiority and reliability of their proposed method. These experiments provide strong evidence supporting the effectiveness and robustness of the proposed approach."
            },
            "weaknesses": {
                "value": "1. The evaluation of large language models using only 1740 instances is inadequate to ensure thorough verification. Furthermore, the limited number of instances makes it challenging to encompass a wide range of domains, thereby diminishing the effectiveness of the proposed method.\n\n2. The experimental results presented in Table 4 reveal a relatively low level of agreement among different evaluation models. It is possible that a large language model performs well in a specific skill but receives a lower score due to the evaluation model's inability to effectively address problems within the current domain or skill set."
            },
            "questions": {
                "value": "Does the FLASK-HARD set encompass all 12 skills utilized in this study? Furthermore, what is the distribution of these 12 skills within the FLASK-HARD set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3099/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838041976,
        "cdate": 1698838041976,
        "tmdate": 1699636256008,
        "mdate": 1699636256008,
        "license": "CC BY 4.0",
        "version": 2
    }
]