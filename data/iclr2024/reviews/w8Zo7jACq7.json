[
    {
        "id": "m8nPwFU8Fn",
        "forum": "w8Zo7jACq7",
        "replyto": "w8Zo7jACq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_y9UN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_y9UN"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs), focusing on the development of a model-free algorithm with low regret that identifies an optimal policy with high probability. A new algorithm, Pruning-Refinement-Identification (PRI), is proposed, leveraging a newly discovered structural property of CMDPs named limited stochasticity. PRI ensures near-optimal policy output with a high probability and guarantees improved regret and constraint violation bounds in the tabular setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The result that there are at most $N$ states with a stochastic policy seems interesting. Here $N$ is the number of constraints."
            },
            "weaknesses": {
                "value": "1. Some assumptions of this paper seem a bit strong. In particular, this paper requires that each state-action pair can be visited with nontrivial probability $p_{min}$. Also, Assumptions 2 and 5 seem unnatural to me. When comparing with existing works, it would be nice to also compare with the assumptions made in these works. \n\n2. Technically, this works seems a combination of Triple-Q with additional policy fine-tuning. It would be great to highlight the technical novelty. In particular, I wonder whether PRI can be used together with any online RL algorithm for CMDP with sublinear regret and constraint violation.\n\n3. It would be great if the authors could conduct some simulation experiments that shows the the optimal policy is found, not just regret and constraint violation."
            },
            "questions": {
                "value": "1. When will Assumptions 2 and 5 hold?\n2. Why do you need the policy to be unique in the first part of the theory?\n3. In Theorem 4, do you get exactly an optimal policy or an approximate one as in Theorem 3.\n4. Can you replace Triple-Q with any efficient CMDP algorithm as long as the regret and constraint violation are sublinear? Suppose the regret is $K^{\\alpha}$ and constraint violation is $K^{\\beta}$. What would be the error in learning the policy? I am asking because it seems that the proof of Theorem 3 only requires some regret and constraint violation results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739231290,
        "cdate": 1698739231290,
        "tmdate": 1699636212722,
        "mdate": 1699636212722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gMlcW1d2vx",
        "forum": "w8Zo7jACq7",
        "replyto": "w8Zo7jACq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_S5ah"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_S5ah"
        ],
        "content": {
            "summary": {
                "value": "The authors present a model-free algorithm for episodic constrained MDP that identifies the best policy within $\\tilde{\\mathcal{O}}(1/\\sqrt{K})$ error and generates the optimal $\\tilde{\\mathcal{O}}(\\sqrt{K})$ regret which vastly improves upon the best known model-free result."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors present a novel technique for proving the regret guarantees of CMDP. This might be useful in other constrained optimization setups. Overall, the paper reads well."
            },
            "weaknesses": {
                "value": "Please see the questions below."
            },
            "questions": {
                "value": "1. Episodic CMDP can be considered a special case of infinite-horizon average reward CMDP. An average reward CMDP can be transformed into an episodic CMDP by substituting $T=HK$ and augmenting a time index modulo $H$ in the state description. Therefore, Table 1 should also include the equivalent results obtained from the average reward CMDP literature.\n\n2. It is not clear from the related works if the model-free best policy identification (BPI) approach has been considered in the literature solely for unconstrained MDP. If yes, then the best regret bound in that category should be pointed out.\n\n3. The term $M$ in Lemma 2 has not been explicitly defined. \n\n4. There should be a policy initialization in Algorithm 1.\n\n5. Please use a different notation for the coefficients in $(11)$. The current one is similar to the notation of an action.\n\n6. It seems that the number of optimization variables in $(11)$ is $\\mathcal{O}(A^{SH})$ which could make the problem prohibitive for a large state space. This should be clearly stated in the paper.\n\n7. Assumption 1 indicates that the design of the algorithm requires knowledge about the optimal occupancy measure which is highly unlikely in practice.\n\n8. It seems that Assumption 2 is redundant for finite state and action spaces.\n\n9. The policy identification stage is run for $\\mathcal{O}(MKH)$ number of steps and as stated before, $M$, in the worst-case can be $\\mathcal{O}(A^{SH})$. Why this bound does not appear in the final regret should be intuitively explained.\n\n10. Theorem 3 dictates the BPI result assuming perfect pruning which does not happen with at most $\\mathcal{O}(K^{-0.1})$ probability. In the introduction, this probability is mentioned to be $\\mathcal{O}(1/\\sqrt{K})$. Please clarify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2707/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2707/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2707/Reviewer_S5ah"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817282056,
        "cdate": 1698817282056,
        "tmdate": 1699636212648,
        "mdate": 1699636212648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bKJ3aNzrzP",
        "forum": "w8Zo7jACq7",
        "replyto": "w8Zo7jACq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_QLDH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_QLDH"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the reinforcement learning problem for constrained MDPs in the tabular setting, and proposes a model-free algorithm that returns a policy with with sublinear $\\tilde{\\mathcal{O}}(\\sqrt{K})$ regret and constraint violation with high probability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Exploiting specific structural properties of policies and occupancy measures in the constrained MDP case, the paper proposes an effective model-free learning algorithm with a good regret and acceptable constraint violation performance. Instead of best-iterate convergence, a stronger regret result was proved. These results may be good contributions.\n- The paper is extremely well-written. The algorithm design and analysis were discussed very clearly."
            },
            "weaknesses": {
                "value": "- Although the algorithm achieves a better regret bound compared to Triple-Q in (Wei et al., 2022a), this improvement comes at the expense of increased constraint violation. Is there a tradeoff between regret and constraint violation? If so, is it possible to achieve this tradeoff by using different hyperparameters?"
            },
            "questions": {
                "value": "- How does the minimum state-exploration probability $p_{min}$ in Assumption 3 appear in the regret and constraint violation bounds? \n\n- Should Assumption 3 hold for any greedy policy $\\pi$? It looks a little strong in its current form."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837588594,
        "cdate": 1698837588594,
        "tmdate": 1699636212373,
        "mdate": 1699636212373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AOKvOW4q2H",
        "forum": "w8Zo7jACq7",
        "replyto": "w8Zo7jACq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_3osr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_3osr"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a model-free algorithm for online constrained MDP. The algorithm is based on Triple Q and a novel pruning-refinement-identification algorithm. This paper also highlights a limited stochasticity property of the optimal policy for constrained MDP that has been overlooked in the literature. The proposed algorithm enjoys both sublinear regret and constraint violation, which improves the existing algorithm in the literature. Simulation results also show performance improvement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written. \n2."
            },
            "weaknesses": {
                "value": "My only suggestion for improving the clarity is to add a short review of Triple-Q to make the paper more self-contained. Other questions are discussed in the next box."
            },
            "questions": {
                "value": "Q1: in equation (3), does $\\rho^n$ mean the exponential of $\\rho$, or just a constant that differs with $n$? If it is an exponential of $\\rho$, can the authors explain why considering this special form? What's the difficulty of considering general $\\rho_n$? If it is indeed a constant that takes different values with $n$, then I suggest using $\\rho_n$ to avoid confusion.\n\nQ2: In Algorithm 1, what is K is unknown? How to implement the algorithm? Does Theorem 1-3 still hold?\n\nQ3: In section 7, why does Triple Q have a much smaller negative constraint violation? Is it because Triple Q becomes very conservative in the end? But shouldn't the conservativeness reduce as learning continues? Besides, instead of total constraint violation, what's the total number of episodes or stages of constraint violation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699204396092,
        "cdate": 1699204396092,
        "tmdate": 1699636212304,
        "mdate": 1699636212304,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gbQdCrmREI",
        "forum": "w8Zo7jACq7",
        "replyto": "w8Zo7jACq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_ost9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2707/Reviewer_ost9"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors propose a model-free algorithm for deterministic CMDP (deterministic, in terms of rewards and constraints) which guarantees \\sqrt{T} regret bound and constraint violation. Moreover, the algorithm proposed outputs a near-optimal policy with high probability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written. Furthermore, the authors propose the first model-free algorithm achieving \\sqrt{T} regret and violations in CMDPs which outputs a near optimal policy, which is a non-trivial result."
            },
            "weaknesses": {
                "value": "1) Since the paper refers to deterministic CMDP (and even assuming the generalisation to stochastic rewards and constraints to be trivial), the notion of violation proposed seems to be weak. Indeed, [Efroni et al., 2020]  model-based methods, achieves optimal sublinear violation when the cancellations between episodes are not possible. \n2) The algorithm strongly relies on the Triple-Q algorithm, employing it as subroutine. Thus, the algorithmic novelty is partial.\n3) The assumption that the CMDP\u2019s LP has a unique solution is strong. Since it is relaxed in the second part of the paper, I do not see any reason to focus half of the paper on this case. Same reasoning holds for assumption 3. \n4) The theoretical results hold only for Large K, while no guarantees are provided if the number of episodes is small.\n5) In chapter 6, when the assumption on unique solution is relaxed, the authors introduce additional strong assumptions. For example, Assumption 4 states that the algorithm is given as input a lower bound on the probability of visiting every state-action pairs (when it is not 0) under the optimal policies belonging to the extreme point of the decision space.\n6) Given that the main novelty of the paper concerns the model-free nature of the algorithm (indeed, model-based algorithms achieves better theoretical guarantees), the authors should devote more space clarifying which is the improvement in terms of the computational complexity of the algorithm proposed with respect to prior works."
            },
            "questions": {
                "value": "Triple-Q assumes that salter condition holds. I assume this must be true even in your case, since Triple-Q is employed in PRI algorithm. Is it right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2707/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699268973377,
        "cdate": 1699268973377,
        "tmdate": 1699636212216,
        "mdate": 1699636212216,
        "license": "CC BY 4.0",
        "version": 2
    }
]