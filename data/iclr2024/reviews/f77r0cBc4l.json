[
    {
        "id": "9ZvL62zRWO",
        "forum": "f77r0cBc4l",
        "replyto": "f77r0cBc4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_ThD7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_ThD7"
        ],
        "content": {
            "summary": {
                "value": "This work comprehensively evaluates 10+ leading LLMs, such as OpenAI's GPT series models, Claude 2, and Llama2, on 20+ curated benchmarks across 7 carefully chosen capability categories, including knowledge, reasoning, comprehension, math, code, multilingual, and safety. The comparison results offer valuable insights into the evolutionary path from GPT-3 series to GPT-3.5 series and GPT-4, and partially answer some important questions that are of curiosity to the community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Significance**: This work provides a high-quality and comprehensive benchmark for LLMs research, which may provide a good foundation for LLMs development and comparison. \n\n**Quality**:  Each dimension of the evaluation benchmark (e.g., metric, model, used prompt, black-box evaluation vs white box evaluation, etc) is carefully chosen. The analysis about the evaluation results is well conducted and deliver some useful information. \n\n\n**Clarity**: The paper is well-written, and the structure and figure is very clear."
            },
            "weaknesses": {
                "value": "**Originality**: l have seen that the authors clearly compare this work with previous LLMs benchmark work (in the penultimate paragraph in the introduction section), and it appears to be the first benchmark that consistently evaluates so many LLM models across multiple capability dimensions. I am curious to know if there are any novel evaluation dimensions proposed in this work."
            },
            "questions": {
                "value": "1. Have the authors open-sourced the benchmark work, and what costs are associated with evaluating a newly trained model? \n2. Are the current capability dimensions sufficient to systematically evaluate current Low-Level Models, or is there any important metric that this work is missing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697966519800,
        "cdate": 1697966519800,
        "tmdate": 1699636784137,
        "mdate": 1699636784137,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HgKY6Eyecc",
        "forum": "f77r0cBc4l",
        "replyto": "f77r0cBc4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_Fohj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new benchmark GPT-Fathom for comparing the performance of closed-source and open-source LLMs. The benchmark is comprehensive and consistently compares different LLMs. The paper has shown the impact of evolution of closed-source LLMs, inclusion of coding datasets during training, as well as alignment methods such as SFT and RLHF."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Creating a well-designed benchmark for LLMs is an important problem statement\n- Consideration of both open source and closed source LLMs in the evaluation\n- Focus on reproducibility, consistent settings, and ablation of methods/prompts.\n- Extensive experiments and analysis of LLMs\n- Explanation of why the black box evaluation was considered for all benchmarks and LLMs.\n- Interesting analysis of evolution of OpenAI LLMs."
            },
            "weaknesses": {
                "value": "The research questions addressed by the paper is unclear. For a research publication focused on benchmarking, it is insufficient to study a new set of LLMs and explain the results. The paper needs to explain the benchmark design and how it has resulted in a substantial improvement over existing benchmarks. Listing a few points below. \n- The paper claims results are based on \"aligned\" settings, but still includes numbers from other papers (in brackets) and optimized results (with a star). Instead, it will be useful to compare the numbers in existing papers, and show the impact of the aligned evaluation settings. Did the results change? If so, what was the reason? Such an analysis would confirm their posed hypothesis that aligned evaluation settings lead to more insights than those already published.\n- Similarly, it would be good to understand each new feature introduced by GPT-Fathom compared to prior benchmarks, and show why it led to a better evaluation outcome not just for LLMs considered but also for future LLMs that will get evaluated.\n- The paper claims that they have picked representative LLMs, benchmarks, and settings. Why are the choices made representative? No explanation have been provided. Without an explanation, the benchmark looks like a collection of benchmarks from other papers, and the benefits of the proposed benchmark is not clear. \n- The paper acknowledges that the categories in the benchmarks are chosen based on their \"own interpretation\". But no justification is provided to explain why this  interpretation should be adopted by the research community as the best benchmark to use for LLMs. Some analysis on how these benchmarks cover the range of tasks that LLMs are used for will be useful. \n- The paper repeatedly states that it is trying to answer open questions from the community. The open questions that these benchmarks provide answers for is not clearly stated. \n- Prompt sensitivity is an important issue. Two prompts show there is an issue, but it is unclear if LLMs work well for these two prompts that the sensitivity issue is resolved. A better design to evaluate sensitivity with an appropriate metric will be more useful. \n- In consistency in the number of shots across benchmarks and types of ablation does not show \"aligned\" settings claimed by the paper."
            },
            "questions": {
                "value": "Some clarification questions:\n- How are you automating the testing of web-version of LLMs? Is that done manually or through some web toolkit?\n- I did not understand what is meant by \"pass@K\". Do you pick the best answer out of K retries? \n- Why does Table 5 and 6 not use zero-shot prompting? \n- Why do different benchmarks use different shots?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520020108,
        "cdate": 1698520020108,
        "tmdate": 1699636784008,
        "mdate": 1699636784008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "olZ5vouFJr",
        "forum": "f77r0cBc4l",
        "replyto": "f77r0cBc4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_aiZP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6788/Reviewer_aiZP"
        ],
        "content": {
            "summary": {
                "value": "This paper curates a benchmark suite to evaluate the performance of LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed benchmark covers a range of aspects to study, including knowledge, math, coding, etc. \n\nIt also provides performance and analysis of several popular LLMs on the proposed benchmark."
            },
            "weaknesses": {
                "value": "I appreciate the experiments and analysis, but I am mostly concerned with mismatched claims and unclear novelty.\n\n1. mismatched claimed: The paper underscores that the paper sheds light on \"the evolutionary path from GPT-3 to GPT-4,\" several times in the abstract, intro, and conclusion. However, after reading the main text, I could not find enough evidence and/or analysis on the evolutionary path. Figure 1 gives a visualization of OPENAI's announcements of different features/models over time, which the authors defined as evolutionary path. But how is it related to the proposed benchmark?\n\n2. unclear novelty: The proposed benchmark, GPT-Fathom, is effectively a selection/collection of (subsets of) existing benchmark datasets (MMLU, Bigbench, etc). Prompting and evaluation metrics are also quite standard. The analysis seems to resonate many well-known assertions, e.g., proprietary models are more performant. It is unclear to me what new message this paper brings in."
            },
            "questions": {
                "value": "Please see my question in the weakness parts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728732701,
        "cdate": 1698728732701,
        "tmdate": 1699636783900,
        "mdate": 1699636783900,
        "license": "CC BY 4.0",
        "version": 2
    }
]