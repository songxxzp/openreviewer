[
    {
        "id": "eQuHhPY5QQ",
        "forum": "wYmcfur889",
        "replyto": "wYmcfur889",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_gGZJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_gGZJ"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the performance drop in DMs with limited sampling steps, attributing it to the weak denoisers used in their training. To mitigate this issue, the authors introduce the Data Prediction Denoising Model (DPDM), a multi-step generative model that outperforms DMs with few sampling steps. DPDM enhances data recovery capabilities by minimizing distribution divergence, which results in stronger denoisers capable of better recovering data distributions from noisy data. A corresponding sampling algorithm, DPDM sampler, is introduced to generate samples from DPDMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper addresses a useful and practical issue with Diffusion Models (DMs). The research is novel and the exposition is clear and well-structured. \n2) The paper begins with a clear and well-supported empirical observation regarding the performance drop in DMs when the number of sampling steps is limited. The paper introduces the Data Prediction Denoising Model (DPDM) as a novel approach to address the limitations of DMs. DPDM is designed to enhance data recovery abilities, and its effectiveness is rigorously demonstrated through experiments.\n3) The paper provides some mathematical foundation for DPDM by emphasizing the importance of minimizing distribution divergence. This adds depth and theoretical support to the proposed model.\n4) The paper goes beyond presenting DPDM and conducts extensive comparisons with existing multi-step generative models."
            },
            "weaknesses": {
                "value": "1) While the paper claims to solve a practical problem by needing less compute resources for sampling compared to DMs. However, training DPDMs require an auxiliary diffusion model, a retrained DM and a multi-step denoiser model which may bring additional memory costs and training time requirements.\n2) The paper predominantly focuses on low-resolution image generation tasks, such as 64x64 pixel images. While it demonstrates the effectiveness of DPDM in this context, it does not explore or provide results for higher-resolution image generation or other types of data generation tasks.\n3) Minor comment: When NFE is first introduced in Page 2, the full form is not mentioned. Please include this in the revised version to improve readability."
            },
            "questions": {
                "value": "While the paper is generally well-written, I have the following questions.\n\n1) Can the authors demonstrate the results for generating higher resolution images such as 256x256 or higher and how DPDMs compare with DMs?\n2) Can the authors comment on the training and inference times and compute requirements for DPDMs and comparable DMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688007544,
        "cdate": 1698688007544,
        "tmdate": 1699636506476,
        "mdate": 1699636506476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V3AYFAqsLU",
        "forum": "wYmcfur889",
        "replyto": "wYmcfur889",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_twcE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_twcE"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an algorithm to train a denoiser given a pre-trained diffusion model, use the denoiser to sample an image, and shows that this method is better than general diffusion on small sampling steps."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A new algorithm to accelerate the diffusion sampling.\n- Excellent results on small sampling step."
            },
            "weaknesses": {
                "value": "With my carefully proofreading, I still suffer from understanding the key idea of this paper. From what I understand, this paper aims to train a separate denoiser such that (1) the denoiser can predict a clean image from noisy image, and (2) when Gaussian noise is added to the predicted clean image, the distribution of the new noisy image is consistent to that of the noisy images used to train the diffusion model. However, training this separate denoiser is the same with the training loss of the original diffusion model. From this aspect, it seems like this idea is more like a fine-tuning (or just training it longer) method. I cannot follow why such a fine-tuning idea is effective when having small sampling step. Could the author clarify this point?\n\nThe sampling method considers the clean image as the mean of the next step distribution, which contradicts the theoretical analysis of DDPM. Could the author also justify this point? \n\nMoreover, this paper claims it provides a solid mathematical foundation for the proposed method, which is unclear what that means."
            },
            "questions": {
                "value": "Need justification and more explanation of the proposed method. (see the weakness)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5134/Reviewer_twcE",
                    "ICLR.cc/2024/Conference/Submission5134/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714486704,
        "cdate": 1698714486704,
        "tmdate": 1700699947220,
        "mdate": 1700699947220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZfVHlxJLQd",
        "forum": "wYmcfur889",
        "replyto": "wYmcfur889",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_X9iJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_X9iJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Data Prediction Denoising Models (DPDM)s, a new class of generative models that incorporates a sequence of strong denoisers for data generation. It is argued that conventional diffusion models embody weak denoisers, which in turn requires a high number of steps at inference time for generation, reducing the overall efficiency. To alleviate this challenge, DPDM training uses a teacher DM for initialization, and minimizes probability divergences between denoiser-recovered data distributions and the ground truth data distribution. In addition, a sampler suitable for DPDMs is presented. It is shown that DPDM can attain strong performance on CIFAR-10, and ImageNet64x64 with only a few number of sampling steps."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is novel, and the technical contribution is strong. DPDMs provide a new framework for generative modeling which is different from diffusion models, although it draws many inspirations from diffusion models.\n- The experimental results are convincing where DPDMs outperform many recent baselines, illustrating their capability on data generation using a small number of NFEs."
            },
            "weaknesses": {
                "value": "- The scope of the experiments section is limited where the results are presented on low-resolution datasets such as CIFAR-10, and ImageNet 64x64. The experiments would benefit from demonstrations with high-resolution datasets to illustrate the generalizability of the method. \n- Although the focus of the paper is on inference efficiency, inference metrics are not provided. The comparison is made in terms of NFEs which is an important aspect. However, metrics such as inference memory, seconds per iteration at inference, and overall inference time until convergence should be provided for a valid comparison."
            },
            "questions": {
                "value": "Questions:\n- How does inference memory and time compare with state-of-the-art baselines?\n- What does $\\tilde{x}$ stand for in Equation 1? I don't think it has been defined anywhere.\n\nSuggestions:\n- Typographical issues: There are issues such as duplicate references, (Song et al., 2020b and Song et al., 2020c), duplicate paragraphs (Training Efficiency and GPU-memory Cost in main and appendix) and other issues which should be fixed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5134/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5134/Reviewer_X9iJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699338819451,
        "cdate": 1699338819451,
        "tmdate": 1699636506286,
        "mdate": 1699636506286,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IO2HqVoy78",
        "forum": "wYmcfur889",
        "replyto": "wYmcfur889",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_u1HV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5134/Reviewer_u1HV"
        ],
        "content": {
            "summary": {
                "value": "This paper studies student-teacher fine-tunning method that improves and accelerates sampling steps for Diffusion Models (DM). The primary hypothesis motivating this research asserts that the conventional score matching objective used in training leads to suboptimal denoisers for DM, thereby limiting the generation of high-quality samples when constrained to a minimal number of sampling steps (NFE < 10). Results are shown on two datasets comparing to several most recent  baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1, The beginning of the article (Section 1 and Section 2) is well written and presents the motivation behind the introduction of the proposed student-teacher fine-tunning method in a very didactic way.\n\n2,  There are some theoretical justifications for the training gradient of the proposed smoothed KL.\n\n3,  Numerical experiments on small datasets (e.g., 64X64) verify that the proposed DPDM outperforms previous sampling methods in terms of quality (FID) with fewer function evaluations"
            },
            "weaknesses": {
                "value": "1, While Diff-Instruct was not explicitly designed to accelerate diffusion model sampling, the proposed DPDM still shares many similarities in format, such as the smoothed KL divergence, student-teacher fine-tuning, and the gradient update of the 'objective.' More importantly, Diff-Instruct tested its sampling quality with small NFEs as well. These aspects make the actual novelty of DPDM not clear to the reviewer.\n\n2, While there are some theoretical demonstrations concerning the gradient of KL divergence for a single denoiser, equivalent analyses for multiple denoisers in Eq.3.4 appear to be missing, making the work somehow incomplete.\n\n3,  Compared to other training-free sampling acceleration methods, the computation costs of DPDM are still relatively heavy, making the practical impacts unclear.\n\n4, This work is only demonstrated for image synthesis on small datasets, while other conditional sampling tasks (e.g., text-to-image or image-to-image translation) are missing."
            },
            "questions": {
                "value": "1, What is the runtime comparison between the proposed and other methods ?\n\n2, Can this strategy be directly adapted to other conditional sampling methods, such as image-to-image translation? On the other hand, given that the sampling trajectory is implemented by different denoising operators, how robust is the DPDM to variations at intermediate stages, especially at low noise levels ?\n\n3, In Table 4, it seems that DPDM performs worse than Diff-Instruct for NFE < 4. What accounts for such differences?\n\n4, When training multiple denoisers, how does convergence occur in those student networks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699500779144,
        "cdate": 1699500779144,
        "tmdate": 1699636506137,
        "mdate": 1699636506137,
        "license": "CC BY 4.0",
        "version": 2
    }
]