[
    {
        "id": "WscZXqGGIZ",
        "forum": "Tr0lPx9woF",
        "replyto": "Tr0lPx9woF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a post-training N:M pruning solution for LLMs. The method is built upon two components: (1) relative importance and activation, which considers both the weights and activations within LLMs for a better weight importance estimation; and (2) channel permutation, which can better preserve important weights under n:m sparsity through rearranging channel orders. Experiment results demonstrate the effectiveness of the proposed method and its superiority over existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The channel permutation and the Hungarian algorithm are novel techniques, and the experiments demonstrate the effectiveness under n:m sparsity\n2. The paper is easy to follow, and the authors conduct extensive experiments."
            },
            "weaknesses": {
                "value": "1. The first technique, relative importance, and activation, seems to be an incremental improvement. \n2. In Section 5.3, which discusses N:M sparsity, I was anticipating experimental results on smaller LLMs such as Llama2-7b, and a higher sparsity ratio than 50%. This would potentially highlight the advantages of the proposed method over SparseGPT and Wanda more effectively. Could the authors provide their insights on this?\n3. Regarding the inference latency under n:m sparsity, I would like to suggest that instead of providing layer-wise speedup, the authors could consider providing end-to-end latency for the pruned LLMs. My reason for this suggestion is that I am curious about whether n:m sparsity is indeed an effective structured pruning pattern within LLMs, especially when compared to pure structured pruning methods such as LLM-Pruner. Could the authors provide their perspective on this?"
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_dzPo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717386534,
        "cdate": 1698717386534,
        "tmdate": 1700640732856,
        "mdate": 1700640732856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gm3A8gSp0e",
        "forum": "Tr0lPx9woF",
        "replyto": "Tr0lPx9woF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
        ],
        "content": {
            "summary": {
                "value": "* The paper introduces a relative importance pruning metric which leads to more uniform pruning patterns.\n* The paper proposes to apply channel permutations found by a scalable heuristic in order to relax the pattern restrictions imposed by n:m pruning.\n* Both techniques are evaluated on Llama models for perplexity and zero-shot tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed techniques are relatively simple to implement in practice and in particular the channel reordering seems to be quite effective.\n* The paper is easy to understand and provides clear visualizations of the key algorithms.\n* Evaluation is carried out on strong Llama models and not just on older OPT ones.\n* The Appendix contains interesting additional studies like combining RIA with SparseGPT reconstruction.\n* The paper also considers practical acceleration of sparse models in Table 5."
            },
            "weaknesses": {
                "value": "* The observation that encouraging a more uniform sparsity pattern is beneficial was also made by Wanda, RIA seems to be an extension of that (also across columns). Similarly, that permutation reordering can be helpful for n:m sparsity was found by [Pool & Yu, 2021], this paper only introduces a simpler but more scalable heuristic for finding such a permutation, based on average activation values. While there is some novelty, it is not particularly high.\n* For unstructured sparsity, the improvements of RIA over prior work are relatively small at around 0.1-0.2 points in perplexity. The impact of the more advanced linear sum assignment permutation method also seems rather minor. At the same time, perplexity increases from the dense baseline are still quite large, especially for 2:4. Hence, it is not clear how useful the corresponding sparse models would be in practice.\n* There does not appear to be any code in the Supplementary. I hope the authors plan to open-source their work to aid reproducability.\n\nWhile I do not think that the paper brings particularly strong new ideas or practical results, I find it interesting that encouraging even more uniformity than Wanda is beneficial, and that permutation reordering is quite effective for n:m pruning even for massive LLMs. Hence, I am currently leaning towards acceptance."
            },
            "questions": {
                "value": "* How does 4:8 perform with channel permutations in the setup of Table 3?\n* Did you carry out any additional ablation studies around parameter a other than Table 2? I am curious if a = 0.5 is generally the sweet spot or if it was just picked for simplicity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1198/Reviewer_hn9p"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769804682,
        "cdate": 1698769804682,
        "tmdate": 1699636046121,
        "mdate": 1699636046121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "godfHfldna",
        "forum": "Tr0lPx9woF",
        "replyto": "Tr0lPx9woF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_ee73"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_ee73"
        ],
        "content": {
            "summary": {
                "value": "Main Contribution:\n- The paper proposes two new methods for efficient post-training pruning of large language models (LLMs):\n    1) Relative Importance and Activation (RIA), a new pruning metric that considers both weight and activation information to determine weight importance. \n    2) Channel Permutation, a method to maximize retention of important weights when converting a model to N:M sparsity for hardware acceleration.\n\nNovelty:\n- RIA provides better pruning performance than prior state-of-the-art methods by avoiding pruning entire channels and using activations to assess weight importance.  \n- Channel Permutation reformulates the input channel permutation problem as a linear sum assignment problem, allowing efficient optimization using the Hungarian algorithm.\n\nExperiments:\n- Experiments conducted on LLMs including LLaMA, LLaMA-2, and OPT ranging from 7B to 70B parameters.\n- Tasks: Language modeling (Wikitext-2 perplexity) and zero-shot classification (5 commonsense datasets).\n- Compared RIA to magnitude pruning, SparseGPT, and Wanda for unstructured pruning.  \n- Evaluated Channel Permutation combined with RIA and other methods under N:M sparsity.\n\nResults:\n- RIA outperforms prior state-of-the-art post-training pruning methods in both unstructured and N:M sparsity settings.\n- Channel Permutation further improves performance under N:M sparsity by efficiently finding better channel arrangements.\n- Together, RIA and Channel Permutation provide an effective pipeline for LLM pruning and acceleration with negligible performance loss.\n\nConclusion:\n- RIA and Channel Permutation establish new state-of-the-art results for efficient one-shot post-training pruning of LLMs.\n- The proposed methods enable practical acceleration and size reduction of large models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Proposes two novel methods (RIA and Channel Permutation) that provide state-of-the-art performance for post-training pruning of large language models.\n\n2. Comprehensive experiments conducted on multiple popular LLMs across a range of model sizes from 7B to 70B parameters.\n\n3. Evaluated on diverse tasks including language modeling and zero-shot classification to demonstrate generalization. \n\n4. Provides both theoretical analysis and empirical results to demonstrate the efficiency and efficacy of the proposed techniques.\n\n5. RIA and Channel Permutation can be readily combined into an effective pipeline for practical LLM pruning and acceleration, with negligible performance loss."
            },
            "weaknesses": {
                "value": "Overall the manuscript has solid contributions, but expanding the variety of models, tasks, and languages could strengthen the demonstrated effectiveness. Testing scalability and comparing to other recent techniques would also help round out the evaluation. But within the chosen scope, the paper delivers valuable advancements for efficient LLM pruning.\n\n* For \"We employ 128 samples from the C4 dataset\":\nusing only 128 samples from C4 as the calibration data is quite limited. With so few samples, the activation statistics may not sufficiently capture the full distribution."
            },
            "questions": {
                "value": "* what is the channel here in the Transformer models? Transformer models does not have channel or column.\n\n* for \"\"We employ 128 samples from the C4 dataset\", is it possible/worth to do the experiments on a larger and more diverse calibration set (128 might be limited)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814196205,
        "cdate": 1698814196205,
        "tmdate": 1699636046039,
        "mdate": 1699636046039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gGvsqofxWr",
        "forum": "Tr0lPx9woF",
        "replyto": "Tr0lPx9woF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1198/Reviewer_NPvK"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the growing demand for efficient memory and computation in large language models (LLMs). Existing post-training pruning methods have attempted to reduce model size and computation but have not achieved optimal performance. The paper introduces a plug-and-play solution for post-training pruning of LLMs, featuring two innovative components: 1) Relative Importance and Activations (RIA), a novel metric that efficiently considers weight and activations in LLMs, and 2) Channel Permutation, a new approach to maximize the preservation of important weights with N:M sparsity. These components can be combined to enhance N:M structured pruning of LLMs. Empirical experiments demonstrate that RIA alone surpasses existing pruning methods on various LLMs. Moreover, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, while providing practical speed-up on specific hardware."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Consider both weights and activations for unstructured pruning in LLMs is novel.\n- Particularly, the consideration of relative weight importance is a novel approach.\n- Channel permutation is a simple yet effective method for achieving N:M sparsity."
            },
            "weaknesses": {
                "value": "The reviewer recognizes the novelty and simplicity of the overall approach but has raised substantial concerns. The main issues pertain to the weaknesses in the baselines, which make it challenging for me to be convinced of the effectiveness of the proposed method. Moreover, the motivation and analysis provided appear to be inadequate. For example, concerning the former issue, the following questions come to mind: even if the proposed method can enhance the performance of N:M sparsity-based approaches, are N:M sparsity-based methods genuinely effective? Are they superior to contextual sparsity-based methods? \n\nI describe specific questions and suggestions regarding concerns as follows: \n\n- Insufficient experimental support for motivation: This paper argues the existence of 'channel corruption' asserting that removing input/output channels results in decreased performance as observed in prior works. However, the paper lacks empirical evidence to substantiate this claim. It would be valuable if the authors could include preliminary experiments to provide a basis for their motivation.\n- According to AWQ [1], activation-aware weight quantization, which selects important weights based on activation distribution rather than weight distribution, outperforms traditional weight-based quantization. Inspired it, the reviewer suggests that it would be meaningful to consider baseline methods based on activation-based weight pruning for comparison. Therefore, the authors might incorporate and compare unstructured pruning based solely on activations in Table 2.\n- In addition to the comparisons with N:M sparsity methods in Table 4 and Table 5, it is advisable to include a comparison with other structured pruning techniques in terms of performance and inference speed improvement. For instance, including a method like Dejavu [2] in the comparison would enhance the comprehensiveness of the evaluation.\n- What is the relevance of the experiments in Figure 2 to the claim that activation outliers exist independently of the dataset and model's parts? The reviewer thinks that even if activation values exhibit a high correlation between two datasets, it is possible that activation outliers can be eliminated. Therefore, it would be helpful to clarify the connection between Figure 2 and the claim about activation outliers.\n- Can we expect additional performance improvements when combined with post-training quantization methods such as Smooth Quant [3] or AWQ [1]?\n\n[Minor]\n- Why is the title \"plug-and-play\"?\n- The hyperlink in the 6-page appendix seems to be incorrect.\n\n[1] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n\n[2] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time, ICML 2023\n\n[3] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, ICML 2023"
            },
            "questions": {
                "value": "Please address the concerns in Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1198/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212339001,
        "cdate": 1699212339001,
        "tmdate": 1699636045980,
        "mdate": 1699636045980,
        "license": "CC BY 4.0",
        "version": 2
    }
]