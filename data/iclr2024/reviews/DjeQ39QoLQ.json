[
    {
        "id": "DLsrz2WnJu",
        "forum": "DjeQ39QoLQ",
        "replyto": "DjeQ39QoLQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_X2Rr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_X2Rr"
        ],
        "content": {
            "summary": {
                "value": "Explores the diagonal state matrix used by recent linear SSMs such as S4D and S5 which were proposed as an approximation to the original diagonal plus low rank conjugation of the HiPPO matrix explored in the S4 paper. The work analyzes the differences between the S4 and S4D initialization through the transfer function and finds the convergence of S4D to S4 is not uniform. Experiments show that S4D models are sensitive to input perturbations which degrades the model's robustness. A proposed fix is adding a perturbation to the HiPPO matrix to allow for stable diagonalization. The proposed initialization is evaluated on the LRA tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The theoretical analysis seems sound and leads to several interesting insights including implications on the robustness of the initialization of the diagonal models. \n\n- The empirical robustness experiments (synthetic and sCIFAR) help to illustrate the implications of the theoretical divergence \n\n- The proposed fix is relatively straightforward and simple to implement, yet seems to lead to improved robustness results in the synthetic and sCIFAR tasks"
            },
            "weaknesses": {
                "value": "- I am not convinced by the claim the S4-PTD model outperforms the S4D models on LRA. The LRU paper (https://arxiv.org/abs/2303.06349) reports results for S4D that are much better than the original reported S4D paper results. In addition, the appendix of the current paper under review states that mild hyperparameter tuning was performed for the S4-PTD models. Would mild hyperparameter tuning improve the S4D results as well?\n  - To be clear, I am not claiming that S4-PTD must outperform S4D on all tasks to be a valuable contribution, simply that I think the claim about improving performance is too strong, and I would expect to see more results in a fair comparison to support this claim\n\n- More experiment results on a broader range of robustness tasks would help strengthen the paper. At a minimum, perhaps modified versions of the speech and BIDMC tasks from previous papers in this line of work could have allowed for further evaluation of this method on some form of real world data.\n\n- There seems to be a missing piece related to the training dynamics that I think would strengthen the paper. The theory is related to the dynamics matrix $A$ at initialization, but the $A$ matrices are trained during the experiments. How do the trajectories of the eigenvalues differ during training such that the PTD models are more robust during the perturbation experiments? This should be relatively to simple to track during training.\n\n- While it is true the proposed perturbation method could be applied to any dynamics matrix, no other matrices are explored. Perhaps some of the other diagonal matrices proposed in the S4D paper could have been experimented with. Would a similar approach be beneficial if applied to initialize the effective dynamics matrix of the Liquid-S4 model?\n\n- The paper doesn't really seem to support the claim that the S5 models suffer from the same perturbation \nissue. I believe this is likely the case, but no empirical results are presented that support this. Can an example of S5 and S5-PTD be included in the perturbation experiments also?"
            },
            "questions": {
                "value": "Please see weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Reviewer_X2Rr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631140868,
        "cdate": 1698631140868,
        "tmdate": 1700588854150,
        "mdate": 1700588854150,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eCPIgilYTe",
        "forum": "DjeQ39QoLQ",
        "replyto": "DjeQ39QoLQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_faKE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_faKE"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical and empirical study of the frequency-domain implications of various approximations common across deep state space models.  The authors make the connection between the structure of approximation, non-normality of operators, and the non-uniformity of the approximation across frequencies.  This pathology is tested across several synthetic examples. This insight is used to propose a new initialization scheme, which appears to ameliorate the degeneracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper itself is pretty well written, with figures and tables prepared excellently.  I believe the analysis is both a novel and useful insight into this new class of model. The theoretical analysis itself appears correct (although I did not go through the proofs in detail).  The experiments conducted also back-up the theoretical claims (modulo my comments below)."
            },
            "weaknesses": {
                "value": "I think the paper is pretty sound.  My main concerns are about some of the presentation choices, and the \u201csecret sauce\u201d in the contribution.  I have tried to outline some concrete weaknesses here that, if remedied, would improve the paper.  I have then tried to write out my slightly more high-level question in Questions.\n\n**W.1.: The core method**:  Please can the authors clarify what the method actually entails.  As far as i can tell, the \u201cmethod\u201d essentially specifies an initialization for S4D, given in (10) followed by diagonalization.  This is used instead of the diagonalization of the Normal component of the HiPPO matrix in S4D.  If this is true, this should be absolutely, concretely clearly stated somewhere.  I think right now it is tied up in the introduction with spectral analysis, PTD, backwards/forwards approximations etc.  \n\n**W.2.: Choice of what the paper emphasizes**:  The authors have clearly done _a lot_ of theoretical analysis.  However, I think the net result is an incredibly simple result \u2013 adding a random perturbation makes models more robust.  I really think this result, and any usable empirical analysis of this result, should be highlighted.  Some of the theoretical points should be relegated to the appendix.  If the results are to be believed, then the banner result should be \u201cignore DPLR/diagonal state spaces and use random perturbations\u201d.  This core message is not highlighted.  \n\nSimilarly, there are some nice experiments in the supplement demonstrating the pathology experimentally, e.g. fig 7-10.  These should be brought up to the main, as they help flesh out the arguments far more than a detailed introduction to the intricacies of the conjugation of HiPPO or some of the theorems.  I think this will help improve the reach and impact of the paper.  \n\n**W.3.: Figure 2 results**:  I find the results in Figure 2 very hard to parse out.  The core point (to my understanding) is that S4-PTD is a faithful approximation of S4.  However, I do not necessarily see why S4Ds predictions _should_ be reminiscent of S4 \u2013 it\u2019s a different model! \u2013 or that it should be worse in the way predicted.  I think it is coincidental that S4D is so much worse than S4(-PTD) on this task.  I ask the authors to clarify the intuition and results of this experiment.  (See also Q.1.)  \n\nI think it is also somewhat coincidental that S4-PTDs predictions look like S4 \u2013 surely different random seeds should result in different predictions? \n\n**W.4.: Missing definitions and exposition**: Lots of terms are basically undefined, e.g.:\n- The introduction of \u201cbackward stability\u201d is deferred to the appendix, but is really the core of the method.  There is no discussion in the main really of why forward/backwards even relevant, or why existing methods fall down here.\n- Pseudospectral theory is undefined, nor why adding a random perturbation ($E$) helps.  \n- Why spectral information with a poor condition number is \u201cmeaningless\u201d.\n\nThis makes the contribution a little less self-contained.  I would rather drop some of the theoretical analysis from the main to the supplement and bring up more of the core definitions.  This would dramatically improve the impact of the work.  \n\n**W.5.: No conclusion or discussion**:  Not really a weakness, but there is no conclusion, discussion or outlined future work.  It would be nice to see some critical self-reflection on the work.  It would also help you reinforce the core messages you are trying to convey in the paper.  \n\n## Minor weaknesses\n- (e.g.) \u201csection 3.4\u201d should be capitalized.\n- The explanation of Figure 2 on Page 6 is very poor.  \n- use \\citet and \\citep where appropriate.\n- Author style in the bibliography is inconsistent."
            },
            "questions": {
                "value": "**Q.1.**  This is a very general question, but something that I cannot wrap my head around.  I invite the authors to try and help me understand.  (The remedy to this will also affect my perception of the weaknesses) \n\nThroughout the text this is an analysis of (e.g.) HiPPO initialization, but the results presented are for trained models.  I am surprised that the pathology predicted by the initialization is preserved through training quite so neatly.  For instance, in Figure 3, how do you obtain the frequencies that the model should be perturbed at?  Do you inspect the trained model, or the a-priori specified HiPPO matrix?  Each channel in the S4D model should have different frequencies, and so are you corrupting them all?  Or just one? \n\nMy follow-up is why does the approximate diagonalization on initialization lead to more robust trained models?  Presumably $E$ isn\u2019t learned?  Once the model is diagonalized, the initialization is essentially thrown away.  As a result, there is a real disconnect in Figure 3.  It looks like S4-PTD might be _slightly_ worse than S4, but the training curves are ostensibly identical.  But the robustness to perturbations is clear.  Why do these pathological frequencies survive training in S4?  \n\nI think there might be two contributions here that are currently wrapped up in one-another.  The first contribution is an analysis of the frequency properties of a hypothetical (single) SSM.  The second contribution is an initialization scheme for systems of deep SSMs that is derived from some of this insight, but doesn\u2019t necessarily directly solve the initial problem.  \n\nFor instance, I would like to see an experiment sweeping over the perturbation frequency (e.g. combining Figures 1 & 3), and showing that there are certain choice frequencies that destroy performance, as opposed to just a model that is less robust to any perturbations.  I think this is an important distinction that is somewhat predicted by the theory, but is unverified.  I think this is what Figure 6 might be driving at, but to my eye, it looks like the diagonal system is just unilaterally worse than DPLR.  This would also be an empirical validation of Figure 1.  I would like to see the results for an untrained system, a single trained DPLR system, and also for an S4D model (where there are multiple S4 systems in parallel, each with their own characteristics).  \n\nIn that vein, I'd also like to see this graph traced out over training and across different input frequencies.  E.g. If the training data has a lot of power in a frequency where there is a peak in the transfer function, then how does/does the model learn to move that peak away from that frequency?  \n\nI invite the authors to comment on this.  I realize my thoughts on this are a little jumbled, but I think this is maybe emblematic of a shortcoming in the presentation or cross-linking of the ideas.  If the authors want more clarification, I will happily try and restate my concerns.  \n\n**Q.2:**  Why is it relevant that the final peaks in Figure 1 are all (roughly) the same height?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679911360,
        "cdate": 1698679911360,
        "tmdate": 1699636378416,
        "mdate": 1699636378416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ou5URGZr9H",
        "forum": "DjeQ39QoLQ",
        "replyto": "DjeQ39QoLQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_eMiw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_eMiw"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a deep theoretical analysis of various aspects of S4 models.\n\nFirst, it provides a much more fine-grained analysis of the difference between the S4-DPLR and S4-Diag models, the two most well-known instantiations of S4 models (more precisely with the \"HIPPO LegS\" initializations), than prior work, including\n- Exact analysis of the difference in transfer functions\n- This allows providing non-asymptotic analysis of the convergence between the DPLR HIPPO init and its diagonal approximation by dropping the rank-1 term. As opposed to prior work which only showed it asymptotically in the limit of state size $n$\n- This also leads to better understanding of what type of input perturbations lead to divergence between the approximation\n\nThe technical machinery leads to a very simple but well-motivated solution to the stability issues of S4D, by perturbing the $\\mathbf{A}$ matrix before diagonalizing. This method presents modest empirical improvements over the original methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Extremely clearly written technical background and presentation of technical results.\n2. The discussion of the relationship of S4-DPLR and S4-Diag from a functional analysis and transfer function perspective is strong and substantially contributes to the theoretical understanding of these popular models. The theoretical derivation of their properties and validation through synthetic experiments is well-done.\n3. The main motivation of the perturb-then-diagonalize (PTD) being justified because only backwards error (instead of forwards error) matters is a nice idea.\n4. The empirical results are strong and ablations are well-motivated.\n\nOverall, the paper has high originality, quality, and clarity."
            },
            "weaknesses": {
                "value": "The only potential weakness of this paper is the practical significance to the machine learning community. To my knowledge, in practice most applications of S4 use the diagonal variants to no noticeable detriment, and also often use the simpler S4D-Linear initialization to which the present theory has less practical implications for. Put another way, while technically strong, it is not clear whether the ideas can lead to truly new capabilities for machine learning models.\n\nOn the other hand, this paper does provide a new direction toward revitalizing interest in the HIPPO theory and other (perhaps yet undiscovered) flavors of SSMs, which may still provide practical benefits in more specific regimes (e.g. limited data or when a strong inductive bias towards the memorization interpretation of HIPPO is desired). And as a mainly theoretical paper with strong contributions towards the theory of these popular models, in the context of this paper this is not a major weakness."
            },
            "questions": {
                "value": "End of Sec 3.1:\n> Moreover, for every $n \\ge 1$, zooming into the last spike...\n\n1. is the constant magnitude a conjecture, or easy to show from Lemma 1? How were the final spike locations derived? Seems like it should be possible to find the exact location for every $n$ and calculate the magnitude of the spikes from Lemma 1, although maybe the calculations are actually difficult.\n2. I'm confused how to interpret left side of the plot in relation to the right: are the final yellow/orange/blue spikes supposed to actually go up to magnitude $\\approx 1$ (same as the purple spike), but the sampling of the plot x-axis is too coarse grained to get the exact location of the spike so is not rendered precisely?\n3. The text claims the last spike is located at $|s| = \\Theta(n^2)$ but it seems like larger $n$ correspond to earlier spikes (this seems important as it is related to a later question)\n4. Is there a limiting result of the nature of $G_{\\text{Diag}}(n \\to \\infty)$? These plots seem to suggest that the width of the spikes also decreases, but the rate is not clear. Depending on the nature of this limit, it's not actually clear to me that $|G_{Diag}(si)|$ does not converge uniformly: the domain is bounded from below ($s \\ge 0$) so the location of the spike can't move too fast; if the spikes also have width that does not decrease too fast, then it seems like it actually could converge uniformly? On the other hand if the domain is transformed logarithmically as in Figure 1, then I would believe that $G_{Diag}(e^s i)$ does not converge uniformly, so maybe that's what you mean? (Also, is there a reason Figure 1 only plots frequencies $\\ge 1$?)\n\nIn Section 3.3:\n> the outputs of the two systems diverge given the unit impulse (i.e., the Dirac delta function) as the input\n\n5. I'm not sure of the accuracy of this statement, or at least am confused how it relates to results from the S4D paper. If I understand correctly, the outputs of the two systems given the unit impulse should just be the impulse response or the \"continuous convolution kernel\" $K(t) = C e^{tA} B$. However the S4D paper indicates that this does converge, at least pointwise. (See S4D Figure 2, and the corresponding reproduction: https://github.com/HazyResearch/state-spaces/blob/main/notebooks/ssm_kernels.ipynb) To my understanding those results are generated in the same setting as Appendix F.3, Figure 9 in this submission, but the results look quite different.\n6. Additionally the associated discussion says \"The oscillatory behavior can be explained by our observation in Figure 1: the larger the $n$, the later the spike emerges. This means that for a larger $n$, the outputs of two systems differ at a higher frequency (i.e., a more oscillatory mode)\". But Figure 1 seems to say that larger $n$ corresponds to spikes at lower frequencies (which is related to another question above)\n\n7. Section 3.4: I think the description of the experiment does not state which $n$ is used; based on Fig 1, it seems like it is fixed to $n=10000$?\n\n8. Theorem 3: how tight is the upper bound empirically? This result says that as $n$ grows, even if the total norm of the error matrix is controlled (hence the entries decrease), the total output deviation still increases with larger $n$. In practice, does the output deviation increase with state size?\n\n9. How is the hyperparameter $\\gamma$ set for each dataset? It doesn't appear in the hyperparameter table. From Figure 3(c) it seems as if the results can be somewhat sensitive to this parameter.\n\nOverall I think this paper is a very interesting and technical deep-dive into the theory of S4 models.\nI am happy to increase my soundness score and overall score after discussion of some of these questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Reviewer_eMiw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757737202,
        "cdate": 1698757737202,
        "tmdate": 1700706552752,
        "mdate": 1700706552752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y4qExAvEc6",
        "forum": "DjeQ39QoLQ",
        "replyto": "DjeQ39QoLQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_Xrm4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4131/Reviewer_Xrm4"
        ],
        "content": {
            "summary": {
                "value": "The authors study the State-space models (SSMs), which have shown promising results on long range sequence tasks. To speed up SSMs, diagonal approximation has been considered, which works well in practice but lacks theoretical guarantees. In this work, the authors propose a generic, backward stable perturb-then-diagonalize (PTD) strategy. The theoretical analysis shows convergence guarantees and proves the stronger robustness of the proposed method. The empirical evaluation also shows strong performance of the proposed method with the compared baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method has theoretical guarantees for convergence and robustness.\n2. The empirical result shows solid performance of the proposed method."
            },
            "weaknesses": {
                "value": "1. The writing is quite dense and can be improved if the authors make it more self-contained."
            },
            "questions": {
                "value": "1. Can the author elaborate more on why the proposed method can outperform the ones without diagonalization? This seems to be quite interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4131/Reviewer_Xrm4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831899049,
        "cdate": 1698831899049,
        "tmdate": 1699636378248,
        "mdate": 1699636378248,
        "license": "CC BY 4.0",
        "version": 2
    }
]