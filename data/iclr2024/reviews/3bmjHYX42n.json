[
    {
        "id": "SHMnc06F7t",
        "forum": "3bmjHYX42n",
        "replyto": "3bmjHYX42n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_Qjey"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_Qjey"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Revision-Aware Reward Models (RARE), which leverages human revisions to strengthen alignment in the context of generative layout models for mobile screens. The authors involve expert designers in fixing layouts generated by a pretrained generative layout model and train a reward model based on how these designers revise the generated layouts. By optimizing the model using reinforcement learning from human feedback (RLHF) with the learned reward model, RARE can enhance the generative model's ability to produce modern, designer-aligned layouts. On a dataset of 276 corrected UI Layouts from designers, the authors compare the proposed method with Supervised Finetuning (SF), Preference Reward + RLHF, Chamfer Distance + RLHF, and the results show the potential of the method quantitatively and qualitatively."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method focuses on utilizing nuanced feedback, such as corrections, explanations, and reasoning, to enhance generative models. While prior works have primarily relied on high-level labels, the authors' emphasis on more involved feedback represents a novel perspective. By introducing the concept of Revision-Aware Reward Models (RARE) and applying it to generative layout models, the research offers a unique contribution to the field.\n\n2. The involvement of expert designers in fixing layouts generated by a pretrained model adds credibility to the evaluation process. The training of a reward model based on human revisions and the subsequent optimization of the generative model using reinforcement learning demonstrate a rigorous and systematic approach. The reported results strengthen the overall quality of the work.\n\n3. Overall, the paper is well-written and easy to follow. It provides a clear explanation of the research objectives and the proposed methodology. The authors effectively communicate the significance of utilizing nuanced feedback and human revisions in improving generative models."
            },
            "weaknesses": {
                "value": "1. The analysis of the dataset used in this work is not comprehensive. It would benefit the research to provide an overview of the dataset, including its characteristics and why the proposed method is well-suited for this specific dataset. Additionally, to ensure the generalizability and robustness of the proposed approach, it is crucial to evaluate its performance on diverse datasets or domains. Conducting experiments with the RARE approach on different types of layouts, such as web or desktop interfaces, would provide a more comprehensive understanding of its effectiveness and applicability in various contexts.\n\n2. While human revisions are utilized to train the reward model, there is a notable absence of analysis or insights into the specific patterns, reasoning, or design principles underlying these revisions. Gaining a deeper understanding of the factors driving the revisions made by human designers would offer valuable insights for further improving the generative model. Conducting a thorough analysis of the revisions, such as identifying common patterns or design choices, would enrich the understanding of the alignment between human values and generative outputs and provide guidance for refining the model's performance.\n\n3. The qualitative results presented in the paper lack detailed analysis, as only a few examples are provided. To strengthen the research, it would be beneficial to offer expert opinions and insights to support and explain why the results obtained with the RARE approach are considered good.\n\n4. Some minor typos are present in the paper, such as in section 5.2, paragraph 2, where the first sentence contains two instances of the word \"that.\""
            },
            "questions": {
                "value": "1. What is the size of the collected dataset?\n2. For the qualitative results, are there any implications derived?\n3. As the data collection involves human subjects, is the study proved by IRB?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Reviewer_Qjey"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698294563205,
        "cdate": 1698294563205,
        "tmdate": 1699636434397,
        "mdate": 1699636434397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wrvGjs5KFW",
        "forum": "3bmjHYX42n",
        "replyto": "3bmjHYX42n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_d2Zb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_d2Zb"
        ],
        "content": {
            "summary": {
                "value": "The paper collects a dataset of sequences of human designers revising model-generated mobile app layouts. The paper proposes a method called RARE to learn a reward model based on the collected data and use the reward model to perform RLHF finetuning on a pre-trained layout generation model. Experimental results show that the proposed method is better than simple finetuning or other reward models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper collects high-quality datasets from expert app layout designers. The dataset could be important for the community. \n- The method is simple but effective. \n- The proposed method is much more effective than simple finetuning."
            },
            "weaknesses": {
                "value": "- The novelty might be limited. It seems that the novel part of the method is how training samples are constructed from the collected dataset to train RARE. Other parts like the diffusion models and RLHF are similar to existing work. \n- I am not quite convinced by sec. 4.2 on the reward model pretraining. The construction of pretraining data seems a bit too heuristic and are not grounded on any reasonable arguments/observations. Why do you assume dropping needs 1 time step, revised elements needs 2 time step, and added element needs 3 time steps? The parameters for each operation, e.g. resize 0.5-2 times have no explanation as well. \n- Limited evaluation metrics. Previous work has considered other metrics like NLL, Coverage, or Overlap. Are these reasonable metrics for the experiments considered in this paper? If not, are there other possible metrics beyond FID? Is it reasonable to conduct user study to make the results more convincing?"
            },
            "questions": {
                "value": "Table 1 implies that RARE and Preference has the same FID, yet Fig. 6 shows that RARE is better than Preference. Is it the case that RARE is better than Preference for most of the evaluation samples? If so, why do these methods have the same FID?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Reviewer_d2Zb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727742680,
        "cdate": 1698727742680,
        "tmdate": 1699636434308,
        "mdate": 1699636434308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aJNkWzeV0S",
        "forum": "3bmjHYX42n",
        "replyto": "3bmjHYX42n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_otPj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_otPj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes improving the layout generator model with human revision feedback. The work experiments with using the time-taken by human editor, distance between the revision, sft, and binary preferences to test improving the model RLHF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper investigates using more nuanced feedback rather than binary preference, which is a less explored area of research."
            },
            "weaknesses": {
                "value": "The papers outline is not in a typical format, the dataset description comes later, I struggled to imaging the dataset while reading the experiments section without reading the dataset description before.\n\nThe equations used in the paper aren't fully explained and in some places the symbols used in the equation and the description are inconsistent. I did not get a full understanding of the background reading the paper because of this. Maybe the authors can reduce the size of the figures or move them to the appendix section to get more space.\n\nThe experiment aren't rigorous and the results were not analyzed properly\n- The explanation of why the Chamfer distance did worse than even the preference-based model isn't convincing. The appendix section shows that Chamfer models were trained by much more iterations in all the steps (49000 vs 2000) than all the other models, could it be just that the Chamfer model just got overfitted? To analyze the problem the author should compare chamfer vs time distance distributions and/or have more comparable training iteration numbers to start with.\n- Since the proposed approach performs similarly to the preference-based model, the authors should investigate more into this with more random seeds to start with."
            },
            "questions": {
                "value": "Although this is a more recent work, it might be worth looking into this work which also looks into using human revision information https://arxiv.org/abs/2310.05857.\n\nCan you train without RL? Like what is done in the paper? It probably needs alignment between the human edits (which element got changed into what).\n\nI am surprised that there were no guidelines for humans who revised the layout. Have you seen any undesirable edits in the dataset?\n\nAlthough it isn't completely clear, I am assuming you are using every edit by a person on the layout as a separate edit. It might be worth clustering some of them.\n\nAccording to the description, reward model predicts the time/distance between the revisions. Does it then mean that it is a penalty model rather than a reward model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Reviewer_otPj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819657593,
        "cdate": 1698819657593,
        "tmdate": 1700846053885,
        "mdate": 1700846053885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "otJFQLblh8",
        "forum": "3bmjHYX42n",
        "replyto": "3bmjHYX42n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_RQ9B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4564/Reviewer_RQ9B"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to train a reward model from human designer revisions on generated layouts by a pre-trained layout model. Then, they optimizer the down-stream model by RLHF with the trained reward model. In this way, RARE aligns the model with human preference to produce more designer-aligned layout."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel approach to integrate different human feedbacks into model training, i.e., the step-by-step revision sequences. \n2. The reward is designed to correlate with revision time, which provides better signals than binary comparison rewards."
            },
            "weaknesses": {
                "value": "1. Though the paper presents a new notion of human feedback, i.e., revision sequences, its application to layout generation makes its applicability quite constrained. The first time I read the abstract, I thought the paper seemed to propose a general methodology for RLHF. After going through the paper, I realized that the proposed reward training is only specifically designed for the text-to-layout generation domain. \n2.\tThe evaluation is not sound to me. In section, the major quantitative evaluation results are presented in Table 1; the remaining evaluation are mainly shown by generative examples. All of results are about one task. More quantitative evaluation evidence can make the conclusion sounder. \n3.\tThe presentation can be improved. The equation (4) can be confusing. It will be better to list them following time ordering."
            },
            "questions": {
                "value": "Can authors provide more details of the CLAY dataset, like its statistics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4564/Reviewer_RQ9B"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699248468271,
        "cdate": 1699248468271,
        "tmdate": 1699636434120,
        "mdate": 1699636434120,
        "license": "CC BY 4.0",
        "version": 2
    }
]