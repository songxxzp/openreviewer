[
    {
        "id": "2N2qmhU0YW",
        "forum": "Bo6GpQ3B9a",
        "replyto": "Bo6GpQ3B9a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_w3Ke"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_w3Ke"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes techniques for semi-supervised learning using distributional robust optimization and self-training. In addition, the methods proposed can utilize unlabeled samples that come from a different (but similar) underlying distribution. The paper also describes theoretical results that show generalization bounds in situations where the data follows Gaussian distributions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The research topic is very relevant since the usage robust learning methods can facilitate the usage of unsupervised samples. In addition, the usage of unsupervised samples corresponding with a different distribution is also of interest and common in practice. Also, the development of theoretical guarantees in these settings is very relevant"
            },
            "weaknesses": {
                "value": "The paper's contributions are not clear. Firstly, there are several methods for semi-supervision based on DRO [Najafi et al., 2019], [R1], [R2]. The methodological contributions in the submitted paper with respect to those works are unclear. The usage of unsupervised samples from a different distribution seems novel in this topic, but the distribution shift assumed in the paper seems too simplistic and straightforward to address by DRO methods (small change in the covariates' marginal). In addition, the authors should extend the experimental results, compare with existing methods for semi-supervision and use more common benchmark datasets, otherwise it is not possible to assess the relation with existing methods.\n\n[R1] Jose Blanchet and Yang Kang. Semi-supervised learning based on distributionally robust optimization. In Data Analysis and Applications. 2020.\n\n[R2] Charlie Frogner, Sebastian Claici, Edward Chien, and Justin Solomon. Incorporating unlabeled data into distributionally robust learning. Journal of Machine Learning Research. 2021."
            },
            "questions": {
                "value": "Utilizing bounds as those in (8) does not seem very meaningful for the case of linear classifiers for which a bound based on the norm of the parameters (Rademacher complexity) would be tighter. Is that the case?\n\nAfter (6), \u201cHilbert space of constrained probability measures\" is a typo?\n\nThe usage of the word self-supervision can be misleading since the methods proposed utilize techniques that obtain pseudo-labels for unsupervised samples that are usually referred to as \"self-training.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7586/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7586/Reviewer_w3Ke",
                    "ICLR.cc/2024/Conference/Submission7586/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697819249747,
        "cdate": 1697819249747,
        "tmdate": 1700407972602,
        "mdate": 1700407972602,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CVi35Ai47Y",
        "forum": "Bo6GpQ3B9a",
        "replyto": "Bo6GpQ3B9a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_Nric"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_Nric"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a robust learning approach that leverages slightly out-of-domain unlabeled observations to improve generalization performance. Both theoretical and empirical analysis of the method are provided. The approach leverages a robust learning objective and minimizes this objective on a combination of labeled data and pseudo-labeled out-of-domain data. This approach forces the decision boundary to avoid crowded areas of the input space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method pushes the classifier to avoid crowded areas, which is similar in spirit to large margin methods. Making use of unlabeled data seems to improve its ability to do this. The analysis provides a novel non-asymptotic learning bound for Gaussian Mixtures. The method also has well motivated controls for dialing in the bias-variance trade-off."
            },
            "weaknesses": {
                "value": "How does this method compare against large-margin based methods? What if one treats the \"slightly out-of-distribution\" data as in distribution? Some comparison with similar approaches is warranted. \n\n\"given\" is misspelled in the abstract.\n\nWhile I did not work through all the details, the \"new set\" of bounds appear to be based heavily on an upper bound of the Rademacher complexity. I feel this really ought to be stated in the abstract and main body of the paper. I don't think that comparing Rademacher complexity with VC dimension based bounds is really an advancement. \n\nWhy doesn't the amount of \"slightly out of domain\" enter into the bound $n>\\Omega(\\frac{m^2}{d})$? Certainly data from totally different domains wouldn't improve generalization, right?"
            },
            "questions": {
                "value": "Can the authors add more details about the nature of their bounds in the main text to either confirm or soften claims of novel methodology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709268854,
        "cdate": 1698709268854,
        "tmdate": 1699636918934,
        "mdate": 1699636918934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2tdlOQcC4S",
        "forum": "Bo6GpQ3B9a",
        "replyto": "Bo6GpQ3B9a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_v7JJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_v7JJ"
        ],
        "content": {
            "summary": {
                "value": "The advantage of unlabeled data in coming up with sample-efficient robust classifiers is well known and most studied in the canonical case of classifying a mixture of Gaussian models. Prior works that analyze the tradeoff between labeled and unlabeled data for robust classifiers consider robustness w.r.t. perturbing inputs, in other words, adversarial robustness; while this paper views robustness as obtaining unlabeled data from a perturbed distribution in a Wasserstein ball of some radius, commonly known as being 'distributionally robust'. The authors propose an algorithm that adds a regularizer to the conventional ERM loss function and obtain sample complexity bounds for this algorithm to obtain PAC-optimal linear classifiers for classifying a mixture of Gaussian models. The regularization term is derived from the computing a 'robust loss' on the unlabeled data using labels obtained from a model that is trained on the labeled data. The robust loss is taken from the dual form of the distributionally robust optimization problem, Experimental results on both real and simulated data show that using a large amount of unlabeled data almost achieves the performance of the optimal classifier."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While multiple works have proposed algorithms that show the advantage of unlabeled data for obtaining robust classifiers with high accuracy, this paper's contribution lies in providing an algorithm for the distributionally robust framework that has theoretical guarantees for the linear classification for the Gaussian mixture model case. The latter has also been studied in other robustness frameworks thus highlighting the significance of studying it in a different robustness setting. The algorithm seems quite natural since the robust loss has been considered in prior works. While I haven't thoroughly verified the correctness of the proofs, the theorems seem reasonable. The paper is well-written and clear barring a few exceptions."
            },
            "weaknesses": {
                "value": "A few correctable weaknesses follow that could help strengthen the paper: \n1) The comparison with related works isn't thorough in the sense that the paper mentions these related works but doesn't provide any comparison of their current work with it. For e.g. the paper doesn't mention that the works of Carlini et al, Carmon et. al. etc. were for the adversarial robustness setting. It would make the contributions stand out more clearly if there is precise comparison as to how earlier work is different from the current. \n2) The experimental results could be more systematic and require more explanation. Please see the following sections for specific questions that can help strengthen this part. \n3) A few minor clarifications are required in the theoretical section. I have elaborated in the following section."
            },
            "questions": {
                "value": "1) There are multiple anomalies that are unexplained in the experiments. Perhaps addressing that would make the results less surprising. Few examples: i) In the simulated data, it would helpful to know roughly the least number of labeled samples required to obtain the same accuracy as the optimal. ii) In the real dataset case, the different distribution case achieves higher accuracy for fewer labeled samples. It is not clear why this column shouldn't be the same or worse as the same distribution case since it is using only the labeled samples that are drawn both from the NCT-CRC-HE-100K dataset? \n2) It should be possible to empirically verify the dimension dependence? \n3) On Pg5, what is \"crowded areas\"? This notion makes an appearance in the conclusion as well, but there is no explanation about what this is? \n4) In Theorem 4.1, why is the expectation only w.r.t P_0 since we also get unlabeled samples from P_1. If this is included in the high probability statement, then that seems strange because I would assume the algorithm randomness to be separately considered from the unlabeled dataset randomness. \n5) Is there any intuition for why the error in robust loss increases with \\gamma? \n6) Minor comment: I found the notation in (1), \\hat{R}(\\theta, S) confusing and also not used later (unless in the proofs). Isn't this just R(\\theta, \\hat{P}^{m}_S) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7586/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7586/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7586/Reviewer_v7JJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836616404,
        "cdate": 1698836616404,
        "tmdate": 1700478549079,
        "mdate": 1700478549079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J2Kdhv4uVb",
        "forum": "Bo6GpQ3B9a",
        "replyto": "Bo6GpQ3B9a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_TZRA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7586/Reviewer_TZRA"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to utilize unlabeled samples from a perturbed distribution to improve the generalization error in both adversarially robust and non-robust settings. It introduces a new algorithm that leverages adversarially robust optimization and self-supervised learning. Subsequently, by focusing on the linear Gaussian mixture model, the paper shows that the use of unlabeled samples can lead to an improvement in error rates compared to traditional ERM, which does not make use of unlabeled samples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper analyzes the generalization error of the newly introduced algorithm, which utilizes self-supervised learning and adversarially robust optimization, demonstrating an improvement in error compared to traditional ERM.\n\n- It provides experimental results corroborating their theoretical findings that unlabeled samples from a perturbed distribution can reduce the test error."
            },
            "weaknesses": {
                "value": "- The paper's linear Gaussian mixture model is very restrictive.\n\n- The manuscript dedicates a substantial portion to discussing established definitions and findings in the literature. In contrast, the final three pages primarily center on discussing the paper's contributions."
            },
            "questions": {
                "value": "When n=0, no out-of-domain samples are utilized and the problem reduces to simple ERM. But in Theorem 4.2, when n=0, the dependence of the error on dimension is d^{3/8}, meaning that this reduction in the exponent of the dimension is not related to the utilization of out-of-domain samples. Furthermore, in the case of n=0, why is the non-robust error better than error obtained through ERM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863723863,
        "cdate": 1698863723863,
        "tmdate": 1699636918708,
        "mdate": 1699636918708,
        "license": "CC BY 4.0",
        "version": 2
    }
]