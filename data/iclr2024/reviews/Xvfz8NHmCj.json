[
    {
        "id": "zx6D9zwrno",
        "forum": "Xvfz8NHmCj",
        "replyto": "Xvfz8NHmCj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_uZcN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_uZcN"
        ],
        "content": {
            "summary": {
                "value": "The authors study continual learning with a restricted computational budget (defined by FLOPs) per time step. Demonstrating that pre-existing supervised and semi-supervised methods struggle in this setting with overfitting or lack of adequate computation resources, the new semi-supervised method DietCL is proposed."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The setting of limited compute and labeled data is important and realistic, and the proposed method is well-adapted to this setting\n- Defining compute budget by FLOPs allows us to consider being unable to even complete a full epoch, which is challenging for learning but useful and realistic for large datasets and small compute capabilities\n- Evaluation of other prior methods seems to be fair and complete"
            },
            "weaknesses": {
                "value": "- Fig 2 shows that DietCL does still suffer from overfitting, just not as bad as other methods (particularly ER) and takes much larger computational budget for the effect to be bad. In the caption and paper text authors claim DietCL doesn't suffer from overfitting, which seems to be a bit of an overstatement"
            },
            "questions": {
                "value": "- In the task-balanced buffer $\\mathcal M$ introduced in Section 4, why do you include just data from the current and previous time step? If budget allows it, why not include data from more time steps or ensure you have an adequate diversity of examples from different classes included in the buffer?\n- Questions about the reported empirical numbers: How many runs are these reported across (for instance to produce the figures 2-6) and how large is variance across the runs? Does the ordering examples are presented during training impact learning a lot, and are some methods more robust to this than others?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Reviewer_uZcN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698428572923,
        "cdate": 1698428572923,
        "tmdate": 1699636135446,
        "mdate": 1699636135446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2WrPkeI1l2",
        "forum": "Xvfz8NHmCj",
        "replyto": "Xvfz8NHmCj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_dSE4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_dSE4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes \u201cCL on Diet\u201d, which is a large-scale semi-supervised continual learning setting, which is an extension of budgeted continual learning to a semi-supervised setting.\nDietCL utilizes budget efficiently with joint optimization of labeled and unlabeled data, and comes with a computation budget allocation mechanism to harmonize the learning of current and prior distributions.\nThe experiments demonstrate that DietCL outperforms both supervised and semi-supervised continual learning\nalgorithms in sparsely labeled streams by 2% to 4%."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. By asking several questions, Section 3.2 successfully gives the motivation of DietCL by showing the shortcomings of existing methods under low budget scenario.\n2. DietCL is well-designed with sufficient details given. For example, using unlabeled data as a regularizer, DietCL can effectively eliminate overfitting and the stability gap issues.\n3. Sufficient details are given for the experiment part."
            },
            "weaknesses": {
                "value": "1. Comparison with (Prabhu et al., 2023) is needed in the experiment section, since it belongs to the supervised continual learning methods.\n2. need more connections to real-world problem: It is unclear how far is a real-scenario from the low budget setting given in this paper.  It would be better to further discuss a real example (such as snapchat example), by quantifying how many budget per time step is actually possible for a real scenario (with typical hardware).\n3. The Ablation study uses 'Replay -> Lm -> Balanced Buffer -> Lr', how about trying 'Replay -> Lm -> Lr -> Balanced Buffer'  or another order. some different observations may be obtained.\n4. small presentation issues\n  (1) Confused notation. In Appendix A, subscriptions {l,u,m} are used to demonstrate labeled, unlabeled and buffer data. While in section 4, {r,m,b} are used.\n   (2) typo in Figure 6 'to make them easier to compare?'"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Reviewer_dSE4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698546036572,
        "cdate": 1698546036572,
        "tmdate": 1699636135363,
        "mdate": 1699636135363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZNzIrZ62TC",
        "forum": "Xvfz8NHmCj",
        "replyto": "Xvfz8NHmCj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_TT9Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_TT9Y"
        ],
        "content": {
            "summary": {
                "value": "This paper explores a problem setting of data efficient continual learning, where only a small portion of available data is labeled and the use case demands fast training compute. The paper introduces a continual learning algorithm that combines self-supervised on the unlabeled data with the conventional supervised continual learning strategy on the labeled subset. The algorithm divides the compute budget equally between these components to demonstrate that the proposed method outperforms existing approaches and helps to reduce overfitting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem setting is specific and there is detailed discussion on the challenges of existing methods. The proposed method is simple and seems to perform well under many ablations."
            },
            "weaknesses": {
                "value": "1. The overall method is quite simple, which in itself is fine, but would warrant extensive ablation on the design choices. For instance, the algorithm depends on MAE as a way to generate good pre-trained features. How would the algorithm perform with alternative representation learning strategies? Furthermore, the buffer threshold is determined via cross-validation. How difficult is this cross-validation to perform and how many time steps / tasks are needed to learn it?\n2. The ablations provided seem to compare against the semi-supervised algorithms. However, it is clear from Table 1 that the comparable semi-supervised algorithms are outperformed by DietCL, whereas supervised techniques such as ER and ER-ACE are competitive and almost as good as DietCL. It would have been more interesting to see ablations against these supervised learning baselines, for example to demonstrate the diet limits under which the proposed method is challenged.\n3. Notation in page 6 defining A(t) and A seem to be incorrect due to overloading $t$"
            },
            "questions": {
                "value": "What is the non-monotonic behavior of DietCL in Fig 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603428869,
        "cdate": 1698603428869,
        "tmdate": 1699636135266,
        "mdate": 1699636135266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SwIzpszPqv",
        "forum": "Xvfz8NHmCj",
        "replyto": "Xvfz8NHmCj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_GSPx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2037/Reviewer_GSPx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DietCL to conduct semi-supervised continual learning under a label and computation budget. The main idea is to formulate a loss function that considers a reconstruction loss, a masking loss, and a budget loss at the same time."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Continual learning under a constrained labeling and computational budget is an interesting problem.\n* The approach seems to perform well in the evaluated scenarios."
            },
            "weaknesses": {
                "value": "* The paper lacks clarity and is not well-organized. For instance, there is a related work section (Sec. 2), but the discussion of prior methods continues all the way until page 5 and there is a separate section (Sec. 3.2) for additional coverage of prior work. The challenges of prior work is reiterated numerous times in the first 5 pages, and despite their mention in the abstract and introduction, they are mentioned again on page 4-5 \u201cChallenges facing existing semi-supervised continual learning algorithms.\u201d It is not until page 5 that DietCL is introduced and its coverage is limited to less than a page, which leads to an incomplete presentation of the method.\n* The method\u2019s novelty and clear exposition is lacking. What is the key insight here? It seems that the method boils down to using an existing self-supervised learning method (MAE) coupled with masking out logits of classes not shown in the current time step. \n* The loss function (Eq. 4) simply adds the three different loss terms without any hyper-parameters in front of them to scale them appropriately. For instance $\\mathcal L_r$ is the reconstruction loss (Euclidean distance squared of vectors), and it is being added to the loss function with two other losses that are cross entropy. It seems like there will inevitably be a scaling issue. Even if additional hyper-parameters, e.g., $\\alpha_r, \\alpha_m, \\alpha_b$ were added in front of the loss terms, this would entail hyperparameter optimization to find the right values."
            },
            "questions": {
                "value": "1. What is the motivation for the loss function in (4), where each term is weighted equally? How would this generalize to other settings where equal-weighting may not lead to desired behavior (e.g., it might overemphasize the reconstruction loss over others, or the budget loss over others)?\n2. What is the key novelty that enables the method to outperform prior approaches? Is it the explicit consideration of the budget in the loss function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2037/Reviewer_GSPx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2037/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774055799,
        "cdate": 1698774055799,
        "tmdate": 1701021954213,
        "mdate": 1701021954213,
        "license": "CC BY 4.0",
        "version": 2
    }
]