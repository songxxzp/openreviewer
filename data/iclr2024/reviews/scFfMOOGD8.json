[
    {
        "id": "V0WHlLWvUL",
        "forum": "scFfMOOGD8",
        "replyto": "scFfMOOGD8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_RZ4Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_RZ4Z"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the realm of diffusion models, which are vital for high-quality image generation but come with potential security risks. A significant challenge has been the vulnerability of these models to backdoor attacks, primarily using noticeable, manually-designed triggers. The authors introduce a cutting-edge optimization framework that develops undetectable triggers, enhancing the stealth and robustness of the inserted backdoor. This framework is adaptable to both unconditional and conditional diffusion models, with the latter's application to text-guided image editing being a pioneering contribution. Conclusively, this research not only establishes the profound security vulnerabilities these models may possess but also hints at the prospective exploration of effective defense mechanisms. Based on the insights provided, my recommendation is to accept the paper with minor modifications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022\tInnovation & Novelty: The paper opens doors to a fresh domain by presenting an innovative approach towards generating invisible triggers, setting it apart from conventional methods. The introduction of backdoor capabilities for text-guided image editing in conditional diffusion models further signifies its novelty.\n\u2022\tExperimentation & Analysis: The authors have conducted extensive experiments on various samplers and datasets, solidifying the efficacy of the proposed model. Such detailed empirical evidence enhances the paper's credibility.\n\u2022\tClarity & Supplementary Materials: The research is presented with clarity, making it comprehensible. The inclusion of supplementary materials augments the paper's integrity."
            },
            "weaknesses": {
                "value": "\u2022\tLack of Training Details: The paper omits crucial details related to the overall training time and specifics about hyper-parameter tuning. This information is vital for replication and a deeper understanding of the proposed framework.\n\u2022\tRisk Mitigation & Practical Implementation: While the paper acknowledges the security risks, it could benefit from a more comprehensive discussion on the practical implications of mitigating these risks and the challenges in real-world implementation."
            },
            "questions": {
                "value": "\u2022\tConvergence Concerns: The paper doesn't fully address the convergence issues, especially considering the two-level optimization problem. There is ambiguity on how to ensure convergence and the steps to be taken if inner and/or outer problem doesn't converge."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7326/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7326/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7326/Reviewer_RZ4Z"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608244776,
        "cdate": 1698608244776,
        "tmdate": 1699636875969,
        "mdate": 1699636875969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3hZ6vDKAzo",
        "forum": "scFfMOOGD8",
        "replyto": "scFfMOOGD8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_ZqQk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_ZqQk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel optimization framework to inject invisible backdoors into diffusion models. Diffusion models are used for high-quality image generation but can also pose security threats. Current backdoor attacks on diffusion models rely on visible patterns that are easily detected. To address this, the authors propose a new framework to learn invisible triggers, making the inserted backdoor more stealthy and robust.\nThis framework can be applied to both unconditional and conditional diffusion models, with the latter being the first to show how to backdoor diffusion models in a text-guided image inpainting pipeline. The authors conduct experiments to verify the effectiveness and stealthiness of the proposed framework. This paper highlights the security risks associated with diffusion models and provides insights into mitigating these vulnerabilities through the use of invisible triggers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper pioneers the study of backdoor attack problems for inpainting diffusion models."
            },
            "weaknesses": {
                "value": "1. The threat scenario presented seems impractical. Assuming that an attacker can control both the training process (using the bi-level training approach suggested in this work) and the usage of a diffusion model (specifically, choosing the noise used for inference) appears unrealistic in the context of diffusion models. If the attacker can dictate which noise the user employs for inference, the question of whether the trigger within the noise is visible or not becomes less significant.\n\n2. The experiments conducted are insufficient. For instance, an ablation study for the crucial hyperparameter - the norm bound of the trigger - is absent.\n\n3. I would like to encourage the authors to discuss the performance of the proposed attack when faced with defensive measures."
            },
            "questions": {
                "value": "1. What is the value of norm bound of trigger C actually used in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763051345,
        "cdate": 1698763051345,
        "tmdate": 1699636875857,
        "mdate": 1699636875857,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tIP8vLrstN",
        "forum": "scFfMOOGD8",
        "replyto": "scFfMOOGD8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_hgfj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_hgfj"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on developing a backdoor technique in diffusion models. the introduced technique is novel mainly because the designed trigger is invisible  (by human inspection). this work is the first to demonstrate how to backdoor diffusion models in text-guided image editing/inpainting pipelines. The results show successful attempt at implanting a backdoor and accessing it."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* this work is the first to demonstrate how to backdoor diffusion models in text-guided image editing/inpainting pipelines.\n* the proposed attach works on inpainting tasks"
            },
            "weaknesses": {
                "value": "the biggest weakness is the the backdoor is quite visible. When I zoom in on, say Fig 7 can I clearly see the images with trigger have been tempered with.\n\nanother weakness is the lack of comparison with other existing works"
            },
            "questions": {
                "value": "why is the backdoor invisible when its' clearly visible? is it a special terminology people use in this field of work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "this paper exposes a way to implant backdoor in diffusion models. but did not offer a solution. the authors say \"For future work, we will explore effective defense methods to mitigate possible backdoor in diffusion models.\". I'm flagging this just to be on the safe side"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808935890,
        "cdate": 1698808935890,
        "tmdate": 1699636875736,
        "mdate": 1699636875736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aKJybqhqzF",
        "forum": "scFfMOOGD8",
        "replyto": "scFfMOOGD8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_KSEM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7326/Reviewer_KSEM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a backdoor attack method on diffusion models. The proposed attack generates imperceptible triggers for both the unconditional (random noise input) and conditional (text input and masked image) cases. The trigger is essentially learned via a generator in a bi-level optimization approach, which alternates between learning the trigger and learning to inject the backdoor. The empirical results show that the proposed attack generates imperceptible triggers while maintaining the utility of the main task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n\n1. The proposed attack seems to be capable of both conditional and unconditional cases, exposing the backdoor risks on training the diffusion models.\n2. The experimental results show the effectiveness of the proposed methods, specifically the intended imperceptibility."
            },
            "weaknesses": {
                "value": "While the paper solves an important problem for understanding the security risks of using diffusion models, there are some major weaknesses:\n\n1. Achieving imperceptibility for the unconditional case is not well-motivated. For example, even when having a small residual, the random noise generated by this method and of previous works are still not recognizable. \n\n2. Another concern is that, while the adversary in backdooring classification can benefit from targeted, incorrect prediction (e.g., turn a decision of rejecting a credit application into an accepting one), it's unclear what is the benefit of backdooring diffusion model, since the adversary is the one who receives and uses the generated images for their own benefits. In order words, the paper lacks a discussion on why the adversary wants to attack themselves.\n\n3. The novelty of the technique is trivial. It is merely an application of previous bi-level optimization works developed for backdooring classification models (e.g.,  Doan et al. ICCV 2021, Doan et al. NeurIPS 2021), but replacing the classifier with a diffusion model. However, these works in the classification backdoor are not mentioned at al, which make the technical contributions of the paper very limited. \n\n4. Limited evaluation of defense techniques. In backdooring classification, there are several defenses that rely on input perturbation, and I think that their evaluation on the proposed method is essential for a comprehensive understanding of the attacks. \n\nDoan et al. Lira: Learnable, imperceptible and robust backdoor attacks. ICCV 2021.\nDoan et al. Backdoor attack with imperceptible input and latent modification. NeurIPS 2021."
            },
            "questions": {
                "value": "Please see the comments on weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844811867,
        "cdate": 1698844811867,
        "tmdate": 1699636875601,
        "mdate": 1699636875601,
        "license": "CC BY 4.0",
        "version": 2
    }
]