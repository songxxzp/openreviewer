[
    {
        "id": "7e4EqgpQDQ",
        "forum": "YUbvEYncfv",
        "replyto": "YUbvEYncfv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission568/Reviewer_b5rW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission568/Reviewer_b5rW"
        ],
        "content": {
            "summary": {
                "value": "In realistic FL, it is difficult to ensure that large-scale clients efficiently communicate with a central server. This work studies an essential scenario of Decentralized Self-Supervised Learning (DSSL) based on decentralized communications, in which only unlabeled data is used during the pre-training stage, and the communication between clients involves only model parameters. This paper proposes a method, Decentralized Navigator (DeNAV), utilizing a lightweight pre-training model, namely the One-Block Masked Autoencoder, with a training navigator to evaluate selection scores for the connected clients and plan the training route, eliminating the reliance on server aggregation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed scenario is important for FL. The massive clients may lead to in-efficient communication with central server.\n2. This paper is written clearly."
            },
            "weaknesses": {
                "value": "1. The motivation is not strong enough, due to the lack of literature review. The Gossip learning does not constrain that every client must communication with all neighbors. Some Gossip federated learning works also propose to let clients only communicate with one or several neighbors [1][2].\n2. In section 3.1, it seems that the described scnario looks like the continual learning. Specifically, the trained model is communicated and trained across clients. How to guarantee the convergence of this training scheme?\n3. The proposed methods seem to be limited in the context of transformer-based model architectures.\n4. The proposed selection score (6) seems to be little heuristic. How is this equation derived? Can such a selection score ensure convergence?\n5. Experiment settings are not clear. What is the non-IID degree used, i.e. alpha in dirichlet sampling? For CIFAR-100 and Mini-INAT show that IID accuracy is better than non-IID, which seems to be impossible.\n\n[1] MATCHA: Speeding Up Decentralized SGD via Matching Decomposition Sampling. In ICC 2019.\n[2] GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication. In TPDS 2022."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission568/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission568/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission568/Reviewer_b5rW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654761206,
        "cdate": 1698654761206,
        "tmdate": 1700455169502,
        "mdate": 1700455169502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4k16Er6F7z",
        "forum": "YUbvEYncfv",
        "replyto": "YUbvEYncfv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission568/Reviewer_rt1q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission568/Reviewer_rt1q"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a decentralized self-supervised learning approach based on pre-training an auto encoder that can then be extended and fine-tuned on downstream tasks. The pre-training includes a client selection approach with heuristically defined utility functions. Experiment results confirm the advantage of the proposed DeNAV algorithm compared to baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Self-supervised learning is an important and practical topic. This paper considers it in the decentralized/federated scenario, which is good."
            },
            "weaknesses": {
                "value": "- Decentralized learning has been widely studied in the literature. The training of auto encoders as in this paper is simply a specific type of decentralized learning, where common decentralized SGD algorithms can be applied. It is not quite clear what is new. \n- In the same way, client selection has been widely studied in the context of federated learning, where different client selection algorithms have been proposed with convergence analysis. This paper presents a heuristic client selection method. Its advantage over other existing methods that have more theoretical rigor is not clear. \n- There is no theoretical analysis of the overall DeNAV algorithm proposed in this paper. Proposition 1 is too informal as a mathematical claim, since any model can be approximated as a linear model if one allows an arbitrarily high approximation error. It is the bound of the approximation error that is more interesting, but such a bound has not been derived. Theorem 1 seems to be simply a least-squares regression result, which is straightforward. In general, it is not quite clear what is the usefulness of the theory presented in Section 3.2, since it is based on possibly inaccurate linear approximation and the main result is straightforward. It does not show the convergence of the overall algorithm, particularly with the client selection mechanism in Section 5."
            },
            "questions": {
                "value": "Please clarify the concerns mentioned under weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699364525557,
        "cdate": 1699364525557,
        "tmdate": 1699635984073,
        "mdate": 1699635984073,
        "license": "CC BY 4.0",
        "version": 2
    }
]