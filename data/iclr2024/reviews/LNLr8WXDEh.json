[
    {
        "id": "mJgDMZ3oKS",
        "forum": "LNLr8WXDEh",
        "replyto": "LNLr8WXDEh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission148/Reviewer_n8LA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission148/Reviewer_n8LA"
        ],
        "content": {
            "summary": {
                "value": "This paper shows diffusion models have better scene understanding capabilities than DINO and CLIP. This is demonstrated by training a linear classifier on the output of specific layers of these pre-trained models to learn to classify whether the relationships between different regions in a 3D scene can be distinguished."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is easy to read, and generally well written.\n- This paper examines the ability of a diffusion model to understand different physical properties of the 3D scene, including scene geometry, material, support relations, shadows, occlusion, and depth. \n- The paper also shows that stable diffusion models can be used as a feature extractor for a wider range of downstream tasks."
            },
            "weaknesses": {
                "value": "1) The paper does not compare the proposed method to other generative models, such as StyleGAN[1] and VQGAN[2], and other diffusion models such as DDPM[3] and GLIDE[4]. This would help to provide more insights on whether generative model generally gains good performance on 3D scene understanding or only diffusion models. This is a significant weakness, as it is difficult to assess the significance of the power of the diffusion model without knowing how it compares to other methods.\n\n2) Diffusion models have been used in different tasks for 3D data [5][6] and demonstrated promising results in different tasks. There are also other different important aspects of 3D scene understanding, eg. scene categories, attributes, and structures.[7] How does the author choose these 7 properties, are these 7 properties enough to \u201cunderstand\u201d the 3D scene, what problems this paper is trying to solve by experimenting on these properties, and the challenges that lie on them?\n\n3) This paper lacks of a more complete ablation study, which could provide more insight. For example, how does the number of time steps or layers affect the task? Is it sensitive to these parameters?\n\n4) The paper title is not really what the paper is about. This paper is more on the interpretability of a trained model, no matter it is part of the DINO or CLIP models, or stable diffusion models.\n\nSummary Of The Review:\n\nThis is an empirical paper and the paper could be further improved by including a more comprehensive experiments and ablation study, e.g. comparison to other state-of-the-art methods, in particular other generative models and other diffusion models. This would make the paper stronger and more informative for readers.\n\nReferences:\n[1] A Style-Based Generator Architecture for Generative Adversarial Networks\n[2] Taming Transformers for High-Resolution Image Synthesis\n[3] Denoising Diffusion Probabilistic Models\n[4] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\n[5] Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data\n[6] Diffusion-based Generation, Optimization, and Planning in 3D Scenes\n[7] Basic level scene understanding: from labels to structure and beyond"
            },
            "questions": {
                "value": "See my questions in 2) and 3) above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698300540080,
        "cdate": 1698300540080,
        "tmdate": 1699635939964,
        "mdate": 1699635939964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fZjGWSARyN",
        "forum": "LNLr8WXDEh",
        "replyto": "LNLr8WXDEh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission148/Reviewer_ssWi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission148/Reviewer_ssWi"
        ],
        "content": {
            "summary": {
                "value": "To better understand and measure the 3D scene modeling capability of the 2D stable diffusion, the author has designed an interesting evaluation protocol by probing for explicit features that represent physical properties. A linear SVM is leveraged to examine how well these these averaged patch-wise features from SD can be used to answer questions on physical properties. This work has done extensive experiments on different 3D properties including scene geometry, scene material, support relations, lighting, and view dependent measures, and throw insights on SD's superior modeling of a few physical attributes, but less performant for occlusion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[**originality**] \nThe author proposes to use the capability of serving as robust physical properties classifier as standards to evaluate the SD's feature on physical attributes modeling, which is novel and makes sense. The author further uses domain-specific datasets for each individual attribute, which disentangles different physical property in a unified framework. \n\n[**significance**] \nWhile there are several previous works touch the field of interpreting pre-trained generative model like SD, quantitative evaluation is still missing, and this work has partially addressed this issue. The proposed framework, including the designed the binary questions are fully automatic using existing labels from available datasets, this can save human labeling efforts, and make the pipeline applicable to further T2I model development. \n\n[**quality** & **clarity**] \nThe paper is written in a structured way, while complete experiments have been performed in various settings with a clear conclusion."
            },
            "weaknesses": {
                "value": "- I am a little skeptical of the key conclusion presented in Table 2, does that simply mean Layer 3 handles all physical properties while other layers fail? This might be true for early layers, but I am not quite sure about D4. A related concern is related to the feature averaging, maybe the conclusion is wrongly drawn since we didn't choose the suitable patch size, a bigger or smaller patch size might lead to a different conclusion. For designing such average feature, would the feature be more informative if we do average pooling to get 2x2 grid and then flatten the feature, in this way, the physical spatial relationship is also preserved. The author might want to have more ablations on designing the feature.\n- The author's experiments are limited to feature quality evaluation, but there are no explicit visualization of the latent feature, and there is no example how such evaluation can benefit and help us design the next-generation T2I model, or Text-to-3D models by improving over the feature space.\n- For the Question 6 design on occlusion, I feel it is not appropriate since there are two concepts: 1. belonging to same object; 2. whether there are occlusion. Before we evaluate occlusion, we need to first know how the classifier will work for parts belonging to same object but without occlusion."
            },
            "questions": {
                "value": "- These datasets with GT labels might serve as domain-specific experts, in order to fully capture the learnt image distribution of SD, how do we select these training/test images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682414804,
        "cdate": 1698682414804,
        "tmdate": 1699635939876,
        "mdate": 1699635939876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3r1Tnbjwqx",
        "forum": "LNLr8WXDEh",
        "replyto": "LNLr8WXDEh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission148/Reviewer_G3yb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission148/Reviewer_G3yb"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a study on the 3D awareness of stable diffusion in the setting of infilling uncertain areas. They focus on questions of perpendicular/same plane for geometry, surface material, support relation, lighting (shadows), occlusion and depth for viewpoint. They experiment across different datasets depending on the information provided within. To answer the binary question (yes/no) they train a classifier extracted from features within the U-Net of Stablediffusion, which they ablate on layer depending on each setting, where the authors find that a single generator layer is responsible for most decisions. They show a comparison against other methods DINO and CLIP and show that Stable Diffusion out performs in these reasoning questions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The benchmark setting is an interesting way of testing future image completion methods. The author's test suite, although relatively small, can be easily applied, although the computational cost and fairness of the ablation of layers could be an issue depending on the architectures.\n- Given the prominence of Stablde Diffusion, the analysis is an interesting way to quantify what it is aware of. There are currently very limited studies (as the authors highlight) exploring a number of different questions about spatial awareness of models."
            },
            "weaknesses": {
                "value": "- The paper oversells the approach at the beginning of the paper. It would be better to have the title/abstract/introduction define the constraint of in-filling, as this changes the problem significantly. In general, we see that in-filling is a significantly easier task than generating from scratch, which is more the common use case for Stablediffusion. Therefore, it would be better to constrain the arguments of the paper to be specific to this, as the benchmark would not generalise to unconstrained image generation. (This is why reduced Soundness and Presentation scores as the paper is misleading).\n- The use of ScanNet is problematic for plane tests as the segmentation is quite incomplete. Therefore, in a large case the part you are infilling you can see the rest of the wall/floor/surface giving strong context clues. These tests provide limited insights.\n- It would have been good to see other models evaluated in the same setting, even older approaches like GANs or VQ-VAE or StableDiffusion derivatives to provide contextual information. While DINO and CLIP provide insights, they are focused on a different task and probably explain the worse performance. A single focus to the paper has a limited setting for arguing a benchmark.\n- It is unclear why they only ablate across the SVM C parameter and not the Gamma as well. While in a large amount of cases, this has limited effect, it should be justified ideally empirically.\n- The authors argue a complete study in the beginning and then only evaluate DINO and Clip on Material and Support. The claim should either be consistent or the experiments completed.\n- Missing human performance in the table to judge the difficulty of the task as the results are very high, implying the questions are quite easy."
            },
            "questions": {
                "value": "- It would be useful to understand why a single model for evaluation can justify a benchmark setting for future evaluation. Or if the authors weren't thinking of it as much as a benchmark and more of an analysis of StableDiffusion alone. \n- Whether they tested human performance on the model? And how they came up with the groundtruth"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829895494,
        "cdate": 1698829895494,
        "tmdate": 1699635939781,
        "mdate": 1699635939781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DiUxCv4I6s",
        "forum": "LNLr8WXDEh",
        "replyto": "LNLr8WXDEh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission148/Reviewer_oLXV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission148/Reviewer_oLXV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for testing whether the features from a pre-trained network can be used to classify certain geometric and physical relations between a pair of masked regions in an image. These relations can be about 3D structure, e.g. whether two regions are on the same plane. They can also be about physical properties, e.g. whether two regions contain the same material. The classifier is a linear SVM trained on real images with annotations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper provides a new perspective into understanding the features generated by popular foundation models.\n- The writing is very clear and easy to follow."
            },
            "weaknesses": {
                "value": "- It is not clear to me that the proposed probing method is sufficient in proving that a model understands 3D relations. The biggest assumption made by the paper is that the result of the linear SVM is equal to the actual 3D relation in the image. This is simply not true. First, the linear SVM cannot get 100% on classifying the relations (this number is not mentioned anywhere in the paper). Second, even if the linear SVM gets 100% on the training dataset, it is not guaranteed to work on out of distribution images. Using the result of such a linear probe as a quantitative measure on how well the model understands 3D structure is very unreliable.\n- The paper seems to claim that stable diffusion can be used as a way to extract features from images and is better than alternatives like OpenCLIP and DINO. However, I\u2019m not sure whether it is a good idea of adding 8 different noises to the image and average the features. It can be very slow since diffusion is a sequential process and multiple diffusion passes are needed, while OpenCLIP and DINO just need a single forward pass on the original image.\n- The comparison between other foundation models and stable diffusion in Table 4 is not entirely fair, since stable diffusion uses grid search to find the best layer for the task, but other models use the last layer features. Although in Table 3 the authors mention that the difference between last layer and best layer is about 4% for two of the tasks. I don\u2019t think it is enough to conclude that grid search is not needed for all tasks.\n- The question designed to verify the relations are too simple. For example, in Fig 7 and 8, the generated shadow does not quite match the shape of the object, but the question only cares about whether the shadow belongs to the object, not about whether the shape matches."
            },
            "questions": {
                "value": "- Please provide accuracy of the linear SVM on the train/val set for each question.\n- In the qualitative examples, the inpainting mask is very tight around the desired region (e.g. the shadow of an object). What if a larger region is masked (e.g. the entire ground)? Can stable diffusion still generate shadow in the correct location and direction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission148/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission148/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission148/Reviewer_oLXV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission148/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699240476295,
        "cdate": 1699240476295,
        "tmdate": 1699635939668,
        "mdate": 1699635939668,
        "license": "CC BY 4.0",
        "version": 2
    }
]