[
    {
        "id": "tHSRqmh3VH",
        "forum": "bAMPOUF227",
        "replyto": "bAMPOUF227",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_ucxK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_ucxK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an SLM-LLM integration framework, which aims to inject supervised knowledge into LLMs with the aid of the task-specific language model. The motivation behind is that SLMs have more task knowledge thanks to the supervised training while LLMs capture more general knowledge with large-scale pretraining. The methodology is simple and straightforward, i.e., directly incorporating the predictive results from SLMs with the prompts. Experiments are conducted on a set of natural language understanding tasks and a QA task. Some performance improvements are observed under their OOD settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is a good motivation to enhance LLMs' task-specific ability with the aid of supervised models trained on the task.\n\n2. Some performance improvements are observed.\n\n3. The writing of the paper is overall good."
            },
            "weaknesses": {
                "value": "1. The novelty of the method is not significant. The technical contribution of the paper is insignificant.\n\n2. The tested tasks, i.e., NLU and QA, are too simple to illustrate the authors' statements. I would like to see more positive results on more challenging tasks, such as reasoning or code generation.\n\n3. I have some questions regarding the experiments. See the question part."
            },
            "questions": {
                "value": "Q1. The supervised model is trained on the in-domain corpus and then used to enhance the inference of LLMs. Have you tried fine-tuning the LLMs with the in-domain corpus? Although it has a much higher cost, I wonder whether there is a much larger performance improvement after SFT.\n\nQ2. I cannot capture the motivation of presenting Section 4.2. It seems that this section is not related to your core idea of the paper.\n\nQ3. Apart from appending the model prediction to the prompt, you also include the model confidence score to the prompt. How much does the framework benefit from this design? If removing the model confidence score, how does the performance change?\n\nQ4. How do you explain the extremely low scores in Table 3, i.e., 5.32, 6.08 and 3.72?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7605/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7605/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7605/Reviewer_ucxK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661288793,
        "cdate": 1698661288793,
        "tmdate": 1699636922521,
        "mdate": 1699636922521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nrbuXWg88G",
        "forum": "bAMPOUF227",
        "replyto": "bAMPOUF227",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_9mey"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_9mey"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an innovative in-context learning method through integrating the outputs from small discriminative models fine-tuned on supervised knowledge into LLM prompts. The outputs from small discriminative models are expected to include the importance prediction of different in-context examples for the current test case, and the corresponding explanation for such importance prediction. Experiments demonstrate that the proposed method effectively enhance the performance of LLM on four tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides a pioneering approach to systematically integrate supervisedly fine-tuned models into LLM inference.\n\n2. The proposed method significantly improves the performance of LLM, especially in managing OOD data and mitigating hallucinations.\\\n\n3. Compared with recent work Li et al. (2023b), the proposed approach is cheap and generalized."
            },
            "weaknesses": {
                "value": "1. Illustration of the proposed method in Figure 1 is not clear. It is unclear how the supervised knowledge participate in the whole procedure, while the capture states that it plays a core role.\n\n2. Equation (1) is misleading. On one hand, the left term contains \"\\{x_i,y_i\\}_{i!=j}\" as part of the condition for LLM inference, while \"i!=j\" is not a complete expression for the range of \"i\". On the other hand, the right term contains \"\\{x_i,y_i\\}_{i \\in S_j \\subset [1,...,N]\", i.e., the few-shot examples, as part of the condition, which may conflict with the statement in context: \"where i \\in [1,...,N]\". I can only assume the author intended to mean \"\\{x_i,y_i\\}_{i!=j, i \\in [1,...,N]}\" in the left term instead of the right term. However, the following context called this condition as \"the concatenation of the task description\", which leads to puzzle again.\n\n3. The explanation of proposed method in section 2.2 is not clear enough or may have grammar error:\na. \"Specifically, our receipt r_i\": receipt from what? in form of what?\nb. \"learning from which ... and which ... is important\": who learn what? how does it learn?\nc. \"as typical data points are OOD for the model\": for the discriminative model or for the LLM?\n\n4. Table 2 has two lines for the same baseline \"ELECTRA-large\" with different scores and no explanation for such difference. Specifically, the scores of ELECTRA-large in those two lines have small divergence except that it got 63.60 on MRPC in the first line but 37.59 in the second line. This raises serious questions about the reliability of the experimental results.\n\n5. The improvement of proposed method in managing OOD data is not obvious or even negative. For example, equipped with both ELECTRA-large and LLMs, the scores of proposed method only surpass ELECTRA-large within 1 point on most datasets in Table 2, and even become less than ELECTRA-large on some of the datasets. Nevertheless, there is no analysis for the casualty in evaluation, e.g., the variance of those scores."
            },
            "questions": {
                "value": "1. How is the discriminative model trained?\n\n2. The discriminative model provides the index of influential in-context examples, so do you rerank all the in-context examples according to this signal before/during concatenation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808721136,
        "cdate": 1698808721136,
        "tmdate": 1699636922377,
        "mdate": 1699636922377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j1hZGt6A9w",
        "forum": "bAMPOUF227",
        "replyto": "bAMPOUF227",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_yxT3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7605/Reviewer_yxT3"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework to enhance the generalizability and factuality of Large Language Models (LLMs) in natural language understanding and question answering. The framework uses task-specific finetuned Language Models (SLMs) to improve LLMs' in-context learning during inference. The approach is demonstrated to be effective in enhancing LLMs' performance, including models like Llama 2 and ChatGPT, across various tasks and datasets, while minimizing errors in generative tasks. The study emphasizes the benefits of incorporating discriminative models into LLMs for improved reliability. The authors provide a range of resources, including datasets, prompts, model checkpoints, and empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed approach serves as a plug-in method, resulting in enhanced versions of Llama 2 and ChatGPT that outperform their original counterparts in terms of generalizability and factuality. SuperContext can bring decent performance benefit compared to few-shot in-context learning and outperform original SLMs and LLMs with both zero-shot and few-shot settings."
            },
            "weaknesses": {
                "value": "1. I disagree with the statement in the introduction that says, \"However, since our goal is to allow reliable task adaptation rather than knowledge acquisition, the consulting agent becomes SLMs rather than search engines.\" It seems that the approach in this paper is quite similar to what Hugging Face's [1] GPT does, with the only difference being that it uses its own trained API instead of Hugging Face's SOTA small model API.\n\n2. The paper lacks significant novelty, essentially enhancing the output of small models with larger models. However, it extensively validates the effectiveness of this approach (even though Hugging Face's GPT has also done similar validations).\n\n3. The paper intentionally incorporates confidence when using the output of small models, but it lacks a detailed ablation study on the role of confidence. I am particularly interested in understanding the significance of confidence in this context.\n\n[1] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. NeurIPS 2023"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832668109,
        "cdate": 1698832668109,
        "tmdate": 1699636922238,
        "mdate": 1699636922238,
        "license": "CC BY 4.0",
        "version": 2
    }
]