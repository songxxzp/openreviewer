[
    {
        "id": "KbmhiWbChK",
        "forum": "FlhjUkC7vH",
        "replyto": "FlhjUkC7vH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_QDoi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_QDoi"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to disentangle identity-relevant and identity-irrelevant information from the reference set to achieve better identity-preserving performance in personalized text-to-image generation. By guiding the finetuning of the diffusion model denoising process with disentangled textual identity-preserved embedding and visual identity-irrelevant embedding, the personalization results can achieve high fidelity to the target object and flexibility to adapt to the background of any reference image. The disentanglement is constrained by proposed weak denoising and contrastive embedding objectives to ensure a non-trivial solution. Numerical and qualitative performances of the work demonstrate the superiority of the work."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Valid motivation and objective\n- Reasonable thoughts of ideas to address the identity-preserving problem in personalization with disentangling feature embeddings\n- Thorough experiments"
            },
            "weaknesses": {
                "value": "- Missing reference and comparison [1][2][3]\n- Lack of experimental results on real human figures. Since the focus of this work is identity-preserving, it is of great interest to me to see how it works on human objects. Ideally, the authors should provide a comparison figure like Figure 3 in [1].\n\n[1] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models. arXiv preprint arXiv:2302.12228 (2023).\n[2] Chen, Wenhu and Hu, Hexiang and Li, Yandong and Ruiz, Nataniel and Jia, Xuhui and Chang, Ming-Wei and Cohen, William W. Subject-driven Text-to-Image Generation via Apprenticeship Learning. NeurIPS 2023.\n[3] Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models. arXiv preprint arXiv:2304.02642, 2023."
            },
            "questions": {
                "value": "- Why ELITE is not included in quantitative comparison?\n- InstructPix2Pix uses instructions to guide image editing. I wonder how it is used for comparison in this paper. \n- A small problem with the structure. Since the finetuning objective consists of several separate losses and two of them are novel ones proposed for disentanglement, the ablation of the disentangled objectives is what readers are most curious about. I think the authors should consider moving Appendix A.1 to the paper section 5.3 if possible.\n- Typo: Figure 5 caption line 1. \"The identity-irrelevant images are generated using the text prompt Ps.\" should be \"The identity-relevant images\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Reviewer_QDoi",
                    "ICLR.cc/2024/Conference/Submission5364/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698395382170,
        "cdate": 1698395382170,
        "tmdate": 1700641946625,
        "mdate": 1700641946625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dqmQth2ArU",
        "forum": "FlhjUkC7vH",
        "replyto": "FlhjUkC7vH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_UqfN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_UqfN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the DisenBooth for disentangled and identity-preserving subject-driven text-to-image generation. It learns two distinct embeddings to capture the identity-relevant and identity-irrelevant information, respectively, with a weak denoising loss and contrastive embedding loss to facilitate disentanglement. The experimental results show good qualitative performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is simple but effective for disentangled finetuning.\n2. The results are good, and extensive ablation experiments are conducted.\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. To exclude the identity-irrelevant information such as background, one straightforward way is to filter them using a subject mask, which can be easily obtained by a segmentation model (e.g., SAM). Recent studies like Break-a-scene[1] have also explored masked diffusion loss for disentangling objects and backgrounds. This paper should also compare with these methods.\n2. The proposed method might rely on multiple images to learn the identity-relevant and irrelevant embeddings. When there is only one single image for training, it may be challenging to disentangle them. Although the paper includes experiments on a single image in Section A.3, there is a lack of visual results to verify the effectiveness (like Figure 5), and the number of testing samples is also limited (only 3).\n3. More recent methods should be compared, including Break-a-scene[1], Custom Diffusion[2], and SVDiff[3]. \n\n\n[1] Avrahami, Omri, et al. \"Break-A-Scene: Extracting Multiple Concepts from a Single Image.\" In SIGGRAPH Asia 2023.  \n[2] Kumari, Nupur, et al. \"Multi-concept customization of text-to-image diffusion.\" In CVPR 2023.  \n[3] Han, Ligong, et al. \"Svdiff: Compact parameter space for diffusion fine-tuning.\" In ICCV 2023."
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Reviewer_UqfN",
                    "ICLR.cc/2024/Conference/Submission5364/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761234266,
        "cdate": 1698761234266,
        "tmdate": 1700669113344,
        "mdate": 1700669113344,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5RN3IuyXAc",
        "forum": "FlhjUkC7vH",
        "replyto": "FlhjUkC7vH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_9xGd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_9xGd"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the problem of entanglement of the global and the local (object-specific) features while editing or personalizing a Diffusion model. The argument put forward is that the current state-of-the-art methods used for text-to-image editing tasks in the diffusion domain are not aware of the identity of the object. This means that while editing the images, the background information might dominate the editing process, ignoring the foreground object, or the identity of the object may be compromised. The paper argues that this is due to the entangled nature of the information used in the denoising process of these diffusion models. To address this, the paper introduces disentangled embedding derived from CLIP-based image and text encoders to make sure the global and object-centric features are extracted and used in a disentangled manner with the diffusion model. The proposed method, DisenBooth,  further designs the novel weak denoising and contrastive embedding auxiliary tuning objectives to achieve the disentanglement. Throughout the paper, identity-preserving edits are shown and compared quantitatively and qualitatively with the competing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper proposes a way to disentangle the identity of the object used for editing or personalization in diffusion models. This has an impact on personalized editing, where the user does not want the identity of the object to be compromised while the editing is performed. To achieve this, the paper employs identity-preserving embeddings and two losses to ensure that the foreground information is disentangled from the background information while denoising the images.\n\n2) The paper shows the results of their method in different scenarios. The supplementary shows results on the anime dataset. The quality of the results is impressive compared to the competing methods.\n\n3) The paper also conducts a thorough ablation study of the components used in the method. The study shows the contribution of loss terms and the mask affect the efficiency of the said method. The paper also shows editing scenarios where only one image is used for the personalization task. The results show that the proposed method is better than the competing method in this scenario as well."
            },
            "weaknesses": {
                "value": "1) The paper employs a LoRA optimization strategy for fine-tuning the diffusion model, but it doesn't address how this choice impacts the quality of their approach compared to fine-tuning the entire UNet. It raises questions about whether the proposed loss functions and encoding strategies would still be effective in such a scenario and what impact this might have on result fidelity.\n\n2) The paper primarily concentrates on editing and personalizing single, often centrally located objects. While this is a challenging task, it would be intriguing to see how the method performs when dealing with scenes containing multiple objects. It is particularly important to understand how the encoding and disentanglement processes behave in such complex scenarios.\n\n3) It remains uncertain whether the mask could be explicitly integrated into the images before the encoding steps. How this approach might affect performance and how does it compare to the current masking strategy employed in the adaptor?\n\n4) The method appears to primarily focus on objects placed at the center of an image. What remains unclear is how it performs when objects are located in the corners of the image or occupy fewer pixels. It's essential to investigate potential failure cases in this context and assess whether the image and text encoders can accurately capture identity-preserving information in such scenarios."
            },
            "questions": {
                "value": "1) Regarding the choice of LoRA optimization over fine-tuning the entire UNet, how does this affect the quality of the proposed method compared to alternative fine-tuning strategies?\n\n2) While the paper focuses on editing single objects, have the authors explored how the method performs with multiple objects in a scene? What challenges and insights arise when dealing with such complex scenarios, especially in terms of encoding and disentanglement?\n\n3) Concerning the inclusion of masks, could you discuss the potential benefits and drawbacks of explicitly integrating masks into images before encoding? How does this compare to the current masking strategy in the adaptor, and under what circumstances might one approach be more advantageous?\n\n4) The method appears to emphasize objects at the center of an image. Have you tested its performance with objects located in the corners or covering fewer pixels? What are some challenges or failure cases in such scenarios, and how do the image and text encoders adapt to encode identity-preserving information effectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Reviewer_9xGd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788310569,
        "cdate": 1698788310569,
        "tmdate": 1699636541144,
        "mdate": 1699636541144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k7yMpPej5o",
        "forum": "FlhjUkC7vH",
        "replyto": "FlhjUkC7vH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_nfwg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5364/Reviewer_nfwg"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses a very important limitation in subject-driven T2I models - entangled representations of subject with background/irrelevant information. The key idea of the work is to learn e to learn a textual identity-preserved embedding and a visual identity-irrelevant embedding for each image containing the subject, through two novel disentangled auxiliary objectives. Experiments show superior results compared to the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very interesting and the experiment evaluation sufficiently demonstrates the utility of the proposed method. While I have few concerns in the presentation (see Weaknesses), the work could potentially have wide applicability across several applications in Gen AI space. Another attractive aspect of the paper is that the results look great with fine-tuning a small number of parameters compared to the baselines."
            },
            "weaknesses": {
                "value": "Despite multiple readings, I struggle to understand concretely the use of identity irrelevant branch in section 4.1 - which is a key proposal of the paper.  I suggest authors to expand and provide more details on this line \u201cHowever, since we only need the identity-irrelevant information in this embedding, we add an adapter followed by the CLIP image encoder to filter out the identity-relevant information.\u201d How exactly the filtering is achieved in equation (4)? Results in Figure 5 look great and show that the disentanglement is indeed working. I am happy to revise my scores after reading the rebuttal. Another question to the authors - in comparing the results with baselines such as Dreambooth, are the same number of subject images utilized? Did authors also explore fine-tuning the CLIP Text encoder? Curious to know if that would create more issues with overfitting? Overall I liked the work, results are solid - but the proposed method needs more clarity."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5364/Reviewer_nfwg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698881312260,
        "cdate": 1698881312260,
        "tmdate": 1700781458744,
        "mdate": 1700781458744,
        "license": "CC BY 4.0",
        "version": 2
    }
]