[
    {
        "id": "zzj92ttLIw",
        "forum": "e9PSTbhlSQ",
        "replyto": "e9PSTbhlSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_ET4e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_ET4e"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method of continual learning inspired by a human brain mechanism, namely, memory consolidation during sleeping. The proposed method was compared with the original VCL and other existing methods in the literature over Split MNIST, Split CIFAR-10, and Split Tiny-ImageNet datasets. The proposed method showed superior performance, especially in the class incremental learning scenario."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction and the relation works sections are well written and clearly describe the goal of the work.\n\nThe inspiration from a human brain mechanism is exciting, and the proposed learning within sleeping (LwS) showed remarkable performance improvements."
            },
            "weaknesses": {
                "value": "While the idea is interesting and meaningful, the proposed method seems to combine the existing methods in the literature rather than a new one. In learning while sleeping, the memory reactivation is similar to the replay-based methods, and the distribution generalization is similar to the regularization-based methods. \n\nFrom a neuroscience perspective to this reviewer\u2019s knowledge, learning during sleep is more related to the stimulus given before sleeping (memory reactivation); thus, it has nothing to do with a new stimulation (no sample from a new task is given). In this regard, this reviewer believes that the proposed LwS doesn\u2019t follow the neural mechanism. \n\nMany typos in the context and the equations make it challenging to read and understand the paper. Especially for the equations, please carefully check the super/subscripts."
            },
            "questions": {
                "value": "Check the Weaknesses above.\n\nFor the consistency, the notations in Eq. (1) should be corrected as follows: (left) $f_{\\theta}(g_{W}(x))$ -> $f_{\\theta, W}(x)$ or $g_{W}(h_{\\theta}(x))$; (right) $f_{\\theta}(g_{W}(x_{i}))$ -> $f_{\\theta, W}(x_{i})$ or $g_{W}(h_{\\theta}(x_{i}))$.\n\nSince the notations are unclear, it isn't very clear for Hypothesis 1, Theorem 1, and the Proof. How are the notations of $(\\theta_{t+1}^{*\u2019}, W_{t+1}^{*\u2019})$ and $(\\theta_{t+1}^{*}, W_{t+1}^{*})$ different? Especially the last sentence in Proof needs checking carefully, \u201c~, it follows that $H~~~ <= H~~~~$.\u201d \n\nIn Algorithm 1, $D_{0}$ is not defined.\n\nThe authors also need to provide the results of VCL and LwS on other datasets in task-IL and class-IL scenarios.\n\nIn Figure 2, the markers in the graphs and the legends do not match."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698036395269,
        "cdate": 1698036395269,
        "tmdate": 1699636205385,
        "mdate": 1699636205385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q0yzaKrKgB",
        "forum": "e9PSTbhlSQ",
        "replyto": "e9PSTbhlSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_LsLM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_LsLM"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the reason why variational continual learning does not work well for the class-incremental learning and then proposes two techniques to improve the performance including replay and knowledge distillation. The resulting method indeed shows enhanced performance on the class-incremental learning scenario from the results demonstrated in the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The mathematical formulation seems to be correct.\nThe effectiveness of replay and feature distillation are again demonstrated."
            },
            "weaknesses": {
                "value": "Motivation of this paper is not clear. The paper extends on variational continual learning to the class-incremental learning scenario but it is unclear what are the particular benefits of extending over VCL rather than other approaches. \n\nIt is nice that the paper writes the problem and solution in a clear mathematical form but what the conclusions and solutions can be easily explained with natural language. The mathematical proof seems unnecessary, which can be moved to supplemental. \n\nThe novelty of the proposed method is very limited. The resulting solution is replay (similar to all replay methods like GEM, DER, DER++, GSS, [1], [2], [3], etc.) and perform distillation (similar to LwF) on the feature extraction part. But the solution is widely recognized as useful and proved in many prior works. I cannot identify any other contributions. \n\nThe comparison is not convincing. Results are not compared to more recent works such as DER and [1][2][3] so it is not convincing that the result is significant at the current time. \n\n[1] Rishabh Tiwari et al. GCR: Gradient Coreset based Replay Buffer Selection for Continual Learning. CVPR 2022\n[2] Elahe Arani et al. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. ICLR 2022\n[3] Da-Wei Zhou et al. A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. ICLR 2023"
            },
            "questions": {
                "value": "No questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698554507965,
        "cdate": 1698554507965,
        "tmdate": 1699636205299,
        "mdate": 1699636205299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lVrA806vNF",
        "forum": "e9PSTbhlSQ",
        "replyto": "e9PSTbhlSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_KMAy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_KMAy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new Bayesian continual learning method inspired by human memory consolidation process during sleep. The idea is interesting and novel, but in implementation it seems the method still trying to find a balance between the old and new data/features on a unified model, which human brain may not necessarily working in this way to process new memory. Results indicate this method is effective in a range of benchmarking experiments but fails to outperform some existing algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of memory consolidation is novel and it is interesting to dig this further. A strong and convincing point of the justification in the paper is that model transformation by new data is a primary reason that cause the forgetting issue. \n\n2. Split the model into FE and FC seems a good idea to make this complicated problem into simpler and easier ones.\n\n3. The way of presentation is good, easy to follow."
            },
            "weaknesses": {
                "value": "1. It seems this work was somehow rushed, especially the results section. I found a number of obvious typos, mainly spelling error. It would be good to at least carefully proofread few times before submission.\n\n2. The idea and the inspiring source is novel, but unfortunately when it comes to implementation, the proposed method is still trying to find a balance between old and new data/features. I doubt this might be also the reason that makes its performance fairly good but not outstanding amongst existing work."
            },
            "questions": {
                "value": "1. Fig 1, top right, the yellow connected points before integration, shouldn't it named 'new knowledge' rather than 'old knowledge'?\n\n2. It seems a higher buffer number favours the proposed work, could the authors further justify this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Reviewer_KMAy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627668604,
        "cdate": 1698627668604,
        "tmdate": 1699636205206,
        "mdate": 1699636205206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MiYnJEOdQl",
        "forum": "e9PSTbhlSQ",
        "replyto": "e9PSTbhlSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_HYLs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2651/Reviewer_HYLs"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel brain-inspired Learning within Sleeping (LwS) approach to solve continuous learning problems, which simulates memory reactivation and distribution generalization mechanisms. It can be considered as a combination of a rehearsal technique with parameter distribution regularization. The experiments conducted show the performance of LwS on various CL benchmark scenarios, including both task incremental and class incremental learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper is clearly written and easily understood.\n\n(2) The proposed solution originates from a theoretical analysis of the problem of catastrophic forgetting in VCL models.\n\n(3) The experimental results demonstrate the performance of LwS compared to the state-of-the-art, especially in class-IL scenarios."
            },
            "weaknesses": {
                "value": "(1) Although somewhat original, the model presented seems to be a combination of known solutions.\n\n(2) The authors claim to \"conduct a comprehensive theoretical analysis of VCL in the class-IL scenario\". I find this a strong statement, so one would expect a serious in-depth analysis. Meanwhile, the reasoning presented in Section 3 seems only to formalize (perhaps sometimes in an unnecessarily complicated way) some natural observations.\n\n(3) Since LwS includes a data buffer, its fair experimental competitors are the other replay-based models. In this respect, the authors only present a comparison with iCaRL (2017) and A-GEM (2019)."
            },
            "questions": {
                "value": "(1) Regarding Tab. 1, what is the role of a buffer in the VCL? (Note that increasing a buffer does not increase accuracy).\n\n(2) I would suggest expanding the experimental setup to include newer rehearsal-based competitors.\n\n(3) Will you share the source code for performed experiments?\n\n(4) Minor comments:\n\np. 7, Eqs. (9) and (10): $h\\theta_{t-1} \\to h_{\\theta_{t-1}}$,\n\np. 8, l. 1 from bottom: evaluate $\\to$ evaluate,\n\np. 9, under Tab. 1: replya $\\to$ reply,\n\np. 9, l. 3-2 from bottom: that, our $\\to$ that our, indicates $\\to$ indicate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2651/Reviewer_HYLs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832974412,
        "cdate": 1698832974412,
        "tmdate": 1699636205098,
        "mdate": 1699636205098,
        "license": "CC BY 4.0",
        "version": 2
    }
]