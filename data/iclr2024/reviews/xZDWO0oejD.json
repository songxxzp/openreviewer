[
    {
        "id": "3kDI7QRqSI",
        "forum": "xZDWO0oejD",
        "replyto": "xZDWO0oejD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_WBEq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_WBEq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method to improve the quality of text generations on specific tasks by specifying explicitly to which portions of the prompt the model should pay close attention. To this end, the authors introduce 4 instruction tasks which have been annotated explicitly in order to place emphasis on a particular section of the prompt.\n\nThe authors run a \"model profiling\" phase on a training subset of each of these tasks. This phase aims at detecting which attention heads should see their outputs amplified in order for the LLM to follow more closely the operator's instructions. At inference time, the post-softmax attention scores of the corresponding heads are boosted by a constant factor \\alpha.\n\nThe results show strong gains against raw LLMs in zero-shot and few-shot setups, and interesting robustness properties related to prompting."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Originality: the proposed method for increasing attention weights for sections that have been manually emphasized is novel. The method is relatively lightweight, which makes it compelling to adapt very large models dynamically to new tasks.\n\n* Quality: the code provided with the paper is problematic, as it means that the proposed evaluations may not correlate with human judgement on these tasks. In addition, a more thorough comparison with off-the-shelf instruction-tuned models may be relevant. See Weaknesses section for more details.\n\n* Clarity: the paper is well-written and easy to follow. However, the definition of the main metrics could be updated to more closely match the code contents. See weaknesses section for more details.\n\n* Significance: getting LLMs to adapt to new tasks at a reasonable cost is an exciting endeavour. However, human annotations are still required to emphasize specific chunks of texts. This human-in-the-loop requirement is comparable to that of instruction tuning, but it is not clear whether the results are comparable between the two methods."
            },
            "weaknesses": {
                "value": "# Problems in evaluation code\n\nAs can be seen in the code (https://anonymous.4open.science/r/PASTA-10E9/pasta/evaluator.py), the evaluation metrics for JSON Format and pronoun changing tasks do not seem aligned with any intuitive definition:\n\n* the paper describes the metrics for the JSON format task as follows: \"Format accuracy (F. Acc.) measures the accuracy at generating valid JSON. (b) Prediction accuracy (P. Acc.) measures the accuracy at generating the correct target in JSON values after loading the JSON-formatted generations.\". However, in practice:\n  * the evaluation for format accuracy measures whether there is any subsequence of the generated text that is valid json. It is highly debatable whether a model that would generate natural text intermingled with valid json is actually accurate at generating json.\n  * the evaluation for prediction accuracy checks whether the desired string is present anywhere in the json values. As such, an answer that would print out e.g. all of the words in the context into one large json would score well.\n* worse, the evaluation for pronoun change looks only at whether specific pronouns (\"she\", \"he\", \"her\"...) are no longer in the generated text, without verifying that the rest of the text is unchanged. As such, an empty generation \"\" would score 100. This explains partially Figure 3b, where the model's fluency seems to be decreasing steadily as more heads are steered, while the pronoun change accuracy keeps increasing.\n\nThese flaws in evaluation are troubling: a high score on these metrics may be paired with a substantial decrease in overall fluency and perceived quality of the model. This does not seem in line with the paper's stated contribution which is to \"enhance an LLM\u2019s ability to follow user instructions\".\n\n# Overlap with instruction-tuning\n\nThe main focus of the paper is to get models to generate text that follows specific instructions. While the authors cite existing publications on instruction-tuning, their claim that \"PASTA can be used in addition to these approaches to improve some aspects of model steerability\" would need to be substantiated. Indeed, one could assume that instruction fine-tuning will alter attention weights to have the model attend closely to the operator's instructions. The notion that PASTA improvements would transfer to such models would thus need to be tested. Furthermore, the cost of the model profiling phase should be compared to the cost of a lightweight instruction-tuning setup."
            },
            "questions": {
                "value": "* Could you clarify how we should interpret results on json generation and pronoun change given the shortcomings of the evaluation code?\n\n* Do the findings transfer on readily-available instruction-tuned models?\n\n* In effect, the prompt the paper is operating on has two components: the input text and the emphasis. In section 5.2, you demonstrate that PASTA mitigates noise and poor wording choices in the input test. For a full comparison with zero-shot prompting, could you test the effect of poor choice of emphasis in the input text?\n\n* In the same section, what is the reason to compare against zero-shot and not a few-shot setup? It would seem that few shot prompting would be a fairer comparison against multi-task PASTA, as both assume that they have access to some training examples on the task at hand."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Reviewer_WBEq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2076/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678590321,
        "cdate": 1698678590321,
        "tmdate": 1699636139636,
        "mdate": 1699636139636,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iHTJ8dfnIq",
        "forum": "xZDWO0oejD",
        "replyto": "xZDWO0oejD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_pBs7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_pBs7"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a post-hoc method, called PASTA, that allows users to steer the model's attention towards user-specified information by highlighting relevant parts of the input. The idea is to select a small subset of attention heads and applies precise attention reweighting on them; that is, we select some attention heads and steer the corresponding attention weight to focuses more on the specified input spans. This proposed method is evaluated on four tasks with GPT-J-6B and LLaMA-7B. Experimental results show that PASTA achieves an average accuracy improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is easy to read and the method is not difficult to implement. The idea of steering attention to enhance large language models' adherence to instructions proffers significant value. This paper also introduces a PASTA method, which can select a small subset of attention heads and applies precise attention reweighting on them during the inference process."
            },
            "weaknesses": {
                "value": "- This proposed method necessitates the predefined highlighted input spans. However, in the context of numerous tasks, defining such input spans to be a formidable challenge. One idea is whether an automatic extraction mechanism could be designed to improve this method. This could entail the development of a dedicated extraction model, or the crafting of specialized prompts to guide Large Language Models in executing this task more effectively. \n- The PASTA method does not necessitate alterations in model parameters, resulting in enhanced performance efficiency; however, a pertinent issue arises concerning the potential disruption of the established generative law from language modeling via manual attention steering. This makes a conjecture that such intervention could inadvertently impair other abilities of LLMs, such as generative fluency and reasoning.\n- The current experimental design appears to be limited in scope. For a more comprehensive evaluation, it would be beneficial to test the effectiveness of the proposed method on diverse large language models, including but not limited to, 7B-LoRA, 13B-LoRA, and 13B, across specific tasks.\n- There have been previous studies on improving the standard attention with the prior knowledge, such as syntax-based attention and multiscale attention. These should be considered baselines for comparison, either in related work or experiments. Additionally, although the four tasks previously considered do not adequately represent the breadth of generative tasks commonly utilized as benchmarks, they do not represent the prevailing benchmarks within generative tasks. It is crucial to evaluate the proposed methodology's efficacy by applying it to some commonly used generative tasks, including but not limited to, machine translation and question answering tasks."
            },
            "questions": {
                "value": "- Did you perform the SFT produce on the models in the experiment? \n- You should present more details in computing accuracy on these used datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Reviewer_pBs7",
                    "ICLR.cc/2024/Conference/Submission2076/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2076/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731379230,
        "cdate": 1698731379230,
        "tmdate": 1700632520285,
        "mdate": 1700632520285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ONRV7t7yXJ",
        "forum": "xZDWO0oejD",
        "replyto": "xZDWO0oejD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_s9kK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_s9kK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces PASTA (Post-hoc Attention STeering Approach), which addresses the need to guide the attention of large language models (LLMs) towards user-specified information. Unlike existing methods that are limited to plain text processing, PASTA enables LLMs to read text with user-specified emphasis marks, mimicking the subtleties of text style in human-written articles. PASTA achieves this by identifying a select group of attention heads and applying precise attention reweighting, directing the model's focus towards user-specified parts of the text. The method is applied at inference time and does not require any changes to the model parameters. Experimental results demonstrate that PASTA significantly improves an LLM's ability to follow user instructions and integrate new knowledge from user inputs. The performance improvement is substantial across various tasks, with an average accuracy boost of 22% observed for LLAMA-7B."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe authors propose a simple yet effective method called post-hoc attention steering. The method improves performance in multi-task and task-agnostic settings by forcing the model to focus on several important positions.\n\n2.\tThe structure of this paper is really easy to follow and understand.\n\n3.\tThe experiments are pretty comprehensive from my perspective, all the claims are properly proved."
            },
            "weaknesses": {
                "value": "1.\tExperiments on larger models should be conducted to ensure the performance across different models."
            },
            "questions": {
                "value": "1.\tOne thing I am really curious about is whether we have any preferences layer-wise. Since different layers are known to possess different functions. Which layer is more important for attention steering? By Figure 2 it seems that it\u2019s approximately random, is there any explanation for this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Reviewer_s9kK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2076/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786526457,
        "cdate": 1698786526457,
        "tmdate": 1699636139482,
        "mdate": 1699636139482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OSg7YqDsx0",
        "forum": "xZDWO0oejD",
        "replyto": "xZDWO0oejD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_ppCr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2076/Reviewer_ppCr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel attention steering approach, PASTA, to make the model focus on a subset of the prompt/instruction specified by the user at inference time for large language models. PASTA re-weights the attention scores by reducing the scores of non-highlighted tokens by a factor of $\\alpha \\sim 0.01$. After normalization, the attention scores of the highlighted tokens are increased. The modified attention matrix is used for computing the projections of the tokens, which are further passed to the feed-forward layers and eventually the next transformer layer. The paper also proposes a multi-task model profiling algorithm to select a subset of the heads in each layer of the model. Empirically, this leads to performance gains over simple attention head-selection heuristics or using all the attention heads. The paper conducts experiments on four synthetic tasks - JSON Formatting, Pronouns changing, Counterfact, and BiasBios using the LLama-7B and GPT-J models. The proposed approach achieves significant improvements over zero-shot, *-marked, \"\"-marked, and few-shot prompting methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and fairly easy to follow.\n- The proposed approach is novel and intuitive as increasing the attention scores of highlighted tokens will lead to more contribution in the output projection of tokens during generation.\n- The multi-task model profiling to select a certain subset of attention head improving performance on evaluation tasks leads to better generalization even on unseen tasks.\n- The empirical results on the four tasks show significant improvements over the standard zero-shot and few-shot prompting baselines."
            },
            "weaknesses": {
                "value": "- The paper mentions that the proposed approach, PASTA, does not need access to model weights, but I believe that is not correct? Once the attention scores have been modified, you would need access to $W_{v_{h}}$ to compute $H^{(l, h)}$ in equation 1. Furthermore, you'll need the weight matrices of the two feedforward layers $W_{ffn1}$ and $W_{ffn2}$ (assuming non-gated activation) to compute the output of a given transformer block layer, which can be passed to the next layer. Please correct me if I am wrong, but I think that the next layer's output cannot be computed without access to these weights.\n- The results are on those tasks only where the highlighted instruction is at the end of the prompt (except for the rephrased JSON Formatting task). What is the general effect of having highlighted instruction at the start or some other place in the prompt? This would give an idea if the proposed approach is general enough.\n- Not a weakness, but there's a typo in footnote 3: access $\\textbf{to}$ model weights ..."
            },
            "questions": {
                "value": "I have asked most of my questions in the weakness section, but here are some followup questions that I am interested in:\n\n- Beam search sampling and top-k sampling are much better compared to greedy. The baseline performance would definitely improve using these generation methods. How does PASTA compare to the baselines using these sampling approaches?\n- *-marked and \"\" - marked have very low performance. Have the authors tried better markers like <mark> </mark> or <emphasize> </emphasize>. I believe these markers are much more frequent in the training set, and might improve performance.\n\nAlso, there are a couple of formatting issues:\n- Appendix section A is empty in the submission. Please remove that in the updated version.\n- Citations and links are coloured. They should be black for ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2076/Reviewer_ppCr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2076/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816245935,
        "cdate": 1698816245935,
        "tmdate": 1699636139398,
        "mdate": 1699636139398,
        "license": "CC BY 4.0",
        "version": 2
    }
]