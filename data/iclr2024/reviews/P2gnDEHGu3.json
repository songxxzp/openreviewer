[
    {
        "id": "NVanXt7vWe",
        "forum": "P2gnDEHGu3",
        "replyto": "P2gnDEHGu3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_dGcN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_dGcN"
        ],
        "content": {
            "summary": {
                "value": "In the context of LLM, this paper shows there exist four distinct and independent mechanisms that additively combine, constructively interfering on the correct attribute. This generic phenomena is termed as the additive motif: models compute correct answers through adding together multiple independent contributions; the contributions from each mechanism may be insufficient alone, but together they constructively interfere on the correct attribute when summed. In addition, this paper extends the method of direct logit attribution to attribute a head\u2019s output to individual source tokens."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well written and easy to follow.\n2. The experiment is sufficient."
            },
            "weaknesses": {
                "value": "This finding  seems to be not profound enough. It only demonstrates that LLMs perform better under the additive motif, but it appears insufficient to prove that the additive motif is the underlying factual recall behind LLMs."
            },
            "questions": {
                "value": "1. This finding may explain the fact that models trained on \u201cA is B\u201d fail to generalize to \u201cB is A\u201d.   Is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings? Does it bring any other insights or explanations for other phenomena that are difficult to explain in LLMs? For example, is there any possible to explain the CoT prompting such as \u201clet\u2019s think step by step\u201d by using your findings?\n\n2. Can this finding contribute to prompt engineering\uff1f\n\n3. Some tables are too wide and are out of page, e.g., table 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795855868,
        "cdate": 1698795855868,
        "tmdate": 1699637169659,
        "mdate": 1699637169659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I9gGUMQtQY",
        "forum": "P2gnDEHGu3",
        "replyto": "P2gnDEHGu3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a set of experiments on a small hand-crafted data\nset for identifying the mechanisms at play during \"factual recall\" in\nlarge language models. The paper defines four \"mechanisms\" based on\n(1) attention heads that focus (mostly) subject of a factual\npredicate, (2) attention heads that focus (mostly) relation, (3)\nattention heads that attend to both, and (4) MLP layer. The main claim\nis that these mechanisms additively determine the correct attribute."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The study tackles an important/interesting problem and the paper reports a substantial amount of experimentation."
            },
            "weaknesses": {
                "value": "Although I believe the idea is interesting, and there may be some\nvaluable finding in the paper, I have difficulties seeing a clear\ntake-home message based on the results presented, and probably also\ndue to the way they are presented. I have some concrete points of\ncriticism listed in the comments below (with approximate order of importance).\n\n- The main claim, additivity of the multiple mechanisms, is not very\n  clearly demonstrated in the paper. The separation of the\n  subject/relation heads (as displayed in Fig. 2) is impressive.\n  However, neither the roles of the \"mixed head\" mechanism, the MLP,\n  and additivity of all these mechanisms are not clearly demonstrated.\n\n- The dataset is rather small and it is not described in the paper at\n  all. The description of in the appendix is also rather terse,\n  containing only a few examples. Given the data set size (hence the\n  lack of diversity), and the possible biases (not discussed) during\n  the data set creation, it is unclear if the findings can generalize\n  or not. In fact, some of the clear results (e.g., the results in\n  Fig. 2) may be due to the simple/small/non-diverse examples.\n\n- I also have difficulty for fully understanding the insights the\n  present \"mechanisms\" would provide. To me, it seems we do not get\n  any further insights than the obvious expectation that the models\n  have to make their decisions based on different parts of the input\n  (and meaningful segments may provide independent contributions). I\n  may be missing something here, but it is likely that many other\n  readers would miss it, too.\n\n- Visualizations are quite useful for observing some of the results.\n  However, the discussion of findings based on a more quantitative\n  measure (e.g., DLA difference between factual and counterfactual\n  attributes) would be much more convincing, precise, repeatable, and\n  general.\n\n- Overall, the paper is somewhat difficult to follow, relying data in\n  the appendix for some of the main claims and discussion points.\n  Appendixes should not really be used for circumvent page-limits.\n  Ideally, most readers should not even need to look at them.\n\n- The head type (subject/relation) definition uses an arbitrary\n  threshold. Although it sounds like a rather conservative choice, it\n  would still be good to know how it was determined."
            },
            "questions": {
                "value": "Some typo/language issues:\n - Introduction second paragraph: \"(Meng et al., 2023a) find ...\"\n  -> \"Meng et al. (2023a) find ...\" \n- Although it is a very common \"mistake\" in the field, all \n  established style guides I know prescribe that footnote marks\n  to be placed after punctuation. Also, I strongly recommend\n  against placing footnote marks directly on symbols (like R^3).\n- It is a good idea to indicate that figure/table references to\n  the appendix are in the appendix.\n- The \"categories\" defined at the beginning of the results section\n  comes as a surprise, and seem to be an important part of the \n  analysis throughout. This should be defined/explained earlier.\n- End of sentence punctuation missing for footnote 4.\n- There are no references to Figure 2 from the text.\n- It may not be that easy for some figures, but B/W friendly\n  figures would be appreciated by people reading on paper or\n  monochrome devices (like e-ink readers).\n- Some terms like \"OV circuit\" or \"ROME\" that many readers are not \n  likely to be familiar with should be briefly introduced.\n- The same goes for abbreviations of the sort L22H17. Not \n  difficult to guess for most readers, but it would be more reader\n  friendly to explain at first use."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Reviewer_npyu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699051336204,
        "cdate": 1699051336204,
        "tmdate": 1700733765386,
        "mdate": 1700733765386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "24cZWqhaBO",
        "forum": "P2gnDEHGu3",
        "replyto": "P2gnDEHGu3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
        ],
        "content": {
            "summary": {
                "value": "This work target at interpreting the inner mechanisms of LLMs in accomplishing the task of Factual Recall. This work identifies and explains four distinct mechanisms present in the model, as well as the additive cooperation between these mechanisms. This work validates the generalizability of this mechanism across different models and facts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) Based on sufficient experimental results verification, the author has identified and explained the internal mechanisms of LLMs at the granularity level of attention heads and MLPs. More interestingly, it provides an explanation of the \u201creversal curse\u201d phenomenon discovered in recent works.\n(2) This work has thoroughly discussed the related work and proposed a range of possible directions for future works."
            },
            "weaknesses": {
                "value": "(1) There have been many works [1, 2] interpreting the model behavior of Factual Recall. It seems that the novelty is insufficient with only a deeper zooming into attention heads using similar interpretability methods. Additionally, the discovery of the additive motif is not surprising enough, as already explained in work [3] that \"Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream.\" \n(2) Is direct logit attribution (DLA) the same as the interpretability method of Path Patching [4] or Causal Mediation Analysis [5]? If so, it is necessary to explain how the counterfactual data is applied for causal intervention. If it is not, it is necessary to provide a detailed description of the algorithm flow of DLA.\n\n(3) This work extends \u201cDLA by source token group\u201d with a weighted sum of outputs corresponding to distinct attention source position. But how to obtain the \u201cweights\u201d? How to attribute multiple tokens simultaneously? These missing implementation details make it difficult to understand the method and reproduce the results.\n\n\n[1] Locating and Editing Factual Associations in GPT\n[2] Dissecting Recall of Factual Associations in Auto-Regressive Language Models\n[3] A Mathematical Framework for Transformer Circuits\n[4] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\n[5] Investigating Gender Bias in Language Models Using Causal Mediation Analysis"
            },
            "questions": {
                "value": "(1) It would be better to validate the faithfulness of the identified components (e.g., Subject Heads, Relation Heads) for Factual Recall? What would happen to the prediction ability (e.g., accuracy) of the model for Factual Recall task if these components were knocked out? \n\n(2) We wonder if it is possible to explain the behavior of MLPs explicitly, similar to explaining Attention Heads via attention patterns?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9284/Reviewer_ctMu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699373204470,
        "cdate": 1699373204470,
        "tmdate": 1700612227882,
        "mdate": 1700612227882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YInYz2Ns0Y",
        "forum": "P2gnDEHGu3",
        "replyto": "P2gnDEHGu3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_vZgL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9284/Reviewer_vZgL"
        ],
        "content": {
            "summary": {
                "value": "The work concerns the task of factual recall in LLMs i.e. in templated prompts, the LLM is tasked to predict the object attribute of the tuple (subject, relation, attribute). Authors propose that factual recall in the END position (correct logit ranking) occurs by the summation of contributions of different additive circuits in the transformer.\n- Authors extend Direct Logit Attribution (DLA) to compute the joint contribution from different source token groups to the final predicted logits\n- 4 different additive circuits are identified based on the extended DLA: SUBJECT, RELATION, MIXED and MLP\n- SUBJECT attention heads preferentially boost attributes that are relevant to the subject of the query\n- RELATION attention heads preferentially boost attributes that are relevant to the relation of the query independent of the subject\n- MIXED attention heads boost the attributes that are jointly relevant to the subject and relation of the query\n- MLP layers at the end position uniformly boost the attributes relevant to the relation (ignoring the subject tokens)\n\nThe central findings of the paper revolve around the Pythia-2.8b model. Additional experiments in the Appendix report that similar types of circuits may be found in other models but all categories may not always exist.\n\nLimitations:\n- Authors acknowledge that the boundary between MIXED and other attention head types is fuzzy. The definition used to separate attention heads was based on preferential contribution from SUBJECT or RELATION and any other type is considered a mixed type."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper uses established mechanistic interpretation tools and extends them to identify mechanisms in the transformer that perform very specific purposes\n    - The SUBJECT-head, RELATION-head, and MLP additive behaviors are established by showing consistent patterns across a range of fact queries"
            },
            "weaknesses": {
                "value": "- The paper introduction and further discussions claim that the results reported here provide a mechanistic explanation for the limitations of LLMs to learn \"B is A\" from training on \"A is B\" [1]. However, I do not see sufficient evidence to support this claim\n    - They have shown that in the forward direction the transformer selectively promotes attributes relevant to the subject and the relation\n    - This does not show that the transformer CANNOT/DOES NOT perform the same operations in the reverse direction.\n    - E.g. \"Basketball is played by ...\" may contain circuits that selectively promote the known basketball players. The lack of such circuits is not demonstrated by this work\n    - In particular, the authors argue that the LLM learns an \"asymmetric\" look-up. However, the asymmetry is not established.\n\nPresentation\n---\n- Significant space in the main paper is used to describe future work. I believe that there is an interesting and valuable discussion about dataset creation in the Appendix that should be brought to the main paper\n\n\n[1] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\", September 2023. URL http://arxiv.org/abs/2309.12288. arXiv:2309.12288 [cs]."
            },
            "questions": {
                "value": "1. Is it fair to say that the key findings are the presence of SUBJECT-only and RELATION-only heads among the attention heads in the transformer? All other heads are MIXED heads by default?\n2. What fraction of attention heads get categorized into extreme categories (SUBJECT and RELATION)?\n3. How does the contribution to the final logits from the extreme categories (SUBJECT and RELATION) compare to the heads that are categorized as MIXED?\n4. Tagging onto questions 3 and 4: is there a significant drop in model performance when extreme heads are suppressed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9284/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699494635071,
        "cdate": 1699494635071,
        "tmdate": 1699637169327,
        "mdate": 1699637169327,
        "license": "CC BY 4.0",
        "version": 2
    }
]