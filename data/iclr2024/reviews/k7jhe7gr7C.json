[
    {
        "id": "h5UYcFTVvP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_AoyU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_AoyU"
        ],
        "forum": "k7jhe7gr7C",
        "replyto": "k7jhe7gr7C",
        "content": {
            "summary": {
                "value": "The paper proposes removing redundant channels, by hashing. Critically, the method focuses on the post-training scenario and is data-free. The authors evaluate on ImageNet and CIFAR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper conveys the motivation, method, and experimental results decently well. The critical elements are present -- ImageNet results, some ablations, and a clear methods."
            },
            "weaknesses": {
                "value": "- The motivation is clearly written but there's a gap between that and the method. In the introduction, with the paragraph starting from \"However, the application of existing work is restricted...\" clearly motivates post-training methods that are data-free. That makes sense, and your method falls in that category. However, what's less clear is why hashing in particular improves over other methods in this category of data-free, post-training methods. Why is hashing the magic solution that defeats all other post-training methods?\n- For example, another way to leverage the linearity of the convolution (per first few sentences in sec 3.3) is to simply use kmeans. What does hashing buy us that kmeans could not? In fact, maybe kmeans is a reasonable baseline to include?\n- Another question that could be answered with ablations: Why square patches? Structured pruning support on hardware isn't usually in this patch form. A100s for example support 2:4 sparsity."
            },
            "questions": {
                "value": "- nit: Table 3's organization of the right 3 columns are a bit weird. Maybe have a column for FLOPS reduction and then separately have a \"type\" column (tuning, training, or data-free). Or, if you really want to use more space, have three columns with checkmarks. It's hard as-is to compare results/numbers. (Hrm, but this is already in Table 1. Why repeat this in Table 3?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3491/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697276879280,
        "cdate": 1697276879280,
        "tmdate": 1699636302471,
        "mdate": 1699636302471,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g3W6tN8tZy",
        "forum": "k7jhe7gr7C",
        "replyto": "k7jhe7gr7C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_JSr2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_JSr2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to study input clustering for convolutional neural networks. For a given convolutional layer, the method proposes to dynamically cluster spatial features in order to reduce the number of computations performed at inference. The resulting method, dubbed HASTE is data-free and parameter-free. The authors evaluate the method on ConvNets trained for Cifar10 and ImageNet."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The method is fairly well introduced and provides good results on existing convolutional neural networks such as ResNet or VGG. The study of hashlists is often overlooked in the compression literature as compared to pruning and uniform quantization when it can, in fact, provide significant benefits.\n\nIn my opinion, the most considerable strength of this piece of research is the interest on intermediate features compression."
            },
            "weaknesses": {
                "value": "I have multiple concerns regarding this work. First, in terms of novelty, the idea of merging the inputs based on redundancies obtained by hashing and only computing relevant operations is not new [1]. In particular, the differences between HASTE and [1] are marginal, and the empirical evidence suggests that [1] outperforms HASTE.\n\nSecond, the authors claim that the proposed HASTE method is parameter-free (first bullet point in the introduction) but then proceed to define two hyperparameters : L and s. Furthermore, these hyperparameters are dataset specific, and we do not have an automatic way to find them.\n\nThird, the benchmarks are lacking. While I understand that not all researchers have access to the same compute, this research is performed in a data-free context. This means that the authors should be able to benchmark on more recent and larger scale models. From my perspective, this is not sufficient for a venue such as ICLR.\n\nFinally, the authors claim that the method induces a limited overhead. It seems to me that this overhead is only benchmarked in terms of FLOPs. I understand that FLOPs as a metric has benefits (simple to benchmark and is hardware-agnostic) but it should be made explicit that we are not considering latency nor memory footprint, which limits the interest and impact of the method. On most hardware, having multiple sequential operations at a low level added to a network will significantly slow it down. For instance, it is well known that BN layers have very little overhead in terms of FLOPs while removing them can divide by 2 the latency on CPUs and GPUs.\n\n[1] Yvinec, Edouard, et al. \"Red++: Data-free pruning of deep neural networks via input splitting and output merging.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.3 (2022): 3664-3676."
            },
            "questions": {
                "value": "My questions are listed in the form of concerns above. In summary, I would appreciate it if the authors could: 1, detail the difference with [1], 2, remove the parameter-free claim 3, benchmark on an LLM or a diffusion model and 4 provide evidence or comments on the latency and memory footprint impact of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3491/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698867312684,
        "cdate": 1698867312684,
        "tmdate": 1699636302399,
        "mdate": 1699636302399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "08aSuXwSM2",
        "forum": "k7jhe7gr7C",
        "replyto": "k7jhe7gr7C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_fMDs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_fMDs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes HASTE for computation reduction at inference time for CNNs. The key idea is that the in a convolution layer, there might be similar channels forming 'clusters'. The paper uses locality sensitive hashing (LSH) to efficiently identify similar channels in a same LSH bucket, and compute the convolution only once for the averaged vector in each hash bucket. Experiments is conducted to show the trade-off between complexity and accuracy. HASTE is able to considerably reduce the FLOPs and maintain a reasonable model utility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Applying classic ML techniques to deep learning is a promising direction and hashing is one good example. There have been many attempts in recent years trying to use hashing to accelerate deep learning training and inference, and the paper continues this line of work. I appreciate the simple and intuitive idea of HASTE.\n\nThe writing is clear in general, and the paper is easy to follow. The empirical results are convincing; many model architectures are tested."
            },
            "weaknesses": {
                "value": "1. The LSH used for the HASTE module is a variant of SimHash, but it lacks rigorous justification/theory. Different from SimHash using dense Gaussian projections, the paper used very sparse random projection (VSRP) to reduce the complexity. However, SimHash has a strict mapping from the collision probability to the cosine similarity, but signed VSRP does not (asymptotic analysis under some data assumptions might be doable, but no formal result in the literature as far as I know). I suggest more details about the hashing methods should be included to help the readers better understand the similarity preserving property of LSH and why the method works. More references on LSH methods and applications can also be added to help the readers appreciate the approach.\n\nFor this purpose, I think Section 3.2 and 3.3 have something in common and can be combined and compressed a little bit. This could free up some space for more introduction/related context on the theoretical foundation of LSH/SimHash and VSRP.\n\n2. There might be other LSH methods used in HASTE, but they are not tested in the paper. I'm curious about the impact of $s$ (which corresponds to different types of projection) empirically. In the experiments, you just fixed $s$. An ablation study on $s$ could be useful and make the paper more self-contained. What's the result for $s=0$? What about using Gaussian projection instead of $\\pm 1$ when $s=0$? etc. \n\nAlso, for the cosine similarity, another possible approach might be the count-sketch (CS) type algorithm and some later developments:\n\n[Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams, Theoretical Computer Science, 2004]\n\n[Kilian Q. Weinberger et al. Feature hashing for large scale multitask learning, ICML 2009]\n\n[Ping Li and Xiaoyun Li. OPORP: One Permutation + One Random Projection, KDD 2023]\n\nA signed (1-bit) alternative of CS is SignOPORP in\n\n[Ping Li and Xiaoyun Li. Differential Privacy with Random Projections and Sign Random Projections, Arxiv 2023]\n\nThese algorithms might be even more efficient than VSRP. They split a data vector into $M$ bins and conduct projection within each bin, so they essentially only need one random projection in the most efficient case---If the dimensionality of the data, $d$, is sufficiently large, setting $M = L$ (the number of hash codes) suffices and the complexity is simply $d$. As a compariton, for sparse RP used in HASTE, this complexity is $dL(1-s)$. If $d$ is small, we can repeat CS for several times. For example, when $L=8$, we can set $M=4$ and repeat CS for 2 times. \n\nIn all, my point is that, there are other LSH methods that can be used in HASTE, but the paper simply used one of them without mentioning others. It will be better if the authors can more fully explore the possible options empirically. Or, at least, more related methods and approaches should be mentioned in the paper to give the readers a more complete picture and possible alternatives.\n\n**Summary:** The paper presents a good combination of deep networks and LSH, with simple and intuitive idea, and satisfactory empirical performance. The organization and presentation can be improved to include more background and discussion on the technical side (the properties, more related LSH methods, possible alternatives/improvements). I think the paper meets the bar of ICLR."
            },
            "questions": {
                "value": "Can you provide the distribution of the average similarity between the input vectors $x$ (flattened channels)? This could be an useful and interesting statistics to better motivate the proposed method, and also useful for algorithm design. Perhaps consider also adding it to the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3491/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904827735,
        "cdate": 1698904827735,
        "tmdate": 1699636302330,
        "mdate": 1699636302330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ooV8cp2SVh",
        "forum": "k7jhe7gr7C",
        "replyto": "k7jhe7gr7C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_988t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_988t"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces HASTE, a CNN pruning method that is both training and data-free. HASTE leverages locality-sensitive hashing to identify redundant channels, offering a substantial reduction in computational complexity. Experimental results conducted on various image CNN models highlight HASTE's ability to decrease FLOPs in models while maintaining accuracy with only minimal accuracy loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The concept of using hash collisions to assess the redundancy of feature channels is both logical and interesting. The paper also provides a compelling rationale for the need to develop a training and data-free CNN pruning method.\n\n2. The method is presented in a clear and comprehensive manner, with detailed explanations. The effectiveness of the approach is demonstrated through experiments conducted on multiple CNN models with results obtained from two different datasets."
            },
            "weaknesses": {
                "value": "The main drawback of this paper lies in the analysis of the experimental results.\n\n1. The paper would greatly benefit from visualizations of feature maps after the hashing process. These visualizations would help readers understand which channels are considered redundant in the context of image classification. In other words, by visualizing the feature maps, it would become apparent whether the pruned model genuinely focuses on channels crucial for semantic concept recognition.\n\n2. The paper's experimental focus is primarily on image classification tasks. However, since the core objective of model pruning is to identify redundant feature channels and use the saved channels for accurate recognition of image concepts, it would be valuable to assess whether HASTE performs effectively in other image-related tasks, such as object detection. I strongly recommend that the authors extend their testing to these areas."
            },
            "questions": {
                "value": "Additionally, there appears to be some missing information in Figure 3(b). It is unclear how the hyperparameter L impacts the method's performance. The values of L should be provided to clarify this aspect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3491/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3491/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3491/Reviewer_988t"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3491/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907911385,
        "cdate": 1698907911385,
        "tmdate": 1699636302168,
        "mdate": 1699636302168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z2Zj7Otle2",
        "forum": "k7jhe7gr7C",
        "replyto": "k7jhe7gr7C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_dkAQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3491/Reviewer_dkAQ"
        ],
        "content": {
            "summary": {
                "value": "The paper under review for ICLR 2024 presents a method for compressing convolutional neural networks (CNNs) without the need for additional computational resources for fine-tuning or retraining. The proposed HASTE module is capable of pruning pre-trained models without requiring access to the original training data, which could significantly enhance the accessibility and usability of existing models, especially on less powerful hardware.\n\nThe paper includes a detailed analysis of the method's performance on various datasets, such as CIFAR-10 and ImageNet, showcasing the trade-offs between the level of compression (measured in FLOPs reduction and compression ratio) and the resulting top-1 accuracy of the pruned models. The results are reported with mean and standard deviation values, calculated over multiple trials with different random seeds to ensure reproducibility.\n\nThe authors also discuss the ethical implications of their work, acknowledging the potential for both positive impacts, like reduced energy and carbon footprint, and negative applications, such as military use or mass surveillance. They express their intention to distance themselves from any harmful uses of their technology.\n\nTo further support reproducibility, the authors plan to release their code upon publication and provide a comprehensive overview of their experimental setup, model design choices, and the computational requirements of their method.\n\nThe paper seems to contribute to the field of efficient CNN architecture design, offering a solution that balances performance with computational efficiency, which is crucial for deploying deep learning models in resource-constrained environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality:\nThe HASTE module for CNN compression introduces a novel approach to model pruning that does not require retraining or fine-tuning, which is a significant deviation from traditional methods that often rely on these computationally intensive processes.\nThe technique's ability to operate without the original training data is particularly innovative, as it addresses a common limitation in scenarios where data may be proprietary or privacy-sensitive.\n\nQuality:\nThe empirical analysis provided is thorough, with the authors conducting multiple trials to ensure reproducibility. The inclusion of mean and standard deviation values over these trials adds to the robustness of the reported results.\nThe method's performance on benchmark datasets like CIFAR-10 and ImageNet is well-documented, providing a solid foundation for the claimed contributions.\n\nClarity:\nThe paper is well-structured, with a clear exposition of the methodology and results. The authors have made an effort to articulate the motivations behind their work and the potential applications of their proposed method.\nThe ethical considerations section, although brief, demonstrates an awareness of the broader implications of their work, which adds depth to the paper.\n\nSignificance:\nThe potential impact of the HASTE module is significant, particularly for deploying deep learning models in resource-constrained environments. This could democratize access to advanced AI capabilities, especially in areas with limited computational resources.\nThe authors' commitment to releasing their code upon publication is commendable and will likely facilitate further research and application of their work, enhancing the paper's significance within the community."
            },
            "weaknesses": {
                "value": "Clarity on Novelty and Differentiation:\nThe paper introduces a method for compressing CNNs, which is a well-explored area of research. To better understand the novelty of the proposed HASTE module, the authors should provide a clearer differentiation between their method and existing approaches. A more detailed comparison with state-of-the-art methods, possibly in a tabular form, could help highlight the unique contributions. References to prior works like [Howard et al., 2017] on MobileNets and [Zhang et al., 2018] on Shufflenet could be used to benchmark and contrast the proposed method.\n\nBroader Impact on Different Architectures:\nThe paper presents results primarily on standard architectures like ResNet and VGG. It would be beneficial to see the impact of the HASTE module on a wider range of architectures, including more recent ones like EfficientNet [Tan and Le, 2019] or Vision Transformers [Dosovitskiy et al., 2021]. This would help validate the generalizability of the method.\n\nQuantitative Analysis of Computational Savings:\nWhile the paper discusses FLOPs reduction, a more comprehensive analysis of the actual computational savings in real-world scenarios would be valuable. This includes memory footprint reduction, inference time measurements on hardware where such models are likely to be deployed, and energy efficiency evaluations.\n\nRobustness and Error Analysis:\nThe paper could benefit from a robustness analysis, showing how the pruned models perform under different conditions, such as adversarial attacks or out-of-distribution data. An error analysis could also be insightful, particularly if there are specific classes or types of data where the pruned models underperform.\n\nHyperparameter Sensitivity:\nThe method's performance appears to be sensitive to the choice of hyperparameters, such as the number of hyperplanes L. A more detailed exploration of this sensitivity, along with guidelines for hyperparameter selection in different scenarios, would be useful for practitioners.\n\nLimitations and Failure Modes:\nA discussion on the limitations and potential failure modes of the proposed method would provide a more balanced view. For instance, under what conditions does the HASTE module fail to compress effectively, and what are the trade-offs involved?\n\nReproducibility and Accessibility:\nWhile the authors have stated their intention to release their code upon publication, ensuring that the code is well-documented and easy to use will be crucial for reproducibility. Additionally, providing pre-trained models and a user-friendly interface could facilitate broader adoption and experimentation by the community."
            },
            "questions": {
                "value": "Could you elaborate on the specific aspects of the HASTE module that distinguish it from prior work in CNN compression? Are there particular mechanisms or theoretical underpinnings that are unique to your approach?\nHave you tested the HASTE module on a broader range of neural network architectures, particularly more recent ones? If not, could you hypothesize on its performance or provide a rationale for the selection of the tested architectures?\nCan you provide a more detailed analysis of the computational savings in terms of memory, power, and inference time, particularly on edge devices where such compression techniques are most valuable?\nWould you be able to include a robustness analysis against adversarial examples or performance on out-of-distribution data? An error analysis on the types of errors introduced post-compression would also be insightful.\nCould you discuss any identified limitations or failure modes of the HASTE module? Under what conditions does it fail to deliver the expected compression or performance?\nHave you considered the impact of dataset bias on the compression method, and how might this affect the generalizability of the pruned models?\nHow does the compression affect performance on different tasks beyond image classification, such as object detection or segmentation?\nCan you provide a more comprehensive ethical analysis of the potential negative uses of your technology and propose strategies to mitigate these risks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "\"Our work aims to enable compression of existing convolutional neural networks without having to spend additional computational resources on fine-tuning or retraining. Furthermore, our proposed HASTE module allows for pruning of pre-trained models without access to the training data. This has the potential to increase accessibility and usability of existing models by compressing them for usage on less powerful hardware, even when the training dataset is not publicly available. Furthermore, our method facilitates the employment of computationally cheaper models, reducing energy and carbon footprint induced by the network\u2019s inference. However, for the same reasons, our method could also be used in undesired applications such as for military purposes or mass surveillance. We distance ourselves from any application with negative societal impact.\"\n\nThis statement acknowledges the dual-use nature of the technology, highlighting its potential benefits in terms of accessibility, cost, and environmental impact, while also recognizing the possibility of misuse in contexts that could have a negative societal impact. The authors explicitly distance themselves from such applications.\n\nPrivacy, Security, and Safety: The compression technique could enable the deployment of advanced neural networks in resource-constrained environments, which may include devices with potential privacy implications, such as smartphones and surveillance cameras. There is a risk that such technology could be used to enhance surveillance capabilities in a manner that infringes on individual privacy rights.\n\nPotentially Harmful Insights, Methodologies, and Applications: The authors acknowledge the possibility of their method being used for military purposes or mass surveillance. This dual-use nature of the technology necessitates a thorough review to ensure that guidelines are in place to prevent misuse and to promote applications that have a positive societal impact.\n\nAn ethics review by experts in AI ethics and dual-use technology is recommended to address these concerns.\nThe ethical considerations for the paper under review are addressed in the following statement from the document:"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3491/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698931755594,
        "cdate": 1698931755594,
        "tmdate": 1699636302093,
        "mdate": 1699636302093,
        "license": "CC BY 4.0",
        "version": 2
    }
]