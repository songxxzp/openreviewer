[
    {
        "id": "yVbsNJyEHN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
        ],
        "forum": "ORUiqcLpV6",
        "replyto": "ORUiqcLpV6",
        "content": {
            "summary": {
                "value": "The paper proposes an interpretable 3D visual grounding framework based on Chain-of-Thought in large language models, i.e., CoT3DReF.\nIt is a sequence-to-sequence (Seq2Seq) model by first predicting a chain of anchors and then the final target.\nTo provide supervision on the intermediate chaining, the paper devises a pseudo-label generator.\nThe proposed framework achieves state-of-the-art performance and shows its data efficiency on several 3D visual grounding benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper integrate the idea of Chain-of-Thoughts into the 3D groudning tasks, and propose a 3D data-efficient and interpretable  framework CoT3DRef.\n2. The technical contribution is sound, including Seq-2-Seq formulation and pseudo labels generation.\n3. The proposed framework achieves state-of-the-art performance on 3D visual grounding tasks and shows its data efficiency."
            },
            "weaknesses": {
                "value": "In general, I am in favor of the paper, which makes an interesting first attempt toward CoT 3D grounding tasks. However, there are still some important while unexplored problems.\n\n1. To make CoT3DRef self-contained and scalable, it is important to generate accurate pesudo labels. Although it has been discussed in Sec.5, I have some remaining questions/concerns:\n- In Alg.1, FIND function maps a localized object, a relation and a set of candidate objects to an output localized object. What is the detailed process of FIND? Is the relations between objects known?\n- Alg.1 receive the input of object proposals P. Is it the output from the 3D object detection model which requires extra training? If so, the anchor localization may be bottlenecked by the object proposals.\n- Considering the elaborate process of psuedo label generation, the propose frameworks seems not \"data efficient\" in data preparation, though it is data efficient in 3D grounding training.\n\n2. The interpretability is limited. From the input end, GPT-3.5 is used for extract the logical order of objects given an input utterance, thus adding interpretability. But the logical relations of the objects is not explicit modeled. Instead, it is implicit learned in the Seq-2-Seq network."
            },
            "questions": {
                "value": "See \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_kjGR"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697355288701,
        "cdate": 1697355288701,
        "tmdate": 1699636736659,
        "mdate": 1699636736659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4CBCiy2uq6",
        "forum": "ORUiqcLpV6",
        "replyto": "ORUiqcLpV6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to model the referring task into a seq2seq task. The authors introduce two new modules: \u201ctarget anchors head\u201d and \u201cpathway head\u201d.  Basically, the main idea behind is to force the model not only to predict the target object but also pay attention to the rest object mentioned in the utterance. The \u201cpathway head\u201d forces the model to learn the logical order information that indicates how the mentioned target object is identified. However this module requires extra annotation, which they obtain from prompting the Language Language Models (LLMs). Benefit from extra clues, they improve the grounding performance on Various grounding datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n1. This paper introduces not only just predict the target object, but also to force the model to predict all mentioned objects (anchors) to best leverage all available clues. Which provides more information for models to use. \n2. The proposed pathway generation method leverages the ability of powerful LLMs (GPT), freeing the burden of human annotators and making the system easy to scale up.\n3. The proposed pipeline not only improve the performance compare with previous SOTA, but also has higher data efficiently when training data is limited. This is valuable since 3D visual language data is not always easy to obtain."
            },
            "weaknesses": {
                "value": "Weaknesses:\n1. The proposed idea which pays attention to not just the target object, has been proposed before in \u201cPhraseRefer\u201d. They use human annotators to obtain fine grained annotation. Which share the same underlying idea with this paper. \nTo me one of the major novelties of this paper is that they explicitly model the logical order while the \u201cPhaseRefer '' uses implicit manner. I think these two works are highly related, but the author didn\u2019t give enough introduction of this \u201cPhraseRefer\u201d, which makes it a bit difficult to understand the performance comparison e.g table 4. SAT with PhraseRefer/ScanEnts and yours. Besides, I believe some missing number in this table.4 e.g. MVT with PhaseRefer.\nThe authors should give more clear comparison with this line of works that use fine grain annotation.\n\n2. Do all numbers in Table 4 use a GPT-generated path? If not, answer my below question:\nIf you have access to the GT anchor label either from \u201cPhaseRefer\u201d or \u201cScanEnts\u201d, but you still require GPT to give you extra information \u201cthe Path\u201d, how is the performance going in this case?\n\n\n3. In Sec.3.3 Anchors parser \u201cThen, we match the objects to their closest matching class from the ScanNet labels\u201d, Would you explain this in more detail? Correct me if I am wrong: I assume if you assign a more fine grain label set , like ScanNet200, you are actually providing richer semantic knowledge to the model. My question is how do you assign the label, do you use the same level of semantic information with the others?\n\n4. From the numbers of MVT in Table.4 and Table.5, it seems that when MVT baseline has access to more data, the performance can improve quite a lot, So it is possible that your proposed method introduce explicit knowledge to the model while MVT can has a chain to learn that out from the data if the dataset is large enough?"
            },
            "questions": {
                "value": "I have listed my question above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_6KUg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698470210917,
        "cdate": 1698470210917,
        "tmdate": 1699636736526,
        "mdate": 1699636736526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UBGBjrYrJx",
        "forum": "ORUiqcLpV6",
        "replyto": "ORUiqcLpV6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
        ],
        "content": {
            "summary": {
                "value": "This paper presents CoT3DRef, a novel and interpretable 3D visual grounding framework that aims to mimic the human perception system. The authors formulate the 3D visual grounding problem as a sequence-to-sequence task, predicting a chain of anchors leading up to the final target, which is a significant deviation from existing methods that directly localize the referred object. This approach not only improves the overall performance but also enhances interpretability, helping to identify and address failure cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Technical Novelty.** The chain-of-thoughts (CoT) approach is an innovative way of addressing the 3D visual grounding problem, providing a sequence of interpretable intermediate steps that lead to the final prediction. This approach is aligned with how humans might approach the task, thereby making the model\u2019s predictions more understandable and transparent.\n\n2. **Data Efficiency.** The proposed framework demonstrates significant data efficiency, especially highlighted by its performance on the Sr3D dataset where it matches state-of-the-art (SOTA) performance with just 10% of the training data. This is a crucial advantage, particularly in domains where acquiring labeled data is expensive and time-consuming.\n\n3. **Applicability.** The authors show that CoT3DRef can be easily integrated into various existing architectures, demonstrating its versatility and applicability. This is substantiated by the comprehensive experiments conducted across different benchmarks and architectures, consistently showing performance gains.\n\n4. **Comprehensive Experiments.** The paper includes extensive experiments and ablation studies, providing a thorough evaluation of the proposed method. The results on Nr3D, Sr3D, and ScanRefer benchmarks are impressive, showcasing the framework's effectiveness and robustness."
            },
            "weaknesses": {
                "value": "1. **Detailed Failure Case Analysis.** While the paper mentions the identification of failure cases as a benefit of the interpretability, it does not provide a detailed analysis or examples of these cases. Including such an analysis could provide valuable insights into the limitations of the model and areas for future improvement.\n\n2. **Broader Impact and Ethical Considerations.** The paper could discuss the broader impacts and potential ethical considerations of deploying such a system, especially in the highlighted application areas like autonomous driving and robotics."
            },
            "questions": {
                "value": "Can you provide a detailed analysis or examples of the identification of failure cases as a benefit of the interpretability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6539/Reviewer_ywux"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836409873,
        "cdate": 1698836409873,
        "tmdate": 1699636736411,
        "mdate": 1699636736411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s7RAp6gowM",
        "forum": "ORUiqcLpV6",
        "replyto": "ORUiqcLpV6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6539/Reviewer_hPTL"
        ],
        "content": {
            "summary": {
                "value": "This submission studies 3D object referring. A new architecture is proposed, which predicts several objects sequentially instead of a single object at last. To generate a dataset that makes this new task setting possbile, the authors exploit grammar parser, GPT in-context logical ordering and rule-based box matching. The new architecture boils down to a decoder that matches several different encoder. Experiments show that the method improves several baselines and notably work quite well under a data efficient setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ A new decoder head for sequential object grounding in a scene. This is a new paradigm that may inspired related fields.\n+ A data curation effort that makes sense to me. This makes the CoT training possible.\n+ Quantitative results showing positive margins on four baselines and notably good performance in a data-efficient setting."
            },
            "weaknesses": {
                "value": "I like the paper and don't see major weaknesses. Why I am not rating higher is the fact that the new thing comes from foundation models, which is in fact a system-level contribution instead of a principled contribution. But I feel it fine and timely to accept this in ICLR 2024."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698850698150,
        "cdate": 1698850698150,
        "tmdate": 1699636736301,
        "mdate": 1699636736301,
        "license": "CC BY 4.0",
        "version": 2
    }
]