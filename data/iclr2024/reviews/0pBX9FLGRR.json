[
    {
        "id": "uB5AXgCiSr",
        "forum": "0pBX9FLGRR",
        "replyto": "0pBX9FLGRR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission678/Reviewer_7ZTA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission678/Reviewer_7ZTA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes \u201cDecoupled Diffusion Models (DDM)\u201d, which is a new forward/backward SDE for diffusion and corresponding learning objective. The authors claimed the new SDE to have de-coupling characteristics, i.e. it \u201cdecoupled\u201d the data-to-zero and zero-to-noise. Authors claim that due to this de-coupled nature, it\u2019s easy to train and also \u201cnaturally perform sampling with arbitrary step size\u201d. The paper shows, in experiment section, unconditional and conditional evaluations of the DDM comparing with competing methods proving it to be competitive and sometimes superior."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The overall idea, the way it is presented, seems new. There is a conceptual contribution of a new diffusion SDE, which might be interesting for others. Also the concept of \u201cde-coupling\u201d the two processes might be useful for future works to analyze."
            },
            "weaknesses": {
                "value": "Despite its apparent novel outlook, the paper has a lot of flaws \u2014 in terms of presentation, conceptual clarity and technical novelty. From a bird\u2019s eye view, the idea seems novel and interesting (which I thought after reading the abstract). However, the flaws were visible after digging into into it a bit more. Also I had a little difficult time deciphering the paper due to its not-so-good writing quality and math notations (more on this later).\n\n**Conceptual clarity and Technical novelty:**\n\n- The core idea revolves around the new forward process in Eq. 6 of the paper, which looks different from that of traditional DDPM. But I don\u2019t necessarily see how it is any novel, apart from it being a special case of traditional DDPM. It is not clearly shown in the paper, but here\u2019s how: From your choice of $\\mathbf{h}_t = -x_0$ (in Appendix B.3 second para) and $\\int_0^td\\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, t \\cdot \\mathbf{I})$, one can easily show Eq.6 to be\n    \n    $$\n    x_t = x_0 + (-x_0)\\int_0^t dt + \\sqrt{t}\\cdot \\epsilon \\\\ = x_0 - x_0 \\cdot t + \\sqrt{t}\\cdot \\epsilon \\\\ = (1-t)\\cdot x_0 + \\sqrt{t}\\cdot\\epsilon\n    $$\n    \n    This is effectively just a standard DDPM with a different noising schedule, i.e. $(\\alpha_t = 1-t, \\beta_t = \\sqrt{t})$ in your notation. Different choices of $\\mathbf{h}_t$ (in table 4) would also yield different noising schedule of standard DDPM.\n    \n- Even if, for a moment, we consider DDM to be indeed different, I still don\u2019t see what exactly is de-coupled here. It seems the author\u2019s concept of \u201cde-coupling\u201d is based on the premise that prior works (DDPM essentially) have something *coupled*. No its not \u2014 the noise $\\hat{\\epsilon} = \\epsilon_{\\theta}(\\cdot)$ and image predictions $\\hat{x_0} = x_{\\theta}(\\cdot)$ are *exactly* related to each other given $x_t$. But in practice, only one of them is used in most cases, not both. I would assume that two quantities are called \u201ccoupled\u201d if they are independent and hence can\u2019t be trivially separated. So, from this perspective, I don\u2019t see any point of \u201cde-coupling\u201d them.\n- For argument\u2019s sake, even if you refute the above point, I still don\u2019t see what exactly is decoupled in Eq. 6 ? It is still a coupled process where image-to-zero and zero-to-noise is added together at every step. By this logic, standard DDPM ($x_t = \\alpha_t x_0 + \\beta_t \\epsilon$) is also de-coupled ? The only reason Eq. 6 seems to have some sort of de-coupling is because of its attenuation process being *additive instead of multiplicative*. But then again, I showed above that\u2019s it\u2019s effectively multiplicative.\n- Regarding the reverse process, the authors credited (4th paragraph in introduction) the \u201carbitrary step jump\u201d property to the *analytic nature* of the zero-to-image process (line 4 of algo 2). However, one must realize that this maybe analytic, but not error-free. The authors used quite a sloppy notation which made it hard to see it clearly. If you use hats (differentiaing from line 4 of algo 1) for predicted quantities, you see that $\\hat{\\mathbf{H}}_t = \\int_0^t \\mathbf{h}^{\\hat{\\phi}} dt = -\\int_0^t \\hat{x}_0 dt$ incurs error too. So I don\u2019t understand how this is a reasonable explanation. In fact, any DPM can perform arbitrary step sampling \u2014 how is this any new? Moreover, contrary to the standard DDPM, you now have two predicted quantity and hence two possible sources of error !\n\n**Experiments and Evaluations:**\n\n- I don\u2019t fully understand what is being claimed to be superior \u2014 training or sampling performance, or both ? I assume its training, since you used the naive Euler-Maruyama sampler for everything (page 7 last line), which by the way, is already a bit worrying since there are far better samplers now.\n- Table 1 results (prior works) are all with stochastic or determinitic (prob. flow ODE) sampling ? There aren\u2019t much clear experimental details for the experiments to be conclusive.\n- For table 1, can you please mention exactly which models have been used ? There is no citation on the prior works. E.g. DDPM/SDE model-size and FID numbers seem inconsistent from what I have personally worked with. Please see the EDM paper by Karras et al. for a comprehensive FID numbers.\n- Figure 3 equates MSE loss with \u201cimage quality\u201d ? Also, how did you measure the MSE error, as in where did you get the target GT ? It is an unconditional case where you cannot know before hand which sample it will generate, right ?\n\n**Writing quality:**\n\n- The overall quality of writing is just okay-ish, not too good.\n    - In abstract: \u201cwe mathematically derive .. 2) for derive the sampling ..\u201d\n    - In still don\u2019t understand what is an \u201canalytic transition function\u201d mentioned several times ?\n    - Para 4 in Introduction: \u201cIn Fig. 4, either ..\u201d \u2014 this seems to be a wrong reference.\n    - Why did section 4.1 end up in the beginning of the method section ? Doesn\u2019t make much sense \u2014 it\u2019s full of implementation details which usually go in experiments.\n- Related to math notations\n    - Just after eq. 1: \u201c$\\beta_t$ is designed to increase ..\u201d \u2014 there is another restriction of being in $[0, 1]$\n    - Eq. 9 should write $\\tilde{\\sigma_t}^2$ instead of $\\tilde{\\sigma}^2$\n    - I am still confused about $\\phi$ and $\\mathbf{c}$ notations with respect to $\\mathbf{h}_t$. The notations here can be optimized a bit.\n    - Why not write $\\mathbf{h}_t^{\\phi}$ instead of just $\\mathbf{h}_t$ to emphasize that it\u2019s a parametric function?\n    - I already mentioned the problem of not using $\\hat{\\ }$ for predicted quantities.\n    - Why not mention the arguments of functions, like $\\phi_{\\theta}(x_t, t)$ and $\\epsilon_{\\theta}(x_t, t)$ in Eq. 10 and elsewhere ? Also, Eq. 10 is missing the HPs $(\\lambda_1, \\lambda_2)$."
            },
            "questions": {
                "value": "Please see the weakness section for questions along with comments and suggestions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission678/Reviewer_7ZTA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698092693598,
        "cdate": 1698092693598,
        "tmdate": 1699635994937,
        "mdate": 1699635994937,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A3V6uxGRyU",
        "forum": "0pBX9FLGRR",
        "replyto": "0pBX9FLGRR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission678/Reviewer_UWnj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission678/Reviewer_UWnj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes decoupled diffusion models (DDMs) that aim to decouple the forward image-to-noise mapping into image-to-zero mapping and zero-to-noise mapping. Based on this idea, the training objective function and reverse sampling process are presented. The experiments show that DDM can produce visually high-quality images in less than 10 function evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The presented DDM seems to be a new diffusion process in DPM literature.\n* Both training and sampling processes are clearly presented in techniques, making the approach easy to follow.\n* Experiments on several image tasks are conducted.\n* The paper is well written."
            },
            "weaknesses": {
                "value": "* This paper aims to split the DPMs into an image-to-zero mapping and a zero-to-noise mapping. However, from Eq. 6 or Table 1, $\\mathbf{x}_0+\\int_0^t\\mathbf{h}_t{\\rm d}t$ is not zero, though the signal is degenerated. I suspect that the \"image-to-zero mapping\" may mean $\\mathbf{x}_0+\\int_0^1\\mathbf{h}_t{\\rm d}t=0$. However, in the DDPM or VP-SDE, the drift term is $\\mathbf{x}_0e^{-\\frac{1}{2}\\int_0^T\\beta(s){\\rm d}s}$ also approaches zero as long as  $T$ is large. In conclusion, the paper is somewhat overclaimed, and meanwhile, the novelty is limited to some extent.\n* On the reverse process, why approximate  $q(\\\\mathbf{x}\\_{t-\\\\Delta t}|\\\\mathbf{x}_t,\\\\mathbf{x}_0) $ by $q(\\\\mathbf{x}\\_{t-\\\\Delta t}|\\\\mathbf{x}_t).$ To my understanding, the approximation is reasonable only for small $\\Delta t$. However, in the sampling process, to decrease sampling steps, $\\Delta t$ should be not small. How about directly using the reverse SDE corresponding to Eq. 6 to develop the reverse sampling process?\n* In Eq. 9, $\\bf \\epsilon\\sim \\mathcal{N}(0,I)$ is a noise sampled from Gaussian distribtion. Why do the authors use a network $\\bf \\epsilon_{\\theta}$ to learn it?\n* $\\mathbf{H}_t$ has a analytical formulation. Why do the authors still use a network to predict its parameters rather than just use the analytical formulation?\n* It's OK that the first term in Eq. 9 is motivated by section 4.1. But it is not mathematically deduced.\n* How about the performance of DDM if we replace the first term in Eq. 9 as predicting $\\mathbf{x}_0$? To show the effectiveness of DDM, such an ablation should be included.\n* It is unclear why DDM can speed up the sampling process.\n* The compared methods are limited, and some SOTA approaches, e.g., \"Elucidating the Design Space of DBGM\", are missing.\n* The generated images by DDM seem to be too \"smooth\" and hence are not realistic. \n* Due to the abovementioned issues on both novelty, claims, and experiments, the reviewer suggests the authors improve the paper, and then resubmit it to a future conference."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675733703,
        "cdate": 1698675733703,
        "tmdate": 1699635994859,
        "mdate": 1699635994859,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fxGkX9slOZ",
        "forum": "0pBX9FLGRR",
        "replyto": "0pBX9FLGRR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission678/Reviewer_Jv4X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission678/Reviewer_Jv4X"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a decoupled diffusion model (DDM) by learning the image attenuation process and diffusion process jointly.\nThe image attenuation process can be constructed in multiple ways. A key observation is to solve a group of constraints.\nThis work derived an analytical framework for this join process, which in turn offers the reverse transition distribution for sampling.\nThe objective function could be split into two targets for predicting the attenuation process and noises using the analytical framework.\n\nTo back up the empirical benefits of the decoupling framework, this paper conducted unconditional and conditional image generation tasks.\nSpecifically, DDMs predict the attenuation process and noises together using a two-branch UNet decoder.\nExperiments validated that the decoupled framework accelerated the sampling process to around 10 NFEs, showing advantages over previous continuous-time diffusion models and latent diffusion models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Analytic framework for the proposed join process. This backs up the proposed method and provides the sampling algorithm for the reverse time process.\n- Detailed experiments. For the design choices in the image attenuation process and other hyperparameters, this work provides a variety of comparisons.\n- Empirical benefits. The decoupled design offers sampling accelerations compared with LSGM and CLD."
            },
            "weaknesses": {
                "value": "I feel this work is overall novel concerning the technical content. However, I'd challenge the motivation of this work. In my view, despite the technical contributions, the analytical framework is more like the post hoc fashion \"validation\" of a simple experimental observation of predicting the attenuation jointly. This paper constructed a formal introduction to this method using the narrative and derivation. However, this is not a fundamental understanding or theory on why acceleration can be achieved by decoupling. It seems to me that cannot be understood as minimizing the transport cost (finding more straight paths), either.\n\nThat being said, although this work derived the analytical framework, it is not motivated in a principled manner or clearly **explained** using theory or experiments."
            },
            "questions": {
                "value": "There are several technical questions for the manuscript.\n\n- Is the diffusion process a linear scheduler to a uniform variable? There is 't \u223c Uniform(0, 1)` in the algorithm box. It remains unclear to me if the linear scheduler is applied. The influence of the diffusion schedulers is also vague compared to the baseline models, as they may use different diffusion designs.\n- The joint prediction design actually enlarges the model sizes. As a result, this work compares with the latent diffusion models like LSGM. LSGM is, for sure, a strong baseline. But I cannot ensure this is a fair comparison. There are also more compact models like EDM, achieving faster sampling under smaller model sizes. In addition, why are the model sizes of DDPM and SDE larger than their original papers'?\n- What if only run 5 NFES?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699089248369,
        "cdate": 1699089248369,
        "tmdate": 1699635994801,
        "mdate": 1699635994801,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6r3IETLDQN",
        "forum": "0pBX9FLGRR",
        "replyto": "0pBX9FLGRR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission678/Reviewer_D5Rq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission678/Reviewer_D5Rq"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an image generation model named decoupled diffusion model (DDM), with the intuition of decoupling the forward process of diffusion models into an \u201cimage-to-zero\u201d mapping and a \u201czero-to-noise\u201d mapping. That is, in the forward process, the clean image will be attenuated to zero and the added noise will grow to a standard Gaussian noise. With this intuition, this work introduces a new training objective that predicts noise and clean image, simultaneously. In the tasks of unconditional and image-conditional image synthesis, this work demonstrates that the proposed method can achieve better performance than SOTA methods when the number of function evaluations is small ($\\leq$10 steps)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Overall, the writing is clear, although there are some minor presentation issues (see comments below). \n- Experimental settings are relatively diverse: Both unconditional and image-conditional image synthesis tasks are considered. Besides, the ablation studies are sufficiently conducted to show the importance of each component or hyperparameter."
            },
            "weaknesses": {
                "value": "- Regarding the new diffusion formula, I do not see much difference with VP-SDE except for the different noise scheduling. Considering the best performing case where $h_t =c$, from $x_0 + \\int_0^1h_t dt =0$ we know $h_t = -x_0$. That is, the forward SDE becomes $dx = -x_0dt + dw$ and the analytic form of $x_t$ becomes $x_t = (1-t)x_0 + \\sqrt{t}\\epsilon$. Compared with VP-SDE where $x_t = s(t) x_0 + \\sigma(t) s(t) \\epsilon$ (note that here I followed the notations in Karras\u2019 EDM paper: https://arxiv.org/abs/2206.00364), we can see the main difference is how to set the noise scheduling. For example, to get $x_t = (1-t)x_0 + \\sqrt{t}\\epsilon$, we can simply set $s(t) := 1-t$ and $\\sigma(t) = \\sqrt{t} / (1-t)$. In such a sense, I don\u2019t think the claimed \u201cdecoupling the forward process into image-to-zero and zero-to-noise mappings\u201d is a special feature of the proposed method. \n- Section 4.2 can be largely improved by focusing on the continuous formulation of diffusion models. In specific, the reverse process can be directly derived from a forward SDE (Anderson, 1982), and this has become a more standard way as it makes everything more clear (see Song\u2019s Score-SDE paper and Karras\u2019 EDM paper). There is no need to use the \u201ccontinuous-time Markov chain\u201d and re-derive the conditional Gaussian distribution from the KL perspective. \n- In the derivation of Eq (10) in Appendix A, I don\u2019t know how you can obtain the second last equation in Eq (24). In particular, how do you evaluate the integral in the first term (from $t$ to $t - \\Delta t$)? How do you get a grid of the cross-product term between ($h_{\\theta} - h_t$ and $\\epsilon_\\theta - \\epsilon$)?\n- I am a little confused as to why the authors introduce $\\phi_\\theta$. It would be good to make it more precise how $\\phi$ is related to $h_t$ and $x_0$.\n- One claimed advantage of \u201csimultaneously predicting the image and noise components\u201d allows $x_t$ to be aligned with both starting and ending points at each time step. But there are already some papers (e.g. EDM https://arxiv.org/abs/2206.00364 and v-prediction https://arxiv.org/abs/2202.00512) that consider the better alignment between image and noise components in their training objective. \n- In Section 3, the training objective Eq (4) is not exactly from (Ho et al., 2020). Precisely, Ho et al., 2020 didn\u2019t consider the integral over t. Also, the denoising score matching objective should be $\\nabla_x \\log q(x_t | x_0)$ instead of $\\nabla_x \\log q(x_t )$. \n- In experiments, I think many strong baselines are missing. For example, on CIFAR-10, what is the performance of EDM and DDIM? On CelebA-HQ-256, what is the performance of LDM? On Super-resolution and Image painting, what is the performance of other diffusion-based methods, such as Palette (https://arxiv.org/abs/2111.05826) and I2SB (https://arxiv.org/abs/2302.05872). In particular, I2SB claims to achieve the SOTA performance within 10 steps. \n- Minor presentation issues: (Zhang & Chen, 2022) is not a distillation-based method (in the last paragraph in the introduction). In abstract and many other places, the authors imply that ODE samplers can result in less diverse samples. It is not quite clear to me. Any evidence or reference on this? In Figure 3, what do you mean by saying \u201csample generated images from $t=0.4$\u201d? Also, which group of curves is about FID and which one is about MSE?"
            },
            "questions": {
                "value": "See my comments in the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699324234119,
        "cdate": 1699324234119,
        "tmdate": 1699635994743,
        "mdate": 1699635994743,
        "license": "CC BY 4.0",
        "version": 2
    }
]