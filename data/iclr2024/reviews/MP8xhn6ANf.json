[
    {
        "id": "diU1jOl5DT",
        "forum": "MP8xhn6ANf",
        "replyto": "MP8xhn6ANf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission824/Reviewer_yGia"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission824/Reviewer_yGia"
        ],
        "content": {
            "summary": {
                "value": "Dataset distillation is known to be expensive in both memory and training time. The authors in this paper propose to address this issue directly through an auto-encoder, where the actual DD only happens in the latent space. The distilled codes can be used to further reconstruct and obtain training images. The authors demonstrate the efficiency and promising training results on the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method tackles the DD problem from another angle. The latent code distillation makes a lot of sense in terms of efficiency and can potentially help the field on larger datasets\n+ The authors demonstrate that the proposed method indeed can achieve descent performance with good efficiency\n+ The authors' writing is pretty clear and easy to follow"
            },
            "weaknesses": {
                "value": "- The algorithm seems to be heavily depending on the quality of the pretrained autoencoder, causing another layer of complexity in the distillation procedure.\n- In a more general field, language or other modality, where AEs are not that popular, the proposed method can be limited in terms of contribution or usage.\n- It seems that the authors only focus on a subset of DD algorithm, how would the latent DD perform using FrePo [1] or momentum-based BPTT [2]? It would be nice if authors can add the comparison and discussion on these two as well.\n\n[1] Dataset Distillation using Neural Feature Regression\n\n[2] Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796863080,
        "cdate": 1698796863080,
        "tmdate": 1699636009880,
        "mdate": 1699636009880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dWKLkthOyh",
        "forum": "MP8xhn6ANf",
        "replyto": "MP8xhn6ANf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission824/Reviewer_cyHK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission824/Reviewer_cyHK"
        ],
        "content": {
            "summary": {
                "value": "This work aims to address three challenges in Dataset Distillation: high time and space complexities, and low data compactness. They proposed LatentDD to move the distillation from pixel space to latent space, leveraging a pretrained autoencoder from stable diffusion. The LatentDD method significantly reduce time and space requirements in DD tasks, allowing the distillation of higher resolution datasets and offer more info-compact latent codes within the same storage limits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-motivated, and well-organized. The observation that \"distilling dataset in the original space (e.g. pixel space for image datasets) will inevitably condense high-frequency detailed information into limited storage budget, which is usually unnecessary for downstream tasks\" is a solid point to serve as motivation for method design. The authors also provide comprehensive experiments on various datasets."
            },
            "weaknesses": {
                "value": "1. While this method has demonstrated its effectiveness for high-resolution dataset distillation, there are no experiments and results comparison on lower resolution datasets such as CIFAR10/100. It leaves concern of whether using an autoencoder from stable diffusion for DD impacts the performance of distillation for such datasets.\n2. To my knowledge, coreset selection does not belong to dataset distillation, and dataset distillation usually refers to the optimization based methods that distill the data into compact synthetic sets. Therefore, the statement in the introduction: \"Some DD methods select a subset from the full dataset according to certain rules (Feldman et al., 2013; Welling, 2009; Sener & Savarese, 2018; Aljundi et al., 2019; Zhou et al., 2023), usually referred to as coreset selection\" seems confusing.\n3. In the introduction, P1: \"computationally intensive bilevel optimization problem\" and P2: \"space complexity, i.e. DD needs to store the whole computation graph\" however, I don't see the present method design that directly aims to address the computation issue of bilevel optimization and I did not see the latentDD method evaluated on bilevel optimization based DD methods. Besides, current method still needs to store the entire computation graph.\n4. All three methods chosen by the authors all falls into surrogate objective DD frameworks which seems limiting and the meta-learning based methods are ignored. It would be a lot more convincing to include methods related to meta-learning based DD as well.\n5. Eqn.2 is for meta-learning DD framework, and the authors did not evaluate their latentDD method on meta-learning DD framework, which seems kinda disconnected to list Eqn.2 here. \n6. The authors claim: \"all the previous works have distilled datasets in pixel space\" which does not seem accurate. Check [1] for more details.\n7. The authors did not report the full dataset (of latent codes) performance, making it hard to evaluate the performance gap between original dataset and distilled ones.\n8. Following 7, since the full dataset (of latent codes) performance is unknown, and there's also no experiment with the performance evaluation of the initial latent code (post-autoencoding and pre-distillation), it's unclear to me whether the performance would be good enough just with the latent code itself even without the distillation process. I think it will be interesting to see how much of the performance gain it has during the distillation process, or this distillation process actually hurts the initial latent code's info-compact ability.\n9. The cross-architecture results reported in table 6 seems confusing, eg., the performance of ResNet18 (56.00) and VGG11 (49.32) performance are consistently better than the ConvNet results (46.72), even the distilled data comes from ConvNet.\n10. The authors only report IPC=1 for Res=512, would be great to see the performance of IPC=10. \n\n[1] Cazenavette, George, et al. \"Generalizing Dataset Distillation via Deep Generative Prior.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "Check weaknesses section for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Reviewer_cyHK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805395646,
        "cdate": 1698805395646,
        "tmdate": 1699636009769,
        "mdate": 1699636009769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zTAkakzXc5",
        "forum": "MP8xhn6ANf",
        "replyto": "MP8xhn6ANf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission824/Reviewer_Ya4Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission824/Reviewer_Ya4Y"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenges in Dataset Distillation (DD) by transitioning from pixel space to info-compact latent space. This shift reduces time and space requirements while maintaining performance, enabling high-resolution dataset distillation. The paper's method delivers more info-compact latent codes within the same storage constraints, enhancing efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper identifies three primary challenges in dataset distillation: high time complexity, high space complexity, and the retention of unnecessary high-frequency information. The authors claim to introduce a pioneering framework that directly addresses these issues by conducting dataset distillation in the latent space, rather than the pixel space."
            },
            "weaknesses": {
                "value": "\u25cf The authors assert that they are the first to successfully address these three challenges in dataset distillation. What specific limitations or hindrances have prevented existing works from generalizing solutions to these problems? Is the proposed method the sole solution, or are there alternative approaches that merit consideration?\n\n\u25cf I've noticed that the paper exclusively presents performance experiments on the Sub-ImageNet dataset. Given the existence of prior works that have addressed the full ImageNet condensation problem efficiently, and the authors' claim of efficiency, it would be valuable to see if the authors can tackle this challenging task and report the results.\n\n\u25cf How could the third challenge be solved in this framework (distilling dataset in the\noriginal space (e.g. pixel space for image datasets) will inevitably condense high-frequency detailed\ninformation into limited storage budget, which is usually unnecessary for downstream tasks.)? Is it tested on the experiments?\n\n\u25cf As far as my current knowledge goes, there are existing works on matching the latent space in the dataset distillation (DD) framework. Could you highlight the primary distinctions between your work and these existing approaches that set your method apart?"
            },
            "questions": {
                "value": "Methodology and Experimental Setup:\n\na. Could you provide a more detailed description of the methodology used in your experiments, including specific hyperparameters, model architectures, and training protocols?\n\nb. How were the datasets prepared and preprocessed before conducting experiments, and what criteria were used for data selection and cleaning?\n\nComparative Analysis:\n\na. In the context of your efficiency claims, can you offer a direct quantitative comparison of your method with existing dataset distillation (DD) frameworks, highlighting the advantages and limitations?\n\nb. Given the broader landscape of DD research, can you elaborate on how your approach compares with other methods in terms of scalability and generalization to different datasets and architectures?\n\nScalability and Generalization:\n\na. To address scalability, how does your method perform when applied to datasets with higher resolutions or complex network architectures?\n\nb. Can you discuss the generalization capabilities of your proposed method, particularly in the context of training on models directly (without transfer learning) and its applicability to large-scale datasets like ImageNet?\n\nLatent Space vs. Pixel Space:\n\na. What are the main advantages of conducting dataset distillation in the latent space, as opposed to the pixel space, and how does this impact the retention of high-frequency information?\n\nb. Could you explain the rationale for not performing experiments on the full ImageNet dataset, given your assertion of efficiency, and how your method could address this challenging task?\n\nDistinguishing Features:\n\na. In light of existing works that match the latent space in the DD framework, what key differentiating features or innovations characterize your approach?\n\nb. Can you clarify the specific mechanisms or techniques that set your method apart from prior works, leading to the successful resolution of the identified challenges in DD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817183852,
        "cdate": 1698817183852,
        "tmdate": 1699636009680,
        "mdate": 1699636009680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sLlpVdfP91",
        "forum": "MP8xhn6ANf",
        "replyto": "MP8xhn6ANf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission824/Reviewer_KLWs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission824/Reviewer_KLWs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to perform dataset distillation in the latent space instead of the pixel space. The proposed method first encodes real images of target dataset into the latent codes. Then three representative (pixel level) distillation methods are adapted to distill latent codes. After that, the distilled latent codes are fed into the decoder to get the distilled images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper shows that, performing the distillation in latent space costs less resources than the distillation in pixel space, without sacrificing much performance."
            },
            "weaknesses": {
                "value": "Most comparisons in this paper are UNFAIR.\n\nIn dataset distillation area, previous works compare the performance under the same IPC (image per class) settings, which means that the AMOUNT of the distilled images fed into the evaluation network is fixed.\n\nThis paper proposes \u2018LPC\u2019 (latent per class) and claims that 1 IPC=12 LPC since their size are the same (latent codes have lower resolution). Then the authors compare their method\u2019s performance (12*n LPC) with previous works (n IPC), which means that the method proposed in this paper actually uses twelve times more images than previous methods for evaluation.\n\nI think comparing performances under the same \u2018storage consumption\u2019 settings rather than IPC are unfair and unacceptable. Otherwise, we can perform the distillation first and then use an auto-encoder to compress the distilled images, such that we can use more distilled data under the same \u2018storage consumption\u2019 setting; accordingly, the performance is improved. Then the development of dataset distillation might turns toward finding a stronger auto-encoder."
            },
            "questions": {
                "value": "In Table 2, the time consumption of LatentDC/DM/MTT is evaluated under the same IPC settings or the proposed LPC settings?\n\nI think the selling point of this paper should be: Performing distillation in latent space is quicker, low-cost, and will not sacrificing performance drastically. It is fine to perform worse than previous works since it is hard to acquire lower cost and better performance at the same time. Please stop performing the evaluation under \u2018LPC\u2019 settings. I suggest the authors to perform a fair comparison and highlight their contributions better (such as low cost)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission824/Reviewer_KLWs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699234558993,
        "cdate": 1699234558993,
        "tmdate": 1699636009584,
        "mdate": 1699636009584,
        "license": "CC BY 4.0",
        "version": 2
    }
]