[
    {
        "id": "aS0HJxVH7M",
        "forum": "9FXGX00iMF",
        "replyto": "9FXGX00iMF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_LWro"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_LWro"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that aims to find informative subset of the original datasets which can be used to train the neural networks with small performance drop compared with model trained with whole dataset. The point of this paper is to propose a method that can do both universal and efficient selection of subset based on the difficulty score. To adaptively select the best subset, the authors propose a method based on kernel ridge regression. The proposed method can be used to select subset for both training from scratch and fine-tuning. Extensive experiments are conducted to verify the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper gives a deep understanding of which kind of data can be useful for different size of subset and use kernel regression to analyze this problem theoretically. \nThe situations  for hard sample and easy sample  to have benign effect is reasonable. \nThe usage of kernel ridge regression for subset selection is interesting.  The details of each parts of the proposed method are illustrated clearly.\nExtensive experiments validate the efficacy of the proposed method for training from scratch. \nThe method can also be effective when used to select subset for fine-tuning.\nAblation studies also validate the robustness of the proposed method."
            },
            "weaknesses": {
                "value": "For the experiments on CIFAR-10 with noise, the proposed method is outperformed by Moderate DS for 3 ratios. Could the authors illustrate the noisy rate of the selected subset to check whether the proposed method is prone to choose noisy data under this setting?\n\nThe experiments on CIFAR-10 fine-tuning on VIT shows that CCS is consistently better than the proposed method, could the author give concrete analysis of this phenomenon?"
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671126556,
        "cdate": 1698671126556,
        "tmdate": 1699636283285,
        "mdate": 1699636283285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cmn3EbSH5z",
        "forum": "9FXGX00iMF",
        "replyto": "9FXGX00iMF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_BL3J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_BL3J"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel and universal coreset selection method called \"Best Window Selection (BWS)\" to strike a balance between sample diversity and model performance across a broad range of selection ratios. BWS first sorts all training examples w.r.t. the difficulty score and then prunes a specific number of the most difficult examples and easiest examples. By comparing BWS with other SOTA baselines, the evaluation results show that BWS outperforms other coreset selection methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel coreset selection method, BWS. Compared to previous work, BWS selects the best window more efficiently with kernel ridge regression, which is faster than training a model from scratch.\n\n2. The evaluation results show that BWS achieves better or comparable results to other SOTA methods.\n\n3. The overall writing is good and easy to follow."
            },
            "weaknesses": {
                "value": "1. Using kernel ridge regression to decide the best window is not quite intuitive. What is the motivation to use kernel ridge regression rather than training a small network to decide the best window?\n\n2. The baseline evaluation results are inconsistent with data reported in the baseline method. For example, moderate are reported to have better performance than random on CIFAR10. CCS seems to have a better performance at 10% subset ratio than the numbers reported in the paper. It may be good to explain why the difference exists."
            },
            "questions": {
                "value": "I don\u2019t fully understand why the performance of $w_s$ can represent the performance of models trained on the same subset. Could the authors further explain the connection between kernel regression and deep learning model training? What I currently feel is that it is more like an empirical transferability stuff studied in [1]: it is possible to use a small model to select coresets that transfer well to larger models.\n\n[1] Coleman, C., et al. \"Selection via Proxy: Efficient Data Selection for Deep Learning.\" International Conference on Learning Representations (ICLR). 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778986512,
        "cdate": 1698778986512,
        "tmdate": 1699636283213,
        "mdate": 1699636283213,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ueP8lEOQaX",
        "forum": "9FXGX00iMF",
        "replyto": "9FXGX00iMF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_j18Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3340/Reviewer_j18Y"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach, known as Best Window Selection (BWS), designed to tackle the challenges associated with data subset selection in machine learning. BWS allows for the adaptable selection of subsets based on sample difficulty scores and consistently delivers competitive performance over a broad range of selection ratios, spanning from 1% to 90%. It excels in comparison to existing score-based and optimization-based methods when applied to datasets like CIFAR-10/100 and ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The problem studied is meaningful and significant: finding a versatile data selection approach capable of sustaining competitive performance across a diverse range of selection ratios.\n2) Experiments show that the proposed BWS consistently outperforms other baselines, including both score-based and optimization-based approaches.\n3) The authors provide code, which enhances the reproducibility."
            },
            "weaknesses": {
                "value": "1) The notion of a \"window\" refers to a fixed-length interval within a sorted dataset. The \"Best Window Selection (BWS)\" algorithm operates under the assumption that the most optimal subset should be contiguous regarding the level of difficulty. However, the paper lacks an in-depth analysis of this particular aspect.\n\n2) It would be intriguing to explore the broader scenario where a \"window\" comprises several smaller intervals and varying starting points.\n\n3) Figure 3's readability could be enhanced by employing more distinguishable colors and markers for clarity."
            },
            "questions": {
                "value": "Kindly refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3340/Reviewer_j18Y"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796522117,
        "cdate": 1698796522117,
        "tmdate": 1699636283148,
        "mdate": 1699636283148,
        "license": "CC BY 4.0",
        "version": 2
    }
]