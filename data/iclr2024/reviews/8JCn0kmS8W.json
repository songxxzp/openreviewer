[
    {
        "id": "0wUfndscxV",
        "forum": "8JCn0kmS8W",
        "replyto": "8JCn0kmS8W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_jF76"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_jF76"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an interesting integrated system pipeline for producing storytelling audio by leveraging the power of LLM, i.e., GPT-4, along with different audio expert models. \n\nAt its core, the pipeline consists of an audio script writer, script compiler and a code executor to generate audio utilizing audio expert models. The input to the system is a short description (prompt) of an audio topic in natural language, and GPT-4 takes the prompt and converts it into event-based (speech, music and sound effects) audio script representation. Such script representation is then compiled by the manually designed script compiler, whose outputs are executables that could be invoked by different audio expert models.\n\nExperimental results show that the method could outperform or achieve on-par results against several state-of-the-art text-to-audio models on AudioCaps and Clotho benchmarks. In addition, the paper presents a new type of benchmark  covering 5 genres, namely audio storytelling benchmark. Human evaluation on the method shows reasonable mean opinion scores in 5 different aspects."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a novel task, text-to-audio storytelling. The task opens up a huge space for AI-powered general audio creation.\n\n2. The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.\n\n3. The new established benchmark is another good contribution.\n\n4. The developed prototype and demos look amazing."
            },
            "weaknesses": {
                "value": "1. The method proposed has limited methodology contribution to the research community. The system reflects a crafted engineering pipeline by combining several off-the-shelf audio expert models through LLM. There is limited contribution in terms of machine learning algorithms.\n\n2. As sort of an ensemble of expert models, the paper does not include any system-level ablation studies by comparing against other design choices.\n\n    a. How much does the system rely on the capabilities of LLMs? How would it perform if LLM is not GPT-4, but rather those open-source alternatives like Llama.\n\n    b. The script writer acts like an agent. There are numerous works and existing methods working on connecting LLM to perform various tasks, e.g., AutoGPT. But the authors didn\u2019t compare against these agent-based system design."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Reviewer_jF76"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698291637233,
        "cdate": 1698291637233,
        "tmdate": 1699636908472,
        "mdate": 1699636908472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d5uLWksw7M",
        "forum": "8JCn0kmS8W",
        "replyto": "8JCn0kmS8W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_HBpU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_HBpU"
        ],
        "content": {
            "summary": {
                "value": "- The paper presents WavJourney, a system for compositional generation of audio assets by leveraging existing specialized audio models and combining them with LLM-based agents.\n- WavJourney is composed of an LLM-based audio script writer which generates an audio script given a text instruction from a user. This script is organized in a JSON formatted object which captures the spatio-temporal relationships between different audio \u2018events\u2019 which would be one of speech, music, or sound effects.\n- A script compiler takes the audio script and generates a computer program that is executed to call one of AudioGen, MusicGen, or Bark models. There is a mechanism to generate background and foreground audio in the script.\n- The authors also propose an Audio Storytelling benchmark using ChatGPT and come up with various evaluation criteria such as engaging, creativity, relevance, emotional resonance, and pace & tempo. Listeners rate audio generated by a system from 1-5 for each of these criteria.\n- The authors evaluate WavJourney on text to audio generation and the aforementioned storytelling benchmark. They use objective and subjective metrics for the text to audio evaluation. The results show that WavJourney performs the best on the text-to-audio generation, and received scores marginally about moderate ratings on the storytelling benchmark. The authors also emphasize that the results of WavJourney are better than ground truth data of the AudioCaps dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes an interesting approach for human-machine co-creation specific to audio. To the best of my knowledge, there is no other system that is leveraging existing LLMs and audio generation models for such a use-case. The use of a deterministic compiler for generate the exact instructions from the output of the LLM is an interesting choice. Additionally, the choice to focus separately on foreground and background seems to work quite well in enhancing the quality of the outputs. \n- While I am not very impressed with the results especially in terms of naturalness, I can see that such an application might be useful for storytelling."
            },
            "weaknesses": {
                "value": "- There doesn\u2019t seem to be substantial technical contribution in the paper. The main technical components I can see are:\u2028\n  1. Using the in-context learning capabilities of LLMs to generate well-formatted outputs for audio scripts.\u2028\n  2. Converting the scripts to calls to 3 different models.\u2028\n  3. Carefully assembling the generated audio into a single unit.\u2028\n\n  While I do appreciate the effort it takes to assemble all these components, I am not too sure of their scientific value to the ICLR community.\n- Based on the text, it is not clear what the basis of the metrics proposed in Audio Storytelling benchmark is. There are no references to any studies from the relevant communities which justify the choice of the specific criteria. I would expect the authors to at least add some information, either in the paper, or an appendix, going into more detail regarding the justification being choosing these metrics. \n- The result showing WavJourney outputs to be better than AudioCaps GT audio seems a little off to me. \n  1. It\u2019s not clear what the preference test is focusing on: the overall quality, or the adherence to the caption. \n  2. I wonder if the authors ran the GT audio through the same audio quality enhancement model for evaluation. \n  3. In table 3, the 6th column seems to have the wrong entry bolded. GT audio has a higher REL score than WavJourney audio.  \n  4. The fact that the results are so one-sided for Clotho dataset gives the impression that the result is not too signification (even though the statement is true), or that there is something wrong in one of the two datasets. \n\n  Based on the samples shared, I felt all the GT audio to be more natural sounding than WavJourney. I would encourage the authors to also showcase examples where WavJourney was rated higher than GT by the listeners.\n- The generated results have pretty severe audio artifacts. This is not necessarily a weakness of WavJourney itself, but is an issue with the audio generation models themselves."
            },
            "questions": {
                "value": "- See weaknesses section.\n- I would also be interested in seeing a Turing-test type question asking users if they can tell whether the audio is generated or not."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698886845782,
        "cdate": 1698886845782,
        "tmdate": 1699636908298,
        "mdate": 1699636908298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wZJzNWeI6C",
        "forum": "8JCn0kmS8W",
        "replyto": "8JCn0kmS8W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_koFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_koFK"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework called WavJourney for compositional audio creation. The WavJounrey first provides high-level text prompts to LLMs to generate an audio script that contains the detailed content description of speech, music, and sound effects. Next, a specific script compiler is used to convert the audio script into computer programs which will execute various pre-trained models to generate and compose the audio content. The framework is training-free and a novel subjective multi-genre storytelling benchmark is proposed to evaluate the generated results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The generated samples are well done.\n2) The framework is training-free.\n3) The framework offers high interpretability and flexible ways to create audio content.\n4) A novel subjective evaluation metrics are proposed."
            },
            "weaknesses": {
                "value": "1) While the generated results are impressive, this work focuses more on production than academic research. It shows how best we can achieve when combining state-of-the-art models, and the contribution is limited from the perspective of technical novelty.\n2) Since the SOTA generative models are not perfect everywhere, sometimes those models might not generate the expected content. It seems like the proposed framework doesn't take it into consideration and may not always be reliable or robust to use.\n3) As an AI audio creation tool, it would be nice to enable the editing of existing audio content. It happens a lot in movie or TV creation. In this case, the framework needs to take the audio as an input as well. However, it seems like the current framework cannot support that.\n4) The evaluations are limited to audio captioning tasks. The evaluation of speech and music qualities is missing."
            },
            "questions": {
                "value": "1) How well does the Wavjourney perform in music and speech benchmarks?\n2) What are some failure cases?\n3) What is the exact time cost to produce an audio sample?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Reviewer_koFK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699469466156,
        "cdate": 1699469466156,
        "tmdate": 1699636908196,
        "mdate": 1699636908196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I1l2gcjeUo",
        "forum": "8JCn0kmS8W",
        "replyto": "8JCn0kmS8W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_xGho"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_xGho"
        ],
        "content": {
            "summary": {
                "value": "The authors leverage large language models to connect various audio models for audio creation, such as storytelling audio content with diverse audio elements from textual descritpions.  The separation capabilities of this paper is related to HuggingGPT, combing multiple audio generation models with the user interaction to generate the final audio content. The novelty of this paper is to use LLM's text parsing ability to encode the long user inputs into the format which is easy to process using script compilers, to utlize the audio generation models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem is interesting."
            },
            "weaknesses": {
                "value": "1. The way this paper deals with audio temporal relationships is to generate audio using multiple models and manually connect the generated audio. However, considering the ambiguity of language and the complexity of natural audio, there may be partly overlap among the generated foreground audio lists. This method cannot well fit the distribution of natural audio data. The audio shown in the demo and the mel-spectrogram shown in the article show this shortcoming: there is a clear separation between different audio contents, and there are obvious traces of artificial processing.The author also pointed out this point in the limitations.\n\n2. The author points out that WavJourney outperforms previous state of the art methods in both subjective and objective evaluations. However, on the objective indicators of the Audiocaps dataset, WavJourney's performance is lower than AudioGen's.\n\n3. The experimental part of the article is rather inadequate, The analysis of composite audio generation capabilities of the method is limited to the scene of storytelling, and the analysis of interactive audio creation in multi-round dialogues is also oversimplified, making an inadequate demonstration of the core capabilities of the model."
            },
            "questions": {
                "value": "1. More ablation study need to be conducted.  Considering that WavJourney uses AudioGen for text-to-audio synthesis, the phenomenon of superiority over AudioGen on objective metrics of the clotho dataset requires further analysis.  Is this due to the introduction of a text-to-speech synthesis model by WavJourney, or is it due to WavJourney's use of LLM to improve its understanding of long captions?\n\n2. More examples of speech content control should be provided. Whether users can control the speech content in the story? It\u2019s important if you want to involve text-to-speech function. I didn't see any relevant examples of speech content control."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699542071472,
        "cdate": 1699542071472,
        "tmdate": 1699636908097,
        "mdate": 1699636908097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "29a6fIyEdE",
        "forum": "8JCn0kmS8W",
        "replyto": "8JCn0kmS8W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_iM1V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7520/Reviewer_iM1V"
        ],
        "content": {
            "summary": {
                "value": "This paper presented a compositional audio content creation method using LLM and speech, sound effect, and music generation models. The procedure is like this. If the user types specific scenario text, then LLM analyze the text and generate audio script based on the detailed audio prompt templates. This audio prompt templates define the type of the output file format (which is multiple json formats depending on the number of sound sources,) and audio_type (music, speech, or sound effect), duration, volume, foreground/background, position in time, and description of the audio characteristics (e.g. types of music, speech text, or description of sound effect). Then, using this json each for sound source, audio generation models are called and generate each sound source. Finally, the generated sound sources are mix down together. The proposed method is quite novel and has some distinguished characteristics from the most relevant previously proposed method, which is AudioGPT in my opinion. In the previous work, the concept of utilizing audio generation models with chatGPT has been already proposed. However, the previous work mostly focused on calling each generation model separately, so tried to focus on conversational audio generation (single task generation, not the mix of the multiple tasks). Also, in this paper, the author utilized position in time, duration, and volume like characteristics, which further opens up the usage of the LLM on audio model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper focused on a compositional audio content creation, not the end-to-end generation model. In audio domain, I believe that this kind of compositional audio content creation will be more useful, because audio has specific physically proven time-frequency relationship, and is really weak for noise (in human perception), so that, a compositional creation can be one solution. And, the paper described one way to do that by tackling in sound types, sounds level, sounds position, etc."
            },
            "weaknesses": {
                "value": "I think this paper is more like a positional paper rather than an experimental paper. The authors evaluated the proposed methods in two ways. One for validating the proposed method, they evaluated the proposed method on already established text2audio generation task. This evaluation showed that the proposed compositional audio generation method is working well within an audio generation task which I think is enough to show the effectiveness of the proposed method. As a second experiment, the authors further explored audio storytelling creation task. And, in my opinion, this experiment is more like what we can do more or further use cases rather than a solid evaluation method. They measured five subjective metrics, however, since the generated audio content contains music, effect, and speech sounds, we are not sure about where the impressions are came from. Also, baseline is not existed (even though it's not for this task, comparing with audiogen and audioldm might give some baselines)."
            },
            "questions": {
                "value": "Therefore, I think if the authors can give more insights, details about experimental setup, and analysis on the proposed methods rather than describing audio storytelling evaluation, would give a lot more insights to the readers. For example, as the authors mentioned in Section 3.2, there are quite not small possibilities that LLM doesn't follow the instructions. The authors mentioned that the proposed script compiler reduced the instability of LLMs. If the authors can explain this kind of part in more detail, by adding the average possibility of the error case with/without script compiler, and how the script compiler is built, then the readers can get more insights about the practical use of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7520/Reviewer_iM1V"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699552212293,
        "cdate": 1699552212293,
        "tmdate": 1699636907977,
        "mdate": 1699636907977,
        "license": "CC BY 4.0",
        "version": 2
    }
]