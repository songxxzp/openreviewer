[
    {
        "id": "xAn4Ah06bW",
        "forum": "xhCZD9hiiA",
        "replyto": "xhCZD9hiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_uof1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_uof1"
        ],
        "content": {
            "summary": {
                "value": "The paper is a theoretical study on batch normalization. A simplified setting is considered where BN does not use mean subtraction and where there are no other non-lineary functions. The paper presents two theorems that hold when the weight matrices are random orthogonal matrices. Theorem 1 states that the representations become increasingly orthogonal with the depth of the network. Theorem 5 states that the gradients do not explode with depth. All results hold in expectation over the random weights. The authors also provide a method for modifying non-linear activation functions by changing the gain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents two theorems which are relevant and non-asymptotic.\n\nNormalization is used everywhere in ML, so studying it has a high impact."
            },
            "weaknesses": {
                "value": "The main question of the paper is \u201cIs there any network with batch normalization without gradient explosion and rank collapse issues?\u201d. It is not clear why this question is interesting. The authors show that orthogonal random weights avoid these issues, but such initialization schemes are not used in practice despite being well known. Thus, in practice, BN works for reasons that are unrelated to the specific weight initialization scheme that is proposed. So the paper is not really \u201cexplaining\u201d why BN works.\n\nThere are still some simplifying assumptions in the theoretical parts \u2013 there are no nonlinearities and the mean subtraction of BN is omitted. \n\nIt is not clear how relevant the empirical results are, though the authors might be able to clarify this for me."
            },
            "questions": {
                "value": "Why is the main question (\u201cIs there any network with batch normalization without gradient explosion and rank collapse issues?\u201d) interesting to practioners? Given that BN works well without orthogonal initialization.\n\nHow is the activation shaping motivated by theory?\n\nWhat is the practical utility of the activation shaping you propose?\n\nDoes your result hold for networks with mean reduction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697498781386,
        "cdate": 1697498781386,
        "tmdate": 1699636701814,
        "mdate": 1699636701814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oqQ1qMo0dQ",
        "forum": "xhCZD9hiiA",
        "replyto": "xhCZD9hiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_GLi5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_GLi5"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses limits of gradient descent loss minization of (very) deep networks using batch normalization (BN). In particular, it mends explosion of gradients during backdrop training in such networks. After a neat introduction a novel theoretical results are laid out in an accessible manner and main idea of proofs is presented (linked to full proofs in Appendix). Using recent results on random matrix theory, namely that BN does not decrease isometry gap (a measure of deviance of sample covariance $XX^T$ from orthogonality), paper proofs its main Theorem 1. It claims that expected isometry gap has an upper bound whose rate of decrease is exponential in depth under assumed linear independence of inputs. It is followed by Theorem 5, which proves that, under mild smoothness and loss assumptions, the $\\textit{expected}$ log norm of gradient of the constructed MLP network, i.e. deep network with linear activations and simplified BN, has finite upper bound $~d^5$.\n\nFollowing section supports the results by limited experiments, showing that rank collapse does not occur when MLP using (simplified) BN as proposed is used.\n\nPaper follows to discuss possible extensions to treat non-linear networks and proposes activation function shaping through additional pre-activation scale hyperparameter, that is tuned towards zero, effectively linearising explored models using $tanh(\\cdot)$ and $sin(\\cdot)$ activations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. To the best of reviewers knowledge presented are novel results (Theorem 1, 5) that are non-asymptotic and hold for networks with finite width, as opposed to previous works. This is very promising and of large interest and value to ML community increasing potential impact of the manuscript. \n\n2. Very well and accessibly written exposition of rather technical methods (random matrix theory, Weingarten calculus) used in proofs.\n\n3. After theoretical results paper (creatively) suggests activation shaping technique in combination with BN increasing and demonstrating practical utility of results."
            },
            "weaknesses": {
                "value": "#1: While the non-asymptotic and finite width results may be largely impactful as noted in Strengths, the importance may be diminished by applicability on \n1. $\\textit{expected}$ grad norm\n2. grand norm bounds of Theorem 5 may be still prohibitively large $\\textit{e^{d^5}}$ and \n3. \"practicality\" may be hindered by \"$\\textit{linear}$ activation\" required (this is somewhat alleviated by last section by introducing more hyper-parameters to bring $sin$ and $tanh$ activations to linear regime).\n\nI believe authors should extend the Discussion or include section to elaborate more on these points. Possibly extend a rather limited experiments to corroborate their arguments in case of a lack of theory.\n\n#2: Technical: Dimensionality discrepancies throughout Section 3, page 3 and 4. For instance:\n - in the definition of $\\varPhi(X)$ on page 4, the $\\varPhi(X)$ is not function of $R^{d \\times d}$.\n - $X$ should be $d \\times n$ matrix instead of previously defined $n \\times d$ or $XX^T$ should be used instead of $X^TX$"
            },
            "questions": {
                "value": "Q1: Related to Weaknesses, ad 3.) \"practicality\", Could authors present their view on what benefits are provided by deep linear networks compared to shallower linear nets? \n\nQ2: To address aforementioned limitations of theoretical results (see Weaknesses #1, 1-3),, e.g., to support non-linear activation, authors propose activation shaping combined with use of BN. From their argument it seems that such technique, tuning pre-activation gain $\\alpha$ towards zero, effectively linearise the network. Section 5 claims that this technique still maintains the benefits of non-linear activations demonstrated in Fig.5. Could authors elaborate on how exactly are benefits maintained? \n\nQ3: In later sections paper presents intriguing idea of \u201cimplicit bias towards orthogonal matrices\u201d, for instance in Fig 6. But one can read experimental results in Fig 6. other way, namely that it suggests that while middle layers have been initialised already \u201calmost orthogonal\u201d their orthogonality gap increased the most during training compared to other layers! Could authors show further evolution of training continued beyond the early stopping or otherwise corroborate \"implicit bias\" claims more extensively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705910201,
        "cdate": 1698705910201,
        "tmdate": 1699636701643,
        "mdate": 1699636701643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BFGc9svZnQ",
        "forum": "xhCZD9hiiA",
        "replyto": "xhCZD9hiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_eDWh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_eDWh"
        ],
        "content": {
            "summary": {
                "value": "The authors address the problem of gradient explosion with depth in networks with Batch Normalization (BN) outlined by Yang et al. [1]. They show that infinitely deep linear networks can be trained with BN while avoiding the problem of gradient explosion as long the weights are orthogonal.\n\n[1] Yang, Greg, et al. A Mean Field Theory of Batch Normalization. International Conference on Learning Representations. 2018."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors present a proof for avoiding gradient explosion in infinitely deep linear networks with BN as long as the inputs are full rank and the weights of the network are orthogonal.\n\n2. In linear MLPs with upto 1000 layers, they show that orthogonality is approximately maintained in the middle layers as required by the proposed theorem.\n\n3. The presented proof and argument is easy to follow."
            },
            "weaknesses": {
                "value": "1. The authors have not included literature on recent work on inducing dynamical isometry in feedforward networks which has shown to also avoid the need for BN. The discussion seems relevant for this paper (see [1], [2], [3]).\n\n2. The proof relies on the fact that the network is has linear activations and the weights are orthogonal during training. Although, this seems to somewhat hold empirically for linear networks, how reasonable is it to expect such a condition to hold for nonlinear activations. The weights will likely not stay orthogonal during training for nonlinear activations. Further, can the analysis be extended to nonlinear ReLU - like activations, by potentially using a first order approximation of the ReLU [4] or other approximations?\n\n3. Since the proof requires for the network weights to be orthogonal, the network is only restricted to learn a rotation of the inputs. This seems restrictive from an expressiveness point of view unless I misunderstand something.\n\n[1] Rebekka Burkholz and Alina Dubatovka. Initialization of relus for dynamical isometry. Advances in Neural Information Processing Systems, 2019.\n\n[2] Yaniv Blumenfeld, Dar Gilboa, and Daniel Soudry. Beyond signal propagation: is feature diver-\nsity necessary in deep neural network initialization? In International Conference on Machine\nLearning, 2020\n\n[3] Andrew Brock, Soham De, and Samuel L. Smith. Characterizing signal \npropagation to close the performance gap in unnormalized ResNets. International Conference on Learning Representations. 2020.\n\n[4] Rebekka Burkholz. Most activation functions can win the lottery without excessive depth. Advances in Neural Information Processing Systems 2022."
            },
            "questions": {
                "value": "See above section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6359/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6359/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6359/Reviewer_eDWh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778654461,
        "cdate": 1698778654461,
        "tmdate": 1699636701529,
        "mdate": 1699636701529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QZqrbUSwE7",
        "forum": "xhCZD9hiiA",
        "replyto": "xhCZD9hiiA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_692A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6359/Reviewer_692A"
        ],
        "content": {
            "summary": {
                "value": "The paper shows the existence of a batch-normalized network that avoids rank collapse and does not suffer from exploding gradients as depth increases. To construct a network that satisfies the above properties, the paper uses an initialization scheme where the weights are random orthogonal matrices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The isometry and log gradient norm bounds are non-asymptotic bounds. Furthermore, the usage of Weingarten calculus to obtain rates for isometry increase is interesting."
            },
            "weaknesses": {
                "value": "The paper uses a modification of batch normalization (in particular the mean centering operation is removed) when proving bounds on the isometry gap and log gradient norms (however, the experiments seem to suggest that this is not an issue).\n\nThe paper also considers the setting where the number of training examples is equal to the dimension of the problem.."
            },
            "questions": {
                "value": "Do you obtain figures similar to F1 when using ReLU activations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6359/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841613212,
        "cdate": 1698841613212,
        "tmdate": 1699636701419,
        "mdate": 1699636701419,
        "license": "CC BY 4.0",
        "version": 2
    }
]