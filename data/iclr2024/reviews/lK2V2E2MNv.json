[
    {
        "id": "85dFa83C5l",
        "forum": "lK2V2E2MNv",
        "replyto": "lK2V2E2MNv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_zAaX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_zAaX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to align the visual representations of pretrained visual encoders into the input space of pretrained language models, using a linear projection layer. The linear layer is the only trainable part in the system, which is supervised by two losses: (a) assignment consistency - the visual features and text features are assigned to the word, and a similarity loss between the assignment results is applied (b) an image captioning objective. Using this method, experiments are done on 3 tasks, including image captioning, VQA, image-text retrieval to show that the method outperforms existing methods. Different variations of visual and text models are studied in the experiments. Additionally, some qualitative visual semantic arithmetic results are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method is simple and clear - train a linear layer with two losses including the newly proposed assignment prediction loss. \n2. Intensive experiments are provided on 3 tasks using different visual backbones (CLIP, BeiT) and text backbones (OPT-1.3B and T5-base), where results consistently outperforms existing methods.\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "My major concern is (a) the lack of ablations and feature space visualizations to show the effectiveness of the proposed loss and (b) the contribution over existing works like MAGMA is not enough. \n1. The paper is an extension of MAGMA (Merullo et. al. Linearly mapping from image to text space. In ICLR, 2023.). While MAGMA is discussed in the paper, the difference is that this paper with MAGMA is the proposed assignment prediction loss. However, the effectiveness of the proposed loss is not shown clearly in the paper.\n2. No ablation results are provided to show the effectiveness of the proposed loss - this is related to weakness-1. Since the major contribution lies in this loss, an ablation to show the contribution of this loss in the final results is very critical.\n3. The finding that a linear layer can transform visual representations into language models is not surprising, given existing works LLaVA (\u201cVisual Instruction Tuning\u201d, as in its first training stage), which is not discussed in this paper, and MAGMA as discussed. Therefore, the contribution of this work is weakened.\n4. The authors motivate the work by criticizing the \u201cdistance-preserving nature of the linear layer\u201d. However, the proposed method is still a linear layer, which doesn\u2019t solve this problem. While Fig-4 provides several examples to show the visual semantic arithmetic, a visualization of feature space would be preferred to show the effects of the assignment loss\n5. The paper would be easier to read if the method names (abbreviations) in the results tables come with citations next to them, or are described in texts to show which is which."
            },
            "questions": {
                "value": "1. Could abalations with and without the assignment loss be provided to show its effectiveness?\n2. Could visualizations (e.g. t-SNE) over the feature space with and without the assignment loss be provided, to show its effects in aligning the features?\n3. The difference/contribution over LLaVA or MAGMA can be more clearly discussed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698518060187,
        "cdate": 1698518060187,
        "tmdate": 1699637184906,
        "mdate": 1699637184906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7KGG0WrJCW",
        "forum": "lK2V2E2MNv",
        "replyto": "lK2V2E2MNv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_dYxb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_dYxb"
        ],
        "content": {
            "summary": {
                "value": "In vision-language modeling, a significant challenge persists: bridging the modality gap between pretrained vision and language models. This gap arises primarily due to the models' pretraining exclusively on unimodal data, leading to inconsistencies in their embedding spaces. Motivated by this limitation and the computational costs of previous methods, this work introduces VLAP, a novel linear transformation-based approach that employs assignment prediction to connect vision encoders and large language models (LLMs). By harnessing the established word embeddings of LLMs and introducing an optimal transport-based assignment prediction objective, VLAP maps visual data representations to LLM's word embeddings, aiming for consistent modality representation. This not only results in visual data representations with the semantic richness of LLMs but also surpasses prior methods in computational and memory efficiency across various vision-language tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The limitations from SOA mentioned in the paper exist, and the motivation is valid.\n2. Resolving the modality gap problem with the cross-modal assignment prediction using word embeddings of LLMs is a better solution than previous methods."
            },
            "weaknesses": {
                "value": "1.  A better alignment (reducing the gap) in multi-modality is the essential contribution of this work. However, it lacks studies or results, apart from the overall performance, to validate that the gap reduction is achieved by the current predicted assignments rather than the linear layers from previous works.\n2. The authors mentioned, ``Mapping visual data to LLM\u2019s word embeddings results in learned visual representations that hold a semantic taxonomy of LLMs.'' However, there's a lack of quantitative/qualitative results to validate that this allows visual representations to inherit a semantic taxonomy of LLMs.\n3. The final objectives are influenced by the assignment prediction loss and captioning loss. However, there's a lack of study on these hyperparameters. Also, which part contributes more to the learning remains a question.\n4. For the probability that the corresponding modality data belongs to each word, $P_{nk}$, what does $P_{nk}^{v}$ in the visual modality signify? Does this ``word'' refer to the single word token in the class label of that visual region?\n5. There's a lack of formal definitions for the terms/operations appearing in equations, i.e., $Tr(\\cdot)$, $[prefix]$.\n\n[Summary] The current limitations and motivations are valid, and the claimed contribution is significant. However, in the paper's delivery, there's a concern about how this performance is achieved by the proposed architecture and mechanism. Additionally, the paper lacks a depth of study beyond introducing a novel architecture."
            },
            "questions": {
                "value": "Please also refer to the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772221901,
        "cdate": 1698772221901,
        "tmdate": 1699637184768,
        "mdate": 1699637184768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NE4BzEScxi",
        "forum": "lK2V2E2MNv",
        "replyto": "lK2V2E2MNv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces VLAP bridges vision encoders and language models through assignment prediction and the use of word embeddings to map visual representations into language space. \nAn optimal transport-based training objective is proposed to enforce the consistency of word assignments for paired multimodal data. This allows frozen LLMs to ground their word embedding space in visual data and use their robust semantic taxonomy visually. \nThe experiments demonstrate that VLAP outperforms the linear transformation-based approaches in a variety of vision-language tasks, such as image captioning, visual question answering, and cross-modal retrieval.\nIt also shows that the visual representations that have been acquired contain a semantic taxonomy of LLMs, thus making it possible to do visual semantic arithmetic."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow.\nThe work proposed a straightforward way of learning the linear projection layer for visual modality to learn multimodal representation, which accommodates the LLM generation.\nThe visualization shows an impressive semantic arithmetic ability to combine multimodality understanding in LLM generation."
            },
            "weaknesses": {
                "value": "(1) The main concern of this work is the methodology is relatively incremental without new concepts or findings.\nConcept-wise and architecture-wise, it is similar to Asano et al. (2020) Selavi, which performs optimal transport across modalities with similar pipelines. Mathematics using the Sinkhorn clustering Swav as Caron et al. (2020).\n(2) The main difference lies in 3 parts: word embedding as fixed center space, different distribution assumptions (polytope), and LLM application.\nThe first two are the most interesting part, which will be different from previous Sinkhorn-based work.\nHowever, there is no ablation study on these two components, which leads the readers to question whether borrowing existing Selavi and Swav will also work.\n(3) Also, there is no ablation on different objectives, such as existing next-word prediction on learning visual projection on LLM."
            },
            "questions": {
                "value": "Either additional ablation, justification, or additional baseline can elaborate the concern in the weakness (2)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9396/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9396/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9396/Reviewer_5A8P"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779201028,
        "cdate": 1698779201028,
        "tmdate": 1699637184640,
        "mdate": 1699637184640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y3CIkpDJh7",
        "forum": "lK2V2E2MNv",
        "replyto": "lK2V2E2MNv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_oDtB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed to bridge the vision and language modalities by predicting the assignment between LLM word embeddings and those two modalities. Specifically, the optimal transport is employed to decide the assignment between LLM word embeddings and image/caption contextualized embeddings, and then the model is required to predict the assignment of one modality from the other modality. Experiments are conducted on multiple tasks/datasets to prove the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Demanding one modality's representation to predict the assignment between the other modality and common feature space (LLM word embedding) is an interesting idea to bridge two modalities. \n2. Evaluations on different tasks show a better performance than previous work."
            },
            "weaknesses": {
                "value": "1. Comprehensive ablation of w/ and wo/ assignment prediction on the same vision/language backbones is missing. \n2. Comparison with other baselines that are designed for alignment is missing. For example, contrastive alignment in ALBEF, BLIP, and the first-stage alignment by BLIP2 which includes image-text matching, and image-grounded text generation.\n3. In experiments, the pre-training data is CC3M which is too small in terms of scale. Whether this method can be generalized to larger scale is not validated.\n4. In Tab1,2,3, when compared with previous works, the vision/language backbone is always different. I wonder if using the same backbones as previous works, will the proposed method still outperform them?"
            },
            "questions": {
                "value": "1. Does the LLM word embedding have to be from the same LLM as used in language encoding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813431322,
        "cdate": 1698813431322,
        "tmdate": 1699637184517,
        "mdate": 1699637184517,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vjq2iwj6Mu",
        "forum": "lK2V2E2MNv",
        "replyto": "lK2V2E2MNv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_m8gW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9396/Reviewer_m8gW"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to align the LLMs (encoder/decoder or just decoder) with image encoders such that the LLMs can comprehend visual input better. It further restricts the design space to freeze the original LLM and visual encoder, just relying on a cheap learned linear transformation. To adapt such a transformation, the paper presents two learning objectives -- assignment prediction and image captioning. Empirical results are presented on 3 different tasks -- image captioning, VQA and cross-modal retrieval (I2T, T2I)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The problem is well motivated with wide applications.\n\n* The paper is mostly well written and explained.\n\n* The empirical results show a big delta which demonstrates the effectiveness of the approach. The studies are also conducted on wide range of problem settings."
            },
            "weaknesses": {
                "value": "* The motivation for restricting the learned parameter space to just linear layers is unclear -- it would have been more interesting to see more analysis around different learned parameter space including non-linear layers."
            },
            "questions": {
                "value": "-- Can the authors show ablation studies for the L_map and L_cap objectives to develop better understanding of each component?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820125031,
        "cdate": 1698820125031,
        "tmdate": 1699637184404,
        "mdate": 1699637184404,
        "license": "CC BY 4.0",
        "version": 2
    }
]