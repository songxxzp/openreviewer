[
    {
        "id": "Bw1kdGHSqo",
        "forum": "tItq3cwzYc",
        "replyto": "tItq3cwzYc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission36/Reviewer_8gWN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission36/Reviewer_8gWN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new image classification network composed of the ResUNet and a self-attention module. Experiments on malware image datasets show its advantage."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The writing is easy to understand. \n2. The performance of the proposed method is greater than the compared networks."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. The proposed SimpleResUNet and attention classifier are somehow very similar to previous works. For example, adopting residual blocks in the middle stage of UNet is a commonly used method for recent UNet-based methods, like StableDuffision. The contribution of the authors seems the adapting UNet-based encoder for image classification tasks. From this perspective, a very relevant work is HRNet and its variants which may need to be discussed and compared. \n2. The motivation is not very clear. In the title and the contribution summary of the introduction, the authors emphasize lightweight networks. However, the analysis in the introduction is about local and global information extraction. \n3. The discussion of related works is not enough. Many recent lightweight networks are not mentioned. In fact, most of the recent networks (e.g., MobileViT, Mobile-Former, MobileOne) achieve a better tradeoff between performance and efficiency compared to the methods in Table 1/2/3. In addition, the variants of self-attention (e.g., [1]) are also needed. In a word, some recent related works need to be discussed and compared.\n[1] Fast vision transformers with hilo attention. \n4. Experiments: 1. lacking the results on Cifar-10 dataset. 2. More recent methods are required in Table 2/3. 3. The results of ours-3 in Table 2 and Table 3 are not the same. 4. The expression in the text does not match Figure 2, as the worst version is ours-32 in Figure 2, not ours-3. In the ablation study, a comparison of the different data sizes is required if the authors want to prove that the suitable dimension of the network is related to data size. 5. More general datasets, like ImageNet are required for the comparison. \n5. Writing and Typos, on Page 7, first paragraph,  H\u2019 and W\u2019. It's better to provide the shape of variables in the section 3.1.1."
            },
            "questions": {
                "value": "In my opinion, this paper needs to be further improved. The questions and suggestions are listed in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission36/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698480219170,
        "cdate": 1698480219170,
        "tmdate": 1699635927534,
        "mdate": 1699635927534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kwaK2pNXj6",
        "forum": "tItq3cwzYc",
        "replyto": "tItq3cwzYc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission36/Reviewer_pv9C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission36/Reviewer_pv9C"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a lightweight network structure based on UNet for small sample classification with only 2.5M parameters. The gradient calculation formula is given to show the structure inherits the gradient calculation advantages of ResNet. The proposed model can handle multi-scale image classification tasks. The interpretability of the model is discussed and an inference is proposed to explain the feature space dimension."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to follow.\n2. The proposed method has small amount of parameters and FLOPs."
            },
            "weaknesses": {
                "value": "1. The proposed method is lack of novelty since it is a simple combination of Unet, Attention, and residual. \n2. The presentation should be re-organized, e.g. figure 1, Table 1, figure 2, etc. There are some typos, e.g. Table 3.\n3. I don't quite understand the motivation to choose the Unet for image classification.\n4. Some powerful baselines like Mobilenetv3, MixNet, GhostNet, etc. are not compared.\n5. I remember the Mobilenet was proposed in 2016. There is something wrong with the reference."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission36/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799635404,
        "cdate": 1698799635404,
        "tmdate": 1699635927453,
        "mdate": 1699635927453,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VNh2TW0gYv",
        "forum": "tItq3cwzYc",
        "replyto": "tItq3cwzYc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission36/Reviewer_YaL5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission36/Reviewer_YaL5"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on lightweighted neural network and combines Resnet with Unet to present a simple ResUnet, where the residual connection is exploited in UNet, and the classifier is designed with attention mechanism. Experiments on some datasets show the superiority and the interpretability is discussed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposal is simple and the idea is intuitive by combining the adverantage of Resnet and Unet.\n\n2. The writting clear reflects the technical idea of this work."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. First, the resnet has been used to improve the Unet, i.e., resUnet. Therefore, the idea is not novel. Second, the attention mechanism is a mature idea, and I could not learn more from this work.\n\n2. The technical contribution is incremental and motivation is not strong. A number of lightweighted networks were designed, but there lacks analysis of such networks, and why the proposed method is a better choice. Therefore, stronger motivation for each component in the proposed method is needed.\n\n3. The framework combines existing techniques, and the overall contribution is weak.\n\n4. There lacks more experiments on benchmark datasets such as Imagenet for image classification, a typical vision task. Therefore, this work contributes less on image classification."
            },
            "questions": {
                "value": "1. The presentation of Fig.4 is too intuitive comparing to Fig.1. Fig.4 seems not a special case of this work. It can also be useful for others. Therefore, this figure can be more specific for interpreting the proposed model.\n\n2. More ablation study is necessary for proving the effectiveness of each component, such as attention classifier."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission36/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699423490862,
        "cdate": 1699423490862,
        "tmdate": 1699635927372,
        "mdate": 1699635927372,
        "license": "CC BY 4.0",
        "version": 2
    }
]