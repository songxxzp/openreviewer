[
    {
        "id": "NxOeL6dOg7",
        "forum": "R3CDj2DLln",
        "replyto": "R3CDj2DLln",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the problem of predicting acoustic scenes such as types, materials, and positions of objects, and proposes a bi-directional encoder-decoder architecture for predicting acoustic scenes as well as synthesizing scene sounds from scene parameters. Since the proposed method assumes the existence of pairs of scene parameters and scene sounds as training examples, we can train both the encoder and the decoder in a supervised manner. This paper also presents a way of visualizing the prediction uncertainty of sound source locations and its application to agent navigation. Experimental evaluations with two public benchmarks demonstrate that the proposed method outperformed simple baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed architecture is interesting. If we can obtain lots of pairs of audio signals and object properties, the proposed architecture might be one of the best choices for simultaneously analyzing and synthesizing audio scenes.\n\n2. Based on the experimental evaluations, the proposed method reasonably work well for several tasks."
            },
            "weaknesses": {
                "value": "1. The task of this paper is closely related to sound source localization. If the authors justify the effectiveness of the proposed method in terms of position errors of sound sources, experimental comparisons with several previous methods for this task would be mandatory. As presented in [Grumiaux+ JASA 2021 https://arxiv.org/abs/2109.03465], lots of previous methods have already been proposed for this purpose. I understand that many of those previous methods typically require multi-channel audio inputs and the proposed method employs a single-channel or binaural audio clip. However, you will find several techniques that work well only with single-channel audio, since single-channel sound source localization has been one of the typical tasks in audio signal processing before the deep learning era.\n\n2. If we want to simply predict acoustic scenes from audio signals, we do not have to introduce decoders that generate audio signals from object properties. In this sense, the ablation study that compares the full proposed method and the one without decoders should be presented.\n\n3. If the main objective of this paper is a proposal of a novel construction of neural acoustic fields, the proposed method should be compared with the original NAFs in the task of audio synthesis."
            },
            "questions": {
                "value": "Please check the above Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698135653726,
        "cdate": 1698135653726,
        "tmdate": 1699636203094,
        "mdate": 1699636203094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HWoLCE7SeB",
        "forum": "R3CDj2DLln",
        "replyto": "R3CDj2DLln",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a disentangled acoustic fields (DAF) as a complementary in acoustic modal for physical scene understanding. The DAF could potentially help embodied agent to construct a spatial uncertainty map over where the objects may have fallen."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe main contribution of this paper is to enhance audio perception by so-called analysis-by-synthesis framework. The major strength is to maintain the (generated/synthesis one) power spectral density (PSD) consistency with the input audio.   \n2.\tThe downstream multi-modal planning experiments demonstrates the effectiveness of proposed framework."
            },
            "weaknesses": {
                "value": "1. The title \u201cphysical scene understanding\u201d could be overclaimed the contribution since the audio perception is limited to constrained scenarios (e.g., fallen objects).\n2. Though the DAFs or (the predict-generate) is novel in audio modality, it is not such innovative and is close to use the cycle-consistency to ensure robustness in vision modality. I would suggest this work more like an ICRA paper instead of ICLR."
            },
            "questions": {
                "value": "1. The title \u201cphysical scene understanding\u201d could be overclaimed the contribution since the audio perception is limited to constrained scenarios (e.g., fallen objects).\n2. Though the DAFs or (the predict-generate) is novel in audio modality, it is not such innovative and is close to use the cycle-consistency to ensure robustness in vision modality. I would suggest this work more like an ICRA paper instead of ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698459488705,
        "cdate": 1698459488705,
        "tmdate": 1699636203012,
        "mdate": 1699636203012,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "86N5OKKfYa",
        "forum": "R3CDj2DLln",
        "replyto": "R3CDj2DLln",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_KbFD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_KbFD"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel approach to multimodal physical scene understanding that leverages a disentangled acoustic field model to capture the sound generation and propagation process. This approach enables the embodied agent to construct a spatial uncertainty map over where the objects may have fallen, which can be used to improve the success rate for the localization of fallen objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Transforming the sound from the waveform domain into power spectral density (PSD) representation rather than sound reconstruction is well-motivated.\n\n+ The proposed disentangled acoustic fields (DAFs) are an interesting and technically sound model that explicitly disentangles sounds into several different acoustic factors.\n\n+ DAFs can be used to infer the physical properties of a scene, represent uncertainty, and navigate and find fallen objects."
            },
            "weaknesses": {
                "value": "+ A video demo would be very helpful for us to understand the model's performance on the localization of fallen objects in the real world.\n\n\n+ Currently, all of the experiments are conducted on synthetic datasets. It would be interesting to see how the model generalizes to real-world data.\n\n+ The proposed method requires full labels to train DAFs. It would be beneficial to develop a self-supervised learning approach to avoid using many labels during model training.\n\n+ Why not incorporate visual information into DAFs? Visual data can provide many acoustic cues."
            },
            "questions": {
                "value": "See the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699078224004,
        "cdate": 1699078224004,
        "tmdate": 1699636202930,
        "mdate": 1699636202930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1jHE9ggn7Y",
        "forum": "R3CDj2DLln",
        "replyto": "R3CDj2DLln",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_oZCW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2633/Reviewer_oZCW"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of multimodal physical scene understanding to infer the fallen objects' properties, direction and distance of an impact sound source. To deal with the limitation of current work NAFs which only captures the structure and material properties of a scene. They propose the disentangled acoustic fields to model acoustic properties across a multitude of different scenes. However, the design of DAF is incremntal to NAFs and the experiments are using only two different scenarios, kitchen and study room, which may not prove the generalization of DAF."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem is interesting."
            },
            "weaknesses": {
                "value": "1. The proposed work lacks novelty. The main contribution of this work is the introduction of DAFs, which are applied to the generative frameworks and multitask learning to address this task. Although it shows promising performance, it lacks some degree of innovation.\n2. The paper mentions that NAFs lack generalization ability for new scenarios, while DAFs can effectively solve this problem. However, no relevant comparative experiments are observed in the experimental section.\n3. The paper mentions that DAFs can address the generalization issue for different new scenarios, but the experimental section only demonstrates generalization for two different scenarios, kitchen and study room."
            },
            "questions": {
                "value": "The authors claim that their method is generalizable, why the experiments are using only two different scenarios, kitchen and study room?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699545711006,
        "cdate": 1699545711006,
        "tmdate": 1699636202841,
        "mdate": 1699636202841,
        "license": "CC BY 4.0",
        "version": 2
    }
]