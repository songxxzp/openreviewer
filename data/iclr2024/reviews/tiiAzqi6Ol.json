[
    {
        "id": "zD3fXvJx6X",
        "forum": "tiiAzqi6Ol",
        "replyto": "tiiAzqi6Ol",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_dDjG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_dDjG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \u2018compositional preference models\u2019 (CPMs) that re meant to overcome issues of transparency and scalability found in regular preference models. CPMs decompose preferences into interpretable features, and subsequently aggregates them (with LR). Generally, results show improvement in generalization and avoidance of over-optimization, which are not themselves \u2019transparency and scalability\u2019."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Although a minor strength, the collection of papers in sections 1, 2, and 6 are sufficient, relatively recent, and relevant.\n- The compositional extension, though simple, is technically a novel contribution that appears to provide several benefits.\n- The setup for best-of-$n$ sampling seems fair. The full set of 25.6K responses, and code, would be appreciated.\n- the use of a separate LLM for evaluation is appreciated."
            },
            "weaknesses": {
                "value": "- Although somewhat minor, the use of logistic regression will naturally cause some confusion, especially to those who want an end-to-end trainable model for this task. Other models should have been attempted.\n- Section 3.1 should be more informative as to the nature of features _c_ and how their set is identified, selected, or defined. This should include both the specific list in Sec 4.1 as well as guidance for other tasks, in general.\n- Although another minor weakness, at least one other LLM  should have been used for extraction (e.g., one of the Llamas)\n- Very minor, but please correct the various issues with your references including capitalization and completeness (e.g., Amodei suffers from both \u2014 use brackets around {AI} and provide full paper details)"
            },
            "questions": {
                "value": "- The definition of \u2018model robustness\u2019 in Sec 4.2 seems incomplete \u2014 surely a factor is the domain or scenario in which the model is to be deployed or evaluated, too?\n- Would it be possible to re-arrange the columns of Fig2a and Fig2b so Standard comes left-most (first)?\\\n- Would there be value in actually performing human evaluations, despite the findings of best-of-n sampling in related work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698435322505,
        "cdate": 1698435322505,
        "tmdate": 1699636198961,
        "mdate": 1699636198961,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sURqzy2O95",
        "forum": "tiiAzqi6Ol",
        "replyto": "tiiAzqi6Ol",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Compositional Perference Models (CPMs), a new perferene model framework that decomposes one global perference assessment into several interpretable features, using a prompted LM to score these features, and finally aggregates these features together with their scores using a logistic regression classifier. Experiments show that CPMs improves generalization and robustness than standard PMs. The main contributions include: (1) new CPMs that allows more transparent supervision; and (2) better results at dimensions of model/overoptimization robustness, generalization, and perference alignment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) new CPMs that allows more transparent supervision; \n(2) better results at dimensions of model/overoptimization robustness, generalization, and perference alignment."
            },
            "weaknesses": {
                "value": "1. prefer to see detailed investigations of applying CPMs to different stages of (1) inference only (2) sft, and (3) peft.\n2. not quite clear of the scalability of the usage of current 13 features to novel langauges/tasks, further investigations are preferred."
            },
            "questions": {
                "value": "1. in Table 5 and Table 6, scores from 1 to 10 are used, and did you try other ranges such as 1 to 5, and how did you decide to use a range of 1 to 10? Also, does different features require different scopes/ranges of scores? In addition, when changing from numbers to words (bad, good, better, best...), how shall the results change?\n2. any comparison of between supervised fine-tuning (SFT) and PEFT when using the CPMs? Or, any comparison of the usage of resources under different model alignment frameworks? So, (1) inference only stage controlling, (2) sft, (3) peft, any investigations on these directions of using CPMs?\n3. page 3, there are 13 features used, any detailed analysis of overlapping or diversities among these features?or when applying your method to other languages/tasks, how shall we reuse these features or how shall we design new features (any common rules?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698451886460,
        "cdate": 1698451886460,
        "tmdate": 1699636198860,
        "mdate": 1699636198860,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J32GiOgah8",
        "forum": "tiiAzqi6Ol",
        "replyto": "tiiAzqi6Ol",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces compositional preference models (CPMs), a new method for aligning language models (LMs) with human preferences. CPMs break down preference scores into multiple features to improve robustness and generalization. This decomposition is accomplished by prompting an LM to assign a value to the answer based on a specific preference type. Experimental findings demonstrate that CPMs effectively mitigate overoptimization in preference modeling."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Modeling human preferences from different types of judgments is a promising research topic.\n2. Experimental results demonstrate that the suggested CPMs indeed improve both robustness and generation.\n3. The paper is generally easy to read."
            },
            "weaknesses": {
                "value": "1. Although CPMs offer a practical method for breaking down preferences by stimulating LMs, I consider it too simplistic and unrealistic to capture intricate human preferences. For instance, easy-to-understand answers and answers with enough details may contradict each other. I have reservations about whether logistic regressors can accurately represent this intricate pattern.\n2. In terms of the experimental setup, CPMs prompt much more than standard PM, which raises concerns about their comparability. I recommend that the author include another standard PM baseline that uses a similar prompt budget as CPMs. For instance, prompting the LM $n$ times (where $n$ represents the number of pre-defined preference types for CPMs) through sampling and selecting the final preference score via majority voting."
            },
            "questions": {
                "value": "1. How is the encoder for $x$ parameterized in logistic regression?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683758093,
        "cdate": 1698683758093,
        "tmdate": 1699636198783,
        "mdate": 1699636198783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "noJCULf2IK",
        "forum": "tiiAzqi6Ol",
        "replyto": "tiiAzqi6Ol",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a compositional preference model for LLM alignment. In contrast to standard monolithic preference models that assign a single scalar value to preference judgments. the model uses a number of features associated with individual scalar values (assigned by an LLM as an automatic evaluator) that are then linearly combined into an overall score.  The authors argue that this provides an inductive bias to the model that makes it more robust to overfitting and reward hacking and results in better generalization and human interpretability. The technique is evaluated with respect to consistency of responses for models trained on different subsets of the training data), comparison against reference PMs from the literature, robustness to overoptimization, and alignment of LLMs trained with the proposed model as opposed to a standard PM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The core idea of the paper is simple yet powerful and addresses known weaknesses in traditional monolithic preference models; it should be of broad interest to the ICLR audience. The presentation is clear and -- with the exception of human alignment evaluation (see below) -- the evaluations are convincing."
            },
            "weaknesses": {
                "value": "For alignment with human preferences, another  LLM (Claude-2) was used rather than genuine human ratings. Although there is more effort associated with a human evaluation study,  and the literature you cite has shown some (imperfect) degree of correlation between human ratings and LLM scores, I really consider human evaluation a must here - otherwise, you are measuring alignment between different LLMs, which can simply result from similar training procedures or similar preference models used."
            },
            "questions": {
                "value": "1. It looks like the feature evaluator LLMs (Flat-T5 and GPT3.5) were used out of the box with prompting for assigning feature scores, without fine-tuning or in-context learning. I would have like to see the comparison against fine-tuned versions for each feature. \n2. How does the robustness of the CPM change with an increasingly larger list of features?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772465933,
        "cdate": 1698772465933,
        "tmdate": 1699636198706,
        "mdate": 1699636198706,
        "license": "CC BY 4.0",
        "version": 2
    }
]