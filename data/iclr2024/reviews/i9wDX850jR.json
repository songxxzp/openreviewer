[
    {
        "id": "74JPzNimzn",
        "forum": "i9wDX850jR",
        "replyto": "i9wDX850jR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_hFRW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_hFRW"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a mathematical framework to explain why some specific algorithms are implemented inside neural networks that were trained on some algebraic tasks. Firstly, they use the result from Wei et al. (2019a) to argue that minimizers of the $L_r$ regularized cross-entropy loss have a maximal margin. Using the proof blueprint introduced in Section 3, they can characterize precisely the internal representations learned when a specific architecture is trained on a specific task. They consider three concrete algebraic tasks: Modular addition, sparse parity and finite groups with unitary, irreducible and real representations. The theoretical findings for these three examples are confirmed in controlled experiments.\n\nSummary of my review: I think the results presented in this work are very interesting and form an important step toward understanding why specific algorithms emerge in neural networks. I found the paper challenging to read, especially Section 3. Part of it might be attributable to the fact that I\u2019m not an expert in this field, but I nevertheless believe that the writing could be improved (see the concrete suggestion I made below). For this reason, I give a score of 6."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The framing of the paper is good. I very much like the direction of explaining mathematically why some specific \u201calgorithms\u201d emerge when some architectures are trained on some dataset. I think this is important.\n- I found the theoretical results very interesting and surprising. They form an important step towards the stated goal.\n- The empirical validation for each special case was convincing.\n- I am not well versed in this area so it is very difficult for me to assess novelty."
            },
            "weaknesses": {
                "value": "- I noticed a few times that notations/terms were introduced without proper definitions. Maybe these are well-known in the field, but I think they should be restated for outsiders like me. I also found that some things should be deduced from context and would gain in being stated more explicitly. A few examples:\n    - In Section 2, it would be helpful to state that the output of the neural network is d-dimensional somewhere. Same for the neuron. I am used to having neurons outputting a scalar, but here $\\phi$ is $\\mathbb{R}^d$-valued, right?\n    - Section 3: $\\Delta(\\mathcal{Y})$ designate the set of functions $f: \\mathcal{Y} \\rightarrow [0,1]$ such that $\\sum_{y \\in \\mathcal{Y}}f(y)$, right? Might be worth making it more explicit. \n    - Might be worth explicitly saying: $\\text{spt}(q)$ denotes the support of $q$ in C.1.\n    - Section 6.1: $R_n$ for $n=1\u2026K$ are not defined.\n- I appreciate that the authors made an effort to provide details about the proofs in the main text, especially in Section 3 which provides blueprints for all results. That being said, I found this section very difficult to digest (more specifically, everything that comes after Theorem 1).\n    - Something that would help me is to state the quantifiers (like \u201cfor all\u201d and \u201cthere exists\u201d) more explicitly. For example, does the first implication of Lemma 2 hold for all $\\theta^*$ and $q^*$?\n    - How strong are conditions C1, C2, C3? I\u2019m not sure I could summarize what they mean after rereading multiple times. I think my confusion is related to quantifiers again.\n    - Lemma 3: Under \u201cSingle neuron optimization\u201d, what is a \u201cdirectional support\u201d? Under \u201cUsing multiple neurons\u201d, I\u2019m confused about the quantifier for the $\\lambda_i$\u2019s. Is it \u201cfor all\u201d or \u201cthere exists\u201d?\n- I think it would be better if the specifics of the architectures used for training in each special cases (Sections 4, 5, 6) were given in the relevant sections themselves instead of before the rather long and technical Section 3.\n- Proof outline of Theorem 4: The part about the constrained optimization leading to sparsity in Fourier space went over my head. Where does this constraint come from? The norm constraint in the max-margin problem? I understand this is a proof sketch, but I think this could be made more transparent without giving all the details. \n\nMinor:\n- $\\\\{u,v,w\\\\} \\in \\mathbb{R}^d$ is unclear. Shouldn\u2019t it be $u,v,w \\in \\mathbb{R}^d$?\n- Proof outline of Theorem 4: I thought the notation $\\mathbb{E}_{a,b}[\\psi\u2019(u,v,w)]$ to be a bit confusing and \u201ctoo different\u201d from the way it was introduced in Section 3.1."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2809/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698092380794,
        "cdate": 1698092380794,
        "tmdate": 1699636223616,
        "mdate": 1699636223616,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gcwOYHCN97",
        "forum": "i9wDX850jR",
        "replyto": "i9wDX850jR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_xfNV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_xfNV"
        ],
        "content": {
            "summary": {
                "value": "The paper shows a 2-layer NN with homogeneous polynomial activations can learn modular addition, parity, and finite group operation by max-margin with SGD. For modular addition, the max-margin solution indeed utilizes Fourier features and cover all frequency. Thus, they show an implicit bias and argue that NN prefers to learn Fourier-based circuits, rather than other potential circuits capable of executing the same function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has two contributions. \n- Generally speaking, it is hard to find all max-margin solutions for general data distributions and general networks. The paper introduces a general framework to analyze the homogeneous NN max margin solution under some distribution with symmetric properties. By Lemma 2 and Lemma 3, we can solve network-level optimal solutions by solving neuron-level search. Thus, we can simplify the optimization problem from a max-min problem over margin functions to a max problem over linear combinations of neurons. As far as I know, this method is novel and useful. It may be powerful to help the NN loss landscape. Also, see the related question in Questions.\n- The paper studies three case studies by utilizing the above framework to show how powerful it is. In particular, for the module addition part, the observation about the max-margin solution being Fourier features and covering all frequencies is insightful. \n\nThe writing is good and the motivation is clear."
            },
            "weaknesses": {
                "value": "- The paper asked, \u201cWhy does the network **consistently** prefer such Fourier-based circuits, amidst other potential circuits capable of executing the same function?\u201d in the introduction section. However, it seems when learning modular addition the NN prefers Fourier-based circuits, while learning parity or finite group operation may not (correct me if I am wrong). Thus, it seems to mislead the reader's expectations. Moreover, it would be good to discuss more here about other potential circuits. (1) For modular addition, what are else potential circuits? Some discussion and comparison would be good. (2) Is there any other data distribution that NN prefers to learn other potential circuits rather than Fourier-based circuits? Some more case studies or illustrations would be good. \n- The paper can only handle homogeneous functions as it needs to use the conclusion from [1]. Then, the neural network cannot have the bias terms. This is restricted. For example, Theorem 5 needs $\\Omega(2^k)$ number of neurons to learn sparse parity, while a 2-layer ReLU NN with bias only needs $\\Omega(poly(k))$ neurons to learn structured sparse parity [2,3]. This weakness may be beyond the scope of this paper but it is important.\n- As the conclusion section mentioned, the paper needs some properties about homogeneous so that it can only handle $L_{2, \\nu}$ norm regularization. It would be good to extend to more general settings. \n\n[1] Kaifeng Lyu and Jian Li. Gradient Descent Maximizes the Margin of Homogeneous Neural Networks. ICLR 2020.\n\n[2] Amit Daniely and Eran Malach. Learning parities with neural networks. NeurIPS 2020.\n\n[3] Zhenmei Shi, Junyi Wei, and Yingyu Liang. Provable Guarantees for Neural Networks via Gradient Feature Learning. NeurIPS 2023."
            },
            "questions": {
                "value": "From my perspective, Lemma 2 works for any homogeneous parametric function while Lemma 3 only works for homogeneous 2-layer NN. If my understanding is correct, can we generalize Lemma 3 to any homogeneous parametric function? If not, what is the problem preventing us?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2809/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698368042525,
        "cdate": 1698368042525,
        "tmdate": 1699636223545,
        "mdate": 1699636223545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P1CjBgcaaf",
        "forum": "i9wDX850jR",
        "replyto": "i9wDX850jR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_jPZB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_jPZB"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a theoretical development to analyze why a neural network with one hidden layer when trained on some algebraic tasks using SGD reliably and consistently decomposes the problem as a Fourier type of decomposition.\n\nThe authors focus on the max-margin classifiers and present a the conditions and a construction from single to many neurons for the fourier representation. The authors show the margin required for the decomposition and show how with a quadratic activation the neurons achieve single frequencies.\n\nThe authors show empirically what their proofs demonstrate."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is technically sound and well written. Although it is very specialized and not easy to read for the general practitioner of deep learning.\nThe empirical evaluation confirms the behavior at maximum margin conditions and demonstrates as well what the theory predicted."
            },
            "weaknesses": {
                "value": "The paper specializes on a particular phenomena and on a restricted architecture. It is great that they found a connection on why this representation works, but it is not clear how this could extend to other architectures or use cases."
            },
            "questions": {
                "value": "I had two questions. The first one is regarding what you mention regarding the optimizer. Is this a behavior that would only be observed with SGD? I can imagine that coordinate descent would achieve the same results if one things about the construction from single neurons to whole network. But I can imagine other optimizers also behaving well and this behavior being more a characteristic of the loss function and the activation function?\n\nThe second one is, this paper reminds me of the positional encoding of language models. You are showing that the network can decompose groups via frequency based representations. I'm wondering if the same is not indirectly happening at the first layers of a language model, i.e., whether this behavior that you observe could happen also at the level of layers and not just over the wide network you constructed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2809/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811004544,
        "cdate": 1698811004544,
        "tmdate": 1699636223451,
        "mdate": 1699636223451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5cTpRrwo6g",
        "forum": "i9wDX850jR",
        "replyto": "i9wDX850jR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_CYd2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2809/Reviewer_CYd2"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the optimal representation for a feedforward neural network that allows for perfect generalization on arithmetic tasks such as modular addition, parity, and other finite group operations. It falls under the recently established field of mechanistic interpretability. The theoretical result shows that a well-trained model uses Fourier features to perform modular addition and the learned features correspond to an irreducible group representation of the input data. Previous works discover that the learned model leverages a particular algorithm based on Fourier features to perform modular arithmetic; this work offers a partial explanation as to why such a strategy is learned, in particular why Fourier features are preferred. \n\nThe first theoretical result defines a class-weighted margin and studies a one-hidden-layer neural network with squared activation trained with cross-entropy loss and norm regularization.  First,  the paper states 3 conditions for the data and margin. Under these conditions, the paper establishes a reduction from the maximization of the margin to the maximization of the class-weighted margin (lemma 2). Then, the paper establishes a reduction for full parameter search to the search of individual neuron $\\omega_i$. \n\nThe second theoretical result demonstrates that under these assumptions, the max-margin solution for modular addition on the cyclic group has an analytical max margin and its features are the Fourier features (Theorem 4). Similarly,  the max-margin solution for sparse parity also has an analytical max margin and different features. Finally, the paper presents a general recipe for tha max-margins solution for group composition (Theorem 6).\nhe c\nExperiments show that the margins of the model indeed reach the value predicted by the theory, thus validating the theoretical results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper attempts to tackle an important problem in mechanistic interpretability on synthetic arithmetic tasks. The original findings of Nanda et al were very surprising. This paper is able to shed some light on the fact that the seemingly bizarre algorithm in Nanda et al. is not so strange after all. More importantly, the paper provides a general recipe beyond those studies in Nanda et al., which I find to be technically interesting. The empirical results match the prediction of the theorem extremely well (almost exactly) so even though I have not checked the proof thoroughly, I am reasonably confident that the technical results are correct. Overall, I think this paper is a nice addition to the emergent field of mechanistic interpretability."
            },
            "weaknesses": {
                "value": "While this paper presents some interesting results, I feel that there is still a somewhat big gap between the theoretical results and the reality. I believe in general it is not clear how studying simple task like this will lead us to deeper understanding \n\n- The setting studied in the paper is an extremely simple MLP that is very different from the one studied by Nanda et al.\n- The results and notations in the paper are somewhat difficult to follow (see some of my questions below). I feel the paper's clarity would benefit from having a more high-level summary of each of the theoretical results and assumptions and their intuitive implications. For example, I find the assumptions C.1-C.3 difficult to follow. It may also be a good idea to have a visual illustration of some of these results.\n- If I understand correctly, it seems like the training is done on all of the data in the support so there is no generalization needed and all analysis is done on the representation, so it does not address one of the most shocking observations in these arithmetic tasks, namely, grokking.\n- There is no optimization analysis so the whole picture is not complete (although we do see that the converged result matches the prediction)."
            },
            "questions": {
                "value": "- What is the relationship between $u,v,w$ and $\\omega$? Concatenation?\n- What is $P(D)$ in Sec 3? Is it an arbitrary distribution?\n- Where does $\\tau(x,y)$ in Sec 3 come from?\n- I'm not sure if I understand C1. How can the support of the distribution be the subset of argmin which is presumably a single point or a set of discrete points ($\\text{sqt}(q^*)\\subseteq \\text{argmin}_{(x,y) \\in D} g(\\theta^*, x, y)$)? Does that mean $q^*$ is a discrete distribution on the $\\mathbb{R}^n$?\n- One impression I get from reading this paper is that the provided construction actually seems to be the only sensible solution given the constraints of the function class. Is this a correct assessment? This is an important statement because it is somewhat similar to saying that we already assume the answer by making all the assumptions, which further undermines these frameworks' ability to help us understand real LLMs. If not,  what other circuits are possible beyond the constructions in the paper? How do we know this is not the only way to do it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2809/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699063502463,
        "cdate": 1699063502463,
        "tmdate": 1699636223374,
        "mdate": 1699636223374,
        "license": "CC BY 4.0",
        "version": 2
    }
]