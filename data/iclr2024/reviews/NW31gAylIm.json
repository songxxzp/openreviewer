[
    {
        "id": "Y2gKRq19EU",
        "forum": "NW31gAylIm",
        "replyto": "NW31gAylIm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_RTLu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_RTLu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a Federated Text-driven Prompt Generation (FedTPG) network to generalize the classification ability of vision-language models to unseen classes. The prompt generation network is conditioned on task-related text input, which makes it suitable to generalize for both seen and unseen classes. The experimental results on 9 datasets demonstrate that the proposed FedTPG achieves overall better generalization on both seen and unseen classes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The structure and the content of this paper are complete. \n\nThe formulation is clear enough. \n\nThe proposed FedTPG is easy to follow."
            },
            "weaknesses": {
                "value": "1. The applicable scenario of the proposed method is unclear. What scenarios require training vision-language models under the framework of federated learning? And test on unknown datasets?\n\n2. Many existing works have studied generative prompt learning methods [1] [2] [3] and cross-modal prompt learning methods [4] [5] [6]. What is the difference between the proposed FedTPG and these methods? Why the proposed FedTPG can generalize to unknown datasets but the existing methods mentioned above cannot?\n\n3. More comparison experiments with existing prompt learning methods [1-6] should be included to verify the effectiveness of the proposed FedTPG.\n\n4. In Table I, the proposed FedTPG did not achieve the best performance on 9 datasets while under Local Base and HM settings. Does this mean that FedTPG sacrifices the classification performance of known categories when considering unknown categories? Therefore, the contribution of this article is difficult to evaluate.\n\n5. The prompt generator network proposed in this article is based on a simple attention mechanism, which is too simple. Using such a network, why can the generated prompt vectors be generalized to unknown categories?\n\n[1] Improving Zero-shot Generalization and Robustness of Multi-modal Models\n[2] Improving Zero-Shot Generalization for CLIP with Synthesized Prompts\n[3] SuS-X: Training-Free Name-Only Transfer of Vision-Language Models\n[4] Class-aware visual prompt tuning for vision-language pre-trained model\n[5] Prompt learning with optimal transport for vision-language models\n[6] A Retrospect to Multi-prompt Learning across Vision and Language"
            },
            "questions": {
                "value": "1. What scenarios require training vision-language models under the framework of federated learning?\n2. What is the difference between the proposed FedTPG with existing generative prompt learning methods and cross-modal prompt learning methods?\n3. Why can the generated prompt vectors be generalized to unknown categories using a simple attention network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697963767421,
        "cdate": 1697963767421,
        "tmdate": 1699636368774,
        "mdate": 1699636368774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lcUPIFee39",
        "forum": "NW31gAylIm",
        "replyto": "NW31gAylIm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_qkce"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_qkce"
        ],
        "content": {
            "summary": {
                "value": "Prompt learning for vision-language models has achieved remarkable success in the adaptation of CLIP to different downstream tasks. This paper considers apply this approach to the field of federated learning. Specifically, they present Federated Text-driven Prompt Generation (FedTPG), which enables the scalable learning of a unified prompt generation network across multiple remote clients. Since the prompt generation network is conditioned on task-related text input, it is context-aware and suitable for generalization to both seen and unseen classes. The comprehensive empirical evaluations on nine diverse image classification datasets demonstrate that the proposed method outperforms existing federated prompt learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis is the first paper that solves the challenge of using prompt learning in FL for unseen classes, which is of great importance. \n2.\tThe idea of adopting a prompt generator for context-aware tasks is novel.\n3.\tThe experimental results demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tThe computational cost is high. Although the CLIP pre-trained model is fixed during local training, it still requires backward propagation over the encoder. Considering the clients may be edge devices with limited resources, this may hinder the wide adoption of the proposed method.\n2.\tThe generator is more likely designed for the centralized settings instead of the decentralized settings. It appears to be a simple prompt strategy in the centralized setting. Distributed properties and NonIID data distribution are not well taken into account. As a consequence, many existing prompt learning strategies for centralized settings may also be directly adopted and should be compared."
            },
            "questions": {
                "value": "no"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Reviewer_qkce"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698497326066,
        "cdate": 1698497326066,
        "tmdate": 1699636368676,
        "mdate": 1699636368676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MVWQGm4uq8",
        "forum": "NW31gAylIm",
        "replyto": "NW31gAylIm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_o1Q3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_o1Q3"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a Federated prompt tuning method, where a unified prompt generation network shared by all clients are proposed to generate specific prompt for each client based on the dataset context. The experiments show that the proposed method outperforms existing federated prompt tuning methods on both based-to-new and dataset generalization task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of a context-aware prompt generation network is reasonable. \n2. The experiments show that the proposed methods outperform existing federated prompt tuning methods."
            },
            "weaknesses": {
                "value": "1. Can the author provide a more detailed analysis on the different between the proposed method and CoCoOp, since it also introduces an extra prompt generalization network. \n2. Like CoCoOp, the image may also provide important context informatio. How is the performance of proposed model compared to a model taking image as input? Will using both visual and textual information as context further improve the performance?\n3. The proposed method does not achieve the state-of-the-art performance on 3 out of 8 datasets in terms of the base-to-new task. Can the authors provide a more detailed analysis on the characteristics of these 3 datasets and why does the proposed method performs relatively worse on these datasets?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Reviewer_o1Q3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652078379,
        "cdate": 1698652078379,
        "tmdate": 1699636368547,
        "mdate": 1699636368547,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "umLWys6yUr",
        "forum": "NW31gAylIm",
        "replyto": "NW31gAylIm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_KFdC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4050/Reviewer_KFdC"
        ],
        "content": {
            "summary": {
                "value": "Prompt learning for vision-language models has shown promising results in adapting CLIP to different downstream tasks. The paper proposes to improve the generalizability of the learn prompts to unseen classes, by proposing to solve this problem in a federated learning setting. Specifically, the prompt generation network is conditioned on task-related text embeddings, making the generation process context-aware and generalizes to the unseen classes. The paper evaluates the proposed approach on a diverse set of 9 datasets, as well as different ImageNet variants and demonstrates better generalization to unseen classes and domains than the baseline CLIP and CoOp."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The generalization of the prompt tuning vision-language models is an important research topic, and the federated learning setting is interesting.\n- The proposed technique effectively improves the generalizability of the learned prompts.\n- The paper is well written."
            },
            "weaknesses": {
                "value": "- What is the reason of not comparing to CoOp on EuroSAT/ImageNet in Table 1? This seems to be the setting followed by many works along this direction.\n\n- What is the reason of not comparing to CoCoOp [1] and MaPLe [2] in the base-to-new generalization setting? Will the proposed technique have the same benefit if naively applied to Fed-CoCoOp / Fed-MaPLe?\n\n- Why are the baseline numbers of CLIP/CoOp different between Table 1 of this paper and CoCoOp Table 1? Is it because the different selection of the base class and why?\n\n[1] Zhou et al. \"Conditional prompt learning for vision-language models.\" CVPR 2022.\n[2] Khattak et al. \"Maple: Multi-modal prompt learning.\" CVPR 2023."
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4050/Reviewer_KFdC"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698953754300,
        "cdate": 1698953754300,
        "tmdate": 1700718460454,
        "mdate": 1700718460454,
        "license": "CC BY 4.0",
        "version": 2
    }
]