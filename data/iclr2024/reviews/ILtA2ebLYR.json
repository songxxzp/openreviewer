[
    {
        "id": "HqMWniLrAQ",
        "forum": "ILtA2ebLYR",
        "replyto": "ILtA2ebLYR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_5t57"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_5t57"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an algorithm for learning based on pairwise preferences in a dueling bandit setting. The proposed method also includes a module for determining the query times, at which the user is queried for preference feedback. The algorithm is then evaluated empirical using single and multi-objective test functions and the sushi domain. It is also applied to a protein structure prediction problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Most preferential optimization approaches control the query times via batch-size hyperparameters or do not consider this problem at all. Therefore, this is an interesting addition. Furthermore, the number of experiments is quite high, spanning everything from very simple to real-world problems, including a diverse range of comparison algorithms."
            },
            "weaknesses": {
                "value": "Most importantly, the work lacks clarity:\n- $\\hat{P}$ denotes the predicted preference matrix, but there is no prediction step described. According to Eq. 4, it seems to be the observed preferences.\n- Unclear how $P$, $\\mathbf{w}$ and $\\hat{P}$ are related.\n- $P$ and $\\hat{P}$ are formalized as probability matrix, but \"user preferences are consistent and stationary\", which is even used in the Algorithm 1 by reusing previous results. Resultingly $p_{ij}$ is an indictor function?\n- Eq. 6 is defined over the matrix $\\hat{P}$, but used with scalar values in Eq.5 ?\n- Eq. 6 depends on $P$, but it is not clear how this is obtained.\n- $L_t$ denotes the predicted preference distribution. How is this different from $\\hat{P}$ and $\\mathbf{w}$?\n- $L$ is used as symbol for the loss as well as the preference distribution\n- $p_min$ is never described and only defined within Algorithm 1\n- Fitness is never introduced, but is likely to be assumes as a function of $\\mathbf{F}(x)$. However, it seems to be also assumed that this function is linear, as Section 2.3 mentions \"weight vectors\"?\n- Unclear how preferences are generated, but it does not seem to be possible to employ arbitrary generation processes, as a scalar fitness function is not able to result in cyclic preferences.\n- Virtual fitness function not formally introduced\n\nThis also relates to how the bandit algorithm is embedded into the EA methods. \n- It seems the bandit algorithm is used to evaluate candidate solutions, but in how far is the EA algorithm still working with the multi-objective targets?\n- What is meant with \"runs as normal\"? \n- What is optimized in between the query times? \n- Section 2.3 mentions \"assigning a fitness value\" and \"storing weight vectors\". This suggest, the preferences are used to learn some feedback function that can be used to evaluate candidates. How does this differ from learning a fitness function?\n- Unclear how \"rounds\" are related to the budget, timesteps and queries.\n\nFurthermore, the experiment section also needs some additional polish:\n- Results are not reproducible, as several hyperparameters are not mentioned, especially concerning the EA algorithms.\n- Fig.3: How is it possible that RUCB-AL is outperforming RUCB after a single feedback round?\n- \"Our proposed method achieves the minimum mean in 8 instances\" - This only seems true if one is able to optimize the EA algorithm used in conjunction with RUCB-AL. Of course, in practice this is not really possible. Under a fair comparison (preselected EA algorithm), IEMO/D seems to be the best?\n- Table 1 hardly readable because of the number format. Maybe use relative values (e.g. relative difference to the optimum).\n- Sec 3.4 not meaningful without the appendix, as one can not evaluate if the results are good or bad.\n- PSP trials do not mentioned used budget, rounds, etc.\n\nAdditionally, the work is not sufficiently considering related work. E.g. \"Preference-based Multiobjective Evolutionary Algorithm for Power Network Reconfiguration\" or \"A Preference Based Interactive Evolutionary Algorithm for Multi-objective Optimization\". It should also be mentioned, that a vast number of query selection strategies already exist (e.g. see Bengs 2021), that aim at reducing the number of queries to be performed. Why should this not be considered \"active learning\"? \n\nIn conclusion, besides the clarity issues, two of the claims are not sufficiently substantiated:\n- The approach does not seem to be fitness free, as Sec. 2.3 mentions something resembling learned functions.\n- The difference between the \"AL\" part of algorithm and other query selection strategies."
            },
            "questions": {
                "value": "See weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698394529864,
        "cdate": 1698394529864,
        "tmdate": 1699636341591,
        "mdate": 1699636341591,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XDKdubkjXK",
        "forum": "ILtA2ebLYR",
        "replyto": "ILtA2ebLYR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework that firstly learns the objective of human users by dueling bandits, and then the learned objective can be optimized by some zero-th order optimization methods. Specifically, the authors first propose an active dueling bandits algorithm, in which the feedback obtained from the comparisons is used to update the preference probability estimates for each solution, which are then used to construct the user-specific objective function. Based on the objective funciton, the second step is to use the learned objective function to guide the optimization process in a multi-objective evolutionary algorithm. The EA uses the learned objective function to evaluate the fitness of candidate solutions and to guide the search towards the region of interest along the Pareto frontier."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem this paper concerns is clearly defined, and all formalizations are clean and easy to follow\n\n- The paper is well-organized with clear writing.\n\n- Real-world application on protein structure prediction is conducted, which may be closely related to AI for science."
            },
            "weaknesses": {
                "value": "- My major concern is the novelty of the proposed framwork, as learning objective functions according to human feedback is not new[1][2][3].\n\nRef.\n\n[1] Roijers, Diederik M., Luisa M. Zintgraf, and Ann Now\u00e9. \"Interactive thompson sampling for multi-objective multi-armed bandits.\" Algorithmic Decision Theory: 5th International Conference, ADT 2017\n[2] Ding, Yao-Xiang, and Zhi-Hua Zhou. \"Preference based adaptation for learning objectives.\" Advances in Neural Information Processing Systems 31 (2018).\n[3] Tucker, Maegan, et al. \"Preference-based learning for exoskeleton gait optimization.\" 2020 IEEE international conference on robotics and automation (ICRA). IEEE, 2020.\n\n- The proposed active dueling bandits algorithm is confusing. In traditional active learning, there should be an uncertainty measure, according to which the learner decides whether to query; in active dueling bandits proposed in this paper, if I'm getting it right, whether to query if sorely focusing on if the pair is compared before, which is a noisy feedback that is not trustworthy. I'm not saying it's wrong, considering the Copeland Winner's assumption that the pairwise winning probability is not independent but with some internal structure that can be used, but it should be discussed why such active query mechanism is reasonable under the Copeland setting. Besides, the regret if RUCB-AL is linear of T, which means it does not converge to the Copeland winner."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Reviewer_ch6a"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698425270858,
        "cdate": 1698425270858,
        "tmdate": 1699636341509,
        "mdate": 1699636341509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PFxLfBUgfc",
        "forum": "ILtA2ebLYR",
        "replyto": "ILtA2ebLYR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of optimizing multi-objectives.  Instead of calculating the fitness function, authors leverage a human preference-type algorithm, motivated by dueling bandits, to solve their problem. The authors further apply their algorithm to practical applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors propose a novel framework to return the optimized objective by using a dueling bandit framework. This motivation is great since, in reality, human's preferences are easier to obtain than a real-valued reward.\n\n2. The authors derived theoretical guarantee for the dueling bandit algorithm and applied it to protein structure prediction."
            },
            "weaknesses": {
                "value": "Main weakness:\n\n1. The result of the theories seems weak. The authors derive this algorithm in an online version, but the regret order is O(T), which is too large and not efficient.\n\n2. Weak connections between the dueling bandit algorithm and the main goal of the paper. It seems only this regret is analyzed instead of how well it optimizes the objectives, which are not discussed in a theoretical manner.\n\n3. Lack of comparison with benchmarks from both dueling bandit and the main optimization objective in the paper"
            },
            "questions": {
                "value": "1. The authors provide a regret for the proposed algorithm with order $O(T)$. This result seems too weak. Could the author further improve this bound? Also, could the authors compare this with some existing benchmarks? Since there are many algorithms that are available in dueling bandit area that can achieve $O(\\sqrt{T})$ upper bound\n\n2. When implementing the dueling bandit, it seems the greed algorithm was implemented. What if we consider an algorithm with an upper confidence interval? Would this give a better regret bound?\n\n3. There is no clear and explicit connection between the dueling bandit algorithm and the key objective of this paper, could the authors provide some theoretical guarantees on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3837/Reviewer_NX1i"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594693337,
        "cdate": 1698594693337,
        "tmdate": 1699636341432,
        "mdate": 1699636341432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mzit7cNv2x",
        "forum": "ILtA2ebLYR",
        "replyto": "ILtA2ebLYR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_Xsp2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3837/Reviewer_Xsp2"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to guide evolutionary algorithms via human preference feedback, so that there is no need to design the fitness function. The authors propose to achieve this by adopting a modified version of the dueling bandit algorithm, RUCB (Zoghi et al. 2014)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea of combining dueling bandits and evolutionary algorithms is novel and well-motivated, as it naturally allows using human preference feedback to guide evolution."
            },
            "weaknesses": {
                "value": "1. The presentation of this paper needs significant improvement. Currently, the problem setting, the objective, and the description of the proposed \"human-dominated preference-based EMO\" are not explained clearly, with several important details being vague/missing.\n\nFor example, there lacks a clear and rigorous formulation of the problem illustrated in Figure 1. The author only provided formulation for the \"consultation module\" in Section 2.2.1, which explains when given a set of individuals, the goal of this module is to find the Copeland winner based on human preference feedback. However, it is not clear why we need the evolutionary algorithm to optimize Eq 1. \nI inferred from the context that, the goal of the whole pipeline is to find the optimal solution of Eq 1. But the learner cannot simply choose the solution in set $\\Omega$ to maximize function $F$, i.e., the learner cannot observe the whole set $\\Omega$. Instead, it has to rely on a evolutionary algorithm to create new solutions. Please correct me if I misunderstood anything. \n\nAlso, it is also quite confusing to me that the multi-objective in Eq 1 plays no role in the formulation of dueling bandit, i.e., how to compare two arms under multi-objective setting.\n\n2. RUCB algorithm relies on the assumption that there exists a Condorcet winner. Relaxation of this assumption, i.e., find Copeland winner instead, requires different algorithm design, like the CCB and SCB algorithm in Zoghi et al. (2015). The authors dropped the Condorcet winner assumption in Assumption 1. I would appreciate it if the authors could elaborate on how this can be achieved with the modified RUCB algorithm in Algorithm 1.\n\nZoghi, M., Karnin, Z.S., Whiteson, S. and De Rijke, M., 2015. Copeland dueling bandits. Advances in neural information processing systems, 28.\n\n3. Related to the previous comment, it is not clear to me why the bonus term of standard RUCB algorithm is modified to Eq 5. In addition, if I am not missing anything, the squared loss of $\\hat{p}\\_{i,j}$ in Eq 5 cannot be computed by the algorithm, since the value of $p_{i,j}$ is unknown?"
            },
            "questions": {
                "value": "The definition of Copeland winner in Eq 2 does not seem to be correct. Moreover, as Copeland winner is guaranteed to exist, it would be better to call this as \"Definition\" instead of \"Assumption\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3837/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698905302461,
        "cdate": 1698905302461,
        "tmdate": 1699636341335,
        "mdate": 1699636341335,
        "license": "CC BY 4.0",
        "version": 2
    }
]