[
    {
        "id": "o0PXDyZuPH",
        "forum": "y01KGvd9Bw",
        "replyto": "y01KGvd9Bw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework that unifies generation of text and images. Specifically, it utilizes a newly introduced <dream> token to encode the representations that will later be forwarded to the diffusion model to decode to images. The authors have conducted extensive experiments across multiple tasks and benchmarks to showcase the ability of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a unified and promising framework for multimodal generation, strong performances are reported. The proposed approach to integrate diffusion models with LLM seems reasonable and could inspire following works in this area."
            },
            "weaknesses": {
                "value": "The major concern I have is the necessity to utilize the token from LLM for image decoding. What is is going to be if you let the LLM to first output the image description, then extract it and feed it directly to the diffusion model? In this case, the original text encoder of diffusion models are leveraged. The results in Table 2 show that the specialists still outperform dreamLLM, which means the above naive alternative could potentially perform better? In addition, is it possible that directly use output text can also alleviate the loss of LLM's original power?"
            },
            "questions": {
                "value": "See weakness ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_jWvU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698564781079,
        "cdate": 1698564781079,
        "tmdate": 1700623905423,
        "mdate": 1700623905423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lq5NctUOaD",
        "forum": "y01KGvd9Bw",
        "replyto": "y01KGvd9Bw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework to allow multimodal large language models combining multimodal comprehension and generation. To enable image generation, instead of fitting CLIP feature, this work directly optimizes the diffusion model's objective to achieve modeling multimodal posteriors. The training pipeline is comprised of three stages: alignment pretraining, interleaved generative pretraining, and supervised finetuning. Extensive experiments on multimodal comprehension and creation have shown the superiority of the proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This model proposed a unified framework for joint multimodal comprehension and generation which shows impressive performance on various tasks, demonstrating the benefits of synergizing these two tasks.\n2. The usage of score distillation avoids the possible information loss to greatly improve the image generation ability.\n3. The proposed training pipeline enables the free-form interleaved generative ability of multimodal models.\n4. The experiments are comprehensive and convincing."
            },
            "weaknesses": {
                "value": "1. For free-form interleaved generation, it is important to ensure the consistency between related images. However, as we can view the model as replacing the CLIP text encoder of stable diffusion with a much more powerful LLM for the image generation aspect, the control of the generated image is still limited. As we can see from the Figure 3, the phones in generated samples have large discrepancy.\n2. The paper does not demonstrate the in-context comprehension ability of the model.\n3. Ablation studies on the choices, combination ways, and the importance of filtering process of the training datasets are not shown, which might provide insights for future study."
            },
            "questions": {
                "value": "Will including samples from training datasets as in-context examples improve the performance during evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1397/Reviewer_t2eT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815033306,
        "cdate": 1698815033306,
        "tmdate": 1699636067450,
        "mdate": 1699636067450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YqLz0OxUjV",
        "forum": "y01KGvd9Bw",
        "replyto": "y01KGvd9Bw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_h8i6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1397/Reviewer_h8i6"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"DREAM LLM\", a novel learning framework designed to enhance Multimodal Large Language Models (MLLMs). The model is designed to 1) The model late-fused three pretrained models, a text-LLM, an image generation model (SD), a CLIP model. Unlike conventional methods that utilize intermediate image representations, DREAM LLM leverages score distillation techniques with a diffusion image generation model inputs raw data modalities and outputs them in the same format. 2) The model is also trained on interleaved multimodal corpora sourced from the internet. \nThis leads to superior performance in several benchmarks and the ability to generate free-form interleaved content."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The model late-fused three pretrained models, a text-LLM, an image generation model (SD), a CLIP model. With the designed dream query and score distillation with SD model, the model is superior than earlier models that utilize intermediate image representation. \n2. What's more, the model is trained on carefully collected various datasets, including, various text-image datasets, interleaved multimodal corpora sourced from the internet, instruction datasets. With joint-training, it also shows synergy of text and image, understanding and creation. \n3. The model shows very comprehensive experiment results in various benchmarks like MS-COCO, MMBench, and MM-Vet, in different setup zero-shot understanding, image generation, interleaved generation, etc."
            },
            "weaknesses": {
                "value": "1. The model is trained on very rich data, and it is not clear how does those data contribute to the zero-shot evaluation. \n\na. several dataset used by the model are derived from COCO datasets, e.g Laion-COCO, LLaVaInstruct, etc. How do we know if there is data leakage in the training datasets. This applies to the results in table 1, as well as in table2. \n\nb. Is the model in table 1 after instruction tuning? \n\nc. if you continue training SDv2.1 with the collected dataset, what are the MS-COCO, and LN-COCO FID number?  \n\n2. Some model details are not clear. \n\na. how is the multi token dream query implemented. For decoder-only transformer training, every token needs a loss (or a score). What us the loss of each query token during training, and are they generated sequentially or altogether during inference?\n\nb. in the interleaved multimodal joint training, for the second/third image generation, do they condition on both the image1 dream query and image1 visual encoder? Or just image1 visual encoder. \n\nc. for the stage1, stage2, stage3, what's the final loss? Do they both have L_DM (formula 5) and L_MLLM (formula 6)? Is there a weight? \n\nd. for the I-GPT training, does the visual projector, condition projector, dream embedding get updated? or only the MLLM transformer got updated?\n\ne. in section 5.1, the L_CLIP is not clear. Which two embeddings are used to calculate the loss?"
            },
            "questions": {
                "value": "1. Can the model do k-shot learning for image understanding task?\n2. table4 shows amazing results that the model is better at text task than the pretrained text LLM, does the author have any hypothesis why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819928284,
        "cdate": 1698819928284,
        "tmdate": 1699636067380,
        "mdate": 1699636067380,
        "license": "CC BY 4.0",
        "version": 2
    }
]