[
    {
        "id": "x4Cj6ee9gW",
        "forum": "WjYNFZEjc7",
        "replyto": "WjYNFZEjc7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_QeFk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_QeFk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for analyzing the contribution of each attention head in Transformer-based speech processing models. Leveraging information bottleneck (IB) theory, which facilitates the interpretable analysis of neural network behavior by constraining mutual information, the authors apply this method to the self-attention mechanism. Specifically, the attention weight is replaced with a trainable weight matrix so that the mutual information between V and Z (V times A) can be controlled. This proposed Head Information Bottleneck (HIB) analysis allows for the computation of the importance of each attention head. By pruning less important heads based on their computed scores, the paper demonstrates a reduction in performance degradation across various speech-related tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This is the first attempt to apply IB theory to self-attention analysis. The proposed method systematically seeks to uncover the importance of self-attention in speech processing, which could help in understanding the role of self-attention.\n* The proposed method naturally aligns with IB theory. Replacing attention weight and restricting the information flow between V and Z seems to be a good approach to managing (limiting) the information flow within the self-attention module.\n* Experimental results exhibit good pruning resilience that surpasses the na\u00efve attention probability-based pruning and matches the performance of the previous AttAttr method. This empirically shows the potential utility of the proposed HIB as a useful tool."
            },
            "weaknesses": {
                "value": "* While the paper references many prior works on attention analysis, it lacks comprehensive comparative analysis. There is only one comparison with the previous work (AttAttr), and it is conducted in only a single experiment.\n* The advantage of HIB requires further clarification. There are several visualizations (Figures 3 and 4) from HIB, but there is a lack of explanation/connection between these visualizations and the actual speech data. In other words, there is no intuitive demonstration of \u2018what\u2019 this attention head is focusing on. The current version of the paper is not enough to persuade readers that HIB is successfully identifying the important heads that capture important information from the input speech.\n* I have several concerns about the assumptions in the paper. The paper may need more clear explanations/justifications on these points to persuade readers.\n    1. First, why Z and V are assumed to be normal distributions scaled to standard normal distributions, and their covariance matrices are assumed to be identity matrices?\n    2. Second, Z is a multiplication of V and Softmax(A), but the Softmax distribution is not expected to be a normal distribution; why Z is assumed to be a normal distribution?\n    3. Third and most important, why does the higher similarity between the actual attention matrix and the learned HIB attention matrix indicate the greater importance of that head?"
            },
            "questions": {
                "value": "* In Figure 3, why does increasing the beta \u2018change\u2019 the important attention weight locations? As I understand, important parts identified from the harsh condition (more limited information flow) should be preserved when the condition is relaxed (allowing more flow). However, it seems that important parts keep changing as beta changes.\n* What do you mean by \u201caccuracy of ASR task\u201d (Figure 5)? The common metric for ASR is word error rate (WER), but I am confused if the used \u2018accuracy\u2019 metric is related to WER.\n* If HIB yields similar results to AttAttr, what would be the advantage of HIB? In other words, why should researchers choose HIB instead of AttAttr for attribution analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7938/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7938/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7938/Reviewer_QeFk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697964480243,
        "cdate": 1697964480243,
        "tmdate": 1700793136359,
        "mdate": 1700793136359,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TqqgxVsUWB",
        "forum": "WjYNFZEjc7",
        "replyto": "WjYNFZEjc7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_sbP7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_sbP7"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an information-bottleneck based approach to measure the importance of attention heads in transformer models for speech processing. The paper shows that using this approach, it is possible to prune 40% of attention heads without sacrificing much accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is the first work in the literature that leverages the information bottleneck framework to understand the attention heads in transformer models for speech processing. The information theory perspective to explain the attention mechanism in transformer models is novel and interesting."
            },
            "weaknesses": {
                "value": "There are room for improvement in the presentation of this work:\n1. many symbols are not defined before used and they are not easy to interpret. For example,  \n   -  it is unclear to the reader what\"KS\" is referring to in abstract; \n   - symbol L and d in Eq (6) is never defined; \n   - the inline equation under Eq(8) is never defined -- it looks like it is a typo ? \n   - I am not sure \\beta in Eq(5) and Eq(15) is the same symbol or not ? it looks like to me they should be different symbols; if not, how Eq(5) is transferred to Eq (15) ? \n   -  Figure 2 is presented without any sentence mentioning what exactly it means \n   -  In section 4.2, the author mentions \"the impact on the dataset is 0.51% and 0.01%\". How the impact is measured here? Are 0.51% and 0.01% relative WERs or absolute WERs or something else ? \n   - Eq (16) `attrh` and `E_x` are not defined and very hard to understand. \n\n2. In section 3, the author claims that they uses a variational approximation to estimate the mutual information between Z and V; then quickly after Eq(10), they stated that \"according to the assumption that Z, V are both d-dimension normal distribution scaled to standard normal distribution, the mean vectors are zero and covariance are identity\". First of all, V is R^{Lxd} matrix not a vector; second, where this assumption is made ? Third, is it reasonable to assume V is zero-mean, identity covariance ? This sounds a very strong assumption which does not seem to be reasonable to me."
            },
            "questions": {
                "value": "It is unclear to me what the proposed model look like ? If the attention map is replaced by a learned attention matrix, does it mean that during inference, the contribution of each of the L frames to other frame are completely fixed ? Does it mean that V is always combined using a fixed weight and how this can explain the dynamics of attention heads (when the input changes, the original attention will combine V differently, while this model is always using the fixed weight, regardless of the input) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698630981616,
        "cdate": 1698630981616,
        "tmdate": 1699636974740,
        "mdate": 1699636974740,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xLdNKb0IxZ",
        "forum": "WjYNFZEjc7",
        "replyto": "WjYNFZEjc7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_zTHX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_zTHX"
        ],
        "content": {
            "summary": {
                "value": "* The paper presents a novel method to estimate the contribution of each attention head in a Transformer model. It uses the concept of information bottleneck that learns a trainable weight $A_r$ to maximize the mutual information between the random variable $Z$ and the label $Y$ while minimizing the mutual information between $Z$ and the input $X$. The similarity between the resulting head information bottleneck matrix $A_{HIB}$ and the original attention matrix $A_o$ can be used as indicator about the contributions of the attention heads.\n\n* The proposed method is the first attempt to use information bottleneck theory to analyze the roles of attention heads.\n\n* The experiment results on wav2vec 2.0 indicate the effectiveness of HIB on analyzing the contribution of attention heads and pruning attention heads on a few downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper proposes a novel information bottleneck acquisition method of the attention mechanism of Transformers, which is the first attempt to use information bottleneck theory to analyze the roles of attention heads.\n\n* The mathematical analysis of the loss function is solid and well organized, which makes the proposed method more convincing and persuasive.\n\n* Extensive experiments on wav2vec 2.0 have been conducted to evaluate the effectiveness of HIB on analyzing the contribution of attention heads and pruning attention heads on a few downstream tasks.\n\n* The paper conducts rigorous experiments on hyper-parameters, comparison with other methods and especially two pruning models, which demonstrate the effectiveness of the proposed method.\n\n* The code of the method is available, making the experiments and results can be easily reproduced."
            },
            "weaknesses": {
                "value": "* In Section 3, it is assumed that the dimensions in $Z, V, Q(Z)$ are normally and independently distributed. The authors attempt to justify the reasonableness of such assumption with reference to (Klambauer et al., 2017). However, such assumptions may not be appropriate for real-world data. And the reference (Klambauer et al., 2017) is actually missing important paper title information.\n\n* The results in Figure 5 demonstrate that AttAttr performs similar or better than the proposed method across various numbers of pruned heads. The actually suggests that the proposed method does not consistently outperforms AttAttr to a significant degree. It would be appreciated if the authors could provide more insights and analysis on this.\n\n* The results in Figure 6 indicate that while the performance of the proposed method is satisfactory for the KS task, there is a steep decline in performance even with the pruning of just 2 or 0.1 heads for the IC and SID tasks. However, the paper lacks detailed analysis and explanations regarding the reasons behind this poor performance on the IC and SID tasks."
            },
            "questions": {
                "value": "* The presence of identical letters used to represent different terms in equations can indeed lead to confusion and misleading interpretations. For example, in Equation 5, the term \u201c$I$\u201d represents the mutual information, whereas in Equation 6, the term \u201c$I$\u201d denotes the unitary matrix.\n\n* In the 6-7th lines of the paragraph under Figure 4, if it refers to \u201cLayer 12 Head 3\u201d in the Figure, and there is mention of the \u201cfist head\u201d, then it is likely a typo. The \u201cfirst head\u201d should indeed be corrected to the \u201cthird head\u201d to accurately correspond with the specific layer and head being discussed in Figure 4.\n\n* The legend entry for Figure 6 should provide clear and concise explanations of the terms \u201cattention head KS/IC/SID\u201d and \u201cHIB KS/IC/SID\u201d to avoid confusion and unclearness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698906270398,
        "cdate": 1698906270398,
        "tmdate": 1699636974638,
        "mdate": 1699636974638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TXg7bun07E",
        "forum": "WjYNFZEjc7",
        "replyto": "WjYNFZEjc7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_oLGF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7938/Reviewer_oLGF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use the information bottleneck approach to investigate the importance of an attention head. Compared with the previous studies based on gradient analysis, the method attaches the latent variable with learnable parameters with an existing transformer-based pre-trained model to design an additional loss. After training, the method can calculate the relationship between the normal attention map and the attached attention map for each head and compute the importance based on them. The effectiveness of the proposed method was evaluated based on widely-used w2v2-based Librispeech ASR experiments or other speech processing downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Besides the layer analysis, attention head analysis in the transformer architecture is fundamental. There is not so much work conducting the head-level analysis, especially in speech processing\n- The method is theoretically founded based on the information bottleneck.\n- The effectiveness is evaluated by various speech processing downstream tasks (e.g., ASR, SID, KS, and IC).\n- Good reproducibility (they use the public data and will release the source code)."
            },
            "weaknesses": {
                "value": "- Clarity: The paper does not fully explain the information bottleneck concept. For example, equation (5) introduces the core objective of this theory, but there is little information about why this objective function yields the information bottleneck and what $Z$ exactly means. I recommend you carefully explain this part more conceptually.\n- Clarity: There are missing explanations for several variables introduced in the equation, e.g., $L$, $T$, $d$, etc. Especially, I'm confused $L$ and $T$ (also $a_i$ and $a ^r _i$). Are they the same or different? Please carefully explain all variables.\n- Clarity: Generally, the experimental discussions miss the detailed configurations (see my questions), making it difficult to understand the analyses.\n- Clarity: All figures are too small.\n- Novelty and effectiveness: I could not fully find the benefits of this method compared with the conventional method, e.g., AttAttr. The performance difference is minimal. Also, although the proposed method claims that it only requires forward computation compared with AttAttr, according to the discussion in Section 3,  it seems to train the model based on the additional loss and parameters and requires backpropagation. Thus, I could not find the benefit of the proposed method."
            },
            "questions": {
                "value": "- I'm a bit confused. Section 3 says that \"this method operates during the forward propagation process of an already-trained network.\" However, it is based on loss optimization, and we still need to perform backpropagation (not for all parameters but parameters related to the information bottleneck) to realize this method. Can you clarify this part?\n- I could not fully understand what is actually trained. I could read that attention matrix $A$ is trained in the final paragraph in Section 3. However, this matrix will be variable depending on the input length, and it is difficult to train. Can you clarify it?\n- Can you clarify the experimental condition of Figure 2?\n- Which head did you use for Figure 3?\n- Section 4.2: \"When these two heads are respectively masked, the impact on the dataset is 0.51% and 0.01%, representing heads with relatively minor and major impacts on the ASR task, respectively.\" Is it about Figure 2?\n- Eq. (16) needs more explanations. I could not understand what this value means.\n- Section 4.3 \"We randomly extracted 200 samples from the Librispeech train-100\ndataset for training.\" Do 200 samples mean 200 utterances? Did you only use 200 utterances to train the model? Can you clarify it?\n- Section 4.3: How to calculate the accuracy? In ASR, it is not easy to calculate a simple accuracy, and people use the word error rate (edit distance between the reference and hypotheses) to evaluate the model performance.\n- Why didn't you compare the proposed method with AttAttr in Section 4.4?\n- Section 5: What is \"attention score?\"\n- Can you apply this method to the cross attention? The cross attention would often be more important to interpret the relationship between input and output in an encoder-decoder architecture. If this method is applied to the cross attention, it would provide interpretability in such an architecture.\n\n\nOther suggestions\n- in Abstract: KS task --> keyword spotting task. Please avoid using the abbreviation without a definition in the abstract. Also, it is better to add which KS task you are using.\n- In Introduction: Whisper is not an SSL model\n- In Introduction: (Finke et al., 1997; Stolcke et al., 2000) are not representative studies for GMM-HMM. You may refer to Rabiner, Lawrence R. \"A tutorial on hidden Markov models and selected applications in speech recognition.\" Proceedings of the IEEE 77.2 (1989): 257-286 or other representative papers.\n- Section 2.2: I recommend you emphasize that AttAttr (Hao et al., 2021) would be used for your main comparisons here.\n- I think this paper would be more attractive if it is applied to other applications (e.g., LLM, Vision transformer) additionally. Please consider the additional evaluations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7938/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699021439140,
        "cdate": 1699021439140,
        "tmdate": 1699636974518,
        "mdate": 1699636974518,
        "license": "CC BY 4.0",
        "version": 2
    }
]