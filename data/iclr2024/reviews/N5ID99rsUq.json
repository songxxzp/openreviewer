[
    {
        "id": "wKi1YpMCts",
        "forum": "N5ID99rsUq",
        "replyto": "N5ID99rsUq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_wev8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_wev8"
        ],
        "content": {
            "summary": {
                "value": "This work utilizes the stability generalization framework to quantify the generalization bound of the Free Adversarial Training algorithm. Additionally, it shows that Free AT has a smaller generalization gap (but test error may not be smaller) and provides some theoretical intuitions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work offers important insights into the convergence properties of free AT. Additionally, the authors highlight intuitive relationships between \"more simultaneous\" gradient updates during training and the resulting generalization capability. These results can inspire further improvements in robust training algorithms to alleviate overfitting. The paper is generally well-presented and easy to follow."
            },
            "weaknesses": {
                "value": "- The theoretical support for FreeAT having a smaller generalization gap than VanillaAT could be more rigorous. Specifically, while Theorem 2 presents pessimistic results for the convergence of VanillaAT, it is unclear whether this bound is tight. It is unclear whether the convergence difference between vanilla and free AT is due to the algorithm itself or some artifacts of the proof technique. While experiment results support this intuition, the paper would benefit from some additional explanations. It would be even better if some lower bounds could be provided for $\\mathcal{E}\\_{\\textrm{gen}} (A\\_{\\textrm{Vanilla}})$.\n- The relationship between a smaller generalization gap and better transferability is unclear. The motivation for the experiment setting of transferring attacks from a robust model to a standard model is weak. I suggest moving the transferability analysis to the Appendix (it's still good to have them) and making space for Table 2, which supports your main claim.\n- Figure 1 should use different line styles for train and test. In the current form, it's hard to distinguish them.\n- Since the proposed convergence bounds depend on the dataset size $n$, this paper would benefit from some empirical comparisons between free and vanilla AT with different $n$ values.\n\nI believe that the value of the theoretical bound on FreeAT's convergence outweighs the above weaknesses. Hence, a rating of 6 is given."
            },
            "questions": {
                "value": "- Do the theoretical results also apply to the $\\ell_\\infty$ case?\n- Has there been any work that empirically estimates the Lipschitz and smoothness constants in Assumptions 1 and 2?\n- Can smoothness and Lipschitzness assumptions (1 and 2) be relaxed? Specifically, instead of having this condition for all pairs of $\\delta, \\delta'$, is it possible to define Lipschitzness and smoothness over $\\delta$ w.r.t. the nominal point ($\\delta = 0$)? This relaxation will make the conditions more realistic.\n- What is Free-4 in Table 2 and some of the figures (including Figure 1)?\n- Theorem 3 lower-bounds $\\mathbb{E}[ || w(S) - w(S') || ]$. However, does a large $\\mathbb{E}[ || w(S) - w(S') || ]$ necessarily translate to large $\\mathcal{E}\\_{\\textrm{gen}}$? Isn't it the case that neural networks with very different weights can have similar behavior?\n- In practice, the attack loss function and the training loss function may not be the same, and using different losses has been empirically shown to decrease the generalization gap. Examples include TRADES [1] and ALP [2]. It's probably a stretch goal, but is it possible to extend the analysis to this scenario?\n\n[1] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" International conference on machine learning. PMLR, 2019. \\\n[2] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. \"Adversarial logit pairing.\" arXiv preprint\narXiv:1803.06373, 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4972/Reviewer_wev8",
                    "ICLR.cc/2024/Conference/Submission4972/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697676183362,
        "cdate": 1697676183362,
        "tmdate": 1700360437000,
        "mdate": 1700360437000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ElWES9T99r",
        "forum": "N5ID99rsUq",
        "replyto": "N5ID99rsUq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_T7Bj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_T7Bj"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the generalization of free adversarial training (AT) which was proposed in Shafahi et al. (2019). \n The authors use the algorithmic stability approach to analyze its generalization behavior and it provides its comparison of the generalization bounds against the vanilla, fast AT methods.  It claims that the free AT algorithm could have a lower generalization bound than the vanilla AT one."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This seems to be the first-ever-known result that addressed the generalization of the free AT method using the algorithmic stability approach in the setting of mini-max formulation."
            },
            "weaknesses": {
                "value": "While the stability results for the free AT method are first-ever-known, the proof techniques seem to be incremental and the paper did not illustrate clearly what the main technical contribution is, particularly considering there is a considerable amount of work on stability analysis.\n \nThe generalization bound in Theorem 4 relies on the restrictive assumption that the gradient $\\nabla_\\delta h(w,\\delta; x,y)$ is lower-bounded by $1 / \\psi$ during the training process.  There is no discussion about when this critical condition holds true."
            },
            "questions": {
                "value": "It is not clear to me what the free AT method aims to minimize or optimize.  The objective function of the vanilla AT method is given on page 3, i.e. $R_S(w)$ or $R(w)$.  From the pseudo-code of Algorithm 3,  there are two random samplings--one for mini-batch and one for $\\{\\delta_j\\}$ and then the $w$ and $\\delta$ are updated by gradient descent and ascent, respectively.  In this sense, does the free AT methods aim to minimize the following objective \n$$ \\min_w \\max_\\delta {1\\over n } \\sum_{j=1}^n \\int_{\\delta_j\\in \\Delta} h(w,\\delta_j; x_j,y_j)$$ \nThe objective functions seem to be very different from each other for the free AT method and the vanilla AT one.   Indeed, the objective function of the free AT method is a low-bound relaxation of the vanilla one.   Could you explain more about this point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697813704821,
        "cdate": 1697813704821,
        "tmdate": 1699636484499,
        "mdate": 1699636484499,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bt3tO2KlnX",
        "forum": "N5ID99rsUq",
        "replyto": "N5ID99rsUq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_ge9g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_ge9g"
        ],
        "content": {
            "summary": {
                "value": "This work studies the role of min-max optimization algorithms in the generalization performance of adversarial training methods. It leverages the algorithmic stability framework to compare the generalization behavior of adversarial training methods. The developed generalization bounds suggest that not only can the free AT approach lead to a faster optimization compared to the vanilla AT, but also it can result in a lower generalization gap between the performance on training and test data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work provides some theoretical results.\n- The theoretical conclusions are easy to follow."
            },
            "weaknesses": {
                "value": "- What is the definition of $\\Delta$?\n- What is the definition of randomized algorithm $A(\\cdot)$? A mapping? If yes, then what is the definition of $\\mathbb{E}_A$?\n- Given $S$, $w=A(S)$ is a random variable or constant\uff1f\n- What's the definition of $S'$ here?\n- What is the definition of \"$A$ is $\\epsilon$-uniformly stable\"?\n- Unclear definition in Theorem 1?\n- I **guess** the theory is developed over \"randomized algorithm $A$\" (and Gibbs loss?), i.e., the output of $A(S)$ is random weights, which is distributed by a posterior. However, this paper only presents empirical results based on deterministic weights. How can these empirical findings provide support for the theoretical results?\n- If $A(S)$ is a random variable, given $S$, what are the specific posterior distributions of $A_{AVanilla}(S)$ and $A_{Free}(S)$? What is the difference between these posterior distributions? Where does the randomness of $A_{AVanilla}(S)$ come from? \n- It seems there are not some interesting insights from the theoretical and empirical results in this work. \n\n(Please correct me if I have some mistakes.)"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4972/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4972/Reviewer_ge9g"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566089284,
        "cdate": 1698566089284,
        "tmdate": 1700319615708,
        "mdate": 1700319615708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fI0C2keEQX",
        "forum": "N5ID99rsUq",
        "replyto": "N5ID99rsUq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_iGFe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4972/Reviewer_iGFe"
        ],
        "content": {
            "summary": {
                "value": "This paper studies stability and generalization of vanilla, free, and fast adversarial training from an algorithmic stability perspective. The generalization error gap bounds are derived for those adversarial training methods, and numerical results are also provided to show the generalization performance and robustness against black-box attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-motivated and well-written. The novelty and contributions are clearly stated and organized. The theoretical findings are provided in a rigorous manner, together with some validation numerical results. In general, the theoretical findings are interesting to the community."
            },
            "weaknesses": {
                "value": "1. This paper is dedicated to generalization performance analysis of existing adversarial training methods and reveals some interesting points. Nevertheless, there is a lack of deep insights on the new advanced designs of adversarial training from the generalization bounds. The authors should have discussed the insights/guidance from the theoretical findings, or discussed certain limitations of the algorithmic stability approach itself.\n2. From the experimental results, e.g., Figure 1, it appears that the reduced generalization error gap of free adversarial training is mainly due to the higher training error. Assuming the generalization error gap maintains, it is unclear if the test error can be further reduced when the training error is reduced. The authors should add some comments on this.\n3. It would expect that new training/regularization methods could be proposed given the obtained generalization error bounds. Otherwise, the impact of the theoretical findings of this work is quite limited. A thorough discussion would be helpful and beneficial. It would be also interesting to know the potential connection between generalization gap and the robustness against adversarial attacks."
            },
            "questions": {
                "value": "See the Weaknesses above. \n\nAdd some comments on the practical usefulness of the theoretical findings with respect to the design of adversarial training methods. The limitations of the algorithmic stability approach for studying generalization performance could be also discussed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672262767,
        "cdate": 1698672262767,
        "tmdate": 1699636484319,
        "mdate": 1699636484319,
        "license": "CC BY 4.0",
        "version": 2
    }
]