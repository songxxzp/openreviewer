[
    {
        "id": "B5VKbXhihS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_ZNqV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_ZNqV"
        ],
        "forum": "x4OPJ7lHVU",
        "replyto": "x4OPJ7lHVU",
        "content": {
            "summary": {
                "value": "The paper proposes a new paradigm for privatizing in-context learning tasks called \u201cDP-ICL\u201d, which achieves a strong utility-privacy trade-off and a comparable performance as the non-private counterpart. The authors demonstrate the effectiveness of DP-ICL on text classification and language generation tasks, and show that it offers a promising overall paradigm for applying ICL in a privacy-preserving way."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- This paper is well-written and well-organized. \n- The proposed paradigm is novel and is a novel paradigm for privatizing ICL tasks.\n- The authors provide a clear and concise explanation of the DP-ICL paradigm. The figures and captions are easy to understand.\n- Experiments on 4 benchmark datasets are extensive and sufficiently demonstrated the promise of the approach towards trustworthy usage of large language models."
            },
            "weaknesses": {
                "value": "- Limitations can be discussed in more details. For example, \u201ca better-trained embedding-to-text model may yield further improvements in the model performances.\u201d Then what can be the potential candidate models here and why did the authors not adopt these models?"
            },
            "questions": {
                "value": "- How do you see the DP-ICL paradigm being applied in real-world scenarios, and what are some of the challenges that would need to be addressed?\n- What are some of the limitations of the DP-ICL paradigm, and how might they be addressed in future research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Reviewer_ZNqV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697342791048,
        "cdate": 1697342791048,
        "tmdate": 1699636400118,
        "mdate": 1699636400118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hnBqbO7UvA",
        "forum": "x4OPJ7lHVU",
        "replyto": "x4OPJ7lHVU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_2dnr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_2dnr"
        ],
        "content": {
            "summary": {
                "value": "The author proposed a new Differentially Private In-Context Learning framework (DPICL) to minimize privacy risk in in-context learning. The main idea is to leverage the private \u201csample-and-aggregate\u201d paradigm. For text-classification, the paper proposed to report the noisy max with gaussian noise; For text generation, the paper proposed to use embedding space aggregation or keyword space aggregation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed methods are intuitive and in the experimental section, authors find DP-ICL achieves a comparable performance with non-private ICL.\n\nThorough experimental studies ranging from text classification, question answering, and summarization. In particular, the proposed framework can be applied to QA and summarization tasks which are complex and challenging tasks."
            },
            "weaknesses": {
                "value": "It is a little confusing on how the author analyzed the sensitivity and how are the neighboring database is defined.\n\nHow much extra cost (monetary and privacy budget) does the proposed framework incurs? It would be great if the author can include some insights on the cost vs accuracy trade off.\n\nIs comparing 4-shot vs 0-shot a fair comparison? In the related work section, the author mentioned work using examples from public dataset. Perhaps a better comparison is between private 4-shot using examples from the training data vs. 4-shot using examples from public data."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Reviewer_2dnr",
                    "ICLR.cc/2024/Conference/Submission4315/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776478349,
        "cdate": 1698776478349,
        "tmdate": 1700545358011,
        "mdate": 1700545358011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NXNeOoMesu",
        "forum": "x4OPJ7lHVU",
        "replyto": "x4OPJ7lHVU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_3Mai"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_3Mai"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of private data leakage of in context examples, during few-shot prompting (in-context learning). A new operation mode of LLMs is to ask a model to perform the task, and provide some examples for it, instead of actually performing any fine-tuning or changing the model parameters. In such use-cases, if the in-context examples have private information in them, there is a chance that it leaks to the output. To prevent this leakage, the authors propose differentially private in context learning methods for classification and generation. \n\nThese methods are all based on some form of noisy voting/decoding. The classification is directly using noisy max, but for the generation task the authors propose 3 different methods, to solve the problem of high dimensionality. They propose three different methods, one where the generation is mapped to embedding space, noisier and then mapped back to sentence space. The other two methods do noisy generation of keywords, and then uses a language model to generate text based on that.\n\nThe authors evaluate their proposed method on numerous tasks/datasets and compared to non-private baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem the authors are studying is timely, as practitioners are not really fine-tuning models any more and rely on prompting more and more, therefore solutions like DP-SGD are not relevant.\n\n\n2. The paper is very well-written and flows really well. The visuals are well-crafted and help the understanding of the paper a lot.\n\n3. I like how thorough the paper is, in terms of the cases they study: classification and generation, and I also like the workarounds proposed for generation to limit the space. They are novel and also address the problem in a suitable way. Pairing up the keyword aggregation method with PTR was very interesting to me."
            },
            "weaknesses": {
                "value": "1. Limited number of queries: as there is budget expenditure per query, the model deployed with DP-ICL can only be queried a limited number of times, making the method/data unusable after the budget is finished. This is not a huge concern in scenarios where there is temporal data available, and the in-context examples would get updated regularly. However, it could be a problem during deployment for long term.\n\n2. Zero-shot performance is already really high, would probably be even higher for GPT3.5 or GPT4, which are not reported in the paper.\n\n3. There are no existing measures or proof of concept attacks that show private in-context data leakage."
            },
            "questions": {
                "value": "1. What are some real-world applications in which the authors think DP-ICL could be applicable, i.e. where the ICL few-shot samples are private, and using them is absolutely necessary, as for most of the existing tasks in the paper zero-shot performance is already high.\n\n\n2. Do the authors have any measure of fluency for the generated text, specially for the embedding aggregate method? Human evaluations are usually performed in NLP generation tasks, human evaluations are usually performed to see how fluent the generated text is, and I wonder how the transformation of sentence to and from embedding impacts its fluency.\n\n\n\nMinor comment: \n\nThe Min et al. 2022 reference is duplicated in the references (there are two versions of it)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821890166,
        "cdate": 1698821890166,
        "tmdate": 1699636399943,
        "mdate": 1699636399943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gVO0FZ67bq",
        "forum": "x4OPJ7lHVU",
        "replyto": "x4OPJ7lHVU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_cEpf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4315/Reviewer_cEpf"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of differential privacy in the context of in-context learning. To this end, the authors propose a framework that consists of partitioning, predicting, and aggregating. For the aggregation, the authors propose several algorithms depending on the scenarios: private voting for text classification & embedding space aggregation (ESA), and keyword space aggregation (KSA) for language generations. Through the experiments on 4 text classification datasets, 1 QA dataset, and 1 summarization dataset, the authors demonstrate the proposed method successfully imposes differential privacy without the loss of accuracy, compared to the non-private counterpart."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Well-motivated problem.** Differential privacy of LLMs is an interesting and important problem."
            },
            "weaknesses": {
                "value": "1. **Rooms for the improvement of the draft.** First, lots of details are currently missing. For example, there is no explanation of how $\\epsilon$ determines the noise $\\sigma$. While there is no explicit mention in the main draft, the results in Appendix B (Theorems 3 and 4) might be used. But, even if this is true, there is no mention of how $\\delta$ is set. In addition, how the given # of queries (e.g., 10,000 queries in Sec 4.1) is used for the algorithm? In Algorithms 1~4, there is no relevant part. \n2. **Limited technical novelty.** While the introduction of differential privacy in the context of ICL is novel, most of the techniques are adopted from the previous works.   \n3. **Limited motivation for ESA.** The authors propose both ESA and KSA for language generation tasks. However, it seems that there is no merit in using ESA over KSA, as KSA always outperforms ESA in the experiments. Since introducing several candidates even makes practitioner confused about what to use, more support for ESA are required. In addition, it would be good to include qualitative examples of ESA\u2019s sentence candidates with 0-shot inference and the finally selected one."
            },
            "questions": {
                "value": "Please address the concerns in above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4315/Reviewer_cEpf"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699595331751,
        "cdate": 1699595331751,
        "tmdate": 1700549316820,
        "mdate": 1700549316820,
        "license": "CC BY 4.0",
        "version": 2
    }
]