[
    {
        "id": "XKvyrvu8jH",
        "forum": "tf6nR1B8Nt",
        "replyto": "tf6nR1B8Nt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_Km89"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_Km89"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes several measurements to quantify the optimization dynamics of training neural networks. Motivated by optimization theory, the authors study how cosine similarity, Lipschitz-type, and convexity-type quantities evolve over the training process. \n\nBy empirically studying those quantities on many datasets, ranging from images to languages, the authors claim that the optimization geometry is stable and simple, and in particular \"optimization trajectories never encounter significant obstacles\". Based on these observations, the authors provide insights and discussions about common training techniques such as initialization and batch size."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Overall the observation that optimization dynamics is benign for state-of-the-art neural networks is informative and interesting. This reinforces prior observations such as those in Goodfellow et al. (2015).  \n\n2. Empirical measurements such as RSI, EB, and cosine similarity are useful diagnostic tools for understanding the optimization property of neural networks"
            },
            "weaknesses": {
                "value": "Overall, I find the paper seems to be overclaiming the results.\n\n1. (main) This paper seems to give a misleading message. The title says the neural networks have \"simple geometry\", and the abstract says \"optimization trajectories never encounter significant obstacles, [....] maintain stable dynamics during the majority of training.\" I think that those properties are examined only for state-of-the-art neural networks. There are lots of techniques to alleviate optimization issues, so the overall claim of this paper severely trivializes the techniques in common practice. (On a related note: It would be interesting to investigate how much each technique contributes to making optimization benign.)\n\n2. (main) I find that RSI is confusing to understand: in Figure 2, the measure for ImageNet and WikiText is initially close to zero, so does this mean the training dynamics is not ideal in the beginning? \"RSI and EB follow predictable trends\" do not imply that optimization dynamics have benign properties.\n\n3. (main) The cosine similarity is low. While I agree with the argument that there is stochasticity, I think cosine similarity should be compared with the baseline where (i) gradients are replaced by noise, or (ii) gradients are calculated based on random labels or random tokens in a minibatch. Without comparison with the pure noise baseline, we do not know if cosine similarity is significantly correlated with the signal direction.\n\n4. (minor) I understand that this paper is mainly about investigating the empirical properties of training neural networks from the optimization perspective. But from a practical point of view, what can we learn? Do we know what part of the architecture of, say a transformer, is crucial for benign optimization? Which techniques (layer normalization, dropout, learning rate warmup, etc) are playing a role?"
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698362314162,
        "cdate": 1698362314162,
        "tmdate": 1699636187784,
        "mdate": 1699636187784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BUnUJmeknS",
        "forum": "tf6nR1B8Nt",
        "replyto": "tf6nR1B8Nt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_LuUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_LuUd"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the geometric properties of NN training loss over the training procedure. The authors consider two conditions called Lower Restricted Secant Inequality (RSI) and Upper Error Bounds (EB). The paper tracks the local and stochastic versions $RSI(G,w,w^*)$ and $EB(G,w,w^*)$ as well as their ratio $\\gamma(G,w,w^*) = \\frac{RSI(G,w,w^*)}{EB(G,w,w^*)}$, which are quantities evaluated with the stochastic gradient oracle $G$ queried for the current point $w$. In particular, $\\gamma$ corresponds to the cosine similarity of the stochastic gradient and $w-w^*$, so $\\gamma > 0$ implies that the stochastic gradient is pointing to the \"right\" direction. For various settings, the paper demonstrates that the RSI, EB, and $\\gamma$ stay quite stable throughout training, despite the nonconvexity and stochasticity. Moreover, the cosine similarity $\\gamma$ is almost always positive. The paper supplements the main results with ablations and detailed discussions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I think this paper is a good submission. It provides an interesting set of results that sheds light on the theory-practice gap in neural network training. The empirical observations suggest that neural network training, although nonconvex and stochastic, could be in fact much more well-behaved than expected.\n\n2. The paper is well-written and delivers the main ideas clearly. The ablation studies and discussions at the end of the paper provide useful insights.\n\n3. The analysis of locally optimal learning rate seems to have a potential to lead to a method for learning rate schedule design or for adaptive learning rate schemes."
            },
            "weaknesses": {
                "value": "1. There is a closely relevant paper titled \"SGD Converges to Global Minimum in Deep Learning via Star-convex Path\" which appeared in ICLR 2019, which also empirically studies the SGD trajectories and shows that the star-convexity is satisfied for most epochs. I have not checked the follow-up results carefully, so there may be more existing results that discover surprisingly benign geometric properties of neural network training. The paper should contextualize itself relative to this existing result(s)."
            },
            "questions": {
                "value": "1. A recently discovered phenomenon Edge of Stability (EoS) shows that GD trajectory experiences progressive sharpening and then reaches and hovers around the stable convergence threshold $2/\\text{(step size)}$. When EoS is in effect in the later phase of training, the training dynamics is rather unstable, with occasional peaks in loss curves. Similarly, when you run SGD instead of GD, we also often observe these loss peaks in the training curves. In contrast, the RSI, EB, and $\\gamma$ curves look quite stable, without any significant peaks or perturbations. My question is: did you observe any peaks or unstable convergence in your experiments? Do the \"fundamental properties\" (Section 4.1) continue to hold when we observe EoS? If so, how can we reconcile these seemingly contradictory trends?\n\n2. In the late phase of training, the curves seem to experience sharp increases in both mean and variance. The explanations based on interpolation vs non-interpolation and the bias introduced by setting $w^* \\approx w_T$ are intuitive, but do they explain the whole story? For example, in Figure 2, the RSI and EB of CIFAR-10 do not show a significant increase in mean curves, but we do observe a consistent increase in variance. What could be an explanation for this?\n\n3. It seems that using large batch sizes increases $\\gamma$ throughout training. What happens if we use full-batch training (i.e., GD)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859632634,
        "cdate": 1698859632634,
        "tmdate": 1699636187717,
        "mdate": 1699636187717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mxnm5WMM7b",
        "forum": "tf6nR1B8Nt",
        "replyto": "tf6nR1B8Nt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_eK7x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_eK7x"
        ],
        "content": {
            "summary": {
                "value": "Given the efficacy of stochastic first-order optimization algorithms on the non-convex landscapes of neural networks, this paper aims to show the simplicity of neural landscapes by tracking two key quantities that occur in the convergence analysis. Namely, they consider the restricted secant inequality (RSI) and the error bound (EB) during the optimization trajectory, whose ratio in turn boils down to the cosine similarity between the sampled minibatch gradient and the difference between the current and the final weights. Their observations seem to suggest that this quantity remains almost positive throughout (and that's why the titular phrase \"no wrong turns\") and has consistent behaviour during training. The results are demonstrated on residual networks in vision and language, and the effects of batch size, longer training duration, and initialization are considered."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Firstly, I think it's a great research direction to understand how the important quantities that occur in the optimization analyses actually behave in the neural landscapes and then ground up uncover properties of the loss landscape. These identified properties, such as the cosine similarity, might reflect an important aspect of the trajectories taken by 1st-order optimization algorithms in loss and the geometry of the landscapes in general. It is interesting to see how these quantities behave across various established empirical settings."
            },
            "weaknesses": {
                "value": "- **The nature of cosine similarity in high dimensions:** One of the fundamental issues about this paper is to what extent cosine similarity is a meaningful measure in high dimensions, which here are in the orders of tens of millions. The particular value of cosine similarity is somewhat difficult to grasp as a result, without the lack of a baseline that contextualizes its value. While the curves look alright and pretty stable throughout, the moment one looks at the value at the y-axis (which, e.g., is in the order of 1e-3 for ImageNet), doubts emerge if one is reading too much signal in the noise. This is in addition to the fact that the cosine similarity is between vectors that at their core are based on the same network structure will tend to have a higher cosine similarity (contrast that with arbitrary random vectors in the same dimensions). \n\n   All of this makes it hard to be convinced by their argument about the stability of this cosine similarity (and optimization proceeding at a regular pace), when this value of cosine similarity almost hints at near orthogonality.  \n\n&nbsp;\n\n- **The lack of clear insights that can be drawn:** Consider Figure 2, middle column, where the results are shown for ResNet50/ImageNet. It is unclear if anything much is being said by these graphs, except towards the end where it hints at being close to convergence. And that too, because the denominator in these expressions $\\|w-w^\\ast\\|$ starts becoming small. More generally, a lot of these plots contain a sense of vagueness about the actual insights that can be drawn from them. The paper says there are \"predictable\" properties, but a less glorified way of phrasing would be they are somewhat banal. \n\n&nbsp;\n\n- **Dependence of the results on the chosen empirical setup** It is not clear if the results are overly reliant on the particular empirical setup chosen in the paper. Does the same thing hold when training with large learning rates? Bigger or smaller weight decay coefficients? When different learning rate schedulers are used? Essentially, my guess would be that something that would make the network jump in the optimization landscape (which is not uncommon in practical settings) should at least result in a different phenomenology than that presented here. Likewise, what happens when there is more noise in the dataset, say different amounts of label noise? Besides most of the experiments have been carried out with residual architectures. What happens if you consider a fully convolutional architecture, such as VGG on cifar10 and/or MobileNet on ImageNet?"
            },
            "questions": {
                "value": "See the weaknesses section. \n\nSome minor questions:\n- Influence of Batch size: Shouldn't the equation there be normalized since when you have a bigger batch size, you don't just add the gradients of smaller batch sizes but also normalize corresponding to their sizes? How does the argument then work out?\n- Why should the set of global minima be convex? (page 3)\n- Model width/depth: How are the relative trends when you normalize the quantities by the number of parameters?\n- loLR: I understand the aim of Figure 3, but can you port these LR schedules for a fresh training of the corresponding networks? What do you observe?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699205836162,
        "cdate": 1699205836162,
        "tmdate": 1699636187646,
        "mdate": 1699636187646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kHB47xXvih",
        "forum": "tf6nR1B8Nt",
        "replyto": "tf6nR1B8Nt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_oKQj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2513/Reviewer_oKQj"
        ],
        "content": {
            "summary": {
                "value": "This paper studies why stochastic first-order optimization methods can succeed in modern deep neural networks, despite the non-convex nature of the training problem. Specifically, this paper considers the ratio between (modified) Restricted Secant Inequality (RSI) and (modified) Error Bound (EB). This ratio turns out to be exactly the cosine similarity between G(w) and w-w*, where G(w) is the stochastic version of the local gradient, and w-w* is the direction from the converged point to the current point. In short, the considered ratio measures how much the local update direction aligns with the true direction pointing to the global optimum.\n\nThe authors conduct experiments in different settings to examine how this ratio performs and evolves during the network training. Surprisingly, this ratio always stays positive and is very stable during the training. This means that the landscape of modern neural networks, despite its non-convexity, is benign in the sense that every local update is positively correlated with the desired direction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposed an effective metric, based on a variant of quantities involved in RSI and EB, to evaluate the training trajectory of modern neural networks. Both RSI  and EB are theoretically grounded metrics in optimization theory, capturing important properties of the optimization landscape.\n\n2. Experiments validate that the cosine similarity metric stays positive and stable during training. This is an interesting finding, which may potentially inspire theoretical landscape study and/or convergence analysis of neural networks. Further, the trends of cosine similarity can partially justify the empirically used learning rate schedule.\n\n3. This paper conducts extensive experiments under different settings and carries out comprehensive discussion. Notably, the proposed metric is dependent on the optimization trajectory, and thus the authors carefully discuss potential factors that might affect the interpretation of empirical results."
            },
            "weaknesses": {
                "value": "1. The paper does not provide any further theoretical analysis based on their empirical findings. Although existing works have demonstrated that well-behaved RSI and EB can lead to the convergence of first-order algorithms, it is not clear whether trajectory-dependent RSI and EB properties can lead to similar results.\n\n2. The paper does not provide practical guidance on network training. Most conclusions are explanatory.\n\nOverall, it is good to point out the \"no wrong returns\" property of the neural network landscape. However, its immediate impact on theory or practice is unclear to me."
            },
            "questions": {
                "value": "1. In contrasting examples, what is w*? The (known) global optimum or the converged point?\n\n2. The authors claimed \"these observed properties are sufficiently expressive to theoretically guarantee linear convergence\" in the abstract. However, the paper considers variants of RSI and EB. Only local but not global properties are examined. Thus, the empirical findings cannot guarantee linear convergence. Is it correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2513/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699536453729,
        "cdate": 1699536453729,
        "tmdate": 1699636187584,
        "mdate": 1699636187584,
        "license": "CC BY 4.0",
        "version": 2
    }
]