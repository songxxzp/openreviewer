[
    {
        "id": "zzjM7Xo5Mw",
        "forum": "NRVW8SShFd",
        "replyto": "NRVW8SShFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a two-stage video editing method, which first leverages an off-the-shelf image editing method to edit keyframes, and then performs interpolation between the edited frames. Quantitative and qualitative experiments demonstrate that MaskINT achieves comparable performance with previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Efficiency. The proposed method achieves comparable performance with diffusion methods, but it is much faster."
            },
            "weaknesses": {
                "value": "- The video editing performance heavily relies on frame interpolation performance. Almost all showed results (in main submission and Supp) are simple motions, such as car translation, rhino translation. The simple motions can be easily interpolated. But for complex motions, it is difficult to perform frame interpolation, and it also suffer occlusions. Actually, in the showed man dancing case, there are obvious artifacts in arms. Also, the proposed method may suffer a lot in case of long-range video editing. Thus, the generalization ability of the proposed method is somehow limited.\n- Evaluation. There are only 11 examples in Supp, and it is difficult to judge the performance. Are the results cherry-picked? Could you give more results?"
            },
            "questions": {
                "value": "- How about the failure cases? \n- How about long videos and complex motions?\n- Would it fail if the first-stage kerframe editing fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8292/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8292/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698039807503,
        "cdate": 1698039807503,
        "tmdate": 1699637031293,
        "mdate": 1699637031293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7AMadlqiza",
        "forum": "NRVW8SShFd",
        "replyto": "NRVW8SShFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_hZt7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_hZt7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a two-stage video editing framework, using T2I diffusion model to edit the key frames and then interpolating between those frames. During T2I diffusion process, the paper leveraged controlnet to jointly keep the edge consistency. After that, a Masked generative transformer model called MaskINT is introduced to generate middle frames. The results show that the proposed network can accelerate generate videos compared with baseline pipelines while suffering slightly temporal and prompt consistency decrease."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed MaskINT leverage masked generative transformer to interpolate between keyframes.\n2. The inference speed outperformed the proposed video editing pipelines.\n3. MaskINT is trained on unlabeled video datasets using masked token modeling, without needing text-video pairs."
            },
            "weaknesses": {
                "value": "1. Although the proposed MaskINT can beat other methods in speed, the method still suffers consistency degradation in both prompt and temporal domain. \n2. Noticeable degradation across key frames and interpolated frames.\n3. No related baseline comparison between video interpolation pipeline."
            },
            "questions": {
                "value": "1. By increasing the decoding step and keyframes, the method can increase the performance in Tem-Con and Pro-Con. Can the method reach comparable qualitative results in less time by increasing those hyper parameters?\n2. The videos in supplementary seem to have heavy moir\u00e9 patterns. Why does this occur?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745838745,
        "cdate": 1698745838745,
        "tmdate": 1699637031172,
        "mdate": 1699637031172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bwf6PjTBGF",
        "forum": "NRVW8SShFd",
        "replyto": "NRVW8SShFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_gZ41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8292/Reviewer_gZ41"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new approach to structure-guided editing for videos. The proposed method is composed of two stages. In the first stage, an image-based diffusion model is leveraged, along with the cross-frame attention technique, to jointly edit a small number of key frames. In the second stage, a structure-guided non-autoregressive masked transformers model is developed for the interpolation task, aiming to propagate the information from the (edited) key frames to the intermediate frames. The experiments in the paper demonstrate the proposed method can enable temporally consistent edit propagation results while achieving better efficiency compared to existing diffusion-based approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The video editing results provided in the paper demonstrate that the non-autoregressive masked generative modelling technique, which have mostly been applied to the unconditional generation or text-condition generation so far, can be effectively adapted to the structure-conditioning generation setting.\n\nThe experiments in the paper demonstrate that the proposed method can achieve better efficiency compared to existing diffusion-based approaches."
            },
            "weaknesses": {
                "value": "While the idea of extending the non-autoregressive masked transformer technique to structure-guided generation is technically sound, the technical contribution on the fundamental side is somewhat limited. Video editing with diffusion model via key-frame edit propagation has been widely explored. The effectiveness of masked generative model in video generation has also been well established. The key contribution of this paper, from my perspective, is in showing that it is possible to incorporate dense structure information into the masked transformer model. There are limited discussions in the paper, however, to provide insights on why such a task is difficult, what are the fundamental challenges in doing that, and why the proposed technique is a good solution for such challenges. \n\nThe discussion on the technical details is somewhat vague. In particular, it seems that the model architecture details were not elaborated.\n\nThe provided evaluation is a bit weak: \n+ I feel that the subjective comparison should be made more complete: video results were only provided (in the supplementary material) for the proposed method, not for competing methods. That makes it difficult to assess the temporal quality of the proposed method in comparison with the other methods.\n+ The comparison is not entirely fair, competing methods are all zero-shot setup, which never trains a video model. Existing video diffusion works have been shown to be effective for interpolation ([1], [2]), a fair comparison would be to compare with adapted versions of those methods to incorporate structure control signal.\n+ It seems that the provided results are all with stylized content, which tends to make it more visually tolerable to temporal inconsistencies. As the main goal of the second-stage model is to perform keyframe propagation, I think one important test that should be done is to apply the model on the reconstructive setting, i.e. perform propagation with the original keyframes instead of edited ones and assess the reconstruction quality of the intermediate frames.\n\n[1] Make-A-Video: Text-to-Video Generation without Text-Video Data. Singer et al., 2022\n\n[2] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. Blattmann et al., 2023"
            },
            "questions": {
                "value": "Please find my detailed comments in the Weaknesses section. Other than that, there are a couple of questions I\u2019m curious about:\n+ Will the proposed technique works for other type of controls such as depth maps or pose map? \n+ How will the method perform in the extrapolation instead of the interpolation setting? Or in the setting where only one key frame is edited?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813861609,
        "cdate": 1698813861609,
        "tmdate": 1699637031051,
        "mdate": 1699637031051,
        "license": "CC BY 4.0",
        "version": 2
    }
]