[
    {
        "id": "dxvBMQKoWe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_V39V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_V39V"
        ],
        "forum": "WFYbBOEOtv",
        "replyto": "WFYbBOEOtv",
        "content": {
            "summary": {
                "value": "This paper applies the JEPA architectures to videos by processing 16-frame videos (with a stride of 4) using a 3d-CNN. The model is trained on roughly 2M videos by combining Kinetics 400/600/700, HowTo100M, and SSv2. The authors show that the representation learned after training the encoder performs well on various tasks (video and image tasks)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The biggest strength of this work is the large-scale training on a huge dataset (VideoMix2M), showing the ability of JEPA to scale up.\n- This work is evaluated on six different tasks/datasets (3 for videos and 3 for images), which provides a solid baseline for future methods.\n- The idea is simple."
            },
            "weaknesses": {
                "value": "### Novelty\n\n- Using an existing image architecture to process videos by flattening the 3D feature maps into 1D does not qualify as an architectural novelty.\n- Videos have different challenges than images, which could have been tackled as novelties, such as how to cover larger temporal context (>2 seconds) or how to capture long-term dependencies (e.g., H-JEPA?)\n- For a paper that uses an existing architecture (JEPA), it is expected to see proposals on how to solve challenges of the video domain.\n\n### Experimentation and Comparisons:\n\n- It seems that the experimental protocol is flawed. Neither Table 1 nor Table 2 show the results of the model when trained on the same datasets. This model is trained on more data than all the other models, making a fair comparison impossible. A common approach would be to show the new model results when trained on the same datasets as previous works, then provide the results with the scaled-up dataset (as a plus). It is hard for the reader to validate the model\u2019s performance if more than one thing changes between the rows of a comparison table.\n- Also, Figure 1 (Left) shows comparisons, but all models used different sizes and cannot be directly compared.\n- Looking through the ablations, the only result I can use for comparison is [Table 3 (b) row 1], [Table 1 row VideoMAE], and [Table 2 row VideoMAE]. All three results use the ViT-L/16 model and pretrain on the same K400 data. Results show that on the Video task for SSv2, VideoMAE outperforms the proposed method by (61.2-50.8=10.4), and on the Image task for IN1k, VideoMAE outperforms V-JEPA by (71.9-66.1=5.8). These results do not show that V-JEPA is SOTA when trained fairly on the same datasets as others.\n\n### Ablations:\n\n- The provided ablations on masking strategy are good, but for a video paper, I expected more ablations on the stride and temporal reception field of the model. It is more likely that the model could perform better if trained on more than 2-second snippets or uses a two-flow architecture (I3D with optical flow?). These ablations are more relevant to video papers."
            },
            "questions": {
                "value": "- In Paragraph \"Comparing with image models, 2nd paragraph\": Is it possible that attentive probing improves the performance over linear probing because of the attention mechanism? How does it do with just linear probing?\n- Have you considered a multiscale representation, where you concatenate 16 frames with small strides at 224x224 with 16 frames at double the stride at 128x128,  and so on?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697229869483,
        "cdate": 1697229869483,
        "tmdate": 1699636305716,
        "mdate": 1699636305716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yN1Ywn9mcJ",
        "forum": "WFYbBOEOtv",
        "replyto": "WFYbBOEOtv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_TgmP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_TgmP"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a self-supervised mask modeling method from videos. Bascially, the style is like the exsiting mean teacher learning, while the self-distillation is performed for masked regions in the latent space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1 The fundamental training method is crtical for both image and video representation learning.\n2 The method seems to be sound.\n3 The paper is written well."
            },
            "weaknesses": {
                "value": "* Methodology\n\n1. Such self-distillation style has been widely used in the literature, which comes from mean teacher [NIPS 2017]. \n\n2. Mask modeling has been used in both image and video literatures. Even predicting the latent feature of masked spatio-temporal regions has been recently investigated in UMT [ICCV 2023].\n\nHence\uff0cthe combination of 1 and 2 is not quite inspired.\n\n* Results\n\n1. The results show in the abs and Figure 1 are not quite unfair actually, since the model sizes are not the same. \n\n2. The pretraining data is VideoMix2M, which are different from others. It is a bit doubtful about whether the improvement comes from the self-supervised method.\n\n3. It would be also great to show the finetuning results on the video benchmarks (e.g., K400, SthV2) for SOTA comparison."
            },
            "questions": {
                "value": "Please see weakness for reference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698463180684,
        "cdate": 1698463180684,
        "tmdate": 1699636305645,
        "mdate": 1699636305645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JXrBBTYDmH",
        "forum": "WFYbBOEOtv",
        "replyto": "WFYbBOEOtv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_eUQX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_eUQX"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the video representation learning via predicting the latent representation of video patches. The argument is that by predicting the latent representation, the model will achieve better results than the model trained via Masked AutoEncoder style training. By comparing with other MAE style approaches, the proposed one achieve higher accuracies on Kinetics, SSv2, and AVA dataset via simple linear probing tuning. The model also achieves quite significant performance on the image recognition. Those results show the proposed approach is quite good for representation learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper idea is quite simple and straight forward. It shares the same high-level idea as the MAE. While the MAE focuses on predicting the patch, the proposed idea focuses on predicting the embedding of the patch.\n\nThe proposed approach achieves quite significant performance boost on all the datasets, which is a little bit surprising. As the proposed approach is very similar to MAE, I wonder the reason behind the performance boost.\n\nIf the proposed approach and its performance can be verified, then it might be a quite good paper for the community to have."
            },
            "weaknesses": {
                "value": "The paper writing should be improved. The methodology part is quite hard to read. The high-level idea (sect. 3.1) is okayish to understand. But the application to video part (sect. 3.2) is not. Concretely, as the input has already been processed by the 3D ConvNet as a 1D feature, I don't know how to obtain the 3D sin-cos positional embedding (mentioned in Predictor section)?\n\nAfter reading the paper, I still don't know how to obtain the self-supervised video features. Is it the output of the target encoder or the output of the context encoder?\n\nFor Table1, it is known that joint training on SSv2 and Kinetics will improve both dataset's performance as mentioned in (Zhang, B., Yu, J., Fifty, C., Han, W., Dai, A. M., Pang, R., & Sha, F. (2021). Co-training transformer with videos and images improves action recognition. arXiv preprint arXiv:2112.07175) For a fair comparison with VideoMAE, the proposed approach should be pretrained on the Kinetics400 dataset."
            },
            "questions": {
                "value": "Please address the questions listed in the weakness.\n\nIn addition to that, I wonder how to do the inference? Which part of the model (context encoder, predictor, or the target encoder) are included in the inference process? \n\nWhy the model could achieve much better performacne than the VideoMAE? Is it because of the mixture video datasets for pretraining (if so, the whole paper might not be that exciting)? If predicting the embedding is better than predicting the patch, could you provide a conceptual reasoning for that?\n\nBefore reading Table 1 carefully, I think this paper is quite good. However, after reading Table 1 and comparing VideoMAE and V-JEPA on the ViT-L/16, I am afraid the main performance improvement is not brought by the proposed approach, but by the mixture of video datasets for pretraining."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699412720253,
        "cdate": 1699412720253,
        "tmdate": 1699636305560,
        "mdate": 1699636305560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c90Rs34rk1",
        "forum": "WFYbBOEOtv",
        "replyto": "WFYbBOEOtv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_zTBB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_zTBB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces VJEPA, a representation learning based on joint embedding predictive architecture (JEPA) to learn from video data. VJEPA is a self-distillation method that predicts embeddings (latents) of masked out regions of the input conditioned on embeddings of visible patches from a context encoder. The predicted latents are compared to latents from a teacher network whose weights are updated in via an exponential moving average (EMA) operation. The paper proposes a new dataset called ``VideoMix2M'' to train VJEPA. Empirical evaluations are performed to show the effectiveness of VJEPA over other video-based and image-based methods for several downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is a nicely executed empirical study that extends I-JEPA [Assran et al, 2023) to video data \n- The main message for me as a user is that (pre)training on video data can be fast by decoding/predicting in latent space. This is not a big surprise given that VJEPA inherits the strengths of I-JEPA. However, the paper shows empirically that there may not be a need need to sacrifice performance for speed \n- Empirical evaluations with ``frozen'' backbones (encoders) are conducted with academic video and standard image benchmarks that suggest that representations on video work better on video data while being competitive on image benchmarks"
            },
            "weaknesses": {
                "value": "- The method proposed in the paper is an extension of I-JEPA to video. The approach used to implement masking across the temporal dimension (``3D-multiblock'') and latent space prediction borrow heavily from I-JEPA. The novelty of the method introduced here is very limited for the reason mentioned above\n\n- A key component that determines the success of VJPEA is multiblock design. I would have liked to have seen careful ablation of the design choices of this block including the scale factors and also aspect ratios used to design the mask\n\n- The method uses an evaluation scheme (attentive probing) which may be the way to probe representations in foundation models. The biggest problem I see with this scheme is that it may make it hard for readers to compare performance across publications.\n\nAlso, please note my questions in the **Questions** section"
            },
            "questions": {
                "value": "-  Potential missing background/related work: the paper \"Siamese Masked Encoders\" (https://arxiv.org/abs/2305.1434) \n- Did you conduct a full ablation of all the design hyperparameters used in multiblock. If so, please this ablation in the appendix as done in I-JEPA for instance\n- Table 3 ablation - is randomly picking images from a video sequence the best approach to create a training dataset for image-based backbones. Also, the backbones used for VJEPA and I-JEPA in Table 3(a) are different. If results are available for the same backbone please consider including those t help a reader understand the design choices better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3520/Reviewer_zTBB"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699471249868,
        "cdate": 1699471249868,
        "tmdate": 1699636305494,
        "mdate": 1699636305494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vTRTaDybe2",
        "forum": "WFYbBOEOtv",
        "replyto": "WFYbBOEOtv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_HoWZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3520/Reviewer_HoWZ"
        ],
        "content": {
            "summary": {
                "value": "This work extends I-JEPA to videos for self-supervised video pretraining. Models of varying scale are trained using a 2 million video custom mix of common datasets (Kinetics-400/600/700, HowTo100M, SSv2). The resulting frozen models are compared to some prior work in literature (e.g. VideoMAE, InternVideo) and shown to have better performance on classification and action recognition."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. As an extension of Data2Vec, I-JEPA etc class of work to videos, it is a useful baseline for the community to know.\n2. The experiments and methodology are clear and easy to understand."
            },
            "weaknesses": {
                "value": "1. As a extension of I-JEPA to videos, technical novelty is extremely limited.\n2. A much bigger issue is that evaluations are problematic, by either not comparing to the relevant work or by comparing to prior work in a misleading way, contrary to the evaluation protocols the whole community has been using. \n\nFor example:\na) Most recent related work has adopted a finetuning evaluation protocol (e.g. VideoMAE, VideoMAEv2, ST-MAE, Hiera, MaskFeat, OmniMAE, MCMAE) and yet such evaluation is almost entirely absent from this work (except for highly limited comparison in Table 11, where V-JEPA performs worse than prior work despite a misleading comparison*), portraying a highly misleading picture of not only the state of the literature but also of the capabilities of the V-JEPA models in comparison. It is well known since even the original MAE work that such models do not have the best performance when frozen, but even finetuning 1 or a small number of layers makes a large difference, demonstrating how frozen evaluation can be quite misleading. Classification aside, on action recognition, the gap between frozen and finetuning can be particularly large, which can again be quite misleading.\n\n\n\nAlso, notably missing are references/discussion and comparison to some of the stronger recent work (e.g. UnMasked Teacher, Li et al 2023; Hiera, Ryali et al 2023; MCMAE, Gao et al, 2022, etc)\n\nb) Transfer to images is also explored, but once again, comparison to prior work is problematic, e.g. notably missing is any comparison to VITO,  Parthasarathy et al, 2023.\n\nc) The sample efficiency discussion is problematic in several ways, e.g. numbers of images are being compared to number of videos (Table 12). Another example is the claim that training schedule is \"shorter\" in terms of numbers epochs and therefore more efficient. It is is not particularly meaningful to compare number of epochs. It is far more meaningful for example, to compare wall clock training time for a comparable accuracy with a recent work, e.g. VideoMAEv2, Wang et al, 2023.\n\n*VideoMAEv1 achieves similar performance with much less data and model size as v2 on the considered metrics, so the speedup in Table 11 is likely in large part an artifact of the larger amount of data that particular model (v2, ViT-g/14) was trained on."
            },
            "questions": {
                "value": "It would be great if the authors can address the discussed limitations/weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699587977647,
        "cdate": 1699587977647,
        "tmdate": 1699636305429,
        "mdate": 1699636305429,
        "license": "CC BY 4.0",
        "version": 2
    }
]