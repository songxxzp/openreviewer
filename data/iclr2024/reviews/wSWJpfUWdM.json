[
    {
        "id": "1Vub0NGPTr",
        "forum": "wSWJpfUWdM",
        "replyto": "wSWJpfUWdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new federated learning training algorithm. The key idea of the algorithms is to share synthetic datasets instead of sharing the model weights. The authors argue that the method is superior to traditional model-averaging because the server can recover an approximation of the global loss landscape. The authors also demonstrate that the proposed method can be adapted to satisfy differential privacy (DP) by replacing the clean gradients with clipped noisy gradients. Some experiments show that the new method can outperform the traditional one in both private and non-private settings and may have better communication cost tradeoffs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors identify the problem of the existing weight-averaging FL training methods.\n* The proposed method can overcome the limitations of the existing algorithms.\n* The proposed algorithm is shown to have better empirical performances than the existing ones."
            },
            "weaknesses": {
                "value": "1. Some operations in the proposed algorithm may need better motivation or explanation. Please refer to Questions.\n2. The privacy description part of the algorithm can be improved. The description in Section 4.4 is not clear enough and self-contained for a main contribution in the main text. Some descriptions may need to be more precise (e.g., \"sanitize\" should be explicitly referred to the clipping operation). Since the privacy composition part mainly relies on existing tools, it may be better to provide a brief conclusion about the composition results. Also, the notations in Appendix (T?) are not consistent with Algorithm 1 (R_b?), which requires extra conjecture to parse the result.\n3. Some hyper-parameters may need to show how to tune ($R_b$ and $R_l$), because they are so different for private and non-private settings.\n4. The experiment setting may need to be more convincing. Please refer to Questions."
            },
            "questions": {
                "value": "1. It is mentioned that the synthetic labels are initialized to be a fixed, balanced set, but the experiments have heterogeneous data. This sounds controversial and may deserve more explanation. When a class does not appear in a local dataset, what should we expect the synthetic data of that class to look like? How do those affect global training compared with relatively iid data?\n2. How do you decide the synthetic dataset size of each client? Besides the factors mentioned (local dataset size and bandwidth), it seems the data complexity and local data distribution should also be considered when deciding the synthetic dataset size.\n3. Section 5.1 mentions that the learning rate is 100. Is it a typo? Otherwise, why we need such a large learning rate may need to be explained.\n4. What do the \"Fixed, Max, Median, and Min\" radius selection mean? Do they matter for whether private or non-private settings? The description needs to be more specific and consistent (i.e., it says r=1.5 or 10 in Section 5.1, but a different wording in Section 5.4).\n5. In the experiment setting of comparing the communication cost and epochs, there may be more reasonable settings. For example, when comparing the communication cost, shouldn't we compare the best end-to-end performance with fixed communication costs (e.g., 500MB) by varying the communication rounds and with the best hyper-parameters? \n6. Another interesting aspect not explored in the experiments FedAvg v.s. FedLAP with different sizes of models. Does a model with more parameters benefit from or loss advantage with the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698255430829,
        "cdate": 1698255430829,
        "tmdate": 1699636143563,
        "mdate": 1699636143563,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yzN7m9qECG",
        "forum": "wSWJpfUWdM",
        "replyto": "wSWJpfUWdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach called FedLAP-DP for federated learning. In contrast to previous methods that share point-wise local gradients for global model update, the new approach proposes to perform global optimization at the server by leveraing synthetic samples received from clients. Experiments are conducted to show the performance of the proposed method as well as some baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is interesting to borrow the idea of Dataset Distillation into federated model training to tackel the non-iid data and privacy issues.\n\nThe paper is overall clearly structured and easy to follow."
            },
            "weaknesses": {
                "value": "Though the idea is interesting, it seems to lack formal guarantees on the approximation achieved for each local synthetic dataset and how approximation level affects the learning performance in theory.\n\nAs for the DP side, the explicit trade-off between the privacy loss and the learning utility is also unclear for the proposed method.\n\nIn experiment, the performance on different settings with different heterogeneity can be further explored. And since you consider the record-level DP, the setting of privacy budget $\\epsilon$ is relatively large even for high privacy regime where $\\epsilon$ is set to be 2.79."
            },
            "questions": {
                "value": "1. Can you provide some formal theoretical guarantees for the proposed algorithm, e.g., approximation of the synthetic data learnt compared with the optimal one, and the learning performance, privacy-utility tradeoffs, so on.\n\n2. Is there any possiblity to extend the proposed method to client-level DP, which is very important in cross-device scenarios such as collaboration between massive IoT devices. If not, then what is the barrier for doing this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751101108,
        "cdate": 1698751101108,
        "tmdate": 1699636143492,
        "mdate": 1699636143492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0mUHRo9hOt",
        "forum": "wSWJpfUWdM",
        "replyto": "wSWJpfUWdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies federated learning with data condensation. In order to handle data heterogeneity, instead of sending local updates as in FedAvg, this paper proposes a method of sending synthetic data samples. Experiments show that the proposed method can improve model performance as well as reduce communication. To protect privacy, this paper use Gaussian mechanism to enforce record-level differential privacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is well written and easy to follow. Extensive experiments are conducted and the results look promising."
            },
            "weaknesses": {
                "value": "My biggest concern is that the novelty may be limited. Data condensation + FL has been well studied, e.g. [1,2]. In particular, using condensed datasets in FL has been discussed in [1]. The novelty of this work looks limited. It would be highly appreciated if the authors can show improvements over existing works in terms of communication, performance, etc. \n\n\n[1] Liu, Ping, Xin Yu, and Joey Tianyi Zhou. \"Meta knowledge condensation for federated learning.\" arXiv preprint arXiv:2209.14851 (2022).\n[2] Behera, Monik Raj, et al. \"Fedsyn: Synthetic data generation using federated learning.\" arXiv preprint arXiv:2203.05931 (2022)."
            },
            "questions": {
                "value": "For DP, the paper says \"we sanitize the gradients derived from real data with the Gaussian mechanism\"\u3002 Do you clip the gradients or do something to bound the sensitivity? How will this affect the model utility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824918759,
        "cdate": 1698824918759,
        "tmdate": 1699636143413,
        "mdate": 1699636143413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E5RFBAduIg",
        "forum": "wSWJpfUWdM",
        "replyto": "wSWJpfUWdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel federated learning (FL) algorithm, namely FedLAP-DP, to address the drawback of bias in global optimization in traditional FL. The approach involves generating synthetic data resembling real data on the client side and substituting local gradients with these synthetic samples during transmission to the central server to approximate the global loss landscape. The central server then iterates using these synthetic samples, thus mitigating the bias in global optimization. Additionally, differential privacy (DP) is employed to protect the privacy of synthetic data of clients. This idea is innovative, and the writing quality is also acceptable."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The approach proposed in this paper involves generating synthetic data resembling real data on the client side and then leverages the synthetic data to update the local model,  thereby reducing the negative impact of DP on the training.\n2. To control the communication cost, the size of the synthetic dataset is much smaller than the real client dataset. Thus, in the local training, the proposed approach leverages a small dataset to update a local model with a satisfactory performance.\n3. In addition to applying this approach to the DP setting, it can also address the issue of data heterogeneity in FL.\n4. Extensive experimental results demonstrate that FedLAP-DP outperforms the traditional approaches with faster convergence under the different DP settings."
            },
            "weaknesses": {
                "value": "1. I have a question regarding the generation of synthetic data and the iterations performed by the central server. In the case of non-iid data distribution, are the synthetic samples submitted by the clients to the central server consistent with the original data distribution? If so, referring to Algorithm 1, would there still be bias in the iterations conducted by the central server? Could you please provide a detailed explanation of the algorithm design on how the synthetic data is generated?\n2. Please further explain the parameters set that appeared in Sec.5 EXPERIMENT Part 5.1.\n3. It would be better to remark on each curve in the experimental figures.  Some notations are not clear.\n4. The baselines used in this paper are too simple. Some advanced DP-FL baselines should be included in this paper, such as [1], [2], [3] and etc.\n\nThe mentioned references are as follows: \n[1] Skellam mixture mechanism: a novel approach to federated learning with differential privacy\n[2] Dpis: An enhanced mechanism for differentially private SGD with importance sampling\n[3] PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation"
            },
            "questions": {
                "value": "The following comments should be addressed.\n1. I have a question regarding the generation of synthetic data and the iterations performed by the central server. In the case of non-iid data distribution, are the synthetic samples submitted by the clients to the central server consistent with the original data distribution? If so, referring to Algorithm 1, would there still be bias in the iterations conducted by the central server? Could you please provide a detailed explanation of the algorithm design on how the synthetic data is generated?\n2. Some notations are not clear. For example, I cannot see the effect of indexes j and l in Algorithm 1.\n3. Please further explain the parameters set that appeared in Sec.5 EXPERIMENT Part 5.1.\n4. It would be better to remark on each curve in the experimental figures. \n5. Some advanced DP-FL baselines should be included in this paper, such as [1], [2], [3] and etc. \n\nThe mentioned references are as follows: \n[1] Skellam mixture mechanism: a novel approach to federated learning with differential privacy\n[2] Dpis: An enhanced mechanism for differentially private SGD with importance sampling\n[3] PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831095412,
        "cdate": 1698831095412,
        "tmdate": 1699636143320,
        "mdate": 1699636143320,
        "license": "CC BY 4.0",
        "version": 2
    }
]