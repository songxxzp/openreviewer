[
    {
        "id": "H83vj1UbJI",
        "forum": "Or6xoO2END",
        "replyto": "Or6xoO2END",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_mXZv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_mXZv"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a theoretical framework for the convergence of refined diffusion models, focusing on the integral probability metric (IPM) and guided by the choice of a discriminator. It presents proof of an identity that quantifies the IPM between the distribution generated by the refined diffusion model and the data, taking into account the discrepancy between the distribution generated by the original diffusion model and the data, along with an additional discrepancy factor determined by the discriminator. The paper highlights that this approach demonstrates the convergence of the refined diffusion model towards the data while also linking it to the model's generalization power."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper provides an in-depth and rigorous theoretical analysis, convincingly demonstrating the convergence and generalization power of the refined diffusion model. Its theoretical results have broad applicability beyond the specific model discussed.\n\n(2) The logical progression of the paper is smooth and easily understandable, facilitating comprehension of the complex concepts presented."
            },
            "weaknesses": {
                "value": "(1) Although the theoretical results guarantee the convergence and generalization power of the diffusion model, it does not explain why the refined diffusion model is better than the non-refined diffusion model by comparing them with the same metric.\n\n(2) It would be better if the authors could provide guidance for choosing the discriminator based on their theoretical results."
            },
            "questions": {
                "value": "Could the author explain why finding $h^*$ in the optimization of $\\mathrm{D}_{f, \\mathcal{H}}$ is equivalent to a binary classification problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Reviewer_mXZv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724566648,
        "cdate": 1698724566648,
        "tmdate": 1699636311158,
        "mdate": 1699636311158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dJqsrAyXDa",
        "forum": "Or6xoO2END",
        "replyto": "Or6xoO2END",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_3Spm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_3Spm"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at explaining from a theoretical viewpoint the success of the generative model from Kim et al., which corresponds to a score-based generative model with discriminator refinement (SGM+D). Their theoretical contribution is threefold: 1) They derive a strong duality theorem (Theorem 1) that holds for specific distributions which recover the case of SGM+D. This strong duality theorem allows to analyse the IPM between the data and the refined distribution. 2) The authors provide an analysis of each term of the IPM.  They give bounds that depend on the main parameters of the generative model and data distribution (Lipschitzness of the underlying score function, second moment of the data distribution,  error of the score network, discretization error).  3) Finally, the authors use the strong duality to link the IPM between the target distribution and the SGM+D distribution to the Rademacher complexity of the discriminator's set."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* First theoretical analysis of the generative model from Kim et al. (SGM+D). Specifically, the authors provide bounds on the IPM between data and SGM+D that depend on the key components of the model and data. \n\n* Prove a strong duality theorem on quantities (IPM, f-divergence, lower-bound on f-divergence) that are widely used in generative models and that could be of interest in their theoretical analysis. \n\n* Give bound on the generalization tradeoff of SGM+D that depends on the capacity of the discriminator. It shows, for example, the potential benefits of using 1-Lipschitz discriminators (WGANs)."
            },
            "weaknesses": {
                "value": "* What we are really interested about, in generative models, is to minimize the distance/divergence/IPM $D(\\mu, \\nu)$, where $\\mu$ is the generative distribution and $\\nu$ the data (or empirical) distribution. In my understanding, the presented theoretical results do not prove that SGM+D is necessarily superior to SGM. Can you present a theorem that shows, in a general setting, that SGM+D will be closer to the data than SGM? This is not clear to me, since the strong duality holds for some specific distributions $\\mu^H$. However, in the paper, the authors claim that their findings advocate for discriminator refinement. Can you be clearer on this point? \n\n* The paper could better explain the impact of Theorem 1, i.e. that $\\mu^H$ is a minimizer of equation 11. Is this theorem only useful because it allows the decomposition and further analysis of the IPM?  \n\n* The paper does not motivate enough the impact of its theoretical results. Could you derive practical recommendations from your theory? Does it give particular insights or ideas of improvements for SGM+D models? For example, could you predict the impact of the choice of $f$ on the behavior of the generative model, whether in terms of distribution fitting or generalization? I noticed that you already comment on the discrimination-generalization tradeoff regarding the use of 1-Lipschitz discriminators, which is interesting but not really new. The impact of discriminator regularization is thoroughly studied in GANs' theory literature. \n \n* The solution of equation 11 is not a unique solution. The strong duality holds for $\\mu^H$, but what about other distributions that might achieve this strong duality? What could you say about other minimizers than  $\\mu^H$? On that matter, in conclusion, you write \"characterizing the exact distribution under which strong duality holds\". It is not exact and it should be clearly stated that you characterize \"an exact distribution\". \n\n* Structure/Clarity: part 5 is dense and should be split into subsections, or at least in distinct paragraphs, for improved clarity. For example, one subsection for the analysis of $D_{f,h}$, one for $I_f$, and one for the last part on generalization using Rademacher complexities. In the current version, the lack of structure makes the reading more complicated. \n\n* (Minor) Theorem 2 and Theorem 3 should rather be Corollaries of Theorem 1. Calling them theorems is overselling since they are direct applications/reformulations of Theorem 1. \n\n* (Minor) Some typos: abstract: \"the discriminator set plays a crucial role\"; bullet point Theorem 6, lacks \"of\" in \"the discriminator-generalization trade-off other generative models\"; incorrect use of \\citet and \\citep; \"Integration Probability Metrics\" instead of Integral, page 3 a few lines after equation 5; example 2: missing \"as\" in \"a parametrized model giving a softmax score such a neural network\"."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Reviewer_3Spm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746190972,
        "cdate": 1698746190972,
        "tmdate": 1699636311072,
        "mdate": 1699636311072,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MAkmyMDN9S",
        "forum": "Or6xoO2END",
        "replyto": "Or6xoO2END",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_G97H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_G97H"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalization properties of discriminator-guided score-based models, originally introduced by Kim et al. (2022a). By leveraging and refining a strong duality result involving IPMs and $f$-divergences, the authors manage to upper-bound the IPM between the true data distribution and the outcome of the diffusion process learned over a finite dataset. This generalization bound leads them to conclude that discriminator guidance in diffusion models is beneficial in terms of generalization."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper tackles an **interesting and relevant problem**: generalization of generative models. Generalization in this domain is often overlooked and, given the scarce literature on diffusion models more particularly, such new contributions are welcome. The choice of studying discriminator-guided diffusion is also relevant as it generalizes and improves standard score-based diffusion models.\n\nThe authors, to the best of my knowledge and understanding, **successfully derive a generalization bound**. They do so by deriving **new results** on strong duality between IPMs and $f$-divergences."
            },
            "weaknesses": {
                "value": "From my point of view -- disclaimer: with little background on generalization --, this paper is not ready for publication. It is hard to read and greatly suffers from a lack of clarity that prevents a proper appreciation of its contribution and soundness, thus motivating my \"strong reject\" recommendation. Therefore, I would recommend the authors to rewrite the paper to be more accessible and clear on its precise contributions. Nonetheless, I look forward to discussing with the authors and other reviewers on this topic.\n\n### Clarity of Exposition and Soundness of Claims\n\nBeyond the heavy notations which might be unavoidable given technicality of the paper, the paper is hard to read because of **organization and exposition issues**.\n\nDespite the outline at the end of the introduction, the structure of the paper is hard to follow. Since the main objective is not recalled clearly until the final generalization bound of Theorem 6, a non-expert in generalization will struggle to follow the reasoning of the paper. In particular, I feel that Section 5 needs an overhaul as it is the most difficult to follow: the intermediate steps do not appear clearly, leading the reader to be lost in the reasoning. This organization issue is aggravated by several lengthy digressions (like Savage\u2019s theory of properness and Example 1) which, while interesting and potentially valuable, distract the reader from the main message.\n\nMoreover, while the presented results may be interesting, the paper provides no clear interpretation which would allow the reader to conclude on their value and soundness. Reading Section 5 and in particular the final comments of Theorem 6, **I could not determine whether the results support the central claim of the paper**, from the abstract:\n> Our findings advocate for discriminator refinement of deep generative models and, more specifically, unveil the generalization effect of using regularized discriminators in this setting.\n\nThis is a crucial prerequisite for a potential publication of the paper.\n\nA typical example of this problem is Theorem 6. Firstly, since the choice of $\\mathcal{H}$ influences the IPM on the left-hand side, it is difficult to compare different choices of $\\mathcal{H}$. Secondly, edge cases should be discussed, like when there is no discriminator guidance (for e.g. $\\mathcal{H} = \\{0\\}$?), which would allow the authors to conclude on the advantage of using discriminator guidance for generalization. Finally, I would like the authors to clarify how this results \"close[s] the generalization gap\" (Section 1).\n\n### Resulting Concerns\n\nStemming from the above issues, I have several concerns on the contributions of the paper.\n\n#### Comparison to Prior Work\n\nThe paper scarcely discusses the only prior work in generalization of diffusion models by Oko et al. (2023). This discussion should be more developed and include a comparison to the obtained results, in order to understand their novelty.\n\nFurthermore, I find that the paper underplays in the introduction the original contribution of Kim et al. (2022) on discriminator guidance. They did theoretically show the relevance of their approach by showing that it closes a gap between the score estimation and the true score. While I believe that the results presented in the submission go further, I would like to better understand their added value.\n\n#### Theoretical Derivation\n\nThe validity of two important points in the theoretical derivation is unclear to me.\n1. At the beginning of Section 5, it seems implicitly accepted that $\\mu_{T, \\vartheta}^\\mathcal{H} = \\mathcal{L}(\\mathsf{Y}_T^{\\mathcal{H} \\vartheta})$ is the same as the $\\mu^{\\mathcal{H}}$ as in Theorem 2. Why is it the case?\n2. Assumption 1 does not seem valid, given that $\\hat{P}_0$ is a collection of Diracs and the density of $\\hat{P}_t$ will indefinitely peak when $t \\to 0$.\n\n### Minor Issues\n\n- The considered diffusion process of Equation (3) is restrictive as it is only one instantiation of more general equations, cf. Karras et al. (2022).\n- Theorem 2 is a direct application of Theorem 1, so maybe naming it \"Theorem\" is not appropriate.\n- Remarks on the form:\n  - The formulation of the sentence \"Despite the practical success...\" (p. 1) should be improved.\n  - The sentence \"In summary, our technical contributions come in three parts:\" (p. 2) is a duplicate of the next one.\n  - The acronym DRO (p. 2) is never specified.\n  - The paper uses the wrong reference for GANs (p. 2) in the related work.\n  - In differential equations, the differential $d$ should be upright for better readability.\n  - P. 4, \"a weaker divergences\" should be \"weaker divergences\".\n\nKarras et al. Elucidating the Design Space of Diffusion-Based Generative Models. NeurIPS 2022."
            },
            "questions": {
                "value": "Cf. the *Weaknesses* part of the review for questions related to paper improvements."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791901663,
        "cdate": 1698791901663,
        "tmdate": 1699636310995,
        "mdate": 1699636310995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1furwhivt9",
        "forum": "Or6xoO2END",
        "replyto": "Or6xoO2END",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_F2Tz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3564/Reviewer_F2Tz"
        ],
        "content": {
            "summary": {
                "value": "The main contribution of this paper is theoretical in nature. There"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides a rather interesting theoretical investigation into discriminator-refined diffusion models. The theoretical results are not too surprising but they are often taken as true in the current machine learning community. The investigation provided in the paper helps shed line for how and why these statements are true. Overall the theory in this paper is interesting and the work seems fairly compelling I think some of the theoretical techniques and results show in this paper could have benefits for other problems in ML."
            },
            "weaknesses": {
                "value": "The paper is rather dense and can be reworded a bit to make its contribution more known. The introduction starts off with a review of diffusions and GANs. A better starting point might be to start with the reasons for using discriminatory refinement for diffusion models in practice before going to how the theoretical results found could better justify and clarify the role of discriminator refinement. There are some definitions in the paper that might also need to be tightened for instance in the definition for IPM the set $\\mathcal{H}$ is not merely a set of functions but must obey some properties in order to be a proper metric, in particular, $f \\in \\mathcal{H} \\implies -f  \\in \\mathcal{H} $. This presents an issue for example 1 as the set choose $\\mathcal{H} = \\{ - L + b : b \\in R \\}$ is not a valid set so you cannot just apply theorem 1 directly. I am also thing some statements have some redundancy in them for instance in theorem 6 the function  $f : R \u2192 (\u2212\u221e, \u221e] $ is stated to be both be a strictly convex lower semi-continuous and differentiable function. When differentiable function implies continuity which implies lower semi-continuity."
            },
            "questions": {
                "value": "I wonder if there are any experiments to show the tightness of bound provided in theorem 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think this paper being theoretical in nature does not warrant any ethical concern."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3564/Reviewer_F2Tz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699227164931,
        "cdate": 1699227164931,
        "tmdate": 1699636310920,
        "mdate": 1699636310920,
        "license": "CC BY 4.0",
        "version": 2
    }
]