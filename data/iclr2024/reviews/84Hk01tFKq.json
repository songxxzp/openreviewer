[
    {
        "id": "RqnZwqwTzj",
        "forum": "84Hk01tFKq",
        "replyto": "84Hk01tFKq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
        ],
        "content": {
            "summary": {
                "value": "The paper presents \"HyperFields\", an approach designed to achieve zero-shot generation of Neural Radiance Fields (NeRFs) from textual prompts. By utilizing a dynamic hypernetwork, HyperFields is able to establish a mapping from text token embeddings, specifically derived from BERT, to the domain of NeRF parameters. The paper introduces NeRF distillation training, where individual NeRF-encoded scenes are distilled into a singular dynamic hypernetwork. The main goal of HyperFields is to efficiently generate scenes that the model has seen during its training (in-distribution), and to quickly fine-tune itself for unseen (out-of-distribution) prompts, if necessary."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) The paper addresses an important gap in zero-shot generation using NeRFs.\n\n(+) The presentation of the method is clear, with a well-structured explanation of the hypernetwork and NeRF distillation.\n\n(+) The paper introduces a fresh perspective on text-to-NeRF synthesis, leveraging hypernetworks, which is a less explored territory in this context."
            },
            "weaknesses": {
                "value": "(-) **Limited experimental results**: The experimental results presented in the paper, particularly in Figure 4 and Figure 5, are quite limited. The in-distribution generalization showcased in Figure 4 uses only 9 basic shapes, and the generalization is restricted to simple uniform color translations. Figure 5, too, is restricted to basic geometry and appearance.\n\n(-) **Low quality of results**: The results presented, especially the simple geometric shapes and uniform color distributions, seem to be of low standard. There is a noticeable disparity in quality when compared to state-of-the-art techniques. Moreover, the ablation study in Figure 6 only provides a single example, which weakens the overall argument.\n\n(-) **Lack of comparative discussion**: It's important to note that recent studies, such as HyperDiffusion (ICCV 2023) and Shap-E (arXiv), have also explored the use of hyperparameters in 3D object generation, resulting in promising results. However, there is a lack of comparative discussion with these methods, which is crucial in positioning HyperFields in the current research landscape."
            },
            "questions": {
                "value": "- In Section 4.2 (referencing Figure 5), the paper mentions, \"...with no optimization, when provided with the out-of-distribution prompt. The model chooses the semantic nearest neighbor from its training data as the initial guess for out-of-distribution prompts...\"; However, it is not clear how the model is capable of retrieving the nearest-neighbor. Could the authors provide more information on the intrinsic capabilities of the model that enable this nearest-neighbor retrieval?\n- How does HyperFields handle highly creative textual prompts?\n- How does the NeRF distillation process handle the potential loss of scene details? \n- It is important to have a discussion regarding the limitations of the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669235930,
        "cdate": 1698669235930,
        "tmdate": 1700738084639,
        "mdate": 1700738084639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8kPC8VBXvn",
        "forum": "84Hk01tFKq",
        "replyto": "84Hk01tFKq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_6JFZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_6JFZ"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a fast way to achieve text-to-3D generation. The key idea is to design a hyper-network that predicts weights that process the spatial latent code of NeRF. The technical contribution is that the hyper-network not only looks at the text input, but also the activations of the MLP to be predicted. Experiment results are shown with only combination of color and shape instead of a full prompt (possibly due to limited computational resources). Qualitative results with very limited examples show that the technique is able to improve text-to-3D speed and achieve certain generalizability toward new concepts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The hyper-network is fast since it\u2019s free of optimization. Such feedforward approach can have computational advantages as the training cost can potentially be amortized \n- The experiment shows that the model can compose concepts in certain fashion. This helps illustrate the benefit that such model can amortize training compute to be used for many inference uses."
            },
            "weaknesses": {
                "value": "- Current methods is trained from scratch, which might be computationally expensive. \n- The key architecture of this paper is a hyper-network that predicts the weights for the MLP. \n- A main concern regarding the result is very limited. Most of the results are shown in simple objects and compositions. \n- An other small concern is regarding the need to create a small dataset of NeRF scenes. Each NeRF can takes minutes, and this prevents it to scale to larger datasets.\n- If the model weight depends on where we sample the activations, then the generated weights can have high variance. I\u2019m a bit concerned that this means different ways to sample the points can lead to different weights, and thus leading to different performance."
            },
            "questions": {
                "value": "- \u201cSDS loss exhibits high variances\u201d - is there any experimental/reference evidence that support it?\n- Quality: Figure 4, why is the image looks washed out? Is it because of artifacts from normalization?\n- I wonder if the main difference between ATT3D and this paper is whether the hyper-network takes activations?\n- Why do we choose three captions during training? Why can\u2019t we spread away these captions throughout different batches?\n- Maybe I\u2019ve missed it, but how do we choose different a_i\u2019s?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812114158,
        "cdate": 1698812114158,
        "tmdate": 1699636746953,
        "mdate": 1699636746953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wvR7uLOxSf",
        "forum": "84Hk01tFKq",
        "replyto": "84Hk01tFKq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_ckby"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_ckby"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel generalizable text-to-3D framework to generate a 3D representation with neural radiance field (NeRF) given text inputs. The text inputs are processed by a pretrained BERT network to get powerful embeddings to condition the NeRF generation process. The dynamic hypernetwork is the key innovation of the design to make it succeed to generalize across different 3D shapes over various text conditions. Then a distillation loss is employed to train the NeRF image rendering process given 3D coordinates and synthesized 2D images from DreamFusion. This work will facilitate significant progress of 3D AIGC and inspire future explorations on generalizable 3D generation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1) The motivation of this work is clear and the technical impact of this work are significant. Limited by the intrinsic mapping relationship between 3D coordinate and color field of NeRF, current work struggles to achieve generalizable 3D shapes with various conditional input with a unified framework. This work proposes a hypernetwork architecture, which is justified as the key innovation to learn a generalized text-to-3D mapping across different inputs.\n(2) Authors conduct extensive experiments on both in-distribution and out-of-distribution samples during testing, and results look consistently appealing.\n(3) Benefit from the use of InstantNGP, the generation is much faster than the baseline DreamFusion, which is crucial for some real-time applications such as real-time rendering and editing.\n(4) Authors have committed that they will release all the code and pretrained models, which will facilitate better reproduction and follow-up for the community."
            },
            "weaknesses": {
                "value": "(1) There seems missing quantitative comparison between the proposed method and baseline (DreamFusion) on CLIP retrieval scores or user study, which may make the work further stronger and more convincing.\n(2) Another ablation study to conduct is to verify the effectiveness of training across multiple shapes then fine-tuning on a single shape, and compare it with a baseline that train the model on this single shape from scratch. This will demonstrate the advantage of training across a wider range of samples to learn a stronger representation between text condition and the 3D representation, by incorporating more samples during training."
            },
            "questions": {
                "value": "I may consider further improve my rating if my concerns (listed in the weaknessed part) are well addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814183945,
        "cdate": 1698814183945,
        "tmdate": 1699636746840,
        "mdate": 1699636746840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "alXHrMSMl3",
        "forum": "84Hk01tFKq",
        "replyto": "84Hk01tFKq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a framework for achieving zero-shot NeRF generation from texts. This is accomplished through the training of a dynamic hypernetwork using hundreds of pre-trained NeRFs to acquire the mapping from text token embeddings to NeRF weights. Extensive experiments demonstrate the capability of the proposed method to predict in-distribution scenes in a zero-shot manner and out-distribution scenes with a few steps of fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is clearly written and easy to follow.\n\n2. The proposed pipeline intuitively makes sense and it is interesting to see the disentanglement of different attributes learned by the proposed hypernetwork."
            },
            "weaknesses": {
                "value": "1. My primary concern is the technical contributions of this work in comparison to the referenced ATT3D study. Specifically, while this paper clarifies the connections with ATT3D, it remains unclear what novel techniques or insights are newly introduced by this work. A more compelling justification is highly desirable.\n\n2. Furthermore, there is a lack of benchmarking against ATT3D and the reported results indicate that ATT3D may achieve much better visualization effects. This discrepancy arises because the proposed method appears to only disentangle objects with simple attributes like colors, while ATT3D's reported visualizations can manipulate higher-level attributes and behaviors, such as \"chimpanzee holding a cup.\" The authors are highly expected to conduct a benchmarking comparison with ATT3D using the same text prompt.\n\n3. Although intriguing, it remains unclear why the hypernetwork can successfully learn the disentanglement of various attributes. This may be attributed to the limited scope of text prompts and attributes during training. The authors are expected to provide more insights on this matter."
            },
            "questions": {
                "value": "My questions are included in the weakness section. I am willing to adjust my scores if my concerns are properly addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698914759258,
        "cdate": 1698914759258,
        "tmdate": 1700782609431,
        "mdate": 1700782609431,
        "license": "CC BY 4.0",
        "version": 2
    }
]