[
    {
        "id": "Odsu7QsPA5",
        "forum": "s25i99RTCg",
        "replyto": "s25i99RTCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach to handle the issue, called the coherence-quality tradeoff, of the Multimodal VAE. Specifically, the authors use a set of independently trained, uni-modal, deterministic autoencoders. And, they introduce a multi-time training method to learn the conditional score network in the diffusion model, which enables multi-modal generative modeling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is clear and well-organized. \n2.\tThe appendix significantly enhances the paper by thoroughly supplementing the technical and experimental details."
            },
            "weaknesses": {
                "value": "1.\tIt is a nice try to facilitate multi-modal generative modeling by concatenating latent variables from different modalities and employing a diffusion model. However, the methodology of the paper presents numerous issues and lacks in-depth discussion. Furthermore, the method proposed principally depends on intuitive reasoning, with a noticeable absence of solid theoretical underpinnings. \n    \n    - The paper employs diffusion on latent variables concatenated from different modalities. One concern is whether the data distributions corresponding to vectors from these various modalities significantly diverge. If so, does utilizing the same noise schedule could result in a lack of synchronization between the denoising and noise-adding processes across the different modalities? \n    - Additionally, the optimization speeds of different modalities may inherently vary, leading to discrepancies in performance. In other words, the proposed method might achieve satisfactory results with simplistic datasets, but training becomes substantially more challenging when scaled to extensive, real-world data scenarios.\n    - In the context of conditional generation, the authors employ a masking technique to generate the desired modality based on the known one. However, the question arises: how is the intensity of the conditions controlled? This aspect is crucial for ensuring the effectiveness of the generative process.\n    - Within the model's framework, a discrepancy arises between the training and inference stages in terms of the number of modalities. For instance, during training, the model might handle three modalities: A, B, and C. However, in a scenario where inference is desired based solely on modality A to predict B, would a masked C still be necessitated? This raises questions about the model's flexibility and its adaptability to accommodate various generative scenarios with different modalities. The capacity to dynamically adjust to these conditions without compromising the integrity of the generative process is pivotal.\n    \n2. Some recent work, such as MMVAE+(Palumbo et al., 2023), should be included as a baseline. Work parallel to this paper, score-based multimodal autoencoders (Wesego et al., 2023), should be discussed in Section 2.\n\n3. The experiments require further refinement. \n\n   - The quantitative comparisons on the CUB dataset should be integrated into the main text. Moreover, the coherence metric for image->caption has not improved, necessitating a comprehensive comparative analysis and case demonstrations of caption generation. Additionally, the visual representations in this section of the paper are quite unclear, making them difficult to interpret. \n   - Considering that the differences in certain metrics on the CUB dataset are not particularly pronounced, it is recommended to augment the study with a comparative analysis on the Bimodal CelebA dataset.\n   - The authors have not made their code available, which makes it hard to reproduce the experimental results.\n\n\n**Reference:**\n\n[1] Palumbo, Emanuele, Imant Daunhawer, and Julia E. Vogt. \"MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises.\" Fifth Symposium on Advances in Approximate Bayesian Inference-Fast Track. 2023.\n\n[2] Wesego, Daniel, and Amirmohammad Rooshenas. \"Score-Based Multimodal Autoencoders.\" arXiv preprint arXiv:2305.15708 (2023)."
            },
            "questions": {
                "value": "Please refer to the specifics outlined in the \u201cWeaknesses\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5378/Reviewer_SqcE",
                    "ICLR.cc/2024/Conference/Submission5378/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698387445011,
        "cdate": 1698387445011,
        "tmdate": 1700637090934,
        "mdate": 1700637090934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cYk6WzcAg9",
        "forum": "s25i99RTCg",
        "replyto": "s25i99RTCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_865d"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method for conditionally generating multiple modalities of data which allows for certain modalities to be generated while conditioned on other existing modalities (e.g. generating images and audio from text). In contrast with VAE-based approaches, which can suffer from information loss due to the explicit separation of distinct modality subsets, the authors propose their method MLD, which avoids the problem by training separate unimodal autoencoders _deterministically_, and allowing a diffusion model to learn conditional generation of each modality\u2019s latent space. The diffusion model is trained to be conditioned on random subsets of modalities so that it remains robust to conditioning on any subset. The authors then compare MLD on several datasets against multi-modal VAE approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Good comparisons to multi-model VAEs with encouraging results\n\nThe authors do a good job of comparing their method MLD to other VAE-based works, and their results on their datasets are encouraging.\n\n### Good comparison in-painting and explanation for why it works less well\n\nThe authors also preemptively address a very natural question of why in-painting and cold diffusion might work less well compared to their approach of robustly training the diffusion model to be conditioned on different subsets of modalities. Their ablation study empirically justifies this claim, or at least it shows that in-painting is not significantly better than their proposed method."
            },
            "weaknesses": {
                "value": "### No comparison to multi-modal diffusion models\n\nAlthough the authors have done a good job comparing to previous VAE-related works for multi-modal generation, there is no comparison with purely diffusion-based works. For example, the rather popular Any-to-Any Composable Diffusion (CoDi) (Tang, et. al., 2023) work is very closely related to MLD. CoDi also attempts to solve the multi-modal generation problem. Like the proposed method, CoDi performs diffusion in latent space and allows for conditioning on arbitrary subsets of modalities. Conditioning is done through \u201clatent alignments\u201d, where the latent space of some modalities are attended to by generated modalities. To tackle the problem of coherence, CoDi performs \u201cbridge alignments\u201d to pre-align the latent representations of each modality. Since this work is so similar in methodology to the proposed work here, it should be benchmarked against, as well.\n\n### Datasets benchmarked against are somewhat limited\n\nThe datasets in this work are fairly small (the MNIST datasets). The CUB dataset is larger, but only has two modalities. Since one of the core claims of this paper is successful multi-modal generation for arbitrary _subsets_ of modalities, this paper would be much stronger if it could also show MLD working well on another large dataset with more than two modalities (e.g. videos with audio and text).\n\n### More background on multi-modal VAEs would be nice\n\nDiffusion models are fairly common and well known at this point, but multi-modal VAEs are less well known (in my opinion). It would have been nice to have more background on how multi-modal VAEs work before describing their limitations.\n\nA main figure which illustrates the structure of these multi-modal VAEs in comparison to the proposed method would also be very helpful.\n\n### Some of the equations and math could be clearer\n\nOftentimes, there are equations presented which are presented without much explanation of each component (e.g. Equation 4, Equation 6, Equation 7). These can be a bit confusing to go through when there are simple English descriptions that can be offered instead (or certainly in conjunction) (e.g. \u201ckeeping the modalities in $A_1$ static throughout the forward and reverse diffusion process\u201d). These equations should be explained in more straightforward English or even replaced with English descriptions, because the equations do not aid in additional understanding of the paper\u2019s contributions.  Other equations like Equation 5 are certainly not needed for the understanding of this paper, since the modification onto diffusion-model training is very minor."
            },
            "questions": {
                "value": "Minor grammatical suggestion: there should be an en-dash in \u201ccoherence\u2013quality tradeoff\u201d, not a hyphen."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685822662,
        "cdate": 1698685822662,
        "tmdate": 1699636543425,
        "mdate": 1699636543425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U24V2XnuL6",
        "forum": "s25i99RTCg",
        "replyto": "s25i99RTCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_yi7k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_yi7k"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenges associated with multi-modal generative modeling, a domain that focuses on generating data across multiple modalities such as images, text, and audio. The primary concern is the coherence-quality tradeoff observed in existing Multi-modal Variational Autoencoders (VAEs), where improving generative coherence across modalities might compromise the generation quality and vice versa.\nTo tackle these challenges, the authors introduce a novel approach named Multimodal Latent Diffusion (MLD). Unlike traditional multi-modal VAEs that often suffer from latent collapse and information loss, MLD employs independently trained, deterministic uni-modal autoencoders. Each modality is encoded into a specific latent variable, and these variables are then concatenated. The joint data generation is facilitated by a score-based diffusion model in the latent space, which reverses a stochastic noising process starting from a Gaussian distribution.\nThe experimental results are promising, outperforming the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The introduction of the Multimodal Latent Diffusion (MLD) method offers a new perspective on multi-modal generative modeling.\n- The paper effectively tackles the coherence-quality tradeoff observed in existing multi-modal VAEs.\n- The inclusion of experimental results provides concrete evidence for the paper's claims, underscoring its quality and relevance."
            },
            "weaknesses": {
                "value": "- Clarity in Presentation: While the paper is comprehensive, certain sections, especially those with plenty of mathematical formulations, might benefit from further simplification or more intuitive explanations for a broader audience.\n- Dataset Diversity in Experiments: The current experiments focus primarily on simple or low-resolution datasets. Would the MLD approach's efficacy be consistent when tested on more popular, high-resolution datasets? Expanding the experimental evaluation to include such datasets might enhance the paper's applicability and appeal to the broader research community.\n- Real-world Applications: The paper could be enriched by providing more real-world applications or use-cases to showcase the practical significance of MLD."
            },
            "questions": {
                "value": "From what I've gathered, the MLD approach involves concatenating latent vectors derived from several uni-modal autoencoders. Following this, a diffusion model is trained within this combined latent space. Subsequently, a mechanism is introduced to facilitate conditional generation. Based on my understanding:\n1. How is the architecture of the autoencoders, such as the image AE and text AE, designed? In the context of stable diffusion, pure convolutional networks are utilized for both encoding and decoding. Does the MLD approach adopt a similar design for images?\n2. How have you determined the latent dimensions for each modality, and what criteria influenced your decision on their dimensionality?\n3. How scalable is the MLD approach when dealing with a large number of modalities or high-dimensional data within each modality?\n4. Given the independent training of uni-modal autoencoders, how do you ensure that the concatenated latent space is cohesive and meaningful? Are there any challenges in ensuring convergence during training?\n5. You mentioned you used 4 A100 GPUs for a total of roughly 4 months of experiments. Could you give more details about the training?\n6. Does your implementation yield results that are in line with those presented in the original baseline studies?\n7. The authors appear to have deviated from the official Style files and Templates as provided by the ICLR 2024 Call for Papers (https://iclr.cc/Conferences/2024/CallForPapers). Notably, there are discrepancies in the citation format."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694932515,
        "cdate": 1698694932515,
        "tmdate": 1699636543326,
        "mdate": 1699636543326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rEoAsPfSdc",
        "forum": "s25i99RTCg",
        "replyto": "s25i99RTCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_mRTE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5378/Reviewer_mRTE"
        ],
        "content": {
            "summary": {
                "value": "this paper focuses on multi-modal image generation. the definite of multi-modal is multiple dataset distributions including text and image.  the proposed method is based on latent diffusion and the authors proposed som modification to make it work on multi-modal datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper focuses on classical problem in machine learning: multimodal dataset.\n* the proposed method works better than other competing methods"
            },
            "weaknesses": {
                "value": "* the results dont look very good. e.g., MLD painted birds in Fig 22. \n* the paper lacks a overall diagram showing the whole model design. its' a bit difficult to understand the model design"
            },
            "questions": {
                "value": "the other exciting methods seem quite weak. e.g., in Fig 20, MVAE cannot even generate digits very well, and in page 22, MVAE and MOPOE can't generate legible birds at all. are theses meaningful benchmark methods in 2023?\nare there strong methods the author can use?\n\nhow is text generation done in the proposed method? I assume in figure 22, the models generated both images and text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816340169,
        "cdate": 1698816340169,
        "tmdate": 1699636543206,
        "mdate": 1699636543206,
        "license": "CC BY 4.0",
        "version": 2
    }
]