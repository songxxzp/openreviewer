[
    {
        "id": "LpzqvR1hSR",
        "forum": "MY0qlcFcUg",
        "replyto": "MY0qlcFcUg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_wtsR"
        ],
        "content": {
            "summary": {
                "value": "In this paper, diffusion training is cast as multi-task learning, where each task corresponds to the denoising task at a specific timestep. The authors present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. Besides, the channel partitioning considers task affinity and task weights in diffusion models. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a simple add-on strategy for existing diffusion model architectures, which is simple yet effective, without introducing additional parameters, and contributes to accelerating convergence during training.\n2. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method.\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Some advanced routing methods [1, 2] improve the random routing by considering the inter-task relationship. Hence, it is better to discuss and compare the proposed method with them.\n\n2. In Figure 9, the images generated by the baseline (the first row) look very strange and both R-TR and DTR methods alleviate it (the second and third rows). So why the random routing method can work well? In particular, in the fifth case/column, the image generated by R-TR looks better than the one generated by DTR. Why?\n\n[1] Pascal et al. Maximum Roaming Multi-Task Learning. AAAI 2021.\n\n[2] Ding et al. Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. CVPR 2023."
            },
            "questions": {
                "value": "Please address my concerns in the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628578245,
        "cdate": 1698628578245,
        "tmdate": 1699636882757,
        "mdate": 1699636882757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jyJlvZpKo8",
        "forum": "MY0qlcFcUg",
        "replyto": "MY0qlcFcUg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. The authors incorporate two prior knowledge aspects of diffusion-denoising tasks\u2014task affinity and task weights\u2014into the model architecture design to mitigate the negative transfer phenomenon. The paper provides empirical results on several image generation tasks and a qualitative analysis to validate the effectiveness of DTR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper effectively addresses the negative transfer phenomena by establishing task-specific pathways for multiple denoising tasks. The concept of integrating key prior knowledge in diffusion and task routing is well-presented and could potentially influence future work on architecture design in diffusion models.\n* The implementation, although simple, is effective and yields significant performance gains on multiple benchmarks.\n* The paper is structured well, making it easy to understand and follow."
            },
            "weaknesses": {
                "value": "* The empirical analysis could be more comprehensive in decoupling the contributions of task weights and task affinity. As I understand, the results in Figure 4 only ablate the significance of the synergy of the two priors. To study the direct contribution of **Task Weights**, it would be helpful to compare `DTR with random routing but task-dedicated allocation channels` with `Random Task Routing (R-TR)`. Similarly, to study the contribution of **Task Affinity**, a comparison between `DTR with task-unified allocation channels but sliding window channels` and `R-TR` would be useful.\n* The paper does not adequately explain the sensitivity of DTR to different masking strategies. The authors should elaborate on why they chose Equation (4) as the masking strategy and discuss potential alternatives."
            },
            "questions": {
                "value": "* Would DTR achieve better performance by reducing the overlap channels at higher timesteps? Given the authors' assertion in Figure 6 that \"at higher timesteps, the model primarily focuses on learning discriminative features that are relevant to specific timesteps, whereas at lower timesteps, the model tends to exhibit similar behavior across different timesteps,\" it appears that denoising tasks at higher timesteps have less correlation. Would assigning these tasks entirely distinct channels be beneficial?\n* Is there a typographical error in Equation (4)? Should the first $t$ be replaced with $t-1$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7379/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7379/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_VZ4w"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822517465,
        "cdate": 1698822517465,
        "tmdate": 1700728994267,
        "mdate": 1700728994267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "67iJkjCxQQ",
        "forum": "MY0qlcFcUg",
        "replyto": "MY0qlcFcUg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Denoising Task Routing (DTR), an add-on strategy for diffusion models that incorporates multi-task learning (MTL). The proposed channel masking strategy effectively boosts performance without introducing any extra parameters. The experiments demonstrate consistent improvement across evaluation protocols."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed routing mask strategy is interesting as it leverages the task similarity between adjacent timesteps. \n\n2. The experiment conducted in this study is comprehensive and demonstrates significant performance improvement."
            },
            "weaknesses": {
                "value": "1. The idea of considering diffusion models as multi-task learning has previously been proposed by Hang et al. (2023) and Go et al. (2023a). The proposed masking strategy in this work is a simple modification of TR (Strezoski et al., 2019). Its novelty is limited.\n\n2. It lacks an ablation study to evaluate the necessity of the proposed masking strategy. Ding et al. (2023) propose to divide channels into shared channels and task-specific channels. Assigning each time-step cluster (Go et al., 2023a) to the respective task-specific channels can serve as an important baseline."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7379/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7379/Reviewer_8VoW",
                    "ICLR.cc/2024/Conference/Submission7379/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698923124240,
        "cdate": 1698923124240,
        "tmdate": 1700723367840,
        "mdate": 1700723367840,
        "license": "CC BY 4.0",
        "version": 2
    }
]