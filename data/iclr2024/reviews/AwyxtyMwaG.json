[
    {
        "id": "tyAnknWDAn",
        "forum": "AwyxtyMwaG",
        "replyto": "AwyxtyMwaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission441/Reviewer_baEP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission441/Reviewer_baEP"
        ],
        "content": {
            "summary": {
                "value": "The authors devise a simple way to extract compact task representations from LLMs. They further investigate how well these task representations generalize and whether these representations can be meaningfully composed to create new task representations (i.e. whether vector-algebraic operations on them are meaningful)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses a highly relevant topic.\n- Experimental support is sufficient.\n- The findings are likely to be of broad interest.\n- The paper is clearly written and is a pleasure to read."
            },
            "weaknesses": {
                "value": "- I feel that the contribution is not sufficiently clearly placing itself in the context of existing work. In particular, the \"related work\" section seems brief and insufficient.\n\nThere is extensive work on distributed, composable and generalizeable task representations (e.g. Lampinen, A. K., & McClelland, J. L. (2020). Transforming task representations to perform novel tasks. Proceedings of the National Academy of Sciences, 117(52), 32970-32981, but see related work also).\n\nI believe that the impact of the present contribution on our understanding of vectorized task representations (both in LLMs and more generally) is not discussed in sufficient depth. It also seems important to broaden the discussion a little, to include not only work on task representations, but also discuss works that looked into the roles of different attention heads in general (see e.g. Clark, K., Khandelwal, U., Levy, O., & Manning, C. D. (2019). What does bert look at? an analysis of bert's attention. arXiv preprint arXiv:1906.04341.). I.e. why they overlooked/were unable to interpret attention heads as encoding specific tasks. Is the proposed probing method superior, or is it because nobody cared to look, etc.\n\n- As a related issue, I think it is crucial for the authors to explain why the results are important. Currently, the authors say \"Our study of function vectors differs qualitatively from these previous works: rather than training a model to create function representations, we ask whether a pretrained transformer already contains compact function representations that it uses to invoke task execution\". In a way, I feel that this is underselling their work.\n\nIt is clear without doing any research that pre-trained LLMs must have some form of task representations (otherwise they won't be able to do zero/few-shot learning). I think that developing a way to extract those representations is valuable, but a more thorough/careful discussion is crucial to highlight why this is important and how it advances existing work."
            },
            "questions": {
                "value": "I might have missed it, but I wonder if the authors could provide a metric for the \"proportion of task representation encoded by FVs\". One potential way to visualize that would be to add lines to figure 4 representing best ICL performance, similarly for table 2, a few-shot baseline column would be very helpful. In other words, I do not fully understand how much task information we lose when we move from ICL to FVs.\n\nA minor suggestion on presentation:\nI feel that figures 1 and 2 are introduced much too early. They are confusing without proper context, which is given much later."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Reviewer_baEP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696026413,
        "cdate": 1698696026413,
        "tmdate": 1699645757491,
        "mdate": 1699645757491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KEzB3wut4u",
        "forum": "AwyxtyMwaG",
        "replyto": "AwyxtyMwaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission441/Reviewer_ZAgB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission441/Reviewer_ZAgB"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the way that tasks are encoded in LLMs, and find that there are a small set of attention heads which can be used to transport the task from in-context learning to unrelated prompts. They refer to these as \"function vectors\", and they find that these vectors can be used to encourage the model to perform tasks in a zero shot manner. The results support that FVs can be used in a variety of contexts to invoke the task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is a strong contribution towards understanding LLMs in terms of their ability to both encode and respond to ICL task information. The paper is fairly easy to follow: the motivation is clear as LLMs are somewhat of a black box, and I believe this paper has framed their study in the context of something easy to digest (ICL tasks) with clear experimentation. This work has significance as I believe that understanding some of the mechanisms of LLMs and possibly manipulated them for ZS performance will be useful in a number of fields that use LLMs. The approach is fairly creative yet intuitive: I appreciate the way they studied which heads have the most casual influence on tasks, and didn't have any issues understanding the motivations behind how they derive FVs."
            },
            "weaknesses": {
                "value": "There are a few things I'm a bit unclear about, notably that there is some back and forth on using all heads versus only manipulating the last head (am I understanding that correctly?). Notably, FVs appear to be a vector over tasks: can you describe again how these are applied to the LLM to invoke task behavior? Do you mean that we are only looking at the attention heads to the last token (last column)? Then you sum over these heads for each task and add it to the last hidden state?\n\nPerhaps it would be best to have a diagram, but it seems like some other choices on how to transport the task information to unrelated contexts would have been possible. Could we have not just added the task value (without summing in equation 5) to each attention head leading to computing the next hidden state?\n\nAlso, why compute the top attention heads over all tasks and not just have a different distinct set for each task?"
            },
            "questions": {
                "value": "Besides the questions above:\n\nIn the intro, I'm not sure a lot of readers will understand the connections to lambda calculus, so I wonder if there's a better way to start of this work.\n\nIn 2.2 J is mentioned but not defined: Number of heads?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698973472810,
        "cdate": 1698973472810,
        "tmdate": 1699635970631,
        "mdate": 1699635970631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6Qv7kx00La",
        "forum": "AwyxtyMwaG",
        "replyto": "AwyxtyMwaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission441/Reviewer_cNFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission441/Reviewer_cNFK"
        ],
        "content": {
            "summary": {
                "value": "This paper works on the analysis of transformer language models. It investigates the information of attention heads and identifies a small number of heads to construct the proposed Function Vector that contains the most causal information. To evaluate the ability of the Function Vector, the paper constructs a set of composable ICL tasks and shows that such a vector not only has some semantic abilities like word embeddings but also has some calculation abilities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper does some deep investigation into transformer language models, especially large ones, which can further help us better understand how they work.\n* The structure of the paper is clear and is to follow. Figures in the paper are drawn well.\n* Experiments are done on different model sizes and model structures. The paper shows great performance improvement of FV compared to Layer Avg."
            },
            "weaknesses": {
                "value": "* Though the paper mainly investigates attention heads, I\u2019m still wondering why attention heads contain such information rather than the MLP layers. I find some papers [1], and [2] seem to find that MLP is more critical from the information or memory aspect. Can the paper discuss more on this part, which would better explain the proposed findings?\n\n  [1]. Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space\n\n  [2]. MASS-EDITING MEMORY IN A TRANSFORMER\n\n* I think the step from identifying useful attention heads to the proposed FV vector sounds empirical. For example, can the author explain how to determine the number of selected heads? I find that the paper uses |A| = 10 attention heads for GPT-J and scales the number for larger models. However, this point including the scaling and the initial 10 heads for GPT-J needs more explanation. Also, can the paper give more description about directly summing the head outputs together as defined in Eq. (5)? Has the paper compared with other ways like summing them up with different weights?\n\n* For experiments, I find though the paper tested 40 tasks, they are very simple. Thus, can the paper think of some more difficult ones? Meanwhile, I think if the paper can add some analyses about the FV ability under model scaling laws, it can be better."
            },
            "questions": {
                "value": "Please check the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Reviewer_cNFK",
                    "ICLR.cc/2024/Conference/Submission441/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699475481239,
        "cdate": 1699475481239,
        "tmdate": 1700518842255,
        "mdate": 1700518842255,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O8JWPmJLZa",
        "forum": "AwyxtyMwaG",
        "replyto": "AwyxtyMwaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission441/Reviewer_9YEq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission441/Reviewer_9YEq"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the idea of function vectors, which appear to encode information about a task or transformation to be performed on a word to produce a response word under the task.  Function vectors are obtained by summing the averaged outputs of attention heads at the position of a test task input for heads that individually play relatively large roles across a set of tasks, where the average is obtained from several 10-shot prompts derived from examples of performance of the task.  Even though the attention vectors come from heads sprinkled across layers of a transformer, the resulting function vector can induce an increased tendency to give a task appropriate response when added to the hidden state of a transformer at many of the early layers of the transformer.  Experiments demonstrate robustness of the effects of FV's; show that FV's can sometimes be composed to cause a model to perform a composite task; and provide some evidence that the FV's are doing more than simply specifying the space of possible responses that are consistent with the task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides an example of an way to probe transformer function that adds to the toolkit for causal analysis of how transformers perform tasks.\n\nThe findings are suggestive of aspects of the way in which in context examples give rise to task performance.  In particular, it suggests that a relatively small number of attention head play a relatively crucial role in inducing a transformer to produce any one of many different transformation tasks.  It is also interesting that the FV can be added into just one layer out of ~30-80 layers of a deep transformer and have a strong effect on its tendency to perform the given task.  It seems to have implications for how we understand the state-to-state transitions across layers in the network -- ie that they are in someways roughly interchangeable at least across many layers.\n\nThe paper uses a diversity of tasks, model types, and base prompts to assess the effects of FV's.  Despite considerable variability in exact magnitudes of effects, the basic effect holds up across all of these manipulations."
            },
            "weaknesses": {
                "value": "The paper is relatively phenomenological rather than mechanistic; we see that there effects, but there is relatively little analysis of how they actually occur.\n\nMore specifically, I thought that the mechanism of action of the FV's could be more forcefully addressed.  At least for some tasks, it is possible that the FV's work by simply specifying the set of responses consistent with the task. E.g. Country-Capital a sufficient mechanism would be the following set-intersection account:  An input country name (France) specifies a set of words related to France; the FV specifies the set of country names; and the correct response is simply their intersection. I recognize that this would not work for all tasks, but it would word for many.  The paper could be strengthened by a detailed analysis of a selected instances of both kinds of tasks.  Some of the tasks specifically appear to require that the FV, if effective, specifies not just how to transform the input, but how to select a response from a string of preceding elements.  None of the 6 tasks selected were of this type, but there were such tasks in the full set.  Highlighting and analyzing these would strengthen the paper. I discuss this further in some suggestions below.\n\nThere were also some details that I would like greater clarity about.  I list these in questions."
            },
            "questions": {
                "value": "Questions:\n\nThe text says 'We can then test the causal effect of an FV by adding it to hidden states at any layer \u2113 as the model resolves a prompt and measuring its performance in executing the task.' I don't understand exactly how the vector is being 'added to hidden states at any layer'. Am I right that this vector is being added to the final state vector at the top of the lth transformer block?  How is this done given that this state vector typically has dimension equal to the number of heads times the dimensionality of a single head?\n\nHow was the 'direct decoding' of FV's performed?  What is the 'optimization' used 'to reconstruct a \u02c6vtk that matches the distribution Qtk when decoded'?\n\nHow was the approach used to determining the FV selected?  It would be informative to understand alternatives you might have tried.  What happens if the FV is the SET of averaged a_lj, with each inserted into the appropriate l,j position?  If this isn't better than 'adding the LV to the hidden state at layer l', why not?\n\nSuggestions:\n\nIt seems to me that a stronger analysis could arise from an examination of how the FV changes the distribution of logpobs of relevant words relative to the baseline prompt, and how this varies as the FV is inserted at different levels.  This approach could be used both for tasks where the set-intersection account could be sufficient, and for tasks where it cannot.\n\nThe paper would be strengthened if the authors could provide some results from carefully constructed test sets in which the set of outputs is the same set as the set of inputs (e.g matched antonym and synonym pairs, months, days of week, letters of the alphabet, progression of planets, chemicals in the periodic table) afford possibilities.\n\nIt might be even more useful to understand tasks in which the task response depends strictly on the words in the context, and we are assured that these words did not enter in to the construction of the FV.  For example, a set of n10-shot prompts involving a random subset chosen from a larger set of words could be used to construct FVs for the m 'chose pth word from list of length m' tasks. Then a test set of word lists of length m could be formed from a sample from the remaining words in the larger word set, and these could be factorially combined with the FV's for each of the values of p in m.  Clearly in this case, the FV's cannot themselves activate the set of answer responses -- if anything they'd activate words in the lists used to form the FV's.  So they must be *inducing* later heads to attend to the correct p in the given list in this case.\n\nConstructing test sets in which x and y come from the same set of words could be helpful.  Antonym and synonym tasks could address this if so constructed, but little detail was provided about the sets of x and y items used, and I did not see a description of explicitly reversed stimulus-response pairs.  (It is possible, for example, that the antonyms tended to be the negative member or less advantaged member of a pair, as the list of decoded words suggests).  I recognize that the full set of tasks include some that make this possibility unlikely (specifically, the chose-item-in-a-specific-list position), since (although not described) it seems likely that x is something uninformative like 'answer'."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission441/Reviewer_9YEq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699529187088,
        "cdate": 1699529187088,
        "tmdate": 1700621736224,
        "mdate": 1700621736224,
        "license": "CC BY 4.0",
        "version": 2
    }
]