[
    {
        "id": "MZJoxtJytr",
        "forum": "62K7mALO2q",
        "replyto": "62K7mALO2q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_rqBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_rqBo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a cognitive interpretability framework (IMO, a Bayesian model selection framework similar to those used in iterated learning or rational speech model) to analyze the dynamics of LLM conducting in-context learning (ICL). The analysis can help us understand what the latent concept is (it seems hard to understand this term without reading [1]) in ICL. Specifically, the paper considers a random binary sequence-generating problem and GPT series models for experiments. They find the model exhibits very interesting behaviors, e.g., it matches the Bayesian hypothesis selection process well; there is a sharp phase change when manipulating the prompting input length, indicating specific hypothesis becomes dominant, etc. Although the paper studies a very important problem by combining results in different fields, I cannot give a clear-cut conclusion of what is the core contribution of this paper. It would be helpful if the authors could highlight some core concepts in a relatively simple way and explain the results more. Also, I have several concerns about the technique details of the analysis (see the weakness part). After reading the paper several times, I believe the paper has big potential. I would give the current version a 3, but I would be happy to increase my score if my concerns are tackled during the rebuttal.\n\n[1] Xie, Sang Michael, et al. \"An explanation of in-context learning as implicit Bayesian inference.\" ICLR 2022"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is an interesting interdisciplinary study of deep learning and cognitive science, which is quite novel to me. The results of the simple binary sequence are quite persuasive."
            },
            "weaknesses": {
                "value": "1. The paper is very hard to follow. Section 1, 2, and 3 provides the background, preliminaries, and the system setting of the paper, but I find it hard to extract the system setting of the paper. Most of the text in this part discusses some insights and hypotheses from related works in these two fields. It would be helpful to squeeze this part and discuss the related works in another section.\n2. The results on binary sequence are good, but it is not sure whether the findings can be extended to other realistic tasks. It would be helpful if the author could verify these findings in more real datasets, e.g., math problems, question answering, etc.\n3. There is not so much discussion about the Bayesian model. Going deeper in this direction will help the paper a lot."
            },
            "questions": {
                "value": "1. For Figure 2, what do x (green), LLM (yellow), and ICL (purple) mean? There is no such color in the figure.\n2. I feel confused about the notation $p(y=Random | x)$, I guess $y\\in[ H, T ]^n$ or $y\\in [ 0, 1 ]^n$ is the output sequence. Should it be $p(h=Random | x)$?\n3. In section 4, the notation of $\\bar{x}_ {t-w\u2026t}$ in $p(y|x)$ is not quite clear. I guess it should be $\\bar{x}_ {t-w,\u2026,t}$.\n4. Figures like Figure-5-right are quite hard to understand. What is the take-away message? Is this saying GPT performs more similar to the window average model and less similar to the Bernoulli model? (in terms of how wide the curves spread around the converged mean)\n5. In the last paragraph of section 4, what is the $C$ in $x=C^{|x|}$ and $y_t\\in C$ means. Where is $C$ defined? (I can find $c$ and $\\mathcal{C}$ in section 2, but cannot find $C$)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Reviewer_rqBo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697600745053,
        "cdate": 1697600745053,
        "tmdate": 1700372039867,
        "mdate": 1700372039867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3KnN3SW0CZ",
        "forum": "62K7mALO2q",
        "replyto": "62K7mALO2q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_F7kD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_F7kD"
        ],
        "content": {
            "summary": {
                "value": "This paper studies in-context learning (ICL) of LLMs with experiments inspired by subjective randomness.\n\nSubjective randomness is concdididerned with the human perception of randomness. A main takeaway of this framework is that strings are more likely to be perceived as random if they can be generated by a short (deterministic) program. More formally, the subjective randomness of a string can be defined as the difference between the log-likelihood of it being generated by uniformly at random, and it being generated by a determinstic program. As the former quantity is fixed (for a given length), the subjective randomness is proportional to the Kolmogorov complexity of the string.\n\nThis paper presents experiments evaluating the ability of several GPT-3.5+ models (hereafter, GPT) to generate seemingly random strings, and distinguish uniformly random strings from deterministicly generated strings. These experiments are carried out by explicitly prompting the model to accomplish each task.\n\nIn the generation experiments, the authors find that the statistics of strings generated by GPT differ from uniformly random. For example, the distribution of the running average bit is tighter around the mean as compared to uniformly random strings (Figure 5); more specifically, GPT avoids long single-value runs, i.e., exhibits the gamber's fallacy (Figure 6). Furthermore, strings generated by GPT have simple sub-sequences (as measured by Gzip file size or Levenshtein (aka edit) distance).\n\nFor the distinguishing experiments, GPT is prompted with strings of increasing length, generated either uniformly at random or from a simple regular language (e.g. (01)^n, or (010)^n). The authors find that each language has a sharp phase shift in GPT's distinguishing ability: for example, when feeding contexts from (01)^n, the likelihood that GPT predicts the context to be determinstically-generated increases from ~0.5 (a random guess) at n=3, to ~0.8 at n=4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important and formidable question: what is really happening in ICL? Considering the complexity and opacity of LLMs, this creative approach could make a welcome addition to the literature.\n- Specifically, this paper is, to my knowledge, the first to propose subjective randomness as a method of analyzing LLMs. This is an original idea.\n- This work can serve as an interdisciplinary bridge for computer scientists into cognitive science. As more works are concerned with the \"cognitive abilities\" of LLMs (and in general, adopt a humanizing/personifying lens on LLMs), it makes sense to draw from the existing rich literature developed in the field of cognitive science itself.\n- As a reader with a computer science background (esp. pseudorandomness), I found the high-level introduction to subjective randomness to be well-written and accessible.\n- The methods presented in this paper are black-box: they only require prompting access to the language model, but do not use the weights, architecture, or training data. This is increasingly important as not all researchers reveal these attributes upon release of an LLM."
            },
            "weaknesses": {
                "value": "In decreasing order of importance.\n### Usage of the term \"pseudorandomness\"\nThe use of the term \"pseudorandomness\" in this paper differs significantly from how the term is commonly used in computer science literature. In particular, a distribution can only be pseudorandom with respect to a (computationally-bounded) class of distinguishers. However, throughout this paper, the ability of GPT to generate \"pseudorandom\" numbers is discussed without an explicit specification of a distinguisher. Perhaps the use of the term \"pseudorandomness\" is more appropriate in the experiments in which GPT is the distinguisher (rather than the generator), but there the generator itself is extremely trivial---it is a determinstic function, which therefiore can be distinguished by an almost-computatioanlly-trivial class of functions (that output 1 iff their input matches the (fixed!) output of the generator).\n\nTo avoid confusion with a long-standing and clearly-demarcated field of research, I suggest the authors to avoid using the term \"pseudorandom\" in this paper. Instead, they could use a term such as \"subjectively-random\" or even \"seemingly-random\", which avoids alluding to a concept that is not appropriate in this experimental setting.\n\n### What do the experimental results actually mean?\nA significant contribution of this paper is in demonstrating how subjective randomness can be used to experiment on LLMs. However, I am not sure what insight can be garnered from the particular experiments presented in this paper.\nMy understanding of the results from the experiments presented in the body of the paper is:\n1. GPT does not generate uniformly random numbers; in particular, it exhibits the gambler's fallacy.\n2. As context length increases, GPT can more confidently distinguish uniformy random strings from extremely simple, deterministically-generated strings (such as (01)^n).\n\nI find neither of these conlcusions particularly insightful towards a deeper understanding of ICL dynamics. As a suggestion for improvement, I would expand the setup of the distinguishing experiments (item 2 above) along two axes:\n- Stretch (length of generated string): Rather than testing on deterministically-generated strings (which are NOT pseudorandom in any non-trivial sense of the word), have GPT distinguish between uniformly random strings and strings generated by an actual pseudorandom generator; that is, one that takes as input a short random seed, and stretches it into a longer string.\n- Generator complexity: The automaton for detecting the language used in the current experiments is only a handful of cells large. Yet, GPT has billions of parameters. If you insist on using deterministically-generated strings in the distinguishing experiments, at least have them be generated by significantly more complex programs.\n\nOne of the main contributions claimed in page 3 was that the subjective randomness framework will provide convincing evidence that ICL carries out a form of Bayesian model selection. I could not see how the experimental results I listed above support this claim. Please let me know if I am missing a connection here.\n\n### Figures\n- The colors in the caption of Figure 2 do not align with the figure itself. This is one of the main figures in the paper! On that note, I'd suggest using \\color{} in the caption when referring to the different colors.\n\n- Please review all figures in the body of the paper for colorblind-accessability. I am colorblind and had a hard time deciphering Figure 8. There are plenty of guides online on how to make accessible figures, and I am happy to point you to specific ones if that is needed.\n\n- What does \"Probability of Concept\" mean on the y axis of Figure 8? My understanding is that it is the probability that, when prompted by a context of length $|x|$ from the regular language, the model responds that the input is deterministically-generated. Is that correct? (See also the last question in Questions section.)\n\n### Writing on page 7\nThe mathematical notation in the sum on page 7 was confusing (the righthand side that following \"and computing\"): I didn't understand what is $(y_d == 1)^{(i)}$--though I may have some guesses, please give a full definition of notation you are using in your paper.\n\nThere is a typo in the same paragraph: \"asses\" should be \"assess\"."
            },
            "questions": {
                "value": "- Thank you for introducing me to the area of subjective randomness. Can you confirm that my understanding of it is correct, as far as needed to understand this paper? Am I missing a key insight from this area?\n- When talking about distinguishing ability, the correct quantity to look at is the \"advantage\" of the distinguisher: Denoting the uniform distribution, the pseudorandom generator, and the distinguisher by $U$, $G$, and $D$ (respectively), this is the quantity $|Pr_{x \\gets U}[D(x)=1] - Pr_{x \\gets G}[D(x) = 1]|$. Note that the minuend is hard to compute (requires enumerating over all strings of the given length), but can be estimated with uniformly random samples. Can you confirm that this is the quantity portrayed in Figure 8? If not (for example, if it is $Pr_{x \\gets G}[D(x) = 1]$ that is portrayed), the graph might falsely claim that the distinguishing ability increases with context length, without any real gain in advantage.\n- Am I missing any key experimental result presented in the body of the paper? How do the experiments support the claim that ICL is Bayesian model selection? (How would a different outcome of the same experiments falsify this claim?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Reviewer_F7kD"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698354841628,
        "cdate": 1698354841628,
        "tmdate": 1700595326032,
        "mdate": 1700595326032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8PnUi9V3XY",
        "forum": "62K7mALO2q",
        "replyto": "62K7mALO2q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_xnie"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_xnie"
        ],
        "content": {
            "summary": {
                "value": "The paper examines how LLMs generate and and detect random binary sequences, drawing analogies to in context learning (ICL) as a type of Bayesian Model selection.  They use the paradigm of Bayesian Model selection to analyze a particular aspect of how LLMs behave.  They make the case that analysis allows investigating cognitive interpretability vs. behavioral benchmarks or mechanistic interpretability.  \n\nThey examine how subjective randomness can be seen as a type of in-context learning, showing that like humans, LLMs have biases in the types of sequences they generate.   In particular, it\u2019s like under-specified program induction, with the posterior over different types of programs being updated with each new observation.  \n\nThey also propose a specific model (window average) that accounts for some of the behavior of the generated random sequences better than the true generative process which limits access to more recent history. They go on to show that earlier versions of text-davinci do poorly at sequence generation, while the most recent version does have \u201csubjective randomness behavior\u201d, similar to their proposed model.  They also show that specific sub-sequences occur more frequently than would be expected from a true Bernoilli process. \n\nThe authors then go on to show how many repeats of a given seed pattern are required to allow chatGPT to identify the sequence as non-random. \n\nAnd finally, they also show many repeats of a given \u201cseed\u201d sequence are required to make chatGPT only repeat that final pattern, vs. continuing with a psuedo-random sequence.   This sudden phase shift in how an LLM generates a pattern is argued to be similar to switching hypothesis in Bayesian model averaging"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## Originality:\nDetermining whether a binary number sequence is assessed to be random by a human has been pretty extensively studied in the literature, and it is an interesting extension to see how it compares to the behavior of an LLM judging a sequence to be random.  Some parts of their framing are novel, in particular their experiments w.r.t to the effect of seeding with a repeating pattern.  Their Window average model appears to match well the real performance of the LLMs, and they have a clear example of the Gamblers fallacy in this model during the generation.  Overall, their biggest novel contribution is that LLMs can be biased by strong local patterns when trying to generate new random sequences, and that this could be interpreted as Bayesian model selection. \n\n## Quality:\nSee weaknesses for more discussion about the potential mismatch in what are claiming and what their experiments are actually doing.  With that proviso, the experiments generally appear to be consistent with their claims.  They have an extensive literature review that is well written and easy to follow.  Their setting is well chosen, with potential to give some insights into how LLMs are finding local patterns.  \n\n## Clarity:\nThe paper is sufficiently well written, and easy enough to follow.  \n\n### Other minor comments:\n> For this reason, in Fig. 6 we show results for P (Tails) = 51%.\nIt would be clearer to also update Figure 6 to show that it is 51%, as opposed to only mentioning it in the text.\n\n> In randomness Generation tasks, we asses concept learning according\nShould be \u201cassess\u201d"
            },
            "weaknesses": {
                "value": "The analogy of generating random sequences to in context learning is limited in that the model depends on LLMs getting confused when they are told they are provided with a random sequence, but in fact the \u201cstarting\u201d sequence has strong local patterns. Admittedly, this is a short-coming of the LLM as it shouldn\u2019t be \u201cdistracted\u201d by this local pattern.  However, this artifact may go away when the base model is improved, and not \u201cdistracted\u201d by the local misleading start, which could limit the usefulness of that part of the analysis. \n \nIn other words, it can be argued that the analysis is simply examining a failure model of an LLM, namely in a regime where local patterns start to dominate the predictive behavior of the system and it \u201cforgets\u201d the prompt. That doesn\u2019t mean the transformer is doing any sort ICL - it\u2019s just dominated by the local signal.   \n\nFor example, if you put in any prompt, then a long enough sequence of 001001, it may ignore the prompt and start predicting 001001\u2026 as it\u2019s \u201cforgotten\u201d about the prompt due to the strong local predictable signal.     This makes the connection to in context learning more tenuous due to the potential disconnect between the prompt and the task, which may not happen in larger/newer/more refined models or in a more complicated ICL problem.  \n\nOne way to address mismatch could be the prompt being explicit about it being an ICL problem.  For example:\n\n\u201c\u201d\u201cContinue the following potentially random sequence: <...>\u201d\u201d\u201d\n\nThen measuring the success rate of the continuation as you know the underlying generate model.   This would remove the mismatch between the prompt and the model. \n\nAdditionally, experiments for determining whether a sequence is random should include some truly random baselines.  For example, Figure 4 has no truly random baseline.  Hence, it is not clear from the current results if the model incorrectly assumes that all sequences are not random given enough observations.  This could be due to limitations in the attention or number of layers, for example, or biases in the training data.  What is the pattern where the LLM starts to fail?"
            },
            "questions": {
                "value": "> ChatGPT generates higher complexity sequences than chance. We speculate that this phenomenon might explained by a cognitive model that avoids sampling with replacement.\n\nI\u2019m unsure what this means - can you elaborate what you mean by higher complexity in this context?\n\nWhat is the tokenizer doing?  Given that the whole paper is based around heads/tails sequences, how are they being represented?  Are there equal number of tokens being used for Heads and Tails?  Are tokens straddling between Heads/Tails at all, or on the punctuation, which may induce some of the observed patterns?  \n\nCorrespondingly, does it matter how the random sequence is represented?  Heads/tails might have their own strong biases not represent in random token choices as there are many examples in the training data of these tokens in various non-random contexts.\n\nWhy do the prompts specify generating 1000 tokens, but only ever the first 50 are used?  Is there a particular reason for that mismatch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Reviewer_xnie"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790645447,
        "cdate": 1698790645447,
        "tmdate": 1700602030333,
        "mdate": 1700602030333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Xyqi2Xaj1",
        "forum": "62K7mALO2q",
        "replyto": "62K7mALO2q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_272r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8758/Reviewer_272r"
        ],
        "content": {
            "summary": {
                "value": "The paper demonstrates that models of certain size exhibit what they call subjective randomness. That is, a sequence that looks random as there is no discernible simple program that can generate such sequence. They also find that model beyond a certain size can distinguish simple programs from random programs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and enjoyable to read. The observation made by the authors is novel as far as I can tell and makes connection with a human cognitive bias, which may increase our understanding of those large models. Such understanding if of great interest in the current time for several reason including safety."
            },
            "weaknesses": {
                "value": "I found the paper to utilize a lot of its space to discuss ideas that seems unrelated to what I understand to be its main message. For example, it discusses in length the idea of \"cognitive interpretability\" which does not seems novel nor relevant to the idea presented in the paper. While such idea could be touched upon, I don't understand the value of Figure 1 in the dissemination of the idea.\n\nThe paper seems a little bit rushed. For example, the caption of Figure 2 seems partly unrelated or outdated to the figure. The caption refers to \"green\" and \"yellow\" concepts which cannot be found in the figure itself, making hard to understand what the authors are trying to convey. Figure 5 left seems to be the same figure three times.\nMoreover, the appendix has several Figures but they are not accompanied by any text.\n\nThe paper utilize closed products which may change making the conclusion made in the paper non-reproducible in the future. The authors are encouraged to use open models or demonstrate that the observed behaviors cannot be attained with the currently available open-source models."
            },
            "questions": {
                "value": "* Does the results change if the authors performed their analysis on an open model instead of a GPT-3.5?\n* I don't understand Figure 5 left. How are each graphs different f rom one to another?\n* I don't understand Figure 2. The caption points to several  color (e.g. green for $x$), yellow for the concept space embedded in the LLM. But I don't see such colors in the Figure.\n* How is the proposed \"Cognitive Interpretability\" different from qualitative analysis generally done in science and also in the deep learning literature when ones try to understand a model's behaviour? For example, some line of works studied the the attention map of convolutional neural networks when fed certain inputs [0]. This seems to fit more the \"Cognitive Interpretability\" category than the other interpretability categories.\n* Do you see a link between this paper and the emerging line of work on the equivalence of ICL to gradient descent [1]\n\n[0] https://openai.com/research/microscope\n[1] https://arxiv.org/abs/2212.07677"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8758/Reviewer_272r"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846383957,
        "cdate": 1698846383957,
        "tmdate": 1700616339502,
        "mdate": 1700616339502,
        "license": "CC BY 4.0",
        "version": 2
    }
]