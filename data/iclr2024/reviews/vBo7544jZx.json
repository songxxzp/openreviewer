[
    {
        "id": "X6oVKNZTdX",
        "forum": "vBo7544jZx",
        "replyto": "vBo7544jZx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
        ],
        "content": {
            "summary": {
                "value": "This work presents a novel architecture inspired by memory systems in cognitive science. The method improves performance across multiple reasoning tasks in both transformer and CNN architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed architecture improves performance across a diverse set of reasoning tasks.\n- A reasonable set of baseline comparisons are included.\n- An ablation study is performed to assess the impact of specific components."
            },
            "weaknesses": {
                "value": "- The primary limitation concerns the framing of the architecture as instantiating both working memory and longterm memory. It is not clear to me that the architecture actually involves longterm memory in any meaningful sense. I think the approach would be better described as a form of relational working memory (utilizing a tensor product to capture relational information). This of course doesn't concern the method itself, which seems to perform well across multiple tasks. But I think the contribution would be much more clearly framed as a kind of working memory that exploits *relational* information. The role of relations in working memory is very well-studied in cognitive science (see references below), and I think this would make an interesting topic for discussion.\n- Is it possible to study an ablation model that includes the 'longterm' memory component but not the 'working' memory? It seems likely that the tensor product in the longterm memory component is primarily driving the gain in performance, and it would be nice if this could be isolated.\n- It would be good to cite work from cognitive science on the role of tensor product representations in working memory [1,2] as this is highly related to the outer product mechanism in the 'longterm' memory module.\n\n[1] Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2), 159-216.\n\n[2] Halford, G. S., Wilson, W. H., & Phillips, S. (1998). Processing capacity defined by relational complexity: Implications for comparative, developmental, and cognitive psychology. Behavioral and brain sciences, 21(6), 803-831.\n\nMinor comments:\n- It sounds like what is referred to as the transformer baseline in this work is actually a 'universal transformer' [3] in which parameters are shared across layers, and what is referred to as a 'high capacity transformer' is just a standard transformer (in which each layer has different parameters).\n\n[3] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, \u0141. (2018). Universal transformers. arXiv preprint arXiv:1807.03819."
            },
            "questions": {
                "value": "- In what sense does the 'longterm' memory module involve long term memory more than the 'working' memory module? They both seem to operate over the same timescale, the only difference being the presence of the tensor product to capture relational interactions (which is not related to longterm vs. working memory).\n- Is it possible to ablate the 'working' memory module?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_4HGz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698135256562,
        "cdate": 1698135256562,
        "tmdate": 1700711606544,
        "mdate": 1700711606544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1mg7nWFHQz",
        "forum": "vBo7544jZx",
        "replyto": "vBo7544jZx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a cognitive framework called PMI that consists of perception, memory, and reasoning modules. It is inspired by human memory mechanisms and aims to improve the understanding and handling of relational questions in AI systems. The memory module includes working memory (WM) and long-term memory (LTM), with LTM having a higher-order structure to retain accumulated knowledge. Current perceptions update WM through competitive write access and are merged with LTM via outer product associations. The inference module retrieves relevant information from both WM and LTM to generate comprehensive insights. The PMI enhancements consistently outperform their original counterparts in tasks such as question-answering, relation calculation, and image classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Integration of cognitive science and AI: The paper draws inspiration from multiple memory systems theory and global workspace theory in cognitive neuroscience, and applies these insights to develop the PMI framework for AI systems.\n\n- Novel memory module: The PMI framework introduces a dual-layer memory block with distinct communion principles, featuring working memory (WM) and long-term memory (LTM). This structure allows for efficient information filtering, storage, and knowledge consolidation.\n\n- Enhanced performance: The PMI enhancements consistently outperform their original counterparts in various tasks such as question-answering, and image classification. This demonstrates the effectiveness of the proposed framework in improving AI systems' understanding and reasoning abilities.\n\n- Clear experimental results: The paper provides detailed experimental results, including accuracy rates and convergence rates, to support the effectiveness of the PMI module. Visualizations of attention patterns further illustrate the model's ability to consolidate and integrate information from different memory sources.\n\n- Reproducibility: The authors plan to share their code once the review process is completed, ensuring the reproducibility of their experiments and allowing for further research and development in this area."
            },
            "weaknesses": {
                "value": "- The text appears to be excessively embellished. I would like to encourage the author to employ conventional terminology, as exemplified by the authors referencing \"relation calculation\" in the abstract.\n\n- The paper includes visualizations of attention patterns between perceptions and memories, but it could benefit from providing more detailed explanations and interpretations of these visualizations. \n\n- Examining the qualitative impact of your modules on various types of tasks would provide valuable insights, rather than solely relying on quantitative results. This approach would enhance the paper's overall credibility. You can achieve this by employing various visualization techniques and similar methods.\n\n\n\n**Additional Feedback and Future Experiments:**\n\n- Enhance the clarity of the text to facilitate a deeper comprehension of the paper. Despite grammatical accuracy, the writing occasionally comes across as artificial\n\n- Incorporate additional visualizations to facilitate a clearer and more easily comprehensible text.\n\n- Consider expanding the scope of this research to include more memory-based and cognition-inspired tasks. You may find the paper titled \"Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory\" by Sikarwar et al. to be a relevant reference in this context."
            },
            "questions": {
                "value": "Please see weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3652/Reviewer_AVcH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837493354,
        "cdate": 1698837493354,
        "tmdate": 1699636321355,
        "mdate": 1699636321355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jsZQpSJU2X",
        "forum": "vBo7544jZx",
        "replyto": "vBo7544jZx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_SKET"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3652/Reviewer_SKET"
        ],
        "content": {
            "summary": {
                "value": "Inspired by human brain\u2019s memory system and cognitive architectures,this paper propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. \n\n\nIn my opinion, the motivation of this paper is meaningful because it comes from the human brain's memory. \nAnd the proposed memory module looks like powerful because it consists of working memory and long-term memory.\nHowever, the experiments may not be enough due to it not compare with other memory augment models, such as the memory augment language model. and this paper not take experiments on language generative task.\nBesides, the"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation is sometimes novel and comes from human's brain memory. \n\n2. The proposed model is meaningful with its novel motivation\n\n3. The paper is well written, and the image is easy to understand."
            },
            "weaknesses": {
                "value": "1. the experiments may not be enough to compare it with other memory-assisted language model\n\n2. The experiments is hard to understand, and i think it is not necessary to conduct experiments on image classification. And there is little work on the memory augment image model due to the image's too long context. \n\n3. I don't see any connection between your work and the title, the author maybe need change a title due to this model hard to help us underanding AI .\n\nI hope the author takes more experiments on the language model to solve the longer context challenge."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699025910984,
        "cdate": 1699025910984,
        "tmdate": 1699636321280,
        "mdate": 1699636321280,
        "license": "CC BY 4.0",
        "version": 2
    }
]