[
    {
        "id": "EyMZdNlpiZ",
        "forum": "cG8Q4FE0Hi",
        "replyto": "cG8Q4FE0Hi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_vr2D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_vr2D"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to improve LLM\u2019s reasoning abilities and address the challenges of overlooking, question misinterpretation and condition hallucination in LLMs\u2019 generated solutions. It proposes RCoT to detect and rectify such factual inconsistency through four steps, including reconstruction, decomposition, comparison, and revision. The experiments are conducted on randomly sampled sub-sets of seven arithmetic datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation is clear and the analysis of challenges is reasonable.\n- The performance of the proposed RCoT is demonstrated to be superior to the standard baselines."
            },
            "weaknesses": {
                "value": "- The experiments are only conducted on randomly sampled sub-sets of the test sets, which may raise concerns about the convincingness of the results. The experiment results do not allow for a direct apple-to-apple comparison with the reported results in other papers, such as those related to Self-consistency.\n- The experiments on other reasoning tasks, such as commonsense reasoning and symbolic reasoning, are absent.\n- The performance improvement is not significant compared to the self-consistency (84.5 v.s 83.5).  In addition, did the paper's testing of the Self-consistency algorithm use 30 paths? In Self-consistency, the typical number of paths used is (1, 5, 10, 20, 40). Why were 30 paths chosen? If the reason is to make comparable comparisons based on average tokens, it would be appropriate to report the performance and average tokens under different numbers of paths.\n- There is a lack of in-depth analysis and evaluation beyond overall performance, such as the absence of assessment regarding improvements (Quantitative or user-study-based evaluations) in the three areas of overlooking, question misinterpretation, and condition hallucination. Table 5 only evaluates on 45 cases.\n- The method is somewhat incremental. The decomposition, comparison, and revision components are not new in the context of CoT. While reconstruction is used in many fields, its application within CoT is new and appears to be the main technical contribution. However, the overall framework of RCoT is incremental and complex.\n- Minor suggestions about the presentation:\n    - On page 1, this paper introduces the condition of  \"2 days away\" in Figure 1 is mistakenly overlooked. However, there are no \"2 days away\" in Figure 1.\n    - Can the three different examples in the Introduction be unified?\n    - The font size in Table 1 is too small.\n    - The order of citing figures and charts is mixed up. For example, Table 4 is cited before Tables 2 and 3, but it is located below them in the paper."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697975781220,
        "cdate": 1697975781220,
        "tmdate": 1699636367429,
        "mdate": 1699636367429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NKPS8OQ4vS",
        "forum": "cG8Q4FE0Hi",
        "replyto": "cG8Q4FE0Hi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_rMGU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_rMGU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a novel approach RCoT to identify and rectify factual inconsistencies in the outputs of large language models (LLMs). The approach works by first prompting the LLM to reconstruct the question based on its answers, and then prompting the LLM to determine whether the reconstructed question is identical to the original question in terms of the conditions derived. If there are discrepancies between the conditions, the finer differences are used to rectify the LLM to provide a more accurate and consistent answer. Experiments show that the proposed RCoT outperform baselines in seven arithmetic datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The general thoughts of the problem are interesting and novel -- proof by contradiction -- use LLM to prove things by contradiction. From my understanding, LLMs are used in the following different scenarios: (1) reconstructing the problem; (2) Listing the conditions of the original and reconstructed problems; (3) determining whether there are hallucinated and overlooked conditions; (4) Determining whether the reconstructed problem is identical to the original one. (5) rectifying the results based on the finer feedback summarized from (3). The above uses of the LLM are interesting and each worth individual study about the effects."
            },
            "weaknesses": {
                "value": "Although the general thoughts of the problem are interesting and novel, the paper itself has many obvious flaws.\n\nFirst, the prompting method is overused. The paper does not establish causal connections between the different prompting stages. For example, to determine whether the reconstructed problem is identical to the original one (Figure 20, \u201cquestion comparison\u201d), the method does not consider the prompted results from \u201cproblem decomposition\u201d and \u201ccondition comparison\u201d. Additionally, to rectify the solution, the model does not take the results from \u201cquestion comparison\u201d into account.\n\nSecond, upon reviewing the examples  (Figure 20 \"I apologize for my mistake...\". ), I believe that the authors are exploiting the \"dialog system\" nature of the LLM interfaces. The dialog system introduces an extra conditioning on the chat history, which means that the actual prompts listed in the paper are all conditioned on previous prompts that have been used. As a result, I believe that the experiments are flawed. In contrast, the methods such as CoT (Wei et al., 2023), Active Prompt (Diao et al., 2023), and Self-Consistency (Wang et al., 2023) are stateless, meaning that they only involve a single interaction between a human and the LLM interface. I recommend that the authors learn from CoT which models P(answer|question, reasoning chain of other examples) to better formulate their condition dependences.\n\nThird, the gain of providing the reason seems to be moderate. From table 2, \"judgement\" (Figure 20 \"question comparison\") should be the key factor while \"reason\" (Figure 20, \u201cproblem decomposition\u201d and \u201ccondition comparison\u201d) seems to be less important. From Table 4, the proposed complicated RCoT seems to be worse than Self-Consistency (Wang et al., 2023)."
            },
            "questions": {
                "value": "I expect the authors to justify their choice of interactively prompting the large language models. Currently, I felt they only proved it kind of works but did not explain the reason. However, I felt the comparison are not fair and the results are not easy to reproduce."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565527647,
        "cdate": 1698565527647,
        "tmdate": 1699636367341,
        "mdate": 1699636367341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QxUw9F0CSC",
        "forum": "cG8Q4FE0Hi",
        "replyto": "cG8Q4FE0Hi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to tackle challenges like condition overlooking, question misinterpretation, and condition hallucination that LLMs meet in arithmetic reasoning benchmarks. On top of chain-of-thought (CoT) prompting, this paper proposes RCoT, which asks the LLM to rewrite the problem, compare it to the original one, and identify fine-grained differences in conditions and questions, thus finding mistakes and revising the original answer. Experiments show consistent improvements across benchmarks and LLMs, verifying effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "*Originality*: The core idea of this paper is novel and original.\n\n*Quality*: The method is well-motivated and extensively evaluated.\n\n*Clarity*: The delivery is very clear and easy to understand, I did not find issues in understanding.\n\n*Reproducibility*: Code is provided to encourage reproducibility.\n\n*Significance*: This work touches a major issue in LLMs, which is of much research significance."
            },
            "weaknesses": {
                "value": "- One drawback of this work could be its complexity, as illustrated in the diagram and verified by token counts. I understand it is comparable to some previous works, but optimizing its complexity is still an important aspect.\n- (minor) The comparisons in tab. 1 is not very clear, eg, the results marked in green are not straightforward to understand except by reading the captions.\n- (minor) Venues are missing from multiple references."
            },
            "questions": {
                "value": "Suggestion: it might be better to call \"reconstruction\" as \"rewriting\" or \"paraphrasing\".\nDisclaimer: since I am not very familiar with related literature, my current rating is relatively conservative, and I'll reconsider it after reading opinions from other reviewers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691246164,
        "cdate": 1698691246164,
        "tmdate": 1699636367238,
        "mdate": 1699636367238,
        "license": "CC BY 4.0",
        "version": 2
    }
]