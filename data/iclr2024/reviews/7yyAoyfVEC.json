[
    {
        "id": "irKkFYPt2Y",
        "forum": "7yyAoyfVEC",
        "replyto": "7yyAoyfVEC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission983/Reviewer_ueoZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission983/Reviewer_ueoZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the use of a MECE (Mutually Exclusive, Collectively Exhaustive) structure in the prompts of an LLM (Language Model) to assist in breaking down complex problems into hypotheses. The prompts also address the prioritization of these hypotheses, actively validating each hypothesis by requesting data from a user, and maintaining a holistic view to determine the depth of analysis. The proposed prompts yield improved results when evaluated in business and medical cases by experts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The proposed techniques for prompting, based on hypotheses and structure, make sense and have broad applicability."
            },
            "weaknesses": {
                "value": "W1. The proposed HS prompting techniques seem to be an extension of existing methods such as ToT and GoT. Yet, the authors did not include these methods as their baselines, making it hard to evaluate the effectiveness of the HS prompting.\n\nW2. In the experiments, the criteria used to evaluate the models were not clearly defined and justified. For example, the background of the professionals and their scoring standards is not clear. It is also unclear whether the experts who set the criteria participated in the model scoring process (which could lead to potential bias).\n\nW3. The business case study did not show a significant advantage for the proposed prompting techniques. More discussion on the cause is needed.\n\nW4. More objective metrics should be considered in the experiments. For example, runtime, user feedback statistics, and other quantitative metrics.\n\nW5. Presentation issues: Some data charts seem misaligned with the text. The paper requires a major revison."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732670399,
        "cdate": 1698732670399,
        "tmdate": 1699636024389,
        "mdate": 1699636024389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "chi7XKupHM",
        "forum": "7yyAoyfVEC",
        "replyto": "7yyAoyfVEC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission983/Reviewer_hnXU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission983/Reviewer_hnXU"
        ],
        "content": {
            "summary": {
                "value": "# Summary\n\n## What is the problem? \nHow can we best use LLMs in highly interactive settings with lots of back-and-forth between users to most effectively solicit information and arrive at correct conclusions given that solicited information.\n\n## Why is it impactful?\nThere are many problems of this format, including medical diagnosis and business case problem discovery. Better methods to leverage LLMs for these applications would be impactful.\n\n## Why is it technically challenging/interesting (e.g., why do naive approaches not work)?\nThere is a breadth of interesting related literature on how to better use LLMs in iterative reasoning problems, much of which is appropriately flagged by the authors. The existence of this prior work illustrates the challenge of this area. These problems are also challenging to evaluate properly, given that whether or not an LLM agent \"reasoned correctly\" and \"most efficiently\" is often ill-posed and challenging to formalize.\n\n## Why have existing approaches failed?\nThe authors allege that existing approaches fail in certain circumstances that (they implicitly argue) are essential in healthcare or business use cases. However, these challenges are not quantified nor sufficiently justified by their empirical results. Further, they fail to sufficiently comment on related resources, such as https://jamanetwork.com/journals/jama/article-abstract/2806457 this paper which examines medical diagnostic performance in a set of published clinical case report challenges and find generally positive results in that setting.\n\n## What is this paper's contribution?\nThis paper proposes a prompting strategy to perform these iterative, chat-based tasks.\n\n## How do these methods compare to prior works?\nThey compare to IO (which is never clearly defined), CoT, and two variants therein that rely on adding a \"ask for more information once\" modifier to the prompts.\n\n## How do they validate their contributions?\nThey perform a set of quantitative vignettes of their model's performance on 3 business case studies and 4 medical diagnosis challenges, evaluated by human experts."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "## Key Strengths (reasons I would advocate this paper be accepted)\n  1. This is an important an interesting challenge."
            },
            "weaknesses": {
                "value": "## Key Weaknesses (reasons I would advocate this paper be rejected)\n  1. You assess your method on only 3 business cases for business consulting. This is dramatically too few examples on which to base any generalizable conclusion for the performance of this method. You need to assess this method on a much larger set of business consulting problems in order to argue that your approach has merit over alternative approaches in a reliable manner that should be expected to generalize across a meaningful subset of business consulting problems.\n  2. Similarly for medical cases, you need to experiment with more than 4 cases (I know you started with 5, but one ended up being excluded).\n  3. The quoted justification below for rejecting baselines feels insufficient. Your framework is also repetitive (in that it is recursive) to arrive at a single end point, and the fact that use ChatGPT rather than the API does not invalidate studies that require API use, as you could (and should, for robustness in your experiments) be able to implement your study using a simple program that leverages the API. Quoted justification: \"While these methods utilize the GPT-4 API to integrate tree or graph search algorithms, they\u2019re not directly adaptable to our chat interface, where humans interact with the model. Furthermore, these methods best suit tasks that can be broken down into repetitive steps, with a clearly defined endpoint, while business and medical diagnosis tasks are not.\"\n  4. IO is not clearly defined in your work. CoT is defined as an acronym, but you don't explicitly indicate what exact prompting strategy is used and how it differs from your approach to justify your experiments.\n  5. A key part of the challenge here is one of evaluation; which evaluation metrics should be used, how can they be efficiently assessed at scale, what factors of the input motivate success or failure on different metrics, etc. You do not offer any significant commentary on these challenges nor offer solutions for them, which significantly undercuts your impact here.\n  6. As you are using human evaluators, you need to state that you have appropriate IRB approval to run this study (in order to solicit the survey responses from your human evaluators)."
            },
            "questions": {
                "value": "Unfortunately I do not foresee any changes that could motivate me to change my review at this time. You would need to fully re-do your evaluation and experiments at a much greater scale for me to consider a change in score here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "They perform a human subjects study in this work (by leveraging human evaluations) but do not assert appropriate IRB approval or ethics review to perform such a study."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807724865,
        "cdate": 1698807724865,
        "tmdate": 1699636024307,
        "mdate": 1699636024307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1hjdjBZFS6",
        "forum": "7yyAoyfVEC",
        "replyto": "7yyAoyfVEC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission983/Reviewer_HT4V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission983/Reviewer_HT4V"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel prompting strategy, HS, which starts by breaking down the problem space using the concept of Mutually Exclusive and Collectively Exhaustive (MECE), and it proceeds to prioritize and validate hypotheses through interaction with the user. The paper also introduces easy-to-follow guidelines for crafting examples for potential users of HS prompts. Experiments on business consulting and medical diagnosis in 'many-to-one' scenarios show that HS prompting outperforms previous prompting strategies, even when modified for these particular tasks, indicating potential applicability for LLMs in challenging, real, domain-specialized scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation of the paper is ambitious and addresses real-world problems using LLMs.\n- The experiments conducted cover both the business and medical domain, which can potentially demonstrate the impact of the paper.\n- The proposed prompting is well-thought-out and convincing when using GPT-4."
            },
            "weaknesses": {
                "value": "- The method heavily relies on GPT-4's ability, which may limit the applicability of the approach to other LLMs. CoT, ToT, and GoT are general methods that can be used in any other LLMs regardless of how much knowledge is stored in LLMs (they act as an aid to LLM reasoners). However, the assumption of HS is that LLMs are very knowledgeable about defining any problem landscape and are great at generating possible hypotheses, which is not just the role of a reasoner but of an oracle-knowledge base and reasoner at the same time). To show the broad applicability of the approach, the authors should use other LLMs to demonstrate the effectiveness of HS (possibly using a retriever if some LLMs do not hold enough knowledge).\n- The proposed method resembles a human-AI interactive version of ToT or GoT. The idea itself is very practical and useful, but the credibility of this approach depends on the user."
            },
            "questions": {
                "value": "- How is the citation (Zheng et al., 2023) for neural architecture search related to real-world modeling? Also, I think 'general tasks' should be 'general NLP tasks'.\n- How can we guarantee that the options that LLMs offer are MECE if used by a non-expert?\n- Are there citations or backup arguments for the claims below?\n1. 'LLMs need to apply this knowledge in a structured and efficient manner, especially when solving many-to-one problems'\n2. 'While CoT excels in one-to-one mapping problems, it falters when multiple potential root causes must be explored'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834583631,
        "cdate": 1698834583631,
        "tmdate": 1699636024236,
        "mdate": 1699636024236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LoB6BJz1KP",
        "forum": "7yyAoyfVEC",
        "replyto": "7yyAoyfVEC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission983/Reviewer_y4hX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission983/Reviewer_y4hX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method called Hypothesis- and Structure-based Prompting (HS) for enhancing the problem-solving capabilities of Large Language Models (LLMs) in healthcare and business. The approach breaks down the problem space using a Mutually Exclusive and Collectively Exhaustive (MECE) framework, enabling LLMs to generate, prioritize, and validate hypotheses through targeted questioning and data collection. The paper provides an easy-to-follow guide for crafting examples, allowing users to develop tailored HS prompts for specific tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper found that adding a single sentence about requesting data improved the performance of HS prompting effectively, similar to a previous study. This finding demonstrates the effectiveness of the HS approach and its potential for further improvement.\n- This approach breaks down the problem space using a Mutually Exclusive and Collectively Exhaustive (MECE) framework, which is a unique and effective way of approaching complex problems.\n- The paper provides an easy-to-follow guide for generating examples, allowing users to create appropriate examples tailored to their specific tasks. By aligning the examples with the structure-based and hypothesis-based approach, users can stimulate the LLMs to solve problems more effectively and efficiently."
            },
            "weaknesses": {
                "value": "- Limited comparison to existing methods: While the paper enlists domain experts to validate the HS method and provide a comparison to existing baseline methods, the comparison is limited to a few specific methods. It would be beneficial to see a more comprehensive comparison to a wider range of existing methods.\n- The paper focuses on the application of the HS method to healthcare and business diagnosis, and it is unclear how generalizable the approach is to other domains or problem types.\n- The paper's qualitative evaluation is limited to a few cases with a panel of consultants or medical doctors. Performance consistency is a major concern with such limited of observed samples."
            },
            "questions": {
                "value": "- Can the authors provide more real-world case studies where the HS method was used successfully in healthcare or business? This would help to demonstrate the practical applicability of the approach and provide more evidence of its effectiveness.\n- How generalizable is the HS method to other domains or problem types? Have you considered applying the approach to other areas, such as finance or engineering? If so, what were the results?\n- Can you provide more details on the process of generating examples for the HS method? How do you ensure that the examples are of high quality and representative of the problem space?\n- How does the HS method compare to other approaches that incorporate human-in-the-loop feedback or other forms of human-machine collaboration? Have you considered the potential benefits and drawbacks of these approaches?\n- Can you provide a more comprehensive comparison to a wider range of existing methods? This would help to demonstrate the relative strengths and weaknesses of the HS method compared to other approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699231276983,
        "cdate": 1699231276983,
        "tmdate": 1699636024157,
        "mdate": 1699636024157,
        "license": "CC BY 4.0",
        "version": 2
    }
]