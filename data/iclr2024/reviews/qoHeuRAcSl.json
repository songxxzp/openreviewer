[
    {
        "id": "Urs8L3BGcO",
        "forum": "qoHeuRAcSl",
        "replyto": "qoHeuRAcSl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_i3As"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_i3As"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework that autonomously discovers mode families within robot manipulation trajectories with the help of an LLM. Once these mode families are identified, they serve two key purposes: 1) facilitating learning mode-conditioned policies, and 2) enabling the creation of pseudo-attractors that enhance the robustness of the policies against out-of-distribution states. \n\nThe LLM plays a vital role in the framework by: 1) decomposing tasks into a sequence of modes, and 2) generating relevant state features for each mode as inputs to the mode classifiers. To train these mode classifiers, the authors propose to augment expert-demonstrated trajectories with negative ones generated by counterfactual perturbations. Additionally, they design several loss terms aimed at aligning the trajectories with the mode sequences generated by the LLM. Specifically, the loss terms encourage: 1) consistent state transitions within the same mode family, and 2) matched mode transitions between the success trajectories and those generated by the LLM.\n\nIn a 2D toy example and a robot manipulation domain, quantitative evaluations verify the effectiveness of the proposed method in 1) accurately identifying modes, and 2) learning mode-conditioned policies that are robust against perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work is a good practice to utilize an LLM to facilitate the learning of motion-level robot - by learning task structures and identifying important state features. The paper is well-written: the problem is sufficiently motivated and the framework is clearly-explained. In the experiments, the authors properly conduct comparisons and ablation studies to demonstrate the effectness of the proposed framework."
            },
            "weaknesses": {
                "value": "My primary concern of the paper pertains to details of the method, specifically the transition feasibility matrix and transition loss. \n\nBased on the definition of transition feasibility matrix $T_{i,j}=max(i-j+1, 0)$, we get $T_{i,j} = 0$ for $i < j$ and $T_{i,j} > 0$ otherwise. But this doesn\u2019t match the illustration in the right subfigure in Figure 3, where there is \u201c-1\u201d in the matrix. More importantly, the usage of the transition feasibility matrix in the transition loss (in Section 3.2) might be problematic. My guess is that $T_{k,l}$ helps penalize mode transitions following the opposite direction for successful trajectories; however, the successful trajectories seem not to contribute to the loss term at all since $\\tau(succ)=0$. I would appreciate more clarifications from the authors on this.\n\nI also think \u201cthis work introduces a groundbreaking framework\u201d in the conclusion section is overclaimed. I agree that this is an interesting work, and could be a solid one if the authors properly addess my concern above. However, I would still regard the contribution as incremental, as it only offers an alternative solution to many of the unsupervised trajectory segmentation approach."
            },
            "questions": {
                "value": "1. When prompting an LLM to generate keypoints, how to make the correspondence between the generated textual description such as \u201cnut-center\u201d and the continuous positions? Do you need to add the list of available keypoints into the prompt?\n2. Sometimes the demonstrated trajectories do not perfectly follow the LLM-generated mode transitions. Can the learning framework handle these cases?\n3. How much does the LLM-based feature selection contribute to learning the mode classifiers and motion policies? Do you by any chance evaluate the framework using the full state for mode learning?\n4. I wonder in the RoboSuite experiment, how do the proposed method (MMLP-Conditional BC) work in the absence of the pseudo-attractor? It will be great to add these results as well as the authors have a better understanding on how different modules contribute to the robustness against perturbations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Reviewer_i3As",
                    "ICLR.cc/2024/Conference/Submission6834/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6834/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698034249492,
        "cdate": 1698034249492,
        "tmdate": 1700625520526,
        "mdate": 1700625520526,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mngFnfdezx",
        "forum": "qoHeuRAcSl",
        "replyto": "qoHeuRAcSl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_Mzap"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_Mzap"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to learn mode-conditioned policy for sequential manipulation tasks. The method proceeds in four stages: 1) prompting LLMs to generate a plan that contains multiple modes as well as keypoints and features for detecting the mode, 2) gather human demonstrations, augment them with noise, and execute in the environment to obtain success/failure labels, and 3) learning a mode classifier, and 4) learning a mode-conditioned policy. The empirical results are shown on a 2D polygon domain, where an agent needs to sequentially traverse through all the modes and reach a configuration in the final mode, and on three tasks in Robosuite."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper text is overall clear\n- The presented idea seems to be novel\n- The finding may be of general interest for the ICLR community"
            },
            "weaknesses": {
                "value": "- It seems that the paper is conflated with two distinct problems: 1) generating modes and key detection features using LLMs and grounding them for sequential manipulation tasks, and 2) learning a robust mode-conditioned policy from a few demonstrations. The paper sufficiently demonstrates (2) but more evidence needs to be shown for (1), and currently the two contributions seem quite disconnected. For example, although in the method section the paper discusses how LLMs may generate reasonable mode families and key detection features for those modes, in experiment section the experiments use manually-defined modes instead of LLM-generated modes and use manually-labeled features instead of relying on automatic mechanism for grounding. Therefore, it is questionable whether the paper should be made relevant to LLMs at all.\n- The clarity of the figures need to be greatly improved. (Figure 1) It\u2019s unclear what the task even is and what the different colored lines represent, and this has to be inferred by the reader. (Figure 3) What does the coloring mean in the center figure? How can we interpret the feasibility matrix? (Figure 4) Again, the readers may need to guess what the task is. (Figure 5) In what order of the color should the agent follows? What does the difference in coloring between each sub-figure mean here?"
            },
            "questions": {
                "value": "- How are the keypoints and features being grounded in the demonstrations?\n- How is the feasibility matrix generated from LLMs and is it being kept fixed for each task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Reviewer_Mzap"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6834/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776185934,
        "cdate": 1698776185934,
        "tmdate": 1700722919474,
        "mdate": 1700722919474,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cH1dUxTUu8",
        "forum": "qoHeuRAcSl",
        "replyto": "qoHeuRAcSl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_Rhgd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6834/Reviewer_Rhgd"
        ],
        "content": {
            "summary": {
                "value": "The central idea is to leverage mode families (defines a specific type of motion constraint among a set of objects and can be chained together to represent complex manipulation behaviors) from manipulation research, in combination with LLMs for robust and grounded manipulation.  The authors framework Manipulation Modes from Language Plans (MMLP)  learns manipulation mode families from language plans and counterfactual perturbations. Given a small number of human demonstrations for the task and a small language description of the task, MMLP automatically reconstructs the sequence of mode switches required to achieve the task, and learns classifiers and control policies for each individual mode families. MMLP has four stages: prompting LLMs with a short task description to create a multi-step physical plan; generating a vast amount of counterfactual perturbed trajectories based on a small set of successful demonstrations, and subsequently, learning a classifier for each mode family outlined in the plan; and using the learned classifiers to segment all trajectories and derive mode-specific policies. Evaluation is shown using a simple 2D continuous-space domain and Robosuite (Zhu et al., 2020) and a simulated robot manipulation environment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- In the sea of prompt-based planning approaches for embodied AI, this work felt creative and novel. MMLP seems to combine best of many worlds e.g. capabilities of LLMs for generating abstract/high-level plans, mode families form manipulation research to ground these plans into motion constraints, and idea of counterfactuals to efficiently learn these mode families and corresponding policies.\n- The use of mode families enables MMLP to be interpretable and robustly generalizable."
            },
            "weaknesses": {
                "value": "- Well-thought out analysis but weak evaluation: The evaluation seemed a bit on the weaker side. \n    - Even though I liked the systematic comparison with BC-based baselines and ablations on loss analysis, the two eval domains are both really simple and in simulation. The comparison with BC really helps in validating the use of mode families, however, the evaluation doesn't seem to provide me an understanding of how MMLP would compare with other SOTA manipulation planning approaches such text2motion, vima etc. To that end, I think addition of a more broader set of baselines would strengthen the paper. \n    - Real-world eval or eval with more complex tasks is also highly encouraged in the same vain. Currently I don\u2019t have any intuition or understanding about how well would MMLP work in real world. I\u2019d love to understand how MMLP would deal with partial observability as well. Would the addition of a \u201csearch mode\u201d for finding the right object in clutter for instance, make MMLP brittle/ineffective? \n- Opensourcing plans? The authors do not talk about releasing their code, which is important for reproducibility. I encourage the authors to consider and comment on this in their rebuttal.\n- The authors highlight how prompting the LLM to find a suitable state representation for learning the classifier requires skill. It seems like this is the case for generation of perturbations as well. It is not clear to me if same style of perturbations would work for more complex manipulation setting."
            },
            "questions": {
                "value": "- The loss terms were a bit difficult to parse at the first go \u2014 I\u2019d love a visualization if possible to better understand the loss terms.\n- While it was clear that the LLM generated sub-plans were used to identify number of modes etc., it wasnt clear to me how exactly were additional things obtained from prompting LLM in the first stage of MMLP used as state in the later stages of MMLP. Could the authors explain this more clearly perhaps by adding an example of what \u201cs\u201d looks like for the classifiers?\n- Unclear how the method would work without a simulator to generate success labels for counterfactuals. Is the availability of sim an assumption for MMLP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6834/Reviewer_Rhgd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6834/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827380552,
        "cdate": 1698827380552,
        "tmdate": 1699636790600,
        "mdate": 1699636790600,
        "license": "CC BY 4.0",
        "version": 2
    }
]