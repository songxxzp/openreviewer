[
    {
        "id": "EWJfrkxiey",
        "forum": "6okaSfANzh",
        "replyto": "6okaSfANzh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_6oKd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_6oKd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the notion of \"model cascades\" which is a method to offload easier problems to weaker models, which saves costs. They propose a simple method: answer consistency of the weaker LLM. Intuitively, this just means when the weaker llm is inconsistent, offload the task, because the model is uncertain. From there, the \"stronger\" llm performs inference on the task to solve it. They find this method that increase performance while decreasing cost by a significant margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea is simple and understandable. It's also quite clear why this would help performance -- namely that in highly uncertain situations, samples from an LLM are likely to diverge and lead to overall increased entropy -- so contributing these cases to a stronger model is likely to lead to improved performance.\n- The evaluation is extremely comprehensive. I appreciate the breadth of evaluation across different reasoning tasks\n- The extended results in 3.6 are quite interesting as well -- clearly stating the limitations around how weak the weak llm can be is useful"
            },
            "weaknesses": {
                "value": "In general, it's clearly stated throughout the paper that this method is aimed at \"reasoning tasks\" which indicated focus on datasets like gsm8k or big bench hard -- where the model must reason or understanding challenging problems. Nevertheless, I'm a bit concerned about how well this method would generalize to factuality based tasks or tasks that concern reasoning about facts/knowledge. In these situations it may be the case the model is highly confident (though it is incorrect) about a few pieces of knowledge which causes it to fail to reason correctly. Understanding that this paper is mostly about reasoning tasks, I'm still a bit concerned about how this method could be limited by the overconfidence in incorrect knowledge, and I believe it could be useful to evaluate this potential limitation to better inform readers about how this method may be useful."
            },
            "questions": {
                "value": "- For tasks that require a specific piece of knowledge are the ever situations where the weaker llm is confident, though incorrect, which causes the task not to be allocated to a more accurate & powerful model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731523768,
        "cdate": 1698731523768,
        "tmdate": 1699636426186,
        "mdate": 1699636426186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LKKEIfSsFN",
        "forum": "6okaSfANzh",
        "replyto": "6okaSfANzh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_BfsR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_BfsR"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of robust and cost-efficient question answering using LLMs. To reduce the cost of accurate question answering, the paper proposes to estimate a weak LLM's uncertainty about its answer, to decide whether to accept the answer or reject it and instead ask a strong (but more expensive) LLM. The paper comprehensively evaluates 10 different approaches to the \"routing\" task, and compares the proposed approach to several baselines. The experiments show that significant cost savings are possible without compromising on task accuracy (relative to always using the strong LLM)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I found the paper to be of high quality and clarity. Specific strengths include:\n\n* This is a clearly written paper that proposes and comprehensively evaluates a simple technique for reducing the cost of question answering with language models. \n\n* The empirical evaluation is impressively thorough, comparing to many interesting baselines. \n\n* The paper surfaces several interesting ideas, e.g., that the sampling distribution of an LLM alone may be insufficient for evaluating how uncertain it is, but by varying the prompting strategy, it is possible to get a broader distribution over LLM answers (which may more accurately reflect the LLM's uncertainty over the correct answer).\n\n* The paper does not overclaim: it honestly represents itself as a careful empirical study of the value of a particular approach to rational cost-aware decision making in the LLM Q&A setting, and does not overstate its novelty w.r.t. related work."
            },
            "weaknesses": {
                "value": "1. The evaluation reports \"end-to-end\" accuracy of the entire cascade under different experimental settings, but does not perform a finer-grained analysis of a key novel component: the uncertainty quantification via sampling. It would be great to see some form of **calibration analysis**: in the vote-based methods, how calibrated is the distribution over sampled answers? That is, for each number 1 <= n <= K, how often are the answers that receive n votes actually correct answers? In a perfectly calibrated model, n/K of the answers receiving n votes (across the entire dataset) would be correct answers. Even without perfect calibration, it is interesting to see if the calibration plot is at least monotone: do answers that receive more votes have a higher probability of being correct? It would be great to see how calibration varies across the various vote-based sampling procedures, and perhaps across different LLM temperatures. \n\nSuch analyses would contribute new evidence on important scientific questions surrounding language models, like the extent to which LLMs \"know what they don't know\", and how this uncertainty can best be quantified. For example, the paper \nhttps://arxiv.org/pdf/2207.05221.pdf reports that explicitly asking an LLM to evaluate the truthfulness of a proposed answer yields a calibrated distribution over the tokens True and False. Does the present paper's \"External Verifier - QA\" setting provide contrary evidence? To evaluate this, it would be helpful to see the calibration of the External Verifier compared to the calibration of the methods this paper proposes. (Also, it would likely be necessary to set the temperature higher than 0.4 -- the other paper reports calibration for temperature 1.0 for base language models, and temperature 2.5 for RLHF-tuned models.)\n\n2. Cost is measured based on the actual cost of using GPT-3.5 and GPT-4. This is not unreasonable (and is the exact calculation that many potential users of this framework might wish to do), but the lack of transparency around OpenAI's pricing model, and how it relates to the actual costs of running strong and weak models, makes it harder to interpret the paper's results. I don't think it's necessary for acceptance, but it would be nice to see whether the results from the paper still hold up when using e.g. Llama 2-7b vs. 70b variants, for some replicable measure of cost."
            },
            "questions": {
                "value": "* How exactly does the MoT-2D setting work? There are now four prompts, rather than two. In the voting setting, this poses no additional problems, but what about the verification setting? Do all four prompt settings have to agree? Or are multiple prompts \"pooled\" when computing two vote-based answers to compare for verification?\n\n* If temperature 0.8 yields better results (Fig. 5), why is this not your default? Did you try increasing the temperature further (e.g. Temperature 1)?\n\n* In Figure 4, what threshold was used to decide whether answers were consistent or not?\n\n* Why do you think QA-based external verification with GPT-3.5 performed poorly? Does it incorrectly validate many incorrect answers as trustworthy? Have you tried increasing the temperature of the QA-based verifier, to understand the actual distribution the model places on \"yes, trustworthy\" vs. \"no, not trustworthy\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4502/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4502/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4502/Reviewer_BfsR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811402612,
        "cdate": 1698811402612,
        "tmdate": 1699636426115,
        "mdate": 1699636426115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RO60IgPmrU",
        "forum": "6okaSfANzh",
        "replyto": "6okaSfANzh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_ijvt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4502/Reviewer_ijvt"
        ],
        "content": {
            "summary": {
                "value": "This work introduces an interesting cascading approach to reduce cost in LLM inference. They devised a method: for easy questions, it will use a cheaper LLM (GPT-3.5). But for really hard questions, we'll use the expensive, stronger LLM (GPT-4). This method consists of a weaker LLM, a stronger LLM, and a decision maker, and they reduced the cost to 40% of the cost in using a stronger LLM for everything. To decide which LLM to use, they check if the simpler version gives consistent answers every time they consider its answer. If it does, the question is probably easy, and they stick with the weaker LLM. But if the answers are all over the place, it means the question is tough, and they switch to a stronger LLM. They tried 10 different strategies using Chain of Thought, Program of Thought, mixture of Thought along with majority vote, and verification-based decision making.to find the optimal way to reduce cost while ensuring equal or better performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* They used unique ways of prompting for better decision-making, especially sampling from different in-context demonstrations and thought representations.\n* In-depth analysis of which strategy worked better and why. Evaluating consistency, robustness, and comparisons to other fine-tuned models gives a deeper understanding of how LLMs work."
            },
            "weaknesses": {
                "value": "I haven't found any major weaknesses"
            },
            "questions": {
                "value": "* Instead of just the answer as a hint, what if we give the entire CoT or PoT from one of the prompts as a hint? Will that help?\n* what if we ask multiple questions at once? won't we reduce the cost more?. (2/3 Questions with context as prompt)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699382087295,
        "cdate": 1699382087295,
        "tmdate": 1699636425962,
        "mdate": 1699636425962,
        "license": "CC BY 4.0",
        "version": 2
    }
]