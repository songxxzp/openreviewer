[
    {
        "id": "UsPEfRLk2J",
        "forum": "J7AwIJvR3d",
        "replyto": "J7AwIJvR3d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_BQhU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_BQhU"
        ],
        "content": {
            "summary": {
                "value": "The authors predict MEG responses to words from a visually presented narrative story (from Harry Potter) using the activations from the last layer of GPT-2. They then selected the 100 words that were best- and worst-predicted by the model. The sentences from these words were then fed into a second proposer-verifier LM to try and generate hypotheses for how these differ. The authors took the hypotheses and gave a subset of these hypotheses labels (\u201cphysical\u201d, \u201cfigurative\u201d, \u201cemotion\u201d) and then attempted to fine tune the base LM on three tasks that correspond to these labels. They report increased prediction accuracy for the models fine-tuned on emotion and figurative tasks, but not the physical task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The overall approach is interesting. The authors attempt to learn in a data-driven way stimuli that are poorly predicted by the model and then try to leverage these insights to improve the model. This approach seems relatively novel and potentially promising. The writing is generally clear and the approach was well motivated."
            },
            "weaknesses": {
                "value": "I don\u2019t think the statistical procedure used to establish significance is sufficient and it is not described in enough detail to evaluate it. In Appendix F, the authors say they permute the \u201cdata\u201d using \u201crandom permutation\u201d. What is the data matrix? Are they computing timepoints? Computing timepoints is not valid as this destroys the temporal structure of the data. In addition, the null hypothesis being tested is that there is no relationship between the response and either of the two predictors, which is not the correct null. The correct null is that there is no difference in prediction between the two predictors (but both predictors explain real variance nonetheless). The authors need a valid and clearly described statistical procedure to establish their main finding. \n\nIn the main figure of the paper, the authors should report a meaningful measure of effect size. The fraction of significant channels does not give one a sense of the magnitude of the improvement both because significance does not reflect effect size and MEG channels are highly correlated, so it is not particularly impressive when all channels pass significance. For example, the authors could plot the correlation between the measured and predicted response (as in the appendix scatter plots) for their base LM and a control model such as GloVE. They could then first test whether their LM showed improved predictions compared with GloVE, which would be a good sanity check, and then could compare the improvement from their fine-tuned model with this increment. I would also find it interesting to evaluate the performance improvement from fine-tuning with the differences between various LM models. Could you get a similar improvement by using a slightly larger vs. smaller model or just training the LM model on a bigger language dataset?\n\nThe analyses used to motivate the fine-tuning metric is not compelling. None of the descriptions are significant (which the authors acknowledge) and the procedures used to establish significance are not described. Even if they were significant it is not clear how reliable the effects are. For example, if you ran the procedure twice on different subjects would you get the same thing? Moreover, the labels the authors give feel arbitrary and do not obviously (to me) capture the consensus from the proposed results. I am not confident for example that if you showed this list to two sets of cognitive scientists that they would reliably yield the same categories (i.e., emotional, physical, figurative). \n\nMoreover, the authors show that performance improves for 2 of the 3 tasks they select with weak improvements for one task. Is there any reason to believe that this is better than what you would do if you had simply skipped the whole initial procedure and just tried to come up with any three cognitively relevant and distinct tasks? I am not convinced that the data-driven approach, while interesting, adds value here.\n\nThere is no investigation of what it is about the emotion task that yields the best improvements. For example, the emotion task has more training examples than the other tasks. Perhaps, the emotion task is simply more difficult in some sense than the other tasks and training the network on harder tasks yields bigger improvements? There is also no evaluation of whether the fine-tuned models perform well on their task on a different dataset from the one they are fine-tuned on. If they do not generalize well to another dataset for the same task, it would not be surprising if they do not generalize well to neural predictions. \n\nI am assuming for Figure 5 that the authors are reporting MSE improvement for the corresponding model, i.e. when comparing emotion words with non-emotional words, they are using the emotion-trained model. Please clarify this. Assuming this is the case, it would be helpful to know if the effect is specific to the training task, i.e. do you also see a boost for the emotional words when you train on the figurative task, and do you see a boost for figurative words when you train on the emotional task. Please also clarify whether there was correction for multiple comparisons in this figure. In addition, in panel C of Figure 5, the authors seem to report MSE improvements for the physical task that are at least as large if not larger than those in the other two panels, which seems to conflict with the findings from Figure 4. Please clarify/resolve this apparent discrepancy. \n\nIt is not clear why they did not select the top 100 sentences that were best and worst predicted, averaging across words. This was the unit of analysis for the proposer-verifier system and so it would be good to know that the sentences were indeed predicted well and badly, respectively.\n\nI think it would be helpful to have a slightly longer (i.e., more than one sentence) description of the propser-validator model. From the description, it is not clear how the hypotheses were validated during trained. \n\nIt would be nice if the authors could list all of the emotional and non-emotional words somewhere (same for the other two categories). I understand if that is not feasible, but I think that might provide an easier way to validate whether the labels are good. The Krippendorff\u2019s alpha values seem modest, which made me wonder whether they are high quality."
            },
            "questions": {
                "value": "Please clarify what dimensions the correlation is being calculated across for Figure 7 through 9 (i.e., time, words, both). \n\nOther questions are described in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786201623,
        "cdate": 1698786201623,
        "tmdate": 1699636744011,
        "mdate": 1699636744011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oirNSVwniu",
        "forum": "J7AwIJvR3d",
        "replyto": "J7AwIJvR3d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_atcy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_atcy"
        ],
        "content": {
            "summary": {
                "value": "The same text sequences are fed at the same time into an LLM and shown to a human test subject. LLM's representations of these text stimuli are captures into an embedding. Human's representations to the stimuli are captured by an MEG device. Then, the representations between humans and LLMs are compared by trying to predict human MEG responses from LLM embeddings.\n\nThe analysis focuses on what were the properties of the text that led to lowest prediction scores. From this the authors hypothesize which the linguistic phenomena are the most different between humans and LLMs, pointing at the differences in how languages is processed by these two systems."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I like the freshness of the ideas in this paper, however (see below) some of them are too fresh. But I still would like to note originality of the experimental paradigm and the questions posed in this work.\n\nThe idea of comparing representations and looking for differences and then figuring out what were the common properties leading to the most different representations is insightful and very interesting."
            },
            "weaknesses": {
                "value": "MEG signal is very noisy and it captures not only the activity associated with the task (reading the text), but also almost all of the cortical activity that happened to happen at the time. For the rest of the analysis of this paper it is very important that MEG responses we are predicting are actually responses to the text stimuli, but, knowing MEG, I would say this is very unlikely the case. So while you are predicting something from LM representations, it is very unlikely that what you are predicting are responses to text stimuli. Please let me know if MEG source separation or some other new technique has made this possible and I am wrong on this point. But here is a simple experiment you can use to see where the problem is: out of 9,651 words you have recorded responses to, how many of those words you can identify from the MEG signal alone? By just training a decoder from MEG to words. Most probably not many, if any. So how do we know that the MEG signal you are trying to predict contains _any_ signal produced by text processing by the brain?\n\nFigure 1: \"LLM-based hypothesis proposer is employed to formulate natural language hypotheses explaining the divergence\" -- this sentence is a bit too vague for a scientific paper, the role and function of the Proposer remains unclear after reading such explanation.\n\nPage 2, Contribution 2: How do we know that the \"explanations\" provided by such a model are even true? Obviously it will generate _an_ explanation, and it would sound plausible as most of the LLM-generated texts do, but how is this approach a method of scientific discovery? How do we replicate / falsify / validate these \"hypotheses\" that the Proposer is generating? They might be complete rubbish, how do we know they are not?\n\nPage 2, Contribution 3: Too far-reaching conclusions. The experiments provided in this work do not do enough to prove these claims to be scientific discoveries.\n\nFigure 2: How much of this correlation can be attributed to the fact that the regression models were trained on very similar (MEG) data? In order to know if those correlations of up to 0.45 are meaningful one needs to conduct an ablation test, where instead of actual LM representations you feed into your regression models just some random noise. Or to compute the correlations with MEG activity that you know is not the right activity for the presented stimuli. My suspicion is that due to the structure of the data that was modelled the correlations would still be quite high.\n\nPage 3, Section 2.3: \"As shown in Figure 2, we observe a temporal pro- gression of accurately predicted areas after word onset\" -- What you see is not necessarily a response to the text that the subject is reading, it might be just a response to a visual stimulus appearing in front of their eyes, or some other stimulus-related brain process that does not necessarily contain any signal generated by the neurons that capture the word representation in the subject's brain.\n\nPage 3, Section 3.1: This idea is novel, but relies on too many assumptions and simplifications to be taken seriously as a tool for scientific discovery. The responses given by such an LLM will always contain _some_ analysis, no matter which pairs of D0 and D1 corpora you give it, you will _always_ be able to pick Top 10 reasons for differences. But it does not mean that those differences are actual, valid, or have any relation to the MEG signal. And there was not explanation provided why we should think that they are or do. What if you measure the same set of subjects on the same task on a different day, and/or use a different numpy random seed for your analysis pipeline - would the same top 10 (and top 3) differences come to the top? What if you use another chapter of the book? Would the top 3 differences still be consistent?"
            },
            "questions": {
                "value": "Figure 1: About the ridge regression from LM embedding to discretized MEG signal -- is there one model per time & channel point? Are there (time points) x (channels) models in total?\n\nFigure 1: How is the text input presented to the subject? Do they read the text or listen to it? Is it a long text or just one word?\n\nPage 3, Section 2.2: How well was your custom 1.5B-parameter model able to generate coherent text? This number of parameters is quite small compared to modern models, and GPT-2 abilities are significantly lower that of the most recent open-sourced LLMs.\n\nPage 3, Section 2.2: \"For split i, we set aside one fold as the test set\" -- How different were the chunks of the signal that were used for the test set from those used for training? Were they just the next time windows, or were they from the same subject, but 10-20-x words away from the training data, or were the test chunks from different subjects?\n\nWere regression models re-trained separately for each subject?\n\nPage 3, Section 2.3: We need some sort of a test here that will convince the reader that all these patterns of activity that you are describing are there because of language and semantic processing. What happens, for example, if instead of actual words we will present test subject with pseudowords, like \"feiolg\", \"dufamping\", etc. Or even just just sets of characters like \"jkxio\", \"erkevsd\" etc. I have a strong suspicion that even in this case we will be able to observe very similar progression in terms of how MEG activity travels along the brain areas.\n\nPage 5, Table 1: In this analysis were all the 8 subject taken together as one large pool? What if you run the analysis on only 4, and then repeat the analysis on the other 4 -- would the ranking of hypothesis stay the same, would the selected phenomena be the same?\n\nPage 5, Table 2: What if you do this analysis separately per-subject, will they all more or less confirm to the similar ranking of hypotheses or each person would have their own ranking?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812977470,
        "cdate": 1698812977470,
        "tmdate": 1699636743894,
        "mdate": 1699636743894,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iog3wWwqpc",
        "forum": "J7AwIJvR3d",
        "replyto": "J7AwIJvR3d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_khQ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_khQ4"
        ],
        "content": {
            "summary": {
                "value": "This study aims to explore the differences between human language processing and machine language processing with the help of language model (GPT-2) representations and MEG recordings from story reading. The primary contributions can be summarized as follows: exploration of MEG brain encoding for GPT-2 representations fine-tuned on 3 tasks such as emotional understanding, figurative language processing, and physical commonsense tasks.  This resulted in tasks related to emotion and figurative language showing improved alignment with brain responses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe exploration of task-based language model representations and their alignment with the brain is an emerging research direction. Pretrained model representations contain various sources of information sources, whereas task-based representations focus specifically on task-related details.\n2.\tConsidering that three NLP tasks, the authors' examination of their task-based encoding model's performance on MEG recordings is intriguing."
            },
            "weaknesses": {
                "value": "1.\tThe idea of task-based modeling and their alignment with Brain is not new. Recently, several research works utilized task-based representations and shown better brain alignment [1], [2], [3], [4].\n\n[1] Oota et al. 2022, Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?, NAACL-2022\n\n[2] Sun et al. 2023, Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations? IJCAI-2023\n\n[3] Aw et al. 2023, Training language models to summarize narratives improves brain alignment, ICLR-2023\n\n[4] Sun et al. 2023, Tuning In to Neural Encoding: Linking Human Brain and Artificial Supervised Representations of Language, ECAI-2023\n\n2.\tThe novelty in the paper is limited as it primarily delves into a comparison between three task-based model representations and their brain alignment. Notably, the authors have not discussed or compared with 3 previous works [1], [2] and [3].\n\n3.\tThere are lot of questions left in the methodology:\n* Why do authors consider the last layer representations of GPT-2? All the previous linguistic brain encoding studies reveal that intermediate layer representations are well aligned with brain [5], [6], [7].\n* Why do authors consider longer context length i.e. 100? Previous linguistic brain encoding studies reveal that context-length of 10-50 have better brain alignment [5], [8]. \n\n[5] Toneva et al. 2019, Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain), NeurIPS-2019\n\n[6] Jain et al. 2018, Incorporating context into language encoding models for fmri, NeurIPS-2018\n\n[7] Oota et al. 2023, Joint processing of linguistic properties in brains and language models, NeurIPS-2023\n\n[8] Oota et al. 2023, MEG Encoding using Word Context Semantics in Listening Stories, Interspeech-2023\n\n4.\tThe paper lacks clarity regarding its implications. Given the emphasis on comparing fine-tuned representations with vanilla model representations, the influence of the dataset is crucial. If the authors had fine-tuned the model on other emotional datasets, different findings might have emerged. \n5.\tAdditionally, the authors present these task-based findings specifically related to narrative story reading. It's essential to ascertain whether these findings hold true for the listening modality, for instance, by considering datasets like MEG-MASC story listening.\n6.\tThe focus of the authors is primarily on contextual word representations from the last layer of Language Model Models (LLMs). It would be valuable to explore the representation similarity between pre-trained and fine-tuned layer representations. Moreover, investigating which layers are predominantly affected during the fine-tuning process would add depth to the analysis.\n7.\tThe clarity can be improved:\n* providing more explicit details concerning the methodology and experimental procedures.\n* Figure 4 is hard to follow. More details are need.\n8. Several citations are missing [1], [2], [4], and [8]"
            },
            "questions": {
                "value": "1. Why do authors consider the last layer representations of GPT-2? What about the performance of other layer representations?\n2. Why do authors consider longer context length i.e. 100?\n3. What happens if authors can tune on 3 tasks at same time? \n4. Whether the paper findings hold true for the listening modality, for instance, by considering datasets like MEG-MASC story listening?\n5. Which layers are predominantly affected during the fine-tuning process would add depth to the analysis?\n6.  Why is the parietal region, particularly the angular gyrus, not exhibiting similar enhancement between 250-375ms, considering its association with high-level semantic processing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698949474888,
        "cdate": 1698949474888,
        "tmdate": 1699636743753,
        "mdate": 1699636743753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zeuPwnI7ya",
        "forum": "J7AwIJvR3d",
        "replyto": "J7AwIJvR3d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_u3A8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6567/Reviewer_u3A8"
        ],
        "content": {
            "summary": {
                "value": "This work explores a critical issue about language models with human neural activity, how LMs and humans acquire and use language. In this paper, it explores systematic differences between human and machine language processing using brain data - Magnetoencephalography (MEG). They found three phenomen that LMs may not capture well, they used a GPT-2 XL model to finetune on the dataset for three phenomena, and found that finetuned LMs shows improved alignment with brain responses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is an interesting to explore the connection and difference between LMs and human brain, and finetunes the LMs on a human brain datasets to show the improved alignment with brain responses on three phenomena."
            },
            "weaknesses": {
                "value": "So far No. (Sorry, I am not an expert for this topic, though I try my best to read this paper)"
            },
            "questions": {
                "value": "So far No. (Sorry, I am not an expert for this topic, though I try my best to read this paper)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698991158997,
        "cdate": 1698991158997,
        "tmdate": 1699636743626,
        "mdate": 1699636743626,
        "license": "CC BY 4.0",
        "version": 2
    }
]