[
    {
        "id": "Hy07H1rJI9",
        "forum": "OXv0zQ1umU",
        "replyto": "OXv0zQ1umU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_QMNd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_QMNd"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Evoke, an automatic prompt refinement framework designed to enhance the performance of large language models (LLMs). Traditional prompting methods often underutilize the potential of LLMs due to ad-hoc prompt selection or inefficient random search. Evoke employs an author-reviewer feedback loop with two instances of an LLM: one acting as a reviewer and scoring the current prompt, and the other as an author, which refines the prompt based on feedback and edit history. Additionally, Evoke incorporates a data selection approach that exposes the LLM to more challenging samples, allowing for deeper task understanding."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method outperforms state-of-the-art approaches.\n\nThe paper is well-written and easy to follow.\n\nThe approach is not human labor intensive.\n\nThe self improving adversarial setting is interesting."
            },
            "weaknesses": {
                "value": "Not enough details are provided about loss functions of each module.\n\nNot enough explanation of the modules; making it difficult to reproduce for other researchers."
            },
            "questions": {
                "value": "What are the loss functions for each module? \n\nWhat are the targets for example for first difficulty ranking module?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698587326901,
        "cdate": 1698587326901,
        "tmdate": 1699636471167,
        "mdate": 1699636471167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KhH9ehLs8m",
        "forum": "OXv0zQ1umU",
        "replyto": "OXv0zQ1umU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_7Mrx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_7Mrx"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a method for refining prompts using GPT4. It uses GPT4 in three settings: as an author (to refine prompts) as a Reviewer (to assess quality of prompts and give feedback) as a selector (to select good examples to include in prompts). \n\nThe author instance takes as input the current prompt, and the historical feedback to create a new prompt. The reviewer takes the previous prompts, end-task accuracy when using the prompts to assess quality of the new prompt. Lastly, the selector identifies instances that would be difficult for the model to answer given the current prompt and these instances are used by the author and the reviewer.\n\n Experiments have been presented with qualitative examples from the Instruction Induction datasets. Comparisons against human-generated prompts, and a baseline method for automated prompt engineering ( a method based on paraphrasing existing prompt candidates) reveal significant improvements on multiple tasks including logical fallacy detection, movie recommendation, common concept identification. Experiments on perturbed prompts reveal that the method also results in improved robustness towards typographical errors as it ends up generating instructions that explain how those should be handled by the model. The paper also presents an analysis indicating a correlation between the Reviewer scores and the end-task accuracy. It also shows that as the cycle repeats, the author generates better prompts (as determined improved end-task accuracy) and that the use of the selector for identifying difficult instances help improve performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Simple method to improve the performance of prompts\n- Experiments on instruction induction tasks show a significant improvement"
            },
            "weaknesses": {
                "value": "- Writing needs to be improved in some parts (more details in questions)\n- Studies only GPT4 - unclear if there are any takeaways from the paper if one isn't using GPT4.  (see question)"
            },
            "questions": {
                "value": "1.  The choice of using a reviewer to estimate scores instead of directly relying on end-task scores is an interesting one? Is it for computational reasons/costs to query GPT4? An elaboration would help (I may have misunderstood this as aspect). I had to re-read the sections describing the author, reviewer, selector multiple times along with the algo block to be sure of whats going on. This section could benefit from some more details in an image and/or writing. \n2. While the paper only uses GPT4, I was wondering if the authors could experiment with open-access models such as Falcon 180B and other smaller-scale models. I am sure that the authors would agree \"LLMs\" cannot do whatever the paper describes in general -- if someone does not use the GPT4 OpenAI model, is there a takeaway from this paper? A study of how this behavior changes with scale (of model sizes) could be a useful addition."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698598464364,
        "cdate": 1698598464364,
        "tmdate": 1699636471084,
        "mdate": 1699636471084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "913Bee3MIs",
        "forum": "OXv0zQ1umU",
        "replyto": "OXv0zQ1umU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_PdAc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_PdAc"
        ],
        "content": {
            "summary": {
                "value": "This article introduces an innovative approach to optimizing the prompts for large language models. It presents the \"Evoke\" method, which combines three instances of the language model to generate, verify, and select prompts for each example. After several iterations, the generated prompts guided by the performance on the training set will generalize well to the test set. The experimental results demonstrate that Evoke outperforms several baseline methods on challenging datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of automatic prompt generation addressed in this paper holds significant importance in the age of Large Language Models (LLMs). While LLMs can generalize across various tasks, the quality of the generated prompts plays a pivotal role in overall task performance. Therefore, I believe that researching this problem is both important and intriguing.\n\n2. The proposed Evoke method offers a straightforward yet highly effective solution. It seamlessly combines different types of LLMs to create a versatile pipeline for task resolution, all without the need for additional training phases while maintaining exceptional performance.\n\n3. The analysis of Evoke's robustness and suitability for fine-grained tasks underscores its versatility and applicability in a wide range of scenarios."
            },
            "weaknesses": {
                "value": "1. The Evoke method shares significant similarities with \"Large Language Models as Optimizers\" from DeepMind. While there are variations in the detailed processes, the core contributions and methodologies closely resemble each other. Since the DeepMind paper was released before the ICLR deadline, it is imperative to include a discussion of this relationship in the paper.\n\n2. Some of the detailed tables and figures could be relocated to the appendix to allow ample space for a more comprehensive description of the experimental settings and the Evoke method itself. A more detailed explanation of the Evoke process would enhance the paper's clarity.\n\n3. The rationale behind selecting the Instruction Induction and Big Bench Instruction Induction datasets is not clearly articulated. Some of these tasks may be practical, while others could be considered more akin to toy tasks. Additionally, the use of the LLM as merely GPT-4, a black-box large language model, raises the question of whether the Evoke method is equally effective with other large language models, such as Llama 2 or ChatGPT.\n\n4. Comparing Evoke to other baseline methods may be perceived as somewhat unfair, given that Evoke leverages supervised information from the training dataset. This distinction should be acknowledged when discussing the comparative results."
            },
            "questions": {
                "value": "1. A key concern revolves around the distinctions between Evoke and \"Large Language Models as Optimizers\" by DeepMind. Clarifying these differences is pivotal for a comprehensive understanding of both approaches.\n\n2. Could you provide more information regarding the number of steps involved in generating prompts for each task? Additionally, is the prompt length substantial for each task?\n\n3. Efficiency may be a concern when dealing with very large training datasets, as it necessitates running the large language model multiple times on the entire training dataset to obtain evaluation scores. A discussion on potential optimizations or trade-offs for such scenarios would be beneficial.\n\n4. Section 4.4 highlights the effectiveness of the LLM-Selector. However, it remains unclear why, if the generated prompt is superior, there isn't consistently better or at least equivalent performance on easier examples. This phenomenon is briefly mentioned without a deeper exploration or analysis. Further elaboration would enhance the paper's clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820989457,
        "cdate": 1698820989457,
        "tmdate": 1699636471006,
        "mdate": 1699636471006,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rtRkBslfa5",
        "forum": "OXv0zQ1umU",
        "replyto": "OXv0zQ1umU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_hcVA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4870/Reviewer_hcVA"
        ],
        "content": {
            "summary": {
                "value": "This work proposed an author-reviewer framework, Evoke, for automatic prompt engineering. In this framework, the same LLM serves two roles: as a reviewer it scores the quality of the prompt; and as an author it refines the prompt, taking the feedback of the reviewer into account. On top of this, the authors further propose a data selection strategy, where they only expose the hard samples to the model. This work also conduct extensive experiments to demonstrate that Evoke outperforms existing automatic prompt engineering approaches.\n\n====After authors' discussion===\nI have read through the authors' response, and I think they have addressed all my concerns. This is a nice work, and many thanks for the efforts!"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "[+] The Evoke framework is novel in its approach to improving LLM prompt refinement using an author-reviewer framework (with a data selector).\n\n[+] The performance improvements seems impressive especially on some very difficult tasks."
            },
            "weaknesses": {
                "value": "[-] Iteration-Dependent: The efficacy of the approach may heavily depend on the number of iterations and the quality of feedback loops, which could vary significantly based on the complexity of the task and the initial prompt quality.\n\n[-] Complexity and Overhead: The Evoke model might introduce additional complexity and computational overhead since we need to do the iterative selection process T times."
            },
            "questions": {
                "value": "- Are all the prompts generated by Evoke concatenated together during the evaluation phase?\n\n- How do the quality of initial prompts affect the effectiveness of the Evoke method throughout iterations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4870/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4870/Reviewer_hcVA",
                    "ICLR.cc/2024/Conference/Submission4870/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698990464045,
        "cdate": 1698990464045,
        "tmdate": 1700728107523,
        "mdate": 1700728107523,
        "license": "CC BY 4.0",
        "version": 2
    }
]