[
    {
        "id": "TNZA8K94NQ",
        "forum": "T2ToleSDk6",
        "replyto": "T2ToleSDk6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_KCBL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_KCBL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an inverse constraint learning RL problem based on the agreement with expert demonstrations. The authors introduce a strange and potentially problematic way to define the cost, i.e., as the divergence to the expert occupancy measure ($D_f(d\\|d^E)-\\epsilon$). The paper then constructs a DICE framework and then models this problem as a bi-level optimization problem: at one level, solving an offline RL problem and on the other level, minimizing the divergence of the optimal state-action occupancy measure $d^*$ and the expert occupancy measure $d^E$. To solve this problem, however, the authors propose a practical implementation that completely deviates from the DICE formalism, that uses an in-sample learning offline RL framework like SQL. There are very large gaps between the theoretical derivation and the practical algorithm. Lastly, I think the paper shares quite a lot of similarities with RGM [1] from high-level ideas to problem formulation, but never mentioned RGM in the paper. This actually makes me wonder whether the proposed method is essentially performing reward correction or constraint learning as claimed by the authors. Please see the following strengths and weaknesses for detailed comments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Constraint learning in the offline setting is a meaningful problem and worth investigating.\n- The paper is easy to read and well organized.\n- Relative comprehensive experiments."
            },
            "weaknesses": {
                "value": "- The biggest concern I have is the way this paper models cost values. It approximates cost value as $\\lambda_d (D_f(d\\| d^E)-\\epsilon)$. This is very problematic since being sub-optimal does not necessarily mean it is unsafe or a cost violation. Matching with expert occupancy measures is essentially doing some sort of inverse RL on rewards rather than conventional constraint learning.\n- The model construction is highly similar to RGM [1], but it is not even mentioned in the paper. RGM considers a reward correction problem using a similar DICE-based framework. Both the proposed IDVE and RGM use an expert dataset and a large unknown/sub-optimal quality dataset as inputs. Both methods model the problem as similar bi-level optimization formulations: at one level minimize $D_f(d^*\\| d^E)$ and on the other level solve an offline RL problem. The only difference is that RGM learns a reward correction term and this paper learns a cost value function $V^c$. Given the similar high-level formulation, I suspect the proposed method is essentially doing some sort of reward correction rather than constraint learning.\n- There are many designs in this paper that look not very principled. For example, although the problem is formulated in a DICE framework, the authors use techniques from in-sample learning offline RL algorithms to construct the practical algorithm. First of all, the \"state-value\" $V$ in DICE-based methods is not the common meaning of state-value functions in typical RL problems, they are actually Lagrangian multipliers. If the authors are familiar with the DICE-class of algorithms, they will notice that the $V$ values in DICE algorithms take very different values and in many cases behave differently from those in common RL algorithms. Hence simply learning value functions using in-sample learning algorithms like IQL or SQL and then plugging them back into a DICE formulation is inappropriate. Also, there are lots of tricks and somewhat arbitrary designs in the practical algorithm, like introducing $\\lambda$ in Eq.(12), and the treatment in Eq.(15)-(17). By contrast, RGM offers a much cleaner and more rigorous derivation from theory to practical algorithm."
            },
            "questions": {
                "value": "- Can you justify the difference between the proposed constraint inference as compared to the reward correction in RGM[1]?\n- Why not consider learning $V^r$ and $V^c$ using typical DICE-based techniques, like RGM[1] or SMODICE[2]?\n\n**References:**\n\n[1] Li, J., et al. Mind the Gap: Offline Policy Optimization for Imperfect Rewards. ICLR 2023.\n\n[2] Ma, Y., et al. Versatile offline imitation from observations and examples via regularized state-occupancy matching. ICML 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698476913111,
        "cdate": 1698476913111,
        "tmdate": 1699636479261,
        "mdate": 1699636479261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "25gruieUXf",
        "forum": "T2ToleSDk6",
        "replyto": "T2ToleSDk6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_86gQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_86gQ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the inverse constrained reinforcement learning (ICRL) problem. Previous works in ICRL primarily consider the online setting which allows online interactions with the environment. However, in safety-critical tasks, it is often dangerous to iteratively collect samples in the environment since the data-collecting policies may violate the constraints. To address this issue, this paper focuses on the offline ICRL and proposes an Inverse Dual Values Estimation (IDVE) framework. IDVE derives the dual values functions for both rewards and costs, estimating their values in a bi-level optimization problem. To implement IDVE, this paper introduces several techniques: 1) handling unknown transitions, 2) scaling to continuous environments, and 3) controlling the degree of sparsity regularization. The empirical results show that IDVE can accurately recover the constraints and achieve high returns."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper identifies the safety issues in online ICRL. To address this issue, this paper introduces the offline ICRL problem which is more practical.\n2. The paper is well-written and easy to follow, providing clear explanations and detailed descriptions of the proposed method and experimental results."
            },
            "weaknesses": {
                "value": "The derivation of the proposed framework IDEV and its practical implementation introduces many unreasonable transformations, which makes the algorithm lack soundness.\n\n1. In the first paragraph of Section 4.2, the authors approximate the expert regularizer $\\lambda_d (D_f (d||d^E) - \\epsilon)$ with $\\mathbb{E}_d [\\delta^c_V (s, a)]$. However, $\\mathbb{E}_d [\\delta^c_V (s, a)]$ is not a reasonable approximation of $\\lambda_d (D_f (d||d^E) - \\epsilon)$. There is even no connection between $\\mathbb{E}_d [\\delta^c_V (s, a)]$ and $\\lambda_d (D_f (d||d^E) - \\epsilon)$: the former is a temporal difference term w.r.t the cost function while the latter is a divergence between two distributions.\n2. The introduction of the lower-level optimization in Eq.(11) is weird. The authors have replaced the expert regularizer with another term. However, they again add this regularizer in the bi-level optimization.\n3. In the last line of Section 5.1, they introduce the approximation $Q^c(s, a)=\\mathbb{E}_\\{\\left(s, a, s^{\\prime}\\right) \\in \\mathcal{D}^o}\\left[\\gamma V\\_{\\theta^c}^c\\left(s^{\\prime}\\right)\\right]$. This approximation is incorrect. The correct one is $Q^c(s, a)= c(s, a) + \\gamma \\mathbb{E}\\_{s^\\prime \\sim p (\\cdot|s, a)} [V^c\\_{\\theta^c} (s^\\prime)]$.\n\n\n\n\nBesides, the experiment setup is a little weird. In particular, the offline dataset in grid-world is collected by random policies, and the offline dataset in MuJoCo is collected by SAC policies. Such offline datasets may contain a large number of unsafe behaviors, contradicting the motivation of this paper. Thus, a more proper choice is to apply safe and conservative policies to collect the offline dataset."
            },
            "questions": {
                "value": "1. What is the meaning of the threshold $\\hat{\\epsilon}$ in Definition 1?\n2. Typos:\n    1. In the last paragraph of Section 3, \u201cThe demonstration dataset $\\mathcal{D}_{O}$\u201d should be \u201cThe demonstration dataset $\\mathcal{D}^{O}$\u201d.\n    2. The notation of $\\sum_{p_{\\mathcal{T}}\\left(s^{\\prime} \\mid s, a\\right)} \\gamma V^r\\left(s^{\\prime}\\right)$ is confusing. The correct one should be $\\sum_{s^\\prime \\in \\mathcal{S}} p_{\\mathcal{T}}\\left(s^{\\prime} \\mid s, a\\right) \\gamma V^r\\left(s^{\\prime}\\right)$ or $\\mathbb{E}\\_{s^\\prime \\sim p\\_{\\mathcal{T}}\\left(\\cdot \\mid s, a\\right) }  [\\gamma V^r\\left(s^{\\prime}\\right)]$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4933/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4933/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4933/Reviewer_86gQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698501553952,
        "cdate": 1698501553952,
        "tmdate": 1699636479176,
        "mdate": 1699636479176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F68dbj7pcj",
        "forum": "T2ToleSDk6",
        "replyto": "T2ToleSDk6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_US7L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4933/Reviewer_US7L"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the offline safe RL scenario where rewards are observable but costs are hidden. The objective is to derive the cost function from the dataset, akin to the IRL setup. The authors introduce IDVE, built on the DICE family's dual formulation of offline RL. It approximates the cost value function by measuring deviations from expert demonstrations since the cost signal isn't directly observed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed setting appears to be realistic to me.\n\n- The overall algorithmic designs make sense, although I do have concerns with a few choices.\n\n- Section 6.1 effectively visualizes the recovered constraints and the impact of various components."
            },
            "weaknesses": {
                "value": "- Related works: \nAlthough IDVE is closely linked to the DICE family, formulated with the distribution correction estimation $w(s, a)$, there is no discussion and referencing to the DICE string of works [e.g., 1, 2]. Inclusion of OptiDICE [3] into discussion is also recommended as it also uses a closed-form solution for the inner maximization, in the DICE framework.\n\n- Experiments: \n\n    - While considering no costs, the comparison between IDVE w/oA and offline RL is somewhat questionable. In scenarios like limited arm halfcheetah and blocked halfcheetah, IDVE w/oA shows notably superior cumulative rewards. The inferiority of offline RL suggests potential issues with the baseline's strength or its implementation. Given that the offline RL's objective solely maximizes returns, one would anticipate it to at least match, if not surpass, IVE w/oA in terms of returns.\n\n    - The annotation for the dashed line in Figure 6 appears to be missing. I would also recommend the authors to plot average cumulative rewards/costs for both $D^E$ and $D^{\\neg E}$. It will be helpful for the audiences to better understand the numbers in Table 2. \n\n    - (Continued) The gap in returns between expert and sub-optimal demonstrations, as well as between offline IL and expert demonstrations, is unclear. This ambiguity arises because the environments have been modified, eliminating the availability of standardized D4RL scores for comparison. Therefore, plotting rewards/costs of both $D^E$ and $D^{\\neg E}$ would be helpful to improve clarity.\n\n\n[1] Kostrikov, I., Nachum, O., and Tompson, J. (2019). Imitation learning via off-policy distribution matching. arXiv preprint arXiv:1912.05032.\n\n[2] Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D. (2019). Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074.\n\n[3] Lee, Jongmin, et al. \"Optidice: Offline policy optimization via stationary distribution correction estimation.\" International Conference on Machine Learning. PMLR, 2021."
            },
            "questions": {
                "value": "- Section 5.2: The choice of $\\pi \\propto \\delta^r-\\delta^c$ is somewhat odd. Such a $\\pi$ won't optimize the inner equation of Eq (7), given the current value estimations. I wonder why was this chosen over $\\pi \\propto \\exp(\\delta^r-\\delta^c)$?\n\n- Figure 3: \nIn offline RL/IL, the value functions are solely asscociated with rewards. How, then, were the constraints derived from these methods?\n\n- Table 5 (Appendix): The varying number of expert transitions for different tasks is appears a bit random to me. Could the authors provide clarity on this decision?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697310269,
        "cdate": 1698697310269,
        "tmdate": 1699636479095,
        "mdate": 1699636479095,
        "license": "CC BY 4.0",
        "version": 2
    }
]