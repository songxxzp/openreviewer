[
    {
        "id": "bQjzUWcesn",
        "forum": "T8Rf1CRbHQ",
        "replyto": "T8Rf1CRbHQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_3q7e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_3q7e"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the convergence of error-feedback two-time-scale SA, and apply their results to three typical application scenarios: error-compensated TTSA with arbitrary compressors, local TTSA with periodic global averaging, and TTSA with delayed updates. In all cases, similar rates $O(1/T+1/T^2)$ are derived, where only the second term is affected by structured perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized and self-contained. The structure is very clear, starting from a generic convergence lemma and delving into three case studies.\n2. The originality and motivation is good. The author considers the error-feedback setting in TTSA, which is not studied in literature."
            },
            "weaknesses": {
                "value": "1. Some definitions and deductions can be further explained to be more reader-friendly. For example,\n - it would be better to explain that $\\bar{x_k}, \\bar{y_k}$ are TTSA iterates without error feedback after Assumption 4.\n - In Assumption 4, the r.v.s should be $\\xi_k, \\psi_k$.\n - Adding explanation and the assumptions and lemmas used after deductions in Appendix would be better, e.g. Lemma 6 (7)(8). (How is $\\nabla y^*(\\bar{x}_k)$ controlled by $\\|\\xi\\|$ in (7)?)\n2. Although the paper is self-contained and the motivation of problem is clear, the content is not as much and is entirely restricted in the EF-TTSA convergence analysis. In addition, comparisons with one-time-scale SA or with non-error-feedback may be desired. It would be better if the authors could add some simulation studies to illustrate the difference between EF-TTSA convergence performance and the above mentioned settings, so as to show the significance of the analysis."
            },
            "questions": {
                "value": "Discussed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Reviewer_3q7e"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389557548,
        "cdate": 1698389557548,
        "tmdate": 1699636439504,
        "mdate": 1699636439504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "paTqocLZt7",
        "forum": "T8Rf1CRbHQ",
        "replyto": "T8Rf1CRbHQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_hjVi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_hjVi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies error-feedback-based two-time-scale stochastic approximation. The authors consider three types of structured perturbations including compression, local updates, and delays, and establish non-asymptotic convergence rates. In particular, the leading term $O(1/T)$ in the convergence rate is not affected by the error terms, demonstrating the robustness of two-time-scale stochastic approximation to these structured perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized and easy to follow.\n2. To the best of my knowledge, this is the first work to show the robustness of two-time-scale stochastic approximation to structured perturbations."
            },
            "weaknesses": {
                "value": "1. The title is somewhat exaggerated. In the theoretical analysis, $\\beta_k / \\alpha_k$ remains a constant. This is only a single-time-scale special case of TTSA. It would be more appropriate to use similar wording to that in [1], e.g., single-time-scale stochastic approximation with two coupled sequences.\n2. The assumptions are too strong. For example, this paper only considers the strongly monotone case, while the two previous works [1,2] also study non-strongly monotone cases. The requirement on noise is also restrictive (see Question 1).\n3. The proof novelty is somewhat restricted as it heavily relies on the proof techniques in the literature, e.g., those in [1,2].\n\n[1] Han Shen and Tianyi Chen. A single-timescale analysis for stochastic approximation with multiple\ncoupled sequences. Advances in Neural Information Processing Systems, 35:17415\u201317429, 2022\n\n[2] Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for\nsgd with delayed gradients and compressed updates. The Journal of Machine Learning Research,\n21(1):9613\u20139648, 2020.\n\nSome typos:\n* Line 8 in Section 1: 'robustness to robust to'\n* The second paragraph in Section 1: TTSA was introduced in (Borkar, 1997)."
            },
            "questions": {
                "value": "1. Assumption 4 requires the noise to have a bounded norm. However, it is generally assumed that the noise has bounded second-order moments. What is the reason for making such a restrictive assumption?\nMoreover, Assumption 4 also requires the noise to have zero mean, while in some examples, e.g. stochastic bilevel optimization, it is almost impossible to satisfy such a condition. It would be more appropriate to assume the bias of noise is bounded in some sense, e.g., by the square root of the step size in [1].\n\n2. All the convergence rates in this paper are in terms of the weighted sum of squared norms, while for unbiased SA or TTSA, the last-iterate convergence is achievable. Is this an inevitable issue when there exist structured errors? Or how does the last iterate behave in this case?\n\n[1] Han Shen and Tianyi Chen. A single-timescale analysis for stochastic approximation with multiple\ncoupled sequences. Advances in Neural Information Processing Systems, 35:17415\u201317429, 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4606/Reviewer_hjVi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504275730,
        "cdate": 1698504275730,
        "tmdate": 1699636439419,
        "mdate": 1699636439419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HdShLsmaKM",
        "forum": "T8Rf1CRbHQ",
        "replyto": "T8Rf1CRbHQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_FQA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4606/Reviewer_FQA6"
        ],
        "content": {
            "summary": {
                "value": "This paper considers error-feedback based two-timescale stochastic approximatio (EF-TTSA) algorithms; and derive convergence bounds for the EF-TTSA algorithm. It later discusses the applications of the results to explain different variants of EF-TTSA, including error-compensated TTSA with compression, local TTSA with periodic global averaging, and TTSA with delayed updates."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Convergence results for the so-called unifying EF-TTSA framework.\n+ Applications of the results to different instances of EF-TTSA in ML/RL."
            },
            "weaknesses": {
                "value": "- The paper presents a mixture of different results and tries to explain them all in a so-called unifying EF-TTSA framework. These results can be illusive and make the readers difficult to follow the key idea/contributions of the paper. \n- Why shall one study error-feedback in the two-timescale setting is not well-motivated. I understand that stochastic bilevel optimization and stochastic compositional optimization can be cast as special cases of or solved by two-timescale SA algorithms in equations (11-22). Yet, how and where does the error feedback come into the picture? It seems also difficult to intepret the two applications/algrorithms as (23-26). Moreover, in (23-26), it would be clearer if what each of the symbols {x_k,y_k,d_k,e_k} stands or represents in the context of error-feedback SA is explained first. The set of equations in (23-26) looks more like \"a four-timescale SA\". It is not clear how the EF-TTSA differentiates it from a four-timescale algorithm like this, and is there any advantage treating it as a EF-TTSA?\n- Large-scale numerical tests should have been provided to demonstrate the effectiveness of the EF for TTSA as well as motivate the study of the paper."
            },
            "questions": {
                "value": "1) In Assumption 4, the authors cited \"we make the following standard assumption Doan (2022)\". Nonetheless, in assumption 4 of Doan (2022), the variables are only assumed zero-mean and with covariances; no bounded assumptions as in (27) have been made. I am not sure whether this can still account for the two instances in Section 2 or not. Furthermore, the paper defines the filtration F_k using only the estimate sequences {x_k} and {y_k} up to time k, which is different than that made in Doan (2022) (which takes also the random variables in the filtration, and assumes the conditional zero mean random variables). Please explain how the random variables are taken care of in the filtration analysis. \n2) In Section 4, Assumption 5, the compression operator Q is assumed to satisfy the condition (31) which is a not a mild assumption. It is not fair to claim the results hold for \"arbitrary compression\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739851250,
        "cdate": 1698739851250,
        "tmdate": 1699636439347,
        "mdate": 1699636439347,
        "license": "CC BY 4.0",
        "version": 2
    }
]