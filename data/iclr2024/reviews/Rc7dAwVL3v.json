[
    {
        "id": "kTpU8lwTfD",
        "forum": "Rc7dAwVL3v",
        "replyto": "Rc7dAwVL3v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_8Wev"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a latent diffusion-based speech synthesis framework for high-quality zero-shot speech synthesis. They utilize an audio codec as a latent representation and a conditional latent diffusion model could generate a latent representation. Then, the codec decoder generates a waveform audio. The zero-shot results show a better performance than the codec-based TTS model and YourTTS. Moreover, the audio quality is good."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "They propose the latent diffusion-based speech synthesis model. This work may be the first successful implementation of a latent diffusion model for speech synthesis. Although recently large-language model (LLM) -based speech synthesis models have been investigated, they have too many problems for speech synthesis resulting from the auto-regressive generative manner. However, this work adopts a parallel synthesis framework with latent diffusion, and successfully shows their generative performance by several speech tasks.\n\nRecent papers only compare their work with YourTTS but I do not think YourTTS is a good zero-shot TTS model. The audio quality of YourTTS is too bad. However, although recent models do not provide an official implementation, the authors tried to compare their model with many other works."
            },
            "weaknesses": {
                "value": "1. They also conducted an ablation study well. However, it would be better if the authors could add the results according to the dataset and model size. The model size of NaturalSpeech 2 is too complex compared to VITS. In my personal experience, VITS with speaker prompt could achieve significantly better performance than YourTTS. \n \n2. For inference speed, NaturalSpeech 2 still has a low latency for its iterative generation. Although this discussion is included in the Appendix, it would be better if the authors could add the discussion of inference speed in the main text. This is just a limitation of diffusion models so I acknowledge the trade-off between quality and inference speed. Furthermore, I hope to know other metrics of NaturaSpeech 2 according to Diffusion Steps (WER or Similarity metric). Recently, Flow matching using optimal transport is utilized for fast speech synthesis. This could be adopted to this work. \n\n3. Some details are missing. Please see the questions."
            },
            "questions": {
                "value": "1. This work utilizes a quantized latent vector for latent representation. In my experience, the quality of the model with the continuous latent representation before quantization showed a better performance in latent diffusion model for singing voice synthesis. Have you tried to train your model with the pre- or post-quantized representation for latent representation?\n\n2. The details of singing voice synthesis are missing. It would be better if you could add the details for pre-processing of musical scores. How do you extract the duration of phonemes in this work?\n\n3. How do you extract the pitch information? This significantly affects the performance so the details should be included. (about F0 min, F0 max, resolution, and pitch extraction algorithm).\n\n4. The authors may train the audio codec with their speech dataset. I think it is important to utilize a high-quality speech codec for high-quality speech synthesis. In this regard, I hope that the authors will mention about this part by comparing your model with the same model utilizing an official Soundstream codec as a latent representation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649811393,
        "cdate": 1698649811393,
        "tmdate": 1699636473179,
        "mdate": 1699636473179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kFgLdFRMxI",
        "forum": "Rc7dAwVL3v",
        "replyto": "Rc7dAwVL3v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new TTS model that is capable of generating speech with diverse speaker identities, prosody, and styles, in zero-shot scenarios and it can also sing. It outperforms the current SOTA methods in both objective and subjective metrics. The way it works is the following. First the neural audio codec that converts a speech waveform into a sequence of latent vectors with a codec encoder, and reconstructs the speech waveform from these latent vectors with a codec decoder. Then the codec encoder extracts the latent vectors from the speech and uses them as the target of the latent diffusion model which is conditioned on prior vectors. During inference it generates the latent vectors from text using the diffusion model and then generate the speech waveform using the codec decoder."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-Paper is very well written and provides good intuition and justification for all model choices that the authors have made. These choices are intuitive to make generated speech more natural and to overcome past bottlenecks in previous methods.\n-The new TTS algorithm has many capabilities such as generating diverse speech (different speakers, prosody, style) and in zero-shot scenarios. Singing is a bonus in this case.\n-NaturalSpeech2 beats current SOTA methods in both objective and subjective metrics.\n-Related work section is quite extensive.\n-In the end I believe that this work is a good contribution to the community."
            },
            "weaknesses": {
                "value": "-One can hear in the more strenuous experiments that the audio samples have some kind of weird pitch or pace of speaking.\n-Paper might not be a very good fit in this venue. Although it has to do with learning representations, NaturalSpeech2 is more fit for a Speech venue such as InterSpeech or ICASSP."
            },
            "questions": {
                "value": "-Why did the authors not include any experiments with single speaker data like LJSpeech.\n-It would be interesting to hear some audio samples with people that have an accent. This has not been explored in the community.\n-As an ablation what would be the shortest prompt in seconds that you can give for zero-speech synthesis?\n-After the phoneme Encoder you have a Duration and Pitch predictor. Why didn't you also include an Energy Predictor like the authors did in FastSpeech2 since the idea seems to be derived form there?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "They authors address this issue in the conclusions section. After all this is a speech synthesis work and it can be misused in the future. For this venue though I wouldn't want to see this work getting rejected because of this."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_69Md",
                    "ICLR.cc/2024/Conference/Submission4886/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778307847,
        "cdate": 1698778307847,
        "tmdate": 1700420444203,
        "mdate": 1700420444203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RoNRaajMH1",
        "forum": "Rc7dAwVL3v",
        "replyto": "Rc7dAwVL3v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_9oT1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_9oT1"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a TTS model combining a number of modern components these include in-context learning (prompting) a diffusion model to connect conditioning information to latents, and latents defined by an autoencoder for waveform reconstruction.\n\nThe resulting model has many of the zero-shot capabilities of LM based TTS that have been presented in recent years, but by maintaining duration prediction for alignment, the model stays robust to a hallucination and dropping errors that impact other generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The model contains innovative structures in the in context learning for duration and pitch, and in the diffusion model.  Moreover the overall structuring of these components is novel.\n\nThe quality of the model is quite high and provides some important balancing between zero-shot capabilities and robustness compared to alternate models"
            },
            "weaknesses": {
                "value": "The paper is sometimes unclear with regards to what the model components represent and how the components fit together.  For example, the use of SoundStream and wavenet is not obvious.  These are previously published approaches, that are used in novel ways here.  It took multiple readings to understand how they are being used in this paper, and even still i\u2019m not 100% sure that my understanding is correct.  Broadly, the paper relies too heavily on Figure 1.0 to describe how the model fits together. \n\nThe argumentation around continuous vs discrete tokens is very hard to follow.  It\u2019s not clear why the discrete token sequence must necessarily be longer than a continuous sequence (Introduction).  The first three pages spend a lot of effort describing why a continuous representation is a better fit for this task.  Then in Section 3.1 \u201cHowever, for regularization and efficiency purposes we use residual vector quantizers with a very large number of quantizers and codebook tokens to approximate the continuous vectors.  This provides two benefits\u2026\u201d This is a particularly surprising turn of the argument to then go on to describe why discrete tokens are useful here.\n\nThe diffusion formulation is too compact to be clearly followed.  Page 5. The following sentence includes a number of ambiguities.  \u201cThen we calculate the L2 distance between the residual vector with each codebook embedding in quantizer j and get a probability distribution with a softmax function, and then calculate the cross-entropy loss between the ID of the ground-truth quantized embedding ej and this probability distribution. Lce\u2212rvq is the mean of the cross-entropy loss in all R residual quantizers, and \u03bbce\u2212rvq is set to 0.1\u201d  I\u2019d recommend including an appendix entry or describing each clause separately in place."
            },
            "questions": {
                "value": "Introduction \u201cthe zero-shot capability that is important to achieve diverse speech synthesis\u201d \u2013 why is zero-shot necessary for diverse speech synthesis?  Also, for what contexts, and use-cases is diverse speech synthesis necessary?\n\nIn the introduction \u2013 the checklist between NaturalSpeech 2 and \u201cprevious systems\u201d is somewhat strange.  Certainly there are previous systems that are non-autoregressive, or use discrete tokens.  I understand that this is not \u201call previous systems\u201d but those listed. But why compare only to those three related systems? The introduction and related work draw contrast with a variety of alternate TTS models.\n\nWhy use a diffusion model instead of any other NAR model?\n\nWhen presenting the \u201cprior model\u201d in section 3.2 is the phone encoder, duration predictor and pitch predictor pre-trained to some other target? or is there some other notion of a prior model here?\n\nWhat is the units used in the L_pitch loss? Hz? log Hz? something else?\n\nThe variable z is used in a number of different ways, could this be clarified (e.g. in Figure 2 between the prompt, input to diffusion model and output?)\n\nSection 4.1\nPage 6 \u201cBoth speakers in the two datasets\u201d are there only 2 speakers in the data sets?\nPage 6 what is value of sigma in the sigma-second audio segment as a prompt?\n\nHow much loss is incurred by filtering the output by a speech scoring model?  E.g.  are 99% of utterances accepted? or 1%?  \n\nNote: VCTK utterances are particularly noisy making is a poor comparison for CMOS, but the comparison to Librispeech is more representative.\n\nSection 4.2 \u201cWe apply the alignment tool\u201d \u2013 which alignment tool?\n\nWhat is the variance of the prosodic measures \u2013 it\u2019s hard to track whether the differences in Table 3 are significant or not.\n\n\u201cWhen we disable the speech prompt in diffusion, the model cannot converge\u201d \u2013 this seems remarkable.  Why does the model require a speech prompt to learn?\n\nBroader Impacts: What would such a protocol to protect users from misuse of this model look like? Presumably this model can generalize to unseen speakers already \u2013 so what protections are in place regarding the use of this model as of publication?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Zero-shot synthesizers have a strong potential for misuse."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786838685,
        "cdate": 1698786838685,
        "tmdate": 1699636472982,
        "mdate": 1699636472982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5TQYisWqJ0",
        "forum": "Rc7dAwVL3v",
        "replyto": "Rc7dAwVL3v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
        ],
        "content": {
            "summary": {
                "value": "This paper presents NaturalSpeech 2, a non-autoregressive TTS model that employs a diffusion mechanism to generate quantized latent vectors from neural audio codecs. It shows enhanced zero-shot TTS performance relative to the state-of-the-art large-scale neural codec language model. The proposed approach exhibits advancements in sample quality, intelligibility, robustness, speaker similarity, and generation speed when benchmarked against the baseline method. The authors further validate the superiority of their method over other alternatives via comprehensive qualitative and quantitative evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper effectively tackles several major challenges inherent to non-autoregressive TTS modeling at scale. \n* The authors have carried out robust and wide-ranging experiments, yielding detailed results.\n* The reference list is both extensive and comprehensive."
            },
            "weaknesses": {
                "value": "The proposed method's intricate modeling could hinder its extension to other applications. While the introduced model applies diffusion, it necessitates two additional losses and requires supplementary modules like a pitch predictor, prompt encoder and the second attention block. As an example, the recent state-of-the-art flow-matching based TTS method, VoiceBox [1] consists of rather simple model architecture; the flow-matching based duration predictor and audio model.\n\n[1] Le, Matthew, et al. \"Voicebox: Text-guided multilingual universal speech generation at scale.\" arXiv preprint arXiv:2306.15687 (2023)."
            },
            "questions": {
                "value": "Given the concerns mentioned in the above weaknesses, it would be interesting to see if the proposed method could be adapted or refined to reduce its dependency on additional modules without increasing complexity or compromising sample quality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4886/Reviewer_c9Qd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843124002,
        "cdate": 1698843124002,
        "tmdate": 1699636472909,
        "mdate": 1699636472909,
        "license": "CC BY 4.0",
        "version": 2
    }
]