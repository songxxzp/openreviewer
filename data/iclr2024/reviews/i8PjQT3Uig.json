[
    {
        "id": "76XC5M0kbk",
        "forum": "i8PjQT3Uig",
        "replyto": "i8PjQT3Uig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_CrA3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_CrA3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a world model for model-based reinforcement learning (RL) which can be learned online and does not\nrequire full retraining on all previous data.\nThe authors highlight that training world models is subject to issues arising in continual learning.\nThat is, each sequential data set can be interpreted as coming from a new task and thus the world model needs to be retrained\nafter each agent-environment interaction.\nThis is because the data collected via agent-environment interaction is non-stationary.\nThey propose a world model based on a linear regression model which uses high-dimensional nonlinear features.\nImportantly, the linear model can be updated given new data whilst retaining good predictive performance on old data.\nThey compare their feature encoding method to other feature encoding techniques in an image denoising task on MNIST.\nThey then evaluate their method's ability to handle training data covariate shift in an artificial online learning experiment.\nFinally, they evaluate their method's ability to combat non-stationary data in model-based RL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper addresses an important problem in model-based RL, which is likely a problem that must be solved for developing lifelong agents.\nThe method for updating the linear model online is simple but appears to be effective.\nThis is also the first time I have seen a non-trainable encoder used for world models.\nIt is very common to see world models with NN encoders and NN transition models operating in the encoder's latent space.\nTypically the dynamic model operates on a latent state which is lower dimensional than the high-dimensional observations.\nPerhaps I am not aware of the relevant literature, but this seems like an interesting and original idea."
            },
            "weaknesses": {
                "value": "This paper has two main weaknesses.\nFirstly, there is no comparison to state-of-the-art MBRL strategies that use world models, e.g. Dreamer/TD-MPC.\nAs such, there is no experiment highlighting the main issue that the paper is trying to address:\nthat NN-based world models suffer from catastrophic interference due to non-stationary data.\nSecond, all of the RL experiments are in simple RL environments.\nFrom the current results, it is impossible to know how practically useful this world model is.\nThere is no discussion about its limitations nor is there a comparison to other model-based RL algorithms that use world models.\nSure the proposed method works on some simple RL environments but can it scale to difficult environments like humanoid and can it handle image-based observations?\nIt is OK if the method cannot do this but it should be addressed in the text.\nMoreover, it should be made clear what benefit it does have over other world model methods (like Dreamer).\nFor example, I'd like to see a state-of-the-art world model method (like Dreamer) performing poorly/failing because it cannot handle non-stationary data.\n\nI also have questions regarding the training of the NNs in the experiments.\nDid the full-replay experiment involve resetting the neural network's weights? If so, what initialization was used?\nWhen was the NN training stopped? Was the data split into train/validation sets and used to stop training when the\nvalidation loss stopped improving?\nThe paper needs more details to explain exactly how this was implemented.\nIn my experience, these steps are important to ensure the NNs don't overfit on early data sets.\n\nI am also unsure why the full-replay strategy (which is model-based), does not appear to have better sample\nefficiency than the model-free experiment. Am I missing something here?\nPerhaps this is an interesting point for discussion. Do the high-dimensional features sacrifice sample efficiency\nin favour of formulating a linear model which can handle the non-stationary data?\nI'm not sure if this is correct.\nMy main point here is that the paper has not answered all of my questions about the method.\n\nThe experiments tell the first part of a nice story.\nTable 1 compares to other encoding methods and Fig. 3 clearly shows how the method handles covariate shift better than NNs.\nFig. 4 also acts as a nice ablation for comparing the method to other CL strategies within the same set-up.\nHowever, the experiments section lacks a comparison to other MBRL strategies which use world models.\nIn particular, there is no experiment highlighting the main issue the paper is trying to address.\nThat is, there is no MBRL experiment failing due to the non-stationarity of the training data.\n\nMinor comments and corrections:\n- The abstract is very long. I would recommend shortening it.\n- Sections shouldn't lead straight into subsections (Section 4/4.1, 5/5.1, B/B.1, C/C.1). There should be text explaining what the reader can expect to read in the section.\n- In the first paragraph of Section 2.2, the reward function is defined as $R(\\mathbf{s}, \\mathbf{a}, \\mathbf{s}')$ but then in the optimal policy equation you use $R(\\mathbf{s}, \\mathbf{a})$.\n- Fourth line of Section 2.2 the initial state distribution is $\\rho$ but earlier it is $\\rho_0$.\n- Third paragraph of Section 2.2 - \"We firstly formulate\" should be \"We first formulate\".\n- Section 2.1 - \"When the input is a convex set $\\mathcal{S}$, the prediction a vector $\\mathbf{w}_{t} \\in \\mathcal{S}$\". This sentence doesn't read properly.\n- $\\rho$ is used to denote the initial state distribution and to denote the dimension of the grids in Section 3.2.\n- What is the value of $\\delta$ in Fig. 1?\n- What are $\\pi_0$, $\\pi_t$ and $\\pi_{t'}$?\n- The first sentence of the abstract says model-based RL has better sample efficiency. Better than what? It's model-free counterparts?\n- It is unusual to end the paper with a section titled \"Summary\". I recommend changing this to \"Conclusion\"."
            },
            "questions": {
                "value": "- What are the limitations of the proposed method? Can it handle image observations? Can it scale to difficult environments like Humanoid?\n- Why haven't you compared to any other world model algorithms? E.g. Dreamer, TD-MPC.\n- How was the NN full-replay experiment implemented? Did the full-replay experiment involve resetting the neural network's weights? If so, what initialization was used? When was the NN training stopped? Was the data split into train/validation sets and used to stop training when the validation loss stopped improving?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9441/Reviewer_CrA3",
                    "ICLR.cc/2024/Conference/Submission9441/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698155553149,
        "cdate": 1698155553149,
        "tmdate": 1700320425377,
        "mdate": 1700320425377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TzMwvS942k",
        "forum": "i8PjQT3Uig",
        "replyto": "i8PjQT3Uig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_LJy1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_LJy1"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for the online learning of a world model for model-based reinforcement learning (MBRL). To obtain efficient updates to the world model, the world model is expressed as a linear combination of a set of spare features. This efficiency allows online learning at a constant computational cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The work is well motivated in the introduction and a sufficient and clear background is provided for non-expert readers in the preliminaries section. The algorithms, definitions, etc. are mathematically rigorously presented. \n\nA comprehensive set of experiments has been conducted demonstrating the efficacy of Losse-FTL"
            },
            "weaknesses": {
                "value": "Significant discussion around catastrophic forgetting was mentioned in the introduction but little discussion is presented in the main text and left in the appendix.\n\n\u201cExample 3.1\u201d could be a regular paragraph. Formatting this as an Example does not improve readability and is, in fact, the only Example in the entire paper."
            },
            "questions": {
                "value": "In figure (1): d(s_(t+1), f(st, at)) and \\delta were not defined in the caption or anywhere obvious in the main text.\n\nIn eq (3), does || . ||^2_F denote the Frobenius matrix norm? It is only stated so after eq (5). It helps to have the notation introduced earlier here. Especially since \u201cF\u201d is the dimension of the feature space"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698642253071,
        "cdate": 1698642253071,
        "tmdate": 1699637188971,
        "mdate": 1699637188971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kQVoyHQ5Cg",
        "forum": "i8PjQT3Uig",
        "replyto": "i8PjQT3Uig",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_2Y3F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9441/Reviewer_2Y3F"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a model-based reinforcement learning approach that utilizes a sparse representation-to-representation model. The use of sparse representation aims to address the challenge of catastrophic forgetting in a reinforcement learning (RL) setting, where data generation constantly shifts. The architecture employed for model-based reinforcement learning is Dyna. The proposed method involves learning nonlinear sparse features and building a model based on this sparse representation. To enhance computational efficiency, a method for updating model weights using sparse representation is presented. Empirical experiments are included to demonstrate the effectiveness of this approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper targets an important topic of learning a model in RL \n\n2. I do not see many related works of building sparse representation-based models."
            },
            "weaknesses": {
                "value": "I will list below main weaknesses for improvements, centred around the main contribution of the paper. \n\n1. Is there any reason for why the particular sparse representation learning method is chosen? Furthermore, in the experiments part, FTA should also be compared as a baseline. It is unclear why you compare it in a supervised learning setting but omit in a RL setting. The performance on a SL setting does not invlidate/validate another. As an empirical paper, I think a rigorous comparison is necessary. \n\n2. Could you clarify do you update both your model and representation every environment time step? \n\n3. There is a critical weakness in the paper: the paper claims to develop a sparse representation-based approach for model learning, but it is not justified the reported benefits come from the use of the sparse representation for policy learning or for model learning. Note that the former has been extensive studied. in general, a full replay method should be the best in mitigating catastrophic forgetting, but the empirical results reported that the proposed algorithm can sometimes even outperform full replay. That raises a natural question that the benefit mainly comes from the policy learning part by using sparse representation, rather than the proposed model learning part. \n\n4. other issues. \n\nAlg 1. it is better to be specific, use the title Dyna architecture, rather than MBRL, as there are numerous MBRL algorithms and not everyone is as Alg 1 described. \n\nAlg 2, line 4 & 5: shouldn\u2019t it be outer product? Please specify the dimension of the matrices capital Phi and letter phi. This is nontrivial as it affects the understanding of the algorithm. \n\nThe term \u201cworld model\u201d might intrigue the readers to see much more challenging tasks than the paper presented, this can be seen by other papers using such terms. It is better to rephrase it to be more precise."
            },
            "questions": {
                "value": "see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9441/Reviewer_2Y3F"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788566706,
        "cdate": 1698788566706,
        "tmdate": 1700531073014,
        "mdate": 1700531073014,
        "license": "CC BY 4.0",
        "version": 2
    }
]