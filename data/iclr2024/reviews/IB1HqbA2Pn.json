[
    {
        "id": "YmnO2X3cP1",
        "forum": "IB1HqbA2Pn",
        "replyto": "IB1HqbA2Pn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_xZda"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_xZda"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LLaVA-Plus, an end-to-end training approach aimed at enhancing large multimodal models (LMM) to create general-purpose multimodal agents. LLaVA-Plus maintains a skill repository consisting of various vision and vision-language pre-trained models, which are activated based on user instructions and input images, allowing the LMM to learn and apply skills dynamically to complete real-world tasks. The approach leverages multimodal instruction-following data for tool use and outperforms existing methods, showing superior performance in multimodal tool use and enabling new scenarios by considering query images throughout the interaction process."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This work does an incredible job of combining tool chaining methods with visual and language models that figures out what tools to use given a task. This method has been thoroughly tested with a large number of tasks and datasets and has been shown to b"
            },
            "weaknesses": {
                "value": "My only gripe with the paper is the presentation. Some parts of the paper have been written in such a way that does not give the impression that this work is sufficiently novel from LLaVa and ToolFormer."
            },
            "questions": {
                "value": "Here are some suggestions to improve the presentation of the work:\n- In the discussion section, include the following subsections `How does our work differ from LLaVA?` and `How does our work differ from ToolFormer?`\n- Figure 2 should be expanded into a bigger pipeline, which shows the different skills in the skill repository. It is hard to tell which part is the contribution of the paper, so emphasizing which modules are added by the paper will help highlight that.\n- Adding a small introduction of the LLaVA paper would help the readers not go back and forth between the two papers to figure out the differences. The authors could consider adding that to a Preliminaries section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Reviewer_xZda",
                    "ICLR.cc/2024/Conference/Submission5663/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698281268599,
        "cdate": 1698281268599,
        "tmdate": 1700798590694,
        "mdate": 1700798590694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x78OvUxbnh",
        "forum": "IB1HqbA2Pn",
        "replyto": "IB1HqbA2Pn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_aMGS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_aMGS"
        ],
        "content": {
            "summary": {
                "value": "The focus of paper is on a general-purpose multimodal assistant, LLaVA-Plus, which is based on a range of large vision and vision-language pertained models. LLaVA-Plus is trained on multimodal instruction-following data. The data can be used for visual understanding, visual generation, external knowledge retrieval and compositions thereof. The approach is based on a combination of tool chaining with LLMs and end-to-end training with LLMs. LLaVA-Plus uses images during human-AI interaction to improve LLM\u2019s planning and reasoning ability. LLaVA-Plus is based on an the LLaVA model, but doesn\u2019t only involve performing user-oriented dialogues, and also performs skill-oriented dialogues. For this purpose, agent calls different tools like various vision and large language models to execute a task. The way LLaVa-Plus works follows: users provide a a task description with a designated image. Given the provided inputs, the agent chooses the tool and writes the appropriate prompt as the tool argument. The assistant outputs the answer to humans. The authors present the results of their approach on multiple benchmarks including VisiT-Bench with different tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I like the approach using the open-weight LLaVA model, which provides a lot more ground for actual meaningful experimentation on the model itself than closed proprietary models like GPT-4.  The authors propose a fairly straightforward augmentation to the LLaVA model that on the surface appears to provide an ability to expand its vocabulary of skills by determining which external models to call on for a given task, using visual-linguistic supervision in an AutoGPT-like approach.  The paper is also written pretty clearly with only a few typos or disfluencies that impede understanding."
            },
            "weaknesses": {
                "value": "Much of the meaningful technical information, such as substantive results, are limited to the appendix.  This makes me question the soundness and what the contribution of the paper actually is. We don't even get to the related work until page 6 and so the experiment and results are squeezed at the very end.\n\nThe word \"planning\" is used but the paper doesn't really have anything to do with planning as commonly understood.  There is no goal to be trained to achieve. The order of which API should be called first and what goes next shouldn\u2019t be confused with planning approaches.  In addition, the use of \"tools\" is misleading. This leads the reader (at least it read me) to think this was about robotic planning and object-affordance exploitation, but instead this is more about using the appropriate API call to different systems in an AutoGPT-style approach.  It is not clear to me how this approach leads to a general multimodal agent.  This was submitted to the \"representation learning for computer vision, audio, language, and other modalities\" but there's nothing about representation learning or conceptual understanding here.  All the model does is call upon other unaltered submodels to perform vision-language tasks.\n\nThe approach seems not to be fully generalizable as there appears to be little error checking and validation of the outputs of the individual sub-models invoked, and therefore errors could propagate downstream and compound.  The authors mention in the conclusion that LLaVA-Plus still suffers from hallucination and tool conflict.  At the very least the tool conflict problem would need to be addressed to drive home the contribution, because otherwise such a plug-and-play architecture is unlikely to generate useful outputs more frequently than a manual chaining together of the individual sub-models.\n\nSome other points:\n\n\"develop general-purpose assistants for natural language tasks have been proved effective\" - \u201cProved effective\u201d is an overstatement.  There are still plenty of reasoning domains (e.g., logic, physical reasoning problems) where ChatGPT and related models at the very least are unreliable.  This point does not negate the central thrust of this paper, but it should be softened.\n\n\"the combinations of these tools lead to emergent abilities that show signs of higher intelligence\" - Are the authors simply mentioning the Society of Mind approach here, or do they claim that LLaVA-Plus also displays signs of higher intelligence?  Extraordinary claims required extraordinary evidence.\n\nA Full Dialogue of LLaVA-Plus (Figure 2) - This doesn\u2019t seem like a full dialogue session.  It\u2019s a diagram.  I would expect to see an actual example at each step along with a representation of the relevant information flow throughout the process."
            },
            "questions": {
                "value": "1) In Figure 3 top right, how would one confirm there is a sea in that image? It looks more like the image generated is of a bike resting against a blue wall.  In other words the prompt didn\u2019t give you what you seemed to want.\n\n2) How did you approach your problem rewriting the questions using GPT4 and making sure output is what you want? How would you resolve the conflict that might be across the tools?\n\n3) What is the objective function?  At the very high level, an auto-regressive objective has been mentioned. Assuming the reader is familiar with this concept, please include the details, as there is an assistant and feedback loops in your approach. \n\n4) Does expanding to new skills degrade LLaVA-Plus's abilities in previous skills?\n\n5) None of the external models leveraged here is perfect.  Can LLaVA-Plus do any kind of error correction or validation to make sure errors from sub-models don\u2019t propagate downstream?\n\n6) \"We follow the self-instruct to curate the data by using GPT-4 as the labeler.\" - I do not understand this sentence (ungrammatical)\n\n7) How much data curation is required for each skill?  This seems like an intensive process and a bottleneck in scaling up.\n\n8) \"For Xq, we use GPT-4 to write a set of instructions that require the use of tools for proper answers.\" - It\u2019s a shame the authors apparently have to turn to the proprietary GPT-4 for this. Was something like plain LLaVA not sufficient?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Reviewer_aMGS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673569159,
        "cdate": 1698673569159,
        "tmdate": 1699636590201,
        "mdate": 1699636590201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2JPmV45eI3",
        "forum": "IB1HqbA2Pn",
        "replyto": "IB1HqbA2Pn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_3S2M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_3S2M"
        ],
        "content": {
            "summary": {
                "value": "The authors key contribution is  instruction-tuning for large multimodal models (LMM) to use diverse tools. Per the authors -- compared with tool-augmented LLMs, LLaVA-Plus is distinct in that the image query is directly grounded in and actively engaged throughout the entire human-AI interaction sessions. They will also release the dataset that they curated for the aforementioned instruction tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Extensive evaluation: authors evaluate their multimodal tool-based reasoning approach with a large number of tools on existing as well as their own benchmark and compare it with other SOTA LMM approaches. I think their exhaustive evaluation would be useful for the community moving forward.\n2. I appreciate the authors' commitment to reproducibility and open-sourcing.\n3. The paper is overall well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Limited novelty: Despite extensive eval, the work's novelty is limited especially from the methodology standpoint since neither instruction tuning nor the use of multimodal tools for reasoning are novel methods.\n2. Opensource instruction tuning dataset is one the contributions of this work. However, since the dataset itself was not human evaluated and GPT generated, it is unclear how much hallucination does it contain.\n* would be good to add a limitation section in the paper and discuss this.\n* would also be useful to provide dataset stats (# of instructions, # tools, # instructions/tool etc.) for completeness and perhaps human evaluation for a subset?\n3. It is unclear whether addition of a new tool would require doing the instruction tuning from scratch or can the model  be fine-tuned for just those new instructions? Also unclear how the assistant deals with contradicting grounding information from two tools. For instance, if detection and segmentation tools disagree about the presence of a particular object, what does the assistant do?  \n\nThese points prevent me from championing the paper."
            },
            "questions": {
                "value": "- Table 5: What does \u201call tools + GPT4\u201d mean? Did the authors mean GPT4Tools?\n- Table 4,5: Would have been great to see performances of atleast some non-tool based but multimodal models\u2019 performances on Llava-bench just to see how they compare with tool-based approaches on this benchmark.\n- Is it also possible to provide comparisons with GPT4-v (using visual inputs) for all benchmarks?\n- Table 7: The authors should add model sizes for all models if possible.\n- Instruction tuning training details are missing from the paper and the appendix.\n- Would be great to see some failure cases in the paper/appendix to understand the limits of the agent's reasoning capabilities."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5663/Reviewer_3S2M"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796891524,
        "cdate": 1698796891524,
        "tmdate": 1699636590088,
        "mdate": 1699636590088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZO4Qmj9zIC",
        "forum": "IB1HqbA2Pn",
        "replyto": "IB1HqbA2Pn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_FDvd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5663/Reviewer_FDvd"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on the development of LLaVA-Plus, an approach that enhances the capabilities and flexibility of LMMs. LLaVA-Plus enables LMMs to leverage a wide range of skills from a skill repository, allowing them to perform various visual tasks. The approach facilitates skill-oriented dialogues, where the LMM initiates requests to call appropriate tools from the skill repository and aggregates the tool execution results. This approach expands the capabilities of LMMs and improves their engagement in human-AI interactions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The significant contributions mostly lie in the data perspective, while the training algorithm and the model architecture are basically following the previous work. The authors create a new multimodal instruction-following tool using data, integrating lots of real-world tools (skills), like detectors, OCR, image generators, et al. The created dataset is useful to train multimodal language models to possess the ability to use pre-selected tools and perform better on downstream tasks."
            },
            "weaknesses": {
                "value": "1.  Lack of novelty. The paper feels more like an industry paper, which has heavy data engineering work to improve performance, instead of a research paper that has novel insights and approaches compared to previous work. Basically, all the design choices in this paper can be anticipated and do not provide too many insights.\n\n2. Lack of flexibility. If I understand correctly, once new tools/skills are added, the models must be retrained on the augmented dataset to master this new tool. Is this correct? If this is the case, the overall approach can only teach the model to learn to use a fixed set of tools, instead of \"learning to use tools\" and \"becoming a good agent that can effectively invoke external tools provided in context\". \n\n3. The tool selection doesn't convince me. I do not understand why a vision-language model, which can directly perceive the visual world, needs to be augmented with some \"visual tools\" and \"vision-language tools\", like OCR, Captioning model. \n\n4.  Lack of ablation study and in-depth analysis. This paper doesn't show concrete evidence for the benefits of tool use. Only the absolute performance is reported. I would suggest reporting the tool usage rate and also conducting an ablation study to investigate what is the benefit coming from, the tool using ability, or just adding more tool data during the instruction fine-tuning stage."
            },
            "questions": {
                "value": "Please see the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808092483,
        "cdate": 1698808092483,
        "tmdate": 1699636589995,
        "mdate": 1699636589995,
        "license": "CC BY 4.0",
        "version": 2
    }
]