[
    {
        "id": "h4GN7UKjBj",
        "forum": "MO5PiKHELW",
        "replyto": "MO5PiKHELW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
        ],
        "content": {
            "summary": {
                "value": "This paper monitors masked language models\u2019 development of syntactic attention structure (SAS) in the attention pattern, grammar capability (measured by the BLiMP dataset) and the GLUE score. They find that\n\n- The model\u2019s grammar capability spikes right after the spike of the model\u2019s SAS score.\n- They measure the complexity of the model throughout the pretraining process, and claim that the trend is aligned with the Information Bottleneck Theorem.\n\nThey also use a \u201cregularization loss\u201d to interfere with the acquisition of SAS. They find that enhancing/suppressing SAS improves/harms the grammar capabilities of the model. \n\nThough the model is able to develop an \u201calternative strategy\u201d for acquiring the grammar capabilities when SAS is suppressed, they find that lifting the suppression of SAS during the \u201cphase transition\u201d leads to worse grammar capabilities, while lifting the suppression before the \u201cphase transition\u201d can result in better grammar capabilities. They discuss this phenomenon as related to the simplicity bias of models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. They empirically show the close relationship between the spike of SAS score, grammar capability, and GLUE score. This is interesting because it may validate the role of understanding syntactic structure for downstream tasks.\n2. This work provides many experimental results, which could be useful for better understanding of the model pretraining dynamics."
            },
            "weaknesses": {
                "value": "# Main concerns:\n## 1. The causal relationship between SAS and BLiMP/GLUE scores\n\nI think even though the spike of the BLiMP/GLUE score follows closely after the spike of the SAS score, it is not well substantiated to say that SAS is necessary for the capabilities required for BLiMP and GLUE. The intervention experiment in Sec 4.2, 4.3 can not support the causal relationship between them either. If there is a latent factor X that causes the better SAS, BLiMP, GLUE scores, adding the regularization term may suppress that latent factor X in addition to the SAS score. In this case, suppressing SAS also leads to worse BLiMP and GLUE scores. So that SAS is necessary for BLiMP and GLUE is not the only explanation for the observation in Sec 4.3, 4.3.\n\n\n## 2. The arguments about the simplicity bias is not clear (to me)\n\nIt seems that this paper suggests that SAS indicates that the model suffers from some simplicity bias issue, which is counterintuitive to me. In general I think people use simplicity bias to explain some robustness issues because some spurious (unreliable/non-causal) features are simpler to learn than causal features, or say the model is doing some shortcut learning. However, it is hard to imagine that the syntactic structure is something the model shouldn\u2019t rely on to solve any NLP problem.\n\nI think to talk about simplicity bias, the author should be more clear about the definition of \u201csimplicity\u201d and provide more evidence that the model\u2019s prediction is really \u201cbiased\u201d by that specific \u201csimple\u201d feature.\n\n\n## 3. The motivation of the study\n\nIt\u2019s unclear to me why we should look at the development of these \u201ccapabilities\u201d. I would like to know how the findings in this paper can potentially direct future research directions?\n\nIn general I feel that it\u2019s cool that this paper uses some fancy techniques to show many findings and defines some interesting terminologies. However, it\u2019s unclear to me what the high-level message of this paper is. Unable to capture the coherent theme of this paper, I found it difficult to put all the information in this paper together.\n\n# Minor issues:\n\n1. This paper should be more specific about the definition of \u201ccapabilities\u201d."
            },
            "questions": {
                "value": "## Q1: About section 4.1.1\n\nThe authors discuss their findings along with the information bottleneck (IB) theory. \n\n1. It\u2019s unclear to me how the findings agree with what part of the IB theory.\n2. It\u2019s also unclear to me how this is related to the findings or the arguments in this paper.\n\n## Q2:  The specific meaning of phase transition in Sec 4.2\n\nIn Sec 4.2, the term \u201cphase transition\u201d. Could you clarify what it refers to? Does it refer to the period between the structure onset and the capabilities onset.\n\n\n## Q3:  The importance of understanding phase transition in general\n\nI understand that *phase transition* is a *hot topic* for some model interpretability community. However, in this work, could you provide more context in which studying *phase transition* is important?\n\n## Suggestions\n\nI understand that every paper needs a reasonable scope to work on and I don\u2019t expect that one single paper explains everything. However, I would suggest that the authors scope this paper more explicitly. \u201cEmergence\u201d, \u201cphase transition\u201d and \u201ccapability\u201d, for example, I think are some very general terms, and this paper focuses only on some specific aspects of them. Scoping more clearly and explicitly in the introduction section will help readers (at least me) understand this paper more easily, especially when this paper is discussing MLM models while these terms are usually co-occur with autoregressive language models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3867/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3867/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697928575767,
        "cdate": 1697928575767,
        "tmdate": 1700626862770,
        "mdate": 1700626862770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "27MPVnFF79",
        "forum": "MO5PiKHELW",
        "replyto": "MO5PiKHELW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
        ],
        "content": {
            "summary": {
                "value": "The research highlights that understanding model behavior requires observing the training process trajectory, not just analyzing a fully trained model. This study looks into syntax acquisition in masked language models, focusing on the Syntactic Attention Structure (SAS). It shows that SAS emerges suddenly during a specific pretraining phase, marked by a significant loss reduction. Further experiments manipulating SAS confirm its essential role in developing linguistic capabilities, whereas the experiments also find that briefly suppressing SAS improves model quality. The authors explain that SAS competes with other effective traits."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. \n- The research idea and findings (including the appendices) are both intriguing and worthy of being shared with the community.\n- The experiments were conducted and executed effectively."
            },
            "weaknesses": {
                "value": "- I didn't find any major weaknesses, just a few minor questions (detailed below).\n- Some individuals might express concerns that the experimental setup is somewhat minimal and may suggest the inclusion of additional elements, such as utilizing RoBERTa or evaluating the model on other, possibly more recent, benchmarks."
            },
            "questions": {
                "value": "- I would like to know what kind of method/technique is used for encoding positional information. Is it the same as the original positional encoding in Vaswani et al. 2017 (https://arxiv.org/abs/1706.03762) or something more recent variants such as Rotary Positional Encoding (https://arxiv.org/abs/2104.09864v4)? I'm asking this because the syntactic dependency relates to positional information in the sentence. I wonder how much it affects (or does not affect) the experimental setup in this paper.\n\n- This is a more open-ended question but I wonder whether we can observe similar breakthrough (steep drop in loss) in auto-regressive (causal / decoder-only) LMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768733279,
        "cdate": 1698768733279,
        "tmdate": 1699636345027,
        "mdate": 1699636345027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QEvAsnvxBM",
        "forum": "MO5PiKHELW",
        "replyto": "MO5PiKHELW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes sudden transitions in the training loss of BERT models, identifying two components to this drop: the development of attention patterns correlated with syntax (SAS), and the subsequent emergence of the ability to make grammaticality judgements. The paper then manipulates SAS via a additional term in the loss and analyzes the effect of these manipulations on the second component. The findings are twofold: (i) acquisition of SAS is a pre-requisite for the grammatical capabilities and (ii) briefly supressing SAS leads to a subsequent increase in grammatical capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is one of relatively few papers which analyzes learning dynamics in BERT and the transitions identified are intriguing. The paper has the potential of stimulating more work in the same vein, applied to models more advanced than BERT."
            },
            "weaknesses": {
                "value": "I did not find any major weaknesses in the paper. There is a lot going on, but this is understandable given the novelty of the approach. \n\nThat said, some of the framing and terminology could be explicated a bit more carefully. I found the issue of simplicity and simplicity bias especially muddled (see questions below)."
            },
            "questions": {
                "value": "Do I understood correctly you equate the syntax-like attention patterns with simplicity bias, and at some point call them \"simple heuristics\" (section 5.1)?\nThis is a bit confusing as in the NLP literature terminology like \"simple heuristics\" refers to undesirable reliance on surface lexical patterns (like bigrams), and reliance on syntax is considered the opposite of a simple heuristic. It would be good to make sure your unusual framing is not a cause of confusion to the readers.\n\nMinor doubt: since you use WSJ data for testing, why use silver Stanford parser dependencies instead of gold, converted from the manually created trees?\n\nDo you have any inkling of what your mystery alternative strategy may involve?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830577398,
        "cdate": 1698830577398,
        "tmdate": 1699636344959,
        "mdate": 1699636344959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sWqYlQ1CP1",
        "forum": "MO5PiKHELW",
        "replyto": "MO5PiKHELW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_qPGn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3867/Reviewer_qPGn"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a detailed _developmental_ account of (masked) language models' acquisition of grammatical abilities: the authors measure the the time at which (i) attention heads which pay attention to syntactic structure (dependencies, following prior work showing they emerge) and (ii) grammatical abilities (performance on BLiMP, a linguistic diagnostic set) emerge during training.  In particular, they find that (i) occurs reliably just prior to (ii), though both are abrupt, and that (i) occurs with a _sudden drop in the MLM loss_.  This suggests that the model reliably acquires a certain bit of important latent knowledge (of dependency structure) before behavioral evidence of a skill that uses that knowledge (e.g. grammaticality judgments).  The paper also explores regularization to promote or demote (i), with many, many interesting results about when the sudden drop in loss occurs and how it connects to downstream performance.  This kind of causal intervention shows that MLMs _do_ in fact use syntactic attention heads both when doing masked language modeling and grammaticality judgments, teaching us much more about these phenomena than existing probing methods based on static model artifacts."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Very detailed analysis of the emergence of certain knowledge and skills _across training time_ in a language model.\n- Demonstrates that drops in loss correspond to acquisition of syntactic knowledge, which then translates to grammatical performance.\n- Methodologically and technically innovative (e.g. regularization as a causal intervention) in a way that moves the state of the probing field forward.\n- Extremely wide range of experiments, helping isolate exactly which features of training and measurement matter for this phenomena.  Crucially, they show that these emergence phenomena are not measurement artifacts, since they persist when a discrete scale (training time) is replaced with several continuous ones."
            },
            "weaknesses": {
                "value": "- All of the results are on a single model architecture (BERT base).  On the one hand, this makes sense, since an extremely wide range of experiments are carried out.  On the other hand, we don't know whether the connection between sudden drops in the loss and syntactic knowledge would apply at larger scales, with causal language modeling, etc.\n- There are so many experiments and interesting observations that the main paper makes very frequent reference to a plethora of appendices for more detail.  This makes it a bit hard in places to figure out _exactly what_ is being reported and what it all means.  (E.g. the discussion of the Information Bottleneck was fairly hard to follow, even to someone who knows a bit about that literature.)"
            },
            "questions": {
                "value": "- Fig 1b: why do you think there's so much more variance in the BLiMP results than in the loss curves and UAS scores?\n\n- I'm curious about whether it matters that silver dependencies were used in regularization.  Did you try any other \"data-free\" regularizers to see if they impact SAS similarly?  E.g. since each token has one head, a regularizer that promotes sparsity of attention should implicitly promote SAS as well and vice versa.\n\n- Missing references: (i) Liu et al 2021, \"Probing Across Time\": https://aclanthology.org/2021.findings-emnlp.71/ .  (ii) p 4: \"Causal methods...\" I have an idea of what works the authors have in mind, but think they should be explicitly cited here.\n\n- Will code and data be made publicly available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698965616444,
        "cdate": 1698965616444,
        "tmdate": 1699636344871,
        "mdate": 1699636344871,
        "license": "CC BY 4.0",
        "version": 2
    }
]