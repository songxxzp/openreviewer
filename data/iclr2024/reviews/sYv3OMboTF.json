[
    {
        "id": "xFQ8AgZM8r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
        ],
        "forum": "sYv3OMboTF",
        "replyto": "sYv3OMboTF",
        "content": {
            "summary": {
                "value": "**The paper studies** machine learning problems on small (<1K objects) tabular datasets (e.g. classification, regression, etc.) with additional experiments on larger (up to 500K objects) datasets.\n\n**The main contribution of the paper** is ExcelFormer -- a deep learning scheme (Transformer-like architecture + custom training recipe) with the following new elements compared to the vanilla Transformer:\n- (architecture) custom attention\n- (architecture) custom feed-forward block\n- (architecture) custom feature embeddings\n- (architecture) custom prediction head\n- (training) custom initialization\n- (training) two custom augmentations\n\n**The main claim:** *\"EXCELFORMER consistently and substantially outperforms previous works\"*"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The story is mostly easy to follow (also, I like the main illustration!).\n- The research direction (designing better tabular deep learning architectures and augmentations) is important.\n- The experimental part includes many datasets.\n- I like the idea of using feature importances (1) to guide the attention between features and (2) to guide one of the two proposed data augmentations."
            },
            "weaknesses": {
                "value": "(1) (major) **Many *orthogonal* changes (listed in the summary above) are proposed *at once*.** It makes it difficuilt to attribute the observed results to any single element, which I believe to be important in the research context, especially for this genre of papers. I believe that the elements should be introduced either in isolation or step-by-step, but not at once (unfortunately, ablating each of the elements using the *final* architecture does not addresses the issue). Also, in my opinion, each of the elements should be compared against existing alternatives (i.e. the proposed augmentations VS existing augmentations, the proposed embeddings VS the existing embeddings, etc.).\n\nOverall, modifying the well-established Transformer architecture in six(!) different aspects (listed in the summary), most of which has dedicated research subfields looks like an extremely ambitious goal to me. And I respect that, however, it makes it extremely hard to properly introduce and analyse each of the elements.\n\n(2) (major) In my opinion, **the storyline around rotation invariance should be extended with specific analysis/experiments/results. Purely intuitive guidance may not be enough to drive the design decisions.** There are multiple places where the *formal* term \"rotation invariance\" is used in *informal* ways. For example, the paper uses terms like \"more/nearly non-rotationally invariant\". Overall, there is nothing wrong with relying on intuition, but after a certain threshold, there is a risk of coming to wrong conclusions.\n\nA potential solution is to design a dedicated experiment that will quantify rotation invariance of any ML model. Then, some of the proposed elements can be motivated as a way to reduce the invariance according to the designed experiment. Again, this should be done *when introducing the elements*, not with the final architecture (as in Figure 5).\n\n(3) Unfortunately, in my opinion, **the novelty is limited.** Some of the proposed modifications (listed in the summary above) are technically new, however, from the same technical perspective, they remain similar to the existing alternatives.\n\n(4) In my opinion, sharing code, starting from the review stage, is important for this kind of studies. I wish I had an opportunity to have a look at the code to review the experimental setup and implementation details.\n\n(5) Instead of Paragraph 2 of Section 1, I recommend writing only ~2-3 high level sentences and then referring to Section 5.4 of \"Why do tree-based models still outperform deep learning on tabular data?\" by Grinsztajn et al.\n\n(6) I recommend proof-reading the paper for English style, vocabulary and grammar issues."
            },
            "questions": {
                "value": "How exactly is the mutual information computed for the continuous features and for regression labels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697402500962,
        "cdate": 1697402500962,
        "tmdate": 1699636473327,
        "mdate": 1699636473327,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lzYSuDS0u2",
        "forum": "sYv3OMboTF",
        "replyto": "sYv3OMboTF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel transformer architecture with two main suggestions:\n- SEA: A semi-permeable attention block that masks the similarity scores from less informative features to the more informative ones, effectively blocking the transfer of information/representation from the less informative one.\n- Interaction Attenuated Initialization: A rescaling of the variance of the weight initialization that in turn reduces the impact of SEA, which restricts feature interaction in the initial stages, making the proposed transformer architecture more non-rotationally invariant.\n\nThe authors then additionally propose two augmentation methods:\nHidden-Mix and Feat-Mix. One works by augmenting the data on the embedding space, while the other one works on the feature space by using the feature importance.\n\nThe authors combine the different components and one of the suggested augmentation methods at a time to yield an architecture that surpasses the baselines in 96 small-scale datasets and 21 large-scale datasets. The method outperforms the baselines without hyperparameter tuning and with hyperparameter tuning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper has a good structure.\n- The authors propose quite a few interesting additions. The additions are ablated individually and the authors additionally show that the algorithm is more non-rotational invariant compared to the other transformer baseline.\n- Experiments are extensive, a large number of datasets is considered and all the major baselines are included."
            },
            "weaknesses": {
                "value": "- The paper can be written better, typos exist here and there throughout the manuscript. (I will list a few of them in the questions section)\n- The work should be self-contained and the \"mutual information\" should be described.\n- In table 1, an interesting investigation would be how ExcelFormer would behave without any data augmentation (compared to the rest, not the ablation that is given) or how FTT would perform with the proposed augmentation approaches. I am additionally surprised that CatBoost performs worse compared to XGBoost consistently.\n- No multi-class classification problems in the 96 datasets for the small-scale tabular datasets and only 4 datasets in the 21 large-scale datasets. 4 datasets in 117 datasets is an underrepresentation. \n- Regarding the evaluation metrics, why would the authors use AUC for binary classification and ACC for multi-class classification? The latter would not be a good metric for imbalanced datasets.\n- An ablation is given when mixup is used as data augmentation, however, I would also prefer to see cutmix usage as an ablation.\n- Without code release, I find it difficult to trust the results, as unfortunately there exist a plethora of recent DL architectures that claim state-of-the-art performance (TabNet, Node, Saint, etc) [1][2][3] only to be debunked later on [4]. It is necessary to validate the proper setup of the baseline algorithms and to verify the results of the method.\n\n[1] Arik, Sercan \u00d6., and Tomas Pfister. \"Tabnet: Attentive interpretable tabular learning.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 8. 2021.\n\n[2] Popov, Sergei, Stanislav Morozov, and Artem Babenko. \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data.\" International Conference on Learning Representations. 2019.\n\n[3] Somepalli, Gowthami, et al. \"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training.\" NeurIPS 2022 First Table Representation Workshop. 2022.\n\n[4] Shwartz-Ziv, Ravid, and Amitai Armon. \"Tabular data: Deep learning is not all you need.\" Information Fusion 81 (2022): 84-90."
            },
            "questions": {
                "value": "- This presents a significant challenge and bottleneck in the broader adoption of neural networks on tasks involve tables. -> that involve tables*\n- DNNs\u2019 leanring procedure -> Learning *\n- Section 2.1, the mask is defined as M, however, Equation 2 and the follow-up text continue with W\n- Section 5.1, TabFPN -> TapPFN \n- Section 5.2, indicating that applies hyperparameter finetuning onto EXCELFORMER can yield -> applying*\n\n- Can you present the results where no augmentation is performed for ExcelFormer to analyze how it would perform against the other baselines? Can you provide the results where the proposed augmentation is applied to FTT?\n- Can you include more multi-class classification problems in the used benchmarks and provide results?\n- It would be interesting to see an ablation of the proposed augmentation methods against maybe cutmix or cutout to observe the overall improvement.\n- **Is the EXCELFORMER more non-rotationally invariant and more noise insensitive?** In my perspective, an interesting addition would be to include the plain architecture of ExcelFormer in the investigation, then with every suggestion included one at a time (SEA, IAI), then both. This would show how the architecture gets more non-rotationally invariant as the different components are added compared to the beginning. Comparing against FTT is interesting, but it does not separate the impact of the overall differences in the architecture vs (SEA, IAI).\n- I would urge the authors to provide the code to reproduce the results.\n\nI am open to increasing my score if my concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698055538665,
        "cdate": 1698055538665,
        "tmdate": 1699636473139,
        "mdate": 1699636473139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1sw1XpFimU",
        "forum": "sYv3OMboTF",
        "replyto": "sYv3OMboTF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a few modifications to the transformer\n  architecture for small tabular data problems. The modifications are\n  motivated by (1) the lack of rotational invariance in GBDTs and by (2) the\n  efficacy of data augmentations in mainstream DL domains.\n\n  To address (1), the authors propose:\n  - semi-permeable attention (regular self-attention is masked such that more important features do not interact with less important features in self-attention)\n  - interaction-attenuated initialization (initializing weights in semi-permeable attention with small values)\n\n  To address (2) authors propose two variations of mixup tailored for tabular data problems:\n  - Feat-Mix: swapping a random subset of features in two samples and mixing the labels taking feature's MI with the target into account\n  - Hid-Mix: mixing channels after feature embedding and mixing labels proportionally, as in mixup\n\n  With those changes, the proposed ExcelFormer outperforms deep-learning\n  baselines (both traditional and more recent transformer-based models)\n  and GBDTs in terms of average rank on 96 small (< 10000 samples) tabular problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly and nicely written (both in overall structure and details in the technical details regarding proposed methods).\n- It builds upon previous observations in its domain (tabular data) and proposes interesting \"domain-specific\" solutions to previously stated challenges/points for potential growth. For example, a large portion of the paper concerns with rotational invariance or the lack thereof as an inductive bias and.\n- It obtains decent empirical results by integrating said solutions to the transformer architecture."
            },
            "weaknesses": {
                "value": "My concerns boil down to two things:\n\n(1) **Using reduced rank as the main and only metric of model performance**. There are multiple problems I see with this approach to reporting the results, which keep me from agreeing with the ExcelFormer performance claims:\n   - On this particular set of datasets DL models already perform on-par with GBDTs in terms of average rank (see FT-Transformer avg. rank), thus win over GBDT\n   - The degree of improvement (in terms of the task metrics) is not quantifiable from the average rank. Did the ExcelFormer improved upon vanilla FT-Transformer by 10%, 50% in terms of AUC, neg. RMSE, ACC? The magnitude of the improvement is also important.\n\nSee also `[1]` regarding issues with comparing average ranks of multiple algorithms across multiple datasets.\n\nI see that you provide all the results (albeit without standard deviations) for all models from Table 1, but this full table from the appendix is on the other side of the spectrum \u2013 too large to make generalizable conclusions. A more \"zoomed in\" view on performance would be very helpful. For example, you could provide metrics for DL baselines, GBDTs and ExcelFormer variants on datasets which were initially \"won\" by GBDT, but the changes introduced in ExcelFormer turned this around (I assume here that ExcelFormer is in essence a Transformer with potentially important domain-specific tweaks, comparison with MLP and FT-Transformer should be enough for a conclusion).\n\n(2) **Limited ablations and comparisons to baselines**. The paper proposes a few architectural tweaks for a base transformer model: SPA instead of MHSA, IAI initialization in attention, new FFN block, new nonlinearity. With SPA and IAI highlighted as the more important ones. But the section with the ablation is rather short and lacking details regarding the setup, reporting only average rank performance. Could you provide a more detailed ablation and comparison to the vanilla transformer. For example:\n- Transformer (no SPA, IAI, fancy embeddings and GLUs in FFN)\n- Transformer + SPA\n- Transformer + IAI\n- Transformer + SPA + IAI\n- ExcelFormer\n\nA subset of datasets with metrics instead of ranks would be enough (see point 1).\n\nFor a second contribution - novel data augmentations, I believe they could be compared with baselines from pertaining on tabular data `[2,3,4]`, where resampling from marginal distributions for a set of columns was shown to be a decent augmentation. The results for the simplest possible setup (like MLP with all features linearly embedded \u2013 MLP-LR from `[5]`) with different augmentation strategies:\n- Resample Augmentation\n- Feat-Mix\n- Hid-Mix\n- Feat-Mix + Hid-Mix\n\nwould greatly improve the understanding of the efficacy of the proposed augmentations for tabular data.\n\nIn SPA and Feat-Mix, ExcelFormer uses mutual information. Could you discuss how different ways of estimating mutual information compare? It seems like a significant detail, but there are no mentions of this in the ablations or the experimental setup.\n\n**References**:\n- `[1]` Benavoli, Alessio, Giorgio Corani, and Francesca Mangili. \"Should we really use post-hoc tests based on mean-ranks?.\" The Journal of Machine Learning Research 17.1 (2016): 152-161.\n- `[2]` Bahri, Dara, et al. \"Scarf: Self-supervised contrastive learning using random feature corruption.\" arXiv preprint arXiv:2106.15147 (2021).\n- `[3]` Yoon, Jinsung, et al. \"Vime: Extending the success of self-and semi-supervised learning to tabular domain.\" Advances in Neural Information Processing Systems 33 (2020): 11033-11043.\n- `[4]` Rubachev, Ivan, et al. \"Revisiting pretraining objectives for tabular deep learning.\" arXiv preprint arXiv:2207.03208 (2022).\n- `[5]` Gorishniy, Yury, Ivan Rubachev, and Artem Babenko. \"On embeddings for numerical features in tabular deep learning.\" Advances in Neural Information Processing Systems 35 (2022): 24991-25004."
            },
            "questions": {
                "value": "Technical details I'd like to clarify:\n  - Could you provide details on how you compute mutual information, used in proposed augmentation and the attention module?\n  - Could you provide more info on how ablations were run? You compare ablated variants to the fully tuned baseline, are the ablated variations also tuned?\n  - How long were the models trained for? Was early stopping used during training? How the number of steps compare across deep models?\n  - How ranks were calculated?\n\nOther remarks:\n- In the figure 3 hid-mix is called hidden-mix (only in the figure and nowhere else)\n- The table with various datasets aggregations looks redundant in its current form. Not much interesting there besides TabPFN comparison. Not sure why grouping by classification vs regression and the number of continuous/categorical features should in differentiate general purpose methods. The results on the aggregated benchmark tell basically the same story: ExcelFormer is better than the baseline in terms of average rank. This space could be used to expand and address weaknesses (more ablations, more metrics).\n\nOverall, I like the paper, and find the proposed architectural tweaks very interesting and important for the field.\n\nI'm open to raise the score if my two concerns are addressed:\n\n1. Results on multiple **challenging for DL datasets** where ExcelFormer significantly outperforms the DL competitors are demonstrated (not in ranks, but in raw metrics improved)\n2. Comparisons for augmentations and ablations for SPA and IAI are presented (preferably on the datasets from point 1).\n\nLooking forward to the discussion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782367945,
        "cdate": 1698782367945,
        "tmdate": 1699636472983,
        "mdate": 1699636472983,
        "license": "CC BY 4.0",
        "version": 2
    }
]