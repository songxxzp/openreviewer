[
    {
        "id": "eIUa5Pn1Ay",
        "forum": "CVldG5ohCy",
        "replyto": "CVldG5ohCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes AdamQLR, which is a modification of the Adam optimizer and tries to adapt some heuristics used in the K-FAC optimizer for Adam, such as Levenberg Marquardt damping (equation (2) in the paper) and learning rate selection (equation 3 in the paper), both based on a truncated second order Taylor expansion of the function computed by the neural network at the current parameters $\\theta_t$ (equation 1 in the paper). The authors perform experiments on 6 tasks (Rosenbrock, UCI Energy/Protein, Fashion-MNIST, SVHN and CIFAR-10) and they compare two versions of their work (Adam QLR Tuned/Untuned) against a few popular optimizers in the literature (SGD Minimal/Full, Adam, K-FAC)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Below I enumerate the strengths of the paper:\n1. clearly written and easy to understand\n2. code is provided\n3. optimal hyper-parameters are clearly stated in Table 2\n4. ablation study for Levenberg-Marquardt heuristic"
            },
            "weaknesses": {
                "value": "The paper lacks novelty and originality because it only combines Adam and K-FAC in a facile way and thus doesn't provide better results for most of the tasks mentioned, such as UCI Energi/Protein, Fashion-MNIST and CIFAR-10 (in these cases, K-FAC and/or original Adam are better than the proposed method because the generalization performance VS runtime is not competitive as stated in the abstract).\n\nThe evaluation was performed on small tasks and I believe that the usage of Rosenbrock function not adding any value to the paper since the tasks that involve Neural Networks are much more complicated. The paper does not have any tables that contains accuracies for classification tasks and it is extremely difficult to figure out what the final accuracies are only by looking at the plots. In the end, it is unfortunate to say that the paper does not meet the novelty and originality requirements for ICLR."
            },
            "questions": {
                "value": "1. how does AdamQLR behave on NLP tasks?\n2. how do you compute the curvature matrix C that is used to update the learning rate $\\alpha$ and the damping factor $\\lambda$? In the manuscript you state that the overhead is only one additional forward pass, while we all know that computing Hessian-vector-products requires an additional backward pass (which, of course, implies a forward pass in the first place)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5299/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698338522441,
        "cdate": 1698338522441,
        "tmdate": 1699636530367,
        "mdate": 1699636530367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pII48hCvqc",
        "forum": "CVldG5ohCy",
        "replyto": "CVldG5ohCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_MsHa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_MsHa"
        ],
        "content": {
            "summary": {
                "value": "This work refers to Adam and proposes to adaptively adjust the learning rate. Specifically, the authors utilize $\\rho$ to denote the ratio between the difference of true loss function $f()$ and the difference of second-order estimation $M()$. Then the authors refine the estimated Hessian matrix through $\\lambda$ according to $\\rho$. Finally, the learning rate is then computed by minimizing $M(\\theta - \\alpha d)$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe method makes sense.\n\n2.\tExtensive experiments show the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1.\tI wonder how to get the matrix $C$ in Eq. 1.\n\n2.\tWhat is the principle of setting $\\omega_{dec}$ and $\\omega_{inc}$, and why $\\lambda$ is adjusted when $\\rho$ larger than 3/4 or smaller than 1/4?"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836783920,
        "cdate": 1698836783920,
        "tmdate": 1699636530270,
        "mdate": 1699636530270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0rRrqfLZEX",
        "forum": "CVldG5ohCy",
        "replyto": "CVldG5ohCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
        ],
        "content": {
            "summary": {
                "value": "In this paper authors propose some symbiosis of two optimization methods: Adam and K-FAC. They combine damping and learning rate selection techniques from K-FAC and use it inside Adam algorithm. The resulting algorithm, called AdamQLR, is then evaluated on different regression and classification tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Lots of numerical experiments.\n2. Clear description of algorithm modification.\n3. Good description of the motivation of the heuristics, adopted from K-FAC.\n4. Description of the experimental setup and hyperparameter search space."
            },
            "weaknesses": {
                "value": "From theoretical point of view, the result seems insignificant. You took some heuristics, that improve the model, and moved it to another model. There is no evidence, that it should work better in theory. From practical point of view, as far as I understand, the number of hyperparameters increased: $\\beta_1, \\beta_2, \\varepsilon$ for Adam vs $\\beta_1, \\beta_2, \\varepsilon, \\lambda$ for AdamQLR (or even $w_{dec}, w_{inc}$ instead of $\\lambda$."
            },
            "questions": {
                "value": "Rosenbrock function example seems unfair, because you use Hessian there, what do you think?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq",
                    "ICLR.cc/2024/Conference/Submission5299/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838978687,
        "cdate": 1698838978687,
        "tmdate": 1700596767552,
        "mdate": 1700596767552,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "27d3ATJfbC",
        "forum": "CVldG5ohCy",
        "replyto": "CVldG5ohCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_BW7Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5299/Reviewer_BW7Y"
        ],
        "content": {
            "summary": {
                "value": "This paper tried to combine the first-order method (such as Adam) with the second-order methods, such as K-FAC. More specifically, the authors propose a novel optimizer AdamQLR: combining damping and learning rate selection techniques of K-FAC. The experimental results illustrate that the proposed method AdamQLR can achieve competitive generalisation performance and training efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of combining first-order and second-order methods is very interesting. In addition, the research direction is also very important. \n2. The proposed method is very easy to understand. I think we should pay more attention to second-order method and improve its efficiency."
            },
            "weaknesses": {
                "value": "1. I think the main results are from the figure 2. But the figure is not very clear for me, maybe you can list the training loss, test loss, convergence steps, and generalization gap ( |training_loss - test_loss| ) in a table. From this figure. I'm not very clear whether the proposed method can solve the overfitting issue and improve the generalization. So I think you can analyze the generalization gap. \n\n2. The experimental results are not very strong for me. Although the proposed method can achieve fast convergence and lower test loss, their performance is still too close. In addition, you try to analyze training loss and test loss in figure 2. But loss value is not a great metric for classification tasks and I think you should show the accuracy. \n\n3. The training task is too simple and the results on complex tasks (such as ImageNet) is not very strong."
            },
            "questions": {
                "value": "1. Loss value in figure 2 is not great enough to compare the performance of different methods and maybe you should provide the accuracy value."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699364701442,
        "cdate": 1699364701442,
        "tmdate": 1699636530090,
        "mdate": 1699636530090,
        "license": "CC BY 4.0",
        "version": 2
    }
]