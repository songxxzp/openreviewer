[
    {
        "id": "8voptv2u9n",
        "forum": "3ZqKxMHcAg",
        "replyto": "3ZqKxMHcAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
        ],
        "content": {
            "summary": {
                "value": "This paper considers using negotiation games to evaluate the intelligence of LLMs. The authors designed a specific structured negotiation protocols, where the agents need to compose a private mental note as well as a public message to the other party each negotiation round. The authors found that GPT-4 is generally more skillful in these negotiation games."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using negotiation game to assess the intelligence behaviors are promising."
            },
            "weaknesses": {
                "value": "The presentation is not clear. The evaluation studies are limited."
            },
            "questions": {
                "value": "I think the idea of using negotiation games to evaluate LLM is great. However, I have concerns about the approaches and evaluations in this paper. Also some of the concepts are not explained well. Specifically:\n\n1. About the negotiation protocol & the way it uses LLM to compose a strategy. Where do $q_{n/m}, \\beta_i$ and context $c$ come from? Are they fixed or sampled from some distribution during each negotiation instance? Why the negotiation strategy has to be constructed in such way, and how does it compare with other approaches? E.g., just directly input previous negotiation rounds results and output a text message.\n\n2. Can the author further clarify what is distributive v.s. compatible negotiation?\n\n3. For the cross-play results, are the scores in Table 5 and 6 averaged across every possible opponents? From Table 13, 14 it appears there certain strategic structure (such rock-paper-scissors cycle). What will be the mean conclusion from there then?\n\n4. In you opinions, why different LLMs behaviors qualitively different?\n\n5. There have been several previous works that evaluate LLMs using negotiation games [1, 2]. Can the authors compare your work with theirs.\n\n[1] \"Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback\" Fu et. al.\n[2] \"Evaluating LLMs with Interactive Multi-Agent Negotiation Games\", Abdelnabi et. al."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7151/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7151/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7151/Reviewer_ZZpj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698178025527,
        "cdate": 1698178025527,
        "tmdate": 1699636846759,
        "mdate": 1699636846759,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L2JRmBY4vS",
        "forum": "3ZqKxMHcAg",
        "replyto": "3ZqKxMHcAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_pM8H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_pM8H"
        ],
        "content": {
            "summary": {
                "value": "- Joint framework to evaluate performance and alignment of LLMs using structured negotiation tasks.\n\n- Creates a negotiation task benchmark, which involves evaluating the success of LLMs negotiating toward goals in self-play and cross-play.\n\n- Incorporates LMs into the evaluation benchmark so that the benchmark \"co-evolves\" with the models they are designed to test.\n\nOverall, I am currently giving this paper a 3 (reject) before discussions, considering the weaknesses outlined below related to empirical design and the lack of formalism. However, I am giving my rating a confidence of 2 since I am unfamiliar with related work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- timely and relevant subject\n\n- interesting idea on evaluating both alignment and performance, notably given the uncertainty around the orthogonality hypothesis.\n\n- interesting cross-play results to compare LLMs to each other."
            },
            "weaknesses": {
                "value": "# Big weaknesses\n\n- empirical design with insurmountable reproducibility issues and cost issues impacting statistical validity. Unless the experiments were performed all simultaneously, it is not obvious that they are valid since these models undergo continuous improvement, meaning you might've been comparing different models across different experiments, even if API access was the same and there would be no way to know, right? For this same reason, the experiments are not necessarily reproducible. It might be better in the future to use open-source LLMs for which the models can be held frozen by selecting a checkpoint.\n\n- no definition of agency despite it being a central concept to the paper\n\n- the metrics in table 1 seem to require a lot of human checking, which makes it difficult to scale this benchmark. Are you also using LLMs to compute these benchmark values(e.g. internal faithfulness)?\n\n- there is a whole range of issues between opposing and aligned interests, e.g. mixed cooperative-competitive settings or variable-sum games. It would be interesting to establish benchmarks on these types of settings as well.\n\n- having each agent play both sides and starting positions and averaging does not control for bias, since different LLMs might be more or less able to take advantage of these asymmetries depending on the game design. You would probably also need to ensure sufficient diversity in the game contexts to help control for bias (i.e. not just rent negotiation games)\n\n- it is not obvious that allowing multiple turns to take place would provide more information into understanding which persona is active especially if the persona mixture depends on previous context and can evolve through a conversation, nor that this persona activation would be consistent across different runs with different random seeds. However, I am not very familiar with this literature.\n\n===\n\n# Small weaknesses\n\n- bad reference formatting: \"Jacob Andreas. Language models as agent models, 2022.\"\n\n- fix typos (\" the challenge is to figure out agent interests are aligned. command, ...\")\n\n- no reference provided for \"Theory of Mind\"\n\n- a cooperative game in game theory is one in which players can negotiate binding contracts, which can be confusing given that we are discussing games in game theory formalism, though with a different meaning for \"cooperative\".\n\n- not obvious that issues necessarily have linearly weighted preferences. A related subject is scalarization in multi-objective optimization.\n\n- \"the possible effects and opportunities of stories, traits, rules, and prompts have been discussed in the previous subsections\" stories were not discussed \n\n- \"providing too much capacity might lead to hallucinations\" citation needed"
            },
            "questions": {
                "value": "- ToM strategy is not introduced formally\n\n- Why is the utility 0 if there is no agreement on all issues?\n\n- How are the prompts designed to test the LLM's negotiation capacities? For any given game, do you test multiple prompt variations with similar semantic meanings? How do you know the elicited personas will be the same across differences in the input prompts?\n\n- Why is the goal to measure if there is a significant difference in performance between the average, expert and novice initialization? I thought the goal was to evaluate LLMs in general, and it's far from obvious that such an initialization will transfer the same way across different LLMs\n\n- Does the co-evolution of the benchmark in terms of cross-play rely on having sufficient diversity among language models? How do you see the benchmark holding up in the future?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The author discusses the ethical considerations of their work in relation to malicious actors. AI governance is outside my area of expertise, therefore I am flagging for Ethics Review such that an ethics reviewer may review the author's claims in Section 5, \"Ethical Considerations\" paragraph, though I do not believe that this framework constitutes a potentially harmful methodology on its own, it is aimed at evaluating AI capabilities and alignment."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709488434,
        "cdate": 1698709488434,
        "tmdate": 1699636846657,
        "mdate": 1699636846657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x2ExLNMuoQ",
        "forum": "3ZqKxMHcAg",
        "replyto": "3ZqKxMHcAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7151/Reviewer_yVDW"
        ],
        "content": {
            "summary": {
                "value": "The authors are proposing a technique to evaluate large language models using a scenario where they are required to participate in a multi-issue negotiation, for instance a rental agreement. The overall claim of the paper is that investigating such a negotiation might lead to a more accurate evaluation of the performance and alignment of the language model compared to other approaches. The authors had tested a number of currently available language models through their APIs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors are making a good case that the proposed evaluation method is a useful aspect of the behaviors of the large language models.\n* The paper proposes a methodology that carefully considers the variety of biases that can be introduced by the measuring process, and takes credible steps to avoid them. \n* Extensive evaluation over six-seven LLMs, including self-play and cross-play."
            },
            "weaknesses": {
                "value": "* Many of the current language models are not trained to sustain a negotiation type conversation. For instance, they don't have a framework to keep track of the issues agreement had been reached upon, or the current alternatives that are under discussion. Thus, the proposed metric measures an aspect on which the models had not been trained, and indeed their performance on it is more a side effect of some artifacts in the training data."
            },
            "questions": {
                "value": "Clearly, the performance of the LLMs in this task can be improved relatively easily, as the underlying mathematical negotiation problem is much simpler than LLM's language abilities. How would one rank a model that would have minimal language abilities, but use a specialized algorithmic plugin?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790368247,
        "cdate": 1698790368247,
        "tmdate": 1699636846484,
        "mdate": 1699636846484,
        "license": "CC BY 4.0",
        "version": 2
    }
]