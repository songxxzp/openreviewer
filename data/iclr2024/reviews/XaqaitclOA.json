[
    {
        "id": "deq8DfSzos",
        "forum": "XaqaitclOA",
        "replyto": "XaqaitclOA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_aEZi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_aEZi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalization error of PINN for Burgers' equation. The theoretical framework is informative of the empirical evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper innovatively studies the PINN generalization error of the Burgers equation.\nEmpirical evaluation validates the effectiveness of the theoretical framework.\nThe bound does not depend heavily on the neural network.\nThe solution of the Burgers equation is stiff, which hinders PINN from learning this part of the mutation. Therefore, the topic studied in the paper is important."
            },
            "weaknesses": {
                "value": "Although I recognize the theoretical contribution of this paper, the actual PINN experiment deviates from the theory to a certain extent.\nBecause the solution to the Burgers equation is very stiff, many PINN variants have been proposed to solve these problems, such as self-adaptive weight PINN, adaptive sampling, or adversarial training. Their core points are to focus the optimization of PINN on these stiff areas with relatively large losses to fit the stiff area of the Burgers equation well.\nSince the theory of this paper is mainly based on PINN's L2 loss to bound the final generalization error. Therefore, I suspect that the conclusions of this paper cannot fit well with these PINN variants, such as self-adaptive weight PINN, adaptive sampling, or adversarial training, because the loss function they use is no longer L2 loss. In other words, the most popular method to solve Burger is adaptive loss. Can the author's theoretical framework be applicable to these variants?"
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698471093124,
        "cdate": 1698471093124,
        "tmdate": 1699636857263,
        "mdate": 1699636857263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UTSNeluOGK",
        "forum": "XaqaitclOA",
        "replyto": "XaqaitclOA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_ndTQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_ndTQ"
        ],
        "content": {
            "summary": {
                "value": "The paper starts by highlighting a gap: current general rules for using neural networks to solve PDEs don't exist if the PDE has a known explosive solution. This pushes the authors to explore how PINNs tackle the Burgers' PDE, especially when it's close to exploding.\n\nFirst, the authors explain PINNs. These are neural networks trained to follow the rules of a physical system, including its boundary and starting points. They then test how well PINNs handle the challenging parts of Burgers' PDE and compare this to older, standard methods.\n\nAfter that, they work out general rules for errors in the Burgers' PDE. These rules estimate how much the neural network might get wrong on new data. The authors find a link between these rules and the solution the neural network comes up with. They suggest these rules can help shape how the neural network is built.\n\nThe paper then talks about the balance between getting the answer quickly and getting it right in PINNs. The authors suggest a new training method for PINNs that finds a good middle ground. They test this on Burgers' PDE and find it gives good answers much faster than older methods.\n\nTo sum up, this paper adds a lot to the world of using neural networks to solve PDEs. It shows how PINNs can handle tough PDE situations, gives rules for estimating errors, and introduces a faster training method. All these can shape how future neural networks are designed for this job, leading to quicker, more accurate results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Fresh Perspective: The paper delves into how Physics Informed Neural Networks (PINNs) handle particular solutions in PDEs, a topic not widely tackled before. Additionally, the authors outline error estimation rules for the Burgers' PDE when using neural networks, marking a pioneering step in neural network-based PDE solutions.\n\nThoroughness: The study dives deep into PINNs' stability, providing a well-rounded theoretical perspective. The authors craft error rules for the Burgers' PDE rooted in robust mathematical studies, bolstering the case for using PINNs to solve PDEs.\n\nPractical Tests: The team showcases how PINNs can manage the Burgers' PDE, especially when it's on the brink of a complex issue, and stack these results against established methods. They also suggest and test a fresh PINN training technique that strikes a balance between speed and precision. These hands-on results further confirm the potential of PINNs in this domain.\n\nClear Writing: The paper is neatly composed and straightforward. With lucid explanations and detailed accounts of their methods and findings, it caters to a broad audience, even those just venturing into neural network-based PDE solutions."
            },
            "weaknesses": {
                "value": "One limitation of this paper is its narrow focus on addressing the Burgers' PDE near a specific complex scenario. Although this is a significant topic, it might not cover the spectrum of PDEs used in real-world situations. This could limit how much the findings in this paper can be applied to other PDEs.\n\nFurthermore, the study works under the assumption that we always know the main equations driving the physical system. However, in real situations, these equations might be unknown or hard to pinpoint. This could reduce the range of situations where PINNs can be effectively used for solving PDEs.\n\nLastly, the paper could have delved deeper into comparing its method with other leading neural network solutions for PDEs. While there's a comparison with classic numerical methods, a broader analysis including other neural network strategies would give readers a fuller understanding of where this method stands in the landscape of PDE-solving techniques."
            },
            "questions": {
                "value": "What is the trade-off between accuracy and speed of inference in PINNs?\n\nHow do PINNs detect finite-time blow-ups in PDEs?\n\nWhat are the generalization bounds for Burgers' PDE and how are they correlated to the neurally found surrogate solution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709065212,
        "cdate": 1698709065212,
        "tmdate": 1699636857154,
        "mdate": 1699636857154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1dgi7xj8TI",
        "forum": "XaqaitclOA",
        "replyto": "XaqaitclOA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the approximation ability of PINNs for the inviscid Burgers equation is theoretically estimated. This equation is known to have so-called blow-up solutions, which are solutions that diverge to infinity in finite time. In this paper, whether PINNs can find such a solution is investigated theoretically. Specifically, two theorems are presented in this paper; the former theorem gives an error estimate for the multi-dimensional Burgers equation, and the latter theorem gives an improved result for the 1-dimensional equation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is just my impression but theoretical error analysis of numerical methods for computing blow-up solutions is a difficult problem. Even for classical numerical methods, such as the finite difference method and the finite volume method, there are not so many papers on this topic. A strength of this paper is that the authors tackle such a challenging problem, and certain results are in fact given."
            },
            "weaknesses": {
                "value": "I suppose that there are a few weaknesses in this paper.\n1) I believe that inequalities estimating numerical errors should show that the error bound converges to zero in some sense. If I understand the result correctly, the error bound in the first theorem does not converge to zero because $C_1$ and $C_2$ include the terms given by the norm of the solutions.  So, the inequality (5) does not appear to make sense as an error analysis.\n\n2) As for the results of the numerical experiments, although it is interesting that certain correlations between RHS and LHS of the inequalities are observed, the magnitudes of them are very different. So, I am not sure whether these results are meaningful or not.\n\n3) Perhaps this is not a weakness, but honestly, it is difficult for me to assess the value of this paper in the ML community. Although the analysis shown in this paper may be an important first step in this direction, I am not sure whether the results of this paper meet the criteria of a top ML conference. My concern is that, in my impression, papers on error analysis of classical numerical methods (e.g., the finite difference method) for the Burgers equation seem unlikely to be accepted by top journals of numerical analysis because the Burgers equation is the simplest partial differential equation with blow-up solutions."
            },
            "questions": {
                "value": "My biggest concern is the first one of the above weaknesses. Does the error bound (5) converge to zero in certain situations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7212/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7212/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7212/Reviewer_7pxF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715231372,
        "cdate": 1698715231372,
        "tmdate": 1699636857045,
        "mdate": 1699636857045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R8PEUraHYE",
        "forum": "XaqaitclOA",
        "replyto": "XaqaitclOA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_QLQE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7212/Reviewer_QLQE"
        ],
        "content": {
            "summary": {
                "value": "This work derives what the authors call a generalization bound for the PINN-based solution of Burgers' equation near the formation of singularities. They show empirically that their bound, while vacuous, is surprisingly correlated with the error vs the true solution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Introducing more analytical techniques to the study of PINN training is a worthwhile cause. I also appreciate the author's openness to admit the vacuousness of their bound and investigating the empirical correlation of their bound with the right hand side."
            },
            "weaknesses": {
                "value": "Listed in decreasing order of gravity\n\n1. The bound derived by the authors depends on $L^\\infty$ norm of the gradient. As the equation approaches blow-up, this quantity approaches infinity. The bound thus does not provide meaningful information in the vicinity of the blowup, which is undercutting the main claimed contribution. \n\n2. The claim by the authors \n>Most importantly, Theorem 3.2 shows that despite the setting here being of proximity to finite-time\nblow-up, the naturally motivated PINN risk in this case 3\nis \u201c(L2, L2, L2, L2)-stable\u201d4 in the precise sense as defined in Wang et al. (2022a). This stability property being true implies that if the PINN\nrisk of the solution obtained is measured to be O(\u03f5) then it would directly imply that the L2-risk\nwith respect to the true solution (10) is also O(\u03f5). And this would be determinable without having\nto know the true solution at test time.\n\nis misleading. If the exact solution is unknown, neither is the $L^\\infty$ value of its gradient at a given time, preventing the bounding of the error vs the true solution.\n\n3. I do find the expression \"generalization bound\" for Theorem 3.1 somewhat misleading. These type of stability estimates (of the operator mapping right hand side and initial condition to the solution) are standard tools in the theory of partial differential equations, making this seem more like a rebranding. It would strengthen the paper if the authors would discuss related results in the PDE literature.\n\n4. The literature review on operator learning approaches misses the works on both neural operators and BCR-NET (the latter predates both neural operators and DeepONet). \n\n5. The referral to the works on the euler singularity of Wang should make more clear the differences between the two works. To my understanding, the work of Wang et al uses a rescaled coordinate system and therefore does not actually solve a PDE with singular solution. The blow-up studied by this community is also specific to incompressible problems as the blowup of the compressible Euler equation (of which the Burgers equation is the zero sound speed limit) arises from a different phenomenon."
            },
            "questions": {
                "value": "I suggest the authors directly respond to my criticism in the last paragraph. I would gladly reconsider my recommendation if it turns out that I overlooked something."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7212/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796543314,
        "cdate": 1698796543314,
        "tmdate": 1699636856912,
        "mdate": 1699636856912,
        "license": "CC BY 4.0",
        "version": 2
    }
]