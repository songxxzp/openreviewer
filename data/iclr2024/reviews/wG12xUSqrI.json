[
    {
        "id": "XHtFlqyM5X",
        "forum": "wG12xUSqrI",
        "replyto": "wG12xUSqrI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_wRd7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_wRd7"
        ],
        "content": {
            "summary": {
                "value": "An exciting result that studies the generalization of the score-based generative modeling under finite sample & shallow neural networks under assumptions of actual sampling density being in the Baron class of functions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Rigorous theoretical analysis of population and empirical losses of score-based diffusion models with explicit bounds.\n2. The work is well-written and structured.\n3. The work answers an intriguing question if there is a class of functions for which score-based diffusion models break the curse of dimensionality, which by itself is not obvious."
            },
            "weaknesses": {
                "value": "Limited to shallow networks and Baron class (later I don't consider it as something concerning, just some discussion of potential extensions to other classes/other architectures will be appreciated with potential caveats, e.g., in Appendix authors discuss about extending to deep networks -- won't there be any issues with dimension dependency in Lipshitz constants?)."
            },
            "questions": {
                "value": "Assume the function is say, just Lipshitz-smooth. Can we upper bound the Baron constant with a dimensionally dependent constant? \nWhy asking -- in constants in proofs there are terms that have said Baron constant in the exponent. \nSo if for densities that are highly non-convex -- we should expect curse of dimensionality to re-appear?\n\nWhat is connection of densities that are $-||x||^2$+Baron class perturbation to $(\\alpha, \\beta)-$dissipative distributions? (convex outside of region)? It seems that this is sub-class of such, and if so, can authors elaborate what obstacles there is to generalise results on such class? (that are common in say analysing Langevin Dynamics generalization). \n\nBaron class assumption needed only for universal approximation results about shallow network, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788751315,
        "cdate": 1698788751315,
        "tmdate": 1699636384067,
        "mdate": 1699636384067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "COuzBWC2KW",
        "forum": "wG12xUSqrI",
        "replyto": "wG12xUSqrI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_2C1B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_2C1B"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a theoretical setting in which a deep network which minimizes the empirical score matching loss produces approximate samples from a high-dimensional data distribution. The curse of dimensionality is broken by assuming that the data distribution is a small perturbation from the Gaussian distribution. The asymptotic sample complexity exponent is independent of the dimension, with a constant that depends polynomially on the dimension and the Barron norm of the perturbation (but exponentially in its $L^\\infty$ norm)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "While sampling guarantees of diffusion models are now rather well-understood, learning guarantees are much less explored, especially in high dimensions. This paper thus studies a central topic, and is the first (to the best of my knowledge) to obtain results on a truly high-dimensional distribution. The paper is also very clearly written and is enjoyable to read: I commend the authors for managing to give intuition about the several steps of the proof despite its technical difficulty."
            },
            "weaknesses": {
                "value": "The results have a very big limitation, which is the exponential dependence of the error bound on the $L^\\infty$ norm of the perturbation $f$. To avoid the curse of dimensionality, this norm thus needs to be $O(1)$ in the dimension. However, in this setting, diffusion models are unnecessary, as much simpler classical techniques are sufficient. Indeed, the Holley-Stroock perturbation lemma [1] implies that the distribution $p(x)$ satisfies a logarithmic Sobolev inequality with a constant that is $\\Omega(1)$ in the dimension. The score matching loss at time $t=0$ then controls the error in KL divergence [2], and knowing the score at time $t=0$ is sufficient to obtain sampling guarantees with Langevin/MALA algorithms [3,4]. Therefore, it is not necessary to approximate the scores at all times with a neural network. After a quick assessment of the proof technique, it seems hard to remove this exponential dependence in this norm as the variance of the Monte-Carlo approximation of the integrals depends exponentially on the $L^\\infty$ norm of $f$.\n\nI still think the paper should be accepted, because the paper is the first (to the best of my knowledge) to study the sampling complexity of score approximation in high dimensions, and the proof technique might be useful for further research which extends the results in a more interesting setting. But the authors should acknowledge and properly discuss the limitation mentioned above.\n\n[1] Holley, R., and Stroock, D. Logarithmic Sobolev inequalities and stochastic Ising models. J. Stat. Phys. 46, 5\u20136 (1987), 1159\u20131194.\n\n[2] Koehler, Frederic, Alexander Heckett, and Andrej Risteski. \u201cStatistical Efficiency of Score Matching: The View from Isoperimetry.\u201d arXiv, October 3, 2022. http://arxiv.org/abs/2210.00726.\n\n[3] Chewi, Sinho. Log-Concave Sampling. https://chewisinho.github.io/main.pdf.\n\n[4] Altschuler, Jason M., and Sinho Chewi. \u201cFaster High-Accuracy Log-Concave Sampling via Algorithmic Warm Starts.\u201d arXiv, February 20, 2023. http://arxiv.org/abs/2302.10249."
            },
            "questions": {
                "value": "I have some minor remarks and questions:\n- Since $f$ is defined up to a constant, the $L^\\infty$ norm of $f$ can be replaced with the \"oscillation\" of $f$ $(\\sup f - \\inf f)/2$, which makes the invariance of $p(x)$ on addition of constants to $f$ explicit\n- The specification of the function class for neural networks makes no mention of the intermediate widths. In general, I was initially confused whether they were assumed to be one or not, as the previous paragraph talks about real-valued shallow networks.\n- The equation (9) seems related to the Miyasawa/Tweedie relationship for the structure of the score (see e.g. equation (4) in [5]), as is commonly used in denoising score matching [6]. I think it would make the paper clearer to state that the network has to approximate the conditional expectation $\\mathbb E[x_0 | x_t]$, which is essentially the ratio of the functions $F_t$ and $G_t$.\n- In section 4.2, it would help the reader to state that the approximations of the intermediate functions (product, exponential, etc) are in the $L^\\infty$ norm restricted to balls.\n\nTypos:\n- Lemma 4, \"K0\"\n- Lemma 12, extra $|\\ell_{R,S}$\n\n[5] Raphan, Martin, and Eero P. Simoncelli. \u201cLeast Squares Estimation Without Priors or Supervision.\u201d Neural Computation 23, no. 2 (February 2011): 374\u2013420.\n\n[6] Vincent, Pascal. \u201cA Connection Between Score Matching and Denoising Autoencoders.\u201d Neural Computation 23, no. 7 (July 2011): 1661\u201374."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4180/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4180/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4180/Reviewer_2C1B"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791836885,
        "cdate": 1698791836885,
        "tmdate": 1699636383970,
        "mdate": 1699636383970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7QeD5QN23B",
        "forum": "wG12xUSqrI",
        "replyto": "wG12xUSqrI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_r8Sf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_r8Sf"
        ],
        "content": {
            "summary": {
                "value": "Recent progress in theoretically understanding score-based diffusion models has shown that sampling (i.e., training a generative model) often reduces to L^2 (or L^\\infty) score estimation [Lee et al., 2023; Chen et al., 2023; Benton et al. 2023]. In light of these results, the logical next step is to understand when neural networks can efficiently learn score functions.\n\nThis work represents a significant step in this direction. The authors restrict their attention to distributions that can be represented as exponentially tilted Gaussian distributions. Then, they show that if the tilting function, which characterizes the target distribution, is of \u201clow-complexity\u201d, then neural networks can *efficiently* approximate the corresponding score function at a rate that is independent of the data dimension. Building on this approximation-theoretic result for the score function, the paper then establishes a sample complexity upper bound for approximating the target distribution in TV distance. Notably, this upper bound avoids the cursed exponential-in-d rate.\n\nThese results are distinct from other recent results which achieve dimension-independent rates by assuming that the target distribution is supported on a low-dimensional manifold.\n\n**References**\n- Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. *NeurIPS* 2022.\n- Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. *ICLR,* 2023.\n- Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. *arXiv:2308.03686*, 2023."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a very well-executed paper. The question addressed in this work is a natural continuation of the recent research effort devoted to understanding theoretical aspects of score-based diffusion models. The main results are timely and significant, and the authors draw ideas (tilted Gaussians, Barron space, path norms) from diverse research areas, and combine them in an elegant manner.\n\nMoreover, the paper is exceptionally well-written, especially considering the technicality of the topic. Ideas are presented clearly and concisely, and intuition behind the proofs are communicated effectively."
            },
            "weaknesses": {
                "value": "I only have minor editorial comments.\n\n- It would help motivate the setting better if examples of non-trivial target distributions that satisfy the assumptions were provided. What are some well-known distributions which are Gaussians exponentially tilted by a \u201clow-complexity\u201d function from Barron space?\n- The sudden appearance of L=8 in Theorem 2 is unnecessarily surprising and mysterious. It would be helpful if this was explained prior to Theorem 2. For example, one can mention that the score function is a composition of the tilting function, which can be approximated by a shallow network, and basic function/arithmetic primitives like exponentiation and multiplication, which can be implemented by stacking more layers.\n- Typo in p.9: \u201callowed to be grow\u201d\n- I would suggest simply adding a table of contents to the Appendix (there is a latex package for this) instead of Section B."
            },
            "questions": {
                "value": "- Aside from the Barron space assumption, what conditions must the tilting function satisfy to ensure that corresponding function is a probability density? An immediate one seems to be subquadratic growth, but are there other conditions that one needs to or would want to assume?\n- What are some well-known distributions which are Gaussians exponentially tilted by a \u201clow-complexity\u201d function from Barron space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889823169,
        "cdate": 1698889823169,
        "tmdate": 1699636383878,
        "mdate": 1699636383878,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IG8oZpiH8D",
        "forum": "wG12xUSqrI",
        "replyto": "wG12xUSqrI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_bdyJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4180/Reviewer_bdyJ"
        ],
        "content": {
            "summary": {
                "value": "The authors prove a bound on the sampling error of score based modeling under the assumption that the target density has the form $p_0(x) \\\\propto e^{f(x)} \\\\gamma_d(dx)$, where $f$ has $\\\\|f\\\\|_\\\\infty < \\\\infty$ and $\\\\|f\\\\|\\_{\\\\mathcal{B}} < \\\\infty$. The latter norm is the spectral Barron norm, which is known to define a function class $\\\\mathcal{B} = \\\\{f : \\\\mathbb{R}^d \\\\to \\\\mathbb{R}, \\\\|f\\\\|\\_{\\\\mathcal{B}} < \\\\infty\\\\}$ that can be well approximated in sup-norm by shallow neural networks with bounded path norm. Under these assumptions, the authors bound on the total variation $\\\\text{TV}(p_0, \\\\hat{p}_{t_0})$, where $\\\\hat{p}_{t_0}$ is the output of an early-stopped SGM. The sample complexity of this bound is $\\\\text{TV}(p_0, \\\\hat{p}_{t_0}) \\\\lesssim N^{-\\\\alpha}$ where $\\\\alpha > 0$ is independent of the dimension. \n\n It has been shown in previous works that the SGM sampling error is controlled by the approximation error of the learned score model, $\\\\hat{s}(t, X) \\\\approx \\\\nabla_x \\\\log p_t(x)$, where $(p_t)_{t \\\\geq 0}$ is the law of an Ornstein-Uhlenbeck Process with initial condition $p_0$ given by the data distribution. Using integration by parts, the score can be rewritten as $$\n    \\\\nabla_x \\\\log p_t(x)  = \\\\frac{1}{1-e^{-2t}}\\\\left( -x + e^{-t} \\\\mathbb{E}[ e^{-t} x + \\\\sqrt{1-e^{-2t}} U \\\\mid X_t = x] \\\\right)$$\n where the expectation is taken with respect to $\\\\text{Law}(U \\\\mid X_t = x) \\\\propto e^{f(e^{-t}x + \\\\sqrt{1-e^{-2t}}U)} \\\\gamma_d(dU)$. The key insight of the paper is that the conditional expectation in this expression inherits the smoothness properties of $p_0$, and in particular, it can be approximated by a composition of shallow neural networks. In Theorem 3, the authors prove an approximation error bound on the score network by an appropriate class of neural networks. In Theorem 2, using standard techniques from empirical process theory, the authors use this bound to prove the TV bound on SGM sampling error."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clarity: while there are some minor typos, the results in this paper are clear and well-explained. They are correct to the best of my knowledge. \n- Dimension free sample complexity: it is interesting and potentially valuable to prove generalization bounds that depend on instance-specific complexity measures, like the Barron norm of $f$, in place of a dependence on the ambient dimension."
            },
            "weaknesses": {
                "value": "- Exponential dependence on $\\\\|f\\\\|\\_{\\\\infty}$. The assumption that $\\\\|f\\\\|\\_\\\\infty < \\\\infty$ seems extremely strong and the constants in this bound depend exponentially on $\\\\|f\\\\|\\_{\\\\infty}$. As a thought experiment, suppose $p_0(x) = \\\\prod_{i=1}^d p_0^{(i)}(x_i)$ is the tensor product of 1-d distributions $p_0^{(i)}(x_i) \\\\propto e^{g(x_i)} \\\\gamma_1(dx_i)$. Then it is reasonable to expect $f(x) = \\\\sum_{i=1}^d g(x_i)$ can have $\\\\|f\\\\|\\_{\\\\infty} = \\\\Omega(d) \\\\|g\\\\|_{\\\\infty}$ if the support is not restricted to a low-dimensional subspace. Because of this, it's not clear to me that the stated bounds can actually 'escape the curse of dimensionalty' or be 'dimension free' in non-trivial cases. \n- Likely suboptimal bounds. There are many techniques in the literature on empirical process theory that could be used to improve the stated bounds (notably localization methods). I am not an expert on these techniques, but it would at least be worth commenting on ways the bounds in this work could be improved.\n- No lower bound. It would also be helpful to prove (or at least discuss) lower bounds for this estimation problem. For example, do you know a minimax lower bound for estimating a generic function in $\\\\mathcal{F}\\_{\\\\text{score}}(\\\\{K_i\\\\}_{i=1}^L)$? How does it compare to the generalization error of $\\\\hat{s}$? It is difficult make any takeaways from this work given that the upper bound is likely suboptimal and there is no lower bound to compare against. \n\nAlso, here are some minor typos:\n- Statement of Theorem 2: please define $\\hat{p}_t$. The theorem statement includes $\\text{TV}(\\hat{p}\\_0, p\\_{t\\_0})$, but based on the proof (pages 31, 32) it seems like it should be $\\text{TV}(\\hat{p}\\_{t\\_0}, p\\_{0})$. \n- Page 7, \"Proposition 3 guarantees that, by choosing $K$ ...\" should be Theorem 3\n- Page 7, display following \"Girsanov's theorem, combined with the data processing inequality\" shouldn't this be an inequality instead of an equality? Also, there seems to be inconsistent notation regarding $\\hat{p}\\_{t_0}$, $\\hat{p}\\_{t\\_0, T}$, etc -- please make the notation consistent and define it. \n- Page 9, first paragraph, \"as the multiplication of $a_i(t)$ and $b_i(t)$ induces a nonlinearity.\" What is $b_i(t)$? I was not able to find any reference of it. \n- Lemma 2 in the appendix is missing parenthesis in the big $O$ notation in both line items. \n- Lemma 2 proof, line two of the series of inequalities (begins with $\\lesssim$), this line seems to be missing a factor of $\\\\|f\\\\|\\_{\\mathcal{B}}$ which appears in following lines. \n- Lemma 3 statement uses functions $F\\_1, F\\_2$ instead of $F, G$."
            },
            "questions": {
                "value": "What is the minimax rate of estimation for functions from $\\\\mathcal{F}\\_{\\\\text{score}}(\\\\{K_i\\\\}_{i=1}^L)$? How does it compare to estimation rates proven in this paper? What should one takeaway from the rates in this paper, and/or what do they say about the relationship between this problem and vanilla density estimation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699291774045,
        "cdate": 1699291774045,
        "tmdate": 1699636383804,
        "mdate": 1699636383804,
        "license": "CC BY 4.0",
        "version": 2
    }
]