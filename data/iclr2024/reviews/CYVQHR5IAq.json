[
    {
        "id": "kKHZwdmIzD",
        "forum": "CYVQHR5IAq",
        "replyto": "CYVQHR5IAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a personalized federated learning (PFL) framework called DFEedMDC, which pursues robust communication and better model performance with a convergence guarantee. Besides, to promote the shared parameters aggregation process, the authors propose DFedSMDC via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work designs personalized local models and training schemes for decentralized federated learning. The authors present theoretical analyses for the convergence, which shows the negative influence of the statistical heterogeneity and the communication topology. Extensive experiments are conducted to evaluate the effectiveness of the proposed methods."
            },
            "weaknesses": {
                "value": "What do the \"Grid\" and \"Exp\" represent in Fig. 3? It would be easier for the readers to understand different communication topologies by visualizing them in the main test or in the Appendix.\n\nIn light of Theorem 1 and Theorem 2, the communication topology (i.e., the eigenvalue $\\lambda$) has an impact on the DFedMDC and DFedSMDC methods. The reviewer suggests the authors report the $\\lambda$ values of different communication topologies in Fig. 3 and discuss the influence of $\\lambda$ on the test accuracy."
            },
            "questions": {
                "value": "The proposed DFedSMDC method, a variant of DFedMDC, achieves better performance by integrating the SAM optimizer into the local iteration update of shared parameters. The reviewer is curious if the incorporation of this optimizer could similarly enhance the performance of other baseline methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698328674806,
        "cdate": 1698328674806,
        "tmdate": 1699636447153,
        "mdate": 1699636447153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "50MW9erjfl",
        "forum": "CYVQHR5IAq",
        "replyto": "CYVQHR5IAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the personalized federated learning problem under fully decentralized setting. The framework of the considered personalized learning is the commonly used model decoupling with a globally shared model and personalized local models. DFedSMDC, an algorithm via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters, is proposed. Theoretical convergence results and numerical experimental results are both presented."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper studies the personalized federated learning problem under fully decentralized setting, and proposed DFedSMDC, an algorithm via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters."
            },
            "weaknesses": {
                "value": "1. The reviewer is quite doubt about the final results as shown in Theorem 1 and Theorem 2. I\u2019ve checked the theoretical proof in the appendix and do not find the exact expressions for the final convergence results, but only the $\\mathcal{O}$ expression. The first questionable part is that the right-hand side of Eqs. (3)-(4) will goes to 0 as the number of rounds $T$ goes to infinity, while in reality, this is not true for non-i.i.d scenarios. There will exists some constant terms related with heterogeneity that are irrelevant to $T$. Please explain this. \n\n2. The second part that may not be true in the theoretical results is that the convergence speed is monotonically related with the spectral gap $\\lambda$. If this is true, it solves the challenging topology design problem of decentralized federated learning, since a fully-connected topology is the optimal topology according to the theoretical results in this paper. There is no discussion about this point in current manuscript and this leads to a doubtful result.  \n\n3. Why is the convergence results not related with the number of workers? This is also a weird part. \n\n4. Why Theorem 1 is related with the cross Lipschitz constant $L_{vu}$, and Theorem 2 is related with $L_{vu}$? How about $L_{uv}$?\n\n5. The results in Fig.3 are questionable according to the second comment. The reviewer is not sure if a fully-connected topology is the best.\n\n6. What is the meaning of Fig. 4? Are multiple local epochs good or bad? How is it related with the theoretical results?"
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638102150,
        "cdate": 1698638102150,
        "tmdate": 1699636447025,
        "mdate": 1699636447025,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tkgjATbopq",
        "forum": "CYVQHR5IAq",
        "replyto": "CYVQHR5IAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_iEM9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_iEM9"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present an innovative framework known as DFedMDC, which leverages model decoupling to address these issues and aims to provide robust communication and superior model performance while guaranteeing convergence. DFedMDC achieves this by personalizing the \"right\" components within modern deep models through alternate updates of shared and personal parameters, facilitating the training of partially personalized models in a peer-to-peer manner. To enhance the shared parameters aggregation process, the authors introduce DFedSMDC, which incorporates the local Sharpness Aware Minimization (SAM) optimizer to update shared parameters. SAM optimizer introduces proper perturbations in the gradient direction to mitigate inconsistencies in the shared model across clients.\n\nThe paper provides a thorough theoretical foundation, offering a convergence analysis of both algorithms in a general non-convex setting with partial personalization and SAM optimizer for the shared model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and exhibits a high degree of clarity, making it accessible and easy to comprehend.\n2. The paper's strength is further underscored by its meticulous convergence analysis, enhancing its overall robustness.\n3. The paper substantiates its claims with an exhaustive array of experimental results, effectively confirming the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. A significant concern revolves around the novelty of the proposed method. The concept of model decoupling in personalized federated learning [1] and the application of Sharpness Aware Minimization (SAM) [2] to address model inconsistencies in decentralized federated learning have both been extensively explored in the literature. As such, the proposed method may appear to be a fusion of existing ideas (resembling an 'A+B' approach). It is essential for the authors to underscore their distinctive contributions in a more prominent manner.\n\n2. In terms of experimental baselines, it is recommended that the authors include the most recent decentralized federated learning method ([2]) for a comprehensive comparison. This will enhance the paper's completeness and relevance in the context of the current state of the field.\n\n3. Regarding the convergence analysis, it would be valuable to incorporate a discussion that compares the proposed method's convergence rate with the state-of-the-art (SOTA) approaches. \n\n[1] Exploiting Shared Representations for Personalized Federated Learning\n[2] Improving the Model Consistency of Decentralized Federated Learning"
            },
            "questions": {
                "value": "See weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682339837,
        "cdate": 1698682339837,
        "tmdate": 1699636446930,
        "mdate": 1699636446930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4HdIMmNqF2",
        "forum": "CYVQHR5IAq",
        "replyto": "CYVQHR5IAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_A4M3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4664/Reviewer_A4M3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes interesting methods DFedMDC and DFedSMDC for PFL, which simultaneously guarantee robust communication and better model performance with convergence guarantee via adopting decentralized partial model personalization based on model decoupling."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study of personalized FL on decentralized FL is meaningful.\n2. The experiments demonstrate that the proposed method is useful."
            },
            "weaknesses": {
                "value": "1. The proposed algorithm seems trivial and common in PFL. It seems its idea is the adoption of the method in DFL. Can you clarify what is the main novelty of this method?\n2. Why introduce the SAM? It is unclear about the advantage of introducing this optimizer. Can you elaborate on it intuitively and theoretically?\n3. In the theorem, why is it $V^{t+1}$, instead of $V^{t}$, and what does it mean?\n4. The experiment results are a bit weird, in Table 1. Why do all baselines achieve better performance under larger heterogeneity? As I know, larger heterogeneity will usually lead to worse performance [1].\n5. Regarding ``The test performance will get a great margin with the participation of clients decreasing\u2019\u2019: What will happen when the client number is less than 10, even 1? Does it mean no collaboration is the best?\n\n[1]Karimireddy S P, Kale S, Mohri M, et al. Scaffold: Stochastic controlled averaging for federated learning[C]//International conference on machine learning. PMLR, 2020: 5132-5143.\n\nMinors:\n\n1.\tIt seems the hyperparameters of the proposed methods are finetuned (like $rho$ and local epoch for the personal part). Are the baselines\u2019 results well finetuned? What's the used hyperparameter for baselines?\n2.\tWhat is the definition of $\\sigma$ in Theorem 2?"
            },
            "questions": {
                "value": "1. Could you give more explanation on Theorem 2? What is the difference/advantage compared with Theorem 1 as you introduce SAM into shared parameters?\n2. Can you provide baseline results with more hyperparameter settings?\n3. Could the authors provide more details about the experiment settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766544380,
        "cdate": 1698766544380,
        "tmdate": 1699636446849,
        "mdate": 1699636446849,
        "license": "CC BY 4.0",
        "version": 2
    }
]