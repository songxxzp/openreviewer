[
    {
        "id": "p99FC4UnpF",
        "forum": "WGP2pHtLtn",
        "replyto": "WGP2pHtLtn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
        ],
        "content": {
            "summary": {
                "value": "This work analyzes the limitations of contrastive learning with two-views and extend it to multiple-views through the lens of information theory. They provide theory to show their objective is a lower bound of the information bottleneck. Finally, their experiments show improved performance and speed-up."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The problem is underexplored and the novelty of the paper is strong.\n\nThe theoretical analysis is rigorous with clear definitions."
            },
            "weaknesses": {
                "value": "The presentation can be improved.\n\nIn method section,\n1) The assumptions of the proposed method are not clearly stated. \n2) What are the failure cases for the method?\n3) What is an intuitive example of view-invisible bias? Could you please clarify \"a mutually exclusive state of Z owing to the invariant nature of label and view information in optimization\"? What are the \"certain approaches\" that \" assume sharable task information ...\"?  Are there experiments to support eq 10? Adding concrete examples could help to better digest the theorems.\n\nIn experiment section,\n\n4) Could you clarify why \"we adopted unselected data augmentations that slightly deviate from a Sweet Spot\"? Does it affect the improvements over baselines?\n5) Is the linear accuracy of the main experiment, table 2&3, omitted?\n6) Is the method scalable to medium size datasets such as ImageNet?\n \nAblation study is not a key contributions to the SSL.\n\nFor three variables MI is defined as I(x; y, z) = I(y; z) - I(y; z | x). There are typos in eq 12."
            },
            "questions": {
                "value": "This is an interesting paper in multiple aspects, however the presentation can be significantly improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_pKGj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733003718,
        "cdate": 1698733003718,
        "tmdate": 1699635938095,
        "mdate": 1699635938095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x4LvnW46Jc",
        "forum": "WGP2pHtLtn",
        "replyto": "WGP2pHtLtn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission126/Reviewer_K5DW"
        ],
        "content": {
            "summary": {
                "value": "The paper considers using multiple positive views in training the SSL model. The paper derives certain theoretical aspects and advantages of introducing the multi view self-supervised learning method. Empirical evidence demonstrates the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The paper explores the integration of multiple positive views during the training of the SSL model. \n\nS2. It establishes specific theoretical implications and benefits associated with the implementation of the multi-view self-supervised learning approach. \n\nS3. Empirical findings affirm the effectiveness of the proposed methodology."
            },
            "weaknesses": {
                "value": "W1. The paper lacks novelty. There are many works introducing multiple views in the SSL pre-training, including DINO, SwAV  and so on, The paper does not compare with these well known SOTA methods. \n\nW2: It is unclear what is the explicit loss function of the proposed method (although it seems Eq. (11) is the loss), and how it hears advantages over SOTA or how it distinguishes in motivation between the existing multiview method such as SwAV (clustering based method with multi-views).\n\nW3: It is unclear if there are fairness issues during the training (empirical evidence), i.e., does the proposed multi-view contrastive learning simply benefits from more \"effective epochs\" because of its multi-view training (more data in each batch) in comparison to other SOTA methods? \n\nPlease help clarify the above concerns. \n\n[A] Mathilde Caron et al., SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. \n\n[B] Mathilde Caron et al., . Emerging Properties in Self-Supervised Vision Transformers"
            },
            "questions": {
                "value": "Please see the above weakness for the questions to be addressed. Please correct me during rebuttal, if there is any misunderstanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780979203,
        "cdate": 1698780979203,
        "tmdate": 1699635938011,
        "mdate": 1699635938011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XWMPlzIr0p",
        "forum": "WGP2pHtLtn",
        "replyto": "WGP2pHtLtn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a \"plug-and-play\" approach to multi-positive-views learning, seamlessly integrating with existing two-view self-supervised learning (SSL) architectures. The authors challenge traditional assumptions about multiview learning and explore its complexities. The proposed method incorporates multiple positive views to enhance traditional SSL models, improving accuracy and speed across various benchmarks and SSL architectures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper explores the complexities of multi-positive-views learning and provides an alternative way to understand multiview learning.\n- Extensive experiments support the effectiveness of multiview learning.\n- The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "- Although the proposed strategy (Eq. 11) is an alternative way for multiple positive view contrastive learning, its novelty is limited.\n- Extensive experiments are conducted. However, I can hardly find insights different from previous multiple positive view contrastive learning methods.\n- In Table 2, the training epochs for each setting are not clear. If all methods share the same training epoch, the comparison is not fair since 4-view models observe more data than 2-view models."
            },
            "questions": {
                "value": "- Could you please highlight unique insights different from existing multi-positive view methods?\n- In Table 2, do all the methods share the same training epochs? If yes, could you please conduct additional 2-view experiments with double training epochs for fairness?\n- In Figure 7, could you please explain why GPU usage decreases as the number of views increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission126/Reviewer_z9y7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission126/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819005301,
        "cdate": 1698819005301,
        "tmdate": 1699635937944,
        "mdate": 1699635937944,
        "license": "CC BY 4.0",
        "version": 2
    }
]