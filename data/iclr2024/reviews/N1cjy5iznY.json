[
    {
        "id": "EQ3dJ6FizU",
        "forum": "N1cjy5iznY",
        "replyto": "N1cjy5iznY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission282/Reviewer_EgZL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission282/Reviewer_EgZL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces FreqTST, a Transformer-based model for time series forecasting. Unlike prior approaches that tokenize time series in the time domain, FreqTST employs the frequency spectrum as a common dictionary for tokenization. It incorporates two pretraining tasks: time series reconstruction and maximum position prediction, aimed at aligning the frequency and time domains. The experimental results highlight FreqTST's superiority over existing state-of-the-art models. The model demonstrates a certain level of transferability across different datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-structured and easy to follow.\n- The concept of utilizing the frequency spectrum as a tokenization dictionary is novel and intriguing.\n- The transferability across different datasets is a bonus."
            },
            "weaknesses": {
                "value": "- Becauses the frequencies of the DFT basis dependences on sequence length, my main concern is that the tokenization process is highly dependent on the length of the input sequence $L$:\n  1. The size of token dictionaries, $G, H \\in \\mathbb{R}^{L\\times P}$ vary with the input sequence length $L$.\n  2. The correspondence between keys in the dictionary and their frequencies also changes with varying sequence lengths: when embedding a sequence of length 10, $g_5, k_5$ in the fixed dictionary correspond to frequency $\\frac{1}{2}$, i.e. period of $2$. While for sequence of length 20, they correspond to frequency $\\frac{1}{4}$, i.e. period of $4$.\n \n  Therefore, the tokenization highly dependences on the seqeunce length and we need a specific dictionary for each different length, even on the same data set. This limits the generalization of the proposed model. \n\n- The tokenization should be orthogonal to pretraining task. However, it seems that FreqTST highly relies on pretraining to achieve SOTA performance.\n\n- The paper uses a fixed input length of 96 in the main experiments, while the forecasting lengths are substantially larger (e.g., 336, 720). This design choice may not be realistic. Moreover, PatchTST and DLinear can get better forecasting accuracy with longer look-back window.\n\n- The claim of zero-shot transferability might be overstated. In section 4.3.2, the model is trained on Electricity and tested on ETTh2 to show its transferability. However, both these two datasets are hourly-sampled and have a period of 24 hours. I infer that this pre-trained model is unlikely to achieve satisfactory results on weather as it is sampled every 10 minutes. Thus \"These results indicate our model has capable transfer ability due to the unified frequency units and common dictionary among all kinds of time series\" seems to be overclaimed."
            },
            "questions": {
                "value": "1. Section 4.1 mentions experiments conducted on the ETTm2 dataset, but the results are missing. Can you provide the outcomes for this dataset?\n2. Why MSE was chosen instead of cross-entropy in Maximum Position Prediction task, consdering this is a classification task and the target is a one-hot vector?\n3. Please compare FreqTST with DLinear and PatchTST with varying look-back window length $L$.\n4. Please evaluate the computation efficiency (memory occupation and speed) of FreqTST with different look-back window $L$ and number of features $M$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Reviewer_EgZL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698128072690,
        "cdate": 1698128072690,
        "tmdate": 1699635954276,
        "mdate": 1699635954276,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "llx9sNv4lC",
        "forum": "N1cjy5iznY",
        "replyto": "N1cjy5iznY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission282/Reviewer_ggum"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission282/Reviewer_ggum"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a frequency-based time series tokenization approach, diverging from the conventionally employed patching strategy. The proposed methodology is somewhat novel, simple, and effective. I particularly appreciate the insights shared in Sec 3.2 and recognize the potential this method offers for pioneering time series pre-training across domains. In terms of experiments, the presented results offer a fairly comprehensive validation of the method's effectiveness. Nevertheless, certain nuances in the paper's composition could benefit from refinement. Please refer to my comments below for specifics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation behind this research is compelling, setting a strong foundation for future studies on time series pre-training across multiple domains.\n2. The concept of tokenizing time series based on frequency units offers a fresh perspective.\n3. The experiments conducted provide a thorough assessment, effectively showcasing the efficacy of the proposed method."
            },
            "weaknesses": {
                "value": "1. The technical soundness of Sec 3 should be further enhanced. See my listed comments & questions below.\n2. The presentation needs to be further improved, and some claims fail to be justified with evidence or in-depth discussion."
            },
            "questions": {
                "value": "I have the following detailed questions and comments:\n\n- The claim in the third paragraph of the introduction that \"... transformer architectures cannot be directly applied to model time series data due to the lack of a clear tokenization scheme\" seems unsubstantiated. Recent studies, such as [1], have showcased the feasibility of such an application.\n- In Fig 1, the demarcation between pre-training and downstream adaptation is not clearly delineated. This aspect remains vague in Sec 3, and one has to refer to the discourse in Sec 4.2 for clarity. For instance, the statement \"the difference between fine-tuning and supervised learning is that we load ...\" in Sec 3.5 appears ambiguous. Furthermore, the utilization of L_pre and L in Sec 3 isn't elaborated upon.\n- The rationale for introducing G and H in Sec 3.2.2 is unclear. The link between positional encoding (specifically, Eq. 4 and 5) and the formulation of G and H requires more substantial justification.\n- Eq. 6 presents a transformation of f_k. Based on my interpretation, the core concept hinges on retaining a set of shared bases, namely G and H. Nevertheless, a deeper analysis or discourse is essential for grasping the underlying principles. For instance, elucidation on the relationship between t_k and f_k and the justification for selecting Eq.6 as an optimal solution would be enlightening.\n- Conducting evaluations on a broader set of datasets, for instance, the remaining three ETT datasets and M4, would enhance the robustness of this work.\n\n[1] Gruver, N., Finzi, M., Qiu, S., & Wilson, A. G. (2023). Large Language Models Are Zero-Shot Time Series Forecasters. arXiv preprint arXiv:2310.07820."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698312161674,
        "cdate": 1698312161674,
        "tmdate": 1699635954181,
        "mdate": 1699635954181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qEPbfWkzvr",
        "forum": "N1cjy5iznY",
        "replyto": "N1cjy5iznY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission282/Reviewer_PGhs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission282/Reviewer_PGhs"
        ],
        "content": {
            "summary": {
                "value": "This work tokenizes continuous valued time-series data using the discrete nature of the temporal sampling. Here tokens correspond to the frequency basis vector of the Fourier transform. This differs from some previous works which have a continous embedding as input into a transformer. This work produced results that are comparable and in some metrics better than the previous state of the art on a subset of temporal benchmarks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors addressed a primary difficulty in applying transformers to continuous-valued time series: transformer models were designed to predict discrete tokens and time series data values are continuous. Bridging this discontinuity has been the subject of many works. The approach here is clever in that discretization naturally arises from the discrete Fourier transform. By leveraging this discretization in time, the authors were able to produce discrete tokens while maintaining a continuous representation of the time series values. \n\nAlthough this is not the only work that uses the Fourier domain, this Fourier representation naturally captures temporal dynamics and long-time behaviors. I believe this representation is more information-rich than the temporal domain. The Fourier domain allows the network to more easily remove noise and less relevant dynamics by masking a single (or a neighborhood) of coefficients."
            },
            "weaknesses": {
                "value": "There are a few general topics in which this paper needs significant improvements. Below I describe them starting from the least and moving to the most severe.\n\n# Language\nThe writing of the paper is overall quite poor with respect to the use of English and Latex, the clarity, and the way information is and is not provided. If the authors are not native English speakers I recommend asking a native English speaker to provide more in-depth feedback. I have also found that software like grammerly is very helpful.\n\n## Writing\n1. The author uses words that do not exist in English (cosinuid should be cosine), and employs similar-sounding words that have completely different meanings (discretion is to speak in a way to not annoy or cause offense).\n2. This paper's grammar needs improvements, below are two such examples.\n    * we discrete the time series into frequency units \n    * It represents we use L sinusoid\n3. The biggest issue with language is that this work is extremely vague which makes the arguments very weak and unconvincing. This vagueness at times leaves me to wonder how well the authors understand what they are saying. This is unfortunate because the authors may be experts with intimate knowledge of this subject, but their language does not always portray this. By detailing their ideas with more precise language the authors can write a significantly more compelling study that will also demonstrate the authors' expertise in the field. Below are a few of many examples. \n    * Transformer-based models achieve better performances in long-term time series forecasting than previous models, *so more and more transformer-based models are designed for better performance*\n        * What about transformer models achieve better performance and why do people want to use them? Saying that \"more and more\" models are being made adds no information and sounds too colloquial. Either list some recent advancements or remove this portion.\n    * Another research trend is the utilization of the frequency domain in forecasting, as time series in the time domain can be transformed into the frequency domain. An intuitive way is to use the frequency domain to extract more features.\n        * \"time series in the time domain\" is redundant, time series are in the time domain. A decomposition into the Fourier or any other domain has a different name. What do you mean by \"extract more features\"? Information is either conserved or lost in Fourier transformations, this process cannot create more information or features. Instead, the Fourier transform has the same number of degrees of freedom by representing the data differently. Consequently, this transformation can elucidate potential patterns or information content of interest. Your vague statement leaves me wondering what you are interested in and if you think that the Fourier transform is creating new information.\n4. What are these data augmentation methods mentioned in section 4.3.3?\n\n## Latex\n1. Only mathematical variables are supposed to be italicized, not function names. Also, please fix parentheses so they are not so small when the arguments are fractions.\n    * In Eq. 2 $sin(2\\pi \\frac{kn}{L})$ should instead be $\\sin \\left(\\frac{2\\pi kn}{L}\\right)$\n    * The subscript in FreqTST$_{pretraining}$ should not be italicized, it is a descriptor not a variable.\n2. If you want to say a number is in the set of real numbers do not just use and $R$, use the correct notation ($\\mathbb{R}$). You can do this using the latex mathbb function.\n3. Do you need to define MSE and MAE in equation 8? I think people know what that is.\n4. Please check if Eqs. 4 and 5 are correct, should $F$ be $H$ or $G$? If not, what is $F$?\n\n## Figures\n1. Figure 3 is hard to understand. To me, all of these values look the same. Only a reader with very good eyes and scale judgment can quantify how different these values are. Please put this information in a table.\n2. The captions for all figures and tables are very poor or effectively non-existent. The caption needs to describe in detail what is happening so a reader and look at the figure or table in isolation and understand what it is. For figures with multiple panels, the panels must be labeled and described within the caption.\n\n# Overembelishment and unsupported claims\nThe vast majority of the claims this work makes and statements about other works are overembellished and are unsupported. Some of which are so egregious that the authors are contradicting themselves. When describing results do not state your own opinion or judgment on the results' quality, state exactly what you observe so that others can make their own judgment of the quality and success. Below are some of the most egregious examples, but this behavior is pervasive.\n\n* \"Firstly, our FreqTST outperforms all other supervised baseline models, including PatchTST and other frequency-based models, which verifies the effectiveness of our model\"\n    * This is one of the primary statements of the paper and comes right after introducing the primary results in Table 2. This statement is completely false! Worse, the evidence of its error is right above it. Between the MSE and the MAE exactly half of the time the pretrained PatchTST outperforms your model. If the PatchTST pretraining is unsupervised this must be made very clear to the reader.\n\n* Experimental results are demonstrated in Figure 4. Both FreqTST and FreqTST$_{pretraining}$ show more promising performances on the zero-shot setting than PatchTST, which even outperform some supervised models (AutoFormer and FEDFormer).\n    * Again, in some cases your method outperforms and in other cases it does not. You need to say this specifically! It would be helpful if you said, in words, when it outperformed and by how much and when your model underperformed and by how much. It is better to state your results clearly and it will be apparent to the reader if your model is superior or not. Do not write about your own opinions and judgments.\n\n* \"Through comprehensive experiments conducted on multiple datasets, we demonstrate superior performance ...\"\n    * I would not consider calculating the MSE and MAE on a subset of the datasets in Nie et. al. comprehensive. Why didn't you apply this method to the illness dataset and others? If you had an appendix that showed me more information about your experiment I may consider it comprehensive, but supplying only the MSE and MAE is not comprehensive.\n    * Do not claim your model is superior unless it is obvious. Your model is regularly not the best performing model in Tables 2 and 3.\n\n* \"The architecture makes full use of the modeling capacity of transformers for discrete and semantic token sequence, and outperforms other supervised models by a large margin\"\n    * Again, this is not true, in some cases your model outperforms others. When your model does outperform I would not consider this by a large margin. Do not state your own judgments and misrepresent your results.\n    * I do not think your model uses the full modeling capacity of transformers (see next section). Please describe why you think this. In general, such a statement is very vague and does not provide insight or understanding of what you are doing.\n\n* In the abstract you say \"Transformer-based models have achieved great success in modeling sequence data, especially text, but fail to understand time series sequences. The reason is that individual data points of time series are hard to utilize because they are numerical values that cannot be tokenized.\" \n    * The premise of this paper is that you are going to tokenize these numerical values, but you state that numerical values cannot be tokenized. This contradicts the primary premise of the paper.\n   * You state that transformers fail to understand time series sequences. I understand this as saying that the current state of the art fails to understand time series. However, this work claims that the method has an \"excellent ability\" to perform such tasks and talks about its precision. This might be believable if you substantially outperformed the previous state of the art, but in general you did not outperform it. Either transformers can predict time series or your method also falls short of predicting time series like the other you claim \"fail\" to do so. The statements in the abstract are not consistent with those later in this work.\n\n* You often use the term *precision* but that is meaningless unless you set a scale. For example, a precision of 1 meter is great if I'm trying to locate a building, but terrible if I am trying to measure the length of a molecular bond. You are saying that this method is in general precise but that is not the case for all problems.\n\n\n# Methods\n## Missing benchmark datasets\nThe PatchTST (Nie et al., 2023) work shows (Table 3) the MSE and MAE on eight datasets for all the models you mentioned. You need to include the results on these datasets as well for a more comprehensive comparison. \n\n## Fourier transform basis cannot generalize\nThis work states that the same Fourier dictionary tokenization method can be applied to any other time series. In fact, the zero-shot experiment aims to do just that. This is not entirely true as the number of Fourier coefficients is dependent on the length of the input sequence. If we train a model with $N$ input time points, then it will only expect $N$ unique Fourier coefficients. If one applies this method to a problem where the input sequence is not $N$, the Fourier bases will no longer be orthogonal. This is a drastic change from how the model was originally trained. Applying this model to a problem that has less than $N$ input points means that some of the Fourier coefficients are undefined. When the input sequence is greater than $N$ the Fourier components will not capture the fastest oscillating dynamics within the data and therefore filter out information that might be crucial for the prediction process.\n\n## Misuse of transformer attention mechanism\nAttention is meant to highlight similarities between inputs or representations when the ordering or representation of these sequences can change. For example, an attention mechanism will highlight different token positions or have different values for \"The ball is blue\", \"The sphere is blue\", and \"The blue ball rolls\" if asked what color is the ball. Here, Eqs. 4 and 5 determine the embeddings up to a constant so the dot product never changes between their unscaled representations. That is, dot products between $G$ and $H$ never change since they are predetermined. Moreover, the frequency that these bases correspond to will set their order in the frequency series. Therefore, one does not need attention to give us any spacial information since the sequence order in the Fourier domain is set. One only needs to multiply the $a$ and $b$ coefficients together and learn a coefficient to get the values fed into the softmax for attention. Therefore, using attention is a waste of computing time by always recalculating dot products that do not change, instead of multiplying together the Fourier coefficients and learning a weighting coefficient. \n\n## No handling of discrete Fourier transform systematics\nThe Fourier transform assumes that the dynamics between times [0,T) repeat immediately after. That is, in the range [T, 2T) there is the same signal. This means that the derivatives and values at times 0 and T need to be the same. If not, this introduces systematic errors into the Fourier domain that manifest across the entire spectrum but are most notable at the higher frequencies. This systematic is determined by the difference between the first and last value and their derivatives. Therefore, it changes with each sample and can be confused as a signal. I do not see any handling or mention of this error."
            },
            "questions": {
                "value": "In the weaknesses section I outlined a number of issues along with examples and, in some cases, ways to make changes. Please address all of them for me to reconsider this work. I have some additional questions below that need to be addressed.\n\n1. Why did the authors choose to represent the Fourier embeddings with Eqs. 4 and 5? This is unclear from the paper. The equations here represent the spatial embedding of a transformer that is added across time, but here they are being used to define the structure of each embedding.\n    * Is $F$ supposed to be $G$ and is the second $G$ supposed to be $H$?\n2. What is the purpose of $W_\\theta$ in Eq. 6? There is nothing special about the imaginary axis. For a Fourier transform the real components represent the even contributions (cosine) and the imaginary components represent the odd (sine) contributions. I do not understand what the complex domain information is that is mentioned.\n3. Please evaluate your model on all the datasets in Nie et. al., 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission282/Reviewer_PGhs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708575021,
        "cdate": 1698708575021,
        "tmdate": 1699635954090,
        "mdate": 1699635954090,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2pZ3ijObqP",
        "forum": "N1cjy5iznY",
        "replyto": "N1cjy5iznY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission282/Reviewer_je2G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission282/Reviewer_je2G"
        ],
        "content": {
            "summary": {
                "value": "This paper propose to use frequency domain information as tokenizer for time series data, and utilize transformer for further modeling.\nThe authors also introduce two pretraining tasks based on the proposed model structure."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper proposes to use frequency spectrum for tokenizing time series sequences.\n2. This paper designs FreqTST to furthur modeling tokenized time series data and design two pretraining tasks to improve the generalization ability of FreqTST\n3. This paper demonstrate that FreqTST achieves promising results on long-term forecasting and zero-short transfer tasks."
            },
            "weaknesses": {
                "value": "1. The overall novelty of the proposed method is limited. Frequency domain has been widely used in deep learning methods for time series analysis. Although the authors claim they are the first to tokenize continuous time series with frequency domain, there is no fundamental difference between this tokenization method between previous work which utilize FFT to conduct feature extraction.\n2. No comparison was made with the latest methods that utilize frequency domain information or employ Transformer structures, e.g., TimesNet and CrossFormer\n3. The setting of zero-shot transfer experiment is quite limited. The author only gives results on a set of pre-training and transferred data sets. And the performance of FreqTST for zero-shot learning can be over-claimed.\n4. The author doesn't seem to have conducted multiple repeat experiments. The standard deviation of the results is not given in the text, which reduces the persuasiveness of the results."
            },
            "questions": {
                "value": "1. Why not use cross entropy loss for MPP task, which is more appropriate as it is a classification task.\n2. You mentioned FreqTST have lower computation costs than PatchTST, however theoretically, FreqTST applies self-attention on a longer sequence (L / 2) than PatchTST does (L / P). Why PatchTST can not be applied on some of the long-term forecasting tasks while FreqTST could?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775057194,
        "cdate": 1698775057194,
        "tmdate": 1699635954007,
        "mdate": 1699635954007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MM7xdE6ehi",
        "forum": "N1cjy5iznY",
        "replyto": "N1cjy5iznY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission282/Reviewer_qa3G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission282/Reviewer_qa3G"
        ],
        "content": {
            "summary": {
                "value": "This paper studies time series forecasting using transformers that processes the time series in the frequency domain. For this a vocabulary of frequency tokens are created to represent time series in terms of frequency tokens. The frequency tokens are used as input to the transformers model. For training the transformer model two pertaining tasks used: i) Time series reconstruction from the frequency domain representation, and ii) maximum position prediction task. The model is evaluated on supervised training setup and transfer learning set up. For the transfer learning setup, the final model is fine-tuned on a large scale time series dataset and applied on other time series predictions. The proposed model achieves comparable results with PatchTST."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Creating vocabulary of frequencies interesting way of presenting the frequency domain vectorisation. \n* The proposed model obtains comparable results with PatchTST\n* The pre-taining tasks sound reasonable"
            },
            "weaknesses": {
                "value": "* The overall paper is well-written.\n* The motivation/need of a discrete tokenised representation is not clear for time series forecasting. Especially the formulation of pre-training not in the form of MLM indicates that it doesn\u2019t necessarily builds on contextualised formulation.\n* There are some issues as over-useage of notation in the methodology section: In Equation 2 i is used for imaginary part, but in Equation 4 and 5 i is used for index of the position.\n* Some experimental details are missing, e.g. It is mentioned beta value is altered between different values.\n* Some work on frequency domain forecasting is not mentioned in the related work\n     * Chen et al. (2023) FrAug: Frequency Domain Augmentation for Time Series Forecasting\n     * Sun et al. (2022) FreDo: Frequency Domain-based Long-Term Time Series Forecasting"
            },
            "questions": {
                "value": "* It is not clear why the data augmentation is only applied for frequency domain models. Could you explain what is rationale behind this? \n* Did you apply data augmentation for the other models? \n* Did you try pre-training with MLM like loss, how did it behave?\n* How the beta value alters, is there a scheduling used for it? \n* Is multi-head attention used? What is head size? What is the embedding size and hidden size for attention layers? Which activation function is used? Were there any drop out? Which optimization is used? What is the optimization hyperparameters? How the learning rate? * How the baseline models trained? \n* How does the model perform on other common time series datasets like traffic dataset that is also used in PatchTST paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699616936335,
        "cdate": 1699616936335,
        "tmdate": 1699635953910,
        "mdate": 1699635953910,
        "license": "CC BY 4.0",
        "version": 2
    }
]