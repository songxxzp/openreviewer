[
    {
        "id": "tI2VD32CIO",
        "forum": "v3K5TVP8kZ",
        "replyto": "v3K5TVP8kZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_hZYD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_hZYD"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on TikZ, a language for vector graphics, and provides a dataset, DaTikZ, consisting of 120,000 pairs of TikZ and their captions. The authors are also evaluating the application of LLaMA and CLIP on the dataset. Experimental results show that models trained on DaTikZ can generate more complex vector graphics compared to those synthesized by closed-source LLMs such as GPT-4 and Claude 2."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is the largest scientific vector image and caption dataset to the best of the reviewer's knowledge.\n- Benchmarking with multiple baseline models is reported.\n- While automatic evaluation of generative models often encounters difficulties regarding their quality, this paper uses multiple scores for automatic evaluation to make the comparison meaningful. Multiple human subjective evaluations have also been considered and performed, as described in Sections 5.2 and D."
            },
            "weaknesses": {
                "value": "- As shown in Table 1, DaTikZ consists of multiple data sources. If DaTikZ consists of multiple data sources, the authors should evaluate which data sources contribute to the accuracy of the generation on the test data and to what extent. This would not only justify the use of each data source, but may also suggest what further data should be collected in the future. The contribution of data augmentation should also be evaluated.\n- Since this paper is also about a novel dataset for scientific vector graphics and their benchmarking, the technical contribution of the baseline method is modest."
            },
            "questions": {
                "value": "- The reviewer expects the authors to respond to the points listed in Weaknesses.\n- Figure 2 shows two radar charts. Although the main purpose of the radar charts is to make relative comparisons within the same chart, it would be possible to make some comparisons among the method in the left and right charts if the ranges of each score were adjusted.\n- When the authors conduct a human annotation campaign, they could also evaluate how well the automatic evaluation metrics used in section 5.1 correlate with subjective evaluation; are there any plans to do so?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Reviewer_hZYD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7987/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682862583,
        "cdate": 1698682862583,
        "tmdate": 1700719287376,
        "mdate": 1700719287376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XoQzeskdRu",
        "forum": "v3K5TVP8kZ",
        "replyto": "v3K5TVP8kZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_stBp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_stBp"
        ],
        "content": {
            "summary": {
                "value": "The paper presents AutomaTikZ, a project aimed at automatically generating TikZ drawings from natural language descriptions. The authors introduce a novel dataset, DaTikZ, which consists of aligned TikZ drawings and captions, and a new model architecture named CLiMA that integrates multimodal CLIP embeddings into LLaMA. The results show that CLiMA outperforms both LLaMA and other proprietary models like GPT-4 and Claude 2 in various metrics. CLiMA's capability to process images opens potential applications in vectorization and sketch conditioning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- The paper introduces a pioneering dataset and a novel model architecture, which is a significant contribution to the field of automatic drawing generation from textual descriptions.\n- The proposed model, CLiMA, demonstrates superior performance over existing models, including well-known ones like GPT-4 and Claude 2, across several evaluation metrics."
            },
            "weaknesses": {
                "value": "- Since the main contribution of the study is introducing DatikZ, it would be better to add a couple of simple captions to the code/image examples in the main draft"
            },
            "questions": {
                "value": "- Can you please provide a couple of examples of different models' behavior to typographic attacks?\n- Have others considered comparing with ChatGPT? Maybe fine-tuned on a very small subset of the dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7987/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796316025,
        "cdate": 1698796316025,
        "tmdate": 1699636983910,
        "mdate": 1699636983910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wXDaEm95WG",
        "forum": "v3K5TVP8kZ",
        "replyto": "v3K5TVP8kZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_L9z3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_L9z3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach to generating scientific images, by generating their TiKZ code, and open-sources the training dataset for this task. The paper investigates two solutions, namely LLaMa and LLaMa combined with CLIP-based image adapter. Both models are LoRA-tuned. The comparison with promping GPT-4 and Claude-2 is performed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: While this is not the first paper to do vector graphics generation with language models, the specific focus on TikZ is novel, and the published dataset is first of its' kind.\n\nQuality & Clarity: The paper is of high quality, with good related work section, clear metrics,and detailed analysis of the performance, including a human study. It provides the code accompanying the submission, and suggests the availability of the models (link removed for anonymity). The writing is mostly clear, doesn't have typos, the dataset composition is clearly outlined, as are many of the choices made by the authors (such as the choice of LLaMa vs LLaMa-2, the specific prompts used for the models, etc.).\n\nSignficance: The main point of signficance is the release of the dataset, fostering future work in this direction, and I believe it will be useful for the community."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the limited ablation study: The authors compare and contrast the two versions of the model, LLaMa and CLiMA (LLaMa + CLIP adapter) but don't highlight other important choices made, ex. \n* The effect of the aritifical samples. 50% of the proposed dataset is the artificial samples, but the evaluation is done on real samples. The effect of augmentation with artificial samples is not measured.\n* The effect of re-sampling the generation. The authors re-generate the output multiple times until the result is compilable, and the average number of re-generations is > 1.5, but the performance without regeneration is not reported.\n* The effect of data augmentation for CLiMa. As specified in section 5, during training CLIP is given either the input caption or the reference image (50-50%) and during inference it is given the input caption. The effect of this data augmentation is not reported.\n* The performance of vanilla LLaMa or prompt-tuned LLaMa is not reported, making the comparison between fine-tuned versions and prompted GPT-4 / Claude-2 not exactly fair.\n\nThe second issue is the limited model output results reported in the paper - the authors only show 6 examples in the appendix, and out of 3 labeled \"good\", the first one shows the image that, in my perception, doesn't really match the label (\"some dots are clustered together\" in the prompt, but shows just rows of points in the image)"
            },
            "questions": {
                "value": "How did the authors arrive at the specific prompt that was used for GPT-4 / Claude-2 and how do they know if that is the one that indeed yields a reasonably high performance from the models?\nGiven the iterative re-generation, the models could have also made use of the TikZ compiler error / stack trace, what was the rationale for not using this information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7987/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834956208,
        "cdate": 1698834956208,
        "tmdate": 1699636983790,
        "mdate": 1699636983790,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QCvba4mv11",
        "forum": "v3K5TVP8kZ",
        "replyto": "v3K5TVP8kZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_1jqj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7987/Reviewer_1jqj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors collect 119k TiKz datat to fine-tune LLaMa, and achieve better performance compared to GPT4 and Claude2 in TiKz generation. \nAlso, authors propose a method to prevent the TiKz code from not compiling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Good approach error handling method. Quite interesting study: on a specific domain, with relatively small data scale (119k) and LoRA, the author's approach can actually perform pretty well.\n2. Evaluation metrics are extensive and conving.\n3. Presentation is clear."
            },
            "weaknesses": {
                "value": "1. Seems like author didn't show any conversation example where users can edit or optimize the code while in the chat. \n2. Recently, [1] propose to leverage SVG, a similar format to TiKz, to conduct image understanding, generation and editing. How do authors judge on this?\n3. The error handling strategy seems quite standard and wildly used in programming languages. \n\n[1] Cai, Mu, Zeyi Huang, Yuheng Li, Haohan Wang, and Yong Jae Lee. \"Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding.\" arXiv preprint arXiv:2306.06094 (2023)."
            },
            "questions": {
                "value": "1. Any code error rate comparsion? (compiling success rate)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7987/Reviewer_1jqj"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7987/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986622508,
        "cdate": 1698986622508,
        "tmdate": 1699636983628,
        "mdate": 1699636983628,
        "license": "CC BY 4.0",
        "version": 2
    }
]