[
    {
        "id": "ylCzme1Ud0",
        "forum": "ZFjp5Q2hLn",
        "replyto": "ZFjp5Q2hLn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_Kx6Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_Kx6Q"
        ],
        "content": {
            "summary": {
                "value": "Assuming that the adversary can append poisoning samples into the minibatch at each iteration, this study investigates the realization of gradient attack through data poisoning. Empirical investigation demonstrates that data poisoning can simulate several types of gradient availability attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This study tries to make a connection between gradient attacks and data poisoning. To this end, this study introduces a novel threat model and an attack algorithm under the threat model."
            },
            "weaknesses": {
                "value": "Overall, the paper lacks proper notations and definitions to represent the authors' ideas.\nFor example, Section 4 introduces an attack method, while the description of the algorithm lacks proper notations and definitions, and it is hard for the reviewer to understand it."
            },
            "questions": {
                "value": "Overall, the paper lacks proper notations and definitions to represent the authors' ideas. For this reason, the reviewer could not properly understand \n\nThreat model: The proposed threat model allows the adversary to append poisoning samples into the minibatch at each iteration. In the federated learning setting, this threat model would be justified because the adversary can take part in the federated learning protocol and can inject any samples into its batch. However, in the centralized setting, the reviewer could not find a good reason why this setting is worth consideration.\n\nPoisoning scheme: I could not understand the meaning of eq. 1. In eq. 1, function f_p is minimized w.r.t. S in \\mathcal{F}. What is f_p? This is called the \"poisoning function\" in the manuscript, but I could not find the definition. Also, I could not understand what S \\in \\mathcal{F} means.\n\nTechnical novelty: The following can be incorrect due to my lack of understanding of the proposed idea. In my understanding, the proposed method is to find poisoning samples that cause a specific type of gradient attack. If the technical contribution is based on this technique, I think its novelty is limited.\n\nIn page 4: The authors defined the learner's goal to achieve the lowest loss on a test set, which is wrong. The goal would be to attain the lowest generalization error, and the mean is to minimize the training error. The test error is simply used as an estimator of the generalization error, which the learner should not minimize."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648862849,
        "cdate": 1698648862849,
        "tmdate": 1699637041077,
        "mdate": 1699637041077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3342fF4El5",
        "forum": "ZFjp5Q2hLn",
        "replyto": "ZFjp5Q2hLn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_AvRP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_AvRP"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method for performing data poisoning attacks that mimic gradient attacks. The effect of such attacks is evaluated in federated learning settings."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ Poisoning attacks are a relevant topic, and they can have an impact on different ML applications."
            },
            "weaknesses": {
                "value": "- The paper exceeds the page limit (this could be, perhaps, a reason for desk rejection). \n- In Section 3.1 the optimization problem is not well formulated. The parameters of the model depend on the training data and the training objective. But this dependency is not show explicitly in the formulation provided by the authors. It is unclear the problem they are trying to solve. \n- The settings and the threat model are unclear: the authors mixed different poisoning attacks strategies from centralized and federated machine learning. \n- Equation 1, which is the basis for the entire attack is not well explained and justified. It is unclear how this attack compares to the formulation of optimal attack strategies (relying on bilevel optimization), gradient-matching strategies like Witches\u2019 Brew, or different model poisoning attacks in federated learning. The novelty and contributions of the paper are also unclear."
            },
            "questions": {
                "value": "It is difficult to understand what is the scope of the paper and the settings where this attack can be applied. There are some aspects that require clarification to understand better the contribution and assess the soundness of the approach. For example: \n+ What is the novelty of the paper?\n+ Can the authors clarify the threat model?\n+ Can the authors explain and justify equation 1?\n+ Can the authors explain the equation in Section 3.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707299436,
        "cdate": 1698707299436,
        "tmdate": 1699637040967,
        "mdate": 1699637040967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wagXK7Dnyy",
        "forum": "ZFjp5Q2hLn",
        "replyto": "ZFjp5Q2hLn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_jPwq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_jPwq"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a data poisoning attack (probably for federated learning) where the attacker can insert poisonous data in a training batch at each training step. The attacker uses auxiliary dataset to approximate a good gradient and computes poisonous data s.t. the corresponding poisonous gradient can reduce the accuracy of the resulting model. To compute poisonous data, the attacker uses an optimization based on existing model poisoning attacks. Experiments show mixed results, i.e., in some case the attack succeeds and in some cases it fails."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Data poisoning is an important problem for federated learning"
            },
            "weaknesses": {
                "value": "- Threat model is not clear but looks like might be very impractical\n- Paper is difficult to read and understand.\n- Evaluation set up is not clear and results are also not promising/clear."
            },
            "questions": {
                "value": "- It seems tough for an attacker to submit data in each batch/round of FL as the threat model; also note that this would be impossible in centralized learning.\n- I feel the paper requires more work at least in terms of presentation. I see that all the model poisoning attacks mentioned in Sec. 4.1 are for federated learning but the paper never clarifies the learning setting. \n- In Sec 4, I am not sure how LIE objective can be used for the attacks: how can you minimize || g^p - g^a || to get an attack? I am assuming z^{max}.\\sigma is a constant. Also what is g^p in this equation? \n- Note about LIE: it is an aggregation rule independent attack unlike what Sec 4.1 says. \n- In method: what is the intuition behind using attacks in Sec 4.1? If I understand correctly, attacker tries to craft poisonous data s.t. it gives a poisonous gradient that minimizes a particular objective. But the objectives considered here are not that of state-of-the-art model poisoning attacks against FL [1]\n- Also Sim_{cos} is not defined anywhere before Table 1\n- Results of attack are not great as the paper says, so I am not sure what is the utility of these data poisoning attacks? Why have you not compared with any baseline SOTA data poisoning availability attacks, e.g., that by Shejwalkar et al. (2021)?\n\n\n\n[1] Shejwalkar and Houmansadr, Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning, NDSS 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8371/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8371/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8371/Reviewer_jPwq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803083695,
        "cdate": 1698803083695,
        "tmdate": 1699637040811,
        "mdate": 1699637040811,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3GrzHMiisa",
        "forum": "ZFjp5Q2hLn",
        "replyto": "ZFjp5Q2hLn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_vYfW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8371/Reviewer_vYfW"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of conducting availability data poisoning attacks against centralized learning framework but still resembles some distributed learning framework, such as robust aggregation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The main idea of demonstrating the possible equivalence between data poisoning attacks and gradient attacks under non-convex settings is interesting.\n2. The proposed attack achieves good performance in certain settings."
            },
            "weaknesses": {
                "value": "1. I do not understand how practical the assumed threat model is: the attacker can continuously monitor the training process of the model in each iteration and correspondingly inject the poisoning points. If there is such an application scenario, the authors should clearly articulate the possible use cases. Simply arguing from the perspective of worst case scenario is not sufficient, as no system can withstand an attack under unrealistically strong assumptions. As an attack paper, it is expected to make as less assumption of an assumption as possible to the attacker capability. \n2. The experiments using poisoning ratio as high as 0.48 is not realistic. If an attacker can continuously inject different poisoning points in each iteration using as high as 48% of poisoning ratio for each mini batch, I would expect the model to completely fail in the end, but is simply not the case with the experiments, indicating that the attack indeed can be significantly improved. Related to this, the designed poisoning attacks also achieve much performance at a much higher poisoning ratio in some settings. \n3. Some of the conclusions are straightforward. For example, using a restricted constraint set to generate the poisoning points can impact attack performance because the valid data points generated may not be able to generate the desired poisoned gradients."
            },
            "questions": {
                "value": "1. Is there an application scenario where the proposed attack can be a threat?\n2. what the data poisoning attacks perform worst at higher poisoning ratios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8371/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699031847558,
        "cdate": 1699031847558,
        "tmdate": 1699637040676,
        "mdate": 1699637040676,
        "license": "CC BY 4.0",
        "version": 2
    }
]