[
    {
        "id": "IR5apZb4YT",
        "forum": "S4YVoQ70b2",
        "replyto": "S4YVoQ70b2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_HKSo"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the challenge of partial identifiability in Inverse Reinforcement Learning (IRL) under the condition of non-exponential discounting. Specifically, it focuses on a hyperbolic discounting model, which is characterized by temporal inconsistency, thereby inducing non-stationarity in the underlying Markov Decision Process (MDP).\n\nTo address the effects of this temporal inconsistency, the paper proposes a series of behavioral models: the resolute policy, the naive policy, and the sophisticated policy. For each of these models, the paper succinctly summarizes their properties, encompassing the uniqueness of the optimal value function, the stochastic nature of the policy, and the stationarity of the policies across varying time steps.\n\nInterestingly, the paper defines the identifiability of reward functions in relation to the optimal policy under an exponential discounting setting. This appears contradictory to the paper's main focus on non-exponential discounting.\n\nThe theoretical findings indicate that no regularly resolute, regularly naive, or regularly sophisticated behavioral model is identifiable under non-exponential discounting or a non-trivial acyclic transition function. These results suggest that IRL is incapable of inferring sufficient information about rewards to identify the correct optimal policy. Consequently, it is implied that IRL alone is insufficient to thoroughly characterize the preferences of such agents."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The matter of identifiability in Inverse Reinforcement Learning (IRL) under a non-exponential discounting setting has yet to be explored in prior studies.\n\n2. The paper's overall structure is logically organized and easily navigable. Definitions are meticulously presented, supplemented with numerous intuitive examples to facilitate reader understanding of the core content.\n\n3. The theoretical findings are well presented, thereby supporting the claims made in the paper."
            },
            "weaknesses": {
                "value": "1. It's challenging to comprehend the concept of sophisticated policy as delineated in Definition 7. For instance, it's unclear why the policy, $\\pi(\\xi)$, is not dependent on the time step and how it correlates with step-wise policies. Similarly, it's puzzling why the Q function $Q^\\pi(\\xi,a)$ is also independent of the time step. Given that the optimal policy can vary at each time step, it becomes complex to determine which strategy exhibits more \"sophistication\". In many Markov Decision Processes (MDPs), the so-called sophisticated policy is not singular. The paper states that \"$\\pi$ is sophisticated if it only takes actions that are optimal given that all subsequent actions are sampled from $\\pi$.\" Could you clarify this definition? Specifically, I'm interested in understanding how one would define optimality in a non-stationary MDP that spans across different (or all) time steps.\n\n2. The definition of identifiability appears to be founded on an exponentially discounted MDP, even though the paper focuses on a non-exponentially discounted setting. The paper attempts to provide some intuitive explanations, but they fall short in terms of persuasiveness. If the term 'optimality' has a clear definition under different behavior models, then the term 'identifiability' should also exhibit the capacity to characterize these models.\n\n3. This paper lacks empirical studies to substantiate its arguments. The main results suggest that IRL alone may be inadequate to fully characterize the preferences of agents in a non-exponentially discounted setting. However, a potential solution has not been proposed, and it is yet unclear how the existence of non-identifiability impacts empirical performance. It would be beneficial to see these points addressed in future research."
            },
            "questions": {
                "value": "1. why the policy, $\\pi(\\xi)$, is not dependent on the time step and how it correlates with step-wise policies?\n2. why the Q function $Q^\\pi(\\xi,a)$ is also independent of the time step ?\n3. How to understand the \"sophisticated policy\"?\n4. how the existence of non-identifiability impacts empirical performance?\n5. What potential solutions could address the issue of non-identifiability in IRL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698337910999,
        "cdate": 1698337910999,
        "tmdate": 1699636329825,
        "mdate": 1699636329825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cgmejcSNZz",
        "forum": "S4YVoQ70b2",
        "replyto": "S4YVoQ70b2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_nfpA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_nfpA"
        ],
        "content": {
            "summary": {
                "value": "The paper defines novel MDP concepts based on novel definitions of discount factors. The authors started by presenting in the background the standard exponential discounting setting. Then they define the non-exponential setting, defining in section 4 the optimality conditions for the policies. Finally, they studied when behavioral models for inverse reinforcement learning are identifiable."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper provides novel results on partial identifiability in IRL with non-exponential discounting. \n\n- They provide the first theoretical results on IRL in a non-exponential discounting setting."
            },
            "weaknesses": {
                "value": "- The main weakness of the work is the motivation of it. The authors do not provide enough reasons why we need to consider a different discounted setting with respect to the exponential discounted one. In literature, when is it used the hyperbolic setting? Why is it relevant in practice? Moreover, If the setting is more general and reasonable, I think it would be better to present directly it in the background section rather than presenting the standard exponential discounted ones and then the new setting.\n\n- The main focus of the paper is (reading the abstract) on Inverse Reinforcement Learning, but, in the end, the IRL contribution of the paper is condensed into only one page and a half. \n\n- There are no experimental or numerical evaluations of the proposed approach at least to show why the proposed setting is relevant."
            },
            "questions": {
                "value": "- A reward function is optimal under more than one policy. Then, why is the behavioral model defined as a mapping between $\\mathcal{R} \\rightarrow \\Pi$ and not $\\mathcal{R} \\rightarrow P^\\Pi$?\n\n- Proposition 1 seems to be not easy to verify. How can we understand if an MDP satisfies it?\n\n- Why is it relevant to choose discount factors that are not temporally consistent? Can the change in preference of an agent be described with a change in the reward function?\n\n- If in the end, we are using exponential discounting to find our optimal policy why do we need to study a different setting before?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764803414,
        "cdate": 1698764803414,
        "tmdate": 1699636329747,
        "mdate": 1699636329747,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OsdaaTFYVz",
        "forum": "S4YVoQ70b2",
        "replyto": "S4YVoQ70b2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_wg8J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3737/Reviewer_wg8J"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the partial identifiability problem in IRL with non-exponential discounting; the authors provide their theoretical conclusion that for some behavioral models with non-exponential discounting, the partial identifiability problem persists."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There are a few theoretical results that seems quite interesting and potentially significant. I appreciate the clear definitions and background. However, I am unable to determine whether these results are easily ported results or more original findings."
            },
            "weaknesses": {
                "value": "So much of the proof is deferred to the appendix, it would be helpful if a proof sketch is summarized in the main text."
            },
            "questions": {
                "value": "From R we could get to different f(R), which is denoted Am(f), a set of rational models follows R. Rather than knowing this set is singleton, I think a more important question maybe how small the set is, and whether it is contiguous. Do you think non-exponential discounting effects contiguity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699073487528,
        "cdate": 1699073487528,
        "tmdate": 1699636329655,
        "mdate": 1699636329655,
        "license": "CC BY 4.0",
        "version": 2
    }
]