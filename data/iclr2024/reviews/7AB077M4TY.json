[
    {
        "id": "fybJ01OOkg",
        "forum": "7AB077M4TY",
        "replyto": "7AB077M4TY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_APZy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_APZy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an adaptive weight updating method for gradient-based training of neural networks. The idea is to model the trajectory of network weights during training and then extrapolate to future steps/epochs. The trajectory is modelled using Dynamic Mode Decomposition (DMD). This can be reviewed as a look-ahead trick to speed up convergence. The authors showed that the additional prediction of weight changes lead to faster covergence in terms of the number of epochs in some cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed algorithms seem to be technically sound."
            },
            "weaknesses": {
                "value": "- The methods and experiments do not seem to completely described. With no code provided, I had to guess and fill in potential details myself.\n- It is unclear whether the proposed methods indeed make consistent improvement compared with SOTA optimisers. The empirical comparisons were not carefully designed, with arbitrary and inconsistent selection of baselines. For example, Adam was compared, but it is also used as a fall-back when KGA (the proposed method) does not seem to help. In addition, the KGA seems to have a significant memory footprint and requires additional running time; there should be fair comparison with the SOTA optimisers, under the same budget of computing resources.\n- Section 3.3 seems to be there just to add more technical content. It is not clear to me whether it actually work significantly better than simpler baselines such as Delta masking or Norm masking."
            },
            "questions": {
                "value": "- I do not understand Figure 3. Why does alpha=10 mean the vector is outside the unit circle? How does it related to Figure 12?\n- In equation 14, how are lamdas indexed? What are \"steady\" modes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729981009,
        "cdate": 1698729981009,
        "tmdate": 1699636447884,
        "mdate": 1699636447884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hqXDjfCObY",
        "forum": "7AB077M4TY",
        "replyto": "7AB077M4TY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_ptUk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_ptUk"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a method aimed at accelerating the convergence of neural network training. Specifically, the proposed approach applies Singular Value Decomposition (SVD) to decompose a set of weights at a given timestep, denoted as W_i. Through this process, the method identifies more important and less important basis components within the weights. Subsequently, the authors project the weights of the next timestep, W_{i+1}, onto these basis components. Finally, they emphasize the major axis (important basis) when calculating the update term (W_{i+1}-W_{i}). This method effectively adds weighting to the important basis components, contributing to faster convergence during training.\nHowever, I have some concerns as follows:\n\n1)\tIt appears that the primary focus of this work is identifying important directions within the current weights (W_i) and emphasizing updates along these directions to facilitate faster convergence during training. However, the background and related works seem to be rooted in dynamic systems knowledge. I think the introduction and related works are not fit to the proposed work.\n\n2)\tThe method proposed in this work involves storing the matrix U for efficient computation. However, this approach makes an assumption that the important directions within the weights exhibit relatively fewer changes over a certain number of iterations or epochs. This assumption is essential for the method's effectiveness, but it should ideally be substantiated through either theoretical or empirical evidence to validate this point and applicability in practice.\n\n3)\tAdditionally, it's worth noting that the KWM method also reuses a mask for an entire epoch to select candidates, which entails a similar assumption about the stability of important directions within that epoch. \n\n4)\tSeveral prior works have applied SVD or Eigen Decomposition to identify important weight components, often for purposes such as pruning or interpretation. It appears that the proposed method shares similarities with training a pruned networks (KWM) with higher learning rates (KGA), or even lifelong training from a small network. To provide a more comprehensive understanding of the proposed method, it would be valuable to conduct a comparison with these related approaches in terms of their underlying ideas and concepts, even if the focus is not solely on performance comparison. This would help clarify the method's novelty and its place within the existing body of research.\n\n5)\tWhile the paper compares the proposed method with Adam, SGD, and some variants of the proposed approach, it's important to note that there are various other methods and algorithms designed for faster trainin. To provide a more comprehensive assessment of the benefits of the proposed method, additional experiments and comparisons with a broader range of optimization algorithms and training techniques would be valuable. \n\n6)\tGiven the similarity between the proposed method and using a higher learning rate for specific weight components, it is important to consider the learning rate issue in the comparison experiments. The experiments provided in the paper, where the same learning rate was used for both the proposed method and the naive optimizers, may not provide a fair comparison.\n\n7)\tFrom my understanding, this work utilizes SVD for the weights of a single timestep, rather than considering multiple timesteps. This approach appears to capture the state of the network at a single previous step, rather than modeling training dynamics that involve interactions across multiple timesteps. Clarification or further exploration of the method's relationship to training dynamics may be beneficial to provide a more comprehensive understanding of its operation and potential limitations.\n\n8)\tIn minor, the citation style is different from other papers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "See above"
            },
            "weaknesses": {
                "value": "See above"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Reviewer_ptUk"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740845248,
        "cdate": 1698740845248,
        "tmdate": 1699636447793,
        "mdate": 1699636447793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lZVticOPmp",
        "forum": "7AB077M4TY",
        "replyto": "7AB077M4TY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_xUqE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_xUqE"
        ],
        "content": {
            "summary": {
                "value": "The work builds upon and investigates prior Koopman operator theory (KOT) based approaches to analysing and boosting ML optimization, taking advantage of the linearized perspective offered by KOT in conjunction with the operator compression methods like DMD to obtain adequate low rank estimates. They propose an algorithm for turning the noted connections between ML optimization and KOT in prior works into practical application by creating a gradient boosting strategy that is further augmented by a masking strategy to lower the burden of the needed Koopman computations. Numerical results are presented to support claimed results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A. This work builds well on prior touchstone papers - showcasing why their insight was useful and applicable, while also showcasing the limits of those early insights. In particular I am pleased to see the comparison to Dogra and Redman 2020, which was the second (see below) systematic proposal and exploration of these connections, yet severly limited in actual practical impact due to the shorter time period of effectiveness of the estimated Koopman operators (their method has a tendency to veer off-course significantly earlier than this work's as shown by the authors).\n\n\nB. The proposed method consistently outperforms on accuracy for the chosen tasks (for caveat, see below), and for reasons that are borne out by the theoretical development well. It is generic enough to scale to many problems as well (for caveats, see below).\n\nC. The scale of the numerical experiments compares very well compared to the older papers (75M parameters vs order 10k parameters in the older works)."
            },
            "weaknesses": {
                "value": "A. In my opinion, by far the most significant weakness is how the cost vs performance gains comparison is handled. For ex, consider Fig. 7d, which is missing the comparison to the costs incurred in SGD (the blue curve is invisible). The proposed method outperforms in accuracy, but if the costs are astronomically high, are those gains worth it? Indeed, this should be tabulated and presented as a central focus of the work (whether in the limitations section or the advantages section I don't know, because I couldn't easily see what the overall cost versus performance trade-offs were). My score will change substantially depending on what the true results are (and my deepest apologies are extended to the authors if I have simply missed the comparison somehow in the main body - which is itself a weakness of the work as well, since this comparison should be unmissable). Table 1, A.3 seems to be the relevant comparison but it being in the appendix is a strong oversight. I am happy to be proven wrong (and change my score aptly) based on this point.\n\nB. The literature review is missing two important early works in the modern ML/KOT intersection. To the best of my knowledge, the idea that KOT could be profitably used to study/boost optimization (especially in ML, but in general as well) was first mentioned in [1] and first properly investigated within the realm of ML in [2]:\n\n[1] Section 4.1, F. Dietrich, T. N. Thiem, I. G. Kevrekidis, On the Koopman Operator of Algorithms, arXiv:1907.10807 (2020)\n\n[2] Section 3, A. S. Dogra, Dynamical Systems and Neural Networks, arXiv:2004.11826 (2020)\n\nC. The applications to which this method has been turned to seem not as impressive as the method or the size of NNs to which it is applied. There is no dearth of substantial ML problems that are intensely resource heavy AND non-trivial - surely the true validation would be to showcase that the method scales well to other kinds of problems too (I am happy to be corrected on this point). The authors do an excellent job of handling large parameter models (if I am right in inferring that 75M means 75 million parameters for ResNet-50). However, as interesting as it is to me to see where the method succeeds is where it would fail - which problems/loss functions are too complicated for the method to hold (or is the method practically so generic that for every type of loss function and problem, it succeeds? That would be a bold claim, even though the theory to KOT does support that: the practice of KOT has seldom shown that however due to how costly it is to compute the needed operators). An easy approach would be to compare to the Hamiltonian solvers Koopman boosted by Dogra and Redman 2020, and show that the authors methods blow the prior work out of the water.\n\nD. While not critical, there are presentation issues in the work. For ex, Fig. 3 is poorly presented and formatted (some symbols are almost impossible to read on the circle. In general the font sizes on all figures should be substantially larger)."
            },
            "questions": {
                "value": "Please see weakness B and answer if such an experiment would be possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Reviewer_xUqE",
                    "ICLR.cc/2024/Conference/Submission4670/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785483170,
        "cdate": 1698785483170,
        "tmdate": 1700678048247,
        "mdate": 1700678048247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9gOhS8O1H1",
        "forum": "7AB077M4TY",
        "replyto": "7AB077M4TY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_aC1Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4670/Reviewer_aC1Y"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests two modifications to training algorithms based on Koopman mode decompositions of the weights. Termed Koopman Gradient acceleration, this modification is to project the gradient (or equivalently, the changes in the weight during a training iteration) on the stable singular vectors of the matrix approximation of the Koopman operator. They also propose a more computationally efficient version of KGA, called KWM or Koopman weight masking, where, the components that have negligible unstable Koopman components are set to 0. These modifications are compared against vanilla SGD in training some fully connected and convolutional networks, for a few epochs; in numerical experiments, they are shown to produce smaller training and test errors."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The focus on numerical experiments to demonstrate KGA and the speed up obtained using KWM\n*  drawing connections between the optimization and dynamics viewpoints."
            },
            "weaknesses": {
                "value": "1. My main concern is that the methods described are based on heuristics and no theoretical justification is presented. Hence, it is unclear whether this method will actually lead to convergence to an optimization dynamics (e.g. SGD) in a given non-convex optimization problem. \n\n2. The terms stable manifold etc are used incorrectly -- these do not refer to manifolds tangent to the singular vectors of the Koopman operator approximations. The demarcation into \"stable\", \"unstable\" and \"neutral\" Koopman modes is also quite hand-wavy and it is not explained what projecting the weight updates to the basis of some singular vectors of a matrix approximation of the Koopman operator, actually does. \n\n3. First of all, the DMD matrix is only an approximation of the Koopman operator on L^2. It would change with the snapshots used. Projecting on the range of a particular approximation induces a representation error and further the ad hoc masking completely changes the optimization dynamics."
            },
            "questions": {
                "value": "All the major questions are about the soundness of the method proposed and why it is expected to work.\n\n1. Please check the equations for the Koopman mode decompositions (the DMD part seems correct to me). It should read as $$K g(x) = \\sum_{k \\in \\mathbb{Z}^+} \\langle g, \\phi_k\\rangle K \\phi_k(x) =  \\sum_{k \\in \\mathbb{Z}^+} \\langle g, \\phi_k \\rangle \\lambda_k \\phi_k(x),$$\nhere, $\\phi_k$ are the eigenfunctions of the Koopman operator on $L^2.$ In the text, are the projections (the inner products) $c_k$ and the eigenfunctions $\\phi_k$? If yes, $\\phi_k$ should be a function of $x.$\n\n2.  Why does the gradient projection accelerate optimization, given that the optimization dynamics is completely changing? \n\n3. Are the different columns of the matrix $W_i$ time snapshots? In the DMD algorithm, they have to be.\n\n3. What is the justification for projection on to the ``stable'' singular vectors? It is true that the eigenvalues close to the unit circle, but within the unit disk, have a correspondence with almost invariant structures or \"slowly decaying modes\", but these are not precisely used (even if qualitatively) in designing KGA. The slow decay is in the correlations between observables. If KGA did involve identifying the observables that show slow decay of autocorrelations for instance, that might more sense qualitatively. \n\n4. In the numerical experiments, do we revert to SGD or some other optimization algorithm after a few epochs? What happens if we train for longer with KGA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4670/Reviewer_aC1Y"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699128665216,
        "cdate": 1699128665216,
        "tmdate": 1699636447601,
        "mdate": 1699636447601,
        "license": "CC BY 4.0",
        "version": 2
    }
]