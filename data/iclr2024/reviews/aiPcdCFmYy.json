[
    {
        "id": "soYgiJ3vyT",
        "forum": "aiPcdCFmYy",
        "replyto": "aiPcdCFmYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_v5sT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_v5sT"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Sinkhorn distributional RL algorithm, which uses sinkhorn divergence to minimize differences between the current and target return distributions. Theoretical proofs show some properties of sinkhorn divergence and confirm its convergence properties. Empirical tests on Atari games show that the algorithm outperforms or matches existing distributional RL methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow, and the use of sinkhorn divergence is novel and interesting. It is complete and has both the theoretical part and the experimental part."
            },
            "weaknesses": {
                "value": "- I am not quite convinced by the experimental results. Specifically, Figure 5 and the subsequent remark indicate that \"sinkhornDRL achieves better performance across almost half of the considered game\". However, it might be possible that both QRDQN and MMD each excel in roughly 50% of the games over the other. Consequently, sinkhorn should, on average, surpass either of the two in 50% of the games, as Sinkhorn essentially interpolates between these two algorithms. Observing Table 3 seems to draw the same conclusion: when Sinkhorn outperforms QRDQN, it often falls to MMD; conversely when it outperforms MMD, it tends to underperform relative to QRDQN. However, I admit that this is not absolute, as there are indeed some games where Sinkhorn surpasses both. But my concern is whether \"better performance across half of the games\" is enough.\n\n- The kernel assumed in the theoretical part (e.g., theorem 1) is different from that used in the experiments (the Gaussian kernel). So I am not sure how are the theory and the experiments connected.\n\n- There seem to be some related works that are missed. I'm not sure if all of them are relevant, but the author can check if they are related. Some remarks are attached to each of them.\n\n  - Li, Luchen, and A. Aldo Faisal. \"Bayesian distributional policy gradients.\" : this paper proposes the policy gradient for distributional RL, and they use Wasserstein distance as well. \n\n  - Wu, Runzhe, Masatoshi Uehara, and Wen Sun. \"Distributional Offline Policy Evaluation with Predictive Error Guarantees.\" : this paper considers the total variation distance, which seems to be stronger than both Wasserstein distance and MMD. Hence I am wondering if it is also stronger than the sinkhorn divergence.\n\n  - Ma, Yecheng, Dinesh Jayaraman, and Osbert Bastani. \"Conservative offline distributional reinforcement learning.\" : this paper learns conservative return distributions, which seems necessary in the offline setting. Their theoretical guarantees are also under the Wasserstein distance.\n\n  - Rowland, Mark, et al. \"An analysis of quantile temporal-difference learning.\" : this paper studies the convergence of the quantile TD algorithm, and thus you may want to compare your analysis with theirs. They also established the fixed point error guarantee, and I am not sure if the same thing holds under sinkhorn divergence."
            },
            "questions": {
                "value": "- I really appreciate that the author established some nice properties of sinkhorn divergence (e.g., theorem 1, proposition 1). However, it is still unclear to me why the authors proposed to use sinkhorn divergence. They claimed that it is an interpolation between Wasserstein distance and MMD, and thus, I am wondering why it is expected to be better.  It would be great if the authors could provide a more either rigorous or intuitive explanation.\n\n- The author proposed to generate finite samples to approximate the distribution (algorithm 1). Hence, I think a question is how large the statistical error will be, i.e., what is the error incurred when learning from finite samples, as compared to learning from the true distribution? Furthermore, how do these errors accumulate within an MDP? This may be a bit beyond the scope of this paper, but it will be interesting to see how statistically robust sinkhorn divergence is to finite samples intuitively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698546180297,
        "cdate": 1698546180297,
        "tmdate": 1699636704914,
        "mdate": 1699636704914,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6VhbDtzoxv",
        "forum": "aiPcdCFmYy",
        "replyto": "aiPcdCFmYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_PTyg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_PTyg"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on solving online reinforcement learning using a distributional reinforcement learning formulation. It proposes a new algorithm called Sinkhorn distributional RL (SinkhornDRL) which: from the theory side, it enjoys the contraction property of the corresponding distributional Bellman operator; and empirically outperforms existing distributional RL baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The algorithms show competitive results compared to existing distributional RL algorithms in the widely used benchmark 57 Atari games\n2. It theoretically shows the contraction property of the distributional Bellman operator and the corresponding theoretical convergence of SinkhornDRL\n3. The empirical analysis is sufficient regarding sensitivity analysis of the hyperparameters, computation cost, and etc."
            },
            "weaknesses": {
                "value": "1. The presentation needs to be polished since a lot of notations have not been introduced and may not be reading-friendly for people who are not familiar with distributional RL literature, as listed later.\n2. The introduction of the algorithms seems not sufficient, for instance, what is the next step after getting the Sinkhorn distance in algorithm 1, such as gradient descent or other to minimize this distance?\n3. Section 4.2 has a sequence of theoretical results, while more intuition will be helpful, such as the term $\\overline{\\Delta}(\\gamma, \\alpha)$ in Theorem 1(3) is represented in a very complicated way. So what do these terms mean and how large are they will be more helpful for the readers? So as the relationships to Gaussian or general kernels."
            },
            "questions": {
                "value": "1. As Sinkhorn is a well-known approach in optimal transport literature, it is curious what is the technical contribution of this paper, is it just an RL application inspired by Sinkhorn?\n\nOther small issues:\n1. Section 2.1, using $\\overset{D}{:=}$ without defining the notation of D.\n2. $Z_{\\theta^\\star}$ has not been introduced"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619668459,
        "cdate": 1698619668459,
        "tmdate": 1699636704743,
        "mdate": 1699636704743,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S9HOaSOBs6",
        "forum": "aiPcdCFmYy",
        "replyto": "aiPcdCFmYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_CJTu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_CJTu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical and empirical study of\ndistributional reinforcement learning algorithms where return\ndistributions are trained to minimize a Sinkhorn divergence, a\ndivergence measure closely related to the entropically-regularized optimal\ntransport cost. The authors establish that the distributional Bellman operator is a\ncontraction under the Sinkhorn divergence, which justifies use of\na Sinkhorn divergence loss for distributional policy\nevaluation. Moreover, the authors demonstrate that their\nimplementation of \"Sinkhorn Distributional RL\" performs well in the\nAtari suite, matching or outperforming competing algorithms in many games."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Sinkhorn divergence/entropically-regularized OT are an important class\nof divergence measures on the space of probability distributions,\nparticularly with regard to computational efficiency and numerical\nstability. This paper is the first, to my knowledge, to formally study\nthese for the purpose of distributional RL. The empirical analysis is\nrigorous and interesting."
            },
            "weaknesses": {
                "value": "The main issues I have with the paper, which I will expand upon\nfurther below, are the following,\n\n1. The writing/organization can definitely use some work -- some parts\n   of the text are awkward to read / not easy to follow.\n2. I have some concerns about the proof of Theorem 1. Ultimately, I\n   believe the claims are correct, but there are some parts that are\n   less clear that should be clarified.\n3. Overall, the math tends to be quite sloppy. There is lots of abuse\n   of notation, some terms are not clearly defined, and that makes\n   some of the steps very difficult to follow (maybe this alone can\n   clarify my concerns about Theorem 1).\n4. Some of the empirical results seem potentially misleading.\n\nSome more explicit details follow.\n\nThe paragraph labeled by \"Advantages over Quantile-based / Wasserstein\nDistance Distributional RL\" is highly unpleasant to read. The \"inline\nbulletpoint\" style is not very easy to follow. Some concepts here are\nnot defined, which limits the utility of this paragraph for motivating\nSinkhornDRL. \n\nRegarding the contraction factor $\\Delta(a,\\alpha)$, it would be nice\nif there was an explicit upper bound given (perhaps in terms of the\nrange of returns). The fact that this term is strictly less than 1\nrelies on some discrepancy between Wasserstein and entropy-regularized\nWasserstein, and it is not intuitive to me how large that discrepancy\nis. Should we expect $\\Delta(\\gamma,\\alpha)$ to be larger or smaller\nthan, say, $\\gamma$ or $\\sqrt{\\gamma}$?\n\nRegarding the empirical results, it is slightly unsettling that Table\n2 reports only the statistics of the \"Best\" scores. In fact, it is not\nactually clear to me what that means.\n\nMoreover, the \"ratio improvement\" figures only show results for games\nwhere SinkhornDRL outperforms its competitors (and the selection of\ngames varies per competitor). The corresponding plots in the Appendix\n(where the human normalized score is shown for each game) shows that\nSinkhornDRL really only outperforms its competitors on roughly half\n(maybe slightly less than half) of the games.\n\n## Proof of Theorem 1\nI don't understand the proof of part 1, and the math is fairly\nimprecise. For instance, the definition of convergence that is\nleveraged between $\\mathcal{W}^{c, \\epsilon}$\nand $\\mathcal{W}^\\alpha$\nis sloppy -- it is being written like as if these are scalar\nfunctions, but they are not. Is the convergence uniform? Also,\nequation (14) does not establish the contraction any better than the\nclaim that $\\mathcal{W}^{c,\\epsilon}\\to\\mathcal{W}_\\alpha$ does, in my\nopinion. You still haven't shown a contraction here. That said, I\nbelieve the claim is true. Same comments for the proof of part 2.\n\nThe correctness of the proof of part 3 relies on a hypothesis that the\noptimal coupling for $W_\\alpha$ and $W^{c,\\epsilon}$ cannot be the\nsame (otherwise, step (b) of equation (21) can be an equality). Is it\nknown that this hypothesis is true? If so, I think this should be\ncited.\n\n## Minor Issues\nIn the last sentence of the \"Quantile Regression (Wasserstein\nDistance) Distributional RL\" paragraph (page 3), \"while naturally\ncircumstances the non-crossing issue\" should probably say \"while\nnaturally circumventing the non-crossing issue\".\n\nIn the output of Algorithm 1, I believe the second argument to\n$\\overline{\\mathcal{W}}^{c,\\epsilon}$ should be $\\{\\mathcal{T}Z_j\\}_{j=1}^N$.\n\nAbove equation (4), \"supreme form of Sinkhorn divergence\" should be\n\"supremal form of Sinkhorn divergence\"."
            },
            "questions": {
                "value": "What is the non-crossing issue?\n\nWhy is a sample/particle representation \"more flexible\" than modeling\nquantiles?\n\nWhy does the RKHS nature of MMD imply failure to capture geometry?\nRKHS are Hilbert spaces after all."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Reviewer_CJTu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701814493,
        "cdate": 1698701814493,
        "tmdate": 1699636704556,
        "mdate": 1699636704556,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C8cktVawIv",
        "forum": "aiPcdCFmYy",
        "replyto": "aiPcdCFmYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_jH87"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_jH87"
        ],
        "content": {
            "summary": {
                "value": "Much of the success of distributional RL is dependent upon how the return distributions are represented and which distribution divergence criteria is used. The authors propose a new variant of distributional RL called Sinkhorn DRL which uses Sinkhorn divergence as the distribution divergence criteria. The authors also provide theoretical proofs of the convergence properties of Sinkhorn DRL. The authors perform experiments that show the superiority of SinkhornDRL to current state of the art DRL algorithms on 55 different Atari games."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors did a great job giving background information on sinkhorn divergence and providing theoretical analysis to strengthen their argument. They also provided the necessary details for the experiments."
            },
            "weaknesses": {
                "value": "The discussion section is rather short, but I understand that this is due to the page limit and the authors giving preference to the more important sections of the paper."
            },
            "questions": {
                "value": "Why were only 3 seeds used during training? Was this due to more of a time/resource constraint?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6372/Reviewer_jH87"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715715424,
        "cdate": 1698715715424,
        "tmdate": 1699636704429,
        "mdate": 1699636704429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AVsYAj4PI7",
        "forum": "aiPcdCFmYy",
        "replyto": "aiPcdCFmYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_VQBS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6372/Reviewer_VQBS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new projection method for distributional reinforcement learning, which utilizes Sinkhorn divergence to learn the target return distribution.\n\nAs two popular existing distributional RL methods, QR-DQN and MMD_DRL provide $N$ return value outputs in terms of quantile and Dirac function respectively, the proposed method SinkhornDRL uses this neural network architecture to represent the return distribution and learn the return distribution function by the proposed Sinkhorn divergence loss function.\n\nTheoretically, this paper extends the result of MMD-DRL into proving the convergence property of the return distribution in terms of Sinkhorn divergence.\n\nIn the experiment, the authors provide the result of Atari-55 performance within 40 million steps, where the number of compared baselines is not enough to show the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The propsed sinkhorn divergence based Bellman update loss includes the regularization loss which is the mutual information of a given distribution and the target distribution.\n\nThis makes the above two distributions statistically independent, and the reguarlizaiton loss with the coefficeint $\\epsilon$ helps to increase the interquantile mean performance of atari-55."
            },
            "weaknesses": {
                "value": "The main theoretical contribution can be seen as a simple combination of the architecture of MMD DRL and Sinkorn divergence. The additional KL divergence term (mutual information) in the proposed loss is a key factor to improve the DRL algorithms except for two corner cases $\\epsilon=0,\\infty$. However, the role of this proposed element is not well explained in the main text. The detailed comments are as follows. The main strength statement in this paper is that Sinkhorn divergence can be useful in the complex environments with high dimensional action space. This discussion is not rigorous, because I cannot find any explanation why the mutual information between two return distribution and the dimension of action space is related.\n\nI also have the following minor concerns.\n\n1. It is hard to get an insight by introducing the Sinkhorn divergence. For example, the paper of MMD DRL provides the figure that explains the propsed scheme can estimate the high order moments better than the existing methods.\n2. The main ablation study (sensitivity anlysis) is provided without any detailed discussion. The authors states taht the property of loss changes as the value of $\\epsilon$ changes in the main theorem, but the empricial analysis and discussion is not provided well. \n3. The experiment protocol in this paper is not standard. Previous works almost conducted in 200M iterations or the authors could have chosen the protocol of the paper, DRL at the edge of statistical precipice to validate the performance.\n4. In table 2 and Figure 2, the propsed method outperforms in terms of mean score but not in median score than baselines, and the proposed method show remarkable performance in venture and sequest. This means that the proposed method is not a generally better algorithm but a specialist for Venture and Sequest. In my point of view, the authors should have stated why the propsee method is better in such envs.\n5. There exists more projection method for distributional RL such as EDRL [A], but the provided baselines is not enough.\n\n\n\n[A] Rowland, Mark, et al. \"Statistics and samples in distributional reinforcement learning.\"\u00a0International Conference on Machine Learning. PMLR, 2019."
            },
            "questions": {
                "value": "I mentioned my main concern in the weakness part. Please refer the weakness section.\n\nIn addition, it is hard to understand that reducing the mutual information helps to build a better projection operator.\nCan the authors provide detailed explanation why the mutual information between the return distribution and target distribution is important?\nIn my point of view, the unresticted statistics can be constructed better, because its deterministic samples can be more uncorrelated by reducing the mutual information.\nIf it is true, the main effect seems related to the sampling scheme, not the discrepency between two ground-truth return distribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6372/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698925905110,
        "cdate": 1698925905110,
        "tmdate": 1699636704207,
        "mdate": 1699636704207,
        "license": "CC BY 4.0",
        "version": 2
    }
]