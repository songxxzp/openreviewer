[
    {
        "id": "HKV2x9BsdU",
        "forum": "lVZ7Tafw51",
        "replyto": "lVZ7Tafw51",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission397/Reviewer_mMin"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission397/Reviewer_mMin"
        ],
        "content": {
            "summary": {
                "value": "This paper introduce two biologically plausible features in spiking neuron networks:\n1.  On the basis of existing AdLIF spiking neurons, a set of parameters are introduced to better describe the adaptive process of neurons, and achieve a steady-state behavior during it\u2019s spiking process by limiting the boundaries of some trainable parameters.\n2.  Introduce the synaptic propagation delay in the network, which enhances the SNN\u2019s ability to process temporal information.\n\nThe SNN incorporating these two features exhibit richer dynamic in the temporal dimension comparing to common LIF node. And through some analysis and experiments on three language datasets. This work verified the superiority of this novel SNN. The performance shows the promise of SNN research on tasks with rich temporal dynamics, and in particular research on biologically inspired extensions to existing SNN models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* A detailed introduction is given to the neural dynamics and operating mechanism of the SNN proposed in this paper.\n\nThe article provides detailed mathematical formulas to describe the operation process of the pulse neural network, with effective explanations for all parameters.\n\n* The inspiration sources and effects of the new features introduce in the proposed SNN were analyzed and explained through effective experiments.\n\nThe article explains the sources of each new feature introduced, and provides paper analysis and specific experiments to demonstrate the benefits of the new features provided. At the same time, the effectiveness of the model was verified through specific experiments on three speech recognition datasets, and the comparison method cited is indeed relatively new."
            },
            "weaknesses": {
                "value": "* There are some slight discomfort in writing.\n\nIn section 3.2.1 and 4.1, there are sentences like \u201cSimilarly to earlier studies / other works\u201d, but these \u201cearlier studies\u201d are not closely followed by the relevant references (though they may be papers that have already been cited earlier). And in the experimental results shown in the Table. 1, It seems that the 96.26 in row <Synaptic delay AdLIF+> is mistakenly written as \u201996, 26\u2019.\n\n* There is a slight lack of completeness in the experimental results\n\nThe AdLIF+ neuron in this paper is modified based on the AdLIF neuron after a certain degree of parameter constraints. Would it be better to supplement the experimental results of \u201cSynaptic delay AdLIF\u201d in the first experiment on SHD (as shown in Table 1)? What\u2019s more, I find that the accuracy of the proposed SNN on SHD in Table 2 is consistent with the so-called \u201cmaximal accuracy\u201d in Table 1. Does this mean that the shown experimental results conducted on SHD are all the highest accuracy among 10 runs? Adopting such experimental results seems a bit imprecise if so (of course, the average accuracy in Table 1 is already high enough. The main concern is the clarity of the results)."
            },
            "questions": {
                "value": "1. Please refer to the above weakness.\n2. What about the performance of the proposed models when applied into the larger dataset?\n3. Is there any advantage on the training speed for the proposed model compared with other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760725313,
        "cdate": 1698760725313,
        "tmdate": 1699635966533,
        "mdate": 1699635966533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HdueRgjmvk",
        "forum": "lVZ7Tafw51",
        "replyto": "lVZ7Tafw51",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission397/Reviewer_fvm6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission397/Reviewer_fvm6"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel Spiking Neural Network (SNN) model, the AdLIF+, in which synaptic weights, adaptation parameters and delays are co-optimized. This heterogeneity leads to improved performance leading to the SNN outperforming an ANN on Neuromorphic datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is clear and easy to read\n- Heterogeneity is an important research direction in SNNs and this paper demonstrates that on multiple datasets"
            },
            "weaknesses": {
                "value": "- There have been previous works that have optimized over neuronal parameters to introduce heterogeneity, thus I feel the novelty of the work is quite limited.\n- Datasets are too simple and the advantages of the proposed approach might not be representative"
            },
            "questions": {
                "value": "- Could you try scaling the size of the model and complexity of the dataset\n- what happens if you optimize the adaptation and delays separately like it is done in https://openreview.net/pdf?id=bp-LJ4y_XC\n- In section 4.4, when you discuss the increase in number of trainable parameters, I think it would be better to discuss it as a function of size: Number of Neurons (N) and Number of weights (W) rather than pure numbers for a single size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783345468,
        "cdate": 1698783345468,
        "tmdate": 1699635966435,
        "mdate": 1699635966435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pTBkEZ7KTI",
        "forum": "lVZ7Tafw51",
        "replyto": "lVZ7Tafw51",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission397/Reviewer_XBEc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission397/Reviewer_XBEc"
        ],
        "content": {
            "summary": {
                "value": "This article has improved the information transmission process of traditional LIF neurons and has incorporated the concept of delay, training it together with the weights. The author investigated its effects in a feedforward network with two hidden layers across three speech datasets, demonstrating its superiority. From a bio-interpretability perspective, this model enriches the heterogeneity and dynamic characteristics of LIF neurons, making it very interesting."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The author's idea of incorporating delay and membrane potential updating into the surrogate gradient training process, enabling the SNN to exhibit more heterogeneity, is intriguing. Especially noteworthy is that there are currently few studies that combine mainstream surrogate gradient methods with biological features like synaptic delay.\n2. Figure 3 is particularly fascinating as it explains how parameters influence the dynamic properties of neurons."
            },
            "weaknesses": {
                "value": "1. As I mentioned in the advantages, integrating these biological characteristics into the surrogate gradient learning process is interesting. However, what motivates this integration? The author seems not to have explained this clearly. As mentioned in the introduction, if the goal is solely to enhance biological interpretability, could we achieve better optimization results by learning more parameters?\n\n2. Eq 1 and 7 are crucial to this paper. The author should use the formulas to analyze the problems with the current model, including insufficient biological interpretability, from a theoretical perspective. Then, introducing these formulas would better help readers understand the author's motivation.\n\n3. Furthermore, I think it is necessary to add more information about how each parameter affects the network. This could be done by experimentally explaining the benefits of adding these parameters and showing that they can be effectively optimized through BPTT. As illustrated in Fig 3, there should be additional information on how different response modes specifically affect model performance.\n\n4. Moreover, the impact of different models and depths on the proposed method should be supplemented. Additionally, conducting experiments on a broader range of datasets would better support the author's claim of comprehensive improvement.\n\n5. Despite the introduction of the intermediary variable w, as shown in Eq 5, the model still cannot overcome the problem of gradient vanishing over time during the optimization process, which ensures its effectiveness in optimizing temporal information.\n\n6. Other issues: It is recommended to change the variable w to another parameter, as it is more commonly used to represent weights."
            },
            "questions": {
                "value": "1. Some issues are as indicated in the weakness section.\n2. While the author has enhanced the performance of SNNs by adding several parameters, the increase in the number of parameters is substantial. Has there been any consideration for a comparison with RNNs or GRUs or LSTM under the same parameter quantity conditions?\n3. Furthermore, after adding so many parameters, how does the loss curve of model optimization change? Is there a possibility of collapse? Therefore, has there been any consideration regarding the selection of parameter boundaries?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission397/Reviewer_XBEc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824118019,
        "cdate": 1698824118019,
        "tmdate": 1699635966367,
        "mdate": 1699635966367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dlTAW4Az8p",
        "forum": "lVZ7Tafw51",
        "replyto": "lVZ7Tafw51",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission397/Reviewer_2Gfg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission397/Reviewer_2Gfg"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a new learning algorithm for the SNNs in which both synaptic weights and delays are co-optimized in collaboration with the neuronal adaptation parameters. Various experiments are conducted to verify its performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1\u3001 Compared to the existing works, this work shows certain advantages in accuracy."
            },
            "weaknesses": {
                "value": "1\u3001The novelty of this work may be limited as there have been numerous studies that have already explored the plasticity of synaptic delays.\n\n2\u3001This work has not been compared to mainstream learning algorithms for Spiking Neural Networks (SNN) using popular datasets such as TET, DVS-CIFAR10, and DVS-Gesture, among others.\n\n3\u3001The figures presented in this manuscript are somewhat rough in terms of their quality and level of detail"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894247203,
        "cdate": 1698894247203,
        "tmdate": 1699635966288,
        "mdate": 1699635966288,
        "license": "CC BY 4.0",
        "version": 2
    }
]