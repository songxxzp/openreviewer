[
    {
        "id": "SYQKPyDhZC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ZxVa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ZxVa"
        ],
        "forum": "uZfjFyPAvn",
        "replyto": "uZfjFyPAvn",
        "content": {
            "summary": {
                "value": "This work studies the task of Implicit Neural Representations using neural networks with wavelet activation functions. Paper provides a theoretical framework for the approximation capabilities of such networks, and demonstrates how the theory can be helpful in designing the INR architectures. Experiments are provided on a few images."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The theory presented in the paper is a solid contribution for research and practice.\n\nPaper is well written. Arguments are clear."
            },
            "weaknesses": {
                "value": "In my view, the weakness of this paper is its small set of experiments. I also think more details can be provided to interpret the experiments and to discuss the theory.\n\nI think providing more details and more discussions would make the paper more approachable for a broader audience.\n\n---------\n\nPossible typo in the abstract: band-pass -> high pass. In my understanding, the method decouples the low-pass parts from the high-pass...\n\n----------\n\nI would have liked to see experiments on more images. I only saw three examples. Specifically, I'd be interested to see more examples of how the method works on more images, its errors, and its possible failure modes.\n\nIt may be useful if authors present cases where one might encounter troubles in training -- and cases where the learned representation may be flawed. The first image in Figure 6 seems to be more of a challenge from the training convergence perspective. Are there images where the reduction of the training loss would be even more challenging?\n\nIn the right column of Figure 6, the curves seem to still have a positive slope even towards the end of the horizontal axis. Is that correct? If yes, how would the curve proceed if training is continued further?\n\nAuthors can consider presenting the likes of figure 6 for earlier stages of training, e.g., for 5, 10, 50 epochs. How would those images look like? The convergence curve (right column in Figure 6) seems to be overly compact, so it is not easy to see the particulars for the early epochs of training. It seems that for the parrot picture, the WMM initialization lags behind the random initialization at the early epochs.\n\nIn Figure 6, the WMM result for the parrot has error at the left corners of the image. It may be useful if authors interpret those errors. Specifically, the patterns at the top left corner seem to appear in that general area of the image. Why does that magnitude of error only happen at that top left corner and not in that whole green area?\n\nOverall, demonstrating results on more images would give a better understanding to a reader.\n\n---------\n\nI think the discussion in the context of wavelet literature could have been broader. For example, I did not see any discussions on the topic of Daubechies wavelets. Are Daubechies wavelets also progressive by the authors\u2019 definition?\n\n------\n\nIf authors think Shearlets may have any potential here, providing a discussion might be useful. Most of the approximation error for the pictures in Figure 6 appear to be at locations where the colors change in a small neighborhood. Could a shear matrix be potentially helpful in reducing the error because of its ability to extract anisotropic features?\n\n------\n\nA relevant prior work that authors may consider citing if they see fit:\n\n-- Grattarola, D. and Vandergheynst, P., 2022. Generalised implicit neural representations. Advances in Neural Information Processing Systems, 35, pp.30446-30458.\n\n-------\n\nUsing wavelets to study approximation properties of neural networks, and leveraging that to design the architecture of the network, is studied in the past, but I did not see a mention of that in the paper.\n\n-- Shaham, U., Cloninger, A. and Coifman, R.R., 2018. Provable approximation properties for deep neural networks. Applied and Computational Harmonic Analysis, 44(3), pp.537-557."
            },
            "questions": {
                "value": "How can we interpret the result for the medical image in the bottom row of figure 6? To me, it seems that the error is more for the WMM case. Is that correct? If that is the case, the description in Appendix 5 may not be as accurate when it says \u201cWMM based initialization has limited advantages \u2026\u201d. This would be a disadvantage for WMM and not an advantage?\n\nIntuitively, how does the definition of progressive wavelets affect the approximation capability of a model? What would happen if we use a non-progressive wavelet to model the same images that authors present in Figures 1, 6, 7? We will lose the guarantees from the theorems, but how would that affect the learned representations? How would that empirically affect the error.\n\nIn Figure 6, how many parameters do the models have?\n\nHow could the theory and the method be used for image compression?\n\nPlease also see questions under weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Reviewer_ZxVa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697190506845,
        "cdate": 1697190506845,
        "tmdate": 1699636961123,
        "mdate": 1699636961123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9g5xwUxPSD",
        "forum": "uZfjFyPAvn",
        "replyto": "uZfjFyPAvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_rBV3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_rBV3"
        ],
        "content": {
            "summary": {
                "value": "The article studies implicit neural representation models which typically uses sinusoidal activation functions and wavelet-based activation functions, in order to represent signals. This work develops theoretical understanding of such architectures and also a practical split architecture to capture singularities in target functions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The theoretical analysis seems to be novel. By using Fourier analysis, theorem 1 gives an ide a about the effective time-frequency support of implicit neural representations. Numerical results such as in Fig 4 further support the theory."
            },
            "weaknesses": {
                "value": "The overall clarity of the presentation still needs to be improved to make results accurate. See questions below."
            },
            "questions": {
                "value": "-\tYour definition of the model in eq 1 seems to have some issue to me. Psi is a function of R^d -> C but somehow W^0 r and b^0 is in R^{F_1 x d}. Thus I do not understand the model. \n-\tThe notation inf Theorem 1 is not clear. What is the definition of the Fourier transform for functions in C_0^inf (U) in eq 2? What does this stand for C_0^inf (U)? What is the product_t from t=1 to t=F_1? Does hat beta_l depends on W^{l} and b^{l}? Is * a convolution, over which domain? As a consequence, the argument in Section 3.2 regarding the support of the product_t term in eq. 2 is not clear. Is W^T a typo of W_l^T? \n-\tIn Section 4.3, what does it mean to model a signal as a sum of a linear scaling INR and a nonlinear INR ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Reviewer_rBV3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659052005,
        "cdate": 1698659052005,
        "tmdate": 1700590850920,
        "mdate": 1700590850920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sl6YcARtWa",
        "forum": "uZfjFyPAvn",
        "replyto": "uZfjFyPAvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ewcV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ewcV"
        ],
        "content": {
            "summary": {
                "value": "The study conducts a time-frequency analysis of INRs by leveraging polynomial approximations to delve into the behaviors of MLPs beyond the first layer. By decomposing a signal into its low-pass and high-pass segments using two INRs, motivated from the scaling and wavelet functions of the wavelet transform. This approach bridges the structure of complex wavelets and the application of INRs, which is a novel concept."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a novel perspective on behaviors of INR models, including the decomposition of low and band-pass approximations, along with specific initialization methods, and enhances both the depth and practical relevance of the study."
            },
            "weaknesses": {
                "value": "Although the performance of the proposed method was supported by several tests and analysis in the paper, it is suggested to include some practical applications such as regression tasks on images or other high-dimensional signals to justify its practicability."
            },
            "questions": {
                "value": "The reviewer is curious if the proposed method and its theoretic analysis/intuition is valid, when the method is applied to practical applications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726666995,
        "cdate": 1698726666995,
        "tmdate": 1699636960873,
        "mdate": 1699636960873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gcB0IOoWEx",
        "forum": "uZfjFyPAvn",
        "replyto": "uZfjFyPAvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_3UQT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_3UQT"
        ],
        "content": {
            "summary": {
                "value": "The authors use Fourier analysis to study the properties and expressive power of implicit neural representations (INR). In particular, they analyze multi-layer INRs whose first-layer activations (which they call the template function) map from Euclidean space into the complex numbers and whose activations on all other layers are polynomials. \n\nThe authors show that when the template function is essentially a band-pass filter, the class of INRs they study also act essentially as band-pass filters. Based on this observation, the authors propose to model signals as a sum of two INRs instead, one that acts as a low-pass filter and one that acts as a high-pass filter.\n\nThe authors also suggest initializing INRs with wavelet template functions placed on estimate singular (i.e., non-smooth) points in the domain of the signal (e.g., edges in an image). They show that this careful initialization results in significantly improved performance compared to random initialization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Analyzing the expressivity of multi-layer INRs, not just one-hidden-layer ones, is a very relevant problem; hence, any progress in the area is nice. I found the low-pass-high-pass decomposition idea and the suggestion to initialize the template function's biases to the signal's singular points interesting."
            },
            "weaknesses": {
                "value": "As someone whose primary area of research or background is not in classical signal processing, I found the paper quite confusing. In particular, the authors provide little to no interpretation of their results and observations. This led me to feel that I was constantly expected to be able to interpret and understand them, which I failed to do in many cases. Moreover, it is unclear what mathematical level the authors expect of the reader. They state some quite elementary results rigorously while handwaving others (e.g., effective support, Minkowski sums). Finally, several technical signal processing terms (such as atoms, band-pass filters, and WIRE) are undefined, which makes reading the paper quite challenging.\n\nFor example, the authors showcase Thm 1 as one of their main results. However, Eq (2) is just the Fourier transform of a slightly rearranged definition of the INR architecture they are studying. On the other hand, some of the assumptions and both conclusions are unclear:\n- Why is restricting our focus to polynomial activations beyond the first layer interesting to study?\n- Why is the Fourier transform stated by considering multiplication by a smooth function $\\phi$?\n- How does the fact that the INR is composed of the sum of the template function's \"integer harmonics\" illustrate the expressivity of the INR?\n- \"Second, the support of these scaled and shifted atoms is preserved so that the output at a given coordinate $r$ is dependent only upon the atoms in the first layer whose support contains $r$.\" - How is this not a trivial statement? What is the relevance of this observation?\n\nSimilarly, what is the purpose of section 3.2? While I follow and agree with the argument, the whole discussion is informal, so it technically contains no actual results. More importantly, though, it's unclear what the insight is.\n\nHowever, Section 4 is perhaps the most confusing part of the paper. In the first paragraph, the authors state that in this section, they \"consider the advantages of using complex wavelets, or more precisely progressive wavelets.\" This sentence has two issues: first, it shows that the paper title is a misnomer because the authors actually only consider the algebra of progressive wavelets. Second, they don't demonstrate any advantages of using progressive wavelets. Perhaps most confusingly, even though the concept is featured in the paper title, the authors never actually make use of the fact that progressive wavelets form an algebra.\n\nEssentially, the authors spend the first two subsections defining a multivariate notion of a band-pass filter using weakly conic sets. Then, in the third section, the authors claim in Section 4.3: \"Based on this property of INRs preserving the band-pass properties of progressive template functions, it is well-motivated to approximate functions using a sum of two INRs: one to handle the low-pass components using a scaling function, and the other to handle the high-pass components using a wavelet.\" I do not follow this argument. I'm not saying it is not a good idea, but I do not see why it is well-motivated based on the multivariate notion of band-pass filters to decompose signals into a sum of a high-pass INR and a low-pass INR.\n\nFinally, the experimental section of the work is severely lacking. The authors only conducted one ablation study for their suggested initialization technique vs. random initialization by reconstructing three different images. Hence, it is unclear how the suggested signal decomposition into two INRs or the initialization technique helps with the usual tasks INRs solve in practice compared to other methods."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7842/Reviewer_3UQT"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699015562288,
        "cdate": 1699015562288,
        "tmdate": 1700578378438,
        "mdate": 1700578378438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2OlTgsCftV",
        "forum": "uZfjFyPAvn",
        "replyto": "uZfjFyPAvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ksCd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7842/Reviewer_ksCd"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a type of Implicit Neural Representation based on a wavelet nonlinearity.\nAn implicit neural representation is a parameterization of a (usually) scalar function of a (low-dimensional variable), e.g. a function f: R^d->R using a neural network. In this setting, an image is a function R^2->R and the input to the function are pixel locations. This setup requires some reflection about the role of in particular the first non-linearity. Using ReLU will lead to lots of ramps, requiring their combination in intricate ways to represent usual real-world signals.\nHere the use of wavelets as first non-linearity is analyzed. The use of wavelets comes out of a line of research leading via sinusoidal components (e.g. SIREN) and presents itself as a clear follow-up. The setting studied here is a first layer with wavelet nonlinearity followed by several layers of pointwise mixing layers with polynomial nonlinearities.\n\nIn this context, using the Fourier convolutional theorem, the functions expressible by this architecture are characterized in Fourier space as essentially a collection of higher-order self-convolutions of affine-transformed versions of the base wavelets.\n\nIntroducing the notion of \"progressive wavelet\" which have their Fourier support on a cone (or a \"weak cone\" that is closed for factors >= 1), it is shown that the wavelet self-convolutions above never leave their cone, leading to a neat characterization of what functions can be matched using a particular wavelet.\n\nTo accommodate low-pass signal, a split of the INR is proposed into the sum of two INRs, one using a wavelet, the other using a low-pass filter. This decomposition is shown to be beneficial in fitting a signal.\n\nFurther, wavelet modulus maxima points are proposed to initialize the representation. It is shown that for certain signals the use of WMM points as initialization leads to a better fit."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is very clearly written and presents its contributions is a highly succinct way. It is a pleasure to read - in fact it reads a bit like an advanced chapter of a wavelet applications textbook. The illustrations contribute to the ease of understanding.\n\nThe paper gives a concise characterization of the function space spanned by the wavelet nonlinearity followed by layers of pointwise polynomial mixing. From this characterization it clearly identifies, using progressive wavelets, that a split into low-pass and wavelets is highly useful.\n\nIn short, it leads to a complete understanding of the particular setting introduced. The hope is that this understanding can extend to adjacent settings."
            },
            "weaknesses": {
                "value": "The provable statements in the paper are not in any way non-obvious. Theorem 1 is a direct consequence of the Fourier convolution theorem.\n\nThe setting in which these proofs work are highly impoverished with respect to the setting of actual interest, which is that of non-polynomial nonlinearities, such as the ReLU, for the pointwise mixing layers. The shifting of frequencies along a cone does not hold for these, and complicated ringing processes emerge that can also define sharp boundaries by the rectifier suppressing negative values. (The effects of ReLU wrt wavelets are partially analyzed e.g. here https://arxiv.org/abs/1810.12136 but it looks intractable to integrate this in the current analysis).\n\nGiven the nature of the expressed function space, what is an obvious advantage of this particular implicit neural representation over a sparse continuous wavelet transform with a sufficiently expressive bank of filters (e.g. some base filters and some of their polynomial powers)? In particular, if one needs to use wavelet modulus maxima to initialize the representation. Could these advantages be concisely stated? (e.g. I can imagine that potentially the representation requires fewer parameters)\n\nIn general, could some numerical comparisons be done to place the analyzed method within context of other INRs? We are not looking for state of the art here, but to have an idea of whether the proposed setup is close or far from that. If it is close, this can also partially alleviate the expressivity concern mentioned above."
            },
            "questions": {
                "value": "The term \"progressive wavelet\" was new to me. However, as far as I can tell, its definition fits exactly what I know as \"analytic wavelets\". Could the authors confirm this is the same and in that case justify the use of a new term, or explain in what way these concepts differ?\n\nThere are two more direct questions listed in the weaknesses section.\n\nRegarding the low-pass + progressive wavelet decomposition for images: Is it possible to show the smooth parts and the wavelet parts? A conjecture for this, if both INRs are allowed to output in RGB space, is that the smooth parts contain much more color than the wavelet parts. Would be interesting to see if that is the case for certain natural images."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238008318,
        "cdate": 1699238008318,
        "tmdate": 1699636960619,
        "mdate": 1699636960619,
        "license": "CC BY 4.0",
        "version": 2
    }
]