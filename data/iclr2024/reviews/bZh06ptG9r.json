[
    {
        "id": "ZCrjAe6kfT",
        "forum": "bZh06ptG9r",
        "replyto": "bZh06ptG9r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_yWWG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_yWWG"
        ],
        "content": {
            "summary": {
                "value": "This paper provide a personalized federated learning FedLoRA. The main idea behind this approach is to decompose each client's personalized model into two components: a full-rank matrix and a low-rank matrix capturing client-specific knowledge. It is a useful method in specific scenario."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "FedLoRA emphasizes personalizing models for each client, which helps in catering to the unique data distribution of each client. \nFedLoRA efficiently decomposes model parameters into full-rank (general) and low-rank (client-specific) matrices. And it uses an alternating training strategy to train low-rank and full-rank parameters."
            },
            "weaknesses": {
                "value": "Even though the low-rank parameters remain private, sharing the full-rank parameters with the server might raise privacy concerns for some applications.\nCompared to LoRA, the complexity and memory cost might be high."
            },
            "questions": {
                "value": "Could the author provide details on the memory cost of this method compared to the general LoRA method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Reviewer_yWWG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816560988,
        "cdate": 1698816560988,
        "tmdate": 1699637139064,
        "mdate": 1699637139064,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sOWavZqo8m",
        "forum": "bZh06ptG9r",
        "replyto": "bZh06ptG9r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_fKpJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_fKpJ"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses data heterogeneity within Personalized Federated Learning (PFL) by leveraging low-rank approximation. Specifically, each client retains a low-rank matrix locally to encapsulate personalized information derived from private data. In contrast, a full-rank matrix is updated and aggregated to capture more generalized information. The effectiveness of this approach is demonstrated through extensive experimentation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Utilizing low-rank approximation for storing personalized information is an innovative approach to managing data heterogeneity in PFL.\n\n2.The paper introduces an alternative training strategy to improve the overall aggregated performance, which could be a meaningful contribution to the field.\n\n3.Different prototypes for the decomposition of CNN and dense layers are presented, broadening the scope of the proposed method.\n\n4.The extensive experimental results show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.The paper could benefit from a more profound exposition of the underlying intuition of the proposed method. Offering analytical comparisons or insights on why this method outperforms state-of-the-art approaches could enrich the narrative.\n\n2.The concern of overfitting associated with the training of the full-rank matrix needs to be adequately addressed. This aspect could affect the generalization capability of the aggregated parameters.\n\n3.The method's scalability seems constrained by model size. As model size increases, transferring the full-rank matrix to the server becomes impractical."
            },
            "questions": {
                "value": "Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698998449919,
        "cdate": 1698998449919,
        "tmdate": 1699637138951,
        "mdate": 1699637138951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GyxGyk3u4V",
        "forum": "bZh06ptG9r",
        "replyto": "bZh06ptG9r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_8LH2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_8LH2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces FedLoRA that uses LoRA for personalized federated learning (PFL). The main idea is that we decompose the full-rank model $\\theta$ to a full-rank shared parameters $\\sigma$ and low-rank personalized parameters $\\tau$ as $\\theta = \\sigma + \\tau$. On each communication round, the clients would first freeze $\\sigma$ and optimize $\\tau$, then freeze $\\tau$ and optimize $\\sigma$ (alternative optimization). Then clients would send $\\sigma$ to the server to average $\\sigma$ on the server side. The server would broadcast the averaged $\\bar{\\sigma}$ back to each client. \n\nThe experiments are ResNet on CIFAR10, CIFAR-100, and TinyImageNet. The authors consider the Dirichlet non-IID client data distribution with $\\alpha \\in \\{0.1, 0.5, 1\\}$ and demonstrate FedLoRA's superiority over SOTA PFL methods. The authors also ablate on (1) the rank of convolution or linear layers for $\\tau$ (2) training epochs for $\\sigma$ and $\\tau$ per communication round (3) alternative optimization vs. simultaneous optimization (4) performance boost from introducing $\\tau$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The motivation behind using LoRA for simultaneously learning client local knowledge and mitigate the data heterogenity issue for FL as we use the additional full-rank weights for averaging is quite good. \n\nMost of the ablation studies are well-formulated and well-executed. \n\nMost of the paper is clearly written and easy to follow."
            },
            "weaknesses": {
                "value": "I cannot find any major weakness of this paper, but there is a claim without an support from strong evidence:\n- In the paragraph of 'Effect of $R_l$ and $R_c$' of section 4.3, the authors claim that the decrease of model accuracy w.r.t. the rank of $\\tau$ after the focal point is because $\\tau$ acquires some of the general knowledge. To support this argument, we should compute the vector similarity of $\\tau$ across each client and we should see an increase after the focal point."
            },
            "questions": {
                "value": "Is it possible to extend the experiments to use a ImageNet-pretrained ResNet to finetune on CIFAR10/CIFAR100? I assume in this paper we need to learn $\\sigma$ because we are training from scratch and we still need to learn the feature extractor. I expect that if the feature extractor is already trained (as a ImageNet-pretrained model), we should expect that $\\tau$ is learned more often than $\\sigma$. ($\\Delta \\sigma$ should be much smaller than $\\Delta \\tau$). This study could be a strong support on the client-specific knowledge statement. \n\nThis is an overall good paper and I will vote for accept."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Reviewer_8LH2",
                    "ICLR.cc/2024/Conference/Submission9045/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699429403701,
        "cdate": 1699429403701,
        "tmdate": 1699640254098,
        "mdate": 1699640254098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "05K2kb1yIT",
        "forum": "bZh06ptG9r",
        "replyto": "bZh06ptG9r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_eXdF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_eXdF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FedLoRA which decomposes shared and personalized parameters like LoRA in fine-tung LLMs, and employ a new training strategy to optimize it in non-IID settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper innovatively introduced LoRA as a personalized model and a new training strategy to mitigate the data heterogeneity problem for general knowledge learning.\n\nThis paper is written in a well-organized way for easy understanding and following."
            },
            "weaknesses": {
                "value": "The paper lacks experiments and analysis about training and communication cost, since the new training strategy needs to train two times for full-rank matrix and low-rank matrix, and the cost for transmitting full-rank matrixes may be huge because they are much larger than low-rank matrix.\n\nThis paper does not include the evaluation results of each client which may lead to sacrificing the performance of some clients for improvement. \n\nThe convergence analysis is missing in this paper, and there are no experiments and analysis about the proposed new training stage to support the intuition in Fig 3."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699446724752,
        "cdate": 1699446724752,
        "tmdate": 1699637138724,
        "mdate": 1699637138724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QhjcVakx0N",
        "forum": "bZh06ptG9r",
        "replyto": "bZh06ptG9r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_PiFj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9045/Reviewer_PiFj"
        ],
        "content": {
            "summary": {
                "value": "This paper applies LoRA to personalized federated learning and proposes an alternative training method for local clients. Experiments on three image datasets show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Personalized federated learning, although extensively studied, is interesting.\n\n2. The proposed method seems to achieve good results reported by this paper.\n\n3. This paper is easy to read."
            },
            "weaknesses": {
                "value": "1. I cannot see too much novelty in applying LoRA to federated learning. It is not clear why LoRA is a better choice for personalized FL than existing methods. Moreover, several existing works have explored this, for example, FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning, which also has the name of FedLoRA.\n\n2. The proposed alternative training seems heuristic and lacks theoretical justification.\n\n3. Experiments are conducted on three small image datasets. More large datasets, especially in different modalities,  should be used.\n\n4. The paper writing quality should be improved. There are many issues, like \"Federated learning (FL) McMahan et al. (2016) allows clients to collaboratively train a global model\""
            },
            "questions": {
                "value": "Why not try larger datasets, and datasets beyond image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9045/Reviewer_PiFj"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699598764873,
        "cdate": 1699598764873,
        "tmdate": 1699637138595,
        "mdate": 1699637138595,
        "license": "CC BY 4.0",
        "version": 2
    }
]