[
    {
        "id": "dFtPGV2BQj",
        "forum": "J44HfH4JCg",
        "replyto": "J44HfH4JCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_VAPt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_VAPt"
        ],
        "content": {
            "summary": {
                "value": "This work shows how the current Large Multimodal Models exhibit significant hallucinations in particular when presented with negative instructions. In this light, it introduces a new finetuning dataset, lRV-Instruction, of 400k visual instructions to fix this issue. This dataset covers 16 vision-language tasks and was generated using GPT-4. In particular, the authors use intermediate models/information as the visual inputs of GPT-4. \n\nThey also introduced a new evaluation benchmark, GAVIE, as a flexible approach to evaluate accuracy (i.e., hallucination in this case) and relevance (i.e., instruction following performance), without human intervention.  This dataset consists of 1000 humanly goldified pairs from the lRV-Instruction dataset. However, if the dataset is humanly goldified, it still reloes on a parametric model, namely GPT-4, to compute the scores."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors conducted a thorough analysis demonstrating the benefits of the lRV-Instruction dataset. For example, they show how finetuning on this dataset reduces models' hallucination (using in particular GAVIE) but also interestingly in three different independent tasks (MME, POPE, GQA). I find this result interesting and important. Also, they analyze better the benefit of the new dataset by looking at different aspects in their datasets (negative vs positive and tasks)\n\nFurthermore, I find the generation process of lRV-Instruction interestingly simple as it leverages good existing LLMs and other visual models."
            },
            "weaknesses": {
                "value": "The authors conducted different experiments to show the feasibility of replacing the human annotator with GPT-4, both for constructing the dataset and for evaluation. If the dataset could be noisy (the authors do mention that the dataset is imperfect), the evaluation should be trusted and reproducible. I think the authors should outline further this limitation. Also, another useful experiment is to report the correlation of GPT-4 rating with human rating. My understanding is that the authors already have this dataset with expert raters but on a different scale. So such a correlation should be easy to report."
            },
            "questions": {
                "value": "1. For GAVIE the scores are 0-10. What is the intuition for such a big scale? My sense is that the task could be summarised to a 0/1 score. I like the analysis of the \u201cIs GPT4-Assisted Evaluation Stable?\u201d. Why not changing the benchmark to a (0-4) scores then?\n\nI am also curious to know if the human experts used all the (0-4) scores provided. If only binary, you may just use a binary score too.\n\n\n2. I don't understand the last sentence of this part\n\"Alternatively, (Li et al., 2023c; Fu et al., 2023) formulate the evaluation of hallucination as a binary classification task that prompts LMM to output \"Yes\" or \"No\". However, it is hard to evaluate the LMM output in an open-ended manner\"\n\nWhy is the evaluation still challenging with a YES/NO question. This becomes just a classification task that does not need any human or parametric intervention.\n\nMisc:\n* Some bad formatting of citation and formulation in section \u201cVISUAL INSTRUCTION TUNING\"\n* 3 incorrect examples in Figure 2 (neg). For example the question is about\"Merkel\" but the answer is about \"Hilary Clinton\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698433414248,
        "cdate": 1698433414248,
        "tmdate": 1699637137538,
        "mdate": 1699637137538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bOXKhkmGnD",
        "forum": "J44HfH4JCg",
        "replyto": "J44HfH4JCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_WNxt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_WNxt"
        ],
        "content": {
            "summary": {
                "value": "Hallucination presents a substantial challenge in contemporary large multi-modal models (LMMs). To tackle this issue of hallucination, this paper introduces the LRV-Instruction dataset, encompassing 400k samples of instruction-following scenarios spanning 16 different vision-language tasks. The dataset is enriched with a substantial amount of negative instruction data, generated through three distinct heuristics: Manipulation of Nonexistent Objects, Manipulation of Existent Objects, and Manipulation of Knowledge within instructions. Additionally, the paper proposes GAVIE, an innovative methodology for the automatic evaluation of LMMs. GAVIE assesses models based on two critical dimensions: accuracy and relevance. Empirical results from the experiments reveal that applying instruction tuning to MiniGPT4 and mplug-owl significantly enhances their performance, surpassing their original instruction-tuning results on numerous established public datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper presents LRV-Instruction, a substantial and varied benchmark tailored for visual instructions. It covers 16 vision-and-language tasks and includes both positive and negative instructions, making it more robust and comprehensive compared to existing datasets. \n\nThe proposed GAVIE allows for the assessment of LMMs' hallucination without the need for human-annotated groundtruth answers. This method substantially increases the efficiency and scalability of assessing adjustments made to visual instructions, with human validation lending further support.\n\nThe empirical investigation underscores the utility of LRV-Instruction in diminishing hallucination and augmenting the performance of models. The finetuning results achieving state-of-the-art performance on both the evaluation set and public benchmarks, add credibility to the proposed approach."
            },
            "weaknesses": {
                "value": "Even though the LRV-Instruction is generated automatically via GPT-4, its dependence on the VG dataset's high-quality annotations, in my opinion, may pose a constraint on its ability to scale. Also I think more analyses on more scenarios of LLMs should be included into discussions."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640229314,
        "cdate": 1698640229314,
        "tmdate": 1699637137373,
        "mdate": 1699637137373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6iE7XGBLT4",
        "forum": "J44HfH4JCg",
        "replyto": "J44HfH4JCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_LKUy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_LKUy"
        ],
        "content": {
            "summary": {
                "value": "- Vision and Language multimodal LLM are known to be prone to hallucination, especially for \"negative (non-existence)\" references. \n- One possible reason is the existing instruction fine-tuning basically relies on massive \"positive\" instructions. \n- The manuscript first introduces a new dataset that contains a variety of \"negative\" instruction cases. \n- At the same time, the manuscript proposes the use of GPT-4 for automatic evaluations of open-ended questions. \n- Experimental results indicate the efficacy of the new dataset for mitigating the multi-modal LLM hallucinations\n\nAfter author feedback, score is upgraded."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- For the reviewer, this is the first proposal of the \"balanced\" instruction fine-tuning dataset for multi-modal LLM. Motivations for such dataset are reasonable. \n- (Class-) balances of the training corpus are fatal for successful model training. In that sense, it is reasonable that the proposed dataset can mitigate the hallucinations of the LLMs. \n- it is good to know that (strong) LLM can help the open-ended questions in V-L: checking the images and the answers are time-consuming."
            },
            "weaknesses": {
                "value": "- As cited in the manuscript, (Liu 2023d) proposed one of the first GPT-based automatic evaluations without human intervention. The manuscript does not explain the main differences from (Liu 2023d), except the application to vision - language domain. \n- GAVIE relies on the black-box GPT-4 engine. It means the evaluation is affected by the system update of the GPT-4, implying we cannot assure the loyal reproducing of GAVIE results in months/years after. Are there any remedies for this problem?"
            },
            "questions": {
                "value": "- Concerning the 2nd point of the \"weakness\", I wonder what happens to GAVIE if we replace the engine from GPT-4 to (possibly weaker) multi-modal LLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Reviewer_LKUy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734039830,
        "cdate": 1698734039830,
        "tmdate": 1700726065655,
        "mdate": 1700726065655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tjRPNXSOPI",
        "forum": "J44HfH4JCg",
        "replyto": "J44HfH4JCg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_68kD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9027/Reviewer_68kD"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Large-scale Robust Visual (LRV)-Instruction dataset with both positive and negative visual instructions. The authors use GPT4-Assisted Visual Instruction Evaluation (GAVIE) as the evaluation tool to evaluate hallucinations. The authors finetuned the popular baselines on the proposed LRV-instruction datasets and improved the hallucinations as well as the other commonly used benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation is good, especially for creating negative instructions to balance the datasets. The experiments are also very sufficient and compact, which is really appreciated by me. The paper writing is also good."
            },
            "weaknesses": {
                "value": "- Based on the claim \"As observed by (Li et al., 2023c), current LMMs tend to answer \"Yes\" for any instructions presented to the model, even when the proper answer should be \"No\". Our investigation reveals that most LMMs are finetuned on unbalanced datasets containing only positive instructions \" in Instruction, the author assumed current LLMs tend to answer \"Yes\" issues caused by unbalanced\". This statement got confused because, so far LLaVA1.5 boosts the model without balancing positive and negative instructions. Also, before LLaVA1.5, even in LLaVA1.3 or 1.1 the model seems to not have this \"always yes\" issue anymore. It will be really helpful if the author can provide more experiments to prove these statements.\n- It's not fair to compare models not trained on VG data with your own evaluation set created from VG. Please add more comparisons with models trained on VG data. Also, it would be great if the author could evaluate the current minigpt4-v2, LLaVA1.5. If the author thinks these two works are too recent, the author can also try Shikra, which also includes VG in the training set. \n- One of the evaluations is \"Relevancy: whether the response directly follows the instruction. \" This actually evaluates the instruction following ability, which is kind of tricky, cause it's hard to say if the training data difference brings instruction domain difference, which makes the evaluation not fair.\n- Human label with 4 scores. It would be great to give some explanation on what kind of aspect humans evaluate and preferred means it will be better if the author could provide a more specific evaluation policy and rules."
            },
            "questions": {
                "value": "How about the performance of generating complex reasoning and detailed captions after training the LVLM on LRV-instruction datasets? Although Llava data may have hallucinations, complex reasoning and detailed captions are still good, does the LRV dataset maintain the capabilities? Can you show us some examples of complex reasoning and detail captions?\n\nFor the other questions please refer to the weaknesses. Basically, the contribution of the dataset is good but we should be careful, I would like to hear from the authors about the concerns I bring up."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9027/Reviewer_68kD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698992797941,
        "cdate": 1698992797941,
        "tmdate": 1700723017771,
        "mdate": 1700723017771,
        "license": "CC BY 4.0",
        "version": 2
    }
]