[
    {
        "id": "qNkod3gfyL",
        "forum": "fLWqIWPMDH",
        "replyto": "fLWqIWPMDH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_fSnX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_fSnX"
        ],
        "content": {
            "summary": {
                "value": "This paper generalizes the cost-aware Bayesian optimization (BO) algorithm from single-stage to multi-stage optimization problem. To achieve this, a new Expected-Expected Improvement Per Unit-cost (EEIPU) acquisition function is proposed, and a memoization-awareness trick is considered for improving the cost efficiency. Empirical results on both synthetic and real experiments show that the proposed EEIPU outperforms conventional EI and cost-aware BO baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The problem in this paper is well motivated and the writing style is good. The proposed algorithm and main idea of this work are easy to follow.\n\n- The experimental design is valid. Both synthetic functions and real-world AI pipelines are used to test the proposed algorithm."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method is limited. The proposed method is a combination of cost-aware BO, unknown-cost modeling, cost cooling, and the memoization trick. All these techniques are not new and have been used in existing BO works (as shown in Table 1). The technical issues or challenges of combining them are not clearly shown in this work. The proposed EEIPU simply replaced the cost function $c(x)$ of EIPU with a total cost function $C(x)$ which is defined as the sum of $c(x)$ over multiple stages. Although the author(s) claimed that computing the expected total inverse cost $\\mathbb{E}[1/C(x)]$ is not a straightforward task, the issue is resolved by conventional MC sampling which does not contribute to a non-trivial solution.\n\n- The important baselines are not fairly compared or discussed. In Table 1, why Multi-stage BO is labeled as not cost-aware or memoization-aware? The \"stock and resume\" scheme of Multi-stage BO is a very similar concept to the \"memoization\" here and the cost has been considered in Algorithm 2 of (Kusakawa et al., 2021). As shown in Table 1, LaMBO and Multi-stage BO should be the most related works which, unfortunately, are not compared in the experiments. The difficulties of reproducing LaMBO are discussed in Section 3.4. However, it's not clear why Multi-stage BO is not tested. Also, even if both algorithms are hard to reproduce, a detailed discussion about the novelty of EEIPU compared to these two baselines is needed for showing the significance of this work. In particular, since the known and fixed cost are claimed to be the major issues of Multi-stage BO and LaMBO (Section 2), is there any difficulties in generalizing their algorithms to the unknown-cost setting? Compared to the memoization strategies shown in these two works, what is the superiority of the proposed memoization method? Are there any scenarios that can be tackled by EEIPU instead of Multi-stage BO or LaMBO?"
            },
            "questions": {
                "value": "- At the end of Section 2, it is claimed that \"In our setting, the cost at a given stage is a function of the hyperparameters provided to that stage, as well as the inputs provided from the previous stage\". Can you provide more technical details or examples to support this sentence? What are the \"inputs from the previous stage\"? Why and how are these inputs used to model the cost function? Is it shown in any part of Section 3?  \n\n- What's the value of $\\epsilon$ used in the experiments? Is the experimental results sensitive to the settings of $\\epsilon$, $M$, or $N$?\n\n- In Fig. 3&4, why does EI outperform both EIPS and CArBO? It seems to be counter-intuitive and inconsistent with the results reported in existing works. The y-axis labels of the graphs in the middle column of Figs. 3&4 seem to be wrong. Shouldn't it be the function value instead of the stage cost? The right graphs of Fig. 3&4 each have one plot that the cumulative costs of EI, EIPS, and CarBO are almost the same. Can you provide some analyses on this observation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3596/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3596/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3596/Reviewer_fSnX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698291359847,
        "cdate": 1698291359847,
        "tmdate": 1699636315062,
        "mdate": 1699636315062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d95AF1tc6B",
        "forum": "fLWqIWPMDH",
        "replyto": "fLWqIWPMDH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_uCj5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_uCj5"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Expected-Expected Improvement Per Unit Cost (EEIPU), a cost-aware BO algorithm able to handle multi-stage pipelines with independent, unknown costs. The acquisition is computed via Monte Carlo sampling to estimate the expected inverse cost which are modelled using GPs on log costs. The multi-stage pipeline is exploited via memoization of previously observed candidates and intermediate outputs. When selecting new candidates, some new candidates reuse the memoized results to avoid incurring costs computing the memoized results again. EEIPU is empirically evaluated with comparison against suitable baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is generally written clearly and is easy to understand.\n2. EEIPU empirically outperforms previous algorithms when the assumption of a multi-stage pipeline holds.\n3. The experiments section is well-designed, the segmentation and stacking pipelines are realistic."
            },
            "weaknesses": {
                "value": "1. The assumption of a multi-stage BO process with independent stages and observable intermediates is a very strong one that can be exploited more than this work currently does. This work only exploits this structure to memoize previously evaluated candidates to save costs expended during the BO process. This structure has been exploited to improve the modelling and guide the search, for example, see \"Bayesian Optimization of Composite Functions\" (Astudillo and Frazier, 2019) and \"Bayesian Optimization of Function Networks\" (Astudillo and Frazier, 2021).\n\n2. Perhaps the $M$ prefixes can be sampled more intelligently than randomly, e.g., by weighting the prefixes based on the results of previous evaluations incorporating those prefixes.\n\n3. The memoization method fails if the intermediate stage outputs are noisy. While AI pipelines could be assumed to have noiseless stages, it is not inconceivable that these stages are noisy, e.g., if the stages are generative models.\n\n4. Some clarifications, see Questions section, in particular Question 2 about the experimental results.\n\n5. Typos/inconsistencies: 1) Written as $EI \\times \\mathbb I^\\eta$ in Algorithm 1, but $EI * \\mathbb I$ and $EI * \\mathbb I^\\eta$ in Equations (5) and (6); 2) in Figures 3 and 4, left and center column plots are supposed to be best objective value, but the y-axis is written as $f(x^*)$ in the left plots and 'stage costs' in the center plots; 3) [()] described as 'empty prefix' and 'empty subset' in different parts of Sec. 3.2."
            },
            "questions": {
                "value": "1. What is the rationale of introducing the $\\epsilon$-cost for stages 1 to $\\delta$? Why not set to $0$?\n\n2. There are a few peculiarities with the results for EIPS and CArBO in Figures 3 and 4. In Figure 3 bottom row right plot and Figure 4 top row right plot, EIPS and CArBO consume the exact same cost in each iteration as the non-cost aware EI. This is very strange since EIPS and CArBO are supposed to be cost-aware. In addition, in all the results shown, the non-cost aware EI outperforms EIPS and CArBO in terms of best objective value achieved against cost incurred, which is again strange given that EIPS and CArBO were designed for this very setting. Could you investigate and explain these anomalies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656991546,
        "cdate": 1698656991546,
        "tmdate": 1699636314987,
        "mdate": 1699636314987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u1pqu5lncz",
        "forum": "fLWqIWPMDH",
        "replyto": "fLWqIWPMDH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_gsaT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_gsaT"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the optimization of hyperparameters for a multi-stage AI pipelines with unknown costs.  By utilizing Bayesian optimization, it solves a black-box optimization problem on the AI pipelines.  Since each stage depends on the previous stages and its computational cost varies, it needs to separately model final function evaluations and costs.  Notably, it extends the expected improvement acquisition function to the expected-expected improvement per unit cost, by calculating the expected inverse costs.  Eventually, the authors show some experimental results on several benchmarks with multiple stages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* It solves an interesting problem related to multi-stage AI pipelines, defined by considering practical scenarios.\n* Proposed method is well-motivated.\n* Paper is generally well-written."
            },
            "weaknesses": {
                "value": "* More compelling experiments can be conducted.  I think that the experiments tested in this paper seem interesting, but the scale of experiments is small compared to the common scale of the experiments in ICLR.\n* I don't fully agree with the need of memoization.  Are the processes memoized really computationally expensive?  I think that the numerical analysis can be provided in order to strengthen the motivation of memoization awareness."
            },
            "questions": {
                "value": "* The reference by Mockus (1998) might be published in 1978, not 1998.\n* Why should costs be positive?  Is it a mandatory condition?\n* In Figure 2, why are $c_1$ and $c_2$ zero in (c) and (d)? Are they correct?  If they are correct, please add description in the rebuttal.\n* Why are the best objective values at iteration 0 are different across methods?  Initialization should be identical across tested methods, such that the initial values should be same.\n* The captions of tables should be located above the tables.\n* I think that the authors need to update Figure 1.  Fonts for mathematical expressions are different from ones in the main article."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705638313,
        "cdate": 1698705638313,
        "tmdate": 1699636314907,
        "mdate": 1699636314907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HiSgXCM37S",
        "forum": "fLWqIWPMDH",
        "replyto": "fLWqIWPMDH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_W9CP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3596/Reviewer_W9CP"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for cost-aware multi-stage BO that is also memoization-aware, meaning that the method stores the outputs of intermediate stages so as to reuse later. This allows for the proposed method to perform more function evaluation compared to other methods without memoization-awareness given the same budget.\n\nThe paper proposes a new acquisition function for this purpose, namely Expected-Expected Improvement Per Unit-cost (EEIPU). The EEIPU consists of 3 components:\n- Cost awareness: the method assumes the cost is unknown, so they will build k GP models for k stages to represent the cost, then compute the expected cost by random sampling with Monter Carlo simulation.\n- Memoization awareness: the proposed method stores the cost and output for each previous k-1 stages. Then, depending on the number of stored stages, the cost is discounted (but keep some overhead as a small cost), reducing the overall cost.\n- Cost cooling: due to the nature of EEIPU, the proposed method may prioritize low-cost regions throughout the optimization, the author proposes to apply a cooling process (seems to base on the paper Lee et al., 2020) for the cost computation. The idea is to apply an exponential decay factor \u03b7\u2208[0,1], which gradually decreases with every iteration, so that eventually, the EEIPU turns back into the common EI acquisition function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe idea of memoization is nice, it can be applied to increase the number of function evaluations for more knowledge to feed into BO process given a fixed budget.\n-\tThe method may do well in the scenario that the budget is the running time allowed for optimization process. This has been proven by the experimental results (left columns of Figure 3,4), where given the same budget, EEIPU can obtain better outputs.\n-\tThe paper writing is clear and easy to understand (although there is one thing regarding the problem setting I will mention in the Weaknesses section)."
            },
            "weaknesses": {
                "value": "- As the topic covered in this paper is quite new, it will aid more if the paper includes a problem statement to describe in detail the problem setting. It took me quite some efforts to go back and forth to understand the setup of the problem tackled in this paper.\n-\tPlease correct me if I'm wrong but it seems the application for this memoization technique is limited, as the stored data in the previous stages can only be reused when a repeated input for a certain stage is queried again (this is related to the 2nd question in the Question section).\n-\tThe number of benchmarks is too few: only 2 synthetic and 2 real-world benchmarks, and with low dimensions.\n-\tIn the synthetic benchmarks, it seems the improvement is only because the method manages to evaluate more data. Given the same number of iterations (middle column of Figure 3), even methods without cost-awareness like EI can find similar (or even some better) results. So this seems to me that the memoization only helps with increasing the evaluated data, but not really help much with the modelling of the surrogate model or the BO process.\n-\tThe novelty of the work seems to be a bit limited. The idea of memoization and how it is incorporated in the proposed method seem to be a bit simple, while the effectiveness of the memoization to help with the modelling of the surrogate model or the BO process seems to be not clear. There is no deep insights to justify the proposed techniques. Finally, the cost cooling process seems to just inspire from previous works without modification."
            },
            "questions": {
                "value": "Besides answering my comments in the Weaknesses section, the authors could answer my following questions:\n- What are the specifications of GPs using for the cost modelling? Which priors, kernels and hyper-parameter settings are used?\n- For synthetic benchmarks, what is the input space for each stage? Is it discrete? If it is continuous, I'm just wondering how exactly does EEIPU re-use the stored data? It is hard for the acquisition function to propose the exact same points in the continuous domain. From the right column of Figure 3, it seems that the amount of reusing data is approximately 40 times over a total of 80 function evaluations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3596/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834219766,
        "cdate": 1698834219766,
        "tmdate": 1699636314822,
        "mdate": 1699636314822,
        "license": "CC BY 4.0",
        "version": 2
    }
]