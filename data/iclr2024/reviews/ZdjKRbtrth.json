[
    {
        "id": "w3hMlSxcqL",
        "forum": "ZdjKRbtrth",
        "replyto": "ZdjKRbtrth",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_KiwB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_KiwB"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a constrained decoding strategy to limit the generative space of large language models, effectively transforming them into retrievers without requiring additional training. This retrieval process is two-fold: initially, the large model generates titles corresponding to relevant documents based on input questions. Subsequently, the model generates paragraphs within these documents that contain the sought-after answers. To alleviate the computational burden associated with generating full paragraphs, the authors introduce a technique where the language model produces prefixes to quickly locate relevant paragraphs. This approach substantially reduces the number of tokens generated by the LLM, thereby achieving computational efficiency. The final output of LLM is generated  based on the comprehension of the identified content obtained above. The authors validate their method through experiments on six KILT benchmarks tailored for knowledge-sensitive tasks. The proposed method demonstrates superior performance across the majority of these benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This study offers a novel approach to transforming large language models into search retrievers by ingeniously leveraging constrained decoding. This technique guides the models to generate document titles or content within predefined boundaries, achieving retrieval capabilities without necessitating extra training, specialized retrieval models, or text chunking processes.\n- Addressing the second-stage bottleneck where the Language Model's (LLM) generation of paragraph text is time-consuming, the paper introduces the Short Prefix Generation and Location (SPGL) method. This significantly improves the retrieval speed, reducing it from 600 minutes to 150 minutes.\n- The paper is exceptionally well-articulated and clear in its presentation."
            },
            "weaknesses": {
                "value": "- Despite the authors' efforts to enhance computational efficiency through the use of shorter prefixes and the Knuth-Morris-Pratt (KMP) algorithm for passage location, the proposed LLM2GR model still necessitates at least three forward computations.  This computational burden remains a significant hindrance to its practical applicability.\n- The comparison between LLM2GR and existing methods such as Dense Retriever (DPR), BM25, and Contriever raises questions of fairness. Specifically, LLM2GR employs a considerably more powerful LLM for generative tasks, which inherently skews the comparison with baselines of lesser model capacity. Using an LLM for retrieval is not inherently novel. A more equitable evaluation might involve integrating LLMs with existing retrieval methods\u2014such as DPR\u2014to compare generative capabilities. For instance, one could employ DPR for retrieval and then use an LLM for the generative task based on the retrieved results, thus providing a fairer baseline for comparison. If the proposed two-staged method can still deliver better results, thus demonstrating the effectiveness of the proposed method.\n- Writing Errors: There are typographical errors in the manuscript, such as in Section 2.3 where the phrase \"generate related contexts for \"retrieval,\"\" is incorrectly punctuated. Additionally, the usage of quotation marks throughout the paper is inconsistent.\n- A significant limitation of the proposed method is its dependence on the universal knowledge gained during the pretraining stage. This raises questions about the method's applicability in scenarios that go beyond the scope of Wikipedia or in specialized vertical fields such as medicine or economics. The paper does not address how the method could be adapted or if it remains effective when applied to these more specialized domains. This limitation is a critical factor that constrains my enthusiasm for strongly recommending the paper for acceptance.\n- While the paper convincingly demonstrates that Large Language Models (LLMs) can be used for retrieval, it lacks an in-depth discussion on why LLMs can perform this task effectively. Is it because the models have been exposed to the documents during the pre-training phase?"
            },
            "questions": {
                "value": "- In section 3.3\uff0cgiven a short text prefix $p_s$ of $l_{p_s}$ tokens, the KMP algorithm is used to determine the start position $st$ of $p_s$, then a complete passage is extracted by catching starting from $st$ with the length of $l_p$ tokens. Here a question arises that if the text prefix $p_s$ is equal to the final retrieved passage $p_final = d[st : st + l_p]$?\n- Regarding the slow generation speed of the LLM, the authors should explore techniques like 8-bit or 4-bit quantization to assess whether such methods can accelerate the model's performance to an acceptable level, while also evaluating the trade-off in retrieval accuracy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Reviewer_KiwB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729941054,
        "cdate": 1698729941054,
        "tmdate": 1699636272808,
        "mdate": 1699636272808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KMakgp661R",
        "forum": "ZdjKRbtrth",
        "replyto": "ZdjKRbtrth",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_M43s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_M43s"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new generative retrieval method called LLM2GR, which prompts LLama (7b,13b) to generate title identifiers, then further filter out the paragraphs from the retrieved documents by prompting the LM again to generate short prefixes using constrained decoding and find documents that correspond prefix. They evaluate proposed methods on the KILT benchmark and compare the retrieval performance with BM25, Contriever, and DPR baselines. \n\nI have several major concerns about this paper, which are detailed in Weaknesses. In summary, I think\n- The technical novelty of this paper is still unclear\n- They should have evaluated their proposed methods' effectiveness with other generative retrieval methods e.g., generating URLs or generative paragraphs in terms of performance and efficiency. \n- The main contributions or advantages (efficiency, not limited by URLs) of the methods are not fully supported by the experimental results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper introduces a new generative retrieval method that does not rely on additional reranking components \n- Instead of genereating full paragraphs, the proposed method only generate short prefix and search documents, which can improve inference time efficiency."
            },
            "weaknesses": {
                "value": "The major three concerns I have are as follows:\n1. Limited technical contributions and novelties \n2. Missing important baselines\n3. Unsupported claims \n\n**1. Limited technical contributions and novelties**\n\nGenerative retrieval methods have been actively studied, and generating full evidence paragraphs (Yu et al., 2023), URLs (Ziems et al., 2023), Wikipedia titles (De Cao et al., 2020), or substrings (Bevilacqua et al., 2022) have been explored. Bevilacqua et al. (2022) generate substrings, which are similar to short prefix generations, and use the FM-Index to constrain the autoregressive generations. It is not discussed in depth how this work is different from Bevilacqua et al. (2022). One difference might be prompting an LM  rather than fine-tuning. Yet, if the novelty is mostly prompting a larger LM rather than fine-tuning a smaller LM, I think it is not a sufficient contribution for ICLR. \n\n**2. Missing important baselines**\n\nDespite that the proposed method is a new generative retrieval method, none of the baselines is generative retrieval. Several prior works including Bevilacqua et al. (2022) or Yu et al. (2023) are tested on KILT or open-domain QA datasets and are open-source, and I wonder why the authors did not directly compare their method with such prior work. \n\n\n**3. Unsupported claims**\n\nSeveral advantages mentioned in the introduction or the related work section have not been validated by experimental results. For instance,\n\n- *The process of generating complete passages can be overly time-consuming, posing a significant drawback to their practical implementation. To address this issue, we propose a novel approach termed Short Prefix Generation and Location (SPGL).*  \n\nIf the authors claim this method is more efficient for practical application by generating less (but searching over FM Index & locating prefixes via KMP), the authors should benchmark inference time latency. Given the complex pipeline of the proposed method and active research of fast inference (e.g., speculative decoding, paged attentions), I am unsure whether this method is indeed practical by skipping longer generations or taking more time due to the expensive search process. Related to the previous point discussed above, the authors should benchmark other generative retrieval methods and quantitatively show the proposed method's effectiveness. \n\n- *Recently, Ziems et al. (2023) have proposed using the large language model GPT-3 to retrieve documents by generating document URLs. However, this method can only be applied to documents retrievable by URLs*\n\nThe proposed method is also limited to the pre-processed Wikipedia corpus as it matches the generated sequences with all titles in DB, which means that the proposed method's search space is also limited by existing Wikipedia titles. For me the difference between the form of URLs v.s. titles does not matter if you only test on Wikipedia, as in Wikipedia converting title into URLs can be achieved by simple text postprocessing most of the time. \n\nYu, Wenhao, et al. \"Generate rather than retrieve: Large language models are strong context generators.\" ICLR (2023)."
            },
            "questions": {
                "value": "- Did you evaluate other generative retrieval model performance as well as inference latency? \n- Di you compare the inference time latency of non-genreative retrieval methods and your method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3242/Reviewer_M43s"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736362545,
        "cdate": 1698736362545,
        "tmdate": 1700683615442,
        "mdate": 1700683615442,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8slnEC97nS",
        "forum": "ZdjKRbtrth",
        "replyto": "ZdjKRbtrth",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_i5R3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_i5R3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LLM2GR, a two-stage generative retrieval based on LLM under the zero-shot setting, which first generates title identifiers to obtain top k documents, and then generates directly the content of passages whose length is around 150 to 200 tokens, among a set of passages that belong to the top k documents retrieved at the first stage. Two scores resulting from both stages are linearly interpolated to finally rank passages. Furthermore, this paper proposes a short prefix generation and location (SPGL) to efficiently locate and extract a relatively long passage. In SPGL, instead of generating a full content of a passage, its prefix is first generated and then the remaining content of the passage is just located based on KMP string matching algorithm. Under SPGL, the score of the second stage is now replaced with the score of generating only prefix (not the score of the full content). Experiment results using LLMs of llama 7b~13b show that the proposed two-stage generative retrieval often outperforms the fully-finetuned DPR method in TriviaQA, HotpotQA, FEVER, and WoW datasets, demonstrating its promisingness. Ablation studies show that the first stage is vital and the prefix-based score is better than the score using the full content."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes the initial work of LLM for generative retrieval under zero-shot setting. While generative retrieval has been popularly studied, the work presents the new exploration of LLM focusing on the zero-shot setting, showing that the zero-shot generative retrieval under LLM is promising, although the proposed method is technically simple. \n- Experiment results are largely interesting, confirming that the zero-shot generative retrieval using LLM shows improved performances over the fully-finetuned DPR, while the reranking method is not compared. Ablation study includes the comparison between the major variants. \n- The short prefix generation and location (SPGL) is further proposed, as an effective way to address the computational overhead of large model to generate long documents. While SPGL is also simple, it is reasonably designed and makes the further improvement."
            },
            "weaknesses": {
                "value": "- It seems that the proposed two-stage method is technically not very attractive. Under LLM, a simpler design like generating title identifier and prefix in a single stage would be preferred. The kind of simpler method needs to be considered and adopted as the baseline for the comparison.\n- It seems that the baselines are weak. In the fully-finetuned case, not only DPR but also the reranking method needs to be provided, as the LLM generates the passages based on a stronger attention-based interaction between a question and a document, than the DRP that uses the inner product between dense vectors. \n- The retrieval performance was only compared with previous dense and sparse retrieval methods, lack of comparison with recent generative retrieval methods.\n- In the proposed method, only zero-shot settings are presented. Few-shot settings of LLM need to also be provided. \nUnder few-shot prompts, other LLM-based method such as query2doc could be compared. \nhttps://arxiv.org/pdf/2303.07678.pdf\n- Other popular datasets are not considered for evaluation, such as MS MARCO, TREC DL 19, and TREC DL 20. To convincingly validate the proposed zero-shot retrieval of LLM, the work needs to provide a full comparison on more IR collections. \n- The construction of FM-index after the first stage seems to be done per a query. Its computational overhead is not fully discussed. Why FM-index is not constructed over the full documents, towards a manner of restricting a full FM-index to the only restricted top-k documents. \n-  The proposed SPGL approach first generates short prefix of the document and then matches the prefix with document fragments using KMP algorithm. It is much faster than directly generating the whole document using LLM, however, whether this approach has a speed advantage over other generative retrieval works was not discussed. Many generative retrieval works also do not directly generate the documents, such as in SEAL [1], where n-grams of documents are generated first and then using a scoring function to determine the documents. The reviewers would like to know if SPGL is faster compared to other generative retrieval works.\n- In section 3.3, the selection method of the unique documents (from document set) and prefix (from multiple prefixes) is na\u00efve. Although in the vast majority of cases the \u201ctop k document\u201d was unique in this dataset, the method of \u201cdefault choice\u201d does not have any ranking or selection capabilities in the case of extending this work to larger corpus or practical applications."
            },
            "questions": {
                "value": "- The proposed two-stage method needs to be compared with a single-stage method that generates a title identifier and prefix of a passage. Why a single-stage method is not considered? \n- What is the computational time of constructing the FM-index? Is it not possible to construct the FM-index over the full passages, and to use it in a restricted way on only top-k documents?\n- In Table 1-2, only DPR is compared. What is the best performance of the existing works on the same collections? The reranking-based method needs to also be compared. \n- In section 3.2, it was claimed that the \u201cinformation loss\u201d issue of text chunking for lengthy documents was overcame by the proposed method, however, this work actually generate a short prefix of the document, which is also less informative than the whole document or document chunk. The reviewer would like to see the authors' opinion on this issue.\n- What is the memory cost of using prefix tree and FM-index structures? Does this work require significant use of additional memory to store these indexes?\n-  The length of the short prefix l_(p_s ) was not explicitly given, the reviewer would like to know how the authors decide the length of short prefixes and how does the length of the short prefix contribute to the retrieval performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858357386,
        "cdate": 1698858357386,
        "tmdate": 1699636272638,
        "mdate": 1699636272638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IROiIMhGwg",
        "forum": "ZdjKRbtrth",
        "replyto": "ZdjKRbtrth",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_7CYr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3242/Reviewer_7CYr"
        ],
        "content": {
            "summary": {
                "value": "This paper is about generative retrieval. The key claim of the authors is to simulate the process of humans finding relevant information. To this end, the authors have developed a framework that tries to mimic the search process adopted by humans. Here the key model is the language model. From Figure 1, the model developed comprises of two stages:\n1. In stage 1, the model finds relevant documents given a query\n2. The model then finds a reference passage in the relevant document.\n\nThe authors then expand upon their idea in Figure 2 where they depict the complete model.\n\nThe authors then conduct a series of experiments to demonstrate the effectiveness of their approach.\n\nJust a minor comment to the authors: the title of the paper is very general. This paper is about passage retrieval. It would have been nice had the authors reflected it in their paper title. Generative information retrieval has now become a broad field and continues to grow."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "One of the key strengths is that the paper focuses on both the effectiveness and efficiency of the task. In terms of effectiveness, the model, from the experimental results demonstrates that it improves upon the existing method. The paper also discusses efficiency issues with large language modelling approaches, especially in retrieval settings where it is important to retrieve the relevant documents as efficiently as possible.\n\nPassage retrieval has been long studied in the information retrieval literature. The goal is to retrieve relevant passages rather than documents so that a user can find their relevant information as quickly as possible. While there are traditional approaches such as the BM25 used in the past, this paper develops a generative learning method that has become popular recently using the transformer architecture.\n\nIn the two-stage method, the model mainly uses the Wikipedia document collection to help retrieve relevant information for the user. The quantitative results demonstrate that the model improves upon existing methods."
            },
            "weaknesses": {
                "value": "The issue with this work is that in the two-stage approach, the model will propagate errors from one stage to the next. While the unified approach would be highly complicated, the authors must realise that this is the shortcoming of this work.\n\nIt must be also noted by the authors that they use only Wikipedia articles that might help improve the efficiency of the model. The medium and large-sized models usually use a large amount of document collections which surely helps encode plenty of information which might not be present in Wikipedia, this does not mean that other models are relatively less efficient than this model. What would have been nice had the authors discussed the pros and cons of using large datasets that go beyond Wikipedia?"
            },
            "questions": {
                "value": "It would be nice if the authors addressed the concerns that I have raised in the weakness section of my comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860817282,
        "cdate": 1698860817282,
        "tmdate": 1699636272529,
        "mdate": 1699636272529,
        "license": "CC BY 4.0",
        "version": 2
    }
]