[
    {
        "id": "z2g2EqTxtD",
        "forum": "MNwXif6AWA",
        "replyto": "MNwXif6AWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Pointwise Distance Distribution (PDD), a continuous isometry invariant for periodic point sets for the representation learning of crystals. It develops a transformer model, Periodic Set Transformer (PST), with a modified attention mechanism that integrates composition information and structural encoding for accurate crystal property prediction. By defining the crystal in terms of a periodic set, the representation of crystals encodes the periodicity of crystals and becomes continuous under perturbations, bridging the gap between crystal descriptors and machine learning models. As a result, the transformer model PST equipped with modified self-attention and PDD-weighted readout has the potential to make accurate predictions for lattice energies. Furthermore, the authors extend PST for crystal property predictions, outperforming graph-based or transformer-based models on some tasks, given the extensive experimental results on Matbench. The evidence from ablation studies further proves the effectiveness of the combination of compositional and structural embeddings for a better understanding of the chemical space of crystals."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The paper proposes PDD for the representation of periodic lattice and overcomes the discontinuity of traditional graph representations. Therefore, the paper uniquely contributes to the field by exploring the reasonable representations for periodic sets which can help machine learning models fully learn the geometry of the space.\n\nQuality: The paper carefully explains the core concepts like PDD with detailed derivation. Besides, the extensive experimental results and visualizations provide convincing evidence to support the statement in the paper.\n\nClarity: The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. \n\nSignificance: The paper focuses on improving the embeddings for crystals so that the transformer model equipped with the adapted self-attention mechanism could be leveraged for crystal property predictions. The experimental results in the manuscript show the potential of PST model to outperform the widely used graph-based models for crystal property predictions. The model could be further improved by pertaining, making it a promising candidate in crystal property prediction and crystal structure optimization."
            },
            "weaknesses": {
                "value": "1. Although the authors' presentation is quite clear in general, the details of the experiments provided in the paper are not enough, especially why the experiments are designed in this way, what are the datasets and targets, and what the difference across experiments is and how they collaborate to support the statements in the manuscript.\n\n2. The description in the section Prediction of Lattice Energy is quite vague. For example, the authors do not specify what the datasets (e.g. T2, P1, S2) are, how they are obtained, and what kinds of data entries are included in them. Otherwise, it's hard to figure out why the experimental results here are significant. The author might consider revising this section so that the logic is more transparent to readers."
            },
            "questions": {
                "value": "1. In terms of the explanation of isometry on page 2, I'm wondering why the isometry has the form $f(S)=Q$ and $g(Q)=S$. From my understanding, isometry means $d_S(a,b) = d_Q(f(a),f(b)), a,b \\in Q$, and I can't tell that this is equivalent to the explanation in the manuscript. \n\n2. Earth Mover's Distance is mentioned in the Introduction part, but I do not see detailed descriptions about how to use it for crystal representation in the manuscript. Besides, have the authors considered comparing with ElMD [1], which has also introduced Earth Mover\u2019s Distance as metrics for chemical similarity and inorganic compound embeddings?\n\n3. In the second experiment of prediction of lattice energy, if I'm understanding it correctly, the datasets consist of crystals with different compositions while the compositional information is not included. Then how does the model make predictions for two similar lattices with different compositions? And even if the model can outperform the baseline on this task, I'm afraid it cannot demonstrate that the model is applicable to practical usage.\n\n4. Why is the PST model evaluated on the training set for the first two tasks of lattice energy prediction? And what is the reason for supplementing P2M data to training data to reduce error? From the results in Table 1 & 2, I cannot be persuaded of the effectiveness of PST.\n\n5. Could you clarify how the contribution is calculated in the ablation study? And I think the errors here are sufficient to demonstrate the impact of compositions and PDD.\n\n[1] Hargreaves, C. J., et. al., The earth mover\u2019s distance as a metric for the space of inorganic compositions. Chemistry of Materials, 32(24), 10610-10620, 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6701/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6701/Reviewer_yW4J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697993053798,
        "cdate": 1697993053798,
        "tmdate": 1699636769433,
        "mdate": 1699636769433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ERgfUktNDC",
        "forum": "MNwXif6AWA",
        "replyto": "MNwXif6AWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_DofE"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new representation for machine learning on crystal structures based on Point Distance Distribution (PDD), which the paper claims is both continuous and isometric. The proposed representation augments augments PDD with composition in order to be able to represent a crystal in a unique manner such that machine learning models can be applied to it. The paper also proposes a modified self-attention mechanism that can utilize the PDD and compositional information to predict a variety of materials properties.\n\nThe paper starts by introducing crystal structures and their associated challenges of predicting their properties that traditional computational chemistry methods that are often computationally prohibitive in evaluating properties for many materials. Next, the paper describes the challenge of finding good representations for crystal structures that are isometric and machine learning friendly and defines a set of properties that a good representation should have including invariance, completeness, and continuity. Following a description of related work, the paper discusses the PDD and their proposed periodic set transformer including detailed mathematical definitions. Next the paper describes the PDD encoding that incorporates atom composition information and how it is incorporated in the periodic set transformer. \n\nFollowing the definition of the method, the paper provides two case studies: one for lattice energy prediction and one for materials property prediction based on Materials Project. In the lattice energy prediction study, the paper investigates the effects of different methods with PDD generally showing better performance. In the case of materials property predictions for Materials Project, the results are more mixed with other methods outperforming PST in some, but not all, cases. The paper then provides an ablation study mostly focusing on the input  representation for Materials Project property prediction followed by the conclusion summarizing the work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n* The paper provides a new representation for machine learning on crystal structures that has very useful properties, including isometry and continuity. The representation itself could be promising for the development of other machine learning methods (originality, significance).\n* The paper provides a new attention mechanism tailored to the PDD representation, which is then applied to different case studies with some results indicating the utility of the representation and the architecture (originality)."
            },
            "weaknesses": {
                "value": "While the paper introduces an interesting and relevant idea, the current form includes some major weaknesses:\n* The description of the PDD representation and the architecture is often unclear and confusing (clarity).\n* The contribution appears limited to the inclusion of the composition on top of the PDD representation, which appears to be prior work (significance, originality). \n* The experiments performed are relatively small in scale with the results often not well presented (clarity, quality). \n* The experiments are in Section 4 are not well described making it difficult to assess their significance (clarity, quality). Given that only PDD representations were used, it is unclear what contributions of the paper are being highlighted here. Also the model architectures used are unclear. My best guess is that it involves Gaussian processes similar to the AMD case.\n* Many of the figures and tables are only sparsely labeled making it difficult to fully understand the takeaways. (clarity)\n* The notation in Section is hard to follow given that there are letters in upper and lower case with different bold fonts each corresponding to different entities. This can be improved for greater clarity."
            },
            "questions": {
                "value": "* What are the model architectures used Section 4?\n* Can you describe in more details how the rows of the PDD representation are collapsed into each, specifically how identical rows are identified?\n* How are the rows of the PDD representation ordered? Does this ordering matter?\n* How do atoms get counted in the PDD construction described in Section 3.1? Since composition is not present yet, are the atoms indexed without atom types?\n* Is there a predetermined way to choose k for the PDD? Based on the information in the appendix it appears to be a hyperparameter that seems significant. It would be good to more details on this.\n* What types of crystals are studied in Section 4? You mention both molecules and crystals here, so are these molecular crystals? Is there a reason you claim that only the lattice matters for these structures? Clarity could be substantially improved by providing more detailed information about the task.\n* In Section 5 - is there a reason that CrabNet cannot use PDD embeddings? I would assume that GNNs use a different representation in your study, which would also be good to clarify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534291975,
        "cdate": 1698534291975,
        "tmdate": 1699636769319,
        "mdate": 1699636769319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SwpVL0L5im",
        "forum": "MNwXif6AWA",
        "replyto": "MNwXif6AWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_V7SM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_V7SM"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a transformer model with a modified self-attention mechanism that adapts PDD (Pointwise Distance Distribution, represented by a k-nearest-neighbor distance matrix), and incorporates compositional information via a spatial encoding method. Specifically, the authors consider the PDD as a set of grouped atoms and use an attention mechanism to find interactions between members of the set. The authors claim that PDD effectively distinguishes periodic point sets up to isometry but doesn't consider the composition of the underlying material, and thus, the newly proposed encoding method can effectively capture this information."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of the Periodic Set Transformer (PST) model is articulated in a straightforward manner. The authors have designed the PST model to incorporate not just structural but also compositional information through Pointwise Distance Distribution (PDD) Encoding. This makes the model versatile and potentially more effective in predicting material properties."
            },
            "weaknesses": {
                "value": "Majors:\n\n1. **Inadequate Experimental Results**: The paper's experimental section reveals suboptimal performance in predicting key electronic properties of crystals, such as formation energy. Notably, the proposed method performs poorly in comparison to other methods listed in the results table. Furthermore, the paper lacks a comparison with the state-of-the-art method coGN [1] at Matbench, which is a significant oversight.\n2. **Lack of Novelty in k-Nearest-Neighbor Construction**: The paper does not sufficiently differentiate its approach from k-nearest-neighbor graph construction of message-passing methods. Common methods for material prediction, such as  CGCNN [3] and ALIGNN [4], also consider both atomic properties and distances, raising questions about what exactly is the authors\u2019 method beyond those message-passing methods with k-nearest-neighbor graph construction. \n\nMinor:\n1. **Omission of Citations**: The authors don't include important baselines coGN [1] and PotNet [2].\n\n\nRef:\n\n[1] Ruff, Robin, et al. \"Connectivity Optimized Nested Graph Networks for Crystal Structures.\" *arXiv preprint arXiv:2302.14102* (2023).\n\n[2] Lin, Yuchao, et al. \"Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.\" *ICML 2023*.\n\n[3] Xie, Tian, and Jeffrey C. Grossman. \"Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties.\" *Physical review letters* 120.14 (2018): 145301.\n\n[4] Choudhary, Kamal, and Brian DeCost. \"Atomistic line graph neural network for improved materials property predictions.\" *npj Computational Materials* 7.1 (2021): 185."
            },
            "questions": {
                "value": "See weeknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793578375,
        "cdate": 1698793578375,
        "tmdate": 1699636769197,
        "mdate": 1699636769197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4FzE83JkoL",
        "forum": "MNwXif6AWA",
        "replyto": "MNwXif6AWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6701/Reviewer_kc8a"
        ],
        "content": {
            "summary": {
                "value": "The Pointwise Distance Distribution (PDD) is a recently developed invariant for periodic crystals that is easy to compute and differentiates between almost all non-isomorphic lattices (generically complete). The authors of the present work propose to use the PDD in a transformer architecture to learn to predict the lattice energy and other material properties.\n\nThe PDD computes for each atom in the unit cell the $k$-nearest neighbour distances and sorts these into a list. Stacking the list for all $n$ atoms gives a $n \\times k$ matrix. The PDD is the distribution over these rows, so a discrete distribution over $[0,\\infty)^k$. This is an invariant, generically complete, and Lipschitz wrt the earthmover distance on the distributions. The PDD can also be represented as a matrix with weighted rows, where similar rows are collapsed, adding up the weight.\n\nThe authors propose to incorporate the PDD data in four ways into a transformer:\n- instead of using the atoms as tokens, it uses the rows of the PDD matrix, so collapsing atoms with similar $k$-NN distances\n- the initial features are the $k$-NN distances, combined with atomic properties (the authors don't collapse different atoms with similar $k$-NN distances)\n- The self-attention is additionally weighted by the PDD weights\n- The transformer output is pooled using the PDD weights\n\nThe authors show in their experiments that using the PDD is superior to using an alternate invariant, and the authors show that their method performs competitively to other material property prediction methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I think it's great to incorporate the powerful PDD invariant into neural networks\n- The authors show strong performance on the material prediction dataset."
            },
            "weaknesses": {
                "value": "- I think an important ablation is missing: just using a typical transformer on the atoms as tokens with the $k$-NN distances and the atomic properties as features. The \"PDD\" ablation study still only uses the PDD in all the four ways I listed in my summary. It'd be great if the authors could ablate these separately. The CGCNN baseline uses the $k$-NN distances as features, but is not a transformer, so is not a substitute to this ablation.\n- A key property of the PDD is its Lipschitz continuity, making it robust to perturbations in the positions. The way the authors use the $k$-NN distances with the hard collapse, then treating the rows as separate tokens, loses this property. Currently, however, the authors are suggesting that the continuity of the PDD is a benefit to their method. The authors should clarify that."
            },
            "questions": {
                "value": "- In their description of the transformer, it appears like each block only uses self attention and normalization. Is there no MLP used in each block, as is typical in a transformer?\n- Could the authors comment on how often the rows of the PDD are collapsed in practice, so how much it matters that the used tokens are aggregates, rather than individual atoms?\n- In Def 3.1, the numbers $c_i$ are said to be integer and contained in $[0, 1)$. This would imply they are zero, which I suppose is not what is intended. Could the authors clarify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699263307078,
        "cdate": 1699263307078,
        "tmdate": 1699636769072,
        "mdate": 1699636769072,
        "license": "CC BY 4.0",
        "version": 2
    }
]