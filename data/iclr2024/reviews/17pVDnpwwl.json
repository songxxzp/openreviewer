[
    {
        "id": "yzU9emh5XG",
        "forum": "17pVDnpwwl",
        "replyto": "17pVDnpwwl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_5jxB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_5jxB"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the scaling limits of residual models in the infinite depth limit $L \\to \\infty$ and their training dynamics. In particular, they study a parametrization where the residual branches are scaled by $1/\\sqrt{L}$ (previously studied at initialization by several prior works). In order to achieve a feature learning limit as both the width and depth tend (sequentially) to infinity, the authors adopt the $\\mu P$ parametrization that guarantees maximal feature learning in the infinite width limit ($N \\to \\infty$). For the proposed model, the authors devise equations describing the training dynamics using the Tensor Programs framework. The authors also introduce the concept of feature diversity, defined as the norm of the difference of the features of nearby layers. They show that among the class of admissible parameterizations in the limit $N,L \\to \\infty$, the proposed parametrization maximizes feature diversity. The authors apply their theory to hyperparameter transfer, showing that the optimal hyperparameters transfer as the model size increases (in terms of depth)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The main strengths of the paper lie in the fact of contributing to multiple research areas, and in striking a combination of important theoretical results and practical consequences. In particular:\n1. The authors devise a parametrization that guarantees feature learning in the infinite width-and-depth limit, nicely extending existing works on feature learning in the mean-field (infinite width) limit. \n2. The authors have extensive theory describing the training dynamics in the limit (Section 4, Appendix).\n3. The concept of feature diversity is novel, and allows the authors to nicely classify the infinite depth parameterizations according to this measure. \n4. The problem of hyperparameter transfer is of utmost importance for practitioners. To my knowledge, this problem was previously addressed when scaling the width to infinity, but not the depth. Hence, this work allows the practitioners to tune the architecture at relatively small widths and depths, and then scale up the architecture with a rough guarantee that the hyperparameters also transfer.  \n5. Experiments, although they do not seem to test more widely used architectures such as convolutions and attention models, test very deep networks, which makes me confident about the validity of most of the proposed claims."
            },
            "weaknesses": {
                "value": "The main weaknesses lie around a few conceptual leap of faiths that in my view are not entirely justified either by theory or experimentally. \n1. **How does maximizing feature diversity result in the \"optimal\" parametrization?**: The authors identify a class of parameterizations that satisfies a number of desiderata (stable, nontrivial, faithful, feature learning). Among these, the authors argue at multiple points in the paper that Depth-$\\mu$P is the one maximizing feature diversity, and hence \"optimal\" (E.g. at the end of the introduction, the beginning of Experiment Section). In which sense is maximizing feature diversity optimal? Suppose it is in terms of convergence to low training loss. In that case, the authors should have included more experiments testing more models in the class of parameterizations that satisfy stability, nontriviality, faithfulness, and feature learning, but are not maximally diversifying features. However, all the baselines are for $\\alpha=0$, which are not in the regime mentioned above ($\\alpha \\geq 1/2$). \n2. **Why does hyperparameter transfer happen?**. While it is understandable that hyperparameters should not transfer under the SP baselines (initialization is unstable under $\\alpha=0$), the authors make little effort to justify (with a heuristic argument or experiments) why hyperparameters should transfer under the proposed feature learning regime. I guess in principle, the constant learning rate $\\eta$ could still shift constantly with increasing model size. Hence even an experiment testing (e.g. showing that other admissible parameterizations do not exhibit transfer) would have made the claims in the paper more supported by evidence. \n3. **Missing citations**. There are a number of works studying feature learning in infinite-width networks. I think some of these works should have been cited. Also, most of the literature on Gaussian process behavior in neural networks is not cited (e.g. see below). \n\nMinor:\n4. \"In Appendix G.4, we provide \u201cheuristic\u201d proofs that can be made rigorous under non-trivial technical conditions\". It is slightly confusing. I thought these claims were the result of the theory developed in Section 5. While I believe it is perfectly fine for a subset of the proofs to be non-rigorous, I am confused as to what are the additional assumptions beyond the ones in the rigorous theory of Section 5. \n\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. \"Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.\"\n\nB Bordelon, C Pehlevan, \"Self-consistent dynamical field theory of kernel evolution in wide neural networks\"\n\nLenaic Chizat and Francis Bach. \"Implicit bias of gradient descent for wide two-layer neural networks\ntrained with the logistic loss\"\n\nLee at al. \"Deep Neural Networks as Gaussian Processes\"\n\nLee et al. \"Wide neural networks of any depth evolve as linear models under gradient descent\""
            },
            "questions": {
                "value": "1. I would like the authors to comment on why they think hyperparameters should transfer in the proposed feature learning limit (as opposed to the NTK limit for instance).  \n2. Training time. How much do the optimal hyperparameters (e.g. learning rate) shift as a function of training time t? It would be nice to comment on this in the main paper, as practitioners might be interested in tuning the architecture with minimal amount of training steps. \n3. Why use the mean-centered version of Resnet? For instance, in https://arxiv.org/abs/2302.00453 the weights are applied after the nonlinearity, thus achieving centering. \n3. Validity of infinite depth limit under the tensor program. It is my understanding of tensor programs that a finite number of tensor variables (i.e. finite number of layers) are required to apply the tensor program framework. How is the infinite depth limit handled? At initialization, the commutativity of the limits was proven in https://arxiv.org/abs/2302.00453. Is the order of the limits still commutative during training? \n\nOverall, I am in favor of acceptance of this paper, although I have some reserves that I hope will be addressed during the rebuttal period."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Reviewer_5jxB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683006592,
        "cdate": 1698683006592,
        "tmdate": 1700740849525,
        "mdate": 1700740849525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t0a5cBvcfm",
        "forum": "17pVDnpwwl",
        "replyto": "17pVDnpwwl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_TGji"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_TGji"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how to transfer optimal hyperparameters for a network with different depths. The authors use the tensor program framework to analyze the covariance of a linear resnet. The authors verified their theory on toy resnets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a solid theory for an important problem. Although from the reviewer's viewpoint, the question is far from being answered this paper makes a solid concrete first-step contribution."
            },
            "weaknesses": {
                "value": "- The resnet archtiecture is too simplifeid. The muP initialization can't work when the residual block is two-layer. How about the theory for linear resnet when the residual block is two-layer?  \n- the tensor program framework can deal with the activation function and normalization layer but this paper dropped the part there. Can even just analyze one step of gd\uff1f (or linear net+noramlization layer)\n- the convergence in the tensor program is just loss function convergence and can't obtain the convergence of optimal hyperparameters. Can the author show that the optimal hyperparameter have faster convergence speed?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817914935,
        "cdate": 1698817914935,
        "tmdate": 1699637048867,
        "mdate": 1699637048867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OSZeFFq2mn",
        "forum": "17pVDnpwwl",
        "replyto": "17pVDnpwwl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_f9Ax"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_f9Ax"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of scaling the depth of neural networks and its impact on model performance especially the norm of the feature will blow up as depth grows. The authors introduce a principled approach called Depth-\u00b5P, which allows for training of arbitrarily deep networks while maximizing feature learning and diversity among layers. They propose dividing the contribution of each residual block and parameter update by the square root of the depth, ensuring more stable training and hyperparameter transferability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces a novel depth scaling strategy called Depth-\u00b5P, which addresses the challenges of increasing the depth of neural networks. This method is designed for a specific network architecture where the feature output of each residual block is divided by a hyperparameter L to control the norm, not blowing up.\n2. The authors establish a theoretical foundation for Depth-\u00b5P by using Tensor Programs to rigorously analyze the behavior of infinitely deep neural networks under the proposed scaling scheme."
            },
            "weaknesses": {
                "value": "1. The motivation described in the introduction `The stacking of many residual blocks causes an obvious issue even at the initialization \u2014 the norm of $x^l$ grows with $l$,...` does not match the real design of the neural network. As there will be a normalization layer after or before the activation layers in common resnet or transformers.\n\n2. The Depth-$\\mu$P algorithm is designed on a modified structure of the residual network, but there is not any performance comparison of this structure and commonly used structure.\nEven though with such a modified structure we can train networks with an arbitrary depth, it does not indicate the performance is better.\n\n3. It is not so easy for me to follow the theoretical analysis. The introduction of the $\\Gamma$ and $C$ functions is not smooth. And why the analysis needs the width of the network to go to infinity is still not clear."
            },
            "questions": {
                "value": "1. It would be better if the author could provide some experimental results on the performance of the modified structure. \nOtherwise, the analysis will be too specific but not related to any real applications.\n\n2. When analyzing the infinite-depth effect, I think it will be better the separate the infinite-width effect. \nHowever the current analysis seems to combine these two cases, and difficult to tell the contribution of the analysis of the infinite depth."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698911301882,
        "cdate": 1698911301882,
        "tmdate": 1699637048752,
        "mdate": 1699637048752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ImIzsogQ3E",
        "forum": "17pVDnpwwl",
        "replyto": "17pVDnpwwl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_Mvpr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_Mvpr"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a desirable scaling of hyper-parameters to achieve feature learning in networks with skip connections, infinite width, and infinite depth. In other words, it is about $\\mu$P for extremely deep networks with skip connections. First, the authors introduce two scaling factors depending on the depth \u2014 $\\alpha$ for the forward propagation and $\\gamma$ for the gradient. These factors are key to enabling feature learning in such deep networks. Second, the authors focus on a deep linear network and obtain an analytical expression for feature learning under the proposed scaling factors. Finally, the effectiveness of these scaling factors is empirically tested through some experiments,  particularly those examining the transferability of learning rates."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-  While some previous work proposed to use $\\alpha=1/2$ for the skip connections,  the scale of gradient updates for such networks has not been investigated. This study addresses this gap by demonstrating that $\\gamma=1/2$ is appropriate for feature learning in networks with infinite depth. \n\n- This study keeps up with the latest research on muP, particularly highlighting Yang & Littwin 2023 for entry-wise adaptive gradients, and further increases the usefulness of muP."
            },
            "weaknesses": {
                "value": "-**Not a few ambiguous points**\n\nAs I have noted in questions (i)-(v), there are numerous unclear aspects in this paper that are critical to understanding its main claim. In particular, the dependencies among theoretical claims are ambiguous, making it uncertain how Main Claim 6.5 is derived, as highlighted in my Questions (i) and (ii).\n\n-**Empirical verification is limited**\n\nFigure 2 is the only evidence that the proposed Depth-$\\mu$P works for non-linear networks because Figures 3 & 4 suppose linear networks and Figures 5-9 are part of Figure 2. However, the activation function used in Figure 2 is not mentioned.  In addition, it is also unclear whether the training is finished (Question (iii)). Thus, it is hard to judge whether the Depth-$\\mu$P works in realistic situations. Because a large part of this work focuses on the linear network, there is a concern that it may only function effectively in models close to the linear network.\n\nOne more concern is that the width is not so large (n=256) compared to the depth.  The theoretical framework of this study considers a scenario where depth << width. However, the experimental conditions seem closer to a proportional limit where depth ~ width. It is surprising that the theory and experiments align despite this discrepancy. It would be advisable to validate this with several types of widths, activation functions and datasets (or tasks)."
            },
            "questions": {
                "value": "(i) Where in the paper do the authors utilize the tensor program formulation to derive the equation $\\alpha+\\gamma=1$? It appears that the dynamics of linear networks discussed in Sections C & D presuppose $\\alpha=\\gamma=1/2$. Furthermore, the general case presented in Section E merely offers a tensor program representation of the training process without shedding light on how we should determine ($\\alpha$, $\\gamma$). \n\nI hypothesize that the assessment of $A_l^2$ in Section 3 leads to the determination of $\\alpha = \\gamma = 1/2$. However, I am unable to find this evaluation in Sections C, D and E.  Could you please elucidate precisely where in these sections the authors derive both $\\alpha + \\gamma = 1$ and $\\alpha = \\gamma = 1/2$, and explain how the tensor program formulation is applied to achieve these conclusions?\n\n(ii) Feature learning is defined as $\\Delta \\boldsymbol{h}_t^{\\lfloor\\lambda L\\rfloor}=\\Theta(1)$ (Definition 6.5). Contrarily, the authors derive the unique parameterization for feature learning (Claim 6.5) from the non-redundant exponent $\\kappa=1/2$. However, the manuscript does not clearly show the connection between definition 6.5 and $\\kappa$. Could you write down explicitly how these two quantities are equivalent to each other? Furthermore, why and in what manner is the case of $\\kappa=1/2$ *unique* for enabling feature learning outlined in Definition 6.5?\n\n(iii) What is the activation function utilized in Figure 2? Additionally, what logarithm base is employed for the log loss depicted on the vertical axis? These may appear as minor details, but I believe they are crucial.  Since the analysis in Section 4 and Figures 3 & 4 focus on the linear network, it seems no guarantee that the Depth-$\\mu$P works for general nonlinear activation functions.  For instance, could you empirically verify whether the results obtained are applicable to Tanh and ReLU activation functions?\n\nRegarding the scale of the vertical axis, the issue lies with the right side of Figure 2.  If the base is $e$,  the model exhibits Loss ~ exp(-3.5) ~ 0.03. This implies that the training is not finished. Consequently, it becomes difficult to judge whether the learning rate transfer is successful in the models that have been completely trained.\n\n(iv) Difference from Jelassi et al. (2023) \n\n> Jelassi et al. (2023) showed that a learning rate scaling of depth\u22123/2 guarantees stability after the initial gradient step\n\nThe distinction between the current work and Jelassi et al. (2023) should be more explicitly stated.  I think that $\\mu$P in the current work is also consistent with the initial gradient step. I mean, the $\\mu$P derived from the initial gradient step would remain consistent across general t steps. Are you suggesting that your Depth-$\\mu$P differs between the initial step and subsequent steps where $t > 1$?\n\n(v) In section G.5., the authors say \n>In our setup, where the input and output layers are initialized at a constant scale (w.r.t. L), it is actually not possible to have a kernel limit. Even in our linear case in Section 4, one can see the learned model is not linear.\n\nI am confused about this explanation and believe a more detailed and comprehensive elucidation is necessary.  Firstly, are you implying that the Depth-$\\mu$P is the unique parameterization that is stable, nontrivial, and faithful? Specifically, in very deep networks with skip connections, are you suggesting that the kernel regime is absent?  As far as I understand, the original work on $\\mu$P [Yang & Hu, 2021] characterizes feature learning as the boundaries of the kernel regime. Thus, it would be surprising (and interesting, if it is true) that the kernel regime disappears, leaving only feature learning as a stable state of training.  \nSecondly, in what sense, is the learned linear network not linear? By definition, the linear network is a linear mapping of the input x.  Could you clarify in what aspect the learned linear network deviates from this linear characteristic?\n\nDue to these ambiguous points, I have currently assigned a lower score. However, I am open to increasing this score if they are clarified or if any misunderstandings on my part are resolved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8419/Reviewer_Mvpr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698914204936,
        "cdate": 1698914204936,
        "tmdate": 1700645029512,
        "mdate": 1700645029512,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Sdk95OH2e7",
        "forum": "17pVDnpwwl",
        "replyto": "17pVDnpwwl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_AGJB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8419/Reviewer_AGJB"
        ],
        "content": {
            "summary": {
                "value": "The paper studies feature learning parameterizations of deep residual networks when depth goes to infinity (after taking infinite width in a muP regime), and studies the impact of two scaling parameters: a multiplier $L^{-\\alpha}$ of each block before adding on the residual stream, and a multiplier $L^{-\\gamma}$ on the learning rate, where $L$ is the depth.\n\nThe authors find that the best regime, termed Depth-muP, is for $\\alpha = \\gamma = 1/2$, leading to various desirable properties. Of particular interest and novelty is the \"feature diversity\" property, which leads to diverse features across neighboring layers and is only achieved for $\\alpha = 1/2$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The findings in the paper are of significant interest for scaling of neural networks to large depths, both for theoreticians and practitioners.\nWhile the multiplier scalings had been studied before, especially in kernel regimes, its extension to feature learning is crucially important, and provides a meaningful criterion for the benefits of $\\alpha = 1/2$, namely feature diversity (while higher $\\alpha$ leads to redundancy across layers)."
            },
            "weaknesses": {
                "value": "see questions"
            },
            "questions": {
                "value": "Here are a few points that should be addressed in order to strengthen the paper.\n\n* feature diversity: it'd be helpful to provide additional experiments on this point, especially for $\\alpha > 1/2$, something which seems to be missing in the current draft. It'd be helpful to provide more intuition about the benefits of feature diversity (which seem closely related to benefits of depth), perhaps with concrete examples. Can the quantity in Def 6.6 be plotted during training to see the effect of $\\alpha$?\n\n* presentation: while the warmup section 3 is quite insightful, I found section 4 to be much more obscure, and perhaps not essential for the later discussions. I encourage the authors to either provide some more intuition on the \"physical\" meaning of the different equations, or to shorten the section and defer details to the appendix. Perhaps an informal version of the actual tensor program would be more insightful? Also, how does this related to the AMP and DMFT literature? some more comparison to related literature would be helpful (in particular, this concurrent [paper](https://arxiv.org/abs/2309.16620) seems highly related and should be cited and compared to in the final version)\n\n* mean subtraction: could you clarify a bit more the role of this? Is MS specifically needed because of the activation? Would adding an MLP output layer as in transformers drop this requirement? Note that in practice LLM practitioners often drop the mean subtraction part of layer-norm, using RMSnorm instead (e.g. in LLaMa, likely for computational reasons, but it seems to convey that MS isn't really needed?)\n\n* scaling: do you expect any different behaviors in different limits, e.g. in the proportional deep-wide limit studied e.g. by Hanin, or what do you expect would change if the number of GD steps can grow with the number of layers? any comparison or intuition here would be useful.\n\nOther comments:\n- on feature diversity: how does the quantity in Def 6.6 behave at initialization vs later in training? what do you mean by \"maximal\" in claim 6.5? any more intuition beyond the footnote on how to get smaller exponents than 1/2?\n- \"analogous to width situation where deep mean field collapses to a single neuron\": can you elaborate? what is deep mean field, and how are these related?\n- Figures: any observations on what happens when changing width and depth together, as opposed to fixing the width?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699121566370,
        "cdate": 1699121566370,
        "tmdate": 1699637048489,
        "mdate": 1699637048489,
        "license": "CC BY 4.0",
        "version": 2
    }
]