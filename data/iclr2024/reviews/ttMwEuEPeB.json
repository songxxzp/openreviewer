[
    {
        "id": "fnQTyA1x6n",
        "forum": "ttMwEuEPeB",
        "replyto": "ttMwEuEPeB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission364/Reviewer_967x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission364/Reviewer_967x"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new 3D scene generation pipeline using procedural 3D modeling together with LLMs. Specifically, LLMs are given the documentations of a procedural 3D modeling tools together with human instructions. Three LLM agents, namely the task dispatch agent, the conceptualization agent and the modeling agent, are designed to work together and generate Python scripts for the 3D modeling tools that generate 3D scenes/ objects that corresponds to the given instructions. Qualitative and quantitative experiments are performed to show the results of such pipeline and prove the effectiveness of using three agents to collaborate on this task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper shows the potential of using LLMs and procedural 3D modeling tools on text-guided 3D generation tasks.\n2. Having three agents collaborating on generating the final Python scripts is interesting and proven to be effective."
            },
            "weaknesses": {
                "value": "1. Though it is a nice application to use LLM with procedural 3D modeling tools for text-guided 3D generation, I think the overall contribution of this paper is not enough to be considered for publication on ICLR. The community has had many similar discoveries on the ChatGPT/ GPT-4's ability on generating parameters for images [1] or 3D objects [2] in a zero-shot or in-context learning way. Therefore, there is no surprise that combining LLM with a better parametric 3D modeling tool, e.g., InfiniGen, could produce better visual results. I think all the three potential directions mentioned in the last section are promising and valuable challenges to solve, which would bring more contribution to the community.\n\n2. The evaluation is limited. I must admit that for a new task of text-guided 3D scene generation, it is hard to construct baseline methods. But at least, there are similar efforts in the community of text-guided 3D object generation that worth being compared with. For example, score-distillation-based methods, e.g., DreamFusion, can be used as baselines for the \"Single Class Control\" experiments.\n\n[1] Bubeck, S\u00e9bastien, et al. \"Sparks of artificial general intelligence: Early experiments with gpt-4.\" arXiv preprint arXiv:2303.12712 (2023).\n[2] https://twitter.com/gd3kr/status/1638149299925307392"
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698622293970,
        "cdate": 1698622293970,
        "tmdate": 1699635963625,
        "mdate": 1699635963625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cXKurwzMjJ",
        "forum": "ttMwEuEPeB",
        "replyto": "ttMwEuEPeB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission364/Reviewer_m12k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission364/Reviewer_m12k"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method that can take in natural language and output code in a Domain-Specific Language. The DSL is a system that can generate 3D scenes and assets in a procedural fashion.\n\nTherefore the overall pipeline converts natural language into realistic, high quality 3D scenes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The results are **incredibly strong** - looking at the submitted video. Especially the results on sky-editing."
            },
            "weaknesses": {
                "value": "**(MAJOR) Lack of detail**\n\n---\n\nThe paper describes the system at a very high level. We are introduced to the \"Task Dispatch Agent\", the \"Conceptualization Agent\" and the \"Conceptualization Agent\" but no details of how they are actually implemented. There is no detail of what subset of the InfiGen language is used, no detail of \u201ctranslate the scene into a winter setting\u201d, it pinpoints functions like add snow layer() and update trees()\"\n\nWhile there are some examples in the Appendix of the prompts and the associated code, it is woefully incomplete.\n\nIn the current iteration, it is almost impossible to implement/reproduce the paper.\n\nI will not knock the paper down for a lack of quantitative results because this space is very new and no metrics apart from user studies really exist and creating a new procedural baseline would itself be a lot of work."
            },
            "questions": {
                "value": "I wonder if the authors would send this work to a compilers conference/journal instead?\n\nIn case they are too rigorous to accept LLM based papers, why not something like ToG or SIGGRAPH. Both have wonderful papers describing procedural systems where the parameters come from some heuristic model. This paper with its strong results and a graphics focus seems like a perfect fit for such venues.\n\nI would be willing to accept this paper only after a major rewrite which would include a lot more description of the various components in the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834669720,
        "cdate": 1698834669720,
        "tmdate": 1699635963516,
        "mdate": 1699635963516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q0Y2aTIP7j",
        "forum": "ttMwEuEPeB",
        "replyto": "ttMwEuEPeB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission364/Reviewer_nMkH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission364/Reviewer_nMkH"
        ],
        "content": {
            "summary": {
                "value": "This paper Introduces 3D-GPT, a training-for-free framework designed for 3D scene generation, which generates Python codes to control 3D software, potentially offering increased flexibility for real-world applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The generation ability of 3D scenes and objects are good according to the demo.\n\n2. The step-by-step refinement of 3D outputs is meaningful and impressive.\n\n3. The paper is easy to follow with good-quality figures."
            },
            "weaknesses": {
                "value": "I'm not an expert in 3D procedural 3D generation, and here are some of my concerns.\n\n1. The paper only shows examples of 3D plants and forests. Can 3D-GPT work on other objects or scenes, such as human and street? Can 3D-GPT generalize well to more complex scenarios?\n\n2. The paper adopts ChatGPT as LLM. How about other open-source LLMs, such as Alpaca, LLaMA-Adapter, or Vicuna?"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698912655311,
        "cdate": 1698912655311,
        "tmdate": 1699635963446,
        "mdate": 1699635963446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j0rQ7ze45m",
        "forum": "ttMwEuEPeB",
        "replyto": "ttMwEuEPeB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission364/Reviewer_QYLZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission364/Reviewer_QYLZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a workflow for procedural 3D scene generation conditioned on text descriptions using pre-trained LLMs. It leverages an existing procedural generator, InfiniGen, to create 3D contents, and uses LLMs to pick a set of procedural functions from InfiniGen and infer their corresponding parameters given the text description of the scene. The authors split the task into three steps (agents): the first step is to select the set of functions given the prompt, the second step is to infer more detailed descriptions given the required informatin, and the last step is to generate the parameters for each function given the detailed description. The experiment results show that this workflow can produce single class objects with details and complex scenes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method does not require training.\n- The multi-agent approach is effective. It might share similar advantages as other methods such as chain-of-thought or tree-of-thought, where the final output from the LLM is guided by a curated step-by-step instruction.\n- It addresses another potential direction for text-to-3D generation: utilizing tools or existing 3D procedural models. \n- It demonstrates the potential of using LLMs to control tools for content creation and 2D/3D modeling."
            },
            "weaknesses": {
                "value": "- Lack of details about the model and experiment setting and how the results are affected. For example, which LLM is used? What is the size of the function set F? Does the size of F affect the quality? How many examples are provided? Do the example related to the prompt L? Does zero-shot/few-shot make any difference? \n- Evaluation can be improved. It would be great to do ablation studies on D, C, I, E, and answer the questions mentioned above.\n- It would be great to further explore the limitations and failure cases. For example, does the complexity of the scene affect the results? Does the number of parameters or the design space (e.g., parameter ranges) affect the results? \n- The proposed method demonstrates that the LLM can convert the input prompts to python codes that controls the functions and parameters. However, since the 3D modeling capability comes from the procedural models, not from the LLM, it seems the task is closer to scene composition or object inference instead of modeling. It will be more interesting to see if LLMs can generate 3D modeling commands or procedural modeling sequences/rules."
            },
            "questions": {
                "value": "- There are many procedural models available for Blender. It will be interesting to see if this workflow will work on any arbitrary procedural models given the same amount of information, i.e., D, C, I , and E, such as house, car, airplanes instead of focusing on scenes in InfiniGen.\n\n- It will be great to see if the selected functions can formulate some dependencies, for example, to generate 'flowers on the trees', the trees need to be created first and the positions of the flowers are based on the positions of the tree branches.\n\n- It will be great to see a full example of the input and output of each agent in the process.\n\n- Have the authors tried to also provide the functions picked by the TDA to CA?\n\n- It is obvious that this method will help users who know nothing about 3D modeling, but I am curious whether the sequential editing task in Fig 4 will be more efficient to professional Blender users (e.g., game developers) to achieve a desired outcome, compared to tweaking the parameters by themselves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699578293470,
        "cdate": 1699578293470,
        "tmdate": 1699635963379,
        "mdate": 1699635963379,
        "license": "CC BY 4.0",
        "version": 2
    }
]