[
    {
        "id": "oqeiq6oCRZ",
        "forum": "6HwamHLDa6",
        "replyto": "6HwamHLDa6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_a9Au"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_a9Au"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an implicit Multi-In-Single-Out (MISO) based VFI model designed to generate accurate intermediate frames that are robust against nonlinear motion. It predicts\na target intermediate frame by taking both a prior frame clip and a subsequent frame clip as input, enabling it to capture nonlinear motion effectively. Meanwhile, a video motion perceptual loss is proposed in this paper. The two contributions presented enable the state-of-the-art performance achieved in this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed structure is simple but effective and capable of achieving state-of-the-art performance.\n2. The ablation experiments are more than sufficient to argue for the validity of the two proposed modules."
            },
            "weaknesses": {
                "value": "1. Will this multiple-input single-output framework have limitations in practical applications? For example, given a T-frame video, how is the first frame or the last stitch of the image processed?\n2. EMA-VFI is incorrectly referenced.\n3. The length of the METHOD section is too short. The model structure is not presented in the whole paper, and it is not enough to show it only in Figure 2.\n4. I think the Table 1 comparison is unfair because the other methods in the table have not been trained on multi-frame video. Therefore Table 1 should not be used as the first table in the experimental chapter. Instead Table 2 will be a fairer comparison."
            },
            "questions": {
                "value": "Training according to the model structure proposed in this paper requires more video frames. How did this paper modify the dataset so that it meets the training requirements of the structure proposed in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Reviewer_a9Au"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657239553,
        "cdate": 1698657239553,
        "tmdate": 1699636411693,
        "mdate": 1699636411693,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tZxJPPKBlz",
        "forum": "6HwamHLDa6",
        "replyto": "6HwamHLDa6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_CFmx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_CFmx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes multi-in-single-out MISO structure that targets at frame interpolation task without explicitly estimate optical flow. The target time t is inserted to the network as a time embedding. Based on such structure and a proposed video motion perceptual loss, the proposed model claims advantages on non-linear motions as well as the capability of handling occlusions between frames. Experiments are conducted on the leading benchmarks with ablation studies on the proposed loss function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed contribution is clear, and the paper is easy to follow"
            },
            "weaknesses": {
                "value": "The network structure is straightforwrd, with the loss as the main technique contribution. The loss consists of a MSE loss, image perceptual loss  and motion perceptual loss, with the last one as the claimed contribution. Although these combined loss design seems reasonable, the motion perceptual loss is adapted from the image perceptual loss, which may be considered as delta modifications or improvements. \n\nI'm not directly work on VFI tasks, but I know that there are bunch of work, e.g., burst image processing, that adopts multi-in-sinle-out structure, such as burst image denoising, burst image super-resolution etc. Therefore, claiming multi-in-sinlge-out is not that novel. If it is the frist to the VFI, then it is ok to claim it. This is what I'm not sure. But even so, it is not that novel."
            },
            "questions": {
                "value": "Optical flow is important to the task of VFI, although it can be omitted, it can also be estimated as an auxiliary inputs upon multi images to the network for assistance. The flow may do something negative, but it also contributes positively on various situations. I have some reservations regarding that flows are completely removed. However, as long as the results are good, I can still buy it. \n\nIn table 4, the metric SSIM, with only 2D-perceptual, the SSIM is 0.9956, however, with both 2D and 3D perceptual loss, the SSIM is 0.995, but bolded. Besides, it seems that the 2D-perceptual loss did the most contributions, the contribution of 3D perceptual loss is small. \n\nI do not find any video examples in the supp. which makes me hard to evaluate the real performances. \n\nThey saying \"as VFI is not a task focused on generating plausible images,\" is not accurate. The definition of plausible image and accurate image should be explained.\n\nEstimating motions can still deal with the occlusion challenges, based on what type of occlusions, and how large the temporal T is. \n\nMore descriptions of how to read the Fig.1, e.g., what are the white and blue balls, what is the shadow regions, should be clearly stated in the figure caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Reviewer_CFmx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723255333,
        "cdate": 1698723255333,
        "tmdate": 1699636411626,
        "mdate": 1699636411626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ufY5RlPsqE",
        "forum": "6HwamHLDa6",
        "replyto": "6HwamHLDa6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_rS3w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_rS3w"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to tackle the challenge of multi-frame-based video frame interpolation. Based on an existing T-shaped architecture (Seo et al., 2023b), the author introduces a motion perceptual loss that measures the distance between 3D conv features. Experiments on datasets including UCF101, Vimeo90K, and Middlebury datasets are conducted to demonstrate the proposed method's effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- This paper is simple and easy to read.\n- Very good performance are reported by the authors."
            },
            "weaknesses": {
                "value": "- Hard to find the contributions of this paper. The model architecture is borrowed from an existing work ( though just on arxiv), it is hard to count as this paper's contribution in my opinion.  The only new thing in this work is the motion perceptual loss, which is too trivial and straightforward. Moreover, as shown in Tab.4, the improvement introduced by motion perceptual loss is too minor  (0.16 PSNR).\n\n- Evaluations are not convincing. Tab-1 compares performance for multi-frame-based interpolation. Yet almost all the selected previous SOTA are not optimized for multi-frame input. The used datasets like Vimeo90K, Middlebury, and UCF101 are also very wired for this setting. In general, the reviewer believes the comparisons are unfair and less convincing."
            },
            "questions": {
                "value": "- What are the training details for experiments in Tab-2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Reviewer_rS3w"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756047499,
        "cdate": 1698756047499,
        "tmdate": 1699677273749,
        "mdate": 1699677273749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RizSjZ8gU4",
        "forum": "6HwamHLDa6",
        "replyto": "6HwamHLDa6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_WQaG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4385/Reviewer_WQaG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel multi-frame video frame interpolation approach that incorporates explicit non-linear motion estimation. Additionally, the authors propose the use of a perceptual loss to enhance the visual quality of the interpolated frames."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The experimental results provide evidence of the superior performance of the proposed model when compared to existing methods.\n2. The utilization of a different pre-trained model for motion perceptual loss enhances the overall effectiveness of the approach."
            },
            "weaknesses": {
                "value": "1. The approach of using an increased number of input frames to interpolate a single intermediate image may lack novelty in its overall concept.\n2. The straightforward substitution of the pre-trained model for perceptual loss can be seen as a technical workaround, and there is a lack of clarity regarding the process of constraining a single interpolated image to achieve 3D-perceptual loss.\n3. There is a lack of visual analysis provided for the non-linear motion estimation.\n4. The quantitative comparison conducted on the Vimeo90K septuplet dataset is not commonly used. It is recommended to provide the results on the triplet benchmark using the 1-1-1 setup for a more standard comparison."
            },
            "questions": {
                "value": "see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4385/Reviewer_WQaG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819494774,
        "cdate": 1698819494774,
        "tmdate": 1699636411464,
        "mdate": 1699636411464,
        "license": "CC BY 4.0",
        "version": 2
    }
]