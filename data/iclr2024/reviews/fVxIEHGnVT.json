[
    {
        "id": "CyxZE2UbHF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_7Q1Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_7Q1Z"
        ],
        "forum": "fVxIEHGnVT",
        "replyto": "fVxIEHGnVT",
        "content": {
            "summary": {
                "value": "This paper focuses on improving Java $\\rightarrow$ Python translation using error correction, rather than retraining the underlying translation model. They devise two error correction techniques (kNN-ECD and kNN-ECS) based on kNN-MT, which entails retrieving from datastores. To build this datastore, they first collect 82,665 Java functions and generate high-quality unit tests for them using EvoSuite. Then, they use TransCoder-ST to translate the Java functions paired with the unit tests to Python. From these they extract pairs of the form (failed Python function, successful Python function), which are then used to build (a) datastore(s). The datastore is organized based on two components: (1) (key, value) pairs and (2) (key, value) $\\rightarrow$ token. The query to this datastore is formed by using the last decoder hidden states corresponding to the full source input (i.e., failed Python function) and partial target (i.e., possible correction generated so far). To reduce noise caused by diverse rare identifiers during retrieval, they apply the unified name rule. In kNN-ECD, only one round of correction is performed. In kNN-ECS_{m}, they perform $m$ rounds of correction, with m smaller datasets (after segmenting the large datastore into $m$ parts). Results show that kNN-ECS outperforms kNN-ECD as well as a vanilla TransCoder-ST with no error correction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed approach successfully corrects errors to a certain extent, without retraining the model or re-sampling the model many times, which is usually done in self-repair.\n- The idea of multi-round error correction and the analysis done with this, varying the number of rounds, and analyzing the performance for each of these, is quite interesting and may inspire future work."
            },
            "weaknesses": {
                "value": "- Evaluation is based on translated unit tests generated by the same model that the authors are trying to correct translation errors for. Therefore, the unit tests that are generated could be wrong, and so the evaluation is unreliable. Evaluation should be performed based on a separate, high-quality set of unit tests. Possibly datasets like HumanEval-X would be better alternatives here.\n- The experiments and results are fairly limited. First, the authors focus on only Java $\\rightarrow$ Python and fail to consider other languages or even the reverse direction of Python $\\rightarrow$ Java. Next, Table 1 seems to be missing many baselines and other models to which their approach should be compared. Namely, the only baseline is the pure TransCoder-ST model, which is only the starting point of their approach. The authors discuss that the main advantage of their approach is that no retraining is required, so it would be important to see how their approach performs relative to a retraining-based one. For this, they could have simply fine-tuned TransCoder-ST on the error correction pairs they collected for building their datastore. Next, latency is not measured, even though the authors discuss latency in related work. It seems that retrieving from a large datastore or retrieving multiple times from smaller datastores could take a long time, so it would be important to understand how the overall latency compares to other approaches. Finally, the authors do not report results on state-of-the-art code models, so it is difficult to assess the true value of their approach.\n- The authors present the unified name rule as a novelty; however, I do not find this to be that novel, given the work the authors discussed in the \"Related Work\" section.\n- There are multiple aspects of the paper that are not clear.  Please see the \"Questions\" section."
            },
            "questions": {
                "value": "1) Based on what is written in the paper, 10 Python functions with unit test cases are generated for each Java function. So, you have $(func_1, tests_1), (func_2, tests_2), (func_3, tests_3)... (func_{10}, tests_{10})$. Success is measured by executing the tests in some Python environment, where $func_i$ is considered a success if it passes all tests in $tests_i$. By this definition, suppose $func_1$ fails $tests_1$ and $func_2$ passes $tests_2$.  The paper states \"we combined the first failed Python function with the first successful Python function to form an error correction language pair.\" Based on this, it seems that $(func_1, func_2)$ would be considered an error correction pair. However, there is no guarantee that $tests_1 = tests_2$, meaning that the two functions could be executed against different test suites. Therefore, $func_2$ may not actually correspond to a correction of $func_1$. Could this please be clarified? \n2) The \"interpretable decision-making\" idea is not clear to me. It seems that you are suggesting that the reasoning for predicting a specific token at a timestep $t$ can be attributed to the source and partial target function predicted so far. This is also the case for transformer-based decoders, so it is not clear to me how your approach can be considered more interpretable than a transformer as they claim.\n3) In 3.2, you state that the hidden representations from the last layer of the decoder are used to build the (key,value) and query. My understanding is that the (key ,value) and query correspond to (failed Python function, partial Python function). It is not clear to me how there would be a decoder state corresponding to the failed Python function since that is passed into the encoder (Figure 1). Or is (failed Python function, partial Python function) meant to actually only represent the representation of the partial Python function generated so far, as labeled as \"Key\" in Figure 1? \n4) You claim that the improvement of ECS over ECD is \"primarily attributed to its distributed structure, which includes diverse datastore variants.\" However, you do not seem to have multi-round experiments with ECD in which you repeatedly perform retrieval/correction on the same large datastore up to $m$ times. Therefore, isn't it possible that the advantage is actually from doing iterative code correction rather than the distributed nature of it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Reviewer_7Q1Z"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697405481282,
        "cdate": 1697405481282,
        "tmdate": 1700589327485,
        "mdate": 1700589327485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CKvDpjTU8U",
        "forum": "fVxIEHGnVT",
        "replyto": "fVxIEHGnVT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_oueh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_oueh"
        ],
        "content": {
            "summary": {
                "value": "To address a need for code-to-code translation accuracy improvements, the authors propose to combine a transformer model, specifically TransCoder-ST, with an error correction model that optionally overwrites the output of the transformer model. They do so in two ways. Initially, they consider the error correction model to be a single error-correction datastore in which they perform kNN (kNN-ECD). Later, they improve on the initial model by dividing the dataset into m-sub-datasets and they construct a distributed datastore (kNN-ECD$_m$). To make the dataset more uniform, the authors also employ a \"unified name rule\", to perform $\\alpha$-renaming while keeping certain type information. They show that this full-pipeline can improve TransCoder-ST performance on Java to Python translation from 68.9% to 89.9%."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Simple framework both in the single and multi-round error correction. \n- Shows generalisation of fixing patterns (and the authors check for data leakage).\n- Extensive ablation to understand which pipeline components contribute to the overall performance (single- vs multi-round error correction, sub-datastore performance, unified renaming)"
            },
            "weaknesses": {
                "value": "- Interpretability feels like an after-thought, it is left to the reader to infer it from the use of kNN-MT derived methods. Indeed, the discussion in S4.2 focuses more on the ability of the model to generalise from the examples (which is interesting and significant), but the showing the mapping to a software engineer would do little to explain the model decision and gain trust in the model. To build on the S4.2 example, a user explanation would be that the final values should be \"int\", and it is difficult to derive this insight from the mapping.\n\n- On dataset construction, EvoSuite will overfit to the current implementation, which means there is an underlying assumption that the Java version is bug free. Further, translation of the PyTests from Java Unit tests can also be erroneous."
            },
            "questions": {
                "value": "In the multi-round error correction(kNN-ECD$_m$), does each subsequent ECD get the mis-corrected version from the previous module or does each ECD get the original wrong translation?\nIf the former, does kNN-ECD$_m$ perform partial corrections that build on top of each other? \n\nOn the dataset construction, have PyTest translations been manually or otherwise been validated to be correct/faithful to the EvoSuite output?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5068/Reviewer_oueh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798967484,
        "cdate": 1698798967484,
        "tmdate": 1699636496999,
        "mdate": 1699636496999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l3nGOeaQDS",
        "forum": "fVxIEHGnVT",
        "replyto": "fVxIEHGnVT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_Uzpj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5068/Reviewer_Uzpj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an error correction method, KNN-ECD, which is based on KNN-MT and improves the performance of code translation.  Building upon this, the paper further propose $kNN-ECS_{m}$, which divides the data store to $m$ sub-datastores. In addition, the paper proposes a new unified name rule to encourage the datastore to focus more on code logic and structure rather than diverse rare identifiers. The experiments show the the proposed methods largely improve the translation accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper applies the $kNN-MT$ to the code translation and obtain a significant improvement of the translation accuracy. Using functional equivalence as evaluation metrics instead of BLEU better reflects the true  code translation quality. And the proposed method increase the accuracy by about 20%.\n\n2. The paper proposes a novel $kNN-ECS_{m}$ framework, which further improves the translation accuracy of program.\n\n3. The paper performance extensive empirical analysis of the proposed method."
            },
            "weaknesses": {
                "value": "1. $kNN-ECD$ is very similar to $kNN-MT$. Therefore, the technical contribution of the paper is limited.\n\n2. The motivation of applying $kNN-MT$ is not very clear. Although $kNN-MT$  is useful for natural language translation, is there some particular reasons that it will be more effective for programming languages.\n\n3. The presentation is experiment results is hard to read, especially for Table 3 and Table 4. I would suggest the authors to use Figures to present this results and put the detailed numbers in the Appendix.\n\n4. The paper does not show the proposed method can perform error correction for OOD errors. The paper uses model $A$ to build the pair of incorrect programs and correct programs. Therefore, the error is specifically related to model $A$ itself. For a new model $B$, it may make different kinds of error, does the proposed method with learning datastore for model $A$ can fix the error of model $B$. If not, the method requires building datastore for every new method, which largely limiting the application of the proposed method.\n\n\nMinor:\n\n\"Uncorrect\" should be changed into \"Incorrect\""
            },
            "questions": {
                "value": "1. For unified name rule, how to identify the variable name or function name in a program and replace them? Is it replaced by some automatic tools?\n\n2. How do the method judge the wrong translations of TransCoder-ST? Is the error correction only applied the wrong programs?\n\n3. What does the method have better interpretability? The key value pairs in the datastore is still based on neural networks.\n\n4. Is there any fine-tuning stage of the TransCoder model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811889897,
        "cdate": 1698811889897,
        "tmdate": 1699636496874,
        "mdate": 1699636496874,
        "license": "CC BY 4.0",
        "version": 2
    }
]