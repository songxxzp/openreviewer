[
    {
        "id": "wH7RfGD2o6",
        "forum": "FBDtqWXfuq",
        "replyto": "FBDtqWXfuq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_stt3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_stt3"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the limitations of current federated learning (FL) frameworks by introducing a novel setting called Modality-Collaborated Federated Learning (MCFL) that focuses on collaboration among uni-modal clients with different data modalities. MCFL aims to leverage the shared knowledge among uni-modal clients while ensuring performance gains across individual modalities, making it a practical and appealing approach for scenarios with diverse uni-modal data. The proposed framework, FedCola, addresses the challenges of model heterogeneity and modality gaps through strategies such as modality-agnostic transformers, attention sharing, and modality compensation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper addresses a significant issue in federated learning and proposes a novel solution.\n- Showcases an optimal combination of parameters and strategies to enhance performance.\n- The well-designed evaluation is provided across multiple scenarios to affirm the efficacy of the proposed solution."
            },
            "weaknesses": {
                "value": "Major:\n\n- The proposed framework is based on FedAVG, can other model aggregation methods be applied to further improve the performance?\n- Although the proposed framework requires fewer resources than other methods when transformers are applied, what about the resources compared with CNNs?\n- During the warm-up stage, are participating clients sampled from only one modality? If so, the comparison might be unfair since more clients on the warm-up modality are participating.\n\nMinor:\n\n- Can this framework handle clients with multi-modal data? For the pointed-out healthcare scenario, one client with multi-modal data is also common.\n- Security and privacy are not discussed. Considering the application scenarios (i.e., hospitals), security and privacy are highlighted."
            },
            "questions": {
                "value": "See the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698462513986,
        "cdate": 1698462513986,
        "tmdate": 1699636074373,
        "mdate": 1699636074373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k3omJgbDnW",
        "forum": "FBDtqWXfuq",
        "replyto": "FBDtqWXfuq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_2xgy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_2xgy"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the research problem of uni-modal clients with different modalities in federated learning. It explores several strategies, including cross-modal parameter sharing, model aggregation, and temporal modality arrangement. The authors provide empirical results to discuss their statement and compare the performance with the selected baseline CreamFL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing of this paper is easy to follow, and the logic is clear.\n2. The main perspectives in Sec. 5 are sound in the multi-modal federated learning.\n3. The authors provide extensive experiments."
            },
            "weaknesses": {
                "value": "1. I have concerns about the motivation of this work. In this paper, each client has one single modality, which is not practical in the real-world setting. Though the authors use the hospital as an example, it is more practical that different clients may have different ratios of different modalities of data.\n2. Also, if I understand correctly, as stated in Sec. 5.2, the authors expect a better-aligned global model. However, assuming each client has one single modality, it should fall into the personalized federated learning domain, where we care more about the clients\u2019 local performance.\n3. Section 5 is one of the key parts of their proposed work. However, some of the parts are put in the appendix.\n4. About the model compensation, I am concerned about the extra communication cost and practicality that we require all the clients\u2019 models to have all the parameters.\n5. I am concerned if the core of the technique is still based on the power of transformers which are able to handle different modalities of data."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716786445,
        "cdate": 1698716786445,
        "tmdate": 1699636074284,
        "mdate": 1699636074284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "omsYiNDPvF",
        "forum": "FBDtqWXfuq",
        "replyto": "FBDtqWXfuq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_RJk2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_RJk2"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel setting in Federated Learning (FL), termed Modality-Collaborated Federated Learning (MCFL), which focuses on collaboration among uni-modal clients with different data modalities. A new framework termed Federated Modality Collaboration (FedCola), which leverages a modality-agnostic transformer, is proposed to address the challenges in MCFL. Several strategies were probed to optimize the parameter-sharing, aggregation, and temporal modality arrangement in the FedCola. Empirical studies were conducted with two modalities, vision and language, and FedCola showed promising performance for both."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The research investigates an under-explored area in FL, moving from uni-modal to multi-modal data, which more realistically reflects the nature of real-world data.\n- The paper presents a very thorough investigation leveraging several strategies to optimize the MCFL framework, including parameter-sharing, aggregation, and temporal modality arrangement.\n- The proposed framework, FedCola, is practical and adaptable to more intricate FL scenarios, not limited to the two-modal setting that was experimented.\n- The authors provided comprehensive experiments and comparisons, demonstrating the superiority of FedCola over other methods in terms of both performance and resource requirements."
            },
            "weaknesses": {
                "value": "- The experiments are primarily conducted on two-modal settings, and the adaptability of FedCola to more complex scenarios with more modalities was not thoroughly studied. Could the proposed framework be extended to scenarios with more modalities?\n- The effectiveness of temporal modality arrangement was linked to the correlation between different modalities. Will the performance be influenced by the semantics of different modalities?\n- Limited discussion on privacy issues: The consideration of privacy issues in federated learning is critical; however, the paper did not sufficiently address this issue in the MCFL."
            },
            "questions": {
                "value": "- The motivation of the modality compensation is based on an equation based on the number of training samples. However, in the figure provided, the misalignment is based on the number of sampled clients. Can the authors further explain the subtle differences here?\n- Is the server model the same as the client models? Can the authors explain more about the relationship between the client and server models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737961213,
        "cdate": 1698737961213,
        "tmdate": 1699636074192,
        "mdate": 1699636074192,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xbmb2BVseW",
        "forum": "FBDtqWXfuq",
        "replyto": "FBDtqWXfuq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_iEDo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1455/Reviewer_iEDo"
        ],
        "content": {
            "summary": {
                "value": "This submission discusses a scheme where in the multi-modal setting, modality-agnostic transformer models used in different modalities can share parameters of self-attention layers. Authors further suggest parameters specific to individual modality that are not shared can still be augmented to to the cross-modal models and averaging can be taken for the augmented model (referred to as \u201ccross-modal aggregation). Together with warming-up training for each individual modality, authors suggest a framework named FedCola to do federated learning across image-text modalities. Experimental results on CIFAR100 and AGNEWS datasets are reported."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic of this work is highly relevant to the theme of ICLR.\n\n\nI very much appreciate authors\u2019 effort to make the narrative to be direct and concise, and to formulate several questions in the text that outline key points in the proposed methodology."
            },
            "weaknesses": {
                "value": "Several technical details should be further clarified to let the paper to be more convincing."
            },
            "questions": {
                "value": "- on methodology: what is the frequency of doing inter-modality update of the shared parameters and that of doing intra-modality update of modality-specific parameters? \n- on methodology: as the number of training data points/clients in different modalities could be different, the shared parameters should be far more frequently updated compared to modality-specific parameters. It seems that authors have proposed the aggregation step to address this problem. It remains unclear to me how gradient back-propagation works with respect to those parameters which are manually aggregated/augmented into a model, and how these external parameters function during the inference process of a certain modality?\n- on methodology: under the assumption of data homogeneity and absence of Byzantine workers, should the Uni-FedAVG method show better performance on the dataset CIFAR100? i.e., for the vision modality itself, I am expecting that the accuracy should be somehow higher than that reported in Table 4. [Benchmarking FedAvg and FedCurv for Image Classification Tasks, Casella et al., 2023]\n- on results: regarding reported average accuracy results in tables 1 and 3, how are the average computed? Is there any weighting assigned to each modalities?\n- on results: intuitively, when comparing the balance of clients for different modalities, why not show the accuracy change in each modality under different $(N_v, N_l)$ setup?\n- on methodology: Convolutional networks are perhaps a simpler type of models for vision related tasks. As in the CIFAR100 case, transformer-based models seem to be somehow below par of the performance of convolutional networks, I wonder if it is possible to incorporate convolutional networks in the study and see relevant results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785578362,
        "cdate": 1698785578362,
        "tmdate": 1699636074103,
        "mdate": 1699636074103,
        "license": "CC BY 4.0",
        "version": 2
    }
]