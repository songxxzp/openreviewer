[
    {
        "id": "Dgeg2rLcry",
        "forum": "b5LJVjwOsB",
        "replyto": "b5LJVjwOsB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_NcNK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_NcNK"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed an explanation method leveraging vector transformation measurement. \nIt evaluates transformation effects by considering changes in vector length and directional correlation.\nIt further incorporates attention and vector transformation information across layers to capture the comprehensive vector contributions over the entire model.\nExperiments demonstrate good explanation performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method is clear and novel. The implementation is sound.\n+ It provides an aggregation framework to trace vector evolution across layers.\n+ It demonstrates better visual results on  object-centric heatmaps.\n+ Numeric studies also prove its advantages over traditional methods."
            },
            "weaknesses": {
                "value": "- The method only demonstrate the aggregation method on plain ViT. I am concerned that it will not work on other vision transformers with window / shifting attentions"
            },
            "questions": {
                "value": "Is proposed method compatible with hybird vision transformers with convolution layers? Consider to re-run table 3 on more networks, such as Swin and PVT"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822149122,
        "cdate": 1698822149122,
        "tmdate": 1699636206950,
        "mdate": 1699636206950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hk9KUPmXjk",
        "forum": "b5LJVjwOsB",
        "replyto": "b5LJVjwOsB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_9oRD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_9oRD"
        ],
        "content": {
            "summary": {
                "value": "This work introduces an explanation method for the vision transformer.  The main idea is taking the changes in vector length and direction into consideration. Then the author builds an aggregation framework for understanding the vision transformer. The empirical results indicate that the proposed method is helpful for improving the performance of the intermediate prediction on classification and localization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper presents a straightforward and intuitive approach by formulating the MLP and FFN as the vectors transformation, making it easy to understand. The analytical process is clearly outlined.\n2. The experimental section is thorough, with a wide range of tasks being evaluated.\n3. The proposed aggregation framework appears to be both simple and efficient."
            },
            "weaknesses": {
                "value": "1. The main concern is the extent to which this method and framework will have an impact.  It seems the proposed method just offers a new way to visualize the highlighted regions in a ViT.\n2. The analysis is currently limited to ViTs trained using supervised image classification.\n3. The explanation section is lacking, leaving it unclear whether any new insights about ViTs have been gained through this framework. \n4. Many neural network architectures can be understood as vector transformations, including LSTM, RNN and CNN). Therefore, the novelty and originality of this work should be more thoroughly discussed."
            },
            "questions": {
                "value": "1. While the proposed method offers a new visualization tool for ViTs, it remains unclear how it can help us better understand ViTs or if it provides any novel insights into their workings.\n2. How to use this method to analyze ViTs trained with self-supervised learning, like the DINO v1/v2, MAE, etc. Please prodive more insights about these models with the proposed method.\n3. What's the difference between the ViTs and CNNs when understanding the network from the vector transformation perspective."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699087751465,
        "cdate": 1699087751465,
        "tmdate": 1699636206851,
        "mdate": 1699636206851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kYXYseaVWy",
        "forum": "b5LJVjwOsB",
        "replyto": "b5LJVjwOsB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_WGBd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2664/Reviewer_WGBd"
        ],
        "content": {
            "summary": {
                "value": "This paper presents VTranM, an explanation method for Vision Transformers that addresses the limitations of current explanation methods. While Vision Transformers draw representations from image patches as transformed vectors and integrate them using attention weights, current explanation methods only focus on attention weights without considering essential information from the corresponding transformed vectors. To accommodate the contributions of transformed vectors, the authors propose VTranM, which leverages a vector transformation measurement that faithfully evaluates transformation effects by considering changes in vector length and directional correlation. Furthermore, they use an aggregation framework to incorporate attention and vector transformation information across layers, thus capturing the comprehensive vector contributions over the entire model. The authors demonstrate the superiority of VTranM compared to state-of-the-art explanation methods in terms of localization ability, segmentation, and perturbation tests. Their experiments show that VTranM produces more accurate explanations in terms of both background regions and foreground objects. Overall, this work provides contributions to the field of vision transformers by introducing an explanation method that can improve the interpretability and transparency of Vision Transformers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work introduces an explanation method, VTranM, that addresses the limitations of current explanation methods for Vision Transformers. The proposed vector transformation measurement and aggregation framework improve the performance of visualization. \n\n- The authors demonstrate the superiority of VTranM compared to state-of-the-art explanation methods in terms of localization ability, segmentation, and perturbation tests. Their experiments show that VTranM produces more accurate explanations in terms of both background regions and foreground objects.\n\n-  The authors conduct a comprehensive evaluation of VTranM, including qualitative and quantitative analyses, ablation studies, and comparisons with baseline methods. This evaluation provides a thorough understanding of the strengths and weaknesses of VTranM."
            },
            "weaknesses": {
                "value": "- The improvement is very limited. From Tables 1 and 2, the improvement is marginal compared to Trans. MM.\n\n- Lack of analysis of the proposed method. What if the proposed module is applied to only one of these transformer blocks, with different positions? How will it affect the results?\n\n- Lack of interpretability. While the proposed method provides more accurate explanations than baseline methods, it is still unclear how well the explanations reflect the underlying decision-making process of the model. \n\n- Missing discussions with [a, b]\n\nMissing References:\n[a] MemNet: A Persistent Memory Network for Image Restoration\n[b] IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers"
            },
            "questions": {
                "value": "- How is the proposed method compared to DINO-v1/-v2 [c, d]? It seems that the visual results are much worse than DINO-v2. The self-supervised training method could be also seen as another explanability framework if its linear head is finetuned on the target dataset.\n\nReferences:\n[c] Emerging Properties in Self-Supervised Vision Transformers\n[d] DINOv2: Learning Robust Visual Features without Supervision"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2664/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699258272243,
        "cdate": 1699258272243,
        "tmdate": 1699636206766,
        "mdate": 1699636206766,
        "license": "CC BY 4.0",
        "version": 2
    }
]