[
    {
        "id": "Cib5XY3Jtq",
        "forum": "d18RgYF6Y7",
        "replyto": "d18RgYF6Y7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_6fEn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_6fEn"
        ],
        "content": {
            "summary": {
                "value": "In fair classification, researchers tend to intervene during pre-processing, training or post-processing. The paper examines how to develop a fair classifier when sensitive attributes are not included in training data and fairness constraints are not employed. The main contributions are theoretical. The theory shows that implementing a distribution shift during pre-processing improves model generalization on average as well as fairness. Building on previous work employing influence functions, the paper suggests sampling influential examples during training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The idea that better generalization error might also improves fairness performance is an important area of theoretical fairness. There are recent and upcoming papers in a similar vein in applied ML but not as many theory papers\n\n(2) Theorem 3.2 on the upper bound of \"fairness disparity\" is mathematically interesting and seems to be the main result"
            },
            "weaknesses": {
                "value": "(1) Implementing distribution shift as a way to improve generalization is already known in studies of adversarial training and reliable deep learning, but the paper does not engage with this existing literature\n\n(2) Exposition needs improvement throughout for clarity. For instance, the paper states \"Our theoretical analysis indicates that training on datasets with a strategically implemented distribution shift can effectively reduce both the upper bound for fairness disparity and model generalization error (Lemma 3.1, Theorem 3.2). This gives us a key insight that fairness and accuracy can be improved simultaneously even with simply traditional training. [Section 3]\" How is training on datasets with a strategic distribution shift simple traditional training? And for the second bullet in contributions, the paper states \"we sample influential examples\" but does not clarify what influence function is used throughout the paper. I see section 4.1.2 but it is not clear to me how this is distinct from the previous definitions in the literature.\n\n(3) The paper over-emphasizes that sensitive attributes are not used during training, which is pretty standard practice in fair classification. Sure, maybe for post-processing techniques but this particular paper is presenting a technique for pre-processing/during training adjustments. Further, even in work on leave-one-out fairness or influence functions and fairness (analysis of perturbing training data in some way), sensitive attributes are not typically used in training. I notice that these are also areas of related work that the paper does not engage with.\n\n(4) Related work section needs improvement. The section splits into pre-processing methods and post-processing methods which is not helpful for the main contribution of the paper"
            },
            "questions": {
                "value": "1. What is the relationship between the strategic distribution shifts and adversarial training?\n\n2. What is the novelty in section 4? Which parts are restating results from the literature and what parts are new?\n\n3. What is the motivation for employing the distribution shift for this reason? Why is a particular type of shift used? And again, how is this different from adversarial or influence function informed training?\n\nminor:\n4. Why are there no captions in the tables?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677575359,
        "cdate": 1698677575359,
        "tmdate": 1699636489051,
        "mdate": 1699636489051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wlrZ4ogrUT",
        "forum": "d18RgYF6Y7",
        "replyto": "d18RgYF6Y7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_B8Mi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_B8Mi"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on addressing the challenge of training fair classifiers with limited number of labeled training samples (without access to sensitive attributes) and an unlabelled dataset. Instead of relying on in-processing fairness constraints due to the lack of sensitive attributes and to avoid accuracy-fairness tradeoff, the authors make a theoretical observation that traditional training on dataset with carefully induced distribution shift can decrease the upper bound of fairness disparity and model error. Based on this observation, the authors propose Fair Influential Sampling (FIS) algorithm. FIS leverages the unlabeled dataset to query examples that help in shifting the training data distribution to maximize fairness without compromising accuracy. The effectiveness of FIS is demonstrated empirically using four datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem setting is relevant and interesting.\n- The paper addresses the issue of mitigating unfairness without access to sensitive information on the training dataset."
            },
            "weaknesses": {
                "value": "- There is a large body of work to improve fairness without sensitive attributes on training data [1, 2] and also on validation data [3, 4, 5]. Is there a reason why the authors did not compare the performance of FIS against such methods? \n- The related works section lacks discussion about fair active learning frameworks like [6]. In addition, the authors should distinguish the proposed framework from existing fair active learning frameworks and perform an empirical comparison. \n- The paper is missing a comparison with in-processing fairness training algorithms, such as MinDiff [7]. Such a comparison would provide a clearer perspective on any accuracy-fairness trade-offs advantages that FIS may entail.\n- How are the hyper-parameters, such as number of new examples in each round r, number of rounds T, tolerance $\\epsilon$, etc., of the training algorithm determined? This question arises because the selection of hyper-parameters appears to be a significant challenge when training fair models, as reported in [2, 4].\n- To gain a comprehensive understanding into the effectiveness of the proposed algorithm, it is imperative to conduct a sensitivity analysis that explores the relationship between the labeling budget and the (accuracy, fairness).\n\n[1] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. \"Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations.\" International Conference on Machine Learning. 2022.\n[2] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.\n[3] Lahoti, Preethi, et al. \"Fairness without demographics through adversarially reweighted learning.\" Advances in neural information processing systems 33 (2020): 728-740.\n[4] Veldanda, Akshaj Kumar, et al. \"Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access.\" arXiv preprint arXiv:2302.01385 (2023).\n[5] Sohoni, Nimit, et al. \"No subclass left behind: Fine-grained robustness in coarse-grained classification problems.\" Advances in Neural Information Processing Systems 33 (2020): 19339-19352.\n[6] Anahideh, Hadis, Abolfazl Asudeh, and Saravanan Thirumuruganathan. \"Fair active learning.\" Expert Systems with Applications 199 (2022): 116981.\n[7] Prost, Flavien, et al. \"Toward a better trade-off between performance and fairness with kernel-based distribution matching.\" arXiv preprint arXiv:1910.11779 (2019)."
            },
            "questions": {
                "value": "Please see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Reviewer_B8Mi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852397367,
        "cdate": 1698852397367,
        "tmdate": 1699636488968,
        "mdate": 1699636488968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T4Z7TuYYIJ",
        "forum": "d18RgYF6Y7",
        "replyto": "d18RgYF6Y7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_UQLS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_UQLS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the fairness problem with a sampling strategy. From the motivation that changing the data distribution would help with the fairness issue, the authors propose to estimation the influence of sample via first-order gradient approach, and re-weight samples individually. Experiments are conducted on images, tabular data, and language to demonstrate their approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed algorithm is technically sound and straightforward. No access to the sensitive attribute of training data is a good property to have.\n\n2. The presentation is clear and easy to follow.\n\n3. The experimental datasets cover multiple types of data, which is good to have."
            },
            "weaknesses": {
                "value": "1. Key reference missing. In [1], the authors also proposed a sampling/reweighing strategy to select good samples based on influence estimation. The algorithm has no access to the sensitive attributes of training data. The two paradigms seem conceptually and technically similar to me. Some discussions are needed in this paper.\n\n2. Weak connection between the theorem and algorithm. I understand the theorem served as a very high-level motivation to change the distribution shift, but from my perspective, it has limited connections to the specific algorithm later on. Also the theorem is closely related to the theorems in domain adaptation established several years ago, so that somehow to be incremental.\n\n3. Since the influence is based on first-order estimation, if there any chance to validate the estimation of influence per sample? Maybe show the actual influence and its estimation would be helpful.\n\n4. The authors consider the accuracy in the algorithm. However, in the experimental section, I didn't see any numerical evaluation for model accuracy. Given the context, involving the model's accuracy and show the tradeoffs between fairness and accuracy make more sense to me.\n\n[1] Achieving Fairness at No Utility Cost via Data Reweighing with Influence, ICML'22"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Reviewer_UQLS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698874400512,
        "cdate": 1698874400512,
        "tmdate": 1700431577875,
        "mdate": 1700431577875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b32g2U1RH4",
        "forum": "d18RgYF6Y7",
        "replyto": "d18RgYF6Y7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_wM5N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_wM5N"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new fair training method when the sensitive attributes are missing. The paper first provides theoretical analyses to show the upper bounds for the generalization error and fairness disparity of the model. For example, their theoretical observations show that the upper bound of the fairness disparity is affected by both distribution shifts and group bias in the data. Based on such analyses, the paper proposes a new sampling strategy that utilizes the influence information of each unlabeled sample to improve the fairness and accuracy of the model. The proposed algorithm is tested on several datasets, including image and tabular datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper focuses on a realistic setting in model fairness, where the sensitive attribute labels are unavailable during the training.\n- The paper provides interesting theoretical analyses, including the upper bounds of generalization error and fairness disparity. \n- The paper uses various benchmark datasets, including both image and tabular scenarios, helping to show the multiple applications of the proposed algorithm."
            },
            "weaknesses": {
                "value": "- The paper needs to clarify the connection between this paper and other related works in the fairness literature.\n  - For example, one of the important discussions in the paper is about the accuracy-fairness tradeoff, which also affects the main proposed algorithm. However, such a tradeoff between accuracy and fairness has been widely studied (e.g., [1]), so it would be better if the paper could clarify what is the difference between this paper\u2019s analysis and previous discussions in the fairness literature.\n  - Also, the paper uses the concept of distribution shifts, which is recently extensively studied in the fairness literature, but the paper does not discuss those works (e.g., [2, 3, 4]). It seems the setups of this paper and recent distribution shifts studies in fairness literature are a bit different, as this paper aims to \u2018utilize\u2019 an appropriate distribution shift for sampling, while many recent studies focus on \u2018solving\u2019 the distribution shifts between training and target distributions for fair training. Thus, it would be much better if the paper could clarify the connection and differences between this work and other distribution shifts studies.\n- The paper does not compare with enough baselines in their experiments. For example, there are various algorithms for training fair models without using sensitive attribute information (e.g., [5, 6]), but the paper does not include clear comparisons with those works.\n\n-------------\n[1] Menon and Williamson, The cost of fairness in binary classification, FAT* 2018.\n\n[2] Singh et al., Fairness Violations and Mitigation under Covariate Shift, FAccT 2021.\n\n[3] Roh et al., Improving Fair Training under Correlation Shifts, ICML 2023.\n\n[4] Giguere et al., Fairness Guarantees under Demographic Shift, ICLR 2022.\n\n[5] Hashimoto et al., Fairness without demographics in repeated loss minimization, ICML 2018.\n\n[6] Lahoti et al., Fairness without demographics through adversarially reweighted learning, NeurIPS 2020."
            },
            "questions": {
                "value": "The key questions are included in the above weakness section.\n\n-------------------\n[After rebuttal] I read both the responses and the revised paper. As most of my concerns have been resolved, I raised my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5007/Reviewer_wM5N"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699021053765,
        "cdate": 1699021053765,
        "tmdate": 1700739116398,
        "mdate": 1700739116398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b9B5auNJq9",
        "forum": "d18RgYF6Y7",
        "replyto": "d18RgYF6Y7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_FJTt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5007/Reviewer_FJTt"
        ],
        "content": {
            "summary": {
                "value": "This paper studies learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Its analysis indicates that training on datasets with a strategically implemented distribution shift can effectively reduce both the upper bound for fairness disparity and model generalization error. It also proposes the sampling algorithm FIS to sample influential examples from an unlabeled dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Sampling influential examples from an unlabeled dataset based on the combined influences of prediction and fairness is interesting. \n2. The theoretical analysis on the upper bound of the fairness disparity is provided.\n3. The experiments on three datasets demonstrate the proposed algorithm is useful."
            },
            "weaknesses": {
                "value": "1.\tThe definition 3.1's symbolism is not clear, are $P$ and $Q$ the same as preliminaries? Why use the model trained on P instead of that trained on Q? Could you give more explanation?\n2.\tAssumption 3.2 seems a bit strong. The assumption before Lemma 3.1 that the loss is bounded is not common. Could you give more justification to these assumptions?\n3.\tIn the first paragraph in Sec 4.1.1, the assumption of an auxiliary hold-out validation dataset is too strong. For my understanding, test data means that we don\u2019t know the distribution of the data. So I am not sure the reasonability of the assumption. \n4.\tAlthough it states the computation cost of the proposed algorithm is low, it seems the algorithm needs to pre-calculate the loss for testing the performance of a sample, which is costly.\n5.\tIt lacks discussion of how to select the initial training dataset for the warm start (influence when applying different proportions or distributions), and how to determine the solicitation budget $r$ which is the declared a small number of sampling data to gain a better result (both accuracy and fairness).\n6.\tRegarding the experiments, the baselines are not sufficient.\n\nMinors:\n1.\tThe symbol used in paper should be unified. Notion of Q and P are not used consistently in Sec 3 and 4.\n2.\tIn the proposed algorithm section, the proposed strategy I or II should be in Line 6, and the calculation of prediction's and fairness's influences should be in Line 7, 8.\n3.\tTypo: That -> that in paragraph before Def. 3.2"
            },
            "questions": {
                "value": "1.\tIt is not clear about the statement \"an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error\". I think the Theorem 3.2 tells us that no distribution shifts will help lead to the smaller generalization error bound, and a smaller shift leads to smaller error (straightforward). \n2.\tIn Eq. 1, if $f$ is a classifier, then $x$ seems to be the feature instead of original data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699450387587,
        "cdate": 1699450387587,
        "tmdate": 1699636488701,
        "mdate": 1699636488701,
        "license": "CC BY 4.0",
        "version": 2
    }
]