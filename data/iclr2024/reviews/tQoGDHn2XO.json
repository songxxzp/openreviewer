[
    {
        "id": "6PrWxiO1hA",
        "forum": "tQoGDHn2XO",
        "replyto": "tQoGDHn2XO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_WVYX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_WVYX"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a multi-fidelity BO method for HPO optimization with adaptive fidelity identification. The authors use a GP as a surrogate model combined with EI as an acquisition function. The authors use a combination of learning curve models to estimate the fidelity budget that accurately represents the performance of the hyperparameter configuration (efficient point) based on the partial observations obtained from a warmup phase. In the end, the top $k$ configurations are trained until their saturation points.\n\nThe authors provide results on 10 datasets out of 3 diverse benchmarks (LCBench, NB201, FCNet). The authors additionally apply their algorithm components in other non-multi-fidelity surrogate models and present the improvement in results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work has a good structure and is well-written.\n- I found the application of the proposed algorithm on methods that do not support multi-fidelity optimization particularly interesting."
            },
            "weaknesses": {
                "value": "- The coverage of related work is lacking, in particular, the authors have missed two related multi-fidelity model-based baselines that feature a dynamic schedule:\n    \n    \n    **DyHPO** [1], a deep GP kernel multi-fidelity method that does not follow a SHA schedule, but a dynamic one, moving one step at a time and predicting the performance for the next step of different hyperparameter configurations.\n\n    **DPL** [2], a deep ensemble that conditions the predictions to follow a power law formulation, also following a dynamic schedule, moving one step at a time, however, in contrast to DyHPO it predicts the final validation performance for every configuration and pushes the predicted best configuration with one step.\n\n- The paper advocates that it is able to adaptively decide the fidelity for which every configuration should be evaluated, however, it incorporates fixed design choices, e.g $w$, the number of warmup steps necessary to have for every learning curve is 20% of the total curve (Although the authors have a heuristic that can stop the warmup phase if no improvement is observed).\n- The combination of a surrogate model together with LC prediction for an individual hyperparameter configuration can be inefficient from my perspective.\n\n[1] Wistuba et al. \"Supervising the multi-fidelity race of hyperparameter configurations.\" Advances in Neural Information Processing Systems 35 (2022): 13470-13484.\n\n[2] Kadra et al. \"Power Laws for Hyperparameter Optimization.\" Thirty-seventh Conference on Neural Information Processing Systems (2023)"
            },
            "questions": {
                "value": "- **In related work:** PAHSA -> PASHA\n- **In related work (Multi-Fidelity setting):** The best performance does not necessarily need to be at $r_{max}$ for the incumbent configuration, it could also be at an intermediate fidelity level. The proposed target of optimization is not correct and the rest of the paper is based on that.  As the authors note later on: \"However, we observe that an increase in fidelities does not always result in performance improvement.\"\n- **In section 4.2:** $w$ is used, although it is defined only in 4.3.\n- Unless I missed it, the surrogate model is referred to as $M$, however, no information is given on what the surrogate model is, until Section 5 with **FastBO uses a Matern $\\frac{5}{2}$ kernel**. I believe it should be clearly described in the manuscript that $M$ is a GP.\n- **\"It is worth noting that the additional time required is factored into the overall time.\"** -> Is the benchmark configuration evaluation time, and the warmup stage included in the wall-clock time? Is the wall-clock time multiplied by 4 since there are 4 parallel workers?\n- Why are there only 10 datasets selected from LCBench, FCNET, and NB201? What was the criterion used for the selection? \nLCBench alone has 35 datasets.\n- Could the authors provide a summary plot of the ranks that all the methods achieve in the different benchmarks?\n- Could the authors elaborate on whether the difference in results is statistically significant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5572/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5572/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5572/Reviewer_WVYX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698098199765,
        "cdate": 1698098199765,
        "tmdate": 1699636573307,
        "mdate": 1699636573307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YovxAnK7nI",
        "forum": "tQoGDHn2XO",
        "replyto": "tQoGDHn2XO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_ZiMT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_ZiMT"
        ],
        "content": {
            "summary": {
                "value": "Multi-fidelity optimization represents the current state-of-the-art in hyperparameter optimization. The concept behind it is to terminate the evaluation of poorly performing hyperparameter configurations early in order to allocate more resources to promising configurations. A crucial element of this approach is to model the learning curve, i.e validation performance after each epoch, of a hyperparameter configuration.\n\nThe paper introduces a novel multi-fidelity strategy that aims to predict key points of a learning curve. After some initial observations, the proposed method determines the so-called efficiency point, where performance improvements start to flatten out. The evaluation of this hyperparameter configuration is then continued until this efficiency point. Furthermore, for each hyperparameter configuration the saturation point is computed, which is the epoch level where the learning curve is not improving anymore. After optimization, the top-k hyperparameter configurations are evaluated until this saturation point."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall, I found the paper to be well-written and easy to follow. Additionally, I think it nicely captures the relevant literature. The proposed method is well-motivated, and the various steps are convincing.\n\n- The proposed method outperforms state-of-the-art methods across a range of benchmarks from the HPO literature. However, I do have some questions regarding these results (see below)."
            },
            "weaknesses": {
                "value": "- The method introduces additional hyperparameters \\delta_1, \\delta_2. These are not simple to interpret and hence might be difficult to set a-priori in practice. It would be also great if the paper could comment how important it is to set these values correctly.\n\n\n\n- The proposed learning curve model appears to be quite similar to that of Domhan et al., with the key difference being that it does not consider the epistemic uncertainty of the individual models. In contrast, Domhan et al. employed a Bayesian approach to obtain meaningful uncertainty estimates for their predictions. While the paper argues that Domhan et al.'s method is primarily designed for the highest fidelity, I don't see a reason why it couldn't also be used to predict the efficient or saturation point."
            },
            "questions": {
                "value": "- The results seem somehow different to the results reported by Salinas et al. For example A-CQR performs worse than A-BOHB on the LCBench benchmark. Did you follow their experimental setup? If not, what has changed and why?\n\n\n- Did you change the latex template? It feels like the margins are smaller. If so, please make sure to follow the original template.\n\n- Do you factor in the post-processing time into your runtime plots?\n\n\n- Figure 2: I am somewhat surprised that FastBO outperforms ASHA-based methods so rapidly. My understanding is that FastBO assesses each configuration for at least 20% of the maximum number of epochs, which corresponds to 20 epochs on the FCNet benchmarks and 40 epochs on the NASBench202 benchmark. The smallest resource level for ASHA-based methods is typically set to 1 epoch, which implies that it should be able to evaluate a considerably greater number of configurations than FastBO, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698415404819,
        "cdate": 1698415404819,
        "tmdate": 1699636573209,
        "mdate": 1699636573209,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lqdmy7Li5e",
        "forum": "tQoGDHn2XO",
        "replyto": "tQoGDHn2XO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_gzF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5572/Reviewer_gzF7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes hyper-parameter optimization in which different fidelity of observations are incorporated. The proposed method approximates the learning curve by combining three simple parametric models, and adaptively determine the appropriate stopping point for each observation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The basic idea is clear and the proposed method would be simple to implement."
            },
            "weaknesses": {
                "value": "Overall, the proposed method consists of several simple heuristics whose justification as a general methodology is not fully clear, and for me, it is somewhat difficult to see significant technical contributions in the paper.\n\nThe model of the learning curve is too simple. I do not agree with that these three functions have enough flexibility to handle a variety of shapes of learning curves. In particular, learning curves can have non-convex shapes. In this case, it seems almost impossible to obtain accurate future prediction from warm-up small amount of observations.\n\nMost of well-known BO acquisition functions such as EI, UCB, knowledge gradient, predictive entropy search, and max-value entropy search. A lot of multi-fidelity counter-part has been proposed. The authors ignored most of them."
            },
            "questions": {
                "value": "Appropriate hyper-parameter values for alpha, delta_1, and delta_2 should depend on a given dataset. How can the current setting be justified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699006352955,
        "cdate": 1699006352955,
        "tmdate": 1699636573095,
        "mdate": 1699636573095,
        "license": "CC BY 4.0",
        "version": 2
    }
]