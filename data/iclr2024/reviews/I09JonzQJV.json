[
    {
        "id": "fRhNGaTEP7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_HGLj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_HGLj"
        ],
        "forum": "I09JonzQJV",
        "replyto": "I09JonzQJV",
        "content": {
            "summary": {
                "value": "The authors consider the problem of learning a predictive model under the long-term counterfactual fairness constraint where the features of each individual are affected by the prediction. This problem is very important in fairness community, though few causality-based methods have been developed. The proposed fairness criteria seem sound. However, the proposed learning framework seems a bit weak, mainly due to the strong functional assumptions on the underlying structural causal models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors address a very important problem setting.\n- Few causality-based methods have been developed in this problem. The only work I know is [Hu+; AAAI2022], which is already cited in this paper. \n- The proposed fairness criteria (Definitions 2.1 & 3.2) seem reasonable."
            },
            "weaknesses": {
                "value": "(A) Formulation of response function $r$\n\nWhereas [Hu+; AAAI2022] use time series data to infer how the feature attributes dynamically change depending on the prediction, this paper makes the functional assumptions about this change, using the existing strategic classification model in ML community. This raises several questions to me:\n\n- **Violation of the SCM definition**: The authors seem to consider the cases where the exogenous variables $U$ are also affected by prediction $\\hat{Y}$ (e.g., real-world experiments in Sec. 4.2). However, this contradicts the definition of structural causal model (SCM). As the authors clearly noted in Section 2, in SCM, unobservable (exogenous) variables \"are not caused by any variable in $V$\". Since the prediction $\\hat{Y}$ is usually modeled as one of the endogeneous variables $V$, this completely confuses me. If the authors consider the reduced form of the SCM, then it might be OK. However, for instance, in Example 2.3, each structural equation in the SCM seems to be given as the structural form (i.e., the inputs contain $V$), despite the fact that $U_1'$ and $U_2'$ are exogenous variables. I am not sure whether such an SCM is properly defined. No remark is given for this.\n\n- **Parameter $\\eta$**: I am not sure about strategic classification model $r$ at the beginning of Section 3, but is $\\eta$ a hyperparameter? This seems a very important factor because it defines how the feature attributes change. If it can be estimated from the data, how can we achieve it? \n\n- **The motivation of response function $r$**: Are there any other function forms for strategic classification models in ML community? If so, what are the advantages and disadvantages of the assumed models?\n\n(B) The proposed learning framework seems ineffective\n\nThe authors make restrictive assumptions about the function form of the structural equations (e.g., the linear SCM for feature attributes $X$). I believe that it is OK because this paper is a pioneer work for a novel and important problem setting. However, despite these assumptions (i.e., Eqs. (5) and (6)), the proposed method only addresses parameter $p_2$ and $p_3$ in Eq. (6) and cannot optimize the second-order coefficient $p_1$, and authors explain that it is a hyperparameter. \n\n- **Unlearnable parameter $p_1$**: The optimal value of $p_1$ is given using parameter $\\eta$ in $r$. This goes back to my question in (A): Is $\\eta$ a hyperparameter? How is it determined? Can the proposed method optimize both $\\eta$ and $p_1$?\n\n- **The form of distribution $P(U)$**: Does the proposed method require the knowledge about $P(U)$? \n1. There is no description about this in Theorem 3.1. I am not sure, but is the SCM in Eq. (5) deterministic? \n2. I believe that performing MCMC at line 2 in Alg. 1 requires the form of $P(U)$. In practice, how can we obtain it? Although the real-world data experiments in Sec. 4.2 seem to consider the cases where it is available, it is usually unknown, isn't it?\n\n(C) Clarity issues\n\nAuthors nicely explain the motivation of this work using several examples. I recommend elaborating these examples:\n\n- Example 2.2 is a very important example for illustrating the importance of long-term fairness. However, it seems a bit unclear. \n1. Please clearly illustrate what features $A$ and $X$ can be considered in this example. For instance, is a credict score included in $X$? \n2. The sentence connection between \"Assume that ...\" and \"However, in the counterfactual world, ...\" is unclear. Why is he/she not qualified? Please elaborate more.\n\n- Example 2.3 is not intuitive. The description in Footnote 2 is correct if the SCM is given. In usual, however, since the SCM is unknown, it is difficult to imagine how we can construct the prediction model only with unobservable inputs $U$.\n\n\n(D) Other minor comments\n\n- In Section 2.2, \"the prediction $\\hat{Y}$ in the factual world should be the same as that in the counterfactual world ...\" is not exactly correct because the counterfactual fairness is defined via marginalization over $U$ and hence is probabilistically defined, as stated in the first equation in Sec. 2.2.\n\n- Please clearly state that Theorem 3.1 considers the causal graph structure in Figure 1 (b). In particular, there is no definition of $U_1, ..., U_d$. Please clearly claim the assumption that these exogenous variables do not include unobserved confounders, i.e., $X_i \\leftarrow U_i \\rightarrow X_j$ ($i \\neq j$).\n\n- Add \"Assume that\" or \"Suppose that\" to the second sentence in Theorem 3.1."
            },
            "questions": {
                "value": "See all questions in (A) and (B)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Reviewer_HGLj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6635/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697433958256,
        "cdate": 1697433958256,
        "tmdate": 1700646657502,
        "mdate": 1700646657502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AAqpY8yEk8",
        "forum": "I09JonzQJV",
        "replyto": "I09JonzQJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_VAjK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_VAjK"
        ],
        "content": {
            "summary": {
                "value": "This paper revisits counterfactual fairness (CF) by Kusner et al. 2017 by considering the ML model\u2019s impact on the behavior of the individual in the long-term. Under CF, we only consider a static setting and whether the ML model\u2019s predictions $\\hat{Y}$ are counterfactually fair. Here, the work considers the case where a counterfactually fair model induces a future outcome $Y\u2019$ (the outcome $Y$ is used for training the model) that is not counterfactually fair. It introduces the fairness notion of lookahead counterfactual fairness (LCF) and proposes an algorithm to insure it for a given ML model. To do so, it assumes strategic agents. The paper tests and compares LCF to CF on two datasets, including Law School Success from Kusner et al. 2017.\n\nAlthough the topic is relevant, the problem formulation is unclear and (I suspect) at odds with counterfactuals as described by Pearl, meaning non-backtracking or interventional counterfactuals. It needs a stronger and clearer formalization, though I encourage the authors to continue with this line of work. See my comments below: I\u2019ll use S to denote strengths, W to denote weaknesses, and O to denote opportunities for improvements.\n\nI'm willing to increase my score if the authors address my questions/concerns."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "S1: Evaluating the long-term effects (or, overall, the impact of a deployed model\u2019s prediction) is an extremely important topic. The authors are correct to raise concerns around how having a counterfactually fair model might not mean anything if we don\u2019t consider how individual will reach to that model when deployed.\n\nS2: Similarly, formulating LCF as an in-processing step is interesting as it forces us to train models that are forward looking."
            },
            "weaknesses": {
                "value": "O1: Based on the title, I thought it would be a work on counterfactual fairness involving a human (i.e.,  AI-Human interactions). It\u2019s unclear to me how the human is in the loop in this work -- also, there\u2019s a typo in the title: Counterfactual Fairness with a Human in the Loop. I suggest changing the title to avoid confusion, maybe something along the lines of \u201cLookahead Counterfactual Fairness\u201d or \u201cCounterfactual Fairness under Strategic Classification\u201d.\n\nO2: Example 2.1 is welcome, as many causal fairness papers take for granted counterfactual generation, and such a section can help unfamiliar readers. I\u2019d suggest improving it by being explicit about the abduction, action, and prediction steps.\n\nO3: From (1), it seems causal sufficiency is assumed for the paper, no? Please clarify if so, and whether it\u2019s a limitation to the problem formulation. Especially in this setting where the authors are considering time (though this somewhat unclear as well) and the risk of hidden confounders increases.\n\nW1: In the motivating example (Example 2.2: starting from \u201cAssume that this person\u2026\u201d), it\u2019s unclear how there is a counterfactual world not captured by the causal graph used for CF. The counterfactual instance generated there to test for counterfactual fairness is specific to the SCM available or assumed (i.e., the worldview). That same counterfactual should be, in principle, the closest possible world to what is observed, no? How is this individual not qualified in the counterfactual world? Are you implying model bias?\n \nI understand that (correct me if I\u2019m wrong) this individual is unlabeled, but somehow we know that in his/her the factual world $Y=1$ and in the counterfactual world $Y^{CF}=0$, while the model being studied for counterfactual fairness gives $\\hat{Y}=\\hat{Y}^{CF}=1$. This reasoning I understand, but it\u2019s unclear how and from where this additional information is available. This is important because this ambiguity appears also in the formalization of LFC.\n\nW2: I\u2019m not sure if strategic classification is suitable as a framework for LFC. Under strategic classification, individuals essentially are assumed/expected to cheat: i.e., to lie about a hidden information such that the model changes its decision. Hence, why we train the model to account for such strategic responses. How does this translate into the fairness setting? Are the individuals improving over time or are they in fact just cheating? If so, then the present problem formulation is just strategic classification but with the consideration of counterfactual fairness. Please clarify if I\u2019m wrong.\n\nTo me this is clear when looking at the operationalization of LFC, which is based on changing the latent variables $U_i'$ (Section 3: response function for $U_i\u2019$). Under counterfactual fairness, the $U$\u2019s are often interpreted as inherent but unobserved skills to the individual (like aptitude). These are given and materialize through the observed variables (like SAT scores). These variables, though, are exogenous to the system. We can infer them using the abduction step, but how exactly is an individual basing his/her response on a variable that is out of his/her control?\n\nThe formulation in Section 3 seems like a backtracking counterfactual formulation where we imagine a setting were the individual can update the exogenous variables (see Sander et al.\u2019s Backtracking Counterfactuals). The issue here then is that counterfactual fairness is built on non-backtracking counterfactuals, meaning once the exogenous variables are set, all interventions occur in the observable space. Therefore, I don\u2019t understand the second term in the response function for $U_i\u2019$.\n\nFurther, I\u2019d argue this misconception is due to using the strategic classification framework: these strategic agents are not strategic in that they improve because of $\\hat{Y}$ (as in their given aptitude $U_X$ changes and $X$ increases) but are strategic in that they lie about improving so that $\\hat{Y}$ changes (the aptitude $U_X$ cannot change, but they can lie about $X$ to give the impression that it does). This type of agent doesn\u2019t seem useful for what LFC aims to achieve.\n\nFor example, in the Implementation part for 4.2, the authors argue \u201cwe assume that individuals adjust knowledge $K$ in response to the prediction model $\\hat{Y}$\u201d. $K$ here is measured through GPA, LSAT, and FYA (as shown in Figure 4(a)) and is exogenously given to the SCM. How exactly can $K$ change under the current SCM? \n\nFurther, conceptually, what exactly does it mean for the individual to change $K$ when, according to the SCM, the individual has no other mechanisms to do so? The setup is incomplete here. Maybe, this is where the human-in-the-loop needs to appear, or an enhancement of the causal graph denoting, e.g., an exogenous intervention in the form of an educational program.\n\nW3: Overall, it\u2019s not clear what is the long-term fairness problem LFC addresses (as highlighted by the Law School Success example). What would help LFC is explicitly admitting $\\hat{Y}$ into its formulation. The authors claim that the model has a \u201cdownstream impact\u201d on the future outcome $Y\u2019$. Doesn't this imply $\\hat{Y} \\rightarrow Y\u2019$? Where is $\\hat{Y}$ in (4)? As it is setup, it states that $X$ and $A$ have (at least) some association to $Y\u2019$ but so does $\\hat{Y}$, no? Please clarify."
            },
            "questions": {
                "value": "See O3, and all Ws."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6635/Reviewer_VAjK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6635/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698481477862,
        "cdate": 1698481477862,
        "tmdate": 1699636757818,
        "mdate": 1699636757818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5dB5XP8DI2",
        "forum": "I09JonzQJV",
        "replyto": "I09JonzQJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_pE5E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_pE5E"
        ],
        "content": {
            "summary": {
                "value": "The paper considers fair prediction tasks where individuals can respond to predictions by changing their features, and extends the definition of counterfactual fairness to apply to the downstream future value of the outcome variable after this responsive feature change.\n\nThe paper provides some theory, describes how to train models to satisfy the new fairness criteria, and explores the use of its ideas on real and synthetic data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The practical problem involving strategic responses and (sequential) fairness over time is well motivated, interesting, and potentially impactful.\n\nThe paper is fairly thorough and clear given the space constraints.\n\nThe relaxation in Definition 3.2 is interesting, and perhaps could be made a bit stronger by requiring the future difference to be smaller by a certain pre-specified fraction (or addition amount)?"
            },
            "weaknesses": {
                "value": "1) It seems there is a strong assumption regarding no direct effects of A on Y, e.g. in the motivating Example 2.3 with Figure 1, and in the structural equations (5) used in Theorem 3.1 and in Algorithm 1. If this is a necessary assumption of the method it would be appear to be a severe limitation for any fairness fairness (apparently there is no \"direct\" discrimination?), and if it is not necessary then it should be relaxed.\n\n2) While the application problem is interesting, I think the extension of counterfactual fairness is somewhat incremental. For example, why not simply relabel $Y$ and treat $Y'$ as the actual response for the prediction task? The response function (3) and structural equation for $Y$ are fixed. The intermediate prediction of $Y$ is largely irrelevant, inconsequential since the fairness of $Y'$ is the deciding issue.\n\n3) The main text explores the case of linear causal models. The non-linear case would be more interesting for ICLR, and more generally applicable."
            },
            "questions": {
                "value": "Could the Example 3.2 be used to make a stronger case for why LCF cannot be considered a minor extension of CF (after renaming $Y$ and treating $Y'$ as the main outcome)?\n\nCould the assumption of no direct discrimination be relaxed? How would the algorithms need to be modified?\n\nCould the paper focus more on the non-linear case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The method has some limitations which I believe are not adequately discussed (see e.g. the first comment about assumed causal structure in the weaknesses section above). There is a risk of discriminatory applications if the limitations are not handled properly."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6635/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828838917,
        "cdate": 1698828838917,
        "tmdate": 1699636757692,
        "mdate": 1699636757692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IAbOoIJGCE",
        "forum": "I09JonzQJV",
        "replyto": "I09JonzQJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_ouSR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6635/Reviewer_ouSR"
        ],
        "content": {
            "summary": {
                "value": "This work considers a new setting by combining counterfactual fairness and strategic prediction. They propose a new fairness notion that considers the changes in features of users after getting feedback from the first stage. Then they show counterfactually fair predictors are not satisfying their proposed notion LCF. Then they show a simple predictor can satisfy LCF using predictions from the last stage and this predictor can be better than counterfactually fair predictors under the strong assumptions of the causal model. In experiment they verified those claims in the restricted settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-  Proposed a novel notion for counterfactual fairness under the strategic classification setting. This setting can be useful in some real-world applications.\n- Under restricted settings (strong assumptions on the causal models), this work provides theoretical results to show counterfactual fairness is not enough for their setting and a simple predictor can surely outperform counterfactually fair predictor.\n- They verified their theoretical results with experiments on two datasets."
            },
            "weaknesses": {
                "value": "- Identification of counterfactuals: The authors may want to make it clear if they only consider deterministic counterfactual (each individual has deterministic noise U) or probablistic counterfactual (each individual has a distribution of U). Note that without strong assumptions, it is often impossible to identify U, especially when U is probablistic. As U is the input of the LCF predictor g, U has to be identifiable from observational data to make the whole proposed framework work in practice.\n\n- Limited settings: The structural causal model is very restricted with linearity, no hidden variables and additive noise. It would be better to provide results similar to Theorem 3.1 with different assumptions on causal model. In addition, It would be better to discuss more generally under what conditions LCF can be violated when CF is satisfied. For example, what family of functions r will lead to this and what conditions the SCM has to meet.\n\n- Limited baselines and related work: The authors only compare their method with the counterfactually fair predictor from Kusner et al and their literature review is not comprehensive. However, there exists recent work in counterfactual fairness. The authors may want to compare with multiple different methods that aim to achieve counterfactual fairness such as [1,2,3].\n\n[1] Wu, Yongkai, Lu Zhang, and Xintao Wu. \"Counterfactual fairness: Unidentification, bound and algorithm.\" In Proceedings of the twenty-eighth international joint conference on Artificial Intelligence. 2019.\n\n[2] Xu, Depeng, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu. \"Achieving causal fairness through generative adversarial networks.\" In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. 2019.\n\n[3] Ma, Jing, Ruocheng Guo, Aidong Zhang, and Jundong Li. \"Learning for Counterfactual Fairness from Observational Data.\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1620-1630. 2023."
            },
            "questions": {
                "value": "- The authors might want to discuss why Def 3.2 compares Y' with Y instead of \\hat{Y}, given the counterfactual outcomes Y'_{A\\leftarrow a} and Y_{A\\leftarrow a} are not identifiable in general unless strong assumptions are. If there is a \\hat{Y} in the first step, what is the assumption about the relationship between \\hat{Y} and Y and why is there no \\hat{Y'} in the second step?\n\n- In Eq.(2), it is clear that P(U|O=o) is inferred from the observed data. However, the paragraph above it says replacing noise U with u, which means intervention on noise U, how is this possible?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6635/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032028210,
        "cdate": 1699032028210,
        "tmdate": 1699636757553,
        "mdate": 1699636757553,
        "license": "CC BY 4.0",
        "version": 2
    }
]