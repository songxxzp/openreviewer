[
    {
        "id": "N1jpZqiUZd",
        "forum": "TOE6N8dp4w",
        "replyto": "TOE6N8dp4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission756/Reviewer_B33T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission756/Reviewer_B33T"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors focus on the private synthetic text generation problem. The proposed approach is to fine-tune a publicly pre-trained LLM with DPSGD on the original private data and sample from the model to generate synthetic text dataset with privacy. The authors demonstrate that parameter efficient fine-tuning with DP yields high fidelity synthetic data via experiments on downstream classification tasks. The authors furthermore show that DP synthetic data can also be used to tune the hyperparameters of the downstream classifiers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The presentation is clear, which helps the reviewer to follow the work conveniently.\n* The problem is important, private synthetic version of a text dataset can be useful in many applications and the results demonstrate that DP fine-tuning is an effective approach.\n* It's really great that the authors paid significant attention to the pre-training dataset and made sure that it's disjoint from the private datasets in study.\n* The effectiveness of parameter efficient fine-tuning would be quite advantageous for applying this solution to LLMs efficiently."
            },
            "weaknesses": {
                "value": "* The main weakness of the paper is that although it follows a similar direction to the prior work (Bommasani et al., 2019; Yue et al., 2022; Putta et al., 2023; Mattern et al., 2022), the authors committed an injustice in their presentation and comparison with the prior work. These prior works present impressive results that show the effectiveness of generating high-fidelity synthetic text datasets with DP. The authors shockingly present as if the reverse is true that the prior work failed to obtain good fidelity synthetic data, which is unacceptable.\n\n* For the downstream tasks, the authors choose the binary classification problem (4-way for AGNews?), which does not let the utility of the approach to be seen for more challenging scenarios.\n\n* The authors obtain significant improvements with parameter efficient fine-tuning compared to full fine-tuning. However, this may be due to inadequate hyperparameter search for full fine-tuning as the prior work (Li et al., 2021) has a substantial work on comparing full fine-tuning vs. parameter efficient fine-tuning with DP and show that the two approaches are competitive. A more comprehensive empirical study may be required to convince the reviewer in this regard."
            },
            "questions": {
                "value": "1) The authors follow similar direction to the prior work (Bommasani et al., 2019; Yue et al., 2022; Putta et al., 2023; Mattern et al., 2022) and fine-tune a publicly pre-trained LM with DP and generate synthetic data samples. It seems to me that the difference from prior work (Putta et al., 2023) and (Mattern et al., 2022) is that the authors do not augment the training objective and the difference from prior work (Yue et al., 2022) is merely applying parameter efficient fine-tuning instead of full fine-tuning. Looking at the results of Table 1, the reviewer observes that there is 2-3% difference in performance between real and non-private (private) synthetic. Similar results were demonstrated in prior work as well (+ authors here use much larger 8B model compared to GPT2 series and prior work also consider multiclass classification). Therefore, can authors explain how they arrive to the statements such as \"Firstly, our results in Table 1 indicate that obtaining good fidelity non-private synthetic data is possible, contrary to the results reported in (Yue et al., 2022) and (Putta et al. (2023).\" The reviewer does not really see this contrary. Putta et al. (2023) reports 91.3 for non-private synthetic data, which seems to be not mentioned by the authors whereas the real achieves 93.7. The authors mention that \"Mattern et al. (2022) suggested a modification of the loss (prompt-mismatch loss, to discourage the generation of text inconsistent with the prompt, like generating a negative review when positive prompt was given).They performed experiments on IMDB dataset. When going from non DP synthetic data to eps = 3 synthetic data, authors report the 8% relative performance drop of downstream classifier. Our results suggest 7% relative drop.\" Furthermore, (Yue et al., 2022) also present similar findings with small performance gap between real and non-private (private) synthetic on more challenging multiclass classification (as the authors mention the results are not directly comparable) and using much smaller GPT2 series. Based on these, it's not clear to the reviewer how the authors can state \"Previous approaches either show significant performance loss, or have, as we show, critical design flaws.\" and represent the prior work in the lines of failure and claiming the paper with similar approach and results as success.\n\n2) Regarding prior work Yue et al. (2022), the authors state that \"Additionally, they found that the distribution of the data when conditioned on some features (e.g., domain, sentiment, category) did not reflect the real data distribution, and proposed augmenting the fine tuning process to try to satisfy this constraint.\" Can authors elaborate on the statement \"distribution of the data when conditioned on some features (e.g., domain, sentiment, category) did not reflect the real data distribution\" and point where this is observed? Also can authors elaborate on \"augmenting the fine tuning process\"? Do they refer to the labels (domain, sentiment, category) being used in the prefix? The authors also follow the same approach and use the labels in the prefix. The authors randomly select example label during synthetic data generation but this would result in uniform label distribution in synthetic data, which may not match the real data distribution. The reviewer would appreciate elaborations on these points.\n\n3) How was the hyperparameter search performed for the comparison between full fine-tuning and parameter efficient fine-tuning? Have authors used the same comprehensive hyperparameter search for both approaches and still observed that parameter efficient fine-tuning is significantly better? The reviewer has some doubts on this because the prior work (Li et al., 2021) has comprehensive work on hyperparameter search and demonstrates that full fine-tuning and parameter efficient fine-tuning are competitive for many downstream tasks.\n\nAccording to the reviewer, the paper requires a significant revision regarding how the prior work is treated. The reviewer does believe that the paper has great contributions such as paying attention to the pre-training phase, using a larger-size 8B model, and parameter efficient training etc. along with promising results but the paper should do justice in their presentation and comparison with the prior work.\n\nMinor comments: \n* \"For each dataset we formulated a binary classification problem (sentiment classification) as the downstream prediction task.\" -> I suppose for AGNews this must not be the case because the dataset is for news topic classification which has 4 labels?\n* LoRa -> LoRA\n* Table 1 amd -> and"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Reviewer_B33T"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799852426,
        "cdate": 1698799852426,
        "tmdate": 1699636003008,
        "mdate": 1699636003008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bWpRj1UMw1",
        "forum": "TOE6N8dp4w",
        "replyto": "TOE6N8dp4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission756/Reviewer_gTGD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission756/Reviewer_gTGD"
        ],
        "content": {
            "summary": {
                "value": "The paper gives experimental results on using synthetic data that is generated via DP fine tuning of a pre-trained LLM. It is a systematic study, where various factors have been carefully taken into account: for example, contrary to the previous related works, the dataset which is used for fine-tuning is de-duplicated from the pre-training data (and the pre-training is actually carried out, i.e., no existing model weights are used). Three different fine-tuning techniques are compared: full fine-tuning of the model, LoRa fine-tuning where low-rank factorizations at the linear layers are added and prompt fine-tuning which trains a certain prompt tensor at the input layer. The quality is compared on downstream classification tasks with CNN and BERT models. Additionally, the general approach of using DP synthetic data for training the downstream model is compared to directly training the downstream model with the sensitive data. The DP synthetic data approach seems very competitive (of course has lot of benefits compared to training the downstream model with the sensitive data) and out of the DP fine-tuning techniques the LoRa seems to be the best. There are also positive results on the usefulness of using the DP sensitive data on hyperparameter optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Very clearly written paper on a timely topic, will be really helpful for anyone interested in DP LLMs, and also very accessible to wider audience as well.\n\n- Thorough, rigorous experiments where e.g. the de-duplication of the fine-tuning data from the pre-training data is carried out. I believe the results are very valuable and this can be a good reference for DP LLM studies.\n\n- Interesting finding: Impressive results on the classification accuracy of the downstream models trained with DP synthetic data. Using the LoRa fine-tuning, with $\\varepsilon=1$ one can obtain performance that is not far from the non-DP one."
            },
            "weaknesses": {
                "value": "- One weakness coming to my mind is the lack of novelty as there is not really anything new proposed in the paper. All the experiments are results of combining existing techniques. At the same time, I do think these are really valuable experimental results and the lack of theoretical novelties is not necessarily a problem.\n\n- I would have liked to see more about the computational costs of the experiments. I see there are some compute cost numbers in Appendix M, but would have been interesting too more detailed analysis. I find it interesting that fine-tuning the 20-40k parameter prompt tensor can lead to such impressive results, better than fine-tuning the full model and almost the same as fine-tuning the 20M LoRa parameters. Would have also been interesting to see about the memory requirements."
            },
            "questions": {
                "value": "- You write in Appendix M: \"Diferentially private finetuning of 8B model required between 20 hours (for shortest run on IMDB) and up to 80 hours for some of the longer runs. On the other hand prompt tuning required 1.5 hour for short run and up to 20 hours for longest runs.\"\n\nDoes this mean that LoRa fine-tuning had the same cost as the full fine-tuning of the model? I cannot see it said explicitly anywhere, what is the cost of DP-fine tuning the 20M parameter LoRa part of the model. I find it interesting that DP fine-tuning the 20-40k parameter prompt tensor gives so good results. What makes the cost then relatively high anyways, the forward pass through the 8B parameter pre-trained model? Is there any rule of thumb, how much does each part cost?\n\n- What kind of differences in the memory requirements are there between the different alternatives?\n\nMinor remarks:\n\n- Abstract: \"hyper parameter\"\n- Page 4: \"spliting\"\n- Section 4.2 : The paragraph on Prompt tuning felt bit of repetition since you have explained already in detail earlier\n- Section 4.2 : \" trainable rank decomposition matrices\" does not sound right, perhaps \" trainable low-rank matrices\" ?\n- Page 6: \"hyperparamter\"\n- Title of Appendix J.2: \"Hyperparmeter\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822156841,
        "cdate": 1698822156841,
        "tmdate": 1699636002928,
        "mdate": 1699636002928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c32PiKdSqj",
        "forum": "TOE6N8dp4w",
        "replyto": "TOE6N8dp4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission756/Reviewer_TdTq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission756/Reviewer_TdTq"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of generating synthetic text data that preserves the privacy of the original data and is useful for downstream tasks. The paper proposes to use a pre-trained large language model and fine-tune it with differential privacy on a sensitive dataset, using different parameter-efficient methods such as prompt tuning and LoRa tuning. The paper shows that the synthetic data generated by this approach is of high quality and can achieve comparable or better performance than directly training a downstream classifier with differential privacy on the real data.  The paper also demonstrates that the synthetic data can be used for other tasks, such as hyperparameter tuning of the downstream models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors conduct extensive evaluation and offer valuable empirical insights into DP synthetic text generation, such as highlighting the importance of prefix-LM that assigns zero weights to the prefix during training, random initialization for prompt tensors on DP prompt tuning, and the superior performance of LoRA compared to prompt tuning. \n- Additionally, the paper provides analysis of the synthetic data, such as the effects of synthetic data size, the rank correction of synthetic data for hyperparameter tuning, etc. These empirical findings provide a compelling and informative read.\n- The authors identify a critical issue regarding the overlap between pretrain data and finetuning data in some of the previous studies, underscoring the necessity to mitigate potential pitfalls in future research endeavors."
            },
            "weaknesses": {
                "value": "Novelty:\n- The novelty of the study may be limited, given that DP-SGD is a standard technique for DP synthetic text generation (Yue et al. 2022), and parameter-efficient fine-tuning has already been explored in DP LLM (Yu et al., 2021), albeit not directly applied to synthetic data generation.\n\n\nComparison to Yue et al. (2022): \n- The discussion and comparison with Yue et al. (2022) might be confusing to the readers. It would be helpful if the authors could clarify the difference between 'conditioning on some features' and 'augmenting the fine-tuning process.' mentioned in the Section 2 related work.  In Yue et al. (2022), labels are used in the prompt for conditional generation, with labels considered as non-private. Therefore,  there seems to be no \u201caugmentation\u201d during the finetuning process. This approach seems to be the same as the proposed method in section 4.1 of this paper, where the authors also use label names in the prefix as condition generation. \n- \u201cobtaining good fidelity non-private synthetic data is possible, contrary to the results reported in (Yue et al., 2022) and Putta et al. (2023)\u201d this statement may be confusing. Actually, Table 2 in Yue et al. (2022) shows a similar conclusion: synthetic data can outperform real data in terms of downstream model utility when both datasets are of the same size.\n\nDataset Choice:\n- While the authors acknowledge the presence of IMDB in Pile and perform deduplication accordingly, it might be more beneficial to directly use a dataset from an unseen domain, such as a medical dataset.\n\nDe-deplication:\n- \u201cwe used the suffix arrays to find common sequences of 50 or more tokens which appear in The Pile\u201d A justification for the choice of the hyperparameter value of 50 in the use of suffix arrays would be appreciated.\n- Could the authors elucidate why this de-duplication is \u201cstronger\u201d than simply removing the datasets from the Pile?\n\nDownstream Tasks:\n- All downstream tasks in the study are binary sentiment classification tasks, which may appear monotonous and simplistic. The utility of synthetic data for more complex downstream tasks, such as multi-class classification or classification tasks beyond sentiment, as considered in previous studies, remains unclear. \n\n\nPresentation and Interpretation of Results:\n- The interpretation of results under different metrics in Table 3 could be more clearly explained, particularly for readers who may not be familiar with RBO, Spearman, and Kendall metrics. For example,  how good or bad is the value 0.56 under RBO 25% metric for Bert trained on real data with $\\epsilon=\\infty$?\n- \u201c For n-gram statistics, we determine the frequency of unigrams, bigrams, and sample lengths in characters for both the original and synthetic datasets. \u201c While the authors use these statistics to calculate the rank correction, some qualitative visualizations, such as plots comparing the bigrams/length distributions of original and synthetic data, would be a valuable addition to better illustrate the similarity between the two datasets.\n\n\nTypos: \n- In section 6,  there is a missing space after the comma: \u201cmonitoring,and sharing,\u201d \u2013> \u201c monitoring, and sharing,\u201d"
            },
            "questions": {
                "value": "Please see my questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission756/Reviewer_TdTq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823572690,
        "cdate": 1698823572690,
        "tmdate": 1699636002850,
        "mdate": 1699636002850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iv31H7QGhj",
        "forum": "TOE6N8dp4w",
        "replyto": "TOE6N8dp4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission756/Reviewer_cvof"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission756/Reviewer_cvof"
        ],
        "content": {
            "summary": {
                "value": "The paper shows by pre-training large language models (LLMs) on public datasets and fine-tuning them on private datasets, LLMs can generate DP synthetic data with good quality. The key to success is to fine-tune only a small portion of parameters with LoRA or soft prompts. Experiments across three datasets show that downstream classifiers trained on the DP synthetic data can approach or even outperform downstream algorithms trained on the private data with DP directly."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The writing has good clarity.\n* The paper points out that in the common experimental setup in related work, the private data and pre-training data might overlap. It is an important issue that the community needs to pay attention to. \n* The results are promising."
            },
            "weaknesses": {
                "value": "* The paper downplays and misinterprets the contribution of prior work in several places. As a result, the contribution of the paper is overstated.\n* The proposed framework lacks novelty--the key components are already studied in prior work."
            },
            "questions": {
                "value": "My major concern is that the paper misinterprets the results from prior work and overstates its contribution. Some important prior work is not discussed or mentioned in the relevant place.\n\n* The paper repeatedly claims that prior work shows DP synthetic text results in a significant loss in downstream algorithms, e.g., \"Previous approaches either show signi\ufb01cant performance loss, or have, as we show, critical design \ufb02aws.\" in the abstract, and \"In similar vein, Yue et al. (2022) DP-\ufb01ne tuned pre-trained GPT models of various sizes. However their results suggest that even non-DP synthetic data results in a signi\ufb01cant drop in utility for a downstream classi\ufb01er.\" in Section 2. \n\n    On the contrary, Yue et al. (2022) show that DP synthetic yields good quality. It is clearly stated in the abstract of Yue et al. (2022): \"Through extensive empirical analyses, we demonstrate that our method produces synthetic data that is competitive in terms of utility with its non-private counterpart\" and across the paper.\n\n* The paper claims that Yue et al. (2022) \"found that the distribution of the data when conditioned on some features (e.g., domain, sentiment, category) did not re\ufb02ect the real data distribution, and proposed augmenting the \ufb01ne tuning process to try to satisfy this constraint.\" It would be great if the authors could clarify which results and which \"augmented \ufb01ne tuning process\" in Yue et al. (2022) are referred here.\n\n* The paper claims that in Yue et al. (2022), \"the conditional distributions are not private (i.e., they were not calculated in a differentially private manner).\" It would be great if the authors could clarify why and how the conditional distributions in Yue et al. (2022) are not private.\n\n* The paper claims that \"To the best of our knowledge, we are the \ufb01rst to demonstrate that parameter-ef\ufb01cient tuning performs better than full \ufb01ne-tuning when each is combined with DP\" in multiple places across the paper.  However, both of the parameter-efficient tuning approaches studied in the paper, prompt tuning and LoRa tuning, have already been proposed and explored in prior DP LLM literature. See [1] for LoRa tuning and [2] for prompt tuning. \n\n* Section 5.1 states that \"our results in Table 1 indicate that obtaining good \ufb01delity non-private synthetic data is possible, contrary to the results reported in (Yue et al., 2022) and Putta et al. (2023).\" As the paper itself explained in Section 5.1, the results of Yue et al. and the results of this paper are not comparable as they use different downstream tasks. This statement is confusing to readers.\n\n\nOther questions:\n* The data de-duplication only checks the suffix of the samples. If I understand it correctly, it does not detect duplications that happen in the middle of the samples?   \n* The paper shows that the classifiers trained on DP synthetic data can have even better downstream classification accuracy than the classifier trained on real data with DP. As explained in Section 5.1, it is because \"the private synthetic data bene\ufb01ts from massive amount of public data that was used for pretraining of the LLM\". While it is a nice result to have, it would also be informative to show results when both classifiers are exposed to the same amount of public information (e.g., by pre-training the BERT model on the same public data used for pre-training the LLM, and then fine-tuning the BERT model with either synthetic data or private data (with DP) as the classifier). This way, we can isolate the effect of public information and understand the true gap between training downstream classifiers with DP synthetic data and training downstream classifiers on real data with DP directly. \n* There are some missing numbers in Table 1. Why is that?\n* It would also be useful to show sample length distribution as in Yue et al.\n\n\nOther minor typos:\n* Introduction: a period is missing around \"... is bene\ufb01cial for synthetic data generation\".\n* Section 6: \"monitoring,and\" -> \"monitoring, and\"\n\nIn summary, given that the paper has many incorrect statements about prior work, the key techniques are already studied in prior work, and some of the key messages (e.g., parameter-efficient DP fine-tuning is better than full fine-tuning [1], generating more synthetic data than the size of the original dataset is helpful [3]) are already known in prior DP literature, I am suggesting a negative score.\n\n\n[1] Yu, Da, et al. \"Differentially private fine-tuning of language models.\" arXiv preprint arXiv:2110.06500 (2021).\n\n[2] Duan, Haonan, et al. \"Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models.\" arXiv preprint arXiv:2305.15594 (2023).\n\n[3] Ghalebikesabi, Sahra, et al. \"Differentially private diffusion models generate useful synthetic images.\" arXiv preprint arXiv:2302.13861 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831822950,
        "cdate": 1698831822950,
        "tmdate": 1699636002784,
        "mdate": 1699636002784,
        "license": "CC BY 4.0",
        "version": 2
    }
]