[
    {
        "id": "huc1YzwQJD",
        "forum": "9v5uZPWZoV",
        "replyto": "9v5uZPWZoV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_3cd6"
        ],
        "content": {
            "summary": {
                "value": "The proposes to use text-to-image generative models like Stable Diffusion for interventional data augmentation to simulate interventions over the environmental factors that are likely to change across domains. The authors argue that this interventional data augmentation would improve the generalization behavior of models over out-of-distribution data and reduce the reliance on spurious features during training. The two metrics that the authors measure are Single-Domain Generalization (SDG), which tests the generalization behavior to a new domain for instance natural image to sketch when trained only on the natural image domain, and Reducing Reliance on Spurious Features (RRSF) which measures the reliance on spurious features for model training such as relying on background to classify foreground.\n\nFor SDG, the authors use SDEdit on top of text-to-image models to generate source images in the target domain and use these generated images also for training. Interestingly the authors also show that instead of generating images in the specific target domain, similar performance can also be achieved if images are generated in a different set of target domains.\n\nFor RRSF again, the authors explicitly design prompts that try to reduce the effect of specific spurious correlations. For instance, generate images in various backgrounds to reduce the bias towards the background.\n\nAcross different datasets the authors show improved performance compared to various baseline approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I like the idea of using SDEdit on top of text-to-image generative models to remove the model's biasness to spurious features and also generalizing to new domains.\n\n2. The ablation study showing that a similar performance can be achieved for SDG even if images are are not generated for a specific target domain is nice and very useful."
            },
            "weaknesses": {
                "value": "1. In figure 3, it seems that Text2Image variant has the least biases across all three datasets. But the text on page 8 the authors suggest that \"Text2Image seems to be less effective than other techniques in reducing background and texture biases\". Can the authors clarify this?\n\n2. The results in Figure 2 where the Text2Image variant performs better/comparable to SDEdit variant undermine the idea proposed in the paper.  In the Text2Image variant, there is no image conditioning for generating images. Several other papers[1] have also shown that using synthetic data from generative models improves the generalization performance of the classifiers. Where is the novelty then coming for this paper?\n\n3. To further show that the generalization improves for unseen target domains, can the authors also show results for Cifar10-C and ImageNet-C datasets?\n\n[1] Synthetic Data from Diffusion Models Improves ImageNet Classification. Azizi et al. https://arxiv.org/abs/2304.08466"
            },
            "questions": {
                "value": "I have already mentioned my questions in the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698629728118,
        "cdate": 1698629728118,
        "tmdate": 1699636975933,
        "mdate": 1699636975933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dAbIuDzmn8",
        "forum": "9v5uZPWZoV",
        "replyto": "9v5uZPWZoV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_nrcC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_nrcC"
        ],
        "content": {
            "summary": {
                "value": "This article carried out the first investigation of T2I generators as general-purpose interventionaldata augmentation mechanisms, showing their potential across diverse target domains and potential downstream applications.\n\u2022 Authors perform extensive analyses over key dimensions of T2I generation, finding the conditioning mechanism to be the most important factor in IDA.\n\u2022 Authors show that interventional prompts are also important to IDA performance; but in in contrast with previous works, we find that post-hoc filtering is not consistently beneficial.\n\nGenerally, this article describe why text to image generator from stable diffusion outperforms others methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This article carried out the first investigation of T2I generators as general-purpose interventionaldata augmentation mechanisms, showing their potential across diverse target domains and potential downstream applications.\n\u2022 Authors perform extensive analyses over key dimensions of T2I generation, finding the conditioning mechanism to be the most important factor in IDA.\n\u2022 Authors show that interventional prompts are also important to IDA performance; but in in contrast with previous works, we find that post-hoc filtering is not consistently beneficial.\n\nThis work generally makes efforts on data shift problem. A good mind in solving data augmentation problem."
            },
            "weaknesses": {
                "value": "Not very soundness from technical side. No novelty in the model is presented."
            },
            "questions": {
                "value": "Have you ever compared results with some other generative methods like GAN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641880546,
        "cdate": 1698641880546,
        "tmdate": 1699636975795,
        "mdate": 1699636975795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K4jFSOGS0r",
        "forum": "9v5uZPWZoV",
        "replyto": "9v5uZPWZoV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_MWyF"
        ],
        "content": {
            "summary": {
                "value": "The paper uses text-to-image generators and editing techniques to generate training data. Experiments were performed for domain generalization benchmarks with supportive results. There were extensive ablations and analysis over types of prompts, conditioning mechanisms, post-hoc filtering and editing techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper has extensive experiments and ablations.\n- The analysis comparing different editing methods is insightful."
            },
            "weaknesses": {
                "value": "1. Generalizability of the method\n    - Almost all the results seem to assume that the target domain can be easily described and that the number of domains are known. However, this does not always hold. E.g., in iwildcam, where the target domain consists of images from different camera traps resulting in different locations, viewpoints, etc., it may not be obvious how to describe the target domain. \n    - Furthermore, the proposed method on \u201cBreaking spurious correlations\u201d (Fig 3) requires a human to hand craft prompts which may be expensive to attain.\n    - E.g., [1] uses a captioning model to describe the data, then gpt to summarize into domain descriptions. These descriptions are then used in the prompts. Thus, it doesnt require knowledge of the domains.\n2. \"Describing the Target Domain Is Not Necessary\u201d. Table 2.\n    - It is not clear to me what the message of table 2 is. As it is without target domain information, it seems say that SD is biased towards generating certain domains and those domains happen to be aligned with the target for this dataset, but this may not be the case for other datasets.\n3. From A.1, it seems like there was different number of additional data for generated images and baseline augmentation techniques. Can the authors explain this choice? It may be interesting to see how the performance changes with amount of data similar to (He et al., 2023;  Sariyildiz et al., 2023).\n4. The conclusions of the paper seems similar to that of (Bansal & Grover, 2023) who also used pre-defined text prompts to generate data. They also showed that a combination of real and generated data results in better performance, although on IN-Sketch and IN-R. The evaluation setup may be slightly different but the conclusions from SDG seems to be similar. \n\n\n\n[1] Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation. NeurIPS\u201923"
            },
            "questions": {
                "value": "Other than the questions raised above:\n- What is the performance of Retrieval on DomainNet in Fig 5?\n- I would suggest moving some technical details, e.g. the setup, how many images are generated for each original image, a brief description of how is retrieval done, to the main paper.\n- It may be useful to have an additional column in the table of results for the runtimes. The baseline augmentations should be much cheaper to attain than generating data with SD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716143750,
        "cdate": 1698716143750,
        "tmdate": 1699636975692,
        "mdate": 1699636975692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Swk3QuqZlz",
        "forum": "9v5uZPWZoV",
        "replyto": "9v5uZPWZoV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
        ],
        "content": {
            "summary": {
                "value": "Utility of Text-to-Image generators for interventional data augmentation (IDA) toward improving single domain generalization (SDG) and reducing reliance on spurious features (RRSF) is the subject of this work.\nPrevious works studied using generators for generating training data; the contribution of this work is a deeper study of the same for SDG and RRSF tasks.\n\nThe work is a thorough study with many tasks and ablation. Although I believe the paper do not have any surprising finding and ranks low on novelty, it could be of interest to the research community.\nHowever, I have many questions regarding their setup, which muddled their contributions quite a bit. My assessment therefore is a placeholder at the moment, and would likely change."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Thorough study with four SDG and three RRSF tasks. Comprehensive evaluation with previous augmentation procedures and various image generators.\n- Writing and presentation of results is easy to follow. I enjoyed comparisons made using simple baselines."
            },
            "weaknesses": {
                "value": "- Novelty of the paper is somewhat limited.\n  \nPlease see questions."
            },
            "questions": {
                "value": "**Premise compromised?** The paper started with the premise that IDA is known to be useful for SDG and RRSF, and proceeded with two-fold objective of evaluating T2I generators and establishing a new state-of-art on SDG and RRSF.\nHowever, as observed from Fig. 3 and 5, text2Image and retrieval baselines performed the best, which are both non-interventional augmentations. What then is the role of IDA and conditional generators?\nClearly stating the contributions can help. Is the paper suggesting to only evaluate unconditioned image generators using the task? \n\n**Table 2** results are very interesting. Few questions. \n1. The performance on the sketch domain is better without simulated target domain, why is that?    \n2. For comparison, could you please include the baselines: (a) ERM trained on all but target domain, (b) ERM trained on all the domains, (c) ERM trained only on the target domain.    \n3. It is intriguing that the performance is comparable even without simulated target domain for SDEdit, but none of the other target-agnostic augmentation are even close, why is that?   \n4. I suspect if there are any implementation differences between SDEdit and others (MixUp, CutMix etc.) causing the massive improvement (a common and annoying problem with PACS and other datasets), releasing your implementation can help. Also, can you add to the table (2) the performance of SDEdit with even more irrelevant prompts? How about if we use the prompts from OfficeHome on PACS dataset? How does the performance compare then? \n\n**More information on prompts.** Could you please provide more information on the prompts used for generatring images on the three RRSF tasks? They are more nontrivial than for SDG, and yet their description is rushed in the main paper.\nOverall, how much effort was spent on engineering the prompts and how were they tuned?\n\n**Conclusion and contributions**. I am somewhat lost on the takeaways. Please spell them out. As I see it, conditioning of generators (since text2Image and retrieval work just as well) is not so important but the conclusion says otherwise.\nWhat are the implications for evaluation of generators and SDG/RRSF research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7944/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7944/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7944/Reviewer_bsQg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7944/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766928457,
        "cdate": 1698766928457,
        "tmdate": 1700674900853,
        "mdate": 1700674900853,
        "license": "CC BY 4.0",
        "version": 2
    }
]