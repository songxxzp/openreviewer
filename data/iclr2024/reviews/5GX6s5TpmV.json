[
    {
        "id": "f4FHOqyldq",
        "forum": "5GX6s5TpmV",
        "replyto": "5GX6s5TpmV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_xpHV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_xpHV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a technique for finding adversarial attacks given knowledge that a certification exists for a smaller radius.  The authors demonstrate experimentally on multiple datasets that compared to existing attack methods, their technique is able to consistently find attacks of minimum norm relative to the certification radius quickly."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- interesting, novel problem and proposed attack\n- good experimental scope which encompasses multiple certified defenses, datasets, and attack techniques"
            },
            "weaknesses": {
                "value": "- clarity: I think that the clarity of the experimental section can be improved greatly.  For figures, it would be useful to have a visible legend on the graphs as well as subtitles for each subplot so that they are more easily interpretable from a glance.  It would also help to section out and emphasize main observations since the text in the experimental sections gets quite long and it's easy to lose track of main conclusions.  In tables, it would help to also bold the best performing.  Some reported metrics are also a bit difficult to interpret, it might be useful to give some mathematical definitions (see questions)\n- significance: From what I understand, the approach is finding attacks with radius larger than the certified radius and suggests that this is a security issue caused by certifications.  But this doesn't really come across to me as a problem with certifications as the paper seems to suggest that it is since the certifications give no guarantees for larger radii.  I still find the problem interesting though, but more from the perspective that we generally don't know exactly what perturbation size threshold is considered \"imperceptible\" and using a slightly larger perturbation size can still be an imperceptible attack that we want to defend against.  For this, it would be interesting to see visualizations for what the minimum norm adversarial example found by each attack technique looks like compared to the clean input."
            },
            "questions": {
                "value": "- to confirm, %C metric is the (distance to the attack found - certified radius) / certified radius ?\n- what exactly is the \"Best\" metric in the tables?  I don't understand what the description \"smallest attack proportion\" means.\n- how exactly are other attack methods used to find one of minimum norm?  For example, with PGD how do you decide on what radius ball to project to?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Reviewer_xpHV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7024/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796997142,
        "cdate": 1698796997142,
        "tmdate": 1700707749756,
        "mdate": 1700707749756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZmHtEvbY7m",
        "forum": "5GX6s5TpmV",
        "replyto": "5GX6s5TpmV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_Z9pA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_Z9pA"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates an iterative gradient-based method to obtain small-norm adversarial examples, where the perturbation norm $\\eta$ (i.e., step size) at each iterate $x^t$ is determined dynamically by the certified radius around $x^t$ obtained using a robustness certificate leading to a speedup in iteration complexity. This speedup makes sense because the certificate ensures that no adversarial example having norm smaller than $\\eta$ exists. Experiments demonstrate that this method finds smaller adversarial examples than baseline attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe idea of using certificates to speedup iterative gradient based attacks is interesting, and novel in my opinion. \n2.\tThe reported experimental results improve a lot in percentage for finding minimal adversarial examples over existing approaches."
            },
            "weaknesses": {
                "value": "Most of these weaknesses stem from some surprising trends requiring further investigation in the main evaluation Table 2. \n\n1.\tRelevant Baselines 1: The subject of _minimal adversarial examples_ has been well studied in the literature, and some of these attacks are compared against in Table 2: C-W, AutoAttack. Other methods like PGD, DeepFool aim to find an adversarial example within the given budget, and not necessarily the adversarial example having minimum $\\ell_p$ norm \u2014 it is unclear why these should be relevant baselines (see Sec 2.3 in [1] for more details). Now, AutoAttack is an ensemble of several attacks, out of which FAB is the attack created specifically for finding minimal adversarial examples, and hence this should be the primary comparison both for radius as well as time. There are a few parameters in FAB to tradeoff computational cost for norm-minimization (e.g., the number of random restarts), that should be ablated on. \u2028\n\n2.\tRelevant Baselines 2: If PGD is going to be used as a baseline, the hyper parameters tested in Appendix C are not enough: specifically, one should try a binary search on the perturbation norm that PGD is constrained to to find the minimum norm that PGD can find an adversarial example for (i.e., let Decision(s) be true if PGD can find an adversarial example in a s-sized ball around the input point. Then, one can binary search over Decision(s) to obtain the minimum s for which Decision(s) is true). This is potentially computationally expensive: see next point.\u2028\n\n3.\tComputational Overhead: Table 2 reports that PGD takes a larger time than the proposed method. It is very unclear how this can be true, since the proposed method can be seen as PGD + computing a robustness certificate at every step. Computation of certificates is typically extremely expensive (take orders of many seconds for large datasets), and as such it is quite unclear how can the combined time compare to any adversarial attack. A clearer description for exactly what the Time column corresponds to would be helpful in resolving this.\n\n[1] Minimally Distorted Adversarial Examples with a Fast Adaptive Boundary Attack. Croce and Hein.\n\nAdditionally, some minor writing issues which don\u2019t really affect the evaluation above, but are worthy of a mention:\n\n1.\tUse of the word \u201cparadox\u201d: Finding the minimal adversarial example is not paradoxical just because one is utilizing a certificate to compute the minimum norm. \u201cParadox\u201d seems too strong of a word here. \n\n2.\tEq. (9), typically I[..] is used for the indicator function. What is P in B_P? Since S is fixed throughout the paper, it might be easier notationally to just mention at the start that everything is projected onto S."
            },
            "questions": {
                "value": "Most of the questions stem from some trends in the main result Table 2 that are unclear (see details in weaknesses above). It would be good to clarify those questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Reviewer_Z9pA",
                    "ICLR.cc/2024/Conference/Submission7024/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7024/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861290229,
        "cdate": 1698861290229,
        "tmdate": 1699840769488,
        "mdate": 1699840769488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8NadGiwBB5",
        "forum": "5GX6s5TpmV",
        "replyto": "5GX6s5TpmV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_VDGn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_VDGn"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a thought-provoking analysis of the potential vulnerabilities introduced by the release of certification mechanisms in neural networks. It posits an important question about whether certifications, while intended to establish the absence of adversarial examples within bounded spaces, might paradoxically undermine the security they are designed to bolster.\n\nThrough experimentation, the authors introduce the novel \"Certification Aware Attack,\" which adeptly exploits the provided certifications to mount evasion attacks that minimize the perturbation norm. This approach is shown to generate adversarial examples that are notably smaller and more challenging to detect than those produced by existing attack methods. The paper claims that such attacks can be executed with a reduction in the median perturbation norm, implying that adversaries could create more discreet yet effective perturbations. The authors further claim that these sophisticated attacks require less computational time, highlighting a security trade-off when releasing certifications.\n\nThe paper could impact our understanding of neural network security, suggesting a re-examination of the strategies employed to certify model robustness. It underscores the complexity of defending against adversarial attacks and the unintended consequences that well-meaning security measures can provoke. The findings could catalyze a reassessment of current practices and prompt the development of more resilient security protocols in machine learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of this paper is interesting, and it explores a new setting in the certified adversarial machine learning area.\n2. The methodology introduced in this paper is valid to me, and the formulation is clear \n3. The authors conducted sufficient evaluations to demonstrate that the required distance can be significantly reduced by their proposed attack method."
            },
            "weaknesses": {
                "value": "1. Although it is an interesting setting to me, the practicality of this paper is questionable.\n2. The presentation of the paper could be improved. Some symbols are not defined or hard to understand."
            },
            "questions": {
                "value": "Please see the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Reviewer_VDGn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7024/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698953836573,
        "cdate": 1698953836573,
        "tmdate": 1699992104925,
        "mdate": 1699992104925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UG8XgT0RNE",
        "forum": "5GX6s5TpmV",
        "replyto": "5GX6s5TpmV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_Kwhk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7024/Reviewer_Kwhk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a stronger adversarial attack on neural network classification models. The core idea is to use a certification method (randomized smoothing) to estimate a lower bound of radius where adversarial examples cannot exist, and then focus on searching for adversarial examples only in the regions where certification fails. I do believe this idea, if demonstrated correctly, can be helpful for finding adversarial examples, because with the help of strong certification algorithms, the region that does not include adversarial examples can be excluded and the search space is reduced.\n\nWhile this approach is novel and interesting, the main storyline in this paper is rather misleading - it claims that certified models are more prone to attacks, and claims that certification reduces security. These claims are too extreme and biased.\n\nDespite presenting the results as a big surprise, the result presented in this paper is not really a surprise because certification and attacks essentially solve the same problem, and the strongest certification method also leads to the strongest attack method. For example, using a mixed integer programming solving can give (theoretically) the strongest certification, as well as giving the strongest attack, as it leads to the global optimal solution to the adversarial attack problem.\n\nIn other words, if we have a strong certification algorithm that can give very tight bounds, any part of the input space where the bounds do not hold will necessarily contain adversarial examples by definition. In the author\u2019s claim, a stronger certification algorithm like this will reduce security, which apparently does not make sense to me; instead, a stronger certification algorithm helps us to easily find which part of the model is robust and which part is not, and with this exact quantification of the model, we have more security rather than less. The real security problem is that we don\u2019t know whether the model is secure or not, because in many cases we can neither certify nor attack them. This is also the actual problem certified models try to resolve - at least, we have guarantees in some cases to know the model is indeed safe. This paper does not reduce the provable security guarantees provided by certified models, despite its misleading claims.\n\nI like the novel approach to the adversarial attack problem, but the paper cannot be accepted in its current form because of its extreme, misleading, and confusing claims. None of the theoretical security guarantees in certified defense were actually broken by this paper, and the usage of the certification method in adversarial attacks shows the power and tightness of certification itself. If accepted, it will create misunderstanding in the research community. I am happy to reevaluate the paper if the authors are willing to significantly rephrase the paper and rewrite the story to tune down the misleading overclaims.\n\n-----------------------\n\nAfter discussions with the authors, despite multiple reviewers raising the same concern as me, the authors did not want to change their misleading and confusing claims. Accepting this paper in its current form could lead to confusion and even damage to the certification community. Thus, I have to further reduce my score and firmly reject this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method of using a certification algorithm to guide the procedure of finding adversarial examples is novel and not well explored in prior works. If the paper can be written in a different way, by emphasizing that certification tools are also useful for guiding adversarial attacks and giving better quantification of model characteristics from both attack and certification perspectives, it can become a good paper.\n\n2. The topic studied in this paper is important and relevant. The proposed method also demonstrates improvements in some metrics (mostly the perturbation sizes, but not attack success rates) on some models."
            },
            "weaknesses": {
                "value": "1. The claims in this paper are misleading and confusing (see my comments above).\n \n2. Many samples are used to estimate $E_0$ and $E_1$ during the attack. To make a fair comparison, for other attacks such as PGD, AutoAttack and CW, the number of random restarts must be adjusted, so each baseline contains the same numbers of queries to the model. With more random restarts, other attacks also become stronger.\n\n3. Attacks like AutoAttack are designed for a fixed norm, so I don\u2019t think the comparison on minimal perturbation size (which is one main claim of the proposed method) is fair here. Attacks like CW have hyperparameters to tradeoff between attack success rate and the norm of the adversarial examples. The table just reports one number without showing the tradeoff or selection of the norm for baseline methods, so it is not convincing that the proposed approach is stronger.\n\n4. PGD was usually a strong baseline for L inifty norm. Since this paper focus on L2 norm attack, the improved version of PGD attack for L2 norm should be used as a baseline [1]. \n\n[1] Rony, J\u00e9r\u00f4me, et al. \"Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
            },
            "questions": {
                "value": "1. Why does the proposed approach achieve much worse performance in MACER models in the appendix? (attack success rate is much lower than PGD). How about other certified defense methods such as SmoothAdv?\n\n2. Section 5.3 briefly mentioned IBP models. Did you also use the IBP bounds to guide attacks? For example, you can search for the region where IBP cannot certify.\n\n3. Is the \u201c%-C\u201d metric based on the certified radii of the adversarial examples being wrong classified? Can you give a clear mathematical definition of this metric?\n\n4. In Figure 3, why do the imagenet results with PGD have weird steps, while other baselines do not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7024/Reviewer_Kwhk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7024/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699353613127,
        "cdate": 1699353613127,
        "tmdate": 1700728605557,
        "mdate": 1700728605557,
        "license": "CC BY 4.0",
        "version": 2
    }
]