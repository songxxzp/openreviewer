[
    {
        "id": "l9VhO5eqhX",
        "forum": "efxXzrbgrX",
        "replyto": "efxXzrbgrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_ifzZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_ifzZ"
        ],
        "content": {
            "summary": {
                "value": "this work studies the adversarial robustness of SAM in a more practical black-box setup. This cross-model transferability is conjectured to be limited by the strength of adversarial features indicated by their dominance over other clean images for determining the feature response. It verifies this conjecture by adding a regularization loss to increase the feature dominance. To avoid dependence on other natural images, it proposes to replace them with random patches cropped from the to-be-attacked clean image."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It identifies the challenges in end-to-end approaches for realizing cross-prompt transferability and mitigate it in a prompt-agnostic manner by only attacking the image encoder.\n\nIt identifies relative feature strength as a factor that influences the cross-model transferability, and propose to boost them by a regularization loss."
            },
            "weaknesses": {
                "value": "Although it claims to be black-box attack, the techniques are mainly white-box attack method and transfer attack increasing the transferability of adversarial examples. Besides, in the white-box, it uses SAM-B as the surrogate model to attack. In the black box setting, it uses the adversarial examples generated with SAM-B to attack other models with very similar architectures. It is better to transfer from an architecture to another different architecture, such as from vgg to resnet in related previous papers. Transferring with the same model architecture family makes the problem much easier.\n\nIt only tests with 100 images and report the results on the 100 images. 100 images seem to be a very small number and there may be some randomness in the results. It is better to discuss why 100 images is a reasonable number. \n\nIt is better to discuss the complexity of the proposed method, like computation complexity or what are the costs to run the algorithm. \n\nIt is hard to understand equation (5). The specific definition of $SAM_{embedding}(prompt, x_{target})$ is not provided. Equation (5) is claimed to be prompt-free. However, it still has prompts as the inputs for $SAM_{embedding}$. So it seems that equation (5) depends on prompts. \n\nThere are no baselines. Since the paper mainly uses white-box attack methods and transfer the adversarial examples. It is better to use some white-box attacks to generate and transfer the adversarial examples as a baseline. Without a baseline, we are not sure if the mIoU results are good or not. It is better to provide some basic baselines. \n\nThe technique contribution may be limited. In general, it seems that the mse loss is basically requiring the model outputs of adversarial examples and target images to be the same. This objective and the optimization is very basic in adversarial attacks. The regularization loss mainly follows Zhang et al. (2020). The novelty may be limited."
            },
            "questions": {
                "value": "see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As the paper proposes practical blackbox attacks for large models SAM, it is better to discuss the potential social effects in the paper."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698550144641,
        "cdate": 1698550144641,
        "tmdate": 1699637182887,
        "mdate": 1699637182887,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7diFtZQaYb",
        "forum": "efxXzrbgrX",
        "replyto": "efxXzrbgrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_bXHA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_bXHA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a targeted adversarial attack with black-box setting on Segment Anything Model (SAM), where given an adversarial image, the SAM will constantly output a predefined mask. The paper conjectures that white-box access to SAM models is impractical, where SAM works in online modes and allows users to specify random prompts. Conversely, the paper relaxes the white-box access assumption and utilizes a surrogate SAM for black-box adversarial optimizations, and ensures the attack\u2019s effectiveness in a prompt-agnostic manner. The paper proposes a regularization loss term to further improve the adversarial attack effectiveness and computational efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Relaxed the assumption of white-box access for adversarial attacks on SAM, where the paper proposes to use a surrogate SAM and optimize adversarial examples from it\n- Improved the threat model in adversarial attacks on SAM by introducing a prompt-agnostic adversarial attack. Previous method focuses on attacking SAM with a predefined prompt position, whereas with the proposed method, the users are susceptible to a wider range of adversarial attacks, even with different prompt positions.\n- Analyzes the relative feature strength between the clean image and adversarial image to identify the feature dominance between adversarial features and clean features. The paper provides a clear analysis and explanation on the impact of adversarial features and clean features, and gain insights to improve the strength of adversarial features by introducing a regularization term."
            },
            "weaknesses": {
                "value": "- The claim of black-box might be too strong, as in usual adversarial attacks, black-box settings require validation on the transferability of adversarial attacks onto different families of model. The current setting in this paper is restricted to only ViT image encoder in SAM, which is not a proper evaluation of black-box adversarial attacks\u2019 transferability.\n- The formulation of Loss_reg is unclear in Eqn. 7, and the difference between the Loss_reg for PATA+ and PATA++ is not shown.\n- Cross-model results are not convincing, as the surrogate model is essentially coming from the same genre of models as the evaluated victim model. All models have similar encoder and decoder structures, but with different scales and number of parameters.\n- Insufficient qualitative results to show the prompt-agnostic behavior of the proposed adversarial attack. There is no similar visualizations as Fig. 4 to demonstrate the proposed adversarial attack\u2019s prompt-agnostic behavior.\n- Typo in Eqn. 1: promt \u2192 prompt"
            },
            "questions": {
                "value": "Please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852110804,
        "cdate": 1698852110804,
        "tmdate": 1699637182736,
        "mdate": 1699637182736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VHFZRBvShI",
        "forum": "efxXzrbgrX",
        "replyto": "efxXzrbgrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_Rvz4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_Rvz4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Targeted Adversarial Attack (TAA) on the Segment Anything Model (SAM) called Prompt-Agnostic TAA (PATA). Different from prior works, this paper doesn't require the prior knowledge of user's prompt to the SAM for choosing the object and replace with the target object."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Adversarial attack on SAM is interesting."
            },
            "weaknesses": {
                "value": "- I do not believe Eq. 2 has to be interpreted as a classification task. If you are focussing on the image encoder, you can just compare the features from the encoder. This Eq. 2 seems unnecessary. \n\n- Fig. 1 should accompanied by the example how TAA works with SAM, e.g. visualizing $Mask_{target}$ and $Mask_{adv}$ within the attack framework.\n  \n- Using MSE loss over other mentioned loss functions is a known phenomenon [A, B]. Table 1 is unnecessary. \n\n- Eq. 6 is similar to loss functions used in generative adversarial attacks, where information from \"other clean images\" are used to create perturbations. The behavior of Eq. 6 is expected in Fig. 3.\n\n- The paper severely lacks in comparison with prior dense prediction attacks. SAM doesn't ensure the proposed method is incomparable with these prior works.   \n\n[A] Learning transferable adversarial perturbations, NeurIPS 2021\n\n[B] GAMA: Generative Adversarial Multi-Object Scene Attacks, NeurIPS 2022"
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698950512875,
        "cdate": 1698950512875,
        "tmdate": 1699637182422,
        "mdate": 1699637182422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6wJAYXD5x3",
        "forum": "efxXzrbgrX",
        "replyto": "efxXzrbgrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_JQeG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9373/Reviewer_JQeG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel approach to targeted adversarial attacks (TAA) on the Segment Anything Model (SAM), a computer vision model. The contributions of the paper are noteworthy and add to the existing body of knowledge in this field. The introduction of \"prompt-agnostic target attack\" (PATA) is designed to generate adversarial examples that resemble a given target image's mask without relying on specific prompts. This approach has the potential to enhance the robustness and practicality of adversarial attacks on SAM, making it more applicable to real-world scenarios. The paper's emphasis on relative feature strength is another valuable contribution. This concept offers a new perspective on assessing the dominance of adversarial features over clean images, which is crucial for understanding the transferability of adversarial examples. It adds depth to the evaluation of adversarial attacks and their impact on computer vision models. Furthermore, the regularization method proposed to boost feature strength is a practical and effective addition. The authors show that this regularization significantly enhances the transferability of adversarial features across different models. This has implications for improving the overall effectiveness of TAA methods and their ability to fool various models. In summary, this paper provides novel insights and approaches for TAA on SAM, enhancing the field of adversarial attacks in computer vision. The contributions made in this paper can lead to improved robustness and security in computer vision models, particularly those like SAM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-structured, written clearly, and logically presented. The introduction of \"prompt-agnostic target attack\" (PATA) and the concept of \"relative feature strength\" are innovative perspectives in adversarial attacks."
            },
            "weaknesses": {
                "value": "(1)\tThe paper does not provide a comprehensive comparison or benchmarking against existing methods. A more in-depth evaluation against other state-of-the-art adversarial attack techniques would help assess the uniqueness and effectiveness of the proposed approach. For example, I recommend the results of adding Attack-SAM methods in the section 5.\n(2)\tThe author mentioned in the qualitative results that the prompt information was randomly selected, but the result in the figure 4 and 5 only gives the result in the case of the prompt information to a significant area. This is not rigorous, and this ignores the important position of the prompt-guided to attack. I suggest adding experiments under the condition of different prompt-guided selection conditions."
            },
            "questions": {
                "value": "(1)\tI think that if the prompt-guided is ignored, the process of generating confrontation disturbances will transfer the highly concerned area in the original picture to the regional of strong attention in the target picture. Therefore, when the prompt information is characterized by a more obvious area, it will have a good attack effect, which does not prove the effectiveness of the method. I think the results of comparative experiments to prove the effectiveness of the method.\n(2)\tDo you verify the concealment of the attack? From Figure 4, we can clearly see that the watermarks are similar to the outline of the target picture.\n(3)\tWhat is the direct regularization loss on Page 7? You should use formulas to explain to facilitate readers to understand.\n(4)\tCould you explicitly outline the limitations of the proposed approach? For instance, are there specific scenarios or model architectures where PATA may not be as effective? Addressing potential vulnerabilities or shortcomings would enhance the paper's transparency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022960074,
        "cdate": 1699022960074,
        "tmdate": 1699637181904,
        "mdate": 1699637181904,
        "license": "CC BY 4.0",
        "version": 2
    }
]