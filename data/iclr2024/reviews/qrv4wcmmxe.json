[
    {
        "id": "YBSqSdAgeO",
        "forum": "qrv4wcmmxe",
        "replyto": "qrv4wcmmxe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a prompt-based zero-shot HOI detector. It splits the detection task into two subtasks: extracting spatial-aware visual features and interaction classification. The vision and text prompts are jointly applied to the detector. Experimental results on the zero-shot settings show its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well written and organized. The vision and text prompts are also clearly explained.\n2. Experimental results on the zero-shot settings demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Overall, this work is very similar with the following ICCV2023 paper, including the overall framework, the conditional vision prompts and the learnable modules. What's the difference between the proposed method and the ICCV2023 paper.\nA1: Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory, ICCV2023.\n2. For the Lcls in (11), it is not clear how to connect the model with the GT labels. \n3. This work only presents the HOI results using zero-shot settings. What's the result using the typical experimental settings?\n4. Some important works from CVPR2023 are missing. Besides, the formats of some references are not consistent."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_n8e2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698052609699,
        "cdate": 1698052609699,
        "tmdate": 1699637094954,
        "mdate": 1699637094954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "78R9VxP2YN",
        "forum": "qrv4wcmmxe",
        "replyto": "qrv4wcmmxe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
        ],
        "content": {
            "summary": {
                "value": "The manuscript mainly focuses on the generalization of HOI detection, particularly zero-shot HOI detection. They proposed a Prompt-based HOI detection framework to improve the alignment between the visual and language representations with multi-modal prompts. Specifically, the decouple the visual and language prompts to improve spatial-aware feature learning. Meanwhile, they present several strategies to alleviate the overfitting to seen concepts. Effective experiments demonstrate the proposed method achieves a significant improvement on unseen categories."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed visual-language decomposition strategy seems reasonable and demonstrates its effectiveness.\n2. The proposed method demonstrates a significant improvement in zero-shot HOI detection based on large pre-trained models.\n3. Part of the ablation experiment is beneficial for further research on visual relationship understanding. e.g. the effect of backbone networks."
            },
            "weaknesses": {
                "value": "Overall, the paper mainly borrows the popular adapt large models and prompt strategy for down-stream tasks. Considering that there are massive similar approaches in other fields, the novelty is limited. However, the reviewer still thinks it is beneficial for the development of zero-shot HOI detection. To some extent, the core idea is similar to CoOp and the following work Co-CoOp, though this paper also incorporates the visual prompts and has made some HOI-specific designs."
            },
            "questions": {
                "value": "1. The proposed method achieves smaller gap between seen and unseen category. According to Tab.1, PD is larger in RF-UC setting. Could you explain it? Moreover, do you have any ablation studies to check which module is more important for reducing the PD.\n2. The paper aims to achieve verb-agnostic prior knowledge. Could you explain why the verb-agnostic feature is helpful for interactiveness-aware features? By the way, the local spatial structure is actually verb-dependent, e.g., different action pattern demonstrates different relative human-object positions. Thus, capturing local spatial structure seems to contradict to verb-agnostic representations. \n\n\nIn Table 4, the improvement on Unseen category is clearly better than seen category on RF-UC setting when you use a larger backbone network. Do you have any explanations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_c79G"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681445660,
        "cdate": 1698681445660,
        "tmdate": 1699637094807,
        "mdate": 1699637094807,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gfTMbDLckO",
        "forum": "qrv4wcmmxe",
        "replyto": "qrv4wcmmxe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ"
        ],
        "content": {
            "summary": {
                "value": "In this submission, the authors tackled the problem of zero-shot human-object interaction (HOI) detection, which aims to localize and classify all the potential human-object interactions in a given image. The zero-shot setting for HOI detection further requires the model to detect novel classes of objects and/or actions which are not seen during training. Inspired by the recent trend on leveraging vision foundation models for HOI detection, the authors proposed a novel prompt learning based approach called PID. Specifically, several vision and language prompts are adopted to enhance the visual feature extraction and interaction classification, respectively. Some optimization tricks are also explored to prevent overfitting. Experimental results on HICO-DET partially show the significance of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Overall, the manuscript is well-written and easy to follow.\n2. The use of prompt learning for HOI problems is a good direction to explore (and also a trend in computer vision)."
            },
            "weaknesses": {
                "value": "1. The whole framework seems like a combination of multiple existing modules, e.g., DETR, CoOp/CoCoOp style prompts. The novelty and the motivation behind each of the design are unclear.\n2. In 4.2, the authors mentioned that the DETR used for detecting all the humans and objects in the first stage is fine-tuned on the whole HICO-DET dataset. Does the 'whole' here mean both training and validation sets? If so, this is a weird setting as previous works (including Bansal et al. 2020 and Hou et al. 2020 that the authors claimed) never fine-tuned their detectors on the validation set, which would lead to extremely unfair comparison since the detector can significantly affect the overall performance.\n3. The experiments are only conducted on a single dataset. Why the method is not tested on V-COCO?\n4. The conclusion part lacks objective reflections on the deficiencies of this study and future prospects for improvements."
            },
            "questions": {
                "value": "See the weaknesses part. I'll consider changing the score after reading the authors' responses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review is needed."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8729/Reviewer_YqBQ",
                    "ICLR.cc/2024/Conference/Submission8729/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825372812,
        "cdate": 1698825372812,
        "tmdate": 1700510288092,
        "mdate": 1700510288092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dGtXIl14iP",
        "forum": "qrv4wcmmxe",
        "replyto": "qrv4wcmmxe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_NGK6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8729/Reviewer_NGK6"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of zero-shot HOI detection with the key idea of using conditional multi-modal prompts. Specifically, the language prompts consist of two parts, human-designed prompts and learned ones, with the former being responsible for guiding the learning of the latter. The vision prompts is learned from instance-level visual priors, including bboxes, confidence scores, and semantic embeddings. The proposed method achieves competitive performance on HICO-DET."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is reasonable and the results are competitive."
            },
            "weaknesses": {
                "value": "**1. Lack of analysis.**\n*1)* The language prompts are initialized as the concatenations of  $C_L^a$ and $U_L$, which are subsequently forced to be close to $C_L$, why? In this way, why not just using $C_L$ as language prompts? \\\n*2)* Does $\\mathbb{A}$ contain unseen verbs? If that so, can this model recognize HOIs that are not present in HICO-DET? In other words, if I want to detect a HOI using this model, does the corresponding interaction verb have to be included in $\\mathbb{A}$?  \\\n*3)* For vision prompts, where do the instance-level visual priors come from? Are they extracted by the pre-trained DETR? \\\n**2.** Actually, I do not understand why the vision prompts are useful for zero-shot HOI detection. Concretely, the visual feature extracted in this model do not seem to be very sepcial compared to the that in most two-stage based HOI detectors. \\\n**3.** It is unclear that how these prompts work?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8729/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699429247764,
        "cdate": 1699429247764,
        "tmdate": 1699637094517,
        "mdate": 1699637094517,
        "license": "CC BY 4.0",
        "version": 2
    }
]