[
    {
        "id": "2HoZlr3aDx",
        "forum": "dTpbEdN9kr",
        "replyto": "dTpbEdN9kr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
        ],
        "content": {
            "summary": {
                "value": "This paper tackle the problem of motion generation. Given a pretrained diffusion based text-to-motion model i.e., MDM, this paper introduce three different way of adaptation to account for long sequence generation, multi-person motion generation and joint trajectory control.  For long sequence generation, two sequences are first generated individually and then blended with a fixed length overlapping. For two-person motion generation, a communication module (ComMDM) is trained to modify the intermediate features from two individual pre-trained MDM so as to coordinate the interaction between them. For trajectory control, several MDMs are frist finetuned to be given the trajectory of different joints. To achieve the control of multiple joints, the paper proposes to interpolate between two MDMs trained beforehand."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation of limited long sequence data, multi-person interaction and detailed control is valid and convincing.\n- The proposed method is sound and interesting. \n- The experiments are comprehensive and the results on all three tasks are very good especially for the qualitative results."
            },
            "weaknesses": {
                "value": "- Despite the good results, technically, there are not much contributions. The main contribution is the way of using/finetuning a pre-trained motion diffusion model for three different tasks. However, in each of those tasks the adaptation is straightforward. They are either engineering ticks such as soft or hard masking or  the use of existing techniques such as the interpolation mechanism from Ho & Salimans, (2022).\n\n- The proposed ComMDM is constrained to two persons only. It is hard to scale to multiple persons.\n\n- It is also unclear how the DiffusionBlending can be used to blend more than 2 joints."
            },
            "questions": {
                "value": "For the DiffusionBlending, is the interpolation applied to the final motion or the intermediate feature as the CommMDM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731450157,
        "cdate": 1698731450157,
        "tmdate": 1699636173864,
        "mdate": 1699636173864,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HaPCuyfj8C",
        "forum": "dTpbEdN9kr",
        "replyto": "dTpbEdN9kr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_xoQq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_xoQq"
        ],
        "content": {
            "summary": {
                "value": "This paper leverages a pretrained Motion Diffusion Model (MDM) as a generative prior and introduces three distinct techniques to enhance generation quality and enable new tasks: 1) DoubleTake is employed to reduce artifacts in long-range motion generation, enhancing the smoothness of transitions. 2) The introduction of ComMDM, a compact model integrated after the transformer layer in MDM, allows the model to handle two-person generation by freezing the original MDM and training this new module on a smaller data collection. 3) The authors also propose a fine-tuning method and DiffusionBlending to enhance controllability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper presents compelling quantitative and qualitative results, setting a new state-of-the-art benchmark with a significant lead.\n\n2. This article broadens the scope of existing text-driven motion generation from three perspectives. The conclusions and experiments associated with these extensions are valuable contributions to the research community.\n\n3. The paper is well written, ensuring that its content is readily comprehensible to its readers."
            },
            "weaknesses": {
                "value": "1. The authors should conduct a user study to quantitatively compare the visual results of TEACH with the proposed method for long sequence generation. Model parameters and inference speed should also be provided for a more comprehensive performance comparison between the two.\n\n2. Dual-person motion generation lacks comparative experiments, for example, with InterGen \\[1\\]. This paper introduces the InterHuman benchmark, a large-scale dataset for dual-person motion, and provides more comparative references in the article\n\n\\[1\\] Han Liang, et al. InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions"
            },
            "questions": {
                "value": "Please kindly refer to the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834130790,
        "cdate": 1698834130790,
        "tmdate": 1699636173770,
        "mdate": 1699636173770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oULgYxuA05",
        "forum": "dTpbEdN9kr",
        "replyto": "dTpbEdN9kr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_ijG5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_ijG5"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors address the challenges in human motion generation by introducing three new composition methods that use diffusion based generative models. The authors align these methods to 3 main challenges: long sequence generation, multi-person interactions, and controllable generation. The authors highlight that much of these problems arise from the lack of available data. The 3 proposed methods are sequential composition for long sequence generation, parallel composition for two-person motion generation, and model composition for fine-grained control and editing. The models involved in these methodologies are respectively DoubleTake for generating long motion sequences in a zero-shot manner, ComMDM to combine two frozen priors to enable two-person motion generation, and DiffusionBlending for flexible control of generated motion. The authors present several qualitative and quantitative results demonstrating the positive effects of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I want to highlight the following strengths:\n- The main strength I see is that the methods presented work well without the need of generating more data (or consuming large amounts of unavailable data). This is a strong benefit, since the field of human motion generation is still lagging in terms of data availability. The authors demonstrate in all 3 cases that they can satisfy the task at hand requiring small amounts of extra data/training.\n- I see major novelty in the methods developed for long sequence generation and 2 person generation. Both methods have interesting new ways to combine different generations from diffusion models (one over time and one in space). Adding to it that the method doesn't require a lot of extra training, these proposed methods seem solid and novel to me.\n- The paper is well written, with good experiments on all fronts. The author's explanation of each method is easy to follow. I want to particularly highlight figures 3 and 4, where the choice of colors and graphics makes it very intuitive to understand."
            },
            "weaknesses": {
                "value": "My only concern is on the fine-tuned motion control part. The task seems very similar to controlled motion generation. In that case, there is a body of literature in this subject, many of which uses diffusion models for controlled motion generation. The authors failed to include these methods and compare against them. Of course these methods have different data requirements, but they seem to achieve the same goal. I put a list of these methods below. I ask the authors to explain why they did not include these methods in their comparisons? I'm still happy with the paper and I think the authors could make a case while still including these works, but I would like to hear from the authors on these choices.\n\n- Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. ECCV, 2022.\n- Du, Y., Kips, R., Pumarola, A., Starke, S., Thabet, A., & Sanakoyeu, A. (2023). Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 481-490).\n- Castillo, Angela, et al. \"BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis.\""
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854188814,
        "cdate": 1698854188814,
        "tmdate": 1699636173682,
        "mdate": 1699636173682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ywNt5HHxXt",
        "forum": "dTpbEdN9kr",
        "replyto": "dTpbEdN9kr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
        ],
        "content": {
            "summary": {
                "value": "This paper presents off-the-shelf motion diffusion models as a priori for realizing three forms of synthesis tasks. doubleTake is used to generate long-term human motion. comMDM is used to generate two-person motion. DiffusionBlending is used to enable flexible and efficient fine-grained joint and trajectory level control and editing. Experimental results show that these low-cost composite methods generalize well-trained motion prior to different tasks and outperform previous specialized techniques in the respective tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the paper is well-written with a clear and well-motivated introduction.\n\nThe proposed method outperforms previous specialized techniques in the respective task. The experimental designs are comprehensive and show visually appealing results."
            },
            "weaknesses": {
                "value": "1. For the generation of long sequences, I don't think it makes sense to essentially generate each interval completely independently. A better option would be to use an autoregressive generation method similar to TEACH, but with the smarter option of combining each subsequence. Would it be possible to compare this with a scheme similar to EDGE [1]? Also, the paper admits that a comparison with DiffCollage is not possible due to a lack of publicly available code resources. Their implementation is simple. This may be optional, but would further support the paper.\n \n2. Regarding multiplayer motion generation, considering that MRT is an old paper, it could be considered to include a discussion and comparison with [1,2,3]. MRT focuses on deterministic prediction, and for the evaluation, is the prediction from ComMDM sampled once? A better comparison would be to use [2] as a baseline, to check the performance of diverse prediction and to measure diversity.\n\n[1] Tseng et al. EDGE. Edge: Editable dance generation from music. CVPR 2023\n\n[2] Xu et al. Stochastic Multi-Person 3D Motion Forecasting. ICLR 2023\n\n[3] Peng et al. Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting. CVPR 2023\n\n[4] Liang et al. InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions. Arxiv 2023"
            },
            "questions": {
                "value": "1. The comparison with TEACH is not very fair either. Is it possible to have the same setting with TEACH but using DoubleMDM, e.g. noising the start and end intervals as diffusion inversions and then performing DoubleMDM with additional transition frames? In this case, you can use accuracy measurements instead of current metrics such as FID. This is because current metrics are more of a test of the quality of the movement and whether the movement is consistent with the text. It is also important to determine if the transition is good by measuring smoothness and accuracy.\n\nOverall, the author's response to the concerns is needed to make the final decision. I am also happy to increase the rating if my concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900515512,
        "cdate": 1698900515512,
        "tmdate": 1699636173595,
        "mdate": 1699636173595,
        "license": "CC BY 4.0",
        "version": 2
    }
]