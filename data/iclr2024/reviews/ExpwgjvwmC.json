[
    {
        "id": "8HWktDDhik",
        "forum": "ExpwgjvwmC",
        "replyto": "ExpwgjvwmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model-centric evaluation framework to evaluate the quality of a model's predictions on **all possible inputs**.\nIt first uses a sampler to sample in the input space to obtain the output distribution.\nThen, it annotates the representative inputs with human annotators, results in a human-annotated confidence score for each representative input.\nBased on the confidence scores and model predictions, OmniInput generates a precision-recall curve and report AUC as the metric."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper first proposes a model-centric evaluation approach to evaluate the quality of a model's predictions on **all possible inputs**."
            },
            "weaknesses": {
                "value": "1. The authors didn't provide enough details for OmniInput (see questions). \n2. The proposed OmniInput was not applied to more challenging  / real world scenarios (large scale image classification, face recognition, etc.)"
            },
            "questions": {
                "value": "1. More details of OmniInput need to be provided:\n    1. How many samples do OmniInput samples for each experiment? Does it need to sample many data points to ensure at least 50 samples in each bin?\n    2. The scales of log(recall) on precision-recall graphs need to be provided?\n    3. Can this approach be applied to problems with larger scales, i.e., ImageNet classification (1000 classes)? \n2. Does the proposed metric have more advantages besides improved efficiency? For example, if a classification model achieves a higher AUC under OmniInput, can we say the model is less vulnerable to adversarial attacks (black box or white box)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_8n4h"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698501150072,
        "cdate": 1698501150072,
        "tmdate": 1699636001969,
        "mdate": 1699636001969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "caqszdaRde",
        "forum": "ExpwgjvwmC",
        "replyto": "ExpwgjvwmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission748/Reviewer_HAeb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission748/Reviewer_HAeb"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a new benchmark that uses the output distribution for model evaluation over the entire input space. Existing sampling algorithms such as GWL are used to sample representative inputs from the output distribution. Then the (human) evaluators annotate representative inputs. Finally, the authors compute the precision-recall curve to compare different machine learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Compared with the traditional data-centric evaluations, the authors present a different evaluation benchmark based on model-centric evaluation.\n2. The authors present a detailed analysis on the proposed benchmark. It is a fun read."
            },
            "weaknesses": {
                "value": "1. Over-fitting Concern: The benchmark, based on binary MNIST or the initial two classes of CIFAR10, leverages low-resolution images, making them relatively easy to recognize. Consequently, models tend to over-fit on this training set, as evident from the near-perfect accuracy rates in Table 1. Advanced CNN architectures might face the over-fitting issue, potentially leading to a subpar performance on the proposed benchmark.\n2. Scalability Concerns: The authors may consider using a more diverse or challenging dataset to truly evaluate and validate model capabilities. However, the current approach may face challenges when extended to more complex, real-world datasets. The input space becomes considerably vast for such datasets, and there appears to be an absence of efficient sampling techniques in the current framework. It would be valuable to address how the proposed method plans to tackle these scalability issues."
            },
            "questions": {
                "value": "1. Benchmark Limitation: The proposed benchmark is currently restricted to binary classification. How does this benchmark be extended to handle multi-category settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637335610,
        "cdate": 1698637335610,
        "tmdate": 1699636001898,
        "mdate": 1699636001898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "poZEwse26E",
        "forum": "ExpwgjvwmC",
        "replyto": "ExpwgjvwmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
        ],
        "content": {
            "summary": {
                "value": "Different from traditional data-centric evaluation methods based on the pre-defined test-set, this paper delves into model-centric evaluation, where the test-set is self-constructed by the model itself and the model performance is evaluated using the output distribution. Different from other model-centric evaluation methods, this paper leverages the output distribution as a bridge to generalize model evaluation from representative inputs to the entire input spaces."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A sampler is known to estimate the output distribution over the entire input space given a trained model. This paper demonstrates the importance of the sampler in model-centric evaluation frameworks, which is meaningful and inspirable."
            },
            "weaknesses": {
                "value": "- The core component of the proposed framework, the sampler to estimate the output distribution over the entire input space, just simply follows the existing work [1], which makes the technique contribution not good enough. As mentioned by the authors in the paper, the proposed method is heavily relied on the sampler, while the sampler is simply borrowed from the existing works. To some extents, this paper can be viewed as an application of the sampler in the field of model evaluation. I realize the meaning of the proposed evaluation framework, but the technique contribution is not good enough to reach the bar of ICLR.\n\n- This paper mainly focuses on a simple binary classification task to demonstrate the effectiveness of the proposed evaluation method.\n\n[1] Gradient-based wang-landau algorithm: A novel sampler for output distribution of neural networks over the input space. ICML 2023."
            },
            "questions": {
                "value": "I will make my final rating after reading the rebuttal from the authors and the reviews from other reviewers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_XWBZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699329360774,
        "cdate": 1699329360774,
        "tmdate": 1700657922128,
        "mdate": 1700657922128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AOmNUmqwRU",
        "forum": "ExpwgjvwmC",
        "replyto": "ExpwgjvwmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes OMNIINPUT which utilizes the Gradient Wang\u2013Landau sampler to sample representative data and annotate them for model evaluation. Authors validate the framework on MNIST variants. The metrics of mode evaluation involve precision and recall on the representative subpopulation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-presented with a fair storyline that motivates the work.\n2. The topic of model evaluation with a neural sampler is important.\n3. The experiment design covers a wide aspect of considerations."
            },
            "weaknesses": {
                "value": "1. Authors should show sufficient validation of the framework. The paper demonstrates validations from original MNIST as in-distribution samples and MNIST variants as out-distribution ones. Limited discussions on CIFAR-10 are shown in the appendix. The paper should conduct more convincing results from representative datasets (e.g., CIFAR-100, Tiny-ImageNet) to validate the framework.\n\n2. How to generalize the framework to the state-of-the-art vision models remains a question. The paper only evaluates ResNet variants and should further involve ViT variants.\n\n3. Existing methods of model-centric evaluations have utilized large generative models to sample OOD instances [1,2,3,4] by optimizing targeted objectives. The paper should discuss the uniqueness/effectiveness of the proposed approach compared to these baselines.\n\n[1] (ECCV 2020) SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing.\n\n[2] (ICCV 2021) Explaining in Style: Training a GAN to explain a classifier in StyleSpace.\n\n[3] (CVPR 2023) Zero-Shot Model Diagnosis.\n\n[4] (NeurIPS 2023) LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images."
            },
            "questions": {
                "value": "Please address the issues in the weakness section. I will consider revising the rating based on further responses from the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission748/Reviewer_BbXK",
                    "ICLR.cc/2024/Conference/Submission748/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699491427951,
        "cdate": 1699491427951,
        "tmdate": 1700617338617,
        "mdate": 1700617338617,
        "license": "CC BY 4.0",
        "version": 2
    }
]