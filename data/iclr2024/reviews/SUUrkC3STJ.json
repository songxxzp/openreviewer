[
    {
        "id": "NaY5wkwUFG",
        "forum": "SUUrkC3STJ",
        "replyto": "SUUrkC3STJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_QU1U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_QU1U"
        ],
        "content": {
            "summary": {
                "value": "This work try to combine PPR and transformer together to perform mini-batch training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper aims to solve the mini-batch training of graph transformer.\n\n2. the paper is well written and easy to follow.\n\n3. The performance of this work is good, comparing to the baseline."
            },
            "weaknesses": {
                "value": "1. The novelty of this work is not high, as it mainly combines random walk with attention. Tokenize the graph to sequence use random walk is common used. \n\n2. The code is not provided, its reproducibility is unclear."
            },
            "questions": {
                "value": "1. The novelty of this work is not high, as it mainly combines random walk with attention. Tokenize the graph to sequence use random walk is common used. \n\n2. The code is not provided, its reproducibility is unclear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8156/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270715878,
        "cdate": 1698270715878,
        "tmdate": 1699637010545,
        "mdate": 1699637010545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2DiiXPCYAN",
        "forum": "SUUrkC3STJ",
        "replyto": "SUUrkC3STJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_5Wvh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_5Wvh"
        ],
        "content": {
            "summary": {
                "value": "To scale the Transformer to large graphs, this paper proposes a new graph Transformer, called VCR-Graphormer. The key idea of the proposed method is to sample a token list constructed by related nodes on graphs for each target node. In this way, the mini-batch training strategy could be adopted to reduce the training cost. Moreover, the authors leverage techniques like PPR and virtual connections to preserve both local, global, long-range, and heterophilous information."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper summarizes four metrics for graph tokenization methods.\n2.\tThis paper leverages existing techniques, like PPR and the graph partition method, to generate the token list for each target node.\n3.\tThis paper provides several theoretical analyses for the proposed method.\n4.\tEmpirical results on different scale datasets seem to indicate the promising performance of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tSeveral recent studies on designing graph Transformers with node sampling or node clustering are ignored.\n2.\tExperimental results are inefficient in demonstrating the merits of the proposed method."
            },
            "questions": {
                "value": "1.\tI think the proposed method belongs to the line of designing scalable graph Transformers via node sampling. Hence, several necessary studies [1,2,3] on this research topic should be cited and discussed in the paper. [1] and [3] leverage various node sampling strategies to obtain the token list for each node, where a super node-based strategy is also adopted in [3] to preserve the global information. [2] leverage the graph partition-based strategy to reduce the training cost of the Transformer model. These researches are highly related to the proposed method, especially [1] and [3]. \n2.\tBased on Q1, I think it is necessary to compare the performance of the proposed method with [1] and [3] to demonstrate the superiority of the proposed method for constructing the node sequence.\n3.\tI think the results on heterophilous graph datasets are inefficient in demonstrating that the proposed method can handle graphs with heterophily. Important baselines [4,5,6] and datasets [7, 8] are not considered in the experiment. In addition, [8] has revealed the drawbacks of the Squirrel dataset. Hence, experiments on heterophilous graph datasets need to be reorganized to support the claim of handling heterophily property. \n4.\tSimilarly, the authors have highlighted that efficiency is one of the important metrics for graph tokenization methods. So, the necessary experiment is required to support the above claim.\n5. Does the sampling node set of the third component contain super nodes? If the answer is yes, how do you initialize the features of super nodes?\n\n\n\n[1] Zhao et al. Gophormer: Ego-Graph Transformer for Node Classification. arXiv 2021.\n\n[2] Kuang et al. Coarformer: Transformer for large graph via graph coarsening. arXiv 2021.\n\n[3] Zhang et al. Hierarchical graph transformer with adaptive node sampling. NeurIPS 2022. \n\n[4] Bo et al. Beyond Low-frequency Information in Graph Convolutional Networks. AAAI 2021.\n\n[5] Chien et al. Adaptive universal generalized pagerank graph neural network. ICLR 2022.\n\n[6] Li et al. Finding Global Homophily in Graph Neural Networks When Meeting Heterophily. ICML 2022.\n\n[7] Lim et al. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. NeurIPS 2021.\n\n[8] Platonov et al. A critical look at the evaluation of GNNs under heterophily: are we really making progress? ICLR 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8156/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719629885,
        "cdate": 1698719629885,
        "tmdate": 1699637010419,
        "mdate": 1699637010419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rma2mxozH0",
        "forum": "SUUrkC3STJ",
        "replyto": "SUUrkC3STJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_82Qp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_82Qp"
        ],
        "content": {
            "summary": {
                "value": "In this work the authors propose a way for effective mini-batch training of graph transformers. In their approach, namely Virtual Connection Ranking Graph Transformer (VCR-Graphformer), they build a special token list for each input target node as input to a transformer. This list consists of four components including: \n- (1) the node features of the target node, \n- (2) propagated features for a number of random walk steps and features of nodes with top Personalized PageRank (PPR) when PPR is run on extended graphs (rewiring) with extra virtual nodes connected to original nodes either \n- (3) in the same cluster (structure-based global information) or\n- (4) carrying the same label (content-based global information). \n\nThey provide theoretical justification for their direction, experiment with their mini-batching approach over a collection of small, large and heterophilous graph datasets, employing various Graph Neural Network (GNN) and Graph Transformer (GT) architectures as baselines, and report on (a) superior competitive node classification accuracies for their VCR-Graphormer and (b) ablation studies clarifying the role of components in the token list and parameter choices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation is easy to follow and the intuition of the approach well supported.\n\n- Types, number of datasets and baseline models are adequate for demonstrating the efficacy of the approach for the node classification task in particular. Ablation studies are very informative (Figures 3 and 4)."
            },
            "weaknesses": {
                "value": "- The inclusion of additional graph learning tasks (edge/graph classification) would further establish the validity/generality of the token list preparation approach proposed."
            },
            "questions": {
                "value": "- VCR-Graphormer seems to be an input preprocessing technique (preparation of the token list for input to standard transfromer layers as in Eq (3.4)) that is compatible with the target training mode (mini-batching) rather than a graph transformer architecture (which is what the reader would possibly expect when coming across the term \"VCR-Transformer\"). Perhaps emphasizing this view early in the presentation would be beneficial in comprehending the overall idea?\n\n- Do you identify any obstacles for this technique being effective for other (large-scale) graph learning tasks (not node classification)? \n\n- VCR-Graphormer supports effective heterophilous graph node label learning through one of its token list components. Would this also promote homophilous graph learning?\n\nMinor/typos\n- Page 3: share the exactly same -> share exactly the same\n- Page 7: eigendecompostion -> eigendecomposition\n- Page 9: strcuture -> structure\n- Page 9: Targeting node-level tasks like graph classification -> ? (graph classification is not a node-level task)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8156/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824064197,
        "cdate": 1698824064197,
        "tmdate": 1699637010304,
        "mdate": 1699637010304,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RiwKbPUaaj",
        "forum": "SUUrkC3STJ",
        "replyto": "SUUrkC3STJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_jsyZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8156/Reviewer_jsyZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes VCR-Graphormer, which resolves the computational challenges of graph transformers in large-scale graphs. The authors tokenize nodes using personalized PageRank (PPR), enabling mini-batch training. Additionally, they introduce virtual connections in the graph to encode local and global contexts, long-range interactions, and heterophilous signals. This approach reduces computational complexity and outperforms or is on par with existing methods in node classification across 12 datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The scalable graph transformer is a hot, interesting, and important field in our research community. Training with mini-batches is memory-efficient by nature. The experiments are extensive (but focus on homophilous datasets). The proposed method outperforms baselines by a large margin, especially for heterophilous graphs."
            },
            "weaknesses": {
                "value": "- The paper is hard to follow. Abuse of notations in Eq 3.1, 3.2, and 3.3 might be confusing for readers. Please formally define the operations or use well-known operations. Is {.} a set or a list? Please proper set-builder notations. Please put l (l-th step) to the name of the variable, r_u? Is the concatenation operator applied to both scalars and vectors? The cardinality of T_u in Eq 3.3 is 4 when you note it like this. Plus, it would be nice if the authors explained what insights we can see in a series of theorems 3.2 and 3.3.\n- The eigendecomposition is not a core component in NAGphormer. NAGphormer without structural encoding has shown no bad results. We can choose other cheap structural encodings rather than eigendecomposition, for example, PPR vectors as the authors say. Thus, the superiority of the proposed method against NAGphormer (e.g., cubic complexity) should be rewritten focusing on the core components (Hop2Token).\n- The performance increase in homophilous and large-scale datasets is marginal. VCR-Graformer's additional modules do not seem to be effective for these datasets. Instead, the proposed method is effective for heterophilous datasets. However, these datasets are small-scale thus, only evaluating small parts of this model (that targets large-scale graphs). It would be nice if the authors can use large-scale heterophilous datasets [Lim, Derek, et al.]. In addition, experiments to evaluate modeling long-range dependency (the third component) are not conducted.\n- Runtime evaluation on efficiency is required. Although the time complexity is decreased in theory, the actual computations can be increased by METIS and PPR on three types of graphs.\n\n## References \n- [Lim, Derek, et al.] Lim, Derek, et al. \"Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.\" Advances in Neural Information Processing Systems 34 (2021): 20887-20902."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8156/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840133501,
        "cdate": 1698840133501,
        "tmdate": 1699637010199,
        "mdate": 1699637010199,
        "license": "CC BY 4.0",
        "version": 2
    }
]