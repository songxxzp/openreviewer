[
    {
        "id": "pnApPROPxO",
        "forum": "AF9Q8Vip84",
        "replyto": "AF9Q8Vip84",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission338/Reviewer_g1yJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission338/Reviewer_g1yJ"
        ],
        "content": {
            "summary": {
                "value": "The paper mainly makes two parts of contributions. The first one is a benchmark, SLMTokBench, which assesses the suitability of speech tokens for building speech language models. The second one, which is also the main contribution of the paper, is a unified speech tokenizer that combines both semantic and acoustic tokens. SpeechTokenizer performs comparably with pure acoustic token (EnCodec) in speech reconstruction, and its performance is much higher than SpeechTokenizer in VALL-E."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a novel method to unify the semantic tokens and acoustic tokens. \n\n2. The experiments show that the unified tokenizer, SpeechTokenizer, large improves the acoustic tokens from EnCodec in the zero-shot TTS experiment in VALL-E. The improvements are significant in both objective metrics and subjective metrics. \n\n3. The proposed benchmark, SLMTokBench, is well motivated and reasonable.\n\n4. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. My first concern is about the necessity for a unified speech tokenizer. In the last several sentence of page 1, the authors argue that \"the multi-stage modeling approach is more complex, leading to several drawbacks such as error accumulation and slower processing speed\". Is there any evidence to support the claim? In addition, I may somewhat disagree with how the paper describe the characteristic of \"Semantic LM and Acoustic LM\" in Table 1. In my view, Semantic LM does not generate Accurate Content but it is speed is much faster. And Acoustic LM does generate Accurate Content. \n\n2. The second concern, which is also my major concern, is about whether the complicated training pipeline really leads to better performance in zero-shot TTS than Hierarchical LM. If using VALL-E with the semantic token for HuBERT and RVQ1:7 from EnCodec (It has the same bit rate as USLM), what is performance?\n\n3.  At last, I am doubtful whether the SpeechTokenizer only works for AR + NAR model like VALL-E. As VALL-E only uses the first speech token in the AR model, putting more information on the first token may be beneficial to its performance so that the improvement of USLM in zero-shot TTS is much larger than the improvements of SpeechTokenzier in speech reconstruction. I am not sure such large improvements will still exist in Hierarchical model like AudioLM. \n\nI understand that weakness 3 is hard to verify during the rebuttal. Therefore, If the authors can resolve my first two concern, I am willing to raise my score."
            },
            "questions": {
                "value": "Can the author provide some explainations why the improvement of USLM in zero-shot TTS is much larger than the improvements of SpeechTokenzier in speech reconstruction (Table 4 v.s. Table 2) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The technique of the paper may be used to produce fake speech of humans, which I believe could potentially harm the society."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission338/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission338/Reviewer_g1yJ",
                    "ICLR.cc/2024/Conference/Submission338/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698302058622,
        "cdate": 1698302058622,
        "tmdate": 1700724926900,
        "mdate": 1700724926900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b5whjdKc6A",
        "forum": "AF9Q8Vip84",
        "replyto": "AF9Q8Vip84",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission338/Reviewer_usJL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission338/Reviewer_usJL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed SpeechTokenizer to learn unified speech tokenizer for language models.  Experiments demonstrate that the Unified Speech Language Model (USLM) leveraging SpeechTokenizer SpeechTokenizer shows comparable performance when compared with EnCodec in speech reconstruction and SLMTokBench benchmark. In zero-shot Text-to-Speech tasks, USLM achieves better performance than VALL-E."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of the paper is straightforward and push the boundary of speech language models to more unified representation for different tasks. \n2. Extensive experiments are done to validate the performance comparison."
            },
            "weaknesses": {
                "value": "1. The paper misses the comparison with Hierarchical speech language models as listed in the paper. if the comparison could be added, it would be more convincing.\n2. Although the motivation of the paper is a shining point for the paper, the technical contribution of the paper is limited as the framework is borrowed from existing work, e..g, the framework of RVQ-GANs. If authors could propose some modifications on the training strategy or the model architectures based on some observations from experiments, it would add more insightful points.\n\nIf my above concerns are resolved, I would consider increasing my rating."
            },
            "questions": {
                "value": "My questions are listed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698492562103,
        "cdate": 1698492562103,
        "tmdate": 1699635960939,
        "mdate": 1699635960939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hDoRVdXJJD",
        "forum": "AF9Q8Vip84",
        "replyto": "AF9Q8Vip84",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission338/Reviewer_pbGo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission338/Reviewer_pbGo"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the limitations of current speech representations in large language models. It introduces a benchmark called SLMTokBench to assess the suitability of existing speech tokens for speech language modeling. The results indicate that neither semantic nor acoustic tokens are ideal for this purpose. To overcome this, the authors propose SpeechTokenizer, a unified speech tokenizer for speech large language models.\n\nSpeechTokenizer utilizes the Encoder-Decoder architecture with residual vector quantization (RVQ) and combines semantic and acoustic tokens. It disentangles different aspects of speech information hierarchically across RVQ layers, providing a more comprehensive representation of speech data. The authors also construct a Unified Speech Language Model (USLM) using SpeechTokenizer.\n\nExperimental results demonstrate that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and shows strong performance on the SLMTokBench benchmark. Overall, the paper introduces SpeechTokenizer as a solution for improving speech language models by addressing the limitations of existing speech tokens."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of the paper is strong. Speech research area needs speech specific fundamental innovation. Many papers simply borrow ideas from NLP and CV and test their performance on speech data. Exploring how to discretize continuous speech signals is an intriguing concept. The authors propose a simple and logical approach, which serves as a promising initial step for the field.\n\n\n2. SLMTOKBENCH offers valuable insights for evaluating current speech tokenization methods. It has the potential to become a fundamental benchmark for speech tokenization research. However, it still requires some improvements, as mentioned in the weaknesses . section."
            },
            "weaknesses": {
                "value": "1. My primary concern lies with the experimental section of the paper. Firstly, the authors trained SpeechTokenizer on the LibriSpeech set, while the EnCodec was trained on a mixed dataset of 10,000 hours. As a result, EnCodec should possess better generalization capabilities for speech reconstruction, particularly in the presence of noise. However, the authors only evaluated the performance on LibriSpeech, which is an in-domain test set for their method but falls outside the domain of EnCodec. This unfair experiment fails to demonstrate whether their method outperforms EnCodec. A fair experiment should involve testing the model on various test sets, especially those unseen during training.\n\nSLMTOKBENCH should encompass a range of speech environments, not limited to LibriSpeech alone. Audiobooks represent just one type of speech data, and we should consider incorporating a more diverse test set for a comprehensive evaluation.\n\n\n2.How did you arrive at the WER number of 7.9 in Table 4? I recall that the VALL-E paper reported a WER of around 3.X on LibriSpeech.\nAdditionally, other studies on LibriSpeech have achieved comparable or even lower WER values.\n\n3. In the context of the speech tokenizer, it is necessary to take into account additional baselines like SoundStream and the multi-band diffusion-based method, rather than solely comparing it with Encode."
            },
            "questions": {
                "value": "Why the authors not directly cite the number in VALL-E paper for evaluation? I don't think it is a good idea to re-train VALL-E on a smaller training set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698590427394,
        "cdate": 1698590427394,
        "tmdate": 1699635960855,
        "mdate": 1699635960855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z9yHptTNl7",
        "forum": "AF9Q8Vip84",
        "replyto": "AF9Q8Vip84",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission338/Reviewer_tD35"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission338/Reviewer_tD35"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a new model to learn a speech tokenizer that can both reconstruct speech well and retain enough semantic information in speech. It simplifies previous work that need to use separate tokenizers for semantic and acoustic information. Since good tokenizers play an important role in speech language models, the results in this paper could benefit the advancement of research in speech language models in the LLM era."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: this is the first work trying to inject semantic information to acoustic tokens based on soundstream/encodec which has been successfully applied to speech language modeling, TTS, etc. Though the idea is kind of straight, this paper still deserves originality.\nQuality: Overall the solution is clearly motivated and reasonably implemented, and corresponding evaluations are comprehensive.\nClarity: the paper is easy to read, the results are easy to understand.\nSignificance: A good tokenizer is important for many downstream tasks, especially for speech generation tasks. This study shows it's possible to use a single model as tokenizer and achieve comparable or even better performance than previous studies using combination of different tokenizers encoding different aspects of speech"
            },
            "weaknesses": {
                "value": "1. It's not quite clearly explained why the authors chose the first RVQ to inject semantic information. Why not the later ones or even for all RVQs\n2. It could be better if the authors can show some experiments using the new tokenizers for speech understanding tasks like recognition/speech translation with relatively large scale training data, and compare to Hubert/soundstream/encodec tokens."
            },
            "questions": {
                "value": "1. Is it possible to directly combine the training loss of hubert and soundstream/encodec instead of distillation to achieve unified speech tokenizer?\n2. Since soundstream/encodec is a small model, is it possible the model can learn good representations when the model is large enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820265374,
        "cdate": 1698820265374,
        "tmdate": 1699635960785,
        "mdate": 1699635960785,
        "license": "CC BY 4.0",
        "version": 2
    }
]