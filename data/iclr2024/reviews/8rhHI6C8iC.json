[
    {
        "id": "zpPyvyNIWV",
        "forum": "8rhHI6C8iC",
        "replyto": "8rhHI6C8iC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_3cLo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_3cLo"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a federated learning scheme that captures the advantages of both generic FL and personalized FL. The feature extractor can be trained based on any generic FL strategy (e.g., FedAvg). Then, the remaining plug-in module $\\rho$ is trained based on any personalized FL method. This part is then uploaded to the server. During test-time, each client can selectively download the plug-in module and make the prediction. Experiments show that the methodology performs well on both generic and personalized datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Taking advantage of both generic FL and personalized FL is interesting and important.\n\n2. Experimental results are promising.\n\n3. The paper is generally easy to follow."
            },
            "weaknesses": {
                "value": "1. I believe the test-time strategy of this method is somewhat impractical and unclear:\n- Whenever a test sample appears, the scheme requires the client to download all the features from the server (which is same as the number of clients in the system), and compute (8). This can incur communication, computation, and latency issues. \n- Moreover, each client may need to download multiple plug-in modules during inference, which will again incur additional communication and latency. For example, suppose there are three clients, each having classes (1, 2), (3, 4), (5, 6) in its local dataset, and produce plug-in modules 1, 2, 3, respectively. Now for an arbitrary client X, when the first test sample belongs to class 1, client X will download the plug-in module 1. Then, if the next test sample belongs to class 2, client X will download the plug-in module 2. Finally, it will also download plug-in module 3 whenever classes 5 or 6 appears in the test set. Is this really happening in the proposed method?\n- Finally, it is not clear how the server should generate $h_m$ tailored to a specific client. For example, again consider an example with three clients having classes (1, 2), (3, 4), (5, 6). If class 1 appears as a test sample in client 1, what should the server send to this client? In other words, what are $h_1$, $h_2$, $h_3$ in this case?\n\n2. The discussion with the prior work (Chen & Chao, 2021) is not clear to me. In (Chen & Chao, 2021), the authors are also designing a generic feature extractor, and two plug-in heads: one is used for the generic scenario and the second is used for the personalized scenario. If the authors aim to do both generic and personalized testing during inference, one may simply choose between the generic plug-in module and the personalized plug-in module developed in (Chen & Chao, 2021), instead of selecting the plug-in modules from all clients in the system (this can significantly improve resource consumptions while achieving the similar performance). What is the advantage/contribution of the authors work compared to these methods/concepts? I'm not sure whether the authors' approach will provide any advantages compared to this method. The authors should provide comprehensive experiments as well as discussions regarding this. Related to this comment, probably the authors' scheme can have advantage compared to (Chen & Chao, 2021) when considering mixed dataset of personalized and generic dataset similar to as done in [Ref]: Each client's test dataset has more local classes when the distribution shift is small, while having more classes outside of the client's local dataset when the distribution shift is large.\n\n(Chen & Chao, 2021) ON BRIDGING GENERIC AND PERSONALIZED\nFEDERATED LEARNING FOR IMAGE CLASSIFICATION\n\n[Ref] SplitGP: Achieving both generalization and personalization in federated learning, arXiv e-prints (2022): arXiv-2212."
            },
            "questions": {
                "value": "Please refer to the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Reviewer_3cLo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698466894510,
        "cdate": 1698466894510,
        "tmdate": 1699636186989,
        "mdate": 1699636186989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4zYKm4rhZu",
        "forum": "8rhHI6C8iC",
        "replyto": "8rhHI6C8iC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_TW8h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_TW8h"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Hot-Pluggable Federated Learning (HPFL), a framework that addresses the challenge of adapting federated learning (FL) models to test data distributions via combining a shared global feature extractor and selected personalized plug-in modules based on test data. During inference, a selection algorithm helps clients choose suitable plug-in modules to achieve high generalization. Experimental results show that HPFL outperforms other FL algorithms and has potential applications in some FL scenarios, such as continual federated learning and one-shot FL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper's framework clearly illustrates the process and main modules of the proposed framework, making the method easy to understand. \n- The idea of applying HPFL to continual federated learning, one-shot federated learning, and a federated learning plugin market is interesting, and the authors provide a figure of the effectiveness of continual federated learning with HPFL.\n- The relevant findings in the experiments regarding the impact of plugins at different layers on performance are interesting and insightful."
            },
            "weaknesses": {
                "value": "- The combination of using a separate backbone and classifier in PFL for improvement, along with the idea of customize the classifier on local client, is not novel. The former has been widely validated for its effectiveness in FedPer and FedRep, while the latter's idea is highly relevant to that of KNN-Per [1] and FedBABU [2]. However, the authors do not provide relevant discussion or experimental comparisons of the latter ones. \n- Since the authors compared the performance of both GFL and PFL, the experiments should include more commonly used GFL-related baselines.\n- There is a missing detailed analysis of potential privacy risks in uploading and sharing local features.\n\n\n[1] Marfoq, Othmane, et al. \"Personalized federated learning through local memorization.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Oh, Jaehoon, SangMook Kim, and Se-Young Yun. \"FedBABU: Toward Enhanced Representation for Federated Image Classification.\" International Conference on Learning Representations. 2021."
            },
            "questions": {
                "value": "Regarding the experimental data, I have some concerns.\n- The performance of PerFedMask (Table 1) in the personalized setting is much lower than other baselines. I wonder if there was an error in implementing this method, or if its hyper-parameters were not appropriately fine-tuned? \n- About the experiments with added noise (Table 4). Why do experiments with different levels of noise produce nearly the same results? This appears somewhat counterintuitive. Can the authors provide a more detailed analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Reviewer_TW8h",
                    "ICLR.cc/2024/Conference/Submission2503/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693874181,
        "cdate": 1698693874181,
        "tmdate": 1700637394168,
        "mdate": 1700637394168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tbVnmSwhFV",
        "forum": "8rhHI6C8iC",
        "replyto": "8rhHI6C8iC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_ENEM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_ENEM"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a relatively new and general setup of personalized FL that serves several clients possibly from a new distribution and/or holding only unlabeled test data.  \n\nThe framework is based on general FL with several customized plug-in modules learned by personalized FL. Initially, clients acquire a global shared feature extractor. Subsequently, utilizing this feature extractor as a fixed foundation, clients proceed to train multiple personalized plug-in modules individually. These personalized modules are tailored to the specifics of their local data and are then stored in a modular repository on the server.\n\nDuring the inference stage, a selection algorithm is employed that enables clients to make precise choices, selecting and downloading the most appropriate plug-in modules from the modular repository. The experiments show that the method achieves high generalization performance, particularly when confronted with target data distributions that may differ significantly from the initial training data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper proposes an interesting aspect extending the existing literature of bridging generic FL and personalized FL. The proposed selective FL framework is sound in some FL scenarios. \n* The paper provides discussions and solutions on the practical drawbacks of selective FL including privacy concerns and the system overhead. \n* Extensions to other cases like continual learning are appreciated\n* Limitations, impact, and related works are properly discussed."
            },
            "weaknesses": {
                "value": "* Some parts of the design choices of the proposed selective FL could be elaborated more to give the paper a broader scope. For example, the part of the general feature extractors can be trained with any GFL algorithm. Considering there are many GFL methods proposed, what are their performances in comparison? Are they compatible with selective FL?  For the selection methods in section 4.3, could the author provide more insights and discussion besides the used distance-based methods? Any other potential replacements? \n\n* Considering FedRoD is the sota in this topic evidenced by Table 2, can the authors provide a more fine-grained comparison? Specifically, FedRod (also followed by FedTHE) proposes to train the general model with a class-balanced loss; will this improve HPFL as well? FedRoD also fine-tunes the whole general model for each client with local training; how many personalized epochs are trained like the reported HPFL (e.g., E_p=1 or 10)?\n\n* There are many typos and inconsistent wording"
            },
            "questions": {
                "value": "Can the authors provide a discussion on the data/client assumptions on when should HPFL work or not work? Suppose the new client has very different distributions unseen in the training, how possible will HPFL work well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721361104,
        "cdate": 1698721361104,
        "tmdate": 1699636186845,
        "mdate": 1699636186845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P2TogIt4t7",
        "forum": "8rhHI6C8iC",
        "replyto": "8rhHI6C8iC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_nQmW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_nQmW"
        ],
        "content": {
            "summary": {
                "value": "The paper motivates a new federated set-up in which, at test time, a client may encounter data drawn from the distribution of other clients in the collaboration. The paper then proposes a solution for this set-up, which proceeds in two stages: (1) learning a global feature extractor, and (2) learning personalized plug-in modules that are stored in the server and selected dynamically at inference time. The paper presents results on a variety of standard datasets with a limited number of clients."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- To my knowledge, the introduced paradigm (Equation 3) is novel and the approach to solving it (Theorem 3.2) seems intuitive. I did not check the math carefully. \n- The gains over the presented baselines do seem significant. However, presenting error bars would strengthen the claims in the manuscript.\n- In the same vein, the evaluation framework presented by the paper, focusing on the global data distribution while also presenting the performance on the local distribution, seems appropriate for the task they present."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is its presentation. In particular, the manuscript is not explicit about the form of the test distribution that the clients will encounter. I was confused about this, and could only infer it until I reached Section 3.3 and I finally understood the problem's objective.\n- Similarly, it is not clear to me whether the method is proposed for cross-device or cross-silo settings. Although the motivating example in Section 1 is cross-device, the experiments are performed with a limited number of clients (<= 100). More concerning, in the proposed algorithm, the server stores all the training clients' plug-ins. In cross-device scenarios with a massive number of clients, this would not scale. There is no discussion of this in the paper.\n- Storing all the clients' plug-ins may also be a privacy risk, as there is not any aggregation that protects the clients' privacy from a malicious server. The threat model is not discussed by the paper."
            },
            "questions": {
                "value": "- Although the given set-up is novel, I am still not convinced of its importance, at least not justify the drawbacks of the proposed solution. I recommend the manuscript update its introduction to include more motivating examples that are explicitly related to their problem formulation. \n- Although the presented evaluation framework is good for the presented problem, a more exhaustive framework could be formulated to rigorously test the approach, e.g., one in which train client $m$ is given test data $n$, and we check if it indeed downloads plug-in $n$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2503/Reviewer_nQmW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777907713,
        "cdate": 1698777907713,
        "tmdate": 1699636186772,
        "mdate": 1699636186772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C2r3FY4vxv",
        "forum": "8rhHI6C8iC",
        "replyto": "8rhHI6C8iC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_MHGx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2503/Reviewer_MHGx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an FL framework called HPFL that addresses a problem named selective federated learning, which aims to balance the trade-off between generic federated learning and personalized federated learning in the presence of data heterogeneity and distribution shifts. It consists of a globally trained shared feature extractor and multiple personalized trained plug-in modules. The authors conduct comprehensive experiments to validate the proposed methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper introduces a new problem, SFL, which bridges the generic and personalized FL methods.\n* The paper is well written, and the experiments are comprehensive."
            },
            "weaknesses": {
                "value": "* The paper has low novelty. Federated learning and fine-tuning the sub-modules are really common and baseline personalized FL methods."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699243585437,
        "cdate": 1699243585437,
        "tmdate": 1699636186699,
        "mdate": 1699636186699,
        "license": "CC BY 4.0",
        "version": 2
    }
]