[
    {
        "id": "UGRf7VXkSw",
        "forum": "zH6zBoktYO",
        "replyto": "zH6zBoktYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_KAMN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_KAMN"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework for self-supervised evaluation of Large Language Models (LLMs) by proposing a series of sensitivity (invariance) metrics that assess various aspects of language model behavior without the need for human-labeled datasets. These metrics evaluate the models based on their reaction to input transformations concerning knowledge via negations, toxicity, and word order. The authors claim that these self-supervised evaluation methods can complement traditional supervised benchmarks and provide efficient evaluation in real-world settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed self-supervised evaluation framework addresses the significant challenge of evaluating LLMs without the extensive need for labeled datasets, which is a common bottleneck.\n\n- The authors provide empirical evidence that correlates the proposed self-supervised metrics with existing supervised benchmarks, lending credibility to their approach.\n\n- By analyzing how LLMs react to various textual transformations, the paper offers deeper insights into the behavior and limitations of these models, which can inform future research and model development."
            },
            "weaknesses": {
                "value": "- The paper acknowledges model entropy as a factor that could influence sensitivity scores but does not explore it in detail. Understanding how the entropy of a model's output distribution affects evaluation metrics is crucial for interpreting results accurately.\n\n- The main methods proposed in this work\u2014knowledge probing via negations, toxicity detection, and word order\u2014seem to be presented as separate entities without a unifying theme or rationale that clearly ties them together. The paper could benefit from a more cohesive narrative that explains how these methods collectively advance the understanding and evaluation of LLMs.\n\n- The proposed methods, such as adding \"not\" after certain words for negation or appending trigger words for toxicity, may seem too basic or trivial. This simplicity could lead to questions about the depth and sophistication of the approach, as well as its ability to capture the nuances of language and behavior of LLMs. The straightforward nature of the methods may not generalize well to the complex and varied inputs that LLMs encounter in real-world applications. The paper might not demonstrate that the methods can handle different linguistic constructions, idiomatic expressions, or contextual nuances.\n\n- The motivation behind each method is not evidently articulated. While each method addresses a different aspect of language model behavior, the paper may not clearly explain why these particular aspects are chosen and how they complement each other in providing a comprehensive evaluation.\n\n- The methods may not be backed by a comprehensive set of experiments to validate their effectiveness across different models, domains, prompts, and languages. This could be seen as lacking in terms of the breadth and depth of experimental validation."
            },
            "questions": {
                "value": "- Could you explain the rationale behind the simplicity of the proposed methods? How do you ensure that such straightforward techniques can provide a robust evaluation of complex LLM behaviors?\n\n- To what extent did you consider more sophisticated prompt engineering in your evaluation framework? Could you elaborate on how different prompt designs might affect the outcomes of your proposed metrics? For example, how could changing the position of trigger words in toxicisy detection influence the outcome?\n\n- Can you discuss any additional experiments that might demonstrate the robustness of your evaluation methods across different languages, dialects, or domains?\n\n- How might the entropy of a model's output distribution or its propensity for memorization affect the outcomes of your self-supervised evaluation metrics?\n\n- What steps did you take to mitigate the impact of potential confounding factors, such as model overfitting or exposure to similar data during training, on your evaluation results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750843403,
        "cdate": 1698750843403,
        "tmdate": 1699636985472,
        "mdate": 1699636985472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AeLWDOIsbp",
        "forum": "zH6zBoktYO",
        "replyto": "zH6zBoktYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_tXds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_tXds"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new self-supervised approach to the evaluation of LLMs, alleviating the need for small domain-specific datasets with human-curated labels as in traditional evaluations. The new evaluation method, on a higher level, is through analyzing invariances and sensitivities to transformations. The work provides detailed case studies on several self-supervised evaluation strategies for different aspects of LLMs, including those related to negations, toxicity detection, long-range dependency, and sensitivity to word order, etc. Strong correlations have been shown between the designed self-supervised evaluation metrics and human-supervised evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Originality**: This paper offers a fresh approach to the evaluation of LLMs, moving away from dataset-bound evaluations to a sustainable assessment methodology.\n  \n- **Quality**: The transformations, like handling of negations, word order changes, and others, are impressive steps toward achieving a holistic evaluation of LLMs. \n  \n- **Clarity**: The paper is accessible to even readers that are unfamiliar with the domain. The delineation of their methods and results is commendable.\n  \n- **Significance**: The paper\u2019s methodology, if thoroughly verified and broadly adopted, has the potential to revolutionize how LLM evaluations are conducted, making them more dynamic, comprehensive, and reflective of real-world applications."
            },
            "weaknesses": {
                "value": "- **Terminological Ambiguity**: The usage of \"self-supervised\" is somewhat misleading. Given the context of this paper, an alternative term or a more precise explanation would be helpful.\n\n- **Metric Soundness**: While the paper puts forth several innovative metrics, further clarity and validation are needed. For example, the paper justifies its methods heavily by calculating the correlation between proposed scores and one specific existing task. Then I believe a natural question arises \u2013 if, for example, having high correlation with TriviaQA alone is enough to demonstrate the legitimacy of SSE, then why don\u2019t we just use TriviaQA accuracy (and also HellaSwag) as the evaluation metric? I am therefore doubtful of the significance of the proposed metrics.\n \n \n- **Questionable Conclusion**: Following up on the comment on the correlation analysis, the paper tries to prove the usefulness of their metrics by showing correlation with existing task on some metrics (e.g. TriviaQA accuracy), but also tries to disprove other evaluation metrics by showing they correlate less nicely with their proposed metrics. For example, the paper concludes perplexity is not a robust evaluation metric because it does not have a very high correlation with SSE; the Cohere Command model is an outlier in their analysis which highlights a weakness of TriviaQA.\n\n- **Visualization**: The visualization in this paper is unfriendly to people with color vision deficiency if not anyone. I find it very hard to distinguish to the difference in the sizes and colors in the figures representing different models."
            },
            "questions": {
                "value": "- Can authors provide some examples of how they perform the transformations to evaluate long-range sensitivity.\n\n- How do you identify the neutral corpus?\n\n- \u201cWe further explore why correct normalization is important by cross-referencing the frequency with which perplexity goes down rather than up, see Figure 14 in Appendix A.5.\u201d from the figure I see a nice negative correlation between \u201cPercent PPL Drops\u201d and TriviaQA accuracy. So how can this show correct normalization is import in SSE?\n\n- What data do the metric PPL use? Following this paper\u2019s logic, I think the first step this paper should do is to evaluate the correlation between, for example, normalized PPL and TriviaQA accuracy (and other realistic tasks such as MMLU etc).\n\n- How well do toxicity scores by this paper correlate with those by Perspective API on instruction tuned models? From figure 6 left, it looks like there are no Xs. Same question for word order section and figure 4, 7. Without the results of instruction tuned models, it is hard to see if the scores correlate better on vanilla models than instruction tuned models. And therefore hard to assess the significance of the metrics.\n\n- It is still hard for me to understand how to use this paper in real-world applications. How adaptable are these proposed metrics? Would the methodology need alterations to assess LLMs in more dynamic, real-world scenarios?\n\n- Given the terminological ambiguity around \"self-supervised,\" can authors elaborate on this terminology or propose an alternative name to prevent misconceptions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786794121,
        "cdate": 1698786794121,
        "tmdate": 1699636985358,
        "mdate": 1699636985358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iEXKS5JBwA",
        "forum": "zH6zBoktYO",
        "replyto": "zH6zBoktYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_cArL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_cArL"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a self-supervised method for evaluating LLMs without relying on domain-specific, human-annotated datasets. The authors detail a structured approach for self-supervised evaluation, focusing on LLM invariances and sensitivities. Preliminary tests indicate a correlation between their proposed metrics and established ones that depend on human annotations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The paper unveils a fresh evaluation technique for Large Language Models that minimizes human intervention. This addresses the shortcomings of conventional methods and provides an alternative measure of a model's capabilities.\n- Their approach can be adapted to different data domains by simply varying the unlabeled text in evaluations. Such as the clinical setting, where they show their method has a good correlation with MMLU (clinical)."
            },
            "weaknesses": {
                "value": "- Weak evidence for the robustness of this method: In my opinion, using the correlation to TriviaQA's accuracies to prove their usefulness seems to be a bit of weak evidence, especially when only ~10+ models are considered in the experiments. The correlation results can be simply affected by noises/outliners. For example, you mentioned that \"The Pearson correlation between TriviaQA and Normalized sensitivity score is 0.76 for vanilla models and 0.73 for instruction models after removing the Cohere Command outlier\". It proves that the evaluation method is a little bit brittle. When I have a new model that wants to be tested, how can I know my model is not another outlier?\n- I would suggest the authors increase the number of models to 100+, so as to reduce the effects of noise when computing the correlations. If there are not enough LLMs to be tested, one of the strategies you can use is to do early exiting from the models [1] so that you can get different outputs from different layers of the model, representing different levels of understanding of the data, and thus increase the number of total data points to be compared.\n\n[1] Eliciting Latent Predictions from Transformers with the Tuned Lens\nNora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, Jacob Steinhardt https://arxiv.org/abs/2303.08112"
            },
            "questions": {
                "value": "- In figure 3, 4, 6, and 7, why the models used for comparison is changing all the time? In figure 3 there are 28 data points but in figure 7 there are only 11 data points. What's your standard of selecting these models to be evaluated? I think it's better to include all the models throughout the experiments to reduce the effects of noise/outliers. The experiment results presented make me feel that some of the results are hidden so the correlation may not be that good if we add them back to the figure."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793273308,
        "cdate": 1698793273308,
        "tmdate": 1699636985240,
        "mdate": 1699636985240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NhFFhA6fS3",
        "forum": "zH6zBoktYO",
        "replyto": "zH6zBoktYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_AbSH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8000/Reviewer_AbSH"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a self-supervised evaluation approach for toxicity classifiers. The paper cites that due to training data leakage, toxicity classifiers may have an inflated reported performance. Using the proposed approach where inputs are modified using negation and other techniques, the robustness of content classifiers is evaluated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has the following strengths.\n\nFirst, it is well-written. The paper motivates the problem well, experiments are described clearly. Related work descriptions are reasonable (although the paper missed some key citations, e.g., Gr\u00f6ndahl et al.). \n\nSecond, the motivation for building a domain-specific toxicity classifier is well-received."
            },
            "weaknesses": {
                "value": "The key weakness of the paper is:\n\n1. The modifications it is suggesting are too simplistic (e.g., F-bombing). There are many known modifications that have tripped content classifiers before (e.g., All You Need is \"Love\": Evading Hate Speech Detection by Gr\u00f6ndahl et al.) or examples of real-world examples tripping content classifiers (e.g., Are Chess Discussions Racist? An Adversarial Hate Speech Data Set by Sarkar and KhudaBukhsh). Instead of using obfuscation techniques well-grounded in literature, the paper adopts simplistic techniques to modify inputs.  \n\n2. The second weakness of the paper is it relies on Perspective API. Perspective API's toxic scores are not reliable. Recent research indicates that. Hence, an API that itself has calibration issues cannot be very useful for calibrating other systems."
            },
            "questions": {
                "value": "My questions are how would the authors respond to my two weaknesses listed above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8000/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8000/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8000/Reviewer_AbSH"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8000/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699916249271,
        "cdate": 1699916249271,
        "tmdate": 1699916404107,
        "mdate": 1699916404107,
        "license": "CC BY 4.0",
        "version": 2
    }
]