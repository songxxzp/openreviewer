[
    {
        "id": "09gxmthW7H",
        "forum": "pUIANwOLBN",
        "replyto": "pUIANwOLBN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_yYKF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_yYKF"
        ],
        "content": {
            "summary": {
                "value": "The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically."
            },
            "weaknesses": {
                "value": "Typos and minor suggestions:\n- Discount factor backwards bracket in 2.1\n- Is (1) missing the sum?\n- \u201cThe valley is composed of two floors\u201d: \u201cfloors\u201d is a strange way to phrase it, and the intended meaning was initially unclear to me.  Consider rephrasing to something like \"The valley contains two separate low points\u201d.\n- If there\u2019s room, you could point to an appendix to briefly remind the reader why the middle and RHS of (7) are the same as alpha -> 0.  I know it\u2019s fundamental SGD theory, but it confused me for a few minutes in this context (especially since, at first glance, the equation is not true, since it is just a first-order approximation and is only true as alpha approaches 0).\n- In 4.2, the thorough description of the environment is nice, but I think it\u2019s missing the time limit used.  Also, were the curves given in Figure 3 computed analytically or empirically?  Some more info might improve the paper.\nIn 4.2, you could remind the reader what (4) is so that they do not have to hunt for it.\n\nWeaknesses:\n1) Some of the questions below are clarity weaknesses or other possible weaknesses.\n2) The contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.\n3) The paper overall is a bit challenging to read.  For an example of a well-written bit that does *not* have this issue, the conclusion says \u201cFirst, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to optimal policies.\u201d  This is great; more \u201chigh-level summary\u201d passages like this in sections 3-4 would\u2019ve made it much easier to read.  For an example of a hard-to-understand part, see the last question below.\n\nNote: My primary concerns and slightly negative score come primarily from #2 (contribution), and the second question below (\"reward-engineering\").  The clarity weaknesses are not severe enough to have a large impact on my score."
            },
            "questions": {
                "value": "What is the meaning of the and symbol in (6)?  How is the gradient equal to \u201c0 and L(theta)\u201d?  Or is the and symbol meant to separate this line into two separate equalities?  If so, this is confusing, consider representing this in a more standard way, or using parentheses to disambiguate.  **Update:** upon reading more of the paper, and seeing this used more, I know the latter interpretation is correct, so no need to answer this question in your response, but I\u2019ll leave this here to illustrate the potential confusion to the reader.\n\nIn 4.1, J^d seems less like an exploration term, and more like a \u201creward-engineering\u201d term that simply makes the problem easier.  Am I missing a perspective on this?  This leads to a larger concern, in that much of the contribution of 4.1 hinges on this term, and I have doubts about whether this term can be legitimately thought of as an exploration bonus that is superior to the entropy bonus.\n\nCan you please sum up the core take-away from 4.1?  I\u2019ve reread the paragraph \u201cWe have empirically shown that exploration in policy gradients\u2026\u201d several times, but I\u2019m struggling to understand exactly what I was supposed to take away from this section.  Is the point that the two criteria were good criteria in practice for choosing a good exploration bonus term?  If so, I am not convinced that this result will generalize beyond this specific toy setting (and the issue raised in the question above becomes even more of a concern in this case).  If not (or even if so), I think the intended take-away of this section needs to be spelled out more clearly.  **Update:** I understand better now upon a reread, so no need to address this question directly in your response.  However, the Section 4.1 paragraph noted is a perfect example of the last weakness noted above, so I\u2019ll leave this question in the review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698382792012,
        "cdate": 1698382792012,
        "tmdate": 1699636226084,
        "mdate": 1699636226084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TAwaDz0rlq",
        "forum": "pUIANwOLBN",
        "replyto": "pUIANwOLBN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
        ],
        "content": {
            "summary": {
                "value": "The paper under review took a closer look at the value of exploration for RL algorithms from an analytic point of view.\nOn top of the intrinsic need to achieve global optimal (one has to know then entire environments), \nauthors proposed four criteria: __coherence, quasiconcavity, efficiency, and attraction__, \nto measure the quality of an exploration, and demonstrated the effectiveness of these measures through simulations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of quantify effectiveness of an exploration is novel and well grounded.\nThe paper is well written, easy to follow. Simulations and plots are illustrative and helpful."
            },
            "weaknesses": {
                "value": "My concern lies in practicality. While intuitively, the proposed four criterion all make sense, however, for a general RL question, how to effectively compute these four measures is somehow challenging. For instance, following the paper's notation,  let $J$ be the objective function, \n$J(\\pi_{\\theta^*})$ be the optimal, and $J(\\pi_{\\theta^\\dagger})$ be the optimal with a way of exploration. Coherence requires \nevaluating if $J(\\pi_{\\theta^*}) - J(\\pi_{\\theta^\\dagger}) \\leq \\epsilon$. \nUsually $\\theta^*, \\theta^\\dagger$ are unknown, how would one utilize this coherence, and more generally the other proposed measures are not detailed in the paper."
            },
            "questions": {
                "value": "- Question on Practically: current simulations are good for readers to get the idea of the paper, but insufficient to demonstrate the capacity. It would be nice if the authors could take an concrete example, say a maze, with a few popular regularized objectives, compute (estimate) the proposed four criterion of these objectives, see which one is more effective based on these criterion. Then valid such prediction on the trained agent's behaviors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_qvTF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698432676162,
        "cdate": 1698432676162,
        "tmdate": 1699636225975,
        "mdate": 1699636225975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6AUM4NERbI",
        "forum": "pUIANwOLBN",
        "replyto": "pUIANwOLBN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the impact of exploration in policy optimization algorithms from an optimization perspective. The authors propose different measures of analysis, discuss implications of the techniques and measures proposed, and insights gathered on illustrative examples. Their conclusion is that exploration techniques that smoothify the objective (by adding entropy) eliminate local optima and increase the probability of trajectories through the optimization landscape reaching the goal."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I believe the properties of exploration bonuses on policy gradient algorithms are not entirely novel (for instance, their effect on smoothifying the optimization landscape was known, investigated empirically in \\citet{ahmed19} and theoretically in \\citet{mei2020}). The optimization perspective on exploration has been previously explored in \\citet{mei2020, chung21, mei2022}. The main strength of the paper I believe lies in its clarity and rigor of collecting and expressing these scattered insights. I think the paper makes a good effort in systematically investigating these properties together, and offering a way of looking at them in the same place, on simple illustrative examples. Because of this, I think it has a lot of value to the community. Particularly, I believe the optimization view on exploration is still not known within the community, or understood by RL practitioners, so explaining these things with clarity is very relevant and subtly impactful. I also found the illustrative examples really interesting, particularly the last one.\n\n\n@InProceedings{ahmed19,\n  title = \t {Understanding the Impact of Entropy on Policy Optimization},\n  author =       {Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the 36th International Conference on Machine Learning},\n  pages = \t {151--160},\n  year = \t {2019},\n  editor = \t {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},\n  volume = \t {97},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {09--15 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v97/ahmed19a/ahmed19a.pdf},\n  url = \t {https://proceedings.mlr.press/v97/ahmed19a.html},\n  abstract = \t {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.}\n}\n\n@inproceedings{mei2020,\n author = {Mei, Jincheng and Xiao, Chenjun and Dai, Bo and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n pages = {21130--21140},\n publisher = {Curran Associates, Inc.},\n title = {Escaping the Gravitational Pull of Softmax},\n url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f1cf2a082126bf02de0b307778ce73a7-Paper.pdf},\n volume = {33},\n year = {2020}\n}\n\n@InProceedings{chung21,\n  title = \t {Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization},\n  author =       {Chung, Wesley and Thomas, Valentin and Machado, Marlos C. and Roux, Nicolas Le},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {1999--2009},\n  year = \t {2021},\n  editor = \t {Meila, Marina and Zhang, Tong},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {18--24 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v139/chung21a/chung21a.pdf},\n  url = \t {https://proceedings.mlr.press/v139/chung21a.html},\n  abstract = \t {Bandit and reinforcement learning (RL) problems can often be framed as optimization problems where the goal is to maximize average performance while having access only to stochastic estimates of the true gradient. Traditionally, stochastic optimization theory predicts that learning dynamics are governed by the curvature of the loss function and the noise of the gradient estimates. In this paper we demonstrate that the standard view is too limited for bandit and RL problems. To allow our analysis to be interpreted in light of multi-step MDPs, we focus on techniques derived from stochastic optimization principles&nbsp;(e.g., natural policy gradient and EXP3) and we show that some standard assumptions from optimization theory are violated in these problems. We present theoretical results showing that, at least for bandit problems, curvature and noise are not sufficient to explain the learning dynamics and that seemingly innocuous choices like the baseline can determine whether an algorithm converges. These theoretical findings match our empirical evaluation, which we extend to multi-state MDPs.}\n}\n\n@article{Mei2021,\n  author       = {Jincheng Mei and\n                  Bo Dai and\n                  Chenjun Xiao and\n                  Csaba Szepesv{\\'{a}}ri and\n                  Dale Schuurmans},\n  title        = {Understanding the Effect of Stochasticity in Policy Optimization},\n  journal      = {CoRR},\n  volume       = {abs/2110.15572},\n  year         = {2021},\n  url          = {https://arxiv.org/abs/2110.15572},\n  eprinttype    = {arXiv},\n  eprint       = {2110.15572},\n  timestamp    = {Thu, 29 Jun 2023 16:58:03 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-15572.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n@inproceedings{Mei2022,\n author = {Mei, Jincheng and Chung, Wesley and Thomas, Valentin and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},\n pages = {17818--17830},\n publisher = {Curran Associates, Inc.},\n title = {The Role of Baselines in Policy Gradient Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/718d02a76d69686a36eccc8cde3e6a41-Paper-Conference.pdf},\n volume = {35},\n year = {2022}\n}"
            },
            "weaknesses": {
                "value": "I would have liked to see however how these methods generalize over problem instances, as it is known from \\citet{mei2020} that PG methods are highly impacted by initialization and problem instances.\nSome statements are rather vague, e.g. the authors claim that there is a trade-off between quasiconcavity and eps-coherence, but i\u2019m not sure how general this statement is, and the authors do not provide proofs that guarantee these statements hold on all problem instances. At other times the authors use vague words like \u201cappears\u201d so its difficult to understand if these small illustrations would carry over to agents at scale, and are general enough.\nThe experiment illustrated in Fig 2 is interesting, but wasn\u2019t this already known, the problem generally is that we cannot know the value of lambda beforehand as these are problem dependent, sometimes state-dependent and might also change over the optimization landscape for different policies on the way toward the optimal solution.\n\n\n@inproceedings{mei2020,\n author = {Mei, Jincheng and Xiao, Chenjun and Dai, Bo and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n pages = {21130--21140},\n publisher = {Curran Associates, Inc.},\n title = {Escaping the Gravitational Pull of Softmax},\n url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f1cf2a082126bf02de0b307778ce73a7-Paper.pdf},\n volume = {33},\n year = {2020}\n}"
            },
            "questions": {
                "value": "The authors claim at some point that exploration strategies bias the policy. An exception is which we constrain the policy to stay in the vicinity of the previous policy, in which case we maintain optimality. It would have been interesting to explore such methods which are theoretically optimal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2823/Reviewer_kiQP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782740549,
        "cdate": 1698782740549,
        "tmdate": 1700703970274,
        "mdate": 1700703970274,
        "license": "CC BY 4.0",
        "version": 2
    }
]