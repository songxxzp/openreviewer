[
    {
        "id": "RETplhNAvl",
        "forum": "x6u2BQ7xcq",
        "replyto": "x6u2BQ7xcq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_Nwx3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_Nwx3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the use of tags as part of an otherwise standard V&L model. Tags are incorporated by parsing the captions to produce tags within a vocabulary, and then simply predicting the tags in a standard fashion. Secondly, tags are used, in combination with image features, to predict the caption text in an autoregressive manner. Good results are shown for tagging and for captioning, while some level of control over the generated text by using input tags is also obtained."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I really like the paper. It is clearly motivated, well written, technically well executed, has clear novelty, and has good experimental results. But I want to highlight one thing that is surprisingly hard to find: it seems useful. The reasons: 1) Current V&L models are trained with long captions, which means small tag-like queries are not well supported. There are indeed some object detection methods a-la CLIP (owl-vit and co) but they offer different functionalities and have different requirements. So this is a good addition to the V&L toolbox. 2) it offers a way of controlling caption generation through the use of input tags. \n\n\nMinor suggestions (up to the authors and no reply needed):\nFig. 1 might get a bit confusing as both the \"prior work strategy\" and the current strategy are included in the same flow graph.\nTable 2 shows the last 3 methods seem apart from the rest but it's unclear why they are separated."
            },
            "weaknesses": {
                "value": "The method is based on some relatively standard techniques that are however well executed and put together. While not much of a minus, but maybe a reason for an accept vs strong accept."
            },
            "questions": {
                "value": "1) The tag system works based on pre-existing vocabulary. What happens with out-of-vocabulary terms? \n2) Have you tested retrieval based on short queries? E.g. retrieval using a term within the vocabulary vs some short retrieval query that is not part of the vocabulary (e.g. adjective + noun) vs retrieval with long captions typical of flickr/coco"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698404785479,
        "cdate": 1698404785479,
        "tmdate": 1699636036829,
        "mdate": 1699636036829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V7u8zGLppk",
        "forum": "x6u2BQ7xcq",
        "replyto": "x6u2BQ7xcq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_Kr1V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_Kr1V"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new Visual Language Pretraining (VLP) framework named as Tag2Text, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. It utilizes large-scale annotation-free image tags parsed from image-text pairs through text semantic parsing, and demonstrates a foundational image tagging capability with superior zero-shot performance. Tag2Text re-introduces tag guidance into detector-free vision-language models by seamless integrating image tagging, effectively enhancing the performance of both generation-based tasks and alignment-based tasks. The proposed framework is evaluated over a wide range of downstream benchmarks, and achieves state-of-the-art results with similar model sizes and data scales."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\tThe idea of the utilization of image tags parsing from large-scale image-text pairs is interesting, efficient and effective from improving the performance of vision-language models.\n2.\tThe framework of Tag2Text employs a multi-task pretraining approach, including Tagging, Generation, and Alignment. These tasks are reasonable and share the same visual features obtained from the image encoder, guaranteeing the efficiency of the framework.\n3.\tA large number of experimental results over image tagging, image captioning, and image-text retrieval tasks prove the effective of the proposed methods."
            },
            "weaknesses": {
                "value": "1.\tThe detail of text semantic parser is not clear. Although it is based on existing work of [Wu et al. 2019], it should make clear how to obtain the corresponding tags in the paper.\n2.\tIn Fig. 2, it is difficult to understand which are users\u2019 input desired tags, since they share the same forms of the recognized image tags.\n3.\tIn the Image-Tag-Text Generation paragraph, the introduction of this task is not clear. According to Fig. 4(c), it seems the text embedding should not be used as input, which is conflicting with the introduction in the corresponding paragraph.\n4.\tIn the experiment on controllability analysis, it is not clear how the threshold of tagging head to control the tagging guidance.\n5.\tThere are some typos in the manuscript."
            },
            "questions": {
                "value": "Please try to address the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698408177227,
        "cdate": 1698408177227,
        "tmdate": 1699636036750,
        "mdate": 1699636036750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ekIi9di4rb",
        "forum": "x6u2BQ7xcq",
        "replyto": "x6u2BQ7xcq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_gXiT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_gXiT"
        ],
        "content": {
            "summary": {
                "value": "This work introduces TAG2TEXT, incorporating object tag information into vision-language pre-training. The authors demonstrate its effectiveness across various downstream tasks. In comparison to widely-used object detectors, this method is not only faster but also enriches the model with finer-grained information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is straightforward and easy to grasp.\n- Given the high cost of human labeling and the limited diversity in generating captions for Image Captioning models, such as BLIP, finding new methods for generating comprehensive captions is both challenging and valuable.\n- This work offers a potential benefit in the context of person re-identification (REID).\n- The motivation behind this approach is also quite appealing.\n- Notably, the Tag method diverges from the commonly used Faster R-CNN-based object detectors and demonstrates significantly improved speed."
            },
            "weaknesses": {
                "value": "- Data and Pre-training Settings: The author of the paper used a 4M setting, which includes training data from COCO (Common Objects in Context) and Visual Genome. NoCaps data is sourced from OpenImages and COCO. Importantly, this work does not incorporate any out-of-distribution data. The success of this approach on COCO-related tasks is attributed to the similarity between the pre-training data and COCO style, and it's noted that many works beyond BLIP face similar challenges.\n\n- TagEval Task: TagEval is mentioned as a task, but it is not considered popular, and its persuasiveness is limited. Models trained on tags are noted to perform well in this case.\n\n- Tag Introduction in Pre-training: The introduction of tags in pre-training is not a novel idea. There are existing works, like OSCAR, that have explored the concept of using tags to improve vision-language pre-training."
            },
            "questions": {
                "value": "In Figure 1(b), the authors mention 'Actions.' Could you please clarify the source of this particular type of data or tag? Are there any documented instances of failure cases for Tag2Text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672104777,
        "cdate": 1698672104777,
        "tmdate": 1699636036677,
        "mdate": 1699636036677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MtYflFGaJd",
        "forum": "x6u2BQ7xcq",
        "replyto": "x6u2BQ7xcq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_pdv8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_pdv8"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a straightforward approach to improve Vision-Language models by using tag information extracted from image captions. By utilising standard techniques and losses they show very decent results on tagging, captioning and retrieval tasks"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and easy to follow. \n2. The method is simple and is shown to work well. The idea of using image tags to aid VL pre-training makes a lot of sense. \n3. The results are not always SOTA but are very good. Results on multiple tasks/datasets are provided. I believe the results are sufficient to show that the main idea behind the paper works as well as expected."
            },
            "weaknesses": {
                "value": "I think the main problem with the paper is that all of its components have been proposed before so the paper looks more like a re-implementation of known ideas with more recent architectures and pipelines which is of course expected to work better. Specifically the main idea of using tags to aid VL pre-training appears in many works including OSCAR or more recently in DiHT (Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training) while other losses used like I2T are very commonly used in most works in VL pre-training. But the method for sure can serve as strong baseline. \nSomewhat less important concern: as most experiments are on COCO/Flick which are datasets very close to the ones used for training I am wondering whether the authors could carry out an experiment on out-of-domain data ."
            },
            "questions": {
                "value": "No specific questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771103292,
        "cdate": 1698771103292,
        "tmdate": 1699636036582,
        "mdate": 1699636036582,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VlsCVYBflf",
        "forum": "x6u2BQ7xcq",
        "replyto": "x6u2BQ7xcq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_N6Aw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1106/Reviewer_N6Aw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel framework for vision-language pre-training. The core insight here is that extracting tags from text paired with images offers robust semantic guidance for vision-language pre-training, and these tags can be easily mined from an existing pipeline. Leveraging these mined tags, the paper presents a multi-task learning framework designed for vision-language pre-training. This framework concurrently handles image captioning, image tagging (multi-label classification), and Image-Text Matching. The paper conducts an extensive array of experiments, with the results demonstrating the significant impact of image tagging in this pre-trained framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Including image tagging task to vision language pretraining seems to be promising, which particularly benefit image captioning.\n2. The proposed design that combines multi-tasks for vision language pretraining is interesting.\n2. The experimental results are promising.\n3. The paper is well-written in general and easy to comprehend."
            },
            "weaknesses": {
                "value": "1. This paper discusses VL pretraining methods that are either based on 1) object detection or 2) image tagging (the proposed method) and argues that image tagging is faster and introduces significantly fewer parameters. However, this may not be a compelling motivation for choosing image tagging, as object detectors are fixed (with no additional learnable parameters) and only need to be executed once before training, incurring marginal computational cost compared to the training phase.\n\n2. It would be beneficial for this paper to incorporate specific mathematical formulations to provide a more comprehensive description and discussion of the fundamental problems that require resolution.\n\n3. There is room for improvement in the typesetting.\n\n4. Table 1 appears to be disorganized, making it unclear which numbers to focus on and compare. It is advisable to separate results for different tasks into distinct tables.\n\n5. The \"SOTA\" comparison for image tagging (multi-label classification) seems to omit a substantial portion of recent works. As a result, the reviewer maintains a skeptical stance concerning the associated claims and conclusions.\n\n6. Metrics such as Precision, Recall, and F1 score, which are commonly used for image tagging and multi-label classification, are notably absent from the results.\n\n7. The construction of the tag category system appears to involve human annotation in the process (Section 3.2), which contradicts the earlier claim of being an \"automatic\" approach (Section 1).\n\n8. Conducting an ablation study on the choice of vocabulary set (tag set) size to be mined could offer valuable insights into the proposed method."
            },
            "questions": {
                "value": "1. Could the author give a short explanation of the meaning of title \"Tag2Text\"?\n2. Performing a multi-label classification with 3,429 categories is non-trivial. The reviewer wonder if the authors come across any difficulties? And what the performance of the learned classifiers on those category (for example, on a validation set of VL pretraining)?\n3. Have the authors ablate VL pretraining framework with a single task, for example, Image-Text alignment only based VL, Image-tagging only based VL, Image captioning only based VL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1106/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822018451,
        "cdate": 1698822018451,
        "tmdate": 1699636036499,
        "mdate": 1699636036499,
        "license": "CC BY 4.0",
        "version": 2
    }
]