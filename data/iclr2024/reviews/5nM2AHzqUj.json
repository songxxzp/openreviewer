[
    {
        "id": "AyYspo3vlK",
        "forum": "5nM2AHzqUj",
        "replyto": "5nM2AHzqUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_aZn4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_aZn4"
        ],
        "content": {
            "summary": {
                "value": "This study scrutinizes the self-attention mechanism, focusing on the distribution and concentration behavior inherent in the original self-attention model, and introduces an innovative linear log-normal attention mechanism. Empirical evidence gathered from widely recognized natural language processing benchmarks demonstrates that the proposed method surpasses other linearized attention alternatives in performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Linearized Attention has emerged as a crucial subject in contemporary transformer architectures.\n\n2. Empirical evidence demonstrates its efficacy."
            },
            "weaknesses": {
                "value": "1. The experimental results across various Natural Language Processing (NLP) tasks do not exhibit significant improvements.\n\n2. The absence of latency comparison is a drawback; providing only theoretical analysis is insufficient without practical, comparative data."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5618/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5618/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5618/Reviewer_aZn4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633778873,
        "cdate": 1698633778873,
        "tmdate": 1700625361138,
        "mdate": 1700625361138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TGS9SrKqh8",
        "forum": "5nM2AHzqUj",
        "replyto": "5nM2AHzqUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_m7qC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_m7qC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a linear log-normal attention with unbiased concentration to deal with the quadratic time and memory complexity of self-attention. Experiments on several NLP tasks show its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The analysis of this this paper is good and the design of linearized attention is interesting. The experiments and the visualized results are easy to follow."
            },
            "weaknesses": {
                "value": "1. The main concern of this paper is its results. Compared with Nystromformer, which published two years ago, this paper dose not improve enough, from both results and efficiency, as shown in  Table 1 and Table 2.\n2. Only NLP tasks are adopted, but computer vision based tasks are neglected. \n3. Some related works are missing, such as KVT (ECCV2022). \n4. The experiments are not extensive."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636033842,
        "cdate": 1698636033842,
        "tmdate": 1699636580440,
        "mdate": 1699636580440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FdrLEps1PU",
        "forum": "5nM2AHzqUj",
        "replyto": "5nM2AHzqUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_zqGD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_zqGD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new self-attention mechanism called Linear Log-Normal Attention (LLN Attention) that reduces the quadratic computational complexity of standard softmax attention while aiming to maintain comparable performance.\nThe authors first provide background on transformer models and the standard scaled dot-product softmax attention, as well as prior work on efficient linear attention mechanisms.\nThey then conduct an in-depth analysis of softmax attention, characterizing its statistical, informational, and algebraic properties.\nIn particular, they model the distribution of the attention matrix and prove its log-normal nature.\nFurthermore, they analyze the concentration behavior of softmax attention using entropy and spectral gap metrics.\nBased on this analysis, the authors design the LLN Attention method to emulate the log-normal distribution and concentration properties of standard softmax attention.\nLLN Attention uses exponential feature embeddings of the queries and keys to induce a log-normal distribution.\nThe authors also introduce a temperature parameter and perform moment matching between LLN Attention and softmax attention to align their concentration behavior.\nThis allows LLN Attention to achieve comparable performance to softmax attention.\nThe paper provides proofs that LLN Attention follows a log-normal distribution and that its entropy and spectral gap vary similarly to softmax attention with respect to temperature.\nExperimental results on natural language processing benchmarks demonstrate that LLN Attention outperforms other linear attention methods and achieves competitive accuracy compared to softmax attention."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper made contributions in 1) in-depth analysis and modeling of distributional and concentration properties of softmax attention; 2) Design of LLN Attention method that emulates softmax attention based on this analysis; 3) Introduction of moment matching technique to align concentration behavior; 4) Linear complexity in sequence length while maintaining softmax attention performance. The proposed LLN Attention offers an interesting and somewhat promising approach to enhance transformer scalability for long sequences.\n\n- The paper provides a novel perspective on analyzing self-attention through statistical and information-theoretic lenses. The design of LLN Attention based on emulating properties of softmax attention is a useful technique for developing efficient linear attention. Matching the log-normal distribution and concentration patterns is a creative way to achieve comparable performance.\n- The analysis provides useful insights into the workings of the widely used softmax attention. Characterizing the distribution and concentration can guide better attention design. LLN Attention demonstrates the viability of achieving softmax performance with linear complexity. This can expand the applicability of transformers to longer sequences.\n- The proposed techniques like moment matching may inspire novel ways to analyze and improve attention mechanisms in future work."
            },
            "weaknesses": {
                "value": "- The theoretical analysis relies on several approximations, such as using the Fenton theorem to model log-normal sums. While justified, evaluating the accuracy of these approximations on empirical data could be beneficial.\n- The paper focuses exclusively on natural language tasks. Assessing the effectiveness of LLN Attention on other modalities like computer vision with ViTs could provide more insight into its general applicability.\n- Only accuracy results are reported. Including other metrics like training time, stability, convergence rate, etc. could allow more comprehensive comparison to softmax attention.\n- The moment matching technique matches up to 2nd order statistics of LLN and softmax attention. Have the authors considered or tried extending to higher order statistics?\n- Combining LLN Attention with sliding window or dilated patterns could be an interesting extension to handle local contexts more effectively. What do the authors think?\n- The impact of different feature embedding functions besides the exponential function could be analyzed. Do they provide similar distributions and concentration?\n- More ablation studies on the temperature parameter, block size in diagonal attention, layer depth, etc. could shed light on how to best configure LLN Attention.\n- The quality and diversity of the tasks used for evaluation could be expanded. For instance, adding tasks with longer input sequences could better highlight benefits of LLN Attention."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825592189,
        "cdate": 1698825592189,
        "tmdate": 1699636580333,
        "mdate": 1699636580333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oPK6SSMyYQ",
        "forum": "5nM2AHzqUj",
        "replyto": "5nM2AHzqUj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_xXRJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5618/Reviewer_xXRJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a linear log-normal attention mechanism that has linear time and memory complexity while retaining key properties of the naive softmax attention (SA). The key properties under consideration are the log-normal distribution of the attention matrix and the monotonicity of the entropy as well as the spectral gap with temperature. Several experiments with different NLP tasks are performed to confirm the efficiency of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is very well-written with detailed theoretical justification and many useful intuitive explanations.\n\n- The authors have characterized three important and interesting properties of the SA mechanism: (1) the distribution of the SA matrix $\\mathbf{P}^{(SM)}$ can be approximated by a log-normal distribution, (2) the entropy $H(\\mathbf{P}^{(SM)})$ is monotonically increasing with temperature, while (3) the variance of the attention matrix $\\mathbf{P}^{(SM)}$ is decreasing with temperature. I have not checked the proofs line by line, but it seems to me that the proofs are correct.\n\n- Based on these characterizations of the SA mechanism, the authors carefully selected suitable feature embedding functions $\\phi$ in the linearized attention equation in a way that these characterizations still hold.\n\n- Experiments with different NLP tasks confirmed the advantages of the proposed model in comparison with previous linear attention methods.\n\n- In these experiments, the authors also experimentally show that the entropy and the spectral gap of the attention matrix are actually monotonically increasing, as proved in the theoretical parts.\n\nOverall, I think this is a good paper."
            },
            "weaknesses": {
                "value": "I do not see any major weaknesses of the paper. There are a few minor weaknesses as follows:\n\n- Some notions are introduced with formulas but without intuitive explanations. For example, why is $\\tau_{sm}$ in Eq. (5) called the \"temperature\" of the SA? and why controls the level of exploration and exploitation?\n\n- In Eq. (6), $P_{ij}^{(SM)}$ is used, while in Eq. (7), it is $\\mathbf{P}_{ij}^{(SM)}.\"\n\n- I am not sure where the proof of Theorem 3.4 is in the Appendix. Is it Theorem A.3?"
            },
            "questions": {
                "value": "See weaknesses!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698948284567,
        "cdate": 1698948284567,
        "tmdate": 1699636580236,
        "mdate": 1699636580236,
        "license": "CC BY 4.0",
        "version": 2
    }
]