[
    {
        "id": "gJYXqJnk1T",
        "forum": "gisAooH2TG",
        "replyto": "gisAooH2TG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
        ],
        "content": {
            "summary": {
                "value": "The authors build off recent work on controlling robots via large language models and vision language models. The closest analogue of their work is recent lines of work on generating reward functions via language models (i.e. Zero-Shot Reward Specification via Grounded Natural Language, Language to Rewards for Robotics Skill Synthesis, etc.)\n\nIn general, prior work uses a prompted LLM to generate a reward function based on a natural language command from the user for what task they want to perform, and this reward is optimized by MPC to generate low level actions.\n\nThis work augments this flow by including a VLM \"Perceiver\". After the robot acts according to the generated reward, this perceiver is given an image of the scene + an instruction to either confirm the task is completed or generate a text description of why it has failed. If the robot has failed, this text description feedback is fed back into the high-level planner to generate a new instruction + new reward function. This is the \"replanning\" step of their work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The core idea of using a VLM to generate textual feedback for a model makes sense and is a good addition to this research direction in robot learning. The videos are very helpful for showcasing the resulting method."
            },
            "weaknesses": {
                "value": "Although the high level idea of the paper is reasonable, it is both not evaluated as extensively as would be helpful and is not put in context of recent literature very well.\n\nAs a minor note: please fix the references to surround the authors of citations in parentheses. This would make the paper much easier to read.\n\nOn the experimental front:\n\n* the exact prompts used do not appear in the paper or appendix, which makes it hard to judge how the LLM was told to incorporate past feedback\n* The evaluation setting is 4 tasks, done for 3 trials each. This is an incredibly small number of tasks and trials for the method. As a point of comparison: the Language to Rewards paper used as a baseline was tested on 17 tasks with 10 generated rewards functions per task, run through MPC 50 times each. My understanding is that in Table 1, RePLan without replanning is almost identical to Language to Rewards, but is only successful 1/3 times in the easiest Task 1 setting, compared to 3/3 from Language to Rewards. To me this seems like it is just because of random noise, but if that's the case, why should we trust any of the other numbers in the table? This is the point of issue I find most important about the paper.\n\nOn the prior literature front:\n\nThere are number of prior works based on providing LLM feedback from the environment, either ground truth or from VLMs. Examples are Voyager by Wang et al, Inner Monologue by Huang et al, and Towards a Unified Agent with Foundation Models by Di Palo et al. I would appreciate some discussion about such lines of work, since to me it is not so clear if this is doing anything very different from these works. I believe at most you can argue that this paper is using MPC instead of RL or imitation learning, but otherwise prior work has used chain-of-thought style prompting to decompose high-level language to low level language, generate rewards from said low level language, provide feedback via VLMs, etc. That is not to say that the combination of prior work cannot be novel, but in this instance, it does not feel like much is coming from said combination. Especially given the weakness of the experimental results.\n\nEdit: some more experimental results were provided and I have adjusted score from 3 -> 5."
            },
            "questions": {
                "value": "When doing replanning, is there any cap on how many iterations of replanning the agent is allowed to do? Could the authors also discuss if they see failure cases from the Perceiver, or in deciding if a task is completed or not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8262/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8262/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698104967184,
        "cdate": 1698104967184,
        "tmdate": 1700682878882,
        "mdate": 1700682878882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9G66vyiBLD",
        "forum": "gisAooH2TG",
        "replyto": "gisAooH2TG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a hierarchical method for solving multi-stage manipulation tasks with language models, consisting of a LLM-based high-level planner as well as a LLM-based low-level reward/cost generator. The high-level planner decomposes the task into multiple stages in natural language, with the option to replan based on perceptual input if needed. The low-level reward generator takes as input the decomposed sub-task and generates reward for an MPC controller, which uses predictive sampling in MuJoCo to generate low-level robot actions. The method is evaluated on 4 tasks in 2 scenes, where it is compared against a recent baseline \u201cLanguage to Reward\u201d and ablated across the method\u2019s different components."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presented idea is clear and well-motivated \u2014 leveraging LLMs in a hierarchical framework for both high-level task planning and low-level motion planning. Compared to prior work, \u201cLanguage to Reward\u201d, it is clear that such hierarchical approach is needed for long-horizon tasks and can also offer additional robustness as the system can replan its high-level action.\n- The literature review is also thorough, covering many recent works in this domain. However, this part can be improved because it is now more like a laundry list instead of putting the work in the context of prior works."
            },
            "weaknesses": {
                "value": "- Currently the biggest limitation seems to be the lack of thorough experiments, which can use some improvement along two axes. One is the breadth of the tasks: there are only four tasks investigated in this work while there are also quite some similarities between them. An important advantage of using LLMs is that it is possible to apply to a wider set of tasks more easily. The other axis is the quantitative evaluation: currrently only 3 runs are performed for each entry in Table 1, which makes the quantitative results not very convincing as it is also pointed out in the paper that there is \u201chigh variance of completion\u201d. In addition, the paper does not compare to prior methods that are not based on LLMs, e.g., task and motion planning methods or hierarchical RL methods.\n- Another limitation lies in the use of the simulator ground-truth for MPC. This raises the question whether the approach can be applicable to real-world settings. However, the high-level planning part does use VLM for grounding image observations, but it\u2019s unclear how this can be achieved for the attributes referred in MPC, e.g. \u201cblock_r_side\u201d, \u201ccabinet_handle\u201d.\n- Currently the intro reads more like related works, where it may be confusing to readers what the actual motivation of the work is. More care can be taken to improve the intro while appropriately contextualizing the work.\n- The citation format in many places are currently incorrect \u2014 parenthesis often should be used."
            },
            "questions": {
                "value": "What is the prompt being used for LLMs? Can the authors provide more examples of paired LLM output and environment execution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818499378,
        "cdate": 1698818499378,
        "tmdate": 1699637027151,
        "mdate": 1699637027151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xr2EagjKkn",
        "forum": "gisAooH2TG",
        "replyto": "gisAooH2TG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a framework called RePLan that integrates multiple foundation model components into a system for iterative high-level robot planning and low-level robot reward code generation. There are five modules in RePLan: 1) LLM for High-level semantic planning to propose intermediate language primitives, 2) VLM for perception for scene state estimation and motion error explanation, 3) LLM for low-level reward code generation given a language primitive and scene state, 4) Motion Controller to translate reward code to robot actions, 5) LLM to verify that 1) and 2) outputs are correct. This system is evaluated on 4 simulated robot manipulation scenarios which require from 1 to 4 subtasks to solve, and is compared against Language2Reward, a method that does not do iterative task decomposition or replanning. The method is compared against ablations which remove specific components; only the full RePLan system is able to achieve non-zero success on all evaluation scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow\n- The motivation of the paper is topical since feedback and adaptive replanning is important for foundation models which may hallucinate or require grounding in physical interactions\n- The method does not require additional human input compared to the baseline method Language2Reward (just one human input at the beggining, the rest of the replanning and execution is autonomously completed by the foundation model submodules)"
            },
            "weaknesses": {
                "value": "- The VLM Perceiver is one of the most critical parts of the method, but it is not sufficiently explained. Due to the lack of details, I can only assume how it is utilized based on Algorithm 1, in which case I have some major concerns. Since the VLM is the bottleneck for providing feedback for grounding LLM plans and rewards for future LLM iterations. However, details are not shared about how the Perceiver is used, even though it is mentioned that \"The High-Level Planner [is used] to decide what it wants to query from the Perceiver\". From Algorithm 1, the VLM needs to be used for two use cases: #1 scene state generation `VLM(image_observation)` and #2 `VLM(image_observation, motion_error, language_instruction)`. However, these seem to be quite challenging tasks to naively query for off-the-shelf VLMs. While modern VLMs are fairly robust at narrow vision task domains like object detection or image captioning for internet images, more extended reasoning (such as failure explanation with multiple input contexts) or domain-specific understanding (like robotics reasoning from vision) is still an open problem.\n- The evaluation complexity is very limited and does not justify the claims of a \"large-scale and long-horizon kitchen environment\": it is only in simulation, with relatively high-level and short-horizon tasks. There are two issues: 1) the granularity of intermediate primitives (\"pick up the block\") is coarse, 2) the horizon length is short, going only up to 4 subtasks required. Previous works in BEHAVIOR-1K, ALFRED, SayCan have studied robotic reasoning with much longer horizons with similar granularity of intermediate primitives.\n- The evaluation has very few trials (3 seeds only), so it is hard to draw confident conclusions about the method's quantitative performance.\n- The core claim of the work should clarified. If the contribution is the incorporation of LLM and VLM feedback into high level planning, it needs to be compared/discussed against prior works that ground robot planning with additional foundation model feedback. Specifically, [1] incorporates LLM and VLM feedback for closed-loop environment feedback for a High-Level LLM Planner for robot subtasks. If the contribution is the integration of LLM and VLM feedback into code generation, it needs to compared/discussed with other replanning works from code generation [2] or LLM tool use. If the core contribution is the verifier, this needs to be stated more clearly and more details about this module should be provided. If the contribution is the admittedly impressive integration of existing modules, this should be clarified and more details about the bottlenecks of the system (VLM Perceiver, Verification) should be shared in the main text.\n\n[1] \"Inner Monologue: Embodied Reasoning through Planning with Language Models\", Huang et al. 2022\n[2] \"Improving Code Generation by Training with Natural Language Feedback\", Chen et al. 2023"
            },
            "questions": {
                "value": "- In general, clarifications to my concerns above will be appreciated.\n- Can you explain the VLM Perceiver more? For example: Which VLMs are used? How are they used? Which prompts from the LLM Planner are used? How are the motion errors passed to the VLM? How does the verifier coordinate with the VLM? What is the success rate of the VLM (This is in the context of my concerns in the `Weaknesses` above, where I am doubtful that current VLMs may perform well at extended robot reasoning. Prior works may utilize current VLMs for narrow sub-domains in robotics like object detection or success detection, but RePLan requires textual error explanation \"Block is in the way of opening the door\" which seems quite difficult in an end-to-end zero-shot VLM).\n- Can you clarify the failure reasons in Table 1? For example, does the reward code fail, the high level planner fail, or perceiver fail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863019917,
        "cdate": 1698863019917,
        "tmdate": 1699637027010,
        "mdate": 1699637027010,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jbwFssE1h5",
        "forum": "gisAooH2TG",
        "replyto": "gisAooH2TG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a way to perform multi-stage planning with LLMs. the method combines multiple LLMs and VLM to reason about long-horizon tasks in a closed-loop fashion. overall the approach is good but I feel it requires more testing on wide variety of tasks to test the generalization of the approach. Furthermore, the testing of the motion planner needs to be more thourough."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The overall proposed method seems to be interesting and probably could work for a large variety of planning tasks.\n2. The tasks are a bit more complex then simple pick and place and illustrate more complex reasoning.\n\n==================\nI am increasing the score to 6 after rebuttal discussions."
            },
            "weaknesses": {
                "value": "1. the authors need to provide more environments or tasks to show the robustness of their method. The authors can use LLMs to generate tasks which are long-horizon to come up with more varieties of task so that the proposed method could be more thoroughly tested. I think this remains to be verified."
            },
            "questions": {
                "value": "1. How do you test your motion planner module?\n2. You say the weakness of your method is the VLM perceiver. Have you tried testing rest of your method by providing text description of the scene by creating a template and keeping the other modules? That can provide more insights of reasoning and control modules of your method?\n3. Can provide more results for your system by generating more long-horizon tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8262/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn",
                    "ICLR.cc/2024/Conference/Submission8262/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8262/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699641887051,
        "cdate": 1699641887051,
        "tmdate": 1700961796407,
        "mdate": 1700961796407,
        "license": "CC BY 4.0",
        "version": 2
    }
]