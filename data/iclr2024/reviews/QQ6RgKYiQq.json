[
    {
        "id": "9TrLjlAb3E",
        "forum": "QQ6RgKYiQq",
        "replyto": "QQ6RgKYiQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_qQod"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_qQod"
        ],
        "content": {
            "summary": {
                "value": "This submission takes monocular dynamic NeRF one steps further and tries to discover rigid objects parts. It uses D-NeRF-style monocular RGB input and aims for general articulated scenes. To this end, it uses forward and backward deformation modeling and a cycle consistency loss between them on the feature level (doing this on the features instead of offsets is novel). Then, at test time, the forward motions are merged into an adaptively determined number of rigid groups. Experiments evaluate the reconstruction and part segmentation quality. The results are almost exclusively on quasi-multi-view/D-NeRF-style synthetic scenes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Extending dynamic NeRF towards part discovery is interesting, as also evidenced by a few related works in this direction. \n\nThe proposed method is quite straightforward and thus elegant. It does not appear to have overly involved components.\n\nUsing cycle consistency on motion features rathter than the final offsets is interesting and novel.\n\nThe reconstruction quality on the D-NeRF dataset is high (other datasets are unclear). The same goes for the part discovery, which gives very accurate looking qualitative results.\n\nA few simple downstream applications like basic editing or tracking are presented."
            },
            "weaknesses": {
                "value": "Related Work and Method:\n\n(A) I am not convinced that \"Eulerian\" and \"Lagrangian\" are the right terms. Standard deformable NeRFs like D-NeRF or Nerfies use backward deformation modeling. The paper proposes forward modeling instead. These terms should at the very least be mentioned explicitly somewhere. Both forward and backward modeling focus on particles and are direct functional inverses of each other. They thus resemble the Lagrangian perspective. As for example mentioned in Tretschk et al. State of the Art in Dense Monocular Non-Rigid 3D Reconstruction, I think it's more accurate to think of methods like NSFF that do not have a proper canonical model and rather directly regress the radiance field at each coordinate point in time and space as Eulerian. Such methods do not have a notion of particle, while both forward and backward deformation methods do. Or, to be more precise, they only use temporal consistency losses with auxiliary scene flow between *neighboring* frames and thus only have a notion of *instantaneous* flow to *temporally neighboring* frames at coordinate points in world space, which is exactly what Eulerian flow is. All of this is of course not ultimately relevant for the point of the paper in any way, only the presentation/writing, and I'm willing to hear why the notions proposed in the paper are more appropriate.\n\n(B) The submission argues from the intractability of tracking a particle with backward deformation modeling. But for example Cai et al. Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera use an invertible deformation model that would allow to long-term track a particle in a straightforward manner. They lack expressivity and are computationally expensive but that's a more involved argument than the one in the submission.\n\n(C) Liu et al. also propose cycle consistency of forward and backward modeling for dynamic NeRF in their NeurIPS 2022 paper DeVRF. It does this on the motion and not the feature level though. I think this should be discussed in the paper.\n\n(D) It might be good to discuss how the proposed method relates to articulated NeRF methods like Wei et al. Self-supervised Neural Articulated Shape and Appearance Models. \n\n(E) While not necessary, please consider citing Nerfies when introducing the idea of using an SE(3) parametrization, since they were the first to do that for dynamic NeRFs.\n\n(F) Maybe mention that category-specific methods, e.g. those focusing on the human body or face, do quite easily allow for part understanding and editing/re-posing, etc. and that the proposed method focuses on general dynamic scenes.\n\n(G) The lower half of page 5 talks about averaging all the features in a group. I assume all these features come from all the points in the batch? But what is in the batch? An entire image? I.e. how is good, reproducible coverage ensured such that averaging in this manner always gives pretty much the same final feature for a certain group across different views?\n\n(H) What loss function provides a gradient for D_L, the Lagrangian decoder? All losses except for the cycle consistency loss only go into the canonical model and the Eulerian part. Where is the total variation loss applied exactly?\n\n(I) I don't understand what the group merging module is used for. My only guess is this: During training, the initial, too-large number of groups is fixed/always used. Then, after training is finished, the group merging module is used to determine the final number of groups? So the APE cost is never used for backpropagation?\n\n(J) The Lagrangian/forward module assumes articulated rather than fully non-rigid motion due to its per-group averaging operation. Please state this explicitly.\n\nExperiments:\n\n(K) I am concerned that video results are only shown for the synthetic D-NeRF dataset, which is effectively a multi-view dataset (as discussed in Gao et al. Monocular Dynamic View Synthesis: A Reality Check). I'd much prefer to see the video results of the real-world monocular scenes used in the paper. And the datasets from Figures 6 and 7 (although these also use the quasi-multi-view setting). Otherwise, I will assume that reconstruction only works for quasi-multi-view synthetic scenes and not for input that is truly monocular or real.\n\n(L) Please add video results for part discovery on multiple scenes of the HyperNeRF dataset. Currently, only a handful of still images are shown. Otherwise, I will assume that these are severely cherry-picked and part discovery does not work on real-world scenes.\n\n(M) The same applies for tracking. Using a well-textured canonical space like here https://github.com/pablopalafox/npms for example would help in visualizing the tracking/correspondence quality.\n\n(N) Please add LPIPS scores everywhere. It's standard for NeRF papers (see e.g. the original NeRF paper) and PSNR and SSIM are quite similar, while LPIPS is much more perceptual.\n\n(O) There appear to be no ablations. For example, applying the cycle consistency loss on features vs. positions. Or using SE(3) vs. offsets. And loss ablations, e.g.: Why is the per-point color loss useful? What happens without the total variation loss? Also, an ablation of whether the forward model helps *in reconstructing* would be good. Is this additional component useful? If it isn't, then the proposed part discovery portion of the method (i.e. the group merging module) might be added flexibly as a post-processing step to existing backward dynamic NeRFs to discover parts, which would be interesting to know.\n\n(P) Can Watch-It-Move, the most relevant related work, not be applied to the D-NeRF dataset? I'm curious about the parts it would discover.\n\n(Q) I assume that part segmentation is evaluated in 2D image space rather than in 3D due issues with the unobservable insides of objects?"
            },
            "questions": {
                "value": "My questions are intertwined with the Weaknesses, please see above. \n\nOverall, I unfortunately lean towards reject. I only have minor concerns about the method and related work, which can be easily addressed. However, the experimental evaluation is too lacking in multiple important regards. Still, if these concerns are addressed well in a rebuttal, I would most likely increase my score to accept."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Reviewer_qQod"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697804198461,
        "cdate": 1697804198461,
        "tmdate": 1700580440336,
        "mdate": 1700580440336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jKEjnykIrS",
        "forum": "QQ6RgKYiQq",
        "replyto": "QQ6RgKYiQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_NTzN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_NTzN"
        ],
        "content": {
            "summary": {
                "value": "In this paper is proposed a NeRF-based approach for simultaneous non-rigid scene reconstruction and rigid part grouping from monocular pictures. To this end, the method combines location-based Eulerian and particle-based Lagrangian interpretations, being a slot attention-based motion grouping network as part of the Lagrangian module. Both interpretations are a hybrid representation of volumetric features, which are supervised by a cycle-consistency loss. Every motion group is expressed by a rigid transformation per temporal frame. Experimental results are provided on synthetic and real datasets, obtaining a good trade-off between accuracy and computational cost in comparison with state-of-the-art techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses an important problem in computer vision and learning. The proposed solution is a good combination of multidisciplinary areas. \n\n- In general terms, the paper is well written and clear enough."
            },
            "weaknesses": {
                "value": "- The experimental evaluation is provided on simple synthetic datasets where the amount of deformation is a bit limited. Considering the paper formulation, I believe the method could handle strong deformations with no problem, but that is never validated. \n\n- No full evaluation on real monocular image sequences."
            },
            "questions": {
                "value": "The motivation of the paper is good and the contributions are clearly stated. The technical section is well written and is clear enough. Most of the relevant details are provided as well as the explanations and discussions of every module the authors use. \n\nThe combination of Eulerian and Lagrangian formulations to handle the time-varying problem is a very interesting contribution. The authors exploit both points of view in a unified formulation. \n\nIn comparison with other NeRF algorithms to handle non-rigid objects, the algorithm in this paper can also infer a segmentation, exploiting the local rigidity of the clusters. This is a good point, but also shows us the type of deformation to be considered is a bit limited, as just piecewise rigid deformations are possible. In any case, the joint estimation is a good contribution. Similar to NeRF approaches, the camera poses are assumed in advance, representing, probably, a strong prior in this context. \n\nThe proposed method provides a good trade-off between accuracy and computational cost in comparison with other competing recent works. Moreover, the method can infer the part-based grouping automatically, as Noguchi et al. CVPR 2022 did. The results in terms of computational cost are really relevant. \n\nLack of realistic experimental evaluation. Unfortunately, the method is mainly tested on the D-NeRF synthetic dataset. I would like to see experimental results on real and challenging sequences. In addition to that, in spite of claiming non-rigid motions, the level of deformation on that dataset is very limited, and therefore I consider it is not the best one for evaluating non-rigid scenarios. In other words, the type of motion is piecewise rigid (as it was considered previously), that essentially is not the same as non-rigid. In fact, this can be observed in the estimation of groups. As it can be seen, the number of groups is very reduced and the global behavior in terms of deformations of those groups is quite simple. Again, the use of the dataset ManiSkill is not the best way to validate the strengths of the method due to the simplicity of the motion. In contrast, the authors could consider some real videos with dynamic scenarios to finish the full analysis and claim validation. I like real experiments in the context of scene editing. \n\nThe video is complete and definitely can help the reader."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2456/Reviewer_NTzN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699288176385,
        "cdate": 1699288176385,
        "tmdate": 1700694814421,
        "mdate": 1700694814421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aEDZ9WsNAo",
        "forum": "QQ6RgKYiQq",
        "replyto": "QQ6RgKYiQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_WRpS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_WRpS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for part-aware dynamic NeRF. While dynamic NeRF (D-NeRF) encodes the temporal information as well as the spatial radiance fields, it has not been easy to extract the object part information. To address this problem, this paper uses an idea similar to traditional motion segmentation, which combines the flow between adjacent frames (i.e., Eulerian flow in terms of particle flow simulation) as well as the clusters of motion trajectories (i.e., Lagrangian flow). Experiments show that the proposed method achieves near-SOTA accuracies for novel view synthesis with part-aware representation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method correctly brings the idea of using motion trajectories to NeRF-based representations. In particular, the use of a cycle consistency loss that improves the agreement between Eulerian and Lagrangian motion shows a nice coupling of traditional ideas and modern neural networks.\n\n+ The proposed method focuses on scenarios using a monocular moving camera, which makes the proposed method practical. In fact, the paper shows two practical examples of applications to robotics and scene editing."
            },
            "weaknesses": {
                "value": "- Using long-term motion trajectories is itself a traditional idea in computer vision, especially for motion segmentation (e.g. [a]). While I think that using Eulerian vs. Lagrangian views is a nice introduction to temporal motion representations, it would be nicer if they also briefly discussed the relationship of their idea to these traditional attempts. In my opinion, this would not sacrifice their technical contribution, but rather strengthen the theoretical and academic value of their approach.\n\n[a] Keuper, Margret, Bjoern Andres, and Thomas Brox. \"Motion trajectory segmentation via minimum cost multicuts.\" Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\n- Since their technical contribution may be to combine short- and long-term motion trajectories, readers may want to see the ablation studies comparing with and without these modules."
            },
            "questions": {
                "value": "Q. Related to the above, since the proposed method focuses on monocular input, readers may wonder if motion segmentation is applicable to their input sequence. In this regard, what happens if they apply motion segmentation to input sequences and then reconstruct *part-wise* NeRF?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699414850075,
        "cdate": 1699414850075,
        "tmdate": 1699636181681,
        "mdate": 1699636181681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d8059fUNte",
        "forum": "QQ6RgKYiQq",
        "replyto": "QQ6RgKYiQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_5UoR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2456/Reviewer_5UoR"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach for dynamic scene reconstruction that allows the discovery of parts as a result of the factorization of the scene motion. The approach is based on monocular NeRF. A part corresponds to particles that share a common motion pattern. Scene motion is parameterized with both an Eulerian view and a Lagrangian view. The Lagrangian view allows tracking the particles on objects. Since the particles on a rigid part share a common rigid transformation pattern, a motion grouping module is used to detect the parts. The method is based on three modules, namely: the Eulerian module, the Lagrangian module and the canonical module. The Eulerian\nmodule and the Lagrangian module observe the motion of specific spatial locations and specific particles, respectively. The canonical module serves to reconstruct the geometry and appearance for volume rendering. The the reconstruction and part discovery performance  of the method is experimentally evaluated on a synthetic dataset provided by D-NeRF. The metrics used for evaluation were PSNR and SSIM. The motion grouping is evaluated on a synthetic dataset created using Kubric toolkit. In both evaluations the method obtained performances better or at the level of the state-of-the-art."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strength of the paper stems from being a method based on monocular NeRF that besides allowing to perform dynamic scene reconstruction, allows also the discovery of rigid parts. Another strength of the approach is that it used hybrid feature volume and neural network representation, which allows  fast convergence during training."
            },
            "weaknesses": {
                "value": "The grouping of the parts is based on predicted rigid motion. Rigid motion can be a significant constraint in many applications. The approach is inspired by fluid simulation, with the scene motion being observed from both the Eulerian view and Lagrangian view. The Eulerian and Lagrangian descriptions of a flow field are related by the material derivative. It is unclear whether the method ensures that such relationship is verified or met by the proposed approach."
            },
            "questions": {
                "value": "--Can you use other motion models, other than rigid ones?\n--What is the relationship between the cycle consistency between the Eulerian and Lagrangian modules and the material derivative?\n--How are the coordinates in the Eulerian and Lagrangian modules related?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Ethics is not relevant for the subject of this paper."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2456/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699577835094,
        "cdate": 1699577835094,
        "tmdate": 1699636181593,
        "mdate": 1699636181593,
        "license": "CC BY 4.0",
        "version": 2
    }
]