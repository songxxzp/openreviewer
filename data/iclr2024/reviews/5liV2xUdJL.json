[
    {
        "id": "TGayVuXkqF",
        "forum": "5liV2xUdJL",
        "replyto": "5liV2xUdJL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_wQTz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_wQTz"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new method entitled 'stochastic stateful policies' for performing reinforcement learning (RL) with policies having an internal state. The method can be applied to POMDP settings and other non-Markovian setups where it is necessary to have access to the whole trajectory, not only the current state. The method is compared against the state-of-the-art backpropagation through time (BPTT) approach and is more computationally efficient but may result in higher variance. Fundamental theorems are provided, and the method is evaluated in a series of problems, showing improvement, especially in higher dimensional cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Designing new RL approaches tailored for the POMDP setting is important, as a practical deployment will certainly be non-Markovian. To my knowledge, the existing state-of-the-art is to use existing RL algorithm with stateful actors/critics, e.g., along the current state, a history encoded using an LSTM/RNN/GRU is provided.\n\nThe paper has much content; the theoretical section is rich, with a few fundamental results -- the policy gradient theorem, the deterministic policy gradient theorem, and IMHO, the most important - a variance upper bounds are provided in the studied setting. However, it mainly adapts existing results to the current setting, which is unsurprising. Clean proofs of the theorems are provided. \nThe experimental part is also rich; a few experiments reported (unfortunately mostly in the appendix) demonstrating the approach's potential. I think it is fair to say that the paper contains a thorough experimental study.\n\nThe method performs better with less computational and memory burden than the SOTA BPTT approach in high-dimensional problems. Moreover, the authors are honest, and some settings in which the method does not perform well (likely due to increased variance) are also provided, which is nice. \n\nOverall, after reading the main part of the paper, I saw the paper as a borderline paper. However, luckily, after going through the much longer appendix, I became more positive about the paper. Let me note here that as a reviewer, I am not obliged to make a detailed pass over the paper appendix and should base my judgment upon the main part content. \n\nConsidering the whole content, this is a convincing paper. However, there is a danger if a reader goes through the main part only may not appreciate the results fully. This brings my main concern about the paper - I am not sure if the authors took significant effort in making the main part of the paper stand-alone and convincing enough, considering the amount and quality of produced results. The main part needs a major revision before publication. I will reconsider my score after my concerns are addressed in the rebuttal phase. See detailed remarks and questions below."
            },
            "weaknesses": {
                "value": "My main concern with the paper is that the main part needs to present the available results self-contained and convincing enough, which is not the case. Before accepting the paper, it will need thorough edits addressing the concerns and the questions I present below.\n\n1. The Entire Theoretical part, especially the equations, should be formatted more concisely; there is no reason to display the equations in two lines. Notably, only only circa. 1.5 pages in the main part are left for the experimental results. Only two experiments are presented, and there are a few interesting ones in the appendix. The 'Stateful policies as inductive biases' experiment is especially interesting, and it is also mentioned in the introduction.\n\n2. The main advantage of the approach over BPTT - improved computational efficiency is not shown in the paper. It would strengthen the paper when the computational benefits are demonstrated, using at least a table with actual wall-times comparison. It is only mentioned in the paper without proof: \"The overall results show that our approach has major computational benefits w.r.t BPTT with long histories at the price of a slight drop in asymptotic performance for SAC and TD3\".\n\n3. Often, results are mentioned in the main part with a reference to figures in the appendix. I emphasize that most readers will likely stop reading after the main part, so it would be nice to have at least a 'teaser' of the results in the main part."
            },
            "questions": {
                "value": "### Main questions\n* Provide a wall-time comparison of the introduced S2PG approach with (as I understand more time-consuming) BPTT approach.\n* Used assumptions in the theoretical part - argue their physical motivation and preferably cite some established works that introduced them,\n* Hiding velocity experiment - this will not check the benefits of any long-term history encoding, as only two states can be used to approximate velocity.\n* I have a suggestion for another experiment - simplify further the policy by removing the $\\pi_\\theta^a$ , and keeping only $\\pi_\\theta^z$. \n* What do you mean by  \"our method could be used in combination with complex policy structures such as Neural ODEs.\" can you elaborate?\n\n### Minor remarks/questions\n* p. 3 eq. (1) provide formula for $J(\\tau)$;\n* p. 4, caption Fig. 1 Do you rather mean 'from left to right' ?\n* p. 4 'equations equation' -> 'equation;\n* p. 6 \"causality dilemma: On one hand, the policy can learn a condensed representation of the\nhistory assuming accurate Q-value estimates. On the other hand, the Q-function can be learned using bootstrapping techniques, assuming a reliable condensed representation of the history. But doing\nboth at the same time often results in unstable training.\" be more clear here;\n* p. 6 \"We extend the letter\" -> the latter;\n* p. 8 caption Fig. 2 \"for 1 Mio. steps\" -> what is Mio. ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5888/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5888/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5888/Reviewer_wQTz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669797795,
        "cdate": 1698669797795,
        "tmdate": 1700746153028,
        "mdate": 1700746153028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kSU4SnnzT7",
        "forum": "5liV2xUdJL",
        "replyto": "5liV2xUdJL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_HaqJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_HaqJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for decomposing a stateful policy into a stochastic internal state kernel and a stateless policy, resulting in a new policy gradient that is applicable to POMDPs without the need for backpropagating through time (BPTT). At the heart of this technique is the modification of the policy to output not only an action but also a prediction over the subsequent internal state. The authors have derived both stochastic and deterministic policy gradient theorems for this framework and have expanded the variance analysis of Papini et al. for policy gradient estimators. The experimental results demonstrate that the proposed method rivals algorithms that use full BPTT while requiring considerably less computational effort."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is very clear and provides a thorough presentation of the theory proposed by the method. I believe that this work will serve as a good reference for any work building on alternatives to BPTT for POMDPs.\n\n- To my knowledge, the results presented are novel.\n\n- The authors have conducted a detailed analysis of the algorithm across several complex tasks.\n\n- While the theory occasionally presents straightforward extensions of classical policy gradient results, it is explained with exceptional clarity both in the main text and in the appendix."
            },
            "weaknesses": {
                "value": "- Many details, such as the results for the memory task, are relegated to the appendix. Nonetheless, I do not regard this as a significant weakness, given that the main text is already very dense with foundational results.\n\n- Only ten seeds are utilized for the experiments, although it is well-known that MuJoCo tasks are prone to considerable variances in performance. I would recommend increasing the number of seeds to twenty for the final evaluation.\n\n- The paper would benefit from including an analysis of the variance of the gradient for both the proposed method and BPTT, even if on a very simple benchmark. Additionally, it would be beneficial to examine the issues of vanishing and exploding gradients for both BPTT and the proposed method in at least one benchmark."
            },
            "questions": {
                "value": "- The occupancy measure is introduced without defining $z$\n\n- Is the initial internal state learned, or is it initialized to zero at the beginning of each episode?\n\n- Does the limit of the stochastic policy gradient converge to the deterministic policy gradient when the variance of the action and subsequent internal state approaches zero? In other words, is there a result analogous to Theorem 2 in the Deterministic Policy Gradient paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699015865104,
        "cdate": 1699015865104,
        "tmdate": 1699636623919,
        "mdate": 1699636623919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CLfiXkdLA7",
        "forum": "5liV2xUdJL",
        "replyto": "5liV2xUdJL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_gHcm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5888/Reviewer_gHcm"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of learning policies with long term  history. Traditional methods often employ recurrent architectures, which rely on a persistent latent state that is modified at every time increment. However, this presents a challenge: when calculating the gradient of a loss function in such architectures, the gradient needs to be back-propagated through all preceding time steps. This process can lead to either vanishing or exploding gradients, making the training of recurrent models particularly difficult, especially as the historical data size increases. To address this issue, the authors introduce an alternative approach wherein the model's internal state is represented as a stochastic variable that is sampled at each time step. As a result, the state's stochastic nature prevents the direct computation of an analytical gradient, thereby circumventing the issues associated with backpropagation over time. The paper goes on to adapt established theoretical frameworks to this new model and suggests a method for incorporating actor-critic techniques. Empirical validation is conducted on a range of environments that are structured as Partially Observable Markov Decision Processes (POMDPs) by omitting certain observations.  It is shown that the proposed model achieves reasonable performance in comparison to BPTT-based approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The supplementary appendix, which was not examined in detail, appears to be a valuable extension of the main text. The concept of characterizing the policy's internal state as a stochastic variable is intriguing and yields a sophisticated formulation. Additionally, the paper offers robust theoretical contributions and presents a methodology to modify conventional algorithms to encompass this concept."
            },
            "weaknesses": {
                "value": "I am not convinced by the arguments of the authors. In their formulation, even if using stochastic states prevents one from computing an analytic gradient, I don't understand why it would solve the problem of capturing long-term information. Indeed, when computing p(a_t,z_t|s_t,z_{t-1}), then this probability depends on the previous timestep, and so on, such that finding a good solution to the problem would need to propagate the loss to the previous timesteps to capture a good sequence of states. So there is still backpropagation through time, even if it is not made by the analytical gradient. \n\nThen, usually, relying on stochastic variables decreases the sample efficiency of training methods. This is why people are using for example the reparametrization trick that allows one to compute an analytical gradient over a stochastic variable, to speed up training. Here the authors are claiming the opposite. So there is one point that I didn't catch in this paper, and I would like the authors to better explain why using stochastic nodes would avoid the problem of propagating information to the previous timesteps, and why they would expect a better sample efficiency than using an analytical gradient\n\nUsing stochastic variables as state of a policy is something made typically when using Thompson sampling-like methods. Papers like \"Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables\" are also using a stochastic internal state. How do you position your work w.r.t these approaches ? \n\nIn the experiments, it is not clear how the z distribution is modeled, and there is no discussion about possible choices and their possible impact. For instance, what about using a multinomial distribution? Discussing that point would be interesting.\n\nFigure 1 is misleading since there are no arrows between the s nodes and the z nodes in the graph on the right and it seems that the sequence of z does not depend on the observations"
            },
            "questions": {
                "value": "(see previous comments)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699460560660,
        "cdate": 1699460560660,
        "tmdate": 1699636623822,
        "mdate": 1699636623822,
        "license": "CC BY 4.0",
        "version": 2
    }
]