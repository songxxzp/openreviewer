[
    {
        "id": "8nhjXEjkf6",
        "forum": "TWVMVPx2wO",
        "replyto": "TWVMVPx2wO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_1Fvy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_1Fvy"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of adapting pre-trained models for Deep Metric Learning (DML) tasks while preserving prior knowledge. Existing solutions often fine-tune models on standard image datasets, making it difficult to apply them to local data domains. The paper introduces a novel approach, the Visual Prompts (VPT) framework, which augments the conventional proxy-based DML method. VPT optimizes visual prompts for each class by integrating semantic information from input images and Vision Transformers (ViT). The experimental results demonstrate that this approach outperforms existing methods in DML benchmarks, achieving comparable or better performance with minimal parameter fine-tuning. This parameter-efficient technique offers a promising avenue for improving DML without requiring extensive model retraining."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a novel approach to Deep Metric Learning (DML) by proposing the Visual Prompts (VPT) framework. This framework addresses the challenge of fine-tuning pre-trained models for DML while incorporating semantic information and optimizing visual prompts. This approach represents an innovative combination of existing ideas, extending the traditional proxy-based DML paradigm with an efficient and effective method.\n\n- The paper is well-written and clearly articulates the motivation, methodology, and experimental results. The authors effectively communicate their approach, making it accessible to a broad audience of readers, including those familiar with DML and those new to the field. The paper's clarity enhances its potential for adoption and understanding by the research community.\n\n- The proposed framework has the potential to streamline and improve DML tasks, as it achieves comparable or better performance compared to full fine-tuning approaches, all while adjusting only a small percentage of total parameters."
            },
            "weaknesses": {
                "value": "- The paper lacks a comprehensive comparison with existing DML methods, especially those using transformer based backbone. Additionally, it would be beneficial to compare the proposed Visual Prompts (VPT) framework not only against full fine-tuning but also against other state-of-the-art parameter-efficient methods. This would provide a more complete assessment of its effectiveness and novelty."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676261444,
        "cdate": 1698676261444,
        "tmdate": 1699636575016,
        "mdate": 1699636575016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IGXEsoEuWT",
        "forum": "TWVMVPx2wO",
        "replyto": "TWVMVPx2wO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an efficient method for fine-tuning the pre-trained models for the DML image retrieval tasks. The authors propose a method based on visual prompts (VPT) to partially fine-tune the model instead of tuning all parameters. Based on the proxy-based DML methods, they also initial the proxies with input images and visual prompts based on classes."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1 The contribution is solid. As far as I know, this may be the first work to explore the parameter-efficient (PEFT) methods specifically for deep metric learning.\n\n2 Novelty is fine. Although the VPT fine-tuning is not something new, the local design to improve it specifically for DML is interesting and novel.\n\n3 The writing is clear and easy to follow.\n\n4 Experiments is well organized and convincible. \n\nThe authors evaluate their method widely on popular DML datasets, and the results seem to be strong and solid. They also comprehensively compare other PEFT methods on DML datasets, which might be interesting to the community."
            },
            "weaknesses": {
                "value": "I didn't see significant weakness. Based on the limited results on some datasets compared with SOTA, I hope the authors can provide more analysis and possible ideas to improve it.\n\nIt seems the results are good on large pre-training datasets but fair on small pre-training. Could you explain more about this phenomenon? \n\nIt is suggested to move some results in the Appendix, like results on other datasets and pre-trained models, to the main paper. \n\nMinor:\nSome typo or grammar errors:\n\"which includes a random horizontal flip, cropping at random...\"\n\"we found that it linear probing...\"\n\nSome recent works missing: \n\nWang et al. \"Deep Factorized Metric Learning.\" CVPR 2023\nKim et al. \"HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization\" CVPR 2023\nKotovenko et al. \"Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning\", CVPR 2023"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802191308,
        "cdate": 1698802191308,
        "tmdate": 1699636574920,
        "mdate": 1699636574920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "knNEXukVvK",
        "forum": "TWVMVPx2wO",
        "replyto": "TWVMVPx2wO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_hvFX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5583/Reviewer_hvFX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces visual prompt tuning (VPT) to improve proxy-based deep metric learning. As VPT could generate and integrate semantic proxies to improve the representations in deep metric learning, the proxies generated by VPT is considered to be better than random proxy which is generally used in proxy-based deep metric learning. The experiments are conducted several classification benchmark datasets including CUB-200-2011 (CUB200), CARS196, and retrieval benchmark datasets including Stanford Online Products (SOP), In-shop Clothes Retrieval (In-Shop). The proposed method is compared and shown to outperform or perform on par with various parameter-efficient fine-tuning methods such as Adapter Fine Tuning, and Bitfit."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Although the proposed method is simple, the proposed idea of introducing visual prompt tuning to improve proxy-based deep metric learning is well motivated and valid."
            },
            "weaknesses": {
                "value": "The main concern of this paper lies in the lack of evaluation/comparison:\n\n-- The paper claims the proposed method to be parameter-efficient for deep metric learning (DML) tasks, but there is not comparison in term of efficiency as compared to existing DML methods. \n\n-- The proposed method targets to solve deep metric learning (DML) tasks but the compared methods are mostly fine-tuning methods. More DML methods should be considered for comparison. \n\n-- A baseline using default/vanilla proxy-based deep metric learning (without visual prompt learning) is missing to convince the effectiveness of introducing visual prompt tuning."
            },
            "questions": {
                "value": "What is the computational cost of the proposed method as compared to (1) vanilla proxy-based DML method, (2) other fine-tuning methods such as BitFit and Adapter? Please discuss the comparison on computation cost for model training and inference. \n\nHow does the proposed method compare to the state-of-the-art deep metric learning methods? \n\nHow does the proposed method compared to the vanilla proxy-based deep metric learning in terms of model performance and compute cost?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern on Ethics."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820245511,
        "cdate": 1698820245511,
        "tmdate": 1699636574823,
        "mdate": 1699636574823,
        "license": "CC BY 4.0",
        "version": 2
    }
]