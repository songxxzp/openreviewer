[
    {
        "id": "cm8CWnoo5H",
        "forum": "fcSDt7H8kI",
        "replyto": "fcSDt7H8kI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_uoX8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_uoX8"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new algorithm, MaxMin TD Learning, by modifying $\\epsilon$-greedy exploration in DQN. Specifically, with probability $\\epsilon$, the argmin action is selected given the state-action values. Theoretically, this leads to higher temporal difference error under certain assumptions. In practice, the proposed algorithm is shown to achieve higher sample efficiency than DQN with $\\epsilon$-greedy exploration."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "As far as I know, the presented idea is novel and easy to implement. Generally, the paper is easy to follow. The advantage of the proposed algorithm is supported by both theories and experiments. All algorithms are tested in 100K Atari games."
            },
            "weaknesses": {
                "value": "The major weaknesses are insufficient experiments, a gap between theory and experiments, and a lack of explanation.\n\n- How large is $\\mathcal{D}(s)$, $\\delta$, and $\\eta$ in practice? Is $\\mathcal{D}(s) \u2212 2\\delta \u2212 \\eta$ positive or negative in practice?\n- In Section 4, a fixed step size is used. How does the performance of the algorithms vary with different step sizes?\n- In Section 4, $\\epsilon$ is chosen from $[0.15, 0.25]$. In practice, a smaller $\\epsilon$ is usually used. How is the performance of the algorithms with smaller $\\epsilon$, such as $\\epsilon \\in [0.01, 0.05]$? How sensitive is MaxMin TD learning to $\\epsilon$ compared to DQN?\n- In Figure 4, not all tasks (e.g. Amidar, Bowling, BankHeist, and StarGunner) are trained with 200M frames although it is claimed so.\n- In Section 3, it is claimed that, in the early phase of the training, in expectation over the random initialization $\\theta \\sim \\Theta$, the TD error is higher when taking the minimum value action than that of a random action. However, this contradicts the experimental results shown in Figure 3, especially Figure 3(a).\n- Lack of explanation: Why would a higher TD error help exploration and speed up training in general? I don't see a clear connection between them. I believe that it is very important to explain the logic behind."
            },
            "questions": {
                "value": "- In Definition 3.2 & 3.5: What is $\\hat{a}(s,\\theta)$?\n- In Proposition 3.4, $a_t \\sim \\mathcal{U},(\\mathcal{A})$: typo.\n- In Section 4, it is mentioned that the maximum achievable reward in 100 steps is 10. However, the learning curve in Figure 1 (b) is above 10 in the end. How do you get the data used in Figure 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7623/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698301831373,
        "cdate": 1698301831373,
        "tmdate": 1699636925604,
        "mdate": 1699636925604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NGffzamqvt",
        "forum": "fcSDt7H8kI",
        "replyto": "fcSDt7H8kI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_pNsy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_pNsy"
        ],
        "content": {
            "summary": {
                "value": "The work looks at improving sample complexity of deep reinforcement learning (RL) algorithms from the lens of experience collection. A new method based on minimizing state-action value function to increase information gain is proposed. Modifying episilon-greedy, the algorithm leads to more novel experiences by taking actions with the smallest Q-value. Experimentally, the proposed method demonstrates significant improvement in sample complexity in the Arcade Learning Environment, without additional learning parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is well motivated and empirically shows significant improvements in sample efficiency.\n2. The paper, in general, is well-structured."
            },
            "weaknesses": {
                "value": "1. The first few definitions is unclear and unintuitive.\n2. The definition of $\\hat{a}$ is confusing in Definition 3.2\n3. There needs to be a related work section. It is unclear how this approach position among existing works.\n4. Figure 1 is too small\n5. Missing standard deviation in Figure 3\n6. [Minor] the repetition of the questions in conclusion seems like a waste of space to me."
            },
            "questions": {
                "value": "1. Based on Figure 4, Max-Min TD seems to have higher variance, why is that?\n2. Would the method be as effective in sparse-reward setting given that it ties directly to the size of TD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7623/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698478399,
        "cdate": 1698698478399,
        "tmdate": 1699636925477,
        "mdate": 1699636925477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xHoOFHSHoK",
        "forum": "fcSDt7H8kI",
        "replyto": "fcSDt7H8kI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_zmyD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_zmyD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an exploration method based on minimizing the state-action value function. The method is incorporated into temporal difference based on Q-learning with function approximation. Experiments are conducted using a toy chain MDP and several Arcade Learning Environments. The results are compared to the $\\epsilon$-greedy baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses the important problem of exploration in reinforcement learning.\n- It attempts to provide theoretical justification and analyzes empirical results using toy examples and standard benchmark tasks."
            },
            "weaknesses": {
                "value": "- Several claims in the paper require further evidence.\n- The empirical evaluation lacks detail.\n- Details are lacking in addressing the research questions and contributions proposed in the introduction."
            },
            "questions": {
                "value": "An assumption of the proposed method is that the Q-function, in the initial phase of training, would assign similar values to similar states.\n\u201c...early in training the Q-function, on average, will assign approximately similar values to states that are similar\u2026\u201d, \u201c....when the Q-function on average assigns similar maximum values to consecutive states\u201d. \nIt is unclear how this assumption holds. If a random Q-function processes different consecutive states, then the output value might be arbitrary and not necessarily dependent on the input, even for slightly varied states. Thus, the output could be any random number, not necessarily a similar value.\n\nThe text in the plots of Figure 1 is too small and difficult to read. What do each of the plots represent? Are they for different $\\epsilon$ values? Which plot corresponds to which value? Also, how does a change in \u03b5 affect the results of the proposed MaxMin TD learning?\n\nIt is mentioned that \"All of the results in the paper are reported with the standard error of the mean\u201d. However, Figure 2 shows the results for the median on the y-axis. Could you clarify what this means?\n\nThe claim \u201c.....thus creates novel transitions in exploration with more unique experience collection.\u201d is made, but no evidence is presented in the paper. The results are only compared based on reward performance. How can we be certain that the change in results is due to this particular claim?\n\nIn Table 1, the Human Normalized Median is 0.0927 for MaxMin TD and 0.0377 for $\\epsilon$-greedy. If 1 is the highest achievable score, then these numbers appear quite low. Do both algorithms fail to learn anything useful? In that case, stating a 248% improvement seems misleading.\n\nWhat is the QRDQN algorithm baseline in Figure 5? It is not discussed in the paper. What is the difference between $\\epsilon$-greedy in Figure 1 and Figure 5? While it is briefly mentioned in the footnote of the supplementary materials, detailed references are not presented.\n\nIt is mentioned in the introduction as a contribution that the proposed method \"...reaches approximately the same performance level as model-based deep reinforcement learning algorithms,\" suggesting that the proposed method performs better than model-based. However, no model-based baseline is presented in the experiments, nor is it explained in the text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7623/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7623/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7623/Reviewer_zmyD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7623/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772556411,
        "cdate": 1698772556411,
        "tmdate": 1699636925360,
        "mdate": 1699636925360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3mcGbQ40w6",
        "forum": "fcSDt7H8kI",
        "replyto": "fcSDt7H8kI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_qFHi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7623/Reviewer_qFHi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new exploration strategy in reinforcement learning which focuses on taking extremum actions with minimum Q-values. Theoretically, the authors attempt to prove that the TD computed by taking the action with minimum Q-value (denoted as $a_{min}$) is above average (i.e., expected Q-value for a uniform policy) by an amount approximately equal to the disadvantage gap, which is referred to the expected Q-value for a uniform policy minus the Q-value for $a_{min}$. The proposed MaxMin TD Learning policy follows the $\\epsilon$-greedy style, where the proposed algorithm takes $a_{min}$ instead of uniform random action for exploration."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes an interesting idea of improving the exploration efficiency by taking extremum action, which refers to the action with minimum Q-value. \n\n- The method comes with a nice theoretical motivation, where the authors show the proof of the relationship between TD error inferred by taking $a_{min}$ compared to that for a uniform policy, showing that taking $a_{min}$ as the extremum action more frequently could lead to novel transitions that accelerate learning.  \n\n- The proposed method is very general and simple to apply, leading to no additional computational overhead compared to vanilla $\\epsilon$-greedy.\n\n- The authors show comparison results with UCB and $\\epsilon$-greedy on a toy chain MDP domain and large-scale experimental results by comparing with NoisyNets and $\\epsilon$-greedy on Atari 100K."
            },
            "weaknesses": {
                "value": "- The theoretical contribution of this paper relies on several strong assumptions: (1) expected rewards for a uniform random policy and the $a_{min}$ is $\\eta$-uniformed; (2) the Q-value for consequent states $s$ and $s'$ has little difference ($\\delta$-smooth); (3) the initialized Q-function results in a policy that is close to uniform random. The main theoretical conclusion that the TD achieved by $a_{min}$ is above-average by an amount approximately equal to the disadvantage gap ($D(s)$) would be wrong if $\\delta$ and $\\eta$ are not close to 0, because the gap actually equals to $D(s) - 2\\delta - \\eta$. Also, based on my experience, for value functions parameterized by deep neural nets, the initial policy distribution characterized by the initialized Q-functions is often fairly biased from a uniform distribution. In practice, the value $\\epsilon$ would need to gradually decay during the initial phase of training, which means that in practice the theoretically derived conclusion will quickly be invalid in a real training regime. \n\n- In the proposed Algorithm 1, the RL agent always takes $a_{min}$ for exploration action, and no action with intermediate Q-values could be taken for exploration. Unless $a_{min}$ would keep changing among the action set throughout the training, I think the proposed method would easily result in sub-optimal policy compared to $\\epsilon$-greedy due to the limited exploration strategy.  For exploration, I'm not convinced it would be generally beneficial to always take a_{min}, and in practice, a_{min} is not guaranteed to always lead to the largest TD error. Though the authors attempt to claim their proposed method is better than UCB through the simple motivating task on chain MDP, I'm still not convinced that the MaxMin TD Learning could generally beat the strong UCB policy variants when tackling challenging RL domains like Atari 2600. \n\n- I think the empirical results on the motivating example are flawed. The authors show the learning curves of MaxMin TD, $\\epsilon$-greedy, and UCB, but it is unclear how the exploration policies for the two baselines are specified. For $\\epsilon$-greedy, it seems that the authors fix the $\\epsilon$ value, otherwise, I expect a well-tuned $\\epsilon$-greedy with $\\epsilon$ decay properly defined will succeed in the simple chain MDP. It is suspicious why $\\epsilon$-greedy converges to a sub-optimal average return. I also wonder if MaxMin TD learning can learn properly without $\\epsilon$ decay. It is unfair if the authors allow MaxMin to employ a decayed $\\epsilon$, while keeping that for $\\epsilon$ or UCB fixed. Please specify the details of each policy.\n\n- For the large-scale Atari 100K evaluation, the baselines are insufficient. As the algorithm focuses on exploration policy, at least it should compare with the UCB-variant of baselines. Also, neither noisy networks nor $\\epsilon$-greedy is the SOTA method on Atari 100K. The authors should employ stronger baselines. \n\n- The learning curves for the noisy net are missing in the Atari 100K figures (e.g., Fig 2 and Fig 4). They should be added at least to each game's learning curve.  \n\n- It would be more convincing if the authors could evaluate MaxMin TD Learning on a more inclusive range of tasks, e.g., Atari 2600 and mujoco, where the method could work on top of both value-based and policy-based algorithms to verify its generality."
            },
            "questions": {
                "value": "Please refer to the WEAKNESSES section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7623/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699343296832,
        "cdate": 1699343296832,
        "tmdate": 1699636925214,
        "mdate": 1699636925214,
        "license": "CC BY 4.0",
        "version": 2
    }
]