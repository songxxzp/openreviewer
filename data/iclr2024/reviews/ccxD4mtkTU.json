[
    {
        "id": "n8vonTc8um",
        "forum": "ccxD4mtkTU",
        "replyto": "ccxD4mtkTU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_NKW5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_NKW5"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the possibility of the generation of misinformation from LLMs, whether human evaluators can identify LLM-generated misinformation, and assesses the performance of automated detectors in identifying human vs. LLM-generated misinformation. Among other things, the paper finds that LLM-generated misinformation is harder to detect for humans compared to human-written misinformation and that LLM-generated misinformation is harder to detect for automated methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I would like to thank and applaud the authors for working on this very important and timely problem; this research has the potential to have a big impact on the research community and help mitigate emerging problems like the spread of misinformation online. I liked that the paper creates a taxonomy of LLM-generated misinformation and then goes further and investigates the generation of LLM misinformation across types, sources, and domains. Also, an important strength of the paper is that the paper\u2019s evaluation considers many state-of-the-art LLMs used for in-context learning purposes to solve the problem of misinformation detection."
            },
            "weaknesses": {
                "value": "While I believe that this study is an important research effort, I have some concerns with the way that the paper conducts the experiments and the interpretation of the results. Below, I provide more details on my main concerns with the paper.\n\nFirst, the paper\u2019s evaluation is done on a very small scale, particularly 100 pieces of news, and leveraging only 10 human evaluators to assess the performance of humans and compare it with various LLM-based automated detectors. Due to this, I am wondering how robust and generalizable the presented results are. At the same time, the paper does not discuss whether the presented results and differences between human-written and LLM-generated misinformation are statistically significant. That is, the paper simply presents the results and differences without providing any additional context of how statistically significant the results. I suggest to the authors to consider expanding their evaluation and discussing the statistical significance of these results.\n\nSecond, the paper lacks important details on how the ten human evaluators are selected. Do these evaluators have previous experience with annotating piece of information as misinformation or not? Do you take any steps to ensure that the annotations are of high quality and that the annotators did not use LLM to solve the task? For instance, the paper by Veselovsky et al. [1] demonstrated that crowd workers are using LLMs to solve tasks, so I am wondering if the paper took any steps to ensure that the human evaluators solved the task on their own. I think this is a crucial part of the paper as many results rely on the quality of these annotations and more details can shed light into these concerns. Finally, it is unclear to me why the paper studies the performance of the human evaluations on a per evaluator basis rather than taking the majority agreement of the evaluators per piece of information and then reporting the results on aggregate. Also, I suggest to the authors to include the inter-annotator agreement of the evaluators so that we can assess how difficult was the presented task for them. \n\nFinally, from the paper, it\u2019s unclear how the attack rates in Section 3 are calculated. Are these based on manual evaluations from the authors? I suggest to the authors to provide more details on how the annotated the generated pieces of information, how many people annotated each piece, etc.\n\nReference: \n\n[1] Veselovsky, V., Ribeiro, M.H. and West, R., 2023. Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks.\u00a0arXiv preprint arXiv:2306.07899."
            },
            "questions": {
                "value": "1. How are the 10 human evaluators selected, and did you take any steps to ensure that their annotations are of high quality? Also, did you take any steps to assess if the annotators used LLMs to solve the task (see paper by Veselovsky et al., 2023)\n2. Are the presented results and differences between human and LLM-misinformation statistically significant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Reviewer_NKW5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697702390535,
        "cdate": 1697702390535,
        "tmdate": 1699636983223,
        "mdate": 1699636983223,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "snxPc90DWB",
        "forum": "ccxD4mtkTU",
        "replyto": "ccxD4mtkTU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_bQ63"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_bQ63"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a problem with high significance and urgency: detection of LLM-generated misinformation. More specifically, the development of advanced LLM make it easy for misinformation creators to efficiently generate misinformation. A critical question is: is the LLM-generated misinformation detectable? To understand this question better, the authors built up a LLM-generated misinformation dataset and then compare its detection difficulty with human-written misinformation for both human verifiers and machine learning models. Extensive experiments suggested that compared to human-written misinformation, LLM-generated misinformation is more deceptive and potentially more harmful."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Significance of the research question: AI-generated misinformation is a very critical problem for the development of LLM. The development of RLHF-based LLM can make the misinformation creators easily generate misinformation without any preliminary knowledge about deep learning. We urgently needed exploration on the topic. \n\n2. Contribution to the community: This paper discuss the problem in a great details and can provide us with good resources (dataset and prompts) to study this problem.\n\n3. Experiment details are discussed in details."
            },
            "weaknesses": {
                "value": "1. The dataset seems to be not very large. I understand that for evaluating human detection difficulty, we can not use too large dataset. But the authors can enlarge the dataset for evaluation of machine learning model.\n\n2. For detector difficulty, the authors only discussed the zero-shot detection of generative LLMs. The results on other kinds of models (i.e. in-context-learning boosted LLMs, soft-prompt based LLMs, and encoder-based Large models like BERT and its variants) are not discussed.\n\n3. Dataset is not opensourced. Actually, the data can be opensourced anonymously on GitHub."
            },
            "questions": {
                "value": "1. Will the datset be opensourced once the paper is accepted?\n\n2. Is it possible to generate more data for detection evaluation?\n\n3. Will few-shot learning improve the performance of the detection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698448233184,
        "cdate": 1698448233184,
        "tmdate": 1699636983103,
        "mdate": 1699636983103,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FSDPR6iHYX",
        "forum": "ccxD4mtkTU",
        "replyto": "ccxD4mtkTU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_oGY2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_oGY2"
        ],
        "content": {
            "summary": {
                "value": "Summary: Large Language Models (LLMs) have become increasingly powerful and are capable of generating human-like text. This capability has raised concerns that LLMs could be used to generate misinformation. The authors investigate the difficulty of detecting LLM-generated misinformation compared with human-written misinformation. The authors find that LLM-generated misinformation can be harder to detect for both humans and detectors."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is a well-written and informative contribution to the field of misinformation research. It provides important insights into the potential for LLMs to be used to generate deceptive and harmful misinformation.\n- It is one of the first papers to systematically investigate the detectability of LLM-generated misinformation.\n- It creates a taxonomy and identifies three different types of LLM-generated misinformation: Hallucinated News Generation, Totally Arbitrary Generation, and Partially Arbitrary Generation.\n- It evaluates the detectability of different types of LLM-generated misinformation by humans."
            },
            "weaknesses": {
                "value": "Cencern1: The study is relatively small number of evaluators and only evaluates a limited number of LLM-generated news items. This means that the findings of the study may not be generalizable to all LLM-generated news items.\n\nConcern 2: The study does not evaluate the effectiveness of different detection methods for LLM-generated misinformation. This means that it is not clear how well existing detection methods would perform at detecting the LLM-generated news items used in the study.\n\nJiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, and Bimal Viswanath. Deepfake text detection: Limitations and opportunities. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1613\u20131630. IEEE, 2023\n\nMitchell E, Lee Y, Khazatsky A, Manning CD, Finn C. Detectgpt: Zero-shot machine-generated text detection using probability curvature. arXiv preprint arXiv:2301.11305. 2023 Jan 26 (ICML 2023)"
            },
            "questions": {
                "value": "Cencern1: The study is relatively small and only evaluates a limited number of LLM-generated news items. This means that the findings of the study may not be generalizable to all LLM-generated news items.\n\nConcern 2: The study does not evaluate the effectiveness of different detection methods for LLM-generated misinformation. This means that it is not clear how well existing detection methods would perform at detecting the LLM-generated news items used in the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "It requires IRB due to human evaluators."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809631428,
        "cdate": 1698809631428,
        "tmdate": 1699636982968,
        "mdate": 1699636982968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aovsMtDXqd",
        "forum": "ccxD4mtkTU",
        "replyto": "ccxD4mtkTU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_jt9v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7983/Reviewer_jt9v"
        ],
        "content": {
            "summary": {
                "value": "The paper primarily discusses the ways in which LLMs can generate or can be leveraged to generate misinformation. It discusses its implications via various means of generation (established through an LLM generated misinformation taxonomy) and how easy/difficult it is detect this misinformation when compared to human-written misinformation.\n\nThe paper notes that LLM generated misinformation is harder to detect and can potentially cause more harm, through human evaluations and LLM based detection experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper does a good job at describing the problem statement and their contributions. It's a good survey on the related techniques within this space.\n- The misinformation taxonomy and the generation strategies of hallucination, Arbitrary Misinformation and Controllable Misinformation generation are interesting to note\n- Utilizing CoT and non CoT prompting to study LLM based misinformation detection is interesting\n\nOverall the paper is a comprehensive study on LLM generated misinformation and related techniques."
            },
            "weaknesses": {
                "value": "The paper lacks a review or comparison with pre-LLM era misinformation or fake news detection strategies. There are techniques within fact-finding and source-attribution space which can be leveraged to detect misinformation and those haven't been discussed.\n\nThe paper often uses Appendix sections to support the claims made which makes it less readable and less self-contained.\n\nThe paper establishes what 'detectors' are, rather late.\n\nOverall the paper is a comprehensive study on LLM generated misinformation and related techniques, but found it to be lacking in making a significant/original innovation.\n\nMinor:\nspelling mistake in word 'Appendx' in section 4"
            },
            "questions": {
                "value": "\"against HC method\" in section 3, page 4, is it supposed to be HG?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7983/Reviewer_jt9v"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7983/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699576532986,
        "cdate": 1699576532986,
        "tmdate": 1699636982852,
        "mdate": 1699636982852,
        "license": "CC BY 4.0",
        "version": 2
    }
]