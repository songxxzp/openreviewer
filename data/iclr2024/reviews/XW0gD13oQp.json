[
    {
        "id": "JVf7VOJGtG",
        "forum": "XW0gD13oQp",
        "replyto": "XW0gD13oQp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_35MJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_35MJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel language agent architecture for detecting implicit stereotypes in text-to-image models at scale. The agent is capable of generating instructions, invoking various tools, and detecting stereotypes in generated images. The authors construct a benchmark dataset based on toxic text datasets to evaluate the agent's performance and find that text-to-image models often display serious stereotypes related to gender, race, and religion. The results highlight the need to address potential ethical risks in AI-generated content. The paper contributes a comprehensive framework for stereotype detection and emphasizes the importance of addressing biases in AI-generated content."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1) The paper introduces a novel approach for detecting implicit stereotypes in text-to-image models using a language agent framework.\n\n2) The agent's performance closely aligns with manual annotation, indicating a high quality of its stereotype detection capabilities.\n\n3) The paper addresses an important and timely issue in the field of AI-generated content by highlighting the potential biases and stereotypes present in text-to-image models.\n\n4) The findings of this study underscore the critical necessity of addressing ethical risks in AI-generated content and call for increased awareness and regulation in the field."
            },
            "weaknesses": {
                "value": "1) The benchmark dataset presented, may not fully capture the diversity and complexity of stereotypes present in real-world scenarios. The distribution of subgroups within the benchmark dataset is imbalanced, particularly in the race/ethnicity and religion dimensions. \n\n2) The paper lacks a comprehensive evaluation of the proposed agent framework. While the performance on detecting stereotypes is reported, there is no analysis of false positives, false negatives, or the impact of different parameters.\n\n3) The paper does not compare the proposed agent framework with existing methods for stereotype detection in text-to-image models.\n\n4) The paper does not provide a comprehensive justification for the selection of specific tools within the agent framework, nor does it discuss the optimization process for these tools. \n\n5)  While the paper acknowledges the ethical risks associated with AI-generated content and the need for bias governance, it does not provide a thorough discussion of the potential impacts and implications of stereotype detection in practice. Considerations such as the unintended consequences of bias mitigation strategies, the role of human judgment in determining stereotypes, and the balance between freedom of expression and risk mitigation should be addressed in more detail.\n\n6) The paper focuses on the detection of stereotypes but does not provide explicit recommendations or strategies for mitigating these biases."
            },
            "questions": {
                "value": "1) What are the potential limitations or challenges of using language agents for stereotype detection in text-to-image models? Are there any specific scenarios or cases where the agent may not perform as accurately?\n\n2) In the agent performance evaluation, the proportion of bias detected by the agent is compared to the manual annotation results. Can you provide more information about the criteria used for manual annotation and how the annotators determined the presence of stereotypes?\n\n3) Can you provide more details about the annotation process used to obtain the ground truth labels for the generated images? How many annotators were involved, and was there any inter-rater reliability assessment conducted?\n\n4) How did you select the toxic text datasets used to construct the benchmark dataset? Did you consider any specific criteria or guidelines in selecting these datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The main ethical concern with this paper is the potential for reinforcing stereotypes and biases in AI-generated content. The paper discusses the detection of stereotypes in text-to-image models, but it does not provide explicit recommendations or strategies for mitigating these biases. This raises ethical considerations regarding the responsibility of researchers and developers to address and mitigate biases in AI systems, especially those that have the potential to perpetuate harmful stereotypes and discrimination. Additionally, there is a need to ensure that the benchmark dataset used for evaluation is diverse, representative, and free from any additional biases. Creating a dataset based on toxic text datasets may introduce additional ethical considerations, such as the potential for harm to individuals or communities whose data is included in those datasets. The paper should be reviewed from an ethics perspective to assess the potential impact and implications of the research on society, and to ensure that appropriate measures are taken to mitigate any harmful effects."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2950/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2950/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2950/Reviewer_35MJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2950/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689361336,
        "cdate": 1698689361336,
        "tmdate": 1699636238885,
        "mdate": 1699636238885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ePuw0yt57r",
        "forum": "XW0gD13oQp",
        "replyto": "XW0gD13oQp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_d3He"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_d3He"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an orchestration of LLM-based tools to evaluate and assess bias in a several text-to-image models. The agent framework takes a text given as input and interprets the query in terms of specific instructions in terms of paired prompts and subgroups. These are then formatted into an optimized prompt for the text-to-image model. A stereotype score is then calculated based on the model output.\nThis framework is then used to compare a range of popular models, some of which, such as chilloutmix displaying high stereotype scores according to their model. Finally a comparison with human labels is performed, to show the robustness of the scoring framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is an interesting study that brings novel ideas for how to systematically assess text-to-image models for stereotypical biases. The orchestration of an agent framework makes this model modular so that it can easily be applied to new models, and can easily be extended to include further stereotypes or benchmarks that might be of interest. As the prevalence of AI generated content increases with the wider adoption of such models, understanding their biases and being able to quickly assess new models and releases is of high topical interest in AI safety and fairness."
            },
            "weaknesses": {
                "value": "The technical novelties of this paper are quite limited, as it is an automated assessment framework for stereotypes in text-to-image models. The models as well as the metrics considered are from the existing literature."
            },
            "questions": {
                "value": "While it is included in the submission auxiliary materials, there is no mention of open sourcing the code in the paper.\nIn my opinion this study should only be accepted if the code is included in an easy to access format, such that the study performed in this paper can easily be reproduced by other researchers on new models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2950/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833754832,
        "cdate": 1698833754832,
        "tmdate": 1699636238819,
        "mdate": 1699636238819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BEq0CRVbWB",
        "forum": "XW0gD13oQp",
        "replyto": "XW0gD13oQp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_xoCn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2950/Reviewer_xoCn"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors build a system on top of an LLM to generate images and detect stereotypes in text to image models.  They use the LLM to generate pairs of groups and stereotypes, and then to run classification with a tool on the generated images.  They demonstrate that across multiple models there are significant stereotypes in the generated images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. Detecting stereotyping in text-to-image models is important\n\nS2. Using LLMs to drive stereotype detection is a good idea, and the idea of taking this to higher levels of abstraction for an agent is intriguing.\n\nS3. The approach does seem effective in uncovering stereotypes."
            },
            "weaknesses": {
                "value": "aper seems to try to do too many things and as a result I believe doing none of them sufficiently well and adding confusion to the paper:\n\nW1a. The paper is framed around the method proposing an autonomous agent for stereotype detection. This is a great vision, but the method seems to (a) follow a consistent, pre-determined sequence of actions for the task, and (b) it seems far from autonomous in relying on human intervention and a lot of custom steps (unique datasets, custom prompts, human feedback, etc) at every stage.  This to me is not a critique of the method but that it shouldn't be over-complicated or over-sold as an autonomous agent rather than a reliable process for red-teaming for stereotypes building on LLMs as a tool in that process.\n\nW2a. There has been a fair amount of work on sterotype detection which this work is not compared to and does not grapple with similar issues.  For example, what if there are multiple people in the same image?  How is the diversity and coverage of the generated concerns? When is this better than a curated list? For example, this claim is good but I'd like to experimental evidence: \"However, this approach to bias evaluation has its limitations, as it often neglects the more subtle stereotypes prevalent in everyday expressions. These biases frequently manifest in toxic content disseminated across various social platforms, including racial slurs\"\n\nW3a. [less critical] The method is framed as benchmarking but I think is better explained as automated red-teaming.  Because the metrics and distribution is less controlled, understanding this as a consistent benchmark seems challenging but the discovered issues are still important as in red-teaming.\n\nW2. Related to the above point, it is hard to gauge how diverse the stereotypes uncovered are and how diverse the images are.  (Are all imges generated with \"The people who ___\")"
            },
            "questions": {
                "value": "I'd love to see greater understanding of the diversity of stereotypes, a comparison with past work, and more clarity on the autonomy and flexibility of the agent to new tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper is targeting a good, ethical application (detecting stereotypes) but given the level of automation, I'd like to see an ethics review to ensure the relevant concerns are being handled with sufficient sensitivity."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2950/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024791744,
        "cdate": 1699024791744,
        "tmdate": 1699636238703,
        "mdate": 1699636238703,
        "license": "CC BY 4.0",
        "version": 2
    }
]