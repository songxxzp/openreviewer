[
    {
        "id": "q08EMQvIMe",
        "forum": "SNGXbZtK6Q",
        "replyto": "SNGXbZtK6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission577/Reviewer_iXGU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission577/Reviewer_iXGU"
        ],
        "content": {
            "summary": {
                "value": "The study focuses on the issue of out-of-distribution (OOD) in neural networks by examining the activity of neurons. A notion termed as \"neuron activation coverage\" (NAC) is introduced to describe the operation of neurons under in-distribution (ID) and OOD data. The researchers utilize NAC to attain good OOD detection results and demonstrate a positive association between NAC and the capacity of model generalization. Models that are more robust are chosen through the NAC-based standard and it presents a higher correlation with OOD test outcomes compared to conventional validation standards."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The holistic approach of tackling both OOD detection and generalization issues concurrently is not only commendable but also crucial. \n\n2. The meticulous evaluation, inclusive of visualization and ablation study, is praiseworthy. This level of precision in assessment bolsters the understanding of the research outcomes and solidifies the integrity and robustness of the findings."
            },
            "weaknesses": {
                "value": "1. The introduction contains some issues: The methods like ASH, React do not negatively impact the ID accuracy. The model can employ an unchanged classifier for the ID task, which only introduces a negligible computational cost. \n\n2. The motivation is somewhat perplexing: The author suggests that OOD is more prone to activate neurons with lower activation frequency. However, this paper fails to provide empirical evidence to back up this assertion.\n\n3. Also, I think that the confusion between OOD and ID images stems from the fact that OOD images can potentially activate the same neurons as ID images. How does the author's approach works in such a scenario? It would be beneficial for the author to include near-ood experiments [1] to enhance the comprehensiveness of the study.\n\n4. The performance of NAC-UE, as depicted in Tables 1 and 2, is not consistent and competitive across all OOD datasets, indicating a considerable bias in its effectiveness on different OOD datasets. For example, when CIFAR100 is used as ID and Places365 as OOD, NAC-UE only yields a 73.05% AUROC and 73.57 FPR95.\n\n5. The author has overlooked a discussion on a comparable study [2].\n\n[1]  Fort, Stanislav, Jie Ren, and Balaji Lakshminarayanan. \"Exploring the limits of out-of-distribution detection.\" Advances in Neural Information Processing Systems 34 (2021): 7068-7081.\n\n[2] Bai, Haoyue, et al. \"Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection.\"\u00a0International Conference on Machine Learning. PMLR, 2023."
            },
            "questions": {
                "value": "The absence of open-source code is a setback as it impedes the reproducibility and validation of the results presented in the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Reviewer_iXGU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697536816955,
        "cdate": 1697536816955,
        "tmdate": 1700623355069,
        "mdate": 1700623355069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OsHkg1wES5",
        "forum": "SNGXbZtK6Q",
        "replyto": "SNGXbZtK6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission577/Reviewer_sJ3q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission577/Reviewer_sJ3q"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a new metric named neuron activation coverage (NAC) to quantify the out-of-distribution. It uses neuron behaviors to quantify data distribution. It extends from original raw neuron output, models the neuron influence with combine neuron output with gradient from KL divergence of network output and a uniform vector. It is a simple metric while achieve SOTA results compared to previous methods on three benchmarks. It also shows correlation with generation, and could be used as a metric to quantify the robustness of model, and also correlate with OOD test performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Method:\n\n1. Combine neuron output with gradient from KL divergence of network output and a uniform vector is a novel contribution and well-motivated approach, and it is simple while shows effective results.\n2. It preserves more information in a continuous fashion, without discretizing it to become binary values compared to previous approach. \n3. The measurement is further extended for uncertainty estimation for test samples with using average of all neurons.\n\nEvaluation:\n1. The method demonstrates its soundness on multiple benchmarks and outperforms existing SOTA.\n2. Multiple ablation studies are performed.\n3. Guidance and decisions of hyper parameters choice is discussed.\n\nWriting:\n1. The paper is well-organized, and the presentation is in good structure."
            },
            "weaknesses": {
                "value": "Time complexity:\n\n1. Given the time-consuming process of using KL gradient, computational cost of proposed method with other SOTA should be reported.\n\nEvaluation:\n\n1. The proposed metric is extended for uncertainty estimation, while no quantification or evaluation (calibration of uncertainty) on this dimension."
            },
            "questions": {
                "value": "1. How does the choice of rank correlation instead Pearson correlation might affect the results?\n\n2. How does this metric generalized to regression problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Reviewer_sJ3q"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737200124,
        "cdate": 1698737200124,
        "tmdate": 1699635985094,
        "mdate": 1699635985094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RKChnUJxm5",
        "forum": "SNGXbZtK6Q",
        "replyto": "SNGXbZtK6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission577/Reviewer_pybD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission577/Reviewer_pybD"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the out-of-distribution (OOD) problem in neural networks, which occurs when the encountered data significantly deviates from the training data distribution (in-distribution, InD). The authors approach the OOD problem by studying neuron activation states, considering both the output of the neuron and its influence on model decisions. They introduce a measure called neuron activation coverage (NAC) to characterize the relationship between neurons and OOD issues under InD data.\n\nBy leveraging NAC, the authors demonstrate two key findings. Firstly, they show that InD and OOD inputs can be effectively separated based on neuron behavior. This separation greatly simplifies the OOD detection problem and outperforms 21 previous methods across three benchmark datasets (CIFAR10, CIFAR-100, and ImageNet-1K). Secondly, they observe a consistent positive correlation between NAC and model generalization ability across different architectures and datasets. This correlation enables the use of NAC as a criterion for evaluating model robustness. In comparison to prevalent InD validation criteria, the authors demonstrate that NAC not only selects more robust models but also exhibits a stronger correlation with OOD test performance.\n\nOverall, the paper proposes a novel approach to the OOD problem by studying neuron activation states and introducing NAC as a measure. The findings highlight the effectiveness of NAC in separating InD and OOD inputs, as well as its potential for evaluating model robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Novel approach: The paper introduces a fresh perspective on addressing the OOD problem by studying neuron activation states and proposing the concept of neuron activation coverage (NAC). This approach offers a new and innovative methodology for tackling the OOD problem, contributing to the advancement of the field.\n\nEffective separation of InD and OOD inputs: The paper demonstrates that the proposed neuron behavior-based approach, leveraging NAC, can effectively separate InD and OOD inputs. This separation significantly eases the OOD detection problem and outperforms 21 previous methods across multiple benchmark datasets. The ability to accurately distinguish between InD and OOD inputs is crucial for improving the reliability and robustness of neural networks.\n\nConsistent correlation with model generalization ability: The authors observe a consistent positive correlation between NAC and model generalization ability across different architectures and datasets. This correlation indicates that NAC can serve as a reliable criterion for evaluating model robustness. By considering neuron activation behaviors, NAC provides valuable insights into the model's ability to generalize well beyond the training distribution, facilitating OOD generation. \n\nPractical applicability: The proposed approach holds promise for real-world applications. By focusing on neuron activation states, the method does not require modifications to the neural network architecture, making it easily implementable in existing systems. Furthermore, the demonstrated effectiveness of NAC in separating InD and OOD inputs across different benchmark datasets reinforces its potential for diverse application scenarios."
            },
            "weaknesses": {
                "value": "The definitions of OOD are different for OOD Detection and OOD Generalisation. From my view, the former focuses on the semantic shift and the latter focuses on the covariate shift. Such a difference may raise two problems. First, it seems better if the authors could discern OOD in detection and generalisation, with further discussion and detailed definition. Second, why the purposed method, i.e., NAC, can handle both of these two cases. Heuristically, the proposed method is motivated by previous works in OOD detection (i.e., using gradient information), so why it is also applicable for OOD generalization. \n\nThere are many dimensions that are useless, and even worse, components/directions [1], instead of individual elements, can model the contribution of model outputs in making prediction. It means that there may exist misleading information and redundant message when using NAC to compute the activated rates, of which the performance cannot be guaranteed, at least from my personal view. \n\nWhen it comes to OOD generalization, the authors suggest use NAC-based learning objective to improve OOD performance. It seems that it just makes the embedding features to be sparse, which has been discussed in previous works in OOD generalization, such as [2]. Moreover, the gradient-based learning objective can be computational hard, since the second order gradients are needed to be computed in each optimisation step. \n\nDetailed discussion about the hyper parameter setups should be given, especially for the hyper parameter tuning. More ablation studies should  be given to test the respective power of z \\times g'(z) and p-u.\n\n[1] Intriguing Properties of Neural Networks\n\n[2] Sparse Invariant Risk Minimisation."
            },
            "questions": {
                "value": "please see the weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Reviewer_pybD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817257463,
        "cdate": 1698817257463,
        "tmdate": 1700700158709,
        "mdate": 1700700158709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qMIsDsYbm3",
        "forum": "SNGXbZtK6Q",
        "replyto": "SNGXbZtK6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission577/Reviewer_xpKd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission577/Reviewer_xpKd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the concept of neuron activation coverage (NAC) to characterize neuron behaviors under in-distribution (InD) and out-of-distribution (OOD) data. The key idea is to quantify the coverage degree of neuron activation states using training data, such that rarely covered states likely contain defects that could lead to poor OOD performance. The authors formulate neuron states by combining raw outputs and KL divergence gradients, and model coverage through a probability density function. NAC is applied to improve OOD detection via uncertainty estimation (NAC-UE), and OOD generalization through model evaluation (NAC-ME). Experiments demonstrate state-of-the-art OOD detection results on CIFAR and ImageNet benchmarks. NAC-ME also shows favorable correlation with OOD test accuracy for model selection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Novel perspective of leveraging neuron coverage to reflect OOD issues. This is intuitive and well-motivated through the analogy to software testing.\n* Simple yet effective formulation of neuron activation states and coverage function.\n* Improvements over strong baselines across multiple datasets and tasks. NAC-UE sets new state-of-the-art results while preserving model accuracy."
            },
            "weaknesses": {
                "value": "* For NAC-UE uncertainty estimation, the paper averages coverage scores across layers when applying to multiple layers. Is it possible to weight the contribution of different layers especially given that earlier layers tend to be more general?\n* Missing a related work that shares a similar motivation of leveraging neuron behaviors. [1] uses neural mean discrepancy of neuron activations for OOD detection. Discussing relations to such works could provide additional insights.\n\n[1] Dong, Xin, et al. \"Neural mean discrepancy for efficient out-of-distribution detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022"
            },
            "questions": {
                "value": "* Is it possible to directly train or regularize models based on NAC to improve robustness. For example, one could minimize the entropy of NAC distributions to encourage balanced coverage. Evaluating such NAC-driven training schemes could further validate its usefulness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission577/Reviewer_xpKd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820163114,
        "cdate": 1698820163114,
        "tmdate": 1700626805382,
        "mdate": 1700626805382,
        "license": "CC BY 4.0",
        "version": 2
    }
]