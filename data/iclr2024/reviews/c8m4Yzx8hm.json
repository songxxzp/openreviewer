[
    {
        "id": "OQ8HQ6l1Uh",
        "forum": "c8m4Yzx8hm",
        "replyto": "c8m4Yzx8hm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_mBML"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_mBML"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a visual topic model that applies topic modeling to images."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-"
            },
            "weaknesses": {
                "value": "This paper proposes something as new which exists already. There have been countless works on visual topic modeling, none of them are cited.\n\nThe comparison is only with dimensionality reduction methods, not with other topic modeling methods.\n\nPlease check for example the following overview article for some example references: \nBlei, D., Carin, L., & Dunson, D. (2010). Probabilistic topic models. IEEE signal processing magazine, 27(6), 55-65."
            },
            "questions": {
                "value": "How is your work different from existing visual topic models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697541309144,
        "cdate": 1697541309144,
        "tmdate": 1699636652297,
        "mdate": 1699636652297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s8xo8XxjzH",
        "forum": "c8m4Yzx8hm",
        "replyto": "c8m4Yzx8hm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_GURR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_GURR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes visual topic modeling, which is an approach for mapping images into visual words. Thereafter, LDA is used as a topic modeling algorithm to convert the visual words into visual topics. The generated visual topics are evaluated using standard topic modeling metrics and confirm the interpretability of those topics via a human study.\n\nThe main intuition behind this paper is to consider images as documents, represented by their visual words. Therefore, a topic modeling algorithm can be used to find relevant topics that are more relevant and interpretable as compared to using clustering algorithms.\n\nThe contributions of this paper are as follows:\n- a visual topic modeling approach is proposed to convert images into visual words \n- visual topics generated by this approach are of a good quality according to topic modeling metrics and highly interpretable via human evaluation"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to read\n- The proposed approach is described with a lot of details (specifically in the appendix) and is reproducible."
            },
            "weaknesses": {
                "value": "- The proposed approach for creating a visual vocabulary is very similar to the Bag-of-Visual-Word (BoVW) model, if not the same, and can not be considered a major contribution.\n\n[1] Sivic & A. Zisserman (2003). \"Video Google: A Text Retrieval Approach to Object Matching in Videos\" (PDF). Proc. of ICCV.\n[2] G. Csurka; C. Dance; L.X. Fan; J. Willamowski & C. Bray (2004). \"Visual categorization with bags of keypoints\". Proc. of ECCV International Workshop on Statistical Learning in Computer Vision.\n\n- There are many previous work using a very similar approach to this paper, i.e. extracting visual words and using topic modeling (LDA) for image annotation [3, 4]. The authors failed to mention these papers in the related work section, and also failed to mention how their approach is different as compared to those. \n[3] Topic Modeling of Multimodal Data: An Autoregressive Approach. Yin Zheng, Yu-Jin Zhang, Hugo Larochelle; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1370-1377\n[4] Putthividhy, Duangmanee, Hagai T. Attias, and Srikantan S. Nagarajan. \"Topic regression multi-modal latent dirichlet allocation for image annotation.\" 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010.\n\n- A comparison of the proposed visual word to existing visual word models is missing (for instance the Bag-of-Visual-Word model). Also, comparing the different visual topics using LDA on the different visual word extraction models."
            },
            "questions": {
                "value": "- How the visual words/vocabulary presented in this work is different from prior work?\n- Same question for the visual topics?\n- What are some applications of the visual topics proposed in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814946556,
        "cdate": 1698814946556,
        "tmdate": 1699636652156,
        "mdate": 1699636652156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sRXZIZG3vs",
        "forum": "c8m4Yzx8hm",
        "replyto": "c8m4Yzx8hm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_x5q5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6060/Reviewer_x5q5"
        ],
        "content": {
            "summary": {
                "value": "The idea is to build visual topics for image - in this case  the grouping of similar segments to make visual words - the so called Bag of Word models.\n\nIn this way, the paper shares the ambition (and some of the methods, notably LDA) of the recognition literature before neural algorithms became dominant. \n\nBut, the athours here are seem also looking a co-occurances (which makes it different from the bulk of those pre-NN algorithms.)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I very much like to idea of learning elementary \"words\" from which \"topics\" can be built.\nAnd I like the idea of unsupervised learning."
            },
            "weaknesses": {
                "value": "The topics output are very hard to see (Figure 4). \nI cannot see similar segments (visual words) - there is no Figure with, eg a collection of visual words\nie there is no visualisation of the visual vocablaries the authors claim to produce.\n\nI am not sure what I have learned from this paper.\nThe abstract does not mention co-occurance, but the method (eg Eqn 4) does."
            },
            "questions": {
                "value": "It is not clear how you learn visual words, or even whether you seek to learn visual objects for noun classes.\nThis then interefers with reading how you use co-occurance.\nAnd the figures do not help me out at all - they only add to the problems because they are so hard to see.\nThe conclusion does not match the abstract well.\nThese factors mean I am unable to fully appreciate your paper or the work behind it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6060/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6060/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6060/Reviewer_x5q5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6060/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699191110117,
        "cdate": 1699191110117,
        "tmdate": 1699636652026,
        "mdate": 1699636652026,
        "license": "CC BY 4.0",
        "version": 2
    }
]