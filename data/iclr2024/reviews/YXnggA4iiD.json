[
    {
        "id": "l7mCEb6HJx",
        "forum": "YXnggA4iiD",
        "replyto": "YXnggA4iiD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_aVJe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_aVJe"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a robust distribution-aware learning and sample selection strategy that employs Gaussian mixture model (GMM) to effectively encapsulate both labeled and unlabeled sets for AL.\nA regularization method and an informativeness metric are further proposed to detect overfitted areas. Experiments on mulltiple datasets including balanced and imbalanced datasets demonstrate the validity of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The solution of applying distribution alignment for active learning is valid.\n(2) Multiple datasets including balanced and imbalanced datasets are evaluated to show the advantages of the proposed method."
            },
            "weaknesses": {
                "value": "(1)\tIt seems that the paper pose a relatively strong assumption that the data follows Gaussian mixture model (GMM) and derive the statistics from GMM. However, the author fails to show that GMM is a good model to approximate the data distribution in active learning. Actually, because of the nature of active learning, there are frequently samples from new classes that does not follow the estimated GMM model. In that case, the estimation based on GMM will not be accurate.\n(2)\tThe novelty of the paper is limited as the distribution alignment has been widely studied in the previous work:\n(i)\tZhang et al \u201cDistribution Alignment: A Unified Framework for Long-tail Visual Recognition\u201d. CVPR 2021\n(ii)\t\u201cAgreement-Discrepancy-Selection: Active Learning with Progressive Distribution Alignment\u201d, AAAI 2021\nThe reference (ii) also mentioned distribution alignment with adversarial learning but under a more general setting without GMM assumption. Thus, it seems this paper is talking about an existing method under some special conditions."
            },
            "questions": {
                "value": "As metioned in the weakness, the fundament problem of this paper is that it is merely a special case of a solved general problem, which makes the novelty of the paper very limited. Without detailed discussion of the difference from the existing work as listed, the novelty of the paper does not stay on a safe ground.\n\nMoreover, the author needs to show why the GMM model is a good approximation of the data distribution in active learning as there can be many outliers and unknown classes. How about if the data is a long-tail distribution. The assumption of the paper is a bit too strong.\n\nLast but not least, the paper did not any new insights of this field. The results are merely a small extension and special cases of existing approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Reviewer_aVJe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698622188887,
        "cdate": 1698622188887,
        "tmdate": 1699636279812,
        "mdate": 1699636279812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2m1n0B4BrL",
        "forum": "YXnggA4iiD",
        "replyto": "YXnggA4iiD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_wZHx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_wZHx"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Gaussian Mixtures of labeled and unlabeled dataset to facilitate the model training, regularization, sample selection for active learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. the improvement in performance over baselines is significant. \n2. well-organised and easy to follow and understand, though there are many componenets in the proposed method. \n3. The idea is original and it uses classific GMM to facilitate model training, as well as for high-quality and diverse sample selection for active learning."
            },
            "weaknesses": {
                "value": "1. The main concern would be the computation burden it introduces. With a batch, within each optimization iteration, there are 10 EM runs for both labelled and unlabelled samples. So it would be very time-consuming, even though there are some tricks used to reduce it. So both theoretical and experimental analysis about time complexity would be expected. \n\n2. Some details about loss functions are missing to better understand the training. For example, there is a trade-off parameter $\\alpha$ to balance cross-entropy loss and regularization loss. The value for this constant was not given. Besides, there is another loss for adversarial learning. How to balance with this function is not clear to me. \n\n3. Some intuitive explanation to demonstrate the superiority would be great. For example, with each active learning cycle, for each class, how the selected samples from the proposed method are different from the ones from baselines? Or use the selected samples to verify the claim of 'high quality and diverse'."
            },
            "questions": {
                "value": "see above weakness points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3306/Reviewer_wZHx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747575041,
        "cdate": 1698747575041,
        "tmdate": 1699636279708,
        "mdate": 1699636279708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5sLWrrT1R7",
        "forum": "YXnggA4iiD",
        "replyto": "YXnggA4iiD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_ZH7N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3306/Reviewer_ZH7N"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new active learning strategy that addresses the issue of mismatched distributions between labeled and unlabeled samples by incorporating a Gaussian Mixture Model (GMM). The strategy combines various informativeness metrics for sample selection. Tests on different datasets show that this GMM-based method performs better than existing approaches and can be combined with other active learning methods to enhance results further."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces a new active learning strategy by incorporating a Gaussian Mixture Model (GMM). This strategy aims to address the issue of mismatched distributions between labeled and unlabeled samples.\n\nThis GMM-based method performs better than existing approaches on several datasets."
            },
            "weaknesses": {
                "value": "If I understand correctly, in Section 3.2, two GMMs are fitted for the labeled and unlabeled data, respectively. Is this correct? Equation (2) provides a means to compare two Gaussian distributions, but it is not immediately clear how this extends to the comparison of two GMMs, and whether the weight of each Gaussian component in the mix is considered in this comparison.\n\nRegarding Equation (3), the purpose of the optimization seems to be to minimize the disparity between the unlabeled data, $X_{UL}$, and the labeled data, $X_{L}$, within the embedding space. If the model is trained effectively, it appears that $X_{UL}$ and $X_{L}$ would become indistinguishable in the embedding space. It raises the question of whether this is the intended outcome, as it would seem important for the embedding space to retain the information that differentiates $X_{UL}$ from $X_{L}$.\n\nThe combination method outlined in Equation (5) is not entirely convincing as the best approach. An alternative sampling strategy might involve drawing from each of the three separate rankings and then using the highest-ranked samples from each ranking as the data set for annotation. It would also be insightful to see the results of an ablation study where each component of Equation (5) is used independently as the final selection criterion.\n\nIn Equation (5), it seems that only $I_{Ent}$ leverages the labels from previous iterations. The other two components are influenced by $X_{UL}$ and $X_{L}$ but are label-independent. It may be beneficial for the authors to make this point clearer in their writing."
            },
            "questions": {
                "value": "1. Are two GMMs fitted for the labeled and unlabeled data, respectively?\n2. How does Equation (2) extend to the comparison of two GMMs?\n3. What is the rationale behind Equation (3)?\n4. Why Equation (5) is a good way to combine the information?\n5. In Equation (5), is $I_{Ent}$ the only component that leverages the labels from previous iterations, but the other components label-independent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698891498628,
        "cdate": 1698891498628,
        "tmdate": 1699636279632,
        "mdate": 1699636279632,
        "license": "CC BY 4.0",
        "version": 2
    }
]