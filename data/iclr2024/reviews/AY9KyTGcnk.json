[
    {
        "id": "ESqscJxU2f",
        "forum": "AY9KyTGcnk",
        "replyto": "AY9KyTGcnk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_EXm4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_EXm4"
        ],
        "content": {
            "summary": {
                "value": "The paper investigated strongly adaptive regret under the multi armed bandit set up. In addition to standard multi arm bandit framework that the learner pulls a single arm and suffers a loss generated by the chosen arm, if the learner is allowed to query one extra arm at each iteration free of charge, then the proposed algorithm `StABL` achieves $O(\\log T \\sqrt{n I \\log n} )$ strongly adaptive regret over any interval $I$ for a game consistent of $n$ expert span over a time horizon $T$. \n\nIn addition, a mirroring design of  `StABL`  can also be applied to solve bandit convex optimization problem if the learner has access to query two additional arms free-of-charge. This resultant to $O(dGD\\sqrt{I} \\log T )$ regret for $G$-Lipschitz loss and $d$-dimensional convex domain $\\mathcal{K}$ that can be sandwiched between two $\\ell_2$ ball with diameter $r, D$, respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is good at gathering key ingredients needed from previous literature, the intuition of the algorithm construction, and the necessity of the second query in order to achieve desired result.\n\nThe result is novel. Given previous lower bound on showing strongly adaptivity for bandit is impossible, this paper considers the nearly identical set-up to the impossible scenario and the presented result is optimal. Overall, in order to achieve the optimal regret in comparison to previous works which generally require $O(\\log T)$ requires, the presented algorithm only require two queries for multi armed bandit set-up, three queries for the bandit convex optimization set-up. For the later case, it is also possible to achieve with two queries.\n\nThe experiment on synthetic data supports the construction of the algorithm adapts to changing environment. The proposed algorithm also excel in practical downstream tasks is demonstrated.\n\nOverall, this paper is technical solid. The investigated problem is interesting, and the presented result is strong. It should be accepted provided the clarity of the write-up needs to be improved."
            },
            "weaknesses": {
                "value": "pg 4: sec2.2 second equation, LHS is a scalar, RHS is a vector \n\npg 4: sec 3. The abbreviation 'MWU' appeared the first time\n\nIn general, clarity of the write-up for section 3 needs to be improved. \n\nAlgo1: some part of the algorithm is ambiguous. For example, it might be more concise to gather all initialization parameters of exp3 base algorithms $\\mathcal{A}_k$ together: ( Initialize $\\mathcal{A}_k$ as algorithm 2 with learning rate ... and time horizon... ). Currently it is not immediately clear how $\\mathcal{A}_k$ was initialized, and $v(t,k)$ is the $t^{th}$ output from $\\mathcal{A}_k$ (a copy of algorithm 2) to the meta-algorithm. In addition, due to the preceding ambiguities, it causes confusion with $\\eta_k$ defined in Algorithm 1 given Theorem 1 does not specify learning rate and time horizon for $\\mathcal{A}_k$. \n\nAlgo 2: iterates are inconsistent, varying between $w$ and $x$. Also Lemma 4 is a general analysis for Algo2, it should quote algorithm 2 in stead of using `The EXP-3 algorithm`, which is even not consistent with the algorithm name itself.\n\nLemma 2: In the main text, given the context of Algorithm 2 $\\mathcal{A}_k$, $w_t(i)$ is the probability for picking the $i^{th}$ arm. This is $v(t,k)_i$ defined in Algorithm 1. Not what was stated as the weight of the ith expert at time $t$. Due to this confusion, it is hard to see $C = 2\\log T$. It only can be deduced after reading appendix at page 14. \n\nIt is also necessary to show the expressions of regret of expert k: $ \\sum_{t \\in I } \\hat{ \\ell}_t^T v(t,k) -  \\hat{\\ell}_t^T x^* $ in the main text instead of the appendix. Then it will become apparent why the quantity in Step 3 is needed, because `the regret of the Meta-learner` does not justify the quantity of interests in step 3.\n\nAppendix B: pg15  first line. $\\mathcal{A}_1$ by definition in Algorithm 1 line 3 optimizes interval with length of $2$. The index needs to be fixed.\n\nAppendix C: paragraph starting after the first equation, 'Algorithm 3 is feasible'"
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Reviewer_EXm4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6783/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698297969662,
        "cdate": 1698297969662,
        "tmdate": 1699636783343,
        "mdate": 1699636783343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i8zzegD19Q",
        "forum": "AY9KyTGcnk",
        "replyto": "AY9KyTGcnk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_gNKg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_gNKg"
        ],
        "content": {
            "summary": {
                "value": "This paper studies strongly adaptive online learning with bandits feedback.\nInstead of making algorithms close to optimal over the whole time periods $1,\\dots, T$, strongly adaptive algorithms ensures that the performance on every time interval is close to optimal.\nThis paper achieves $\\mathcal{O}(\\sqrt{n |I|})$ adaptive regret for multi armed bandits setting.\nEspecially, the proposed algorithms only require $2$ queries per round, which improves previous results requiring $O(\\log T)$ queries.\nThe authors also extend their results to bandit convex optimization setting, and conduct experiments to show empirically the advantage of their algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is solid. Instead of reducing the number of base algorithms, the authors try to reduce the total number of interactions between the base algorithms and the environment. This setting makes sense when the cost of running base algorithms is less than the cost of interacting with the environment.\n- Experimental results demonstrate the advantage of the design and show a remarkable improvement over the trivial EXP3 algorithm with uniform exploration.\n- The paper is well writen. The proof in the appendix is well organized and mainly correct."
            },
            "weaknesses": {
                "value": "The main weaknesses is that there is essentially no difference between $2$ queries and $O(log T)$ queries: the high level idea of bounding the base learners regret is to use query $Uniform(A_1,\\dots, A_B)$ instead of using $A_1,\\dots, A_B$ independently.\nGiven that $B=\\log T$, this method will at most scale the regret by O(log T) (as the results indeed show the regret scaling in this order), making it a trivial idea.\nBoth of $2$ queries and $O(log T)$ queries separate exploration and exploitation, which goes against the original intention of the bandits setting.\n\nBesides, the authors should discuss the tradeoff of $O(log T)$ terms between regret and number of queries.\nNote that in this problem the order of $O(log T)$ is not trivial: the number of base algorithms is only $O(log T)$.\nBased on the current results, the cost of reducing the number of queries by $O(log T)$ times is an increase in regret by $O(log T)$ times (possibly more, see the Questions parts below).\nThis should be well specified in the paper."
            },
            "questions": {
                "value": "In the proof of Theorem 1, the author uses inequality $log(1+x)\\ge x-x^2$ for $x\\ge-1/2 (page 15).\nIs there any proof to show $\\eta_k \\widetilde r_t(k)\\ge -1/2$ for every $t$ and $k$?\nIntuitively, $\\widetilde r_t(k)\\$ could be of order $v(t,k)_i/P(t)_i = O(log T)$, which implies that $\\eta_k$ should be scaled $O(1/ log T)$.\nThis may leads to another $O(log T)$ term in the regret."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "theory paper. No ethics converns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6783/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698394530097,
        "cdate": 1698394530097,
        "tmdate": 1699636783206,
        "mdate": 1699636783206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FY8M5zzJnv",
        "forum": "AY9KyTGcnk",
        "replyto": "AY9KyTGcnk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_kZqs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_kZqs"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that strongly adaptive regret is possible in bandit setting if one allows 2 queries in the MAB setting and 3 queries in the BCO setting. The regret is measured wrt only one of the queried points. In the MAB setting, the second query point is used as a free ticket for exploring the base learners: free ticket because, the authors don't care about the loss suffered by the exploration query."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors uncover a previously unknown  phenomenon in the bandit setting.\n\n- The presentation is clear.\n\n- Experiments are conducted to validate theory."
            },
            "weaknesses": {
                "value": "- The main weakness (and hence the low score)  is due to a confusion I have regarding the paper. In the middle of page 15 in the supplementary material, the authors state that the weights are positive to get the inequality $\\tilde W_t \\ge \\tilde w_t(k)$. Is the positivity proved somewhere in the paper? If not, can you provide the arguments for positivity of the weights?\n\nI think this is a really interesting paper and I would be happy to recommend acceptance if the authors can clear this confusion.\n\n- If we care about the average loss of both query points as in the multi-point feedback model of Agarwal et al 2010, does the pessimistic lowerbound on strongly adaptive regret still hold? A discussion on this can be helpful to readers. Also a discussion on the practical applications where your feedback model can be inadequate helps to understand the limitations of the work.\n\nOther comments:\n- In section 1.1, the authors say that their algorithm is run over black-box base bandit learners. Then in the description of Algorithm 1, they take the base learners to be EXP3, essentially not viewing the base learners as a black-box. This seems self-contradictory.\n\n- Line 13, algorithm 1, unclear what $2^k | t+1$ means. I assumed that these are the inactive base learners\n\n- Line 3 of algorithm 2, $w_{t}$ must be $x_{t}$"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Reviewer_kZqs",
                    "ICLR.cc/2024/Conference/Submission6783/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6783/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698456460187,
        "cdate": 1698456460187,
        "tmdate": 1700591758443,
        "mdate": 1700591758443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nurwJ5w0rl",
        "forum": "AY9KyTGcnk",
        "replyto": "AY9KyTGcnk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_wKMs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6783/Reviewer_wKMs"
        ],
        "content": {
            "summary": {
                "value": "The authors show strongly adaptive regret bounds for the algorithmic problems of prediction with experts and online convex optimization in a limited information setting. They specifically consider two problem settings:\n\n1. A generalization of adversarial MABs (decision space is $\\mathcal{K} = [n]$ set of arms) where, at each step $t \\in [T]$, the learning algorithm chooses an arm $x_t \\in [n]$ and suffers loss $\\ell_t(x_t)$, but also is allowed to query the losses for an additional set of arms $X_t \\subset [n]$ with $|X_t| \\leq N$. $x_t$ and  $X_t$ are chosen in parallel, and the adversary chooses the losses after seeing $x_t, X_t$. So the learner gets the $(N+1)$-dim loss vector $[\\ell_t(x) : x \\in X_t \\cup \\\\{x_t\\\\}]$ while suffering only the loss $\\ell_t(x_t)$.\n2. A generalization of Bandit Convex Optimization (BCO) where the regret is against points from a well-conditioned convex decision space $\\mathcal{K} \\subset \\mathbb{R}^d$, where the learner again chooses $x_t \\in \\mathcal{K}$ and suffers the corresponding loss $\\ell_t(x_t)$, but receives the loss values $[\\ell_t(x) : x \\in X_t \\cup \\\\{x_t\\\\}]$ for some set $X_t$ with $|X_t| \\leq N$.\n\nIn both settings, they bound the SA regret: $\\text{SA-Regret}(\\mathcal{A}, I) = \\max_{[r,s] : s - r = I} \\left[\\sum_{t=r}^{s} \\ell_t(x_t) - \\min_{x \\in \\mathcal{K}} \\sum_{t=r}^{s} \\ell_t(x)\\right]$\n\nTheir query-feedback model is very similar to multi-point bandit feedback (Agarwal et al, 2010), but with the crucial difference that the learner suffers loss $\\ell_t(x_t)$ for the chosen arm rather than the average of the losses for all the queried arms.\n\nThey then show two main theoretical results:\n1. A $2$-query algorithm ($N = 1$) for MABs with SA regret bound of $O(\\sqrt{n I \\log n} \\cdot \\log^{1.5} T) = \\tilde{O}(\\sqrt{nI})$.\n2. A $3$-query algorithm ($N = 2$) for BCO with SA regret bound of $O(d G D \\sqrt{I} \\log^2 T) = \\tilde{O}(\\sqrt{I})$, where $\\ell_t$s are $G$-Lipschitz and $\\mathcal{K}$ is contained in the radius $D$ $\\ell_2$-ball centred at the origin.\n\nThey also have basic experimental results with the implementations of the algorithm in synthetic, changing environments. \n\nReferences\n----------------\n(Agarwal et al, 2010) Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback.\n(Daniely et al 2015) Strongly Adaptive Online Learning"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* For MABs, the query complexity per round (2 queries) is optimal for getting sublinear adaptive regret (lower bound from (Daniely 2015)). It is also a significant improvement on earlier work (Lu and Hazan 2023), from $O(\\log \\log T)$ to $2$. In the regret bound, the dependence on $I$ is tight, whereas the dependence on $n$ is, I believe, tight up to a $\\sqrt{\\log n}$ factor. The query complexity is exactly optimal (for sublinear SA regret), and a significant improvement on earlier work.\n* For BCO, both the query complexity and the regret together give substantial improvements on earlier work (query complexity improved from $\\log T$ to $3$ for the existing $\\tilde{O}(\\sqrt{T})$-regret full-information algorithms, substantial improvement in the dependence of regret on $\\sqrt{T}$ (weak) versus $\\sqrt{I}$ (strong) when compared to the $2$-query adaptive regret algorithm of (Zhao et al 2021)).\n* The algorithms innovatively combine existing techniques --- such as the geometric interval sets used by (Hazan and Seshadhri, 2009) and later work, as well as the SAOL-meta-learning approach used by (Daniely et al, 2015) --- with the Exp3 framework."
            },
            "weaknesses": {
                "value": "* The experimental results are a bit weak. For MABs, the $N (= 30)$ and $T (=4096)$ values are too small, and each algorithm is run only $5$ times to average out the reward curves. The experiments also compare their algorithm (StABL) with handicapped variants (StABL Naive and StABL Single Scale, which do illustrate certain requirements on the structure of StABL) as well as Exp3 (which is of course expected to perform badly in a changing environment). It would be interesting to see the practical tradeoff in the reward curve vs. computational efficiency, for say StABL and Full-EFLH from (Lu and Hazan 2023)."
            },
            "questions": {
                "value": "(i) In page 3, paragraph 6, I believe that the \"special case when $\\mathcal{K}$ is a simplex\" is a bit stronger than MAB (since it would effectively consider all convex combinations of arms).\n\n(ii) In the description/pseudocode of Algorithm I, calling Algorithm 2 as a subroutine seems slightly ambiguous, since Algorithm 2 is written as a full learning loop (whereas the actual operation seems to be an update $v(t+1,k) \\leftarrow v(t,k)$ for each $k$).\n\n(iii) In page 6, end of section 3, \"can be extended to any interval .... by Cauchy-Schwarz\" seems very opaque. I would suggest at least mentioning that each arbitrary interval can be decomposed into a disjoint union of $O(\\log T)$ geometric intervals as in (Daniely, 2015).\n\n(iv) Including the code as supplementary material/link in the final version would enhance the value of having the experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6783/Reviewer_wKMs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6783/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842679277,
        "cdate": 1698842679277,
        "tmdate": 1699636782958,
        "mdate": 1699636782958,
        "license": "CC BY 4.0",
        "version": 2
    }
]