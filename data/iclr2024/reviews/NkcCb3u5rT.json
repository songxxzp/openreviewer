[
    {
        "id": "LW9tAQGpMS",
        "forum": "NkcCb3u5rT",
        "replyto": "NkcCb3u5rT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission888/Reviewer_atNd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission888/Reviewer_atNd"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel approach called RRESCUE for optimizing LLMs using ranking metrics. It uses a ranking loss to optimize the model. For the ranking part, instead of a traditional full ordering of responses, the approach advocates for a partial ordering, which is more robust and less sensitive to noise. The authors propose using human labelers, heuristic functions, or model distillation to acquire the suitable order of candidate responses. The system's improved contextual understanding is tested on various benchmarks, including a new multi-document question answering dataset. Ablation studies are conducted to understand key factors such as gathering candidate responses, determining their order, and balancing supervised fine-tuning with ranking metrics. The approach shows promise in enhancing LLMs' contextual understanding through response ranking."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tUsing a ranking-based method to effectively improve the performance of LLM\u2019s responses.\n\n2.\tUsing partial ranking to make the ranking process more robust, less sensitive to noise, and can be acquired through human labelers.\n\n3.\tThe experimental analysis for the hyperparameters is comprehensive."
            },
            "weaknesses": {
                "value": "1. The main contribution part should be proofread more as the current contribution is somewhat unclear to me. To me, the contribution of this paper is as follows: a) inspired by BRIO, the paper uses a contrastive loss to optimize the model in order to produce responses with improved rationales; b) the paper introduces the use of partial ranking loss to enhance the robustness and reduce sensitivity in the ranking process. However, when reading only the abstract and the introduction part, it is difficult for readers (including myself) to fully grasp it.\n\n2. Some claims are overstated:\n\n    a. Although the paper repeatedly claims that the proposed model is more efficient and simpler than reward-based models, there is no comparison made between the proposed model and the RLHF model.\n\n    b. Similarly, although the paper claims that the ranking method is more robust and less sensitive, there are no experiments conducted to verify this point.\n\n3. The presentation is not satisfactory:\n\n    a. In Figure 1, the figure does not include the other experiment in this paper, namely the multi-document QA, which is quite misleading and makes it harder to understand the settings of the QA task.\n\n    b. In Figure 3, the label 'Full Order Provided by GPT' could be placed in the caption of the figure.\n\n4. The method lacks generalization ability and seems to be a method being stuck in task-specific scenarios."
            },
            "questions": {
                "value": "1.\tAfter the RRESCUE training process, is the model still running in a task-agnostic way or it can only conduct the multi-document QA and textual entailment task? Is there any generalization ability for this method?\n\n2.\tIs the ranking algorithm in the retrieval process (for multi-document question answering) using your proposed PO method?\n\n3.\tIs there an oracle baseline to verify that better rationales do provide better performance for the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission888/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission888/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission888/Reviewer_atNd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770106693,
        "cdate": 1698770106693,
        "tmdate": 1699636015207,
        "mdate": 1699636015207,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FE36JUgIon",
        "forum": "NkcCb3u5rT",
        "replyto": "NkcCb3u5rT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission888/Reviewer_KoAg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission888/Reviewer_KoAg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method that employs partial ordering to train a Large Language Model (LLM) ranker. The sources of partial order can be human-prioritized, label-prioritized, or a hybrid of human and label priorities. With a focus on comprehension tasks, the proposed ranker demonstrates improved answer accuracy in datasets like SNLI and multi-document question-answer scenarios."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors present several cost-effective methods to derive preference data, specifically in the form of partial order ranking, for comprehension tasks. A prerequisite for these methods is the availability of human labels. \n- Experimental results on the SNLI dataset and multi-document question answering indicate that utilizing partial order can enhance the accuracy of final answers."
            },
            "weaknesses": {
                "value": "- Paper Presentation & Clarity:\n  - The presentation of the paper is not straightforward, making it difficult for me to fully understand its content.\n  - The manner in which the paper's motivation, novelty, and contribution are presented is not clear. I'd appreciate it if the authors could clarify these aspects in their rebuttal.\n  - I found the abstract confusing. A significant portion discusses the context, but the main content of the proposed ranker doesn't seem to emphasize or study context in any specific way.\n\n- Novelty and Soundness of Proposed Ranker:\n  - The uniqueness of the proposed ranker technique is ambiguous. Specifically, the combined loss function bears similarities to existing approaches like chain-of-hindsight and DPO. I'd suggest the authors elaborate on what differentiates their proposed loss or provide relevant citations to acknowledge previous work.\n  - The absence of direct comparisons with established ranker baselines is a significant weakness. It's crucial to benchmark against recognized high-quality reward/ranker models. The models used for comparison in the paper, specifically similarity and gpt-3.5-turbo, neither qualify as top-tier reward models nor as rankers developed using preference data.\n  - Given these issues, particularly the lack of appropriate baselines, I'm uncertain about the robustness and efficacy of the proposed ranker."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789131247,
        "cdate": 1698789131247,
        "tmdate": 1699636015106,
        "mdate": 1699636015106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "113wnQvJZ4",
        "forum": "NkcCb3u5rT",
        "replyto": "NkcCb3u5rT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission888/Reviewer_Em3N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission888/Reviewer_Em3N"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces partial order rankings to learn higher preference on generating valid rationales that supports the predictions in natural understanding tasks. The concept of training the LLMs with an additional preference loss is similar to RLHF; however, this paper focuses on improving task-specific predictions while generating valid rationales with a limited annotations, whereas RLHF requires full order ranking from human annotators, which is expensive.   \n\nThe authors proposed three partial order ranking of the preference that do not require human involvement: human-prioritized, label-prioritized, and human-label-hybrid. These orderings rank human-generated above the model-generated rationales, correct labels above incorrect labels, and hierarchically considering two prioritizations, respectively.\n\nTo evaluate the proposed rankings, the authors conducted experiments on the e-SNLI and multi-document question-answering datasets. For the e-SNLI dataset, each prompt is provided with one human-generated rationale along with three rationales from Llama 2-7B and one rationale from GPT-3.5-Turbo, for a total of five candidate rationales used to predict the natural language inference task. For the question-answering task, each question is provided with five generated rationales, where one is generated from the reference document, and the others are generated from four random documents. In both datasets, the results indicate that augmenting the partial order ranking of valid and correct rationales into the supervised loss supports natural language inference and question-answering performance compared to using supervised loss alone."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The motivation of the paper is clear. The paper aims to improve natural language understanding tasks by learning preference of the data without collecting human preferences."
            },
            "weaknesses": {
                "value": "- As the one of the distinctive differences between prior works  (mentioned in Section 2. Model Distillation), the authors argue that is uses multiple LLMs to generate rationales. However, for the natural language inference, it uses three rationales from Llama 2 7-B and one from GPT-3.5-Tubo and for the question answering, it uses five rationales from Llama2 7-B. The experiment results lack in supporting the benefit of have multiple LLMs for model distillation.\n- This approach seems to only work with limited training data. As  mentioned in Table 1 caption, the supervised fine-tuning performance better when there are more training data, which reduces the contribution of the work. think even with more training dataset, this approach should be effective.\n    - Exposing more examples to the models enables them to learn something from the generated text related to the prompt/context, which could be correct or incorrect. Recent instruction generation datasets [1,2] show that even the noise and incorrect instruction, input, and output sets have meaningful signals.\n- Training the reward model with the binary/pairwise ranking loss introduced in [3] offers flexibility as it does not necessitate full ordering of the ranking and allows to learn the robust representation of preferences as the number of training instances increases by making full ranking into pairwise set.\n    - In Section 3. Our Approach - Organizing LLM Responses\n        - In the first paragraph, \u201cmodel outputs from best to worst train a reward model (Ouyang et al, 2022)\u201d has wrong citation. Instead of [3], [4] or [5] should be used. When explaining the Eq (4), it has been correctly cited [5].\n    - The binary/pairwise ranking loss is the key component that actually contributes to the flexibility and robustness of the reward model, which will be referred to as LLM in this paper. The proposed partial order ranking is a form of data that can be adapted to the concept of binary/pairwise ranking loss.\n    - It is not evident from the experiments whether the proposed ranking offers greater flexibility and robustness.\n\n\n[1] Unnatural Instructions:Tuning Language Models with (Almost) No Human Labor, ACL 23\\\n[2] Self-Instruct: Aligning Language Models with Self-Generated Instructions, ACL 23\\\n[3] Training language models to follow instructions with human feedback, NeurIPS 22\\\n[4] Deep Reinforcement Learning from Human Preferences, NeurIPS 17\\\n[5] Fine-Tuning Language Models from Human Preferences, arXiv 19"
            },
            "questions": {
                "value": "- Full Ordering Similarity & GPT-3.5-Turbo\n    - Are they trained with Eq (4) or binary/pairwise ranking loss?\n- Section 4. Collecting LLM Responses\n    - Why is Llama 2-7B used for generating rationales? I do understand using Llama 2-7B for fine-tuning the task as mentioned in Section 5.1. Base Language Model. Since the inference requires less GPU memory compared to training the model, it seems possible to generate rationales with 4 A100 GPUs. Was there no difference between generating rationales with different sizes of Llama 2 (e.g., 13B and 70B)?\n    - Could you show some generated examples both experiments (e-SNLI and Multi-document Question Answering)?\n    - Did you use Llama 2-7B Chat for generating rationale or Llama 2 7B for all cases (rationale generation & task fine-tuning)? (In Section 5.4 Discussions Central tendency bias, the reason of the Llama 2-7B is generating natural label is due to training human preference, which tend to favors helpful and safe outputs.\n    - What are the category distribution of the model-generated rationales? (entail, contradict and neutral for e-SNLI) Does the distribution change after the flipping the responses?\n        - For the question-answering task, the categories appear to be two labels: 'incorrect' and 'correct,' which don't really need to be predicted by the rationale-generating model because we already know whether the rationales are generated from the reference document or not. Please correct me if my interpretation is wrong.\n        - In contrast to the question answering, how do you determine whether the model-generated rationales and labels are correct or incorrect?\n- Section 4. Collecting LLM Responses - Flipping Responses\n    - how many model-generated rationales are inverted? did you invert all of the model-generated rationales in training data?\n    - it mentions that new labels are predicted with GPT-3.5-Turbo due the cost-effectiveness. I do agree that GPT-3.5 is cheaper than GPT-4. But cannot it be just inference along with inverting rationales like how the rationales are formed?\n- e-SNLI\n    - In Figure 4 (Right), what model is used for Human Reference? Is it suppose to be Similarity?\n    - In Figure 4 (Left), the model with highest accuracy for each method is selected; however, win rate seems to be pretty high than expected, which also encounters correctness of the prediction. There performance differences are not large. Could you show win, tie, and loose examples?\n    - How many participants were there for the human evaluation? Who participated?\n- Multi-document Question Answering\n    - For this experiment, 1k questions and answers are sampled from the dataset. Did you sample questions and answers by random sampling or balanced sampling across all positions?\n    - Could you report the reference position distribution of your training and test datasets?\n    - Clarification on how Llama2-7B base and the label-prioritized are trained.\n        - For e-SNLI, it was clear on how supervised fine-tuning model was trained since it had human explanation and label for each prompt. However, it is not clear for this task.\n            - Is the Llama2-7B simply trained to output short answer given the documents and questions?\n            - \u201cwithout resorting to supervised fine-tuning\u201d confuses me how the label-prioritized model is being trained.\n- Presentation\n    - The current presentation of Tables and Figures are hard to follow.\n        - Tables and Figures are located different page from the text. I recommend placing Table 1 and Figure 4 in the same page with Section 5.2. Table 2 should be with Section 5.3 and Figure 6 should be with Section 5.4.\n    - Figure 4 (left) could be integrated into Table 1 adding an additional column presenting average and variance.\n    - Figure 5 and Section 5.1. Batch size could be mentioned in the Appendix/Supplement rather than placing it in the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871181572,
        "cdate": 1698871181572,
        "tmdate": 1699636015017,
        "mdate": 1699636015017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wnWGUpfcjo",
        "forum": "NkcCb3u5rT",
        "replyto": "NkcCb3u5rT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission888/Reviewer_K8mh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission888/Reviewer_K8mh"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to enhance reasoning of LLMs. The authors propose a multi-task training formulation that optimizes a ranking loss in addition to the original SFT loss. For the ranking loss part, the authors explore both full order and partial order approaches, where the partial order ones do not need very careful human annotation and demonstrate effective performance in the experiments. Empirical results are presented on the e-SNI dataset and a recent multi-doc QA dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is interesting and well-motivated. Using ranking information in the SFT stage rather than for reward modelling is an interesting direction to explore.  \n2. The authors study the partial order preference data and demonstrate its effectiveness, which should be inspireful since it is much easier to obtain the full order ones."
            },
            "weaknesses": {
                "value": "1. The evaluation in this paper is very weak. e-SNLI is a relatively simple task and rarely used to assess LLMs\u2019 response generation or reasoning. The other multi-doc QA dataset is also not commonly used and its answers are also short-form from the example in Figure 2. I cannot tell why the authors choose it over many other popular LLM datasets, such as MMLU, GSM8K (which also has annotated explanations), MT-Bench, etc.  \n2. The paper emphasizes \u201cenhancing reasoning\u201d, yet the paper does not study the standard reasoning datasets such as GSM8K and BBH, and instead uses an NLI dataset e-SNLI and an QA dataset. I am not sure how the chosen datasets can reflect reasoning abilities well.  \n3. From Table 1, the performance of the proposed approach is not very effective compared to the SFT baseline except for the 0.4% training data column. I feel performance on more serious datasets is necessary.  \n4. Figure 4 left, SFT should be a baseline as well for human evaluation.  \n5. Human evaluation details are missing \u2013 have the authors tried assessing human consistency on the task? Or the have the authors tried any means to verify the human evaluation results?  \n6. The paper\u2019s writing could be improved \u2013 the paper starts talking about results in Page 7, yet the results figures and tables are scattered across the entire paper, and in most of the cases the text refers to figures that are pages away. It makes the results sections difficult to read, and I think these figures and tables could be orgnaized in a better way."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission888/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698931661060,
        "cdate": 1698931661060,
        "tmdate": 1699636014942,
        "mdate": 1699636014942,
        "license": "CC BY 4.0",
        "version": 2
    }
]