[
    {
        "id": "GVFTDil4E1",
        "forum": "M36m3buBVD",
        "replyto": "M36m3buBVD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission772/Reviewer_zHY5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission772/Reviewer_zHY5"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a prompt tuning method called Long-term Spatial Prompt Tuning (LSPT). LSPT leverages LSTM to address the long-term forgetting issue that occurs in the standard VPT method. Additionally, LSPT introduces class-aware spatial prompt coding to integrate global visual representation into the prompts. Experiments conducted on image classification demonstrate the effectiveness of LSPT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is overall well-written and easy to understand.\n2. The proposed LSPT consistently outperforms VPT and GaPT baselines, and ablation studies are thoroughly conducted."
            },
            "weaknesses": {
                "value": "1. The working reason of CSPC is not entirely clear. The author should provide a more detailed explanation of the fundamental distinctions among the three types of sources used to construct the input prompts mentioned in Sec 3.2 and elaborate on why \"adding the average embedding of patch tokens to the output prompts\" leads to performance improvements.\n\n2. About generalizability. The authors have exclusively conducted experiments on the standard ViT architecture under self-supervised pretraining strategies. This raises questions about how LSPT performs when applied to a supervised pretrained model and more advanced ViT architectures. Additionally, comparing LSPT with more advanced parameter-efficient fine-tuning methods [a] [b] could further substantiate its effectiveness.\n\n3. The training and inference processes may be more complex compared to VPT or other parameter-efficient methods. It would be beneficial to include metrics such as FLOPs or inference times to assess the efficiency of the proposed method.\n\n4. The authors do not list the limitations and broader impact of this work. Some potential solutions should also be given.\n\n[a] Revisiting the parameter efficiency of adapters from the perspective of precision redundancy. ICCV, 2023.\n[b] Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR, 2023."
            },
            "questions": {
                "value": "1. If I understand correctly, the authors propose that VPT may be susceptible to the so-called long-term forgetting problem. In response, LSPT solves this problem by integrating information from prompt tokens of different layers using an LSTM.  This raises the question: what is the connection between long-term forgetting problem and performance improvement? If this question can be addressed, would it be more effective to employ an attention-based architecture instead of an LSTM?\n\n2. [c] seems quite closely related to LSPT . This concurrent work is worthy to cite and discuss.\n\n[c] Learning Expressive Prompting With Residuals for Vision Transformers. ICML 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698564150507,
        "cdate": 1698564150507,
        "tmdate": 1699636004645,
        "mdate": 1699636004645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NiSliWOlvo",
        "forum": "M36m3buBVD",
        "replyto": "M36m3buBVD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission772/Reviewer_HDEH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission772/Reviewer_HDEH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents LSPT for prompt tuning of self-supervised pre-trained ViTs. The authors highlighted the importance of long-range dependency on successive blocks' prompts and presented two techniques: class-aware spatial prompt coding and long-term prompt coding. The proposed method achieves better performance than existing techniques on popular benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well-written and easy to understand. \n\n+ The paper successfully includes closely related works and compares with them."
            },
            "weaknesses": {
                "value": "- My major concern is the motivation. Why preserving the spatial information is crucial? For classification, proper abstraction can be more beneficial, which discards unnecessary spatial details or provides not interpretable attention maps. \n\n- In addition, I think the long-term modeling of prompts is not well-motivated. Why the long-term dependency is necessary? Why the long-term modeling is helpful for the classification? The authors have tried to explain this with the human brain, but it seems too ambiguous. No technical evidence was given.\n\n- The class-aware spatial prompt coding is not motivated, too. What is the role of the mean of patch tokens? Why they are added to the prompt tokens? \n\n- The long-term modeling ability is not supported by evidence. The visualization of attention maps do not represent the long-term modeling ability, and in addition, they can be cherry-picked. The authors should find more direct evaluation ways to show the long-term modeling ability, especially with quantitative measures. \n\n- The proposed method achieves better performance than previous SOTAs, but it uses more learnable parameters. For FGVC, 4 times more parameters were used, and for VTAB-1K, 5 times more parameters were used. For a fair comparison, the proposed method should decrease the number of introduced prompts.\n\n- In addition, the proposed method may require more inference time due to LSTM module. Please compare the inference time with other VPT-based methods."
            },
            "questions": {
                "value": "- For long-term modeling, why LSTM is used? The auto-aggressive Transformer layer can also be applied?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727436519,
        "cdate": 1698727436519,
        "tmdate": 1699636004567,
        "mdate": 1699636004567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JHhiZntEXT",
        "forum": "M36m3buBVD",
        "replyto": "M36m3buBVD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission772/Reviewer_isM7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission772/Reviewer_isM7"
        ],
        "content": {
            "summary": {
                "value": "Traditional Visual Prompt Tuning (VPT) relies on short-range learnable prompts from a model's immediate previous block, overlooking the potential of leveraging long-range interactions. This paper introduces LSPT to address this gap by incorporating long-range gated prompts, akin to temporal coding in the human brain, to prevent the loss of earlier learned parameters. Additionally, it employs patch tokens as spatial coding to continuously gather class-specific features, thus improving the model\u2019s capability in recognizing visual categories. The effectiveness of LSPT over conventional VPT methods was validated through extensive tests on 5 Fine-Grained Visual Categorization (FGVC) and 19 VTAB-1K benchmarks, where LSPT demonstrated remarkable performance improvements, setting new standards in the field of visual representation learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The performance of LSPT is outstanding. The ablation studies highlight the effectiveness of both CSPC and LPC, while the visualizations further underscore this efficacy.\n\n2. The authors address the problem of catastrophic forgetting in Visual Prompt Tuning, which is an insightful contribution.\n\n3. This paper is well organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. This paper appears to exhibit a substantial degree of similarity to the methodologies presented in the GaPT paper (Yoo et al., 2023). For instance, section 3.1.2 seems to be a rephrased version of Section 2, 'Visual Prompt Tuning,' from GaPT. Equations (1, 2, 3) in this paper are identical to equations (4, 5) in GaPT. Furthermore, Table (1, 2) and the corresponding experimental results are exactly the same as those in GaPT. The overall structure of this paper closely mirrors that of GaPT. It is highly irregular and unacceptable to replicate aspects of a research paper to such an extent.\n\n2. Including a delineated algorithmic framework would significantly improve the replicability of the proposed LSPT method.\n\n3. The connection between the two proposed modules, CSPC and LPC, is weak. There is no strong motivation presented to justify the inclusion of CSPC, especially since the title of the paper pertains solely to LPC. Nonetheless, the authors introduce CSPC first, which suggests a greater emphasis on the importance of CSPC.\n\n4. More visual prompt tuning papers should be included in the related works. There are only two papers introduced currently."
            },
            "questions": {
                "value": "Besides the questions included in the weaknesses, some other questions are listed below. \n\n1.The scope of the proposed LSPT within the realm of visual prompt tuning is confined to SSL-ViTs. This limitation constrains the real-world applicability of the proposed LSPT method. Additionally, are there any other baselines within this specific experimental context?\n\n2. The visual representation of the pipeline in Figure 2 could be aesthetically improved. The rationale for using a non-standard rectangle to depict the LSTM is unclear. Moreover, the relationship between CSPC and LPC is not adequately illustrated. Implicitly, Figure 2 suggests that LPC serves as an incremental enhancement to CSPC, solely to boost performance.\n\n3. Have the authors replicated the baseline studies themselves, or have they merely cited the results from GaPT? It would be advantageous to detail the variations encountered during the reproduction of baseline and proposed methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698971852594,
        "cdate": 1698971852594,
        "tmdate": 1699636004485,
        "mdate": 1699636004485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CsDN9M4hil",
        "forum": "M36m3buBVD",
        "replyto": "M36m3buBVD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission772/Reviewer_7rGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission772/Reviewer_7rGz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method called Long-term Spatial Prompt Tuning (LSPT) for adapting pre-trained Vision Transformers (ViTs) to downstream visual tasks using learnable prompt tokens. The key contributions are:\n\n- LSPT incorporates long-term gated prompts as a temporal coding layer to mitigate forgetting of parameters learned from earlier ViT blocks. This helps address the issue of \"temporal forgetting\" in previous prompt tuning methods. \n\n- LSPT introduces patch tokens with spatial prompt coding to accumulate class-specific features across blocks, addressing the issue of \"spatial forgetting\". \n\n- Extensive experiments on 5 FGVC and 19 VTAB benchmarks show LSPT achieves new state-of-the-art results compared to prior prompt tuning methods like VPT and GaPT.\n\n- Analysis shows LSPT's temporal and spatial coding help alleviate forgetting issues in attention maps and prompt-patch similarity compared to prior methods.\n\nIn summary, LSPT advances visual prompt tuning by integrating ideas from neuroscience to address forgetting issues, leading to better adaptation and transfer learning performance. The temporal and spatial coding in LSPT are novel techniques for prompt tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Here is a critical assessment of the strengths of this paper:\n\n**Originality**: The ideas of incorporating temporal and spatial coding for prompt tuning are novel and not explored before in prior VPT methods. The use of long-term gated prompts and patch tokens as spatial prompts are creative ways to address the issues of forgetting in VPT. Applying concepts from neuroscience like temporal and spatial coding to transformer prompt tuning is an original combination.\n\n**Quality**: The paper is technically strong, with a clearly explained intuition and motivation behind LSPT's design. The method is evaluated thoroughly on multiple datasets, convincingly demonstrating its effectiveness over baselines. The results are state-of-the-art, showing the quality of the approach.\n\n**Clarity**: The paper is well-written and easy to follow. The description of the temporal and spatial forgetting issues in VPT provides good motivation. The Class-aware Spatial Prompt Coding and Long-term Prompt Coding modules are explained clearly. The ablation studies isolate their contributions. \n\n**Significance**: LSPT makes a significant advance in visual prompt tuning, an important area for adapting pre-trained vision models. The ideas could inspire more work on alleviating forgetting in prompt tuning and transfer learning. Outperforming prior SOTA like GaPT demonstrates the significance of the improvements. The gains are substantial across multiple benchmarks.\n\nIn summary, I found this paper to exhibit strong originality in applying neuroscience-inspired concepts to VPT, technically sound modeling and evaluation, with clearly presented ideas that significantly advance prompt tuning research. The novelty of temporal and spatial coding for prompts is a compelling contribution."
            },
            "weaknesses": {
                "value": "* While the proposed modules make intuitive sense, the explanations lack quantitative analysis or theorems to rigorously justify the designs. Some ablation studies analyze contributions but more analysis connecting the methods to mitigating forgetting could strengthen the approach.\n* The spatial prompt coding uses a simple patch token averaging, which seems like a heuristic. More sophisticated ways to accumulate spatial/positional information may exist. This component could likely be improved.\n* The long-term prompt coding relies on a single LSTM layer. Ablations could explore using multiple LSTM layers or comparing to other sequential modeling approaches like GRUs.\n* The computational overhead and memory requirements of LSPT are not analyzed. This could be important for deployments.\n* There is no investigation of how the approach may fair for other vision tasks beyond classification like detection and segmentation."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699247975586,
        "cdate": 1699247975586,
        "tmdate": 1699636004423,
        "mdate": 1699636004423,
        "license": "CC BY 4.0",
        "version": 2
    }
]