[
    {
        "id": "9j2AzBWAmS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE"
        ],
        "forum": "tuzTN0eIO5",
        "replyto": "tuzTN0eIO5",
        "content": {
            "summary": {
                "value": "* The author proposed a new schedule in Pipeline Parallelism (PP) to reduce pipeline bubbles to close to zero\n* The key idea is instead of the 1F1B schedule of earlier work, splitting B to two stages (aka activation gradient and weight gradient) and a schedule like 1F1B1W can further reduce bubbles of 1F1B\n* The intuition behind it is only Forward calcluation and activation gradient calculation have a pipeline/stage dependency, while weight gradient calculation are not, so activation gradient calcluation should be eagerly scheduled while weight gradient calculation can be rescheduled to fill/balance pipeline bubbles\n* The authors have proposed two schedule algorithms to adapt for real-world workloads when F/B/W can have different runtime."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The author made a key observation that the backprop stage in PP can be split into two parts and scheduled in a finer granularity, i.e., `any pipeline (or producer/consumer) independent workload can be rescheduled to balance/fill pipeline bubbles`, which is not only benifitical to train NN with PP, but should hold generally true for any PP workloads\n\n* The author didn't limit themselves to just a simple TF = TB  = TW assumption but also proposed two optimization implementations to optimize the schedule for potentially unbalanced TF/TB/TW\n\n* The authors have conducted representative experiements and showed that their methods can outperform earlier methods by up to 20% and more importantly close to the upperbound (zero bubble) of PP\n\n* The presentation/feagures are easy to follow and illustrative"
            },
            "weaknesses": {
                "value": "* While I agree the authors have discovered a valuable new angle optimizing PP and indeed bring the bubble rate close to zero, I think it is still a bit overselling to claim the method achives zero bubble PP, given:\n  1. The method still require a large enough number of microbatches, e.g., m >= 2p to achive an amortized zero bubble PP, how does it perform under m < p is rarely discussed. Experiments (at least a roofline analysis) with m < p can make the methods more sound\n  2. There are other methods that can achieve zero bubble PP like PipeDream and PipeMare. These are more strictly flushless (thus bubble-free) methods though they have altered the math of backprop and poses potential accuracy loss or incurs more weight memory. So it is a bit overclaim that this work is the first to achieve pippeline bubbles. Discussions on these previous related works are also encouraged.\n  3. Therefore \"near zero bubble\" is probably more accurate IMHO, emphasis on \"zero bubble\" actually overshadows the solid intuition mentioned as 1st bullet point in Strengths\n\n* Even though authors proposed two algos to schedule unbalanced TF/TB/TW, the experimetns however are not designed to show how their advantages, e.g., with sentence-size = 1024, TF/TB/TW are almost equal (24h >> 8s in table 1) for all model sizes, experiments or roofline analysis with 8s >> 24h (or any models with sifinicantly different T) could be helpful to evaluate the efficacy of these algos"
            },
            "questions": {
                "value": "1. when authors talk about peak memory I assume they refer to max of peak memory of each worker, rather than the entire pipeline, if it is the latter case even ZB-H1 should have a higher peak memory than 1F1B based on figure 3, can you make this more explicit in the texts?\n\n2. what's a \"local search\" in 3.1?\n\n3. what's the runtime of ILP, how much does it improve over HEURISTIC? can you do an ablation with HEURISTIC algo only?\n\n4. My understad is section 4 BYPASSING OPTIMIZER SYNCHRONIZATIONS only works for INF/NAN, how does it work for grad-norm when each stage need the global reduced norm? If the optimizer depends on partially reduced state how can it provide same numerical result of baselines like 1F1B?\n\n5. can you try to recompute the activation gradient (not activation), e.g., redo B during W to save some peak memory in H2/2P cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7283/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_xbKE",
                    "ICLR.cc/2024/Conference/Submission7283/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697336029473,
        "cdate": 1697336029473,
        "tmdate": 1700276414007,
        "mdate": 1700276414007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X16nB2rALc",
        "forum": "tuzTN0eIO5",
        "replyto": "tuzTN0eIO5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
        ],
        "content": {
            "summary": {
                "value": "Pipeline parallelism is a widely used technique in distributed training. However, the efficiency of pipeline parallelism suffers from pipeline bubbles. In view of this, this paper designs a new strategy to reduce the bubble rates. The idea behind the new pipeline strategy is to split the backward computation into two parts, which could reduce the bubble in the 1F1B strategy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem studied in this paper is very important. With the increase of the model parameters, pipeline parallelism is a popular strategy for training large models. However, the bubble in the 1F1B strategy affects the efficiency of the pipeline strategy.\n2. The idea behind this paper is very easy. It is a good signal, I think the paper develops an easy solution with great performance.\n3. This paper contains a mathematical analysis."
            },
            "weaknesses": {
                "value": "1. The authors miss an important point in the experiment section.\n2. The writing of this paper can be slightly improved.\n3. The performance of the method developed in this paper is limited."
            },
            "questions": {
                "value": "The problem studied in this paper is significant in distributed training and the authors give an excellent solution to improve the existing pipeline strategy. \n\nHowever, the main issue of this paper is that the solution mentioned in this paper is likely to increase the memory usage of pipeline parallelism training. Moreover, the experimental results introduce bubble rate which is not important in the pipeline parallelism training. It is known to all that the efficiency of pipeline parallelism suffers from pipeline bubbles. But the bubble rate is not important. We only care about the throughput and memory usage of the pipeline parallelism strategy. Since the pipeline strategy developed in this paper will increase the memory usage of pipeline parallelism training, large memory usage is likely to decrease the size of the micro-batch. A small micro batch will decrease the efficiency of the pipeline parallelism strategy. The experimental results show the developed algorithm has limited improvement.\n\nThe writing of this paper could be improved. For example, we can use different colors to represent B in Figure 3 and Backward in Figure 2. Moreover, the title of this paper is too large. I do not think the method in this paper is zero bubble."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7283/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7283/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7283/Reviewer_b5Up"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698196391292,
        "cdate": 1698196391292,
        "tmdate": 1700635617682,
        "mdate": 1700635617682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OVttDPdQNE",
        "forum": "tuzTN0eIO5",
        "replyto": "tuzTN0eIO5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_Uec3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_Uec3"
        ],
        "content": {
            "summary": {
                "value": "The manuscript titled \"ZERO BUBBLE PIPELINE PARALLELISM\" presents a novel strategy for achieving pipeline parallelism with the primary goal of eliminating pipeline bubbles. This approach includes the development of an automatic scheduling algorithm, which is then compared with established methods like 1F1B and interleaved 1F1B. The key contributions of the paper are as follows:\n\n1. It introduces a pipeline parallelism strategy designed to eliminate pipeline bubbles, assuming that forward, backward, and weight gradient calculations all take the same amount of time.\n\n2. It presents an automatic scheduling algorithm that consistently outperforms traditional methods, with the highlight being the achievement of a zero bubble rate, indicating optimal computational resource utilization. However, this comes at the cost of nearly doubling memory usage.\n\n3. The paper delves into the relationship between memory constraints and bubble rates, providing insights into the trade-offs between memory requirements and scheduling efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel strategy for pipeline parallelism aimed at completely eliminating pipeline bubbles. Leveraging this strategy, an automatic scheduling algorithm is developed and benchmarked against existing approaches, such as 1F1B and interleaved 1F1B. \n\noriginality: fair. The paper separated the backward loss and weight gradient calculation, and achieved zero bubble pipeline parallelism under the assumption that forward, backward loss, and weight gradient calculation time are identical.\nquality: good. The paper has theoretical calculation and also experimental data to support the zero bubble pipeline parallelism algorithm proposed in this paper.\nclarity: good. \nsignificance: medium. The algorithm can achieve zero bubble pipeline parallelism but with memory usage nearly doubled. This will limit the application of this method for larger LLM given larger models will use larger memory."
            },
            "weaknesses": {
                "value": "The algorithm can achieve zero bubble pipeline parallelism but with memory usage nearly doubled. This will limit the application of this method for larger LLM given larger models will use larger memory."
            },
            "questions": {
                "value": "For Figure 5, sometimes ZB-1p is better than 1F1B-I, and sometime not. Can you explain the reason in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730996549,
        "cdate": 1698730996549,
        "tmdate": 1699636869803,
        "mdate": 1699636869803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ORu62cRJu6",
        "forum": "tuzTN0eIO5",
        "replyto": "tuzTN0eIO5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_hNFq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7283/Reviewer_hNFq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach, called ZeroBubble, for eliminating bubbles in pipeline parallelism in order to training efficiency. There are two key aspects to the proposal: (1) splitting the backward pass into the B and W components to increase scheduling flexibility, and (2) eliminating synchronization of optimizer step with a rollback mechanism to preserve semantics. The paper also presents various analysis to illustrate the memory consumption effects of different ZeroBubble strategies. Overall, the evaluation results show that ZeroBubble provides up 30% throughput improvement over 1F1B schedules for Megatron GPT models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I think splitting the backward pass into B (activation gradient computation) and W (weight gradient computation) as a way of improving scheduling flexibility is a nice touch of creativity. \n\nThe paper includes ample analytical details that foster intuition and understanding of the relevant memory and TFLOPs consideration of pipeline parallelism schedules. \n\nThe authors did a really great job of writing and organizing the paper."
            },
            "weaknesses": {
                "value": "My main concern relates to breaking the synchronization of optimizer step because it complicates the synchronous training semantics, and adoption requires close interaction of rollback and model checkpointing.  Moreover, I think there are few design alternatives that the paper failed to explore or discuss. \n1. Evaluating the performance of synchronous optimizer step as way of understanding the trade-off between the simplicity and throughput. \n2. Scheduling W before B, which is less memory-efficient but allows staggering the optimizer step of the stages while preserving the synchronous semantics. \n\nAnother concern, or perhaps a question is whether the more memory efficient schemes (1F1B* and ZB-1p) be evaluated with larger micro-batch sizes than ZB-2p. This would help to confirm that the evaluation is not biased towards ZB-2p."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849835935,
        "cdate": 1698849835935,
        "tmdate": 1699636869685,
        "mdate": 1699636869685,
        "license": "CC BY 4.0",
        "version": 2
    }
]