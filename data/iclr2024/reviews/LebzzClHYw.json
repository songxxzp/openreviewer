[
    {
        "id": "yOgPrwCoS4",
        "forum": "LebzzClHYw",
        "replyto": "LebzzClHYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_wghP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_wghP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new decoding method, instructive decoding, which generates tokens with logit scores constrasted with the logits under perturbed instructions. Extensive experiments show that such a technique is effective across a wide range of tasks,"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presented idea is intuitive and interesting.\n- Extensive experiments and ablations show that the idea works.\n- The paper is generally well-written."
            },
            "weaknesses": {
                "value": "- The empirical gain in Table 1 seems incremental (e.g. only 1-2 points of overall improvement) \n- The idea is not particularly groundbreaking"
            },
            "questions": {
                "value": "- Would there a better way to present the results in Table 1? Currently it feels incremental since the improvement is only 1-2 points, but I could see other arguments against this (e.g., increasing from 3b to 11B also only does not improve much). Would there be a better way to help the readers better contextualize the improvement (or find datasets that are more discriminative between weaker and stronger systems)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603681266,
        "cdate": 1698603681266,
        "tmdate": 1699636426613,
        "mdate": 1699636426613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kTsnlypPzm",
        "forum": "LebzzClHYw",
        "replyto": "LebzzClHYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_sigH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_sigH"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method called Instructive Decoding (ID) designed to enhance the performance of instruction-tuned LLMs when they encounter unfamiliar tasks due to the lack of diverse annotated prompts during training. Unlike other methods that rely on expanding the set of instructions, ID manipulates the model by adding 'noisy instructions' that alter the logits of the next-token prediction contrastively. These 'noisy instructions' are essentially counter-instructions that generate contrasting logits, which are then applied to the original logits to refine the prediction distribution.\n\nEvaluations show that models augmented with ID have an edge over basic models in zero-shot generation and better adherence to instructions for certain tasks. The main metric used for evaluation is the zero-shot Rouge-L score on unencountered tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The Instructive Decoding (ID) approach is interesting. It offers an alternative to enhancing LLMs using a broader set of instructions and doesn't necessitate further fine-tuning.\n\n2) The methodology is straightforward and the evaluation metrics, especially those in the table, offer a comprehensive view of the results."
            },
            "weaknesses": {
                "value": "The results, although promising, are mixed. From table 1, only about half of the tasks demonstrated improvements, while the rest experienced a decline in performance.\n\nThe paper's rationale needs clarity, especially from a machine learning perspective. It seems there's an assumption that noise-based responses will always produce contrasting predictions at every word level. For instance, if the base prediction is 'xxxx', and the noisy prediction is 'xxxx is wrong', the 'xxxx' part remains consistent, which can be misleading.\n\nThe paper references the \"anchoring effect\" but doesn't provide a clear link between it and the proposed method. The model doesn't directly witness the output from the noisy instruction. Instead, this output is indirectly factored into the next-word prediction logits computation. This differs from the anchoring effect, where individuals receive specific information before arriving at a final judgment."
            },
            "questions": {
                "value": "Can the authors shed light on the underlying rationale behind the proposed method, especially concerning the prediction of logits? How can it be ensured that the noisy responses will always lead to contrasting predictions at every word level?\n\nHow is the \"anchoring effect\" relevant to the proposed method? The paper seems to lack a detailed explanation connecting the two."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Reviewer_sigH",
                    "ICLR.cc/2024/Conference/Submission4503/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604874486,
        "cdate": 1698604874486,
        "tmdate": 1700589092784,
        "mdate": 1700589092784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yCA38jIHsp",
        "forum": "LebzzClHYw",
        "replyto": "LebzzClHYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_yaLP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_yaLP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a simple yet effective method named instructive decoding (ID) to boost the performance of instruction-tuned models. First, an noisy instruction is crafted to elicit undesired response from instruction-tuned models; then ID refines the next-word logits based on the logits from the noisy instruction. The method is training-free and demonstrates improvement in many instruction-following tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the method is simple and effective.\n- extensive experiments and analysis"
            },
            "weaknesses": {
                "value": "-it\u2019s still unclear (for a practitioner or researcher) to what extent the noisy instruction would be helpful for a new task. Maybe some qualitative analysis in those no-improvement tasks would provide some intuition on why the noisy instruction is harmful in the proposed instructive decoding."
            },
            "questions": {
                "value": "- is it possible to further fine-tune the model via the new logits (i.e.,  $z_t - \\tilde z_t$ in Algorithm1)\n- I wonder if the improvement from ID correlates with the response length required for a task. E.g., is ID more useful in tasks that require long response generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Reviewer_yaLP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698895463526,
        "cdate": 1698895463526,
        "tmdate": 1700669376488,
        "mdate": 1700669376488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AQh59VGkXL",
        "forum": "LebzzClHYw",
        "replyto": "LebzzClHYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_K2iN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4503/Reviewer_K2iN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach for improving the performance of instruction-tuned LLMs on classification and generation tasks. The key idea is to contrast the token generation probabilities conditioned on the true instruction with probabilities conditioned on noised instructions. The paper performs experiments across four model families (Tk, T0, Alpaca, and OpenSNI) on the UnNatInst and SupNatInst datasets, finding that the proposed approach generally produces improvements across tasks and model families over a standard decoding baseline."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1) The approach is simple but well-motivated: it doesn't require access to additional models (as Contrastive Decoding does) or access to model internals beyond token probabilities. \n\nS2) The experimentation was comprehensive, and I appreciated the analysis and ablation experiments, especially the \"degradation to enhancement\" part, the generalization experiments, and the analysis of sensitivity to the \\epsilon parameter.\n\nS3) The results were convincing to me: the approach produces consistent improvements, with substantial benefits in cross-dataset generalization.\n\nS4) The paper was overall very-well written."
            },
            "weaknesses": {
                "value": "W1) While I appreciated the analysis of sensitivity to the \\epsilon parameter, given that there is only a relatively small region of \\epsilon where the approach outperforms the baseline, I'd appreciate more discussion of how \\epsilon was chosen and whether it was the same across all tasks in each experiment.\n\nW2) There's a bit of room to improve the analysis and discussion of it, see questions and comments below. \n\nW3) This is a minor weakness, but I didn't really understand the connection to \"anchoring\". Anchoring seems to be about sensitivity to the first thing appearing in the text, while this method doesn't privilege any particular positions in the text but contrasts against the negative prompt at all positions.\n\nW4) While the related work was generally good, there is room to mention a few other related papers (below) on contrastive methods for generation. But I also don't think this is a major weakness, as the application to instruction following is new.\n\n- Li et al. A Diversity-Prompting Objective Function for Neural Conversational Models. NAACL 2016\n- Shen et al. Pragmatically Informative Text Generation. NAACL 2019\n- Ho and Salimans. Classifier-Free Diffusion Guidance. 2021"
            },
            "questions": {
                "value": "Q1) How was the \\epsilon value chosen for the important experiments in the paper (e.g. Table 1 - Table 3)?\n\nQ2) I was confused by the discussion of the smoothing coefficient in 3.3. It says that the typical choice was 0.3, but performance tends to decline with positive \\epsilon values. And the equation for decoding is z - \\epsilon \\tilde z, which makes it seem like \\epsilon should be positive in order to get a contrast.\n\nQ3) Can any intuition be given for why the approach seems to work best in the generalization setting (3.3 \"Generalization Capabiltiies\")?\n\nQ4) What method is used to choose the random words from NLTK (i.e. what word list is used? something involving antonyms?)\n\nOther comments:\n- The discussion in the \"ID with Opposite\" section hints that some of the baseline models might be mode-splitting, and that (at least on these classification tasks), approaches might benefit from a decoding approach that uses min-Bayes risk or consensus decoding: group together words based on their similarities and choose to generate a word from a large group.\n- I wasn't really clear on the support for the conclusions in \"Visualization of Embeddings\", that as models get better at understanding instructions (e.g. are larger), performance of ID increases\n- \"aims to steer the models toward particular\" in Section 1. Should \"toward\" be \"away from\"?\n- \"pragmatics\" in the motivation for Label Adherence and Label Coherence is a bit misleading; this is much closer to \"semantics\".\n- it would help to make some of the table/figure captions more self-contained, e.g. defining * in Table 3 caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4503/Reviewer_K2iN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4503/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699131822229,
        "cdate": 1699131822229,
        "tmdate": 1700679718277,
        "mdate": 1700679718277,
        "license": "CC BY 4.0",
        "version": 2
    }
]