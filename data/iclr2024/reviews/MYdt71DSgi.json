[
    {
        "id": "0oCcFmhXFF",
        "forum": "MYdt71DSgi",
        "replyto": "MYdt71DSgi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_Ursz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_Ursz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a post-hoc (no training) way to cluster features, and shows some applications to open-vocabulary segmentation.\nThe paper takes the value-value attention in CLIPSurgery and generalizes it to iterated value-value, key-key, query-query attention (or a combination thereof) to compute similarity. \nExperiments are on object localization, PascalVOC, PascalContext, and OpenImages-V7, where the method beats raw similarity between text and features. There is a nice comparison to k-means clustering, where results are a bit better than K-means (but quite similar nonetheless)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "### Presentation\nThe paper is well-written.\n\n### Experiments\nI think the experiments are well done (though some comparisons are missing, see weaknesses). In particular, I like the insightful comparison to K-means.\n\nExperimental results evaluated on multiple architectures and datasets. There are clear ablations of the different components and settings (temperature, iterations, self-self attention).\n\n### Method\nThe method is simple, and reasonably principled"
            },
            "weaknesses": {
                "value": "Overall I'm not seeing a use case for this method. K-means is a quicker-to-implement clustering/visualization method, and on the other end of the spectrum other techniques like DeepCut seem to offer better performance.\n\n### Presentation\nThe title is \"Grounding Everything Model\". I think at best this is not a good fit for the paper, and at worst has the possibility to be a bit misleading. Firstly, this isn't a model, right? It is a post-hoc clustering technique and the authors evaluate it on several pretrained models. I suppose this is a form of grounding because it is a way to localize queries in an image, when the original architecture doesn't support that. But if you want to ground something, there are better models -- e.g. [Grouding-Dino](https://arxiv.org/abs/2303.05499)\n\n### Experiments\n1. The paper misses a lot of existing literature -- what about comparison to other papers that do clustering of pretrained features (e.g. [Deep Spectral Methods](https://github.com/lukemelas/deep-spectral-segmentation)? The only other clustering method compared to is K-means (and I guess CLIPSurgery, which I believe is just 1 iteration of V-V attention). \n2. The results are pretty weak -- not much better than K-means. Other feature-clustering approaches seem to have much better results -- e.g. [DeepCut](https://sampl-weizmann.github.io/DeepCut/). That paper also has extensive comparisons to existing lit that clusters deep features on VOC and other datasets."
            },
            "questions": {
                "value": "1. I wonder if this could be understood as an iterative approximation to some type of spectral clustering (computing neighbor laplacian, computing eigenvectors, the temperature setting in the alg washes out all eigenvectors with eigenvalues < some thresh, and then run k-means clustering on the eigenvectors). An exact analysis is escaping me here.\n2. For the random gaussian clustering technique -- all vectors come from the same gaussian, right? And the resulting clustering from GEM is similar to that of K-means. That is a somewhat arbitrary clustering because the underlying process all comes from the same cluster. Why not generate data from different clusters, and show that GEM is better able to recover the underlying clusters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725035411,
        "cdate": 1698725035411,
        "tmdate": 1699636134535,
        "mdate": 1699636134535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zmtf3LhR6K",
        "forum": "MYdt71DSgi",
        "replyto": "MYdt71DSgi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_PF2u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_PF2u"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a training-free method to address the image referential localization problem. It proposes GEM model and extends the v-v attention to self-self attention. The experimental results show that the proposed GEM framework improves the performance in training-free open-vocabulary localization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper proposes a training-free method to address the open-vocabulary segmentation problem, and it improves the baseline model.\n2. This paper shows that by merely computing the proposed heuristic similarity and without training, the pre-trained model already does well in open-vocabulary segmentation task."
            },
            "weaknesses": {
                "value": "1. This work claims the localization property is 'emerging', which is somehow overclaimed. The resulting model yields merely less than 50% mIoU in VOC, which is far from 'emerging' performance.\n2. The so-called localization is usually used for a high-level concept. In this work, it should be replaced by the more precise term 'segmentation'.\n3. The overall method is straightforward and tedious. It shows a way to compute a heuristic similarity with a high distinctiveness and does improve the performance. However, the procedures are too tedious and there is still a huge gap with the methods involved with training.\n4. There is always a consensus that the final model usually needs two phases --- pre-training and finetuning. Why do we need a finetuning-free method for a segmentation task? A model pre-trained with a large quantity of data needs finetuning or alignment for a specific task unless it already has the emergent ability for multiple tasks. But today, we have seen that there is still a huge performance between a pre-trained model and a finetuned one, even in a single task. In this way, the finetuning-free property does not really matter in the context of a single task. The authors should elaborate more on this."
            },
            "questions": {
                "value": "The authors should give their explanations for each weakness above. \n\nOverall, I think this research is not really beneficial and promising for the development of this area, and this is not a paper that reaches the acceptance standard of ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2028/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2028/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2028/Reviewer_PF2u"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768923415,
        "cdate": 1698768923415,
        "tmdate": 1699636134420,
        "mdate": 1699636134420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xjJAuhotVK",
        "forum": "MYdt71DSgi",
        "replyto": "MYdt71DSgi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_67Tz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_67Tz"
        ],
        "content": {
            "summary": {
                "value": "The paper present Grounding Everything Model (GEM) in order use pre-trained Vision-Language (VL) models for object Localization without the need of re-training or fine-tuning the VL model. Building on CLIPSurgery (that incorporates value-value attention), the paper extends it to a generalized self-self attention path and propose a set of regularizations that allows the model to better generalize across datasets and backbones. Experimental results on three benchmarks (PascalVOC, Pascal Context, and OpenImages-V7) shows promising performance boost compared to existing training-free works."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. Related literature is being reviewed in a comprehensive manner. The paper provides an in-depth analysis of the properties of self-self attention, along with its connection with k-means clustering, in various pretrained ViT transformer models."
            },
            "weaknesses": {
                "value": "My main point of criticism concerns the technical novelty of this work, since it builds on the recently proposed CLIPSurgery and proceeds by simply swapping value-value with self-self attention. \n\nWhilst this might appear as a marginal technical contribution, the authors provides good experimental results and a thorough analysis of v-v and self-self attention, with an interesting connection to k-means. It is not totally convincing, though, how this work is essentially different than CLIPSurgery. I would expect a more comprehensive comparison to CLIPSurgery, especially in methodological terms -- why the proposed method's perfomance cannot be achieved by CLIPSurgery by simply exploring the attention mechanism?"
            },
            "questions": {
                "value": "In Table 2, k-means achieves the best results for k=3 (PascalVOC) and k=7 (Pascal Context):\n - What would be a possible explanation for this? \n - How would a different clustering method affect the results (in other words., could a different clustering method achieve results than the proposed GEM's ones)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804018365,
        "cdate": 1698804018365,
        "tmdate": 1699636134320,
        "mdate": 1699636134320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZEYcUNc1W1",
        "forum": "MYdt71DSgi",
        "replyto": "MYdt71DSgi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_uWg6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2028/Reviewer_uWg6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Grounding Everything Model (GEM), leveraging the latent localization capabilities of VL models trained on web-scale datasets. In this paper, the authors present an extended version of the CLIPSurgery concept of v-v attention, called self-self attention, which can extract localization information from VL models. The GEM method effectively enables open vocabulary localization without additional training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Overall, this paper is well-written, and the technical details are easy to follow. \n\n2) The main idea of avoiding additional training for open vocabulary localization is unquestionably important.\n\n3) The main contribution of this paper is the self-self architecture, which is an extension of CLIPSurgery's concept of v-v attention."
            },
            "weaknesses": {
                "value": "**Technical Novelty.** The fact that v-v attention was suggested without considering that it can also be applied to keys and queries seems odd. I believe the CLIPSurgery paper may have been lacking in ablation since a properly executed ablation should reveal that v-v attention is not necessarily the most effective approach. According to this paper, they \"...discovered that the major cause of this issue is the parameters (query and key) in the self-attention module\" (see CLIP Surgery). Therefore, it seems strange that the query and key parameters are actually required now.\n\nThis brings me to the main concern I have. The proposed architecture is a slight extension of the v-v attention, and in particular, I cannot see how duplicate self-attention, in a similar manner to v-v attention, could be applied to other tasks or models. According to my understanding, this represents a very small ablation that the first paper missed, and is not something the community could make significant use of in the near future for another task or dataset.\n\n\n**Motivation.** It is understandable that the authors are motivated to use pretrained vision and language models for localization. However, I am unsure why this is important since we already have SAM [1] (and other improved versions), which demonstrate that these supervised models are capable of almost completely solving this problem. The fact that SAM performs extremely well on this task raises the question of whether pretrained vision and language are still required, given its excellent performance.\n\n[1] Segment Anything Model. ICCV 2023.\n\n\n**Experiments.** For Table 2, did the authors use other methods than K-means? The K-means approach appears to be almost equivalent to the proposed approach, although the proposed approach has been modified to achieve the best results, while the K-means approach I assume has not been.\n\nAdditionally, this work has been applied to CLIP, but CLIP is already fairly updated while there are several other existing methods that are better, such as BLIPv2, LLaVa, and others. We cannot be certain whether the problem of object localization properties still exists in more advanced methods, so I would like to see whether the approach can generalize well to other models."
            },
            "questions": {
                "value": "I am concerned that this paper does not present a significant approach to one specific VL model, such as CLIP, for dealing with object localization in a training-free manner. My concerns have been listed above, and I would appreciate it if the authors could address them. I am also open to the authors' feedback and other reviewers' opinions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2028/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947273906,
        "cdate": 1698947273906,
        "tmdate": 1699636134260,
        "mdate": 1699636134260,
        "license": "CC BY 4.0",
        "version": 2
    }
]