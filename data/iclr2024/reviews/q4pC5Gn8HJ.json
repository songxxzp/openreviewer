[
    {
        "id": "w0jy6Y9LBK",
        "forum": "q4pC5Gn8HJ",
        "replyto": "q4pC5Gn8HJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission222/Reviewer_A6SE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission222/Reviewer_A6SE"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically understands the reason why the non-contrastive SSL (only with positive pairs alignment) does not collapse. They propose a framwork called SimXIR through the neighbor-averaging dynamics and discover a novel implicite bias of non-contrastive SSL. They also propose the self-supervised fine-tuning, a new fine-tuning paradiam, to enhance the off-the-shelf models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- They propose a new non-contrastive SSL architecture with neighbor-averaging dynamics. This method is more simple and has solid theoretical guarantee of no collapse."
            },
            "weaknesses": {
                "value": "- The writting needs to be further polished. For example, the term \"neighbor-averaging dynamics\" appears very early, but they do not give even a intuitive explaination, making the non-expert readers confusing.\n- Experiments seem to lack the results of pre-training from scratch by SimXIR. To my understanding, SimXIR is not only a fine-tuning module, but also can serve as a self-supervised pre-training framework. As they have stated, the random initialization may make the group means close to each other, but this case seems very uncommon in practice.\n- The comparison between two variants of SimXIR is missing. \n- How to justify that the neighbors of a data point are mainly its transformed versions? This assumption misses the discussion about its reasonability. For fine-grained data, if this assumption might be violated? If possible, does the SimXIR still work?\n- Some typos. For example, Eq. (3.2)"
            },
            "questions": {
                "value": "- According to Fig. 3, the neighbor-averaging dynamics can boost the existing clustering methods. This result is very interesting. But the details about how to incorporate the neighbor-averaging dynamics into clustering are unclear for me. Please elaborate this problem.\n- The neighbor-averaging dynamics can replace the asymmetric structure and achieve good performance. Assume that the asymmetric structure is still adopted together with the neighbor-averaging dynamics. Does this operation boost the performance of existing method such as BYOL?\n- Why does SimXIR remove the projection layer? This trick is widely used by current SSL methods. Is the projection layer removed for ease of theretical analysis?\n- How does the batch size affect the performance of SimXIR?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653987260,
        "cdate": 1698653987260,
        "tmdate": 1699635948112,
        "mdate": 1699635948112,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2foYUJOIBp",
        "forum": "q4pC5Gn8HJ",
        "replyto": "q4pC5Gn8HJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission222/Reviewer_UTfC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission222/Reviewer_UTfC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the SimXIR framework for non-contrastive self-supervised learning. The SimXIR fixes the online network of the last round as the target network to supervise the learning of the current model, which prevents the collapsed solutions without the asymmetric tricks. Theoretical and visualization results show that the SIMXIR compresses the intra-group distances and discriminates the features of the disjoint groups."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Provide a simple framework for non-contrastive SSL\n+ Visualize data distributions to validate theoretical results.\n+ Self-supervised fine-tuning significantly improves the performance of popular self-supervised models."
            },
            "weaknesses": {
                "value": "This paper provides a sufficient mathematical analysis on the proposed SimXIR architecture; however, I have the following concerns:\n- Comparison with SimSiam, based on Fig. 1(a), the SimXIR without the MLP predictor and applying the l2 loss to train the model. How do such modifications prevent the model from collapsing during the training process?\n- The experimental results are mainly focused on the self-supervised fine-tuning, and the performance of SimXIR working on the randomly initialized models is not presented. In addition, the performance gain of SimXIR on large datasets (such as ImageNet) is insufficient when fine-tuning with the SOTA SSL techniques.\n- The numerical results of SimXIR in the tables are evaluated with only one iteration round. The multi-round ablation study is not reported. \n- In addition to the accuracy results of self-supervised fine-tuning on the popular datasets and the visualization of the toy experiments, please provide more experimental evidence to validate the theoretical results.\n- Typo in Eq. (3.2)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664979116,
        "cdate": 1698664979116,
        "tmdate": 1699635948032,
        "mdate": 1699635948032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Peh7U7gVp",
        "forum": "q4pC5Gn8HJ",
        "replyto": "q4pC5Gn8HJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission222/Reviewer_ByEQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission222/Reviewer_ByEQ"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates non-contrastive learning from neighbor-averaging dynamics. It propose SimXIR which has good theoretical properties: contraction and alienation. Experimental results show that SimXIR can enhance representations of off-the-shelf SSL models"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors give a novel perspective to understand non-contrastive learning methods and show symmetric design can improve the self-supervised learning.\n2.  The paper identifies two theoretical properties of non-contrastive learning. These theoretical justifications are novel.\n3.  Experimental results verify its effectiveness in boosting representations across various SSL models."
            },
            "weaknesses": {
                "value": "1. Limited novelty: The method SimXIR looks very similar to Knowledge Distillation\uff08mean teacher): SSL pretrain then knowledge distillation.\n\n2. The gain performance on CIFAR-10 and ImageNet is marginal (less than 1% and 0.3%[SimCLR -> BYOL],respectively). And the baseline for CIFAR-10/100 is low (accuracy usually is higher than 90% on CIFAR-10 and 60% on CIFAR-100 with 800ep). If baseline is higher, then the gain may decrease.\n\n3. SimXIR works well on fine-tuning, but there is no evidence that SimXIR can work on pretrain. My concern is that SimXIR can prevent collapsed solutions when the initialization is good (fine-tuning on pretain) but can not prevent collapsed solutions when trained with random initialization. So SimXTR may be different from non-contrastive SSL and the explanation about why non-contrastive SSL can prevent collapsed solutions may be not correctly\u3002\n\n4. Typos:  $L_2$-norm in Equation (3.2) should be written correctly, $|\\cdot|_2^2$. $\\R_C(x)$ in the next row has the same typo."
            },
            "questions": {
                "value": "See the concerns and quedstions above in the section of weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission222/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858866827,
        "cdate": 1698858866827,
        "tmdate": 1699635947939,
        "mdate": 1699635947939,
        "license": "CC BY 4.0",
        "version": 2
    }
]