[
    {
        "id": "XRFmhe0zG5",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to investigate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition.  It exploits these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects of prompt interventions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is interesting to view diffusion models from the information theory prespective. For examle, exact expressions for mutual information and conditional mutual information can be expressed in terms of the denoising model.\n2. Compared with attention, the proposed mutual information seems more exact in identifying the relationship between the text and image. Massive visualization in the supplement demonstrate the effectiveness of this method."
            },
            "weaknesses": {
                "value": "1. It is still unclear of the concrete implementation. Without access to the network, but just with API, how to get the mutual information and corresponding visualization.\n2. What is the application field. Attention-based methods are widely applied to image editing and show impressive results. What is the advantage of this decomposition method, compared to attetion.\n3. The paper is poor structured and hard to read."
            },
            "questions": {
                "value": "Refer to the weakness and questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_U1vn"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698419259714,
        "cdate": 1698419259714,
        "tmdate": 1699636492463,
        "mdate": 1699636492463,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aBRMMY2XEB",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf"
        ],
        "content": {
            "summary": {
                "value": "This work discusses the interpretability of denoising diffusion models, which are used for conditional generation and density modeling of complex relationships like images and text. The authors aim to understand the relationships learned by these models by establishing a connection between diffusion and information decomposition. They provide exact expressions for mutual information and conditional mutual information in terms of the denoising model. The paper also introduces methods to quantify informative relationships between words and pixels in an image. The authors utilize these relationships to measure the compositional understanding of diffusion models, perform unsupervised localization of objects in images, and measure effects when editing images through prompt interventions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper provides an approach to illuminate the fine-grained relationships learned by diffusion models.\n- The authors introduce a natural non-negative decomposition of mutual information, which can be beneficial for understanding high-dimensional data.\n- The paper offers practical applications, such as unsupervised localization of objects in images.\n- The research uses information theory to provide a black-box method for understanding relationships in complex spaces like text and images."
            },
            "weaknesses": {
                "value": "- The method is relatively trivial, replacing word in the prompt and comparing the difference in pixel level\n- lack of comprehensive comparison with attention based method"
            },
            "questions": {
                "value": "- The example in first row of Fig. 1 seems to have difference other than the bear and elephant, why does the heat map only show difference in salient objects? Other examples have similar questions, but Fig. 1 is most obvious. \n- What's the advantage compared to attention based method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_gEHf",
                    "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807530538,
        "cdate": 1698807530538,
        "tmdate": 1700581674534,
        "mdate": 1700581674534,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EJ9WnYNaZB",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8"
        ],
        "content": {
            "summary": {
                "value": "Denoising diffusion models are used for complex data relationships like images and text, and this paper has revealed a link between these models and information decomposition, enabling precise characterization of relationships, including mutual information, which can be used to understand and measure the relationships between specific elements in high-dimensional data like words and image pixels."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides a new perspective to explore the relationship between image and text in the Generation field, and quantify compositional understanding capabilities of diffusion models at a certain level."
            },
            "weaknesses": {
                "value": "However, the primary drawback lies in the paper's somewhat inadequate presentation, making comprehension challenging. Additionally, a limitation is the insufficient experimentation, which fails to demonstrate how this method could enhance the generation process. To conclude, I strongly advise the authors to undergo a significant revision."
            },
            "questions": {
                "value": "1. Inadequate preliminary information on related information theory makes Section 2 difficult to comprehend.\n2. Clicking on citations does not redirect to the bibliography, making it challenging to locate the correct reference.\n3. Apart from the formations, understanding the implementation of this method is challenging, necessitating more detailed explanations from the authors.\n4. Improved demonstration of how this method enhances the generation process in experiments would be beneficial.\n5. Does Section 6 exceed the limitation of 9 pages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_S4G8",
                    "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843305117,
        "cdate": 1698843305117,
        "tmdate": 1700565539895,
        "mdate": 1700565539895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9YDsVttBil",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the application of information theory to interpret text-to-image diffusion models. The authors present a way to measure and utilize the mutual information between the image and the text conditioning to visualize \"where\" a specific prompt maps to. They also demonstrate how this mapping from text to pixels can be used to relate images with text and localize words as well as help in interpretability by visualizing the effects of the text prompt in the generation process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a novel approach to measuring the relationship between the text conditioning and the image pixels in text-to-image diffusion models. Previous attempts that focused on interpreting the cross-attention operations within the model were architecture-specific and usually required additional processing or fine-tuning to translate attention into meaningful signals. In contrast, the authors show that their information-theoretic approach can be easily interpreted without additional overhead.\n\n- The experiments section clearly demonstrates both the advantages and disadvantages of the proposed approach. The paper paints a clear picture of when the mutual information can be useful and where attention can be used instead.\n\n- The method is well-grounded, with Section 2 presenting the approach and necessary background thoroughly. The paper is overall well-written."
            },
            "weaknesses": {
                "value": "- The authors present this paper as an approach to interpretability for text-to-image diffusion models but there is no clear demonstration of where this could be applied other than better visualizations of text-image relations. It would be informative to discuss (or even demonstrate) whether the mutual information can be exploited to better align text-to-image models with human preferences during inference time or fine-tuning. The broader scope of interpreting the generation process by relating pixels to words is missing from the paper and could greatly benefit it."
            },
            "questions": {
                "value": "- How is the attention extracted in Figure 2? The proposed per-pixel mutual information averages the result over all timesteps of the diffusion process. However, the attention seems to be extracted from a single forward pass of the model. That could be disadvantageous as the attention mechanism highlights different scales of features at different timesteps. In section 3.2.2 the authors acknowledge the existence of a  \"feature pyramid\" in the attention but is this again for a single time step of the diffusion process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_qKVB"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848537225,
        "cdate": 1698848537225,
        "tmdate": 1699636492184,
        "mdate": 1699636492184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ScLqdfOwT",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_ajkd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_ajkd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to visualize the relationship between image and text caption based on the mutual information (MI) estimated by a pre-trained diffusion model. The core observation is that the log probability density can be estimated up to constant by integrating the diffusion-model loss over timesteps (McAllester, 2023), which enables the MI between image and text-caption to be computed by the difference of the estimated log density from the conditional and the unconditional inference result of a text-to-image diffusion model. While the original MI is the average over the samples, the authors consider per-sample (point-wise) MI and further decompose it to per-pixel MI, which gives rise to a visualization of which region in the image is related with a certain word. In experiments, the proposed visualization method showed superior interpretation over the attention-based method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper considers an important topic in text-to-image diffusion models. Analyzing the influence of each word to the generated image has been attracting many researchers, since it serves as an essential tool to come up with ideas for improving or debugging the semantic alignment of the generated image and for developing image-editing methods.\n\nWhile a large body of literature exploits the cross-attention maps for the problem, this paper considers a new approach acts on pixels. The approach itself is quite similar to the classifier-free guidance that contrasts the conditional and unconditional inference results, which has been used in most of the diffusion models, but have strength by grounding on information theory. \n\nAlso, experimental results demonstrate the proposed method gives better interpretability compared to the attention-based methods."
            },
            "weaknesses": {
                "value": "A weak point of this paper is that the main manuscript seems to be not self-contained. While each of the following can be rather minor, they together drops readability.\n\n- References:\n    - The proposed method is heavily relying on the log-density estimator of McCallester, 2023, but lacks details. Further explanation on the practical behavior of this estimator and how this estimator can be derived, at least in abstract sense, would help the understanding.\n- Rather important figures and tables are in appendix.\n    - There is a fair amount of discussion in the main manuscript with Table 9 and Figure 10, but these are on the appendix thus making difficult to read.\n- Dataset information\n    - CoCo-IT is the main dataset used for evaluation, but how it has been filtered from CoCo is not described at least briefly."
            },
            "questions": {
                "value": "- Are the \u201cstandard definition\u201d and the \u201corthogonality principle\u201d the full list of the decomposition possibilities? If not, we might need some justification that why these two are better suited as pointwise information.\n    - The main manuscript mentions that both $i^s$ and $i^o$ will be explored. Can you locate where the $i^s$ results are?\n- Is there a notable relationship between $i^o$ and the classifier-free guidance, considering their similarity in computation? For example, if the guidance scale gets larger, does it make a enhanced visualization or a larger MI between the text and image?\n- The Eq. (5) and (6) contains expectation over the noise, $\\epsilon$. But different noises will make dramatically different images. How are the visualizations in Fig. 2 are generated, well aligned with the image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853511424,
        "cdate": 1698853511424,
        "tmdate": 1699636492080,
        "mdate": 1699636492080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9tqpR4KZ9h",
        "forum": "X6tNkN6ate",
        "replyto": "X6tNkN6ate",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
        ],
        "content": {
            "summary": {
                "value": "This work presents a new method for information decomposition at both pointwise and pixel-wise level via well-trained diffusion models, utilizing the results such that there is an exact connection between mutual information and MMSE denoising estimators under Gaussian noise, as well as the exact correspondence between denoising and density modeling. The work demonstrates the ability of such information decomposition to measure the compositional understanding of diffusion models, to perform unsupervised localization of objects in images, and to measure effects of selective image editing via prompt interventions. The proposed metric, *Conditional Mutual Information (CMI) estimators*, is displayed as an alternative to attention in capturing the effect of text interventions in image generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The finding that attention to a word doesn\u2019t necessarily imply this word would affect the generated outcome is interesting, and Figure 1 shows how CMI can be helpful in interpreting the model for the cases where attention cannot.\n2. The authors present a standalone section to discuss ethical implications of the models they are working on, as well as how their method could help deal with the potential risks."
            },
            "weaknesses": {
                "value": "1. The walkthrough of some key results that help construct the proposed information decomposition approach is too brief: it would be nice to have a more in-depth review of these results, for example, the derivation of Eq. (2), the part of diffusion model parameterization that\u2019s related to MMSE, etc. Meanwhile, the experiment section is relatively lengthy: some parts could be moved to the appendix, leaving the contents most closely connected to information decomposition in the main paper.\n2. Error bars in Table 3 for the comparison between CMI and attention are not reported.\n3. Source code is not shared to facilitate reproducibility."
            },
            "questions": {
                "value": "1. Could the authors give a quick example of how $i^o(\\mathbf{x};\\mathbf{y})$ from Eq. (5) is computed? Under the DDPM parameterization, the noise network predicts the forward noise, thus it makes sense to start with a real image $\\mathbf{x}$, sample $\\mathbf{\\epsilon}$ to obtain $\\mathbf{x}_{\\alpha}$, and compute the L2 term: this utilizes the forward diffusion. Meanwhile, the goal is to decompose the *generated* samples, which uses the reverse diffusion. Therefore, it\u2019s a bit unclear on how these mutual information estimators are calculated.\n2. The authors mentioned that Eq. (2) would hold for an \u201coptimal denoiser\u201d: what does it mean for a denoiser to be optimal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5031/Reviewer_EMcD"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5031/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699436058627,
        "cdate": 1699436058627,
        "tmdate": 1699636492006,
        "mdate": 1699636492006,
        "license": "CC BY 4.0",
        "version": 2
    }
]