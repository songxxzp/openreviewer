[
    {
        "id": "lJPa2kx2YJ",
        "forum": "g90ysX1sVs",
        "replyto": "g90ysX1sVs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission451/Reviewer_qikv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission451/Reviewer_qikv"
        ],
        "content": {
            "summary": {
                "value": "The authors study the use of rational activation functions for RL.  They show that these functions embed residual connections.  They study them empirically."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "An interesting, well-motivated idea, with a large number of various, potentially significant contributions."
            },
            "weaknesses": {
                "value": "**Small edits and minor recommendations:**\n\n- \"emphasise\" spelling error in 2.1\n- In the \u201cNetwork Architecture\u201d section of the appendix, there is a parentheses mismatch.\n- Does \u201cresp.\u201d mean respectively?  This stumped me for a couple minutes, I suggest being more clear and not using abbreviations like this, or at least defining them before use.\n- 2.3: \u201cHowever, if we circle back to this figure\u201d Which figure? Be more clear; don\u2019t make your reader work unnecessarily hard to understand.\n\n**Clarity Weaknesses:**\n- 2.1: the authors say very little about the architecture.  Is it conv nets?  Is the number of units between the baselines and their approach fixed, and they replace each unit\u2019s activation function with a rational plasticity unit (resulting in many more parameters for their approach)?  Or do they replace whole layers with a single rational plasticity unit (the parenthetical in 2.1 seems to hint at this, but it\u2019s unclear)?  Or do they keep all units from each layer intact, and add a singe additional rational plasticity unit (if I recall correctly, this is what the original rational activation function paper did)?  These things are critically important pieces of information about this paper\u2019s results (this is essentially an architecture paper, but the reader is left guessing about the most basic aspects of the architectures used), and this critical information should not be relegated to an appendix.  Worse, the last last question (how exactly rational plasticity units replace traditional units in an architecture) is unclear to me, even after reading the Network Architecture section of the appendix.  Update on this last: more details are given, almost parenthetically, on page 6 and again on page 8 (in the Q4 discussion).  However, the long delay hinders clarity, and even with these details, the exact details of the approach/architecture remain unclear.  I suggest clarifying all of these questions in 2.1; a figure could be very helpful.\n\n- \u201cTo this end, we also show agents equipped with rational activations that are tempered in Fig. 2. They have been extracted from agents with rational plasticity, that adapted to their specific environment. The plasticity of the rational activation functions is then tempered (\u201cstopped\u201d) in repeated application to emphasise the necessity to continuously adapt during training.\u201d I am not sure exactly what this means; it is too vague and can think of several different interpretations with very different meanings.  In the second sentence, it sounds like they freeze these rational activations after a while, but let the other network weights continue to be optimized.  But then the third sentence seems to contradict this (what does \u201crepeated application\u201d mean, and why would freezing some parameters \u201cemphasize the necessity to continuously adapt\u201d?)\n\n- 2.2 Theorem concern: The precise meaning of this theorem is unclear.  As stated, it can easily be taken to mean that all rational activation functions implicitly contain a residual connection.  But if I\u2019m understanding the proof correctly, it seems to indicate that it depends on the weights; that is, that a residual connection can be learned if necessary.  This is confusing and potentially misleading.  I suggest stating the Theorem more precisely.\n\n- 2.3: \u201cregularized joint-rationals, where the key idea is to constraint the input to propagate through different layers but always be activated by the same learnable rational activation function\u201d.  This is incredibly vague, and does not tell the reader what this approach means (and this sentence seems to constitute the entire description of joint rationals).\n\n**Other weaknesses:**\n- Is plasticity actually causing the better results?  Q3 is a very nice analysis, and provides evidence that the proposed approach reduces overestimation.  However, it is not clearly demonstrated that: 1) the plasticity in particular is what causes this reduction in overestimation OR 2) that the reduction in overestimation is what causes the improved performance in Q1 and Q2. BOTH of these things would need to be shown to support the authors\u2019 claims.  In fact, Figure 2 shows evidence against the former: rational activation functions in general seem to simply perform better even for stationary environments, which is nice, but seems to contradict the core thesis of the paper (rational activation functions cause better plasticity which in turn causes better performance).\n- The above is even more concerning in light of the lack of details about architectures used.  Perhaps the better performance is simply due to their approach using networks with more parameters and thus more capacity?  Unfortunately, it is not even clear how the different approaches\u2019 architectures vary, so it is difficult for a reader to even begin to evaluate this concern.\n\n\nNote: Despite the many significant concerns I have about this paper, I think it already has many nice results and strengths as well, in addition to being a well-motivated work.  If a rating of 4 were an option, I would give it a 4 instead of a 3.  I encourage the authors to work to address the clarity issues and the \"Is plasticity actually causing the better results?\" concern, and resubmit; I think this is a potentially very strong contribution."
            },
            "questions": {
                "value": "See clarity concerns above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697871824192,
        "cdate": 1697871824192,
        "tmdate": 1699635971504,
        "mdate": 1699635971504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RPPoABaa0o",
        "forum": "g90ysX1sVs",
        "replyto": "g90ysX1sVs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission451/Reviewer_mxps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission451/Reviewer_mxps"
        ],
        "content": {
            "summary": {
                "value": "Neural plasticity has been shown to be crucial to reinforcement learning (RL), especially to environments that change constantly and/or rapidly. This work proposes the use of adaptive activation functions to increase plasticity. Particualrly, it employs a form of rational activation functions, i.e., joint-rational activation, to have greater flexibility while avoiding overfitting. Experimentally, the work shows that such functions can make DQN competitive with DDQN and Rainbow without all the additional techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The work tackles the important problem of plasticity in deep RL with a novel method based on adaptive activation functions.\n2. Showing using rational functions can compensate for residual block is an intriguing and insightful finding"
            },
            "weaknesses": {
                "value": "1. Missing related works in the study of plasticity in deep RL (e.g., [1])\n2. There are works out there that tackles plasticity in deep RL. They should be added as baselines to compare with the proposed method (e.g., [2], [3])\n3. Results would be a lot stronger if it can show that the results transfer across algorithms too (e.g., policy gradient methods)\n\n[1] Abbas, Z., Zhao, R., Modayil, J., White, A., & Machado, M. C. (2023). Loss of plasticity in continual deep reinforcement learning. arXiv preprint arXiv:2303.07507.\n\n[2] Nikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W., & Barreto, A. (2023). Deep Reinforcement Learning with Plasticity Injection. arXiv preprint arXiv:2305.15555.\n\n[3] Dohare, S., Sutton, R. S., & Mahmood, A. R. (2021). Continual backprop: Stochastic gradient descent with persistent randomness. arXiv preprint arXiv:2108.06325."
            },
            "questions": {
                "value": "1. I am not very familiar with the area of adaptive activation functions. But what in particular do rational activation functions offer that make it superior over some of the options out there? E.g., I came across [1]\n2. Can the authors provide some intuition on why improved plasticity lead to reduced overestimation?\n\nJagtap, A. D., Kawaguchi, K., & Karniadakis, G. E. (2020). Adaptive activation functions accelerate convergence in deep and physics-informed neural networks. Journal of Computational Physics, 404, 109136."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Reviewer_mxps"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698846588,
        "cdate": 1698698846588,
        "tmdate": 1700731943702,
        "mdate": 1700731943702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aOj6KdzDuM",
        "forum": "g90ysX1sVs",
        "replyto": "g90ysX1sVs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission451/Reviewer_7Gju"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission451/Reviewer_7Gju"
        ],
        "content": {
            "summary": {
                "value": "The manuscript argues that plasticity is important for RL. The paper proposes rational activation functions for RL. It also proposes a regularized version of the activation function (joint rational). The results show that rational activation functions significantly outperform rigid activation functions like leaky-ReLU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of using rational activation functions to inject plasticity is very interesting, and the results are promising. DQN with rational activation functions significantly outperforms vanilla DQN. The use of rational activation in RL seems novel. I'm not aware of any work that uses these activations in RL. Most of the paper is clearly written and easy to understand."
            },
            "weaknesses": {
                "value": "Although the paper contains exciting ideas and promising results, there are many significant weaknesses in the paper that stop me from recommending a full acceptance. \n- **Missing literature on plasticity.** The manuscript needs to include the entire literature on plasticity. There are already many papers that show that there is a loss of plasticity when deep learning systems face a non-stationary data stream, such as in RL (Dohare et al., 2021; Lyle et al., 2022; Nikishin et al., 2022; Abbas et al., 2023; Lyle et al., 2023; Sokar et al., 2023; Nikishin et al., 2023; Dohare et al., 2023). Missing this literature also means that the paper is missing important baselines in the paper methods like CReLUs, selective reinitialization, layer normalization, etc. I realize that doing a full comparison in Atari with all of these methods might not be computationally feasible, but they should be compared in a smaller problem.\n- **Unsupported conclusions.** The manuscript claims that rational activation functions help because they inject plasticity. However, the paper does not provide any direct evidence of this claim. The paper shows that deep RL methods with rational activations outperform deep RL methods with rigid activations. Better performance does not mean better plasticity. However, this problem can be somewhat reduced by comparing rational and rigid activations in a continual supervised learning problem like random label MNIST (Lyle et al., 2023) or permuted MNIST (Dohare et al., 2023). Then, we will have direct evidence that rational activations inject plasticity.\n\nDohare, S., Sutton, R. S., & Mahmood, A. R. (2021). Continual backprop: Stochastic gradient descent with persistent randomness. arXiv preprint arXiv:2108.06325.\n\nNikishin, E., Schwarzer, M., D\u2019Oro, P., Bacon, P.-L. & Courville, A. The primacy bias in deep reinforcement learning. In International Conference on Machine Learning, 16828\u201316847 (PMLR, 2022).\n\nLyle, C., Rowland, M. & Dabney, W. Understanding and preventing capacity loss in reinforcement learning. In\nInternational Conference on Learning Representations (2022).\n\nAbbas, Z., Zhao, R., Modayil, J., White, A., & Machado, M. C. (2023). Loss of plasticity in continual deep reinforcement learning. CoLLAs 2023.\n\nSokar, G., Agarwal, R., Castro, P. S. & Evci, U. The dormant neuron phenomenon in deep reinforcement learning. In Krause, A. et al. (eds.) Proceedings of the 40th International Conference on Machine Learning, vol. 202 of Proceedings of Machine Learning Research, 32145\u201332168 (PMLR, 2023).\n\nLyle, C. et al. Understanding plasticity in neural networks. In Krause, A. et al. (eds.) Proceedings of the 40th International Conference on Machine Learning, vol. 202 of Proceedings of Machine Learning Research, 23190\u201323211 (PMLR, 2023).\n\nNikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W., & Barreto, A. (2023). Deep Reinforcement Learning with Plasticity Injection. arXiv preprint arXiv:2305.15555.\n\nDohare, S., Hernandez-Garcia, J. F., Rahman, P., Sutton, R. S., & Mahmood, A. R. (2023). Loss of Plasticity in Deep Continual Learning. arXiv preprint arXiv:2306.13812."
            },
            "questions": {
                "value": "Can you please clarify what exactly is the rational activation function? The definition says $R(x)$ where x is a real number, but the equation uses $x^j$, what is $x^j$?\n\n------------------\nI've updated my score in light of the new results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Reviewer_7Gju",
                    "ICLR.cc/2024/Conference/Submission451/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776670043,
        "cdate": 1698776670043,
        "tmdate": 1700710795934,
        "mdate": 1700710795934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "94QoptUQhW",
        "forum": "g90ysX1sVs",
        "replyto": "g90ysX1sVs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission451/Reviewer_cKgE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission451/Reviewer_cKgE"
        ],
        "content": {
            "summary": {
                "value": "The paper argues for plasticity in RL through adaptive activation functions. It proposes using (safe) rational activation functions as a plug-in modification to existing value-based RL methods (DQN-style algorithms).\n\nThe authors first motivate the need for plasticity through activation functions in constantly changing reinforcement learning environments. They categorize environments into static, dynamic, and progressively evolving environments. The authors then show how existing methods, such as DQN and non-rational plasticity variants (PELU), fail to adapt to all changes, whereas rational activations can.\n\nHaving established Rational activation functions as promising candidates for imbuing existing neural networks with plasticity, the authors show that rational activation functions can embed residual connections. Using this insight, they develop a joint rational activation function that is a regularized form of rational plasticity through weight-sharing across activations in different layers.\n\nThe authors finally demonstrate through experiments that equipping popular algorithms such as a DQN with rational activations leads to consistent improvements in performance on Atari games, elevating them to be competitive alternatives to highly engineered and computationally expensive methods such as RAINBOW."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality\nThe proposed application of the activation function to the RL pipeline is a novel contribution. \n\n### Quality\nThe quality of the work is generally high.\n\n### Clarity\nThe work is generally presented clearly.\n\n### Significance\nActivation function optimization has shown considerable success in other areas, such as pruning. Therefore, applying them as an alternative to heavy architectural search and per-environment hyperparameter optimization methods is a significant step."
            },
            "weaknesses": {
                "value": "- The work seems to miss some previous work on plasticity in RL. Please see the questions section for more information on related works. Overall, I think discussing this would further concretize the argumentation for rational plasticity while placing this work better in the RL literature. \n- Hyperparameters are not at all discussed as a requirement for adaptation. I believe this is a critical aspect of designing RL pipelines, and therefore, a focus on only adaptable Neural architectures is limiting, in my opinion. It would make a lot of sense if the authors explicitly state how they see their work as related to this literature.\n- I think the baselines need more justification. While it is understandable that the focus is on activation functions, so comparisons with static methods make sense, the authors also argue that the dynamically changing nature of rational activations allows RL agents to adapt to changing environmental circumstances. This usability argument, in my opinion, is not captured by the considered baselines since, most of the time, when we are talking about addressing changing aspects of environments, the focus is either on algorithmic improvements, continual and curriculum-based methods, or dynamic hyperparameter adaptation mechanisms. I believe either making strong arguments as to why the authors do not consider these approaches or performing comparisons to one of them would significantly boost this line of argumentation."
            },
            "questions": {
                "value": "- How is this rational plasticity related to previous works on plasticity in RL? [Sokar et. al, 2023, Abbas et. al, 2023,Lyle et al., 2023,Nishikin et al., 2022]\n- Parametric forms to capture existing activation functions have previously been used in supervised learning [Loni et al., 2023]. How is the rational form placed regarding these works? Does the comparison to SiLU and PELU capture this already?\n- It is argued in sections 1 and 2 that RL can significantly benefit from adaptive neural architectures. However, another critical aspect of RL algorithms is hyperparameter optimization, with potentially the requirement to adapt hyperparameters as an RL agent dynamically trains [Mohan et al., 2023]. To what extent can the adaptable rational activation functions in RL mitigate these requirements?\n- Since the benefit of applying rational activation functions lies in their ability to adjust their parameters dynamically, shouldn't they be additionally compared to Baselines of dynamic hyperparameter optimization, such as hyperparameter schedules [Zahavy et al., 2020], progressive episode lengths [Fuks et al., 2019]? I think it makes sense to at least justify whether relationships between the use of rational activation functions for improving adaptability and methods to tackle the same issue in this literature does exist or not.\n\n### References\n- [Sokar et. al, 2023] Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci.  The dormant neuron phenomenon in deep reinforcement learning. arXiv preprint arXiv:2302.12902, 2023.\n- [Abbas et. al, 2023] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of plasticity in continual deep reinforcement learning. arXiv preprint arXiv:2303.07507, 2023.\n- [Lyle et al., 2023] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. arXiv preprint arXiv:2303.01486, 2023.\n- [Nishikin et al., 2022] Evgenii Nikishin, Max Schwarzer, Pierluca D\u2019Oro, Pierre-Luc Bacon, and Aaron Courville.  The primacy bias in deep reinforcement learning. In International Conference on Machine Learning, pages 16828\u201316847. PMLR, 2022.\n- [Loni et al, 2023] Loni, M., Mohan, A., Asadi, M., & Lindauer, M. (2023). Learning Activation Functions for Sparse Neural Networks. arXiv preprint arXiv:2305.10964.\n- [Mohan et al, 2023] Mohan, A., Benjamins, C., Wienecke, K., Dockhorn, A., & Lindauer, M. (2023). Autorl hyperparameter landscapes. AutoML Conference 2023 (https://openreview.net/forum?id=Ec09TcV_HKq).\n- [Zahavy et al., 2020] Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H. P., ... & Singh, S. (2020). A self-tuning actor-critic algorithm. Advances in neural information processing systems, 33, 20913-20924.\n- [Fuks et al., 2019] Fuks, L., Awad, N. H., Hutter, F., & Lindauer, M. (2019, August). An Evolution Strategy with Progressive Episode Lengths for Playing Games. In IJCAI (pp. 1234-1240).\n\nI have increased my score based on the changes made by the authors"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission451/Reviewer_cKgE",
                    "ICLR.cc/2024/Conference/Submission451/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698883285043,
        "cdate": 1698883285043,
        "tmdate": 1700742254866,
        "mdate": 1700742254866,
        "license": "CC BY 4.0",
        "version": 2
    }
]