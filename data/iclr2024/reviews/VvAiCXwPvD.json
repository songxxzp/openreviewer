[
    {
        "id": "iK4tPqcZXs",
        "forum": "VvAiCXwPvD",
        "replyto": "VvAiCXwPvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the properties of explanations provided by LLMs. Specifically, they investigate a) Whether explanations are `precise`, i.e. can simulate how the model would behave under counterfactual questions b) are `general`, whether they help with the simulatability of queries that are different than the original query. The authors provide a methodology to evaluate explanations for these two criteria and evaluate two LLMs (GPT-3, 4) in two tasks (StrategyQA, SHP)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I find that the paper has an interesting object to evaluate (explanations provided by LLMs) and a well-constructed methodology to do so. It appears to be a good addition to the methodologies being developed to evaluate LLM explanations, such as ones provided in the paper (Turpin et al., Creswell et al.). \n\nMore specifically,\n\n1. The idea that the quality of an expectation could be measured by the simulatability of counterfactuals is a useful one. The ability of explanations to help humans construct mental models is one that is discussed in the existing literature \u2013 however, instantiation in the context of language models appears to be an interesting idea.\n\n2. Formalizing simulatability using logical entailment is again a useful idea. I\u2019m not sure if this exists in the literature \u2013 but could be a good way to automate explanation evaluations in general.\n\n3.  I appreciate the sanity checks in Section 5.1 to justify several design choices made by the authors, before moving on to more complex experiments. This clarifies several questions in the mind of a potential reader."
            },
            "weaknesses": {
                "value": "1. There is a human study in the paper that does not mention whether the study has an IRB approval. Citing ICLR Code of Ethics `Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported. ` **How to address: I recommend the authors to apply for an IRB approval from their respective institution to resolve this concern. Ideally this should have done prior to data collection, but I am leaving this to the judgment of Ethics Reviewers / ACs.**\n\n2. One of the conceptual contributions authors make is the proposed criteria to evaluate explanations. However, I\u2019m a bit concerned with both desiderata (generality and precision) provided by the authors:\n- 2.1 - The terminological choice of precision looks very much like faithfulness (See Q1), which created significant confusion for me. The definition of precision in Section 3.2 is referred to as faithfulness in many other contexts (as in the cited related works). Unless I\u2019m missing something here, I\u2019m not sure why one needs to invent a new name for it.\n- 2.2 - I find the claim `an ideal explanation .. should be generalizable, \u2026 it should also reveal how well the model reasons on unseen outputs` unjustified. Why do we really want generalizable claims? Why is it a non-ideal thing to only explain the answer to the question one is responding to? As long as the explanation is faithful, I cannot say being more general or more specific is preferable in this context. **How to address: I\u2019d be curious to hear authors\u2019 justifications for a) Why Precision is not simply Faithfulness? b) How do we claim that we want generality in these explanations?**\n\n3. I am also unclear about what conclusions we can reliably draw from the experiments, or how these may benefit the users/consumers of the explanations. \n- One general conclusion appears to be around a more capable model (GPT-4) providing `better`(under the criteria in the paper) explanations than a less capable model (GPT-3). \n- However, beyond the relative inferences, I cannot understand whether these numbers are good or not in an absolute sense, thus whether explanations are good or not (Q3). In general, a lack of comparison to interpretable numbers makes the results hard to interpret.  **How to address: One way would be to provide baseline explanations. For instance, one can ask the same set of questions to a set of human users, let the human users provide explanations, and compare those explanations with the ones provided by the models.**\n\nMinor: \n\n1. Imho, the title is not precise. \u201cDo models explain themselves\u201d is pretty generic with the use of the word \u201cmodel\u201d, but really the authors are focusing on a specific model class, which is autoregressive language models. I\u2019d personally err on the side of precision."
            },
            "questions": {
                "value": "1. I\u2019m slightly confused about the proposed terminology here. The authors argue that precision is that `they should lead to mental models that are consistent with the model\u2019s behavior.`; this definition sounds quite a bit like faithfulness, in my opinion (also in the related works authors refer to faithfulness as `It is different from faithfulness, which measures whether an explanation is consistent with the model\u2019s own decision process`). \n\nIn that, this argues explanation should be faithful to the model\u2019s behavior. Personally, I find precision to have a stronger connotation around explaining what could be explained, and nothing beyond it. I\u2019d love to hear the authors\u2019 opinions about why this is not faithfulness and is precision. \n\n2. The authors empirically argue that generality and precision do not seem correlated -- however I do not necessarily agree with this, and I\u2019m unclear about whether an explanation has to be general as long as it is precise. To be more concrete, one could make an extremely local explanation (i.e. hard to find BLT because markets do not sell BLT) that is extremely precise. For instance, using the formalization in the paper, we could have $|C|$ large but $|C^*|$ small, potentially $0$ or $1$, e.g. if the model uses the answer to explain the answer. Are these two desiderata independent? If they are not, I believe the paper would also benefit from a discussion around this, and also further discussion in Section 5.2.2 where the authors suggest independence. \n\n3. I\u2019m not sure how one can claim that 80% is not precise enough (i.e. the claim `explanations have low precision (80% for binary classification)` in the intro); the number seems nontrivial. Why do we expect a larger number? Do we have statistics about how precise human explanations are? Can this number be even larger than for humans\u2019? \n\n4. What is being reported in Table 5? Is it the correlation coefficient? Is it Spearman or Pearson? What are the p-values? Are the numbers statistically significant? \n\nMinor\n1. The inline citations are confusing to me, perhaps it would clarify to replace \\cite calls with \\citep calls to clarify reading. \n2. Page 1, \u201cMuslin countries\u201d -> \u201cMuslim countries\u201d\n3. Figure 2 says \u201crobot\u2019s answer\u201d; I\u2019m assuming this is the LLMs\u2019 answer? Otherwise, what\u2019s the robot?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors perform human simulations via Mechanical Turk, but I do not see whether the authors have IRB approvals or exemptions for the human subject experiment.\n\n**Rebuttal:** Answered by the authors, they suggest they do have an IRB."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n",
                    "ICLR.cc/2024/Conference/Submission2760/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697928841832,
        "cdate": 1697928841832,
        "tmdate": 1700428220958,
        "mdate": 1700428220958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8dHYQQukdg",
        "forum": "VvAiCXwPvD",
        "replyto": "VvAiCXwPvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the concept of counterfactual simulatability to evaluate natural language explanations generated by large language models (LLMs). The authors propose two metrics, simulation generality and simulation precision, to measure the ability of an explanation to enable humans to infer the model's outputs on diverse counterfactuals. They evaluate state-of-the-art LLMs on multi-hop factual reasoning and reward modeling tasks and find that the explanations have low precision and do not correlate with plausibility. The authors suggest that naively optimizing for human approval may not be sufficient to improve the quality of explanations. The paper emphasizes the importance of building explanations that help humans build accurate mental models of model behavior."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This work introduces novel metrics to evaluate the quality of generated explanations by LLMs, with a focus on whether those explanations help humans build mental models of those LLMs.\n\n2) The paper is well written, with clear reasoning and explanations. The methodology and results are clear and easy to follow. \n\n3) The paper addresses an important issue on explainable generations. The proposed evaluation framework provides a valuable tool for assessing the quality of explanations. \n\n4) The experiments are conducted on two different tasks, multi-hop factual reasoning and reward modeling, and the results demonstrate the limitations of existing explanation methods and the need for improvement.\n\n5) The paper's findings and insights have the potential to shape future research and development in the field."
            },
            "weaknesses": {
                "value": "1) The paper does not compare the proposed metrics with other existing evaluation metrics for explanations. Without such comparisons, it is challenging to determine how the proposed metrics perform in relation to other approaches.\n\n2) The evaluation of counterfactual simulatability is limited to classification tasks, and there is a need to extend it to more complex generation tasks.\n\n3) The human simulation task is complex and subjective, leading to only moderate agreement among human annotators, raising concerns about the reliability of human evaluation. That's extended to LLMs as well. \n\n4) The study assumes that language models have some form of \"knowledge\", which is not actually the case. Language models don't \"know\" or \"understand\" information in the way humans do - they generate responses based on patterns they've learned from training data.\n\n5) Some citations do not follow the formatting instructions (When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \\citet{}... Otherwise, the citation should be in parenthesis using \\citep{})"
            },
            "questions": {
                "value": "1) The discussion of LLM simulators as proxies for human simulators is interesting, but it would be beneficial to provide more insights into the limitations of using LLMs in this role. What are the potential shortcomings or biases that LLM simulators may introduce? How well do they capture the full range of human reasoning and decision-making?\n\n2) Could you elaborate more on the \"Forced\" strategy you used for the sanity checks? \n\n3) Could the authors elaborate on how the counterfactual questions were generated? Did they follow specific patterns or were they entirely randomly created?\n\n4) Your evaluation method assumes that the knowledge contained in the explanation is the only information used by the model to make its decisions. How would your method account for scenarios where the model uses additional information not contained in the explanation to make its decision?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2760/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681311924,
        "cdate": 1698681311924,
        "tmdate": 1699636218694,
        "mdate": 1699636218694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qEaN6t0suO",
        "forum": "VvAiCXwPvD",
        "replyto": "VvAiCXwPvD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes counterfactual simulatability, which pertains to whether a human can predict a model's response to several questions when given its previous answer and explanation of a similar question. Due to the disadvantages of human evaluation, the authors leverage GPT-4 and GPT-3 for counterfactual generation and GPT-4 for imitating a human simulator. By leveraging this pipeline, they claim that they discover the defects of LLMs on simulatiprecision and the weak correlation of simulation precision and plausibility, which drive them to draw the conclusion that RLHF may not improve counterfactual simulatability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic of the paper, which can be potentially applied to solving the hallucination of LLM, is interesting and important to the current NLP field.\n2. The analysis of the experiments is abundant.\n3. The paper is clearly written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Whether GPT-4 can approximate human simulators remains a doubt. As demonstrated in Table 2, the authors state that the numbers reported under the columns of  H-GPT3 and H-GPT4 are calculated by the exact IAA with humans devided by the average IAA between humans. Therefore, the number is a percentage and the true number is much lower, which means that GPT-4 has similar agreement with humans as humans do with each other is not convincing.\n2. Within the same context, if GPT-4 exhibits a level of agreement with humans akin to that among humans themselves, it does not inherently establish its competence as an accurate representation of human cognition. An effective approximation should inherently exhibit a strong alignment with human behavioral patterns. In simpler terms, the findings merely attest to GPT-4's ability to replicate the same range of diversity observed among humans in their interactions with one another.\n3. The paper's underlying motivation is not explicitly articulated. The rationale for assessing such a metric remains obscure, as it exhibits characteristics reminiscent of the phenomenon of \"hallucination.\" Consequently, the apparent issue appears to revolve around the occurrence of hallucinatory responses to similar questions, which fails to underscore its novelty or distinctive contribution to the field.\n4. The conclusion of Table 3 is not correct. As shown in the table, GPT-mix demonstrates a higher similarity score compared with other models, therefore the conclusion should be GPT-mix generates more relevant counterfactuals but not more diverse ones.\n5. Typos: The 2nd paragraph of Section 3.1: \"Expectation\"  doesn't match with the formula listed below."
            },
            "questions": {
                "value": "Please refer to weaknesses section for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833573614,
        "cdate": 1698833573614,
        "tmdate": 1699636218625,
        "mdate": 1699636218625,
        "license": "CC BY 4.0",
        "version": 2
    }
]