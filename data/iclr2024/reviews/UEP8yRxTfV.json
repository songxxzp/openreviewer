[
    {
        "id": "x8ZK5ehTLs",
        "forum": "UEP8yRxTfV",
        "replyto": "UEP8yRxTfV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_kayo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_kayo"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates that the use of a constant loss weight strategy in traditional diffusion models leads to biased estimation during the training phase. To remedy this, this paper proposes a weighting strategy grounded in the theoretically unbiased principle to address this problem. Furthermore, it conducts a thorough and systematic investigation to analyze the inherent bias issue resulting from constant weight loss from multiple perspectives. Finally, the effectiveness of the method proposed in this paper is confirmed through experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.This paper exhibits a concise and lucid narrative style in presenting the methodology, rendering the proof process accessible to a wide readership.\n\n2.The proposed methodology unequivocally enhances the performance of diffusion models with much less training iterations and sampling steps.\n\n3.This paper offers a comprehensive exposition of the bias issue within traditional diffusion models, thereby facilitating a deeper comprehension of diffusion models."
            },
            "weaknesses": {
                "value": "1.This study exhibits a deficiency in the comprehensiveness of its experiments, and it lacks validation on commonly used benchmark datasets, such as CIFAR-10 and ImageNet. Is this attributed to limitations in the scalability of the proposed methodology? I suggest to conduct more experiments to prove the scalability of the proposed method.\n\n2.In equation (8), the coefficient $\\frac{1}{2\\sigma ^{2}}$ has been omitted. Though it has no effect on the overall proof, it is better to present this one.\n\n3.This paper, while providing an exposition on biased estimations in diffusion models, places greater emphasis on comparative illustrations with experimental results, thereby rendering the theoretical evidence somewhat less robust.\n\n4.How is the stability of the trainging process? Can you achieve consistent results in every experiment with identical initial conditions?\n\n5.What is the difference of the weighting schedule campare to [1]? It seems the two are very similar and [1] consider more complex situation.\n\n[1] J. Choi, J. Lee, C. Shin, S. Kim, H. Kim, and S. Yoon. Perception prioritized training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11472\u201311481, 2022."
            },
            "questions": {
                "value": "The same as Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Reviewer_kayo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698387116442,
        "cdate": 1698387116442,
        "tmdate": 1699636029959,
        "mdate": 1699636029959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1MRX92L345",
        "forum": "UEP8yRxTfV",
        "replyto": "UEP8yRxTfV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_dmLU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_dmLU"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the training weighting of the loss of diffusion models. They theoretically find that original constant weighting strategy is suboptimal, and further propose an improved training loss weight strategy. Besides, they give in-depth analyses of the sub-optimality of the constant weighting strategy from the perspective of existence, impact and reasons. The effectiveness of the proposed method is verified on several datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. It is an interesting idea to analyze the sub-optimality of the training loss weight. This paper theoretically reveal the inherent bias of constant weighting strategy, and propose a debiased principle on the design of the training weight.\n2. The in-depth analyses of the sub-optimality of the constant weighting strategy from the perspective of existence, impact and reasons are impressive and insightful. These analyses provide valuable insights and inspirations on the opague generation process of diffusion model.\n3. The experiments are solid. The proposed method gains substantial performance improvement on several datasets via simply modifying the training loss weight. The reproducibility is well guaranteed.\n4. This paper is well-organized and easy to read and understand."
            },
            "weaknesses": {
                "value": "1. Allocating higher weight to large t will improve the overall performance. While, what is effect of allocating lower weight at small t. It seems that the MSE error is slightly higher at t=0 than the constant weighting strategy in fig. 4.\n2. The visual difference between different baselines and the proposed method seems not obvious in fig. 5.\n3. Minor suggestions. The \u201cDDPM\u201d is also used to denote constant weight in fig. 7. The authors are advised to general denotation to avoid confusion."
            },
            "questions": {
                "value": "I am curious about the performance of treating the original image as training target and its comparison with noise-prediction mode."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698576515046,
        "cdate": 1698576515046,
        "tmdate": 1699636029889,
        "mdate": 1699636029889,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sgcDQyVMMj",
        "forum": "UEP8yRxTfV",
        "replyto": "UEP8yRxTfV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_kAkY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_kAkY"
        ],
        "content": {
            "summary": {
                "value": "This submission deals with the bias of using constant weights for the denoising loss of diffusion models in the training phase \u2026it first identifies the biased generation issue as a result of constraint weighting that results in artifacts such as poor details, global inconsistency, and color shift. It then proposes a new SNR-based weighting mechanism that lifts the diffusion error to the image space and thus debiases the generation. Experiments with FFHQ, AFHQ-dog, and MetFaces show significant FID gains and sampling steps compared with constant weighting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Improving the training efficiency and sampling quality of diffusion models is a timely topic\n\nExperiments and comparison are extensive and the gains are significant"
            },
            "weaknesses": {
                "value": "The paper misses related work in the literature that have already proposed the idea of SNR-weighting for diffusion models. Glancing through the literature, this reviewer found this related work in [1] that proposes the SNR weighting with the same justifications and derivations for sampling. The major novelties need to be clarified. In particular, the derivations from eq. 9 and 10 are already proposed in [1]. \n\n[1] Mardani M, Song J, Kautz J, Vahdat A. A Variational Perspective on Solving Inverse Problems with Diffusion Models. arXiv preprint arXiv:2305.04391. 2023 May 7."
            },
            "questions": {
                "value": "The abstract is vague and high level. The main idea which is the SNR-based weighting mechanism is not explained well. \n\nDo you use SNR weighting for the sampling phase as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719261482,
        "cdate": 1698719261482,
        "tmdate": 1699636029824,
        "mdate": 1699636029824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PJeAtofpPQ",
        "forum": "UEP8yRxTfV",
        "replyto": "UEP8yRxTfV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_sefk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1036/Reviewer_sefk"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the biased estimation problem of diffusion models, by examining the flaw in the $\\epsilon$-prediction. The authors also conduct several empirical analyses to support the bias effect. Empirically, the proposed objective achieves better FID scores across facial datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper examines the potential bias problem when using $\\epsilon$-prediction.\n\n- Empirically, the proposed weighting scheme outperforms previous ones."
            },
            "weaknesses": {
                "value": "- **No novelty**: It seems that the paper reinvents a well-established objective in the diffusion models literature -- $x_0$ prediction (see the blog https://medium.com/@zljdanceholic/three-stable-diffusion-training-losses-x0-epsilon-and-v-prediction-126de920eb73). The paper \"rediscovered\" the relation between $x_0$-prediction and $\\epsilon$-prediction. The proposed objective in Eq.11 is actually doing $x_0$-prediction type loss: by setting $\\epsilon_\\theta = \\frac{x_t-\\hat{x}_0}{\\sigma}$ and $\\epsilon=\\frac{x_t-x_0}{\\sigma}$ one could recover the $x_0$-prediction loss.\n\nOne step further, there are already works focusing on combining the strengths of $x_0$-prediction and $\\epsilon$-prediction, like the $v$-prediction [1] and the pre-conditioning techniques in EDM [2], in the past year.\n\n- The FID score in Table 1 is way too high in the small NFE regime (NFE<100). It makes the comparison much less convincing.\n\n\n[1] Progressive Distillation for Fast Sampling of Diffusion Models, Salimans et al.\n\n[2] Elucidating the Design Space of Diffusion-Based Generative Models, Karras et al."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1036/Reviewer_sefk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699135575989,
        "cdate": 1699135575989,
        "tmdate": 1699892054031,
        "mdate": 1699892054031,
        "license": "CC BY 4.0",
        "version": 2
    }
]