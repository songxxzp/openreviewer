[
    {
        "id": "1TDDS3QxoC",
        "forum": "Xkf2EBj4w3",
        "replyto": "Xkf2EBj4w3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_8PDP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_8PDP"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the stabilization of contrastive reinforcement learning (RL). Based on the paper \"Contrastive learning as goal-conditioned reinforcement learning\", the authors examine a variety of design elements, including:\n\n- Neural network architecture for image inputs\n- Batch size\n- Layer normalization\n- Cold initialization\n- Data augmentation\n\n\nThrough empirical evaluations conducted on the `drawer` task, the authors give a set of conclusions regarding these design choices. They name this combination of design choices as \"stable contrastive RL\".\n\nFurthermore, the paper compares the performance of their stable contrastive RL approach with previous methods, testing in both simulated and real-world scenarios. Additional experiments are also presented, showing distinct attributes of their proposed method.\n\nThe contributions of this paper go as follows:\n\n- It conducts an in-depth investigation of various design choices based on the paper \"Contrastive learning as goal-conditioned reinforcement learning\". Through experiments on the drawer task, the authors provide empirical evidence supporting their conclusions on the aforementioned design choices.\n\n- Beyond the primary focus, the authors also undertake additional experiments to further evaluate the properties and advantages of their proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper conducts a detailed examination of various design choices in contrastive reinforcement learning, providing readers with a holistic understanding.\n\n- By undertaking experiments on the drawer task, the paper grounds its conclusions in empirical evidence, adding credibility to its findings.\n\n- By contrasting the proposed method with existing ones in both simulated and real-world environments, the paper offers a clear benchmark of its advantages and potential shortcomings."
            },
            "weaknesses": {
                "value": "While the paper presents a lot of experimental findings, it largely feels akin to an experimental report with uncorrelated results. The major motivation remains ambiguous, making it challenging to discern the story the authors aim to convey.\n\nSpecific weaknesses include:\n\n- Motivation is not clear\n\t- The primary objective seems to be stabilizing contrastive RL. However, the question remains: Why is there a need to stabilize \"contrastive RL\" in the first place? The paper falls short in explaining the inherent instability of contrastive RL.\n    - Even if the stabilization of contrastive RL is deemed necessary, the rationale for examining specific aspects like architecture and batch size remains unclear.\n\n- Structure of the experiment section is messy\n\t- The experiment section, segmented into seven sub-sections, lacks clarity. The interrelation of these experiments is not explicitly explained, leaving readers questioning their significance.\n\t- Several experiments don't directly align with the paper's core focus on stabilizing contrastive RL. Queries arise such as:\n\t\t- Why introduce a comparison of pre-trained representation?\n        - What is the relevance of latent interpolation?\n\t\t- How does the arm matching problem fit into the story?\n\t\t- What motivates the test on generalization across unseen camera poses and object color?\n\t- The presentation in section 4.2 further complicates comprehension. While five design decisions are discussed in section 3.2, their individual results aren not distinctly showcased in section 4.2, leading to confusion.\n\n- Limited backbone algorithm\n\t- The entire experimental framework is based on the method \"Contrastive learning as goal-conditioned reinforcement learning\". This singular focus raises concerns about the generalizability of the conclusions.\n\n- Limited Task Support for Major Conclusions\n\t- Major conclusions stem from the `drawer` task experiments (Sec 4.2). Even though additional tasks are addressed in the appendix, the overall spectrum of tasks remains restricted.\n\t- Why not conduct the abalation study on all the benchmarks mentioned in Sec 4.1?"
            },
            "questions": {
                "value": "- See the weakness part of this review.\n- In Sec 3.2, it mentions \"using larger batches increases not only the number of positive examples (linear in batch size) but also the number of negative examples (quadratic in batch size)\". Could you further explain this point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Reviewer_8PDP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810881705,
        "cdate": 1698810881705,
        "tmdate": 1700734761175,
        "mdate": 1700734761175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qOl9FaY9hE",
        "forum": "Xkf2EBj4w3",
        "replyto": "Xkf2EBj4w3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_wQCi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_wQCi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method, stable contrastive RL, that is a variant of contrastive RL with boosted performances over previous contrastive RL and goal-conditioned RL methods.\nThey aim to tackle with real-world robotic tasks where previous contrastive RL methods have not been applied\nBut applying contrastive RL, self-supervised version of goal-conditioned RL, to real-world robotic tasks faces several challenges. \nSelf-supervised learning have been  making innovations in CV and NLP, but those recent innovations may not transfer to the RL setting\nStarting from design decisions  regarding model capacity and regularization, they found important and concrete design factors, such as the architecture, batch size, normalization, initialization, and augmentation, through intensive experiments.\nThey achieved +45% performance over prior implementations of contrastive RL, and 2\u00d7 performance relative to alternative goal- conditioned RL methods in simulated environments. Finally, they showed  that these design decisions enable image-based robotic manipulation tasks in real-world experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors use clear descriptions throughout the paper. Backgrounds,  related works, the definition of  the problem are all clear and sufficiently described and detailed.\nIn previous works heuristically designed visual representations are used without detailed designs and experiments of the visual  representations. The authors did  a great contribution to this field that they did a detailed designs of architectures and algorithms with intensive experiments. Those experiments support their claims well."
            },
            "weaknesses": {
                "value": "The authors did great experiments with variety of tasks with intensive analysis from multiple viewpoints.\nMore analytical descriptions in performance comparisons would contribute more to RL fields. \nFor example, in the simulation analysis of manipulation section, they used the simple sentence analysis,\" perhaps because the block in that task occludes the drawer handle and introduces partial observability. \", for worse performance. \nAlso, while pages are limited, some analysis of learned representations of stable contrastive RL in relation to performance combined with visual situations would help to understand those performance comparison analysis ."
            },
            "questions": {
                "value": "After reading through the paper, I understand the one of the aims and contributions of the paper is to apply contrastive RL to real-world robot applications. But most of the paper are descriptions for simulated situations. This may blur your aims and contributions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824826294,
        "cdate": 1698824826294,
        "tmdate": 1699636889490,
        "mdate": 1699636889490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pHYq7dA1L0",
        "forum": "Xkf2EBj4w3",
        "replyto": "Xkf2EBj4w3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_kFCy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_kFCy"
        ],
        "content": {
            "summary": {
                "value": "Contrastive reinforcement learning is a method for learning goal-conditioned policies from offline data. Prior works have had moderate success with training RL agents using contrastive learning; this paper increases the success rate of contrastive RL agents and deploys them in more domains. To that end, the paper describes a set of techniques, like choosing the right network size and changing the initialization of the last network layer, that greatly improve the agent's success rate. The method is tested in simulated and real-world robotic manipulation tasks as well as in a simulated locomotion task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper clearly describes several tricks that greatly increase the success rate of contrastive RL agents. In particular, it is interesting that the final layer initialization trick does better than learning rate warm-up.\n2. The experiments include a large number of relevant baselines, including baselines that are pre-trained on large video datasets.\n3. The authors identify the \u201carm matching problem\u201d, where the value function cares about the state of the robot arm but not the environment."
            },
            "weaknesses": {
                "value": "1. The paper demonstrates that contrastive RL is a brittle objective, since small changes in the network architecture and initialization lead to huge changes in agent performance. The paper does not study if the objective could be changed to make it more robust and less prone to overfitting.\n\n2. It is unclear why none of the methods can learn, e.g., the \u201cpush can\u201d policy in Figure 5. Overall, the paper does not do a good job of explaining why some of the seemingly simple tasks are difficult and why SOTA robot learning methods cannot solve them.\n\nMinor:\n* The second and third paragraph of the introduction could be more specific. E.g. \u201cvarious design decisions\u201d is too ambiguous.\n* \u201cHowever, to the best of our knowledge these contrastive RL methods have not been applied on real-world robotic systems.\u201d \u2013 https://arxiv.org/abs/2306.00958, https://arxiv.org/abs/2203.12601 use contrastive learning with offline RL and are deployed on real-world systems. This claim is only true if we use a very narrow definition of \u201ccontrastive RL methods\u201d."
            },
            "questions": {
                "value": "1. Could standard imitation learning approaches that use point clouds (like PerAct https://arxiv.org/abs/2209.05451, RVT https://arxiv.org/abs/2306.14896 and Act3D https://arxiv.org/abs/2306.17817) be able to solve the tasks in Figure 5 with high success rate?\n\n2. Why is it difficult to reach high success rates on the seemingly simple robot tasks in Figure 2 and 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7416/Reviewer_kFCy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698948347146,
        "cdate": 1698948347146,
        "tmdate": 1700696087094,
        "mdate": 1700696087094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FgDcKLCsso",
        "forum": "Xkf2EBj4w3",
        "replyto": "Xkf2EBj4w3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_8bYx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7416/Reviewer_8bYx"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to deploy contrastive reinforcement learning on real-world offline data. Towards this goal, the authors study various different design decisions in contrastive RL to improve its stability and performance. They consider architecture, data augmentation, and initialization. \n\nThey examine performance on simulated manipulation tasks and find a combination that outperforms prior work. They also compare against methods that use dedicated representation learning auxiliary losses and find that their stable contrastive RL outperforms these methods.\n\nNext, they use stable contrastive RL to learn goal-directed image-based manipulation tasks solely from real-world offline data. They find that it is more effective than baselines.\n\nNext, they examine the representation learned by contrastive RL compared to self-supervised methods and find that the representations learned by contrastive RL better capture the world dynamics.\n\nAdditional experiments that examine success misclassification, scaling laws, and generalization are also included."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is effective and cleanly applies various improves from the literature to contrastive RL to improve its performance.\n\nThe goal of improve real-world performance is shown by real-world experiments.\n\nThere are multiple additional experiments that provided additional insight and the reviewer found them to be useful additions.\n\nThe appendix contains many useful details and experiments."
            },
            "weaknesses": {
                "value": "The conclusion that a deeper CNN performs worse than a shallow one, likely because of overfitting, indicates that maybe the benchmark tasks used do not align with the paper's goal of leveraging a vast amount of unlabeled data, like current approaches in computer vision and NLP.\n\nThere are some missing citations of RL references for the design decisions. For instance, McCandlish et al and Bjorck et al showed that large batch training and layer normalization, respectively, are effective in RL.\n\nA full grid-search of all the combinations of design decisions would be great to have, but the reviewer understands that this may not be feasible.\n\n### References\n\nMcCandlish et al, An Empirical Model of Large-Batch Training, 2018\nBjorck et al, Towards Deeper Deep Reinforcement Learning with Spectral Normalization, NeurIPS 2021"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699237257240,
        "cdate": 1699237257240,
        "tmdate": 1699636889248,
        "mdate": 1699636889248,
        "license": "CC BY 4.0",
        "version": 2
    }
]