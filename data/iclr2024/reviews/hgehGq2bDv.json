[
    {
        "id": "fr9Rkz9foM",
        "forum": "hgehGq2bDv",
        "replyto": "hgehGq2bDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a one-shot/few-shot NeRF-based talking face system. The contributions are twofold: (1) a point-based expression field (PEF) for 3D avatar animation; (2) a multi triplanes attention (MTA) that supports multiple images as input to handle hard cases like occlusion or closed eyes. I like some ideas in this paper. This is a overall well-written paper and is easy to follow. The experiment shows good performance over previous baselines. \n\nHowever, the identity similarity in the demo video is not as good as previous one-shot 3D talking face methods (such as HiDe-NeRF). Besides, I'm also curious about the performance of this method under large head poses, which is not revealed in the demo."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I like the idea of PEF since it well utilizes the geometry prior of FLAME to help learn the avatar animation in the 3D space. \n- Also, it is the first one-shot 3D talking face paper that focuses on the few-shot setting.\n- The paper is well-written and is easy to follow."
            },
            "weaknesses": {
                "value": "- the PEF could well handle the segment modeled by FLAME (such as head and torso), but it cannot handles other parts, such as hair and clothes. See question 1.\n- The identity similarity in the demo video is worse than some baseline (HideNeRF).\n- The image quality can be improved. For instance, in Figure 1, the predicted images in the second column seems blurry."
            },
            "questions": {
                "value": "- In  PEF, the expression feature of facial part can be queried from the FLAME mesh, but the non-facial part, such as hair, clothes, and background is not modeled by FLAME. The authors said \"we instead search for neighboring points in the entire space\", but it is not clear how it bundles the non-facial part in the 3D space with the learnable features.\n- The identity similarity in the demo video is not as good as previous one-shot 3D talking face methods (such as HiDe-NeRF), what's the cause?\n- The head movement in the provided video is quite gentle. Is there any demo where the head pose is larger (such as side view)? Since one of the biggest advantage of 3D methods over the traditional 2D methods is the good quality under a large view angle, I think this is necessary for the reviewer to assess the performance of a 3D-based work.\n- Could you provide the visualization of the attention weights in the multi-reference setting? I'm also curious about the scalability of the MTA, how is the attention map looks like under the two-in/five-in/ten-in ?\n- In the demo video, the avatars are driven by audio, it is better to illustrate the way used to obtain the facial expression."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659394000,
        "cdate": 1698659394000,
        "tmdate": 1699636022426,
        "mdate": 1699636022426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DsijxVgigI",
        "forum": "hgehGq2bDv",
        "replyto": "hgehGq2bDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that reconstructs 3D head avatars from images and synthesizes realistic talking head videos. It takes as input a single or a small number of face images and reconstructs 3D head avatars in a single forward pass. It extends the formulation of NERFS and proposes a dynamic point-based expression field that is driven by a point cloud, motivated by the need to have an accurate control of facial expressions. In addition, the proposed method adopts a Multi Tri-planes Attention (MTA) fusion module that facilitates the 3D representation of the scene and the incorporation of information from multiple input images. The proposed method is compared with several SOTA methods and achieves promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed method achieves promising results and the supplementary videos show that the videos synthesized by the proposed method are in general realistic and visually pleasing. \n\n+ The experimental evaluation is detailed and systematic. The proposed method is compared with several recent SOTA methods that solve the same problem."
            },
            "weaknesses": {
                "value": "- The presentation in several parts of the paper, especially in the methodology, is unclear and needs several clarifications. See detailed comments in Questions below. \n\n- The following paper is not cited, despite the fact that it is very closely related in terms of methodology:\n\nAthar, S., Shu, Z. and Samaras, D., 2023, January. Flame-in-nerf: Neural control of radiance fields for free view face animation. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG) (pp. 1-8). IEEE.\n\nThe above un-cited paper also uses a FLAME-based representation of the 3D face and extends the formulation of NERFS to achieve realistic face animation with expression control. The similarities and differences with the proposed method are not discussed and the Flame-in-nerf is not included in the comparisons of the experimental section.  This raises concerns in terms of the real novelty and contributions of the proposed method. \n\n- Furthermore, there are also several other closely related works that are not cited. For example: \n\nJiaxiang Tang, Kaisiyuan Wang, Hang Zhou, Xiaokang Chen, Dongliang He, Tianshu Hu, Jingtuo Liu, Gang Zeng, and Jingdong Wang. Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. arXiv preprint arXiv:2211.12368, 2022.\n\nYudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In ICCV, pp. 5784\u20135794, 2021.\n\nYu, H., Niinuma, K. and Jeni, L.A., 2023, January. CoNFies: Controllable Neural Face Avatars. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG) (pp. 1-8). IEEE."
            },
            "questions": {
                "value": "- Section 3.2: the paper fails to clearly explain how the expression information affects the process of building a point-based expression field.\n\n- Section 3.3: the paper provides insufficient details about about how the canonical encoder is defined and built.\n\n- Figure 4: For several columns with results, it is unclear which is the corresponding method. There is one column more than the number of methods in the caption and one column more than the columns of Figure 5. This creates confusions. \n\n- Equation (1): the definition of w_i does not seem to make sense. Is there a missing norm in the denominator?\n\n- After Equation (2): N is refereed to as the \"input number\", but apparently it should be referred to as the number of input images"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As is the case for all methods of this field, the proposed method could be misused in order to create deep fake videos of a person without their consent. This raises issues of misinformation, privacy and security. Section 6 includes some discussion about this issues. However, this discussion could have been more detailed, with a more in-depth discussion of specific mitigation measures."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission965/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission965/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836823250,
        "cdate": 1698836823250,
        "tmdate": 1699636022340,
        "mdate": 1699636022340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u9kmW9LpNF",
        "forum": "hgehGq2bDv",
        "replyto": "hgehGq2bDv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission965/Reviewer_MrBB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission965/Reviewer_MrBB"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a novel framework named GPAvatar is introduced, designed to reconstruct 3D head avatars seamlessly from one or multiple images in a single forward pass. The key novelty lies in the incorporation of a dynamic point-based expression field, guided by a point cloud, to intricately and efficiently capture facial expressions. The authors present the concept of a dynamic Point-based Expression Field (PEF), enabling accuracy and control of expressions across different identities. Additionally, they introduce a Multi Tri-planes Attention (MTA) fusion module, capable of handling a varied number of input images with precision."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall, the paper is well-organized and easy to follow. The motivation is clear. The figures and tables are informative.\n\n- Experimental results demonstrate that the proposed method achieves the most precise expression control and state-of-the-art synthesis quality (StyleHeat, ROME, OTAvatar, and Next3D) (based on NeRF and 3D generative models)n on multiple on VFHQ and HDTF benchmark datasets."
            },
            "weaknesses": {
                "value": "- The model proposed has overall more trainable parameters compared to baseline models, which could potentially bring in some unfairness during comparison with other works.\n- No discussion about the limitations of the approach?"
            },
            "questions": {
                "value": "- It is not clear to me how the model captures the subtle information such as closed eyes ? \n- why the normalization of the weight wi in the equation is required?\n- Several terms in the equation (page 3) I_t= R (....) are not defined. What R means?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Since the proposed framework allows of head avatars, it has a wide range of applications but also carries the potential risk of misuse. The authors are also considering methods like adding watermarks to synthetic videos to prevent misuse."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699208416262,
        "cdate": 1699208416262,
        "tmdate": 1699636022261,
        "mdate": 1699636022261,
        "license": "CC BY 4.0",
        "version": 2
    }
]