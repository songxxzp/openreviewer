[
    {
        "id": "rCMhZ8CDzr",
        "forum": "RcANissyP4",
        "replyto": "RcANissyP4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an automatic evaluation method by measuring text-image faithfulness, showing an agreement with human evaluations in multiple benchmarks. By arguing that CLIP-score has severe drawbacks in DrawBench, for example, they argued that their method, SelfEval, solves or detours these issues, hoping for its role as a reliable and easy-access method to evaluate multimodal diffusion models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- They showed the limitation of CLIP-score in quantitative evaluation with extensive analysis. \n\n- One may exploit a strong text-to-image generativel model to evaluate others."
            },
            "weaknesses": {
                "value": "- W1. Novelty. The motivation of their method is reminiscent of the CLIP-R-Precision of Park et al. (2021), where the goal is to pick the right caption among distractors. Although the authors used the likelihood estimations to get a score for text-image matching, it can be seen as an ad-hoc method to replace the cosine similarity in CLIP. Since we cannot control the pretrained models where one is CLIP and the other one is diffusion models, the authors need to study the effectiveness of the proposed method extensively. The limitation of the CLIP score was also explicitly explored in MID [2].\n\n- W2. State-of-the-art comparison. For the text-image faithfulness, you could compare the state-of-the-art performance with DALL-Eval [1], MID [2], LLMScore [3], and VPEval [4]. The paper failed to cite them. Please allocate the dedicated paragraph in Sec. 2 to include them and compare the related works.\n\n[1] Cho, J. et al. (2022). DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers. http://arxiv.org/abs/2202.04053\n\n[2] Kim, J.-H. et al. (2022). Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. Advances in Neural Information Processing Systems 35. http://arxiv.org/abs/2205.13445\n\n[3] Lu, Y. et al. (2023). LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation. http://arxiv.org/abs/2305.11116\n\n[4] Cho, J. et al. (2023). VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation. https://arxiv.org/pdf/2305.15328.pdf"
            },
            "questions": {
                "value": "Could you let me know why you excluded the closely related works mentioned in Weaknesses W2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_M8yh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698036027370,
        "cdate": 1698036027370,
        "tmdate": 1700702773687,
        "mdate": 1700702773687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BpaZdaCGjf",
        "forum": "RcANissyP4",
        "replyto": "RcANissyP4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that can evaluate text-faithfulness of text-to-image generative models. The main idea is to employ the generative model itself for discriminative tasks, thereby evaluating the performance. Experimental results show that the proposed method can achieve a high degree of agreement for measuring text-faithfulness with human evaluations on several models and datasets, proving the effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method is simple but effective, which can be easily implemented and applied to other tasks.\n2. Experiments show the effectiveness and good performance for measuring text-faithfulness."
            },
            "weaknesses": {
                "value": "As the authors said, the formulation can extend to any conditional diffusion model. However, the current analysis and experiments only demonstrate that the method is effective for text-to-image generation tasks with diffusion models. \n\n1. Considering the method only evaluates the text-faithfulness without image quality in text-to-image generation tasks, the application scope will be very limited. Therefore, the contribution is not overwhelming and might not be enough for this conference, in my opinion.\n\n2. The experiments are also limited. Since the proposed method is an evaluation metric, trying it to evaluate other frameworks, such as VAEs and GANs, is also necessary. If the method can only be applied to diffusion models, it will be less valuable."
            },
            "questions": {
                "value": "Although diffusion models have become very popular recently, this framework has not become the only generative model. Meanwhile, other frameworks also perform well in text-to-image generation tasks. Therefore, I think the method has a limited application scope. In my opinion, the authors should try other conditional generation tasks rather than only text-to-image generation tasks, or try applying the method to other generative models.\n\nHowever, in addition to this problem, I cannot find other technical flaws or missed experiments. Thus, I tend to a marginal score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_jCGn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698200282050,
        "cdate": 1698200282050,
        "tmdate": 1699636062240,
        "mdate": 1699636062240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LEFtjyARKr",
        "forum": "RcANissyP4",
        "replyto": "RcANissyP4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for evaluating text-to-image generative models using the model's own capabilities. The proposed SELFEVAL approach computes the likelihood of real images given text prompts, enabling the generative model to perform discriminative tasks. This allows for fine-grained evaluation of the model's performance in attribute binding, color recognition, counting, shape recognition, and spatial understanding. Paper claims that SELFEVAL is the first automated metric to show a high degree of agreement for measuring text-faithfulness with human evaluations across multiple models and benchmarks. Unlike other metrics such as CLIP-score, which can show severe drawbacks when measuring text faithfulness, SELFEVAL offers reliable, automated evaluation for diffusion models without additional pre-trained models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method provides a way to automate the evaluation process, making it less dependent on often subjective and time-consuming human evaluations. It allows for detailed assessment of various aspects of a model's capabilities, such as attribute binding and color recognition. The ability to evaluate models without needing additional pre-trained models is a significant advantage.\n2. Sufficient evaluations of the proposed metric along with others are given. It shows consistency with human evaluations."
            },
            "weaknesses": {
                "value": "1. The performance of SELFEVAL is inherently linked to the effectiveness of the generative model itself, which could limit its reliability if the generative model has weaknesses in certain areas. From this perspective, more generative models should be evaluated.\n2. The authors don't discuss the potential limitations of their proposed method in detail.\n\nMinor issues:\n1) There may exist some misuse between \\cite and \\citep as some citation formats seem improper in the paper."
            },
            "questions": {
                "value": "1. How well does the SELFEVAL method generalize across different types of generative models and datasets?\n2. Can this method be employed in GAN, VAE, and flow-based generative models?\n3. Including a broader range of comparative studies with current methods for assessing the quality of text-to-image generative models could strengthen your arguments.\n4. How different text encoders impact the performance of SELFEVAL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1350/Reviewer_VHYX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786584301,
        "cdate": 1698786584301,
        "tmdate": 1700731812631,
        "mdate": 1700731812631,
        "license": "CC BY 4.0",
        "version": 2
    }
]