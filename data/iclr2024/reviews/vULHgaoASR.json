[
    {
        "id": "Nkz1mbwiCp",
        "forum": "vULHgaoASR",
        "replyto": "vULHgaoASR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
        ],
        "content": {
            "summary": {
                "value": "In the paper, the authors first comment on the limitations of two existing frameworks: (1) impute and then generate framework is constructing a complete training dataset first, but is biased. (2)generate then impute framework needs MCAR and MAR conditions.  The authors propose a simple MissDiff algorithm that utilizes a masking function on the loss function to handle the missing data values. The authors examine the effectiveness of MissDiff, by theoretically verifying that MissDiff can learn the oracle score on the complete data. Furthermore, MissDiff is an upper bound for the negative likelihood on the observed data.  The experiment shows that MissDiff performs comparably better to various imputation approaches in the imputation tasks, and MissDiff is performing better than impute-and -then-generate or generate-then-impute frameworks training with vanilla diffusion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The methodology is given in a clear manner; the proper notations are used and the authors provide solid explanations, examples and proofs to each of the claims.\n\nS2. The authors have provided a theoretical proof to justify the effectiveness of the masking mechanism by examining the loss functions and verifying that optimal solution of MissDiff can learn the oracle score on the complete data and MissDiff is also the upper bound of the negative likelihood of the generative model on the observed data.\n\nS3. The simulation results seem promising. The authors have provided convincing figures to show that under fidelity evaluation, MissDiff performs better than Diff-delete and Diff-mean under different data missing scenarios."
            },
            "weaknesses": {
                "value": "W1. The experimental setting for Experiment 1 should be detailed described in the main text or appendix. It is confusing to the reviewer how MissDiff\u2019s RMSE and error rate are provided (if the MissDiff is doing unconditional generation according to Algorithm 2), and what is the training hyper parameters that are associated with MissDiff in this scenario? It is also confusing how RMSE evaluation of MIssDiff on MINIC4ED dataset is acquired (see Table 3). The authors should consider providing more experimental details.\n\nW2. Table 2 shows that MissDiff\u2019s utility is significantly lower than Diff-mean for column missing scenario, the authors should have provided an explanation for the inferior performance of MissDiff for column missing Census data.\n\nW3. In terms of writing, Algorithm 2 is not different from the existing variance preserving sampling method. Therefore, it is redundant to include Algorithm 2 in the main text. The reviewer suggest to move the Algorithm 2 into the appendix."
            },
            "questions": {
                "value": "Q1. What do the numbers mean in the parenthesis for Table 1?\n\nQ2. The authors mention that they hope to evaluate MissDiff with more complicated data types such as video or language data, but the reviewer is wondering how missing values are defined in videos and images. It seems trivial to handle the missing values in video and images due to the amount of redundant information the videos and images contain. 70% of missing pixels can still generate reliable images in masked auto-encoders. For languages, missing values are also beneficial to generative models. Masked-out tokens are important for language model training, \n\nQ3. The reviewer is wondering how MissDiff can handle the categorical features and nonnumerical features in Tabular Data. While the categorical features and non-numerical features could be treated as continuous features according to Equation 4, the performance is not as good as directly performing discrete diffusion models (D3PM [1], tauLDR[2]). Could MissDiff be adapted to discrete diffusion models?\n\n[1] Austin, Jacob, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2023. \"Structured Denoising Diffusion Models in Discrete State-Spaces.\" arXiv preprint.  https://arxiv.org/abs/2107.03006.\n\n[2] Campbell, Andrew, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, and Arnaud Doucet. 2022. \"A Continuous Time Framework for Discrete Denoising Models.\" arXiv preprint.  https://arxiv.org/abs/2205.14987."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/a."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JZFN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771817246,
        "cdate": 1698771817246,
        "tmdate": 1699636466662,
        "mdate": 1699636466662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1ZnNyasXu4",
        "forum": "vULHgaoASR",
        "replyto": "vULHgaoASR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_bE5M"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new diffusion-based generative framework, MissDiff, specifically designed for learning from incomplete data and generating synthetic complete data. They highlight the common issue of incomplete data in various real-world applications, such as healthcare and finance, particularly when dealing with tabular datasets. The authors illustrate the drawbacks of existing two-stage inference frameworks, noting they are either biased or computationally taxing.\n\nThe proposed solution, MissDiff, is presented as a unified and computationally friendly framework. It models the score of complete data distribution by denoising score matching on data with missing values. The authors provide a theoretical justification for MissDiff's effectiveness and stress that the proposed training objective serves as an upper bound for the negative likelihood of observed data.\n\nIn the case of incomplete training data, MissDiff can be employed for synthetic data generation and missing value imputations based on the learned generative model. The authors conclude by stating that extensive experiments on imputation tasks and generation tasks show that MissDiff outperforms existing state-of-the-art approaches on multiple tabular datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Good coverage of existing methods\n- Certainly a practically relevant problem to address."
            },
            "weaknesses": {
                "value": "- The technical description is difficult to follow, e.g. in section 2.2. or 3.2. I think it can be well understood by someone who is already familiar with the material but it doesn't do a great job building this up for the less versed reader.\n- 1.1. \"evidential low bound\", I think you mean evidence lower bound.\n- How do you tune hyperparameters, especially for competing methods? I think it's hard to compare against baselines without this.\n- Claiming that existing approaches are either biased or expensive seems very general.\n\n## Stylistic critique\n- The abstract is a bit vague, e.g. \"beyond missing value imputation\" - not sure what that means.\n- Intro: \"It is known that ...\" remove. Just state the claim.\n- Footnote 3, you mean to say you \"use them interchangeably\"."
            },
            "questions": {
                "value": "- I don't think that work like the VAEs for missing data can be described as \"generate-than-impute\". In (at least some) of these models, there is no separate imputation step, but they learn a joint density marginalizing over missing value. I may be wrong. Can you clarify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782369499,
        "cdate": 1698782369499,
        "tmdate": 1699636466577,
        "mdate": 1699636466577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xLloG895Tm",
        "forum": "vULHgaoASR",
        "replyto": "vULHgaoASR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_EFAN"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the diffusion model in the presence of missing data. The basic idea is to compute the score matching objective on observed entries. The authors proved that the solution is the true score function and this objective upper bounds the negative log-likelihood. Experiments are conducted on tabular data imputation and generating new data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a new method to train a diffusion model on missing data. The training objective is simple and the authors provide theoretical guarantees."
            },
            "weaknesses": {
                "value": "1. There are several inaccurate claims about previous literature and related work, which decrease the validation of the paper.\n- In Section 1.1, why did the authors claim that Mattei & Frellsen (2019); Ipsen et al. (2020b) train multiple decoders? From my understanding, they proposed multiple imputation via sampling multiple latent factors, rather than training multiple decoders.\n- Ipsen et al. (2020a) was published in ICLR 2022.\n- In the last line of Page 3, the authors said the references require MCAR assumption. However, I suppose most of them assumed the MAR case. Moreover, one of the contributions of Li et al. (2019) is to deal with NMAR data.\n2. The authors did not mention what kind of missing mechanism was used in Section 4.1. Moreover, it seems that they only compare with baselines under one missing mechanism in this section.\n3. Why did not the authors compare with CSDI_T in Section 4.2? Why are the results of STaSy not included in Table 4?"
            },
            "questions": {
                "value": "While Tashiro et al. (2021) adopted conditional score matching, the work goes back to unconditional score matching. Can the authors provide more discussions about these two approaches and what are the advantages of using unconditional scores?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816530263,
        "cdate": 1698816530263,
        "tmdate": 1699636466479,
        "mdate": 1699636466479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Az86tyZxrE",
        "forum": "vULHgaoASR",
        "replyto": "vULHgaoASR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
        ],
        "content": {
            "summary": {
                "value": "The authors of the paper developed MissDiff, a diffusion-based method for imputing incomplete tabular data under various missingness scenarios (MCAR, MAR, MNAR) and generating data. MissDiff most appealing feature is that it imputes missing values learns the (estimated) generative distribution (i) at a unified (i.e., not multiple-stage) manner, and (b) without the computational burden of training additional networks, as opposed to the corresponding state-ot-the-art methods. The authors performed experiments comparing their method to others both with respect to data imputation and generation tasks across several datasets where, in the majority of the experiments, their method outperfomed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "--method handles tabular data;\n--method imputes and generates data in without needing to train extra networks or diving the task into imputing and generating;\n--both theoretical and experimental justification is presented;\n--method seems to perform well under various missingness scenarios;"
            },
            "weaknesses": {
                "value": "-- inconsistency of word \"complete\" : page 3, paragraph 2.1 : \"complete d-dimensional data ... m=(m_1, ...,m_d) in {0,1}^d\",  page 4, \"framework for learning on incomplete data\"\n-- more metrics need to be presented at the experiments section, eg RMSE is not necessarily representative in high-dimensional cases were the distribution is complex-multimodal;\n-- typos"
            },
            "questions": {
                "value": "-- at fidelity testing (section 4.3, Figure 1) it seems that the method's performance improves as missingness rate increases in (0.1-0.6) and then decreases in the row- and column- missing setups: could you provide some insights on why this happens (also could you also add the case where missingness rate is 90% in the first case) ?\n-- at table 2 it seems that MissDiff and Diff-Mean seem to have similar performance -- could you comment on that and explore more datasets?\n-- could you compare your method with VAE- and GAN-based methods as well (more specifically Mattei and Frelsen, Li and Marlin) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4832/Reviewer_JCWu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699622952374,
        "cdate": 1699622952374,
        "tmdate": 1700701651404,
        "mdate": 1700701651404,
        "license": "CC BY 4.0",
        "version": 2
    }
]