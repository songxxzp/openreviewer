[
    {
        "id": "KsEIh8F5Ur",
        "forum": "PQzdtBiKie",
        "replyto": "PQzdtBiKie",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_jKTM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_jKTM"
        ],
        "content": {
            "summary": {
                "value": "This paper purports to apply fractal theories to language. They claim that language is \u2018self-similar\u2019 at all levels of granularity and \u2018long-range dependent\u2019. Through a few experiments with the PaLM language model and a few disparate datasets, a few hints of self-similarity, narrowly defined, are given. A connection to scaling laws is also briefly discussed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- It is good that four datasets were considered. It is not clear why the 4K word token cutoff is important."
            },
            "weaknesses": {
                "value": "- With regards to the supposed value of Figure 1 \u2014 a single (tripartite) anecdote showing self-similarity would not nearly be enough to show a trend, but this graph doesn\u2019t even show unambiguous similarity \u2014 the mere presence of \u2018burstiness\u2019 is not sufficient evidence. Moreover, the H\u01d2lder exponent of 0.638 is claimed as sufficient to confirm self-similarity but no context is provided to contextualize this single statistic.\n- The amalgamation of fractals and intelligence seems somewhat unmotivated from either an empirical or a deductive perspective.\n- It seems that the authors are claiming, by means of their experimental setup, that PaLM-8B represents all of \u2018language\u2019. Later, PaLM-540B was used (in Sec 2.5) to measure \u2018more accurate\u2019 probability distributions, but wholly other architectures should have been considered and compared, too. Results should be broken down by various language families.\n- The relationship between the predictability of the form that would emerge from fractal structures and those that have been approached by Zipf, Shannon, and their respective camps should be explored. I.e., despite the brief mentions of Zipfian distributions and entropy, they are not contrasted at all with the theory behind the current work; it is not clear if the authors mean to present their line of work as an alternative to well-established methods, but any deeper comparison is advised.\n- Attempting to draw links between language and, say, phenomena of large bodies of water is very tenuous when the only connection is that they each share a Hurst parameter of 0.75. The authors are recommended to establish more concrete outcomes in their empirical results and conclusions, relevant to either understanding the learning process, improving outcomes, or interpreting outcomes of LLMs."
            },
            "questions": {
                "value": "- \u2018Creative content creation\u2019?\n- Is it the case that LLMs <<prompted some researchers to ponder if there was more to intelligence than\n\u201con-the-fly improvisation>>?\n- you seem to have defined the $t-1^{st}$ word in terms of itself (and its prefix) in the experimental setup. You have also seemed to apply the Markov property with length $L=1$ in your definition of $z_t$. Are these true?\n- By means of the $\\tau$-increments, are you suggesting there are fixed, expected lengths for clauses, sentences, paragraphs (and then presumably\u2026sections? Chapters? Articles/novels/scripts/\u2026?).\n- Could you please check your references for completeness (e.g., Aref, 1998; Bubeck et al, 2023)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675043279,
        "cdate": 1698675043279,
        "tmdate": 1699636372873,
        "mdate": 1699636372873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6h8PqwZotF",
        "forum": "PQzdtBiKie",
        "replyto": "PQzdtBiKie",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_iZo5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_iZo5"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to quantify and characterize self-similarity and long range dependency in language (token sequences) by borrowing concepts from analysis of sequential processes like Weiner process/ Brownian motion, fractal processes etc. Specifically, the natural language sequences/sentences are represented as a sequence of negative conditional logprobs (related to bits of information per token) of the tokens obtained via autoregressive language models. The actual token identities are ignored and only the conditional probability under the language model is considered. Using this representation, \u201cincrement\u201d and \u201cintegral\u201d processes are defined to estimate the degree of self-similarity and long-range dependency via estimating the appropriate parameters (\u201cself-similarity exponent\u201d and \u201cHurst coefficient\u201d) via standard methods for analysing real-valued sequential processes. The language model used for this purpose is Palm-8B and 4 language corpora (Big Patent, Newsroom, Scientific papers, Wikipedia) are analyzed using the proposed method. The estimated coefficients indicate that language exhibits self-similarity i.e. it exhibits burstiness over different granualrities (sentence/paragraphs and so on), and it also exhibits long range dependency. This analysis is extended to varying model sizes and training and inference context lengths. Overall higher degree of self-similarity and long range dependence is observed as the model size and context length is increased."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2013 This is a fresh perspective on analysis of natural language and autoregressive language models. It provides connection to insights about the hierarchical nature of language and interconnected and recursiveness in language.\n\n\u2013 The paper is well-written and the technique is clearly described. The approach is well-motivated and connections to prior work and techniques are illuminating.\n\n\u2013 The experiments are well-designed. The results show robustness across a variety of domains and the relationship to scaling laws substantiates the understanding about how language models improve with scale."
            },
            "weaknesses": {
                "value": "\u2013 The identity of tokens is ignored in the analysis and the representation is just a sequence of negative logprobs. While this is still indicative of some macro features of language, this analysis is limited to an impoverished representation of language.\n\n\u2013 As an upshot of the point above, this analysis doesn\u2019t provide detailed insight into the exact latent hierarchical organization of language and is limited to merely an indication that language exhibits burstiness over long-range. I am unable to imagine how these concepts and methods can be extended to yield deeper insights about language and LMs beyond this paper.\n\n\u2013 While this is a fresh perspective, the main finding that next-token prediction is a difficult task because language tends to show burstiness at varying granularities and exhibits long-range dependence is not surprising or novel. For example, the hierarchical and recursive nature of language is an actively studied area of research in cognitive sciences and linguistics.\n\n\u2013 \u201cSelf-similarity\u201d and \u201cfractalness\u201d are not identical concepts. The findings in fact suggest that language is not necessarily fractal which makes predictability challenging. This is acknowledged in the paper but yet the two concepts seem to have been used interchangeably in the discussion."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699048653191,
        "cdate": 1699048653191,
        "tmdate": 1699636372783,
        "mdate": 1699636372783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iqAcffMSGH",
        "forum": "PQzdtBiKie",
        "replyto": "PQzdtBiKie",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_hBB9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4084/Reviewer_hBB9"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the fractal structure of language and makes connections between self-similarity, long-range dependence, and the capabilities of large language models (LLMs). Fractal parameters like the Hurst exponent and self-similarity exponent are estimated on text datasets. The key ideas are that the self-similar nature of language allows LLMs to capture patterns across levels of granularity, while long-range dependence requires modeling broader contexts spanning indefinitely into the past. Experiments with PaLM on different datasets (e.g., Wikipedia, patent) empirically estimates fractal parameters like the Hurst exponent and self-similarity exponent on text datasets, finding values that suggest the presence of self-similarity and long-range dependence in language."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) The perspective of analyzing language through fractal patterns and quantifying self-similarity and long-range dependence provides a novel angle for understanding LLMs. While intuitions about self-similarity have existed, this paper makes them concrete and rigorous.\n\n(+) The analyses seem technically sound overall. Estimation of fractal parameters is done carefully and situated within the literature. \n\n(+) these insights about fractal structure connecting to capabilities of LLMs could potentially be an important conceptual advance. It formalizes useful notions about complexity and long-range dependencies in language."
            },
            "weaknesses": {
                "value": "(-) The connection between fractal structure and capabilities of LLMs is suggestive but not conclusively proven. The arguments linking self-similarity to modeling patterns across granularities seem reasonable but remain speculative without more analysis.\n\n(-) The analysis is conducted solely on text datasets encoded using the PaLM language model. The conclusions may not generalize to other modalities or other LLMs, which is a limitation. Evaluating universality across models and data types could strengthen the results. Also, only PaLM-8B was examined, while the comparison between small and large LMs provides additional insight beyond confirming consistency. The pretraining experiment is quite brief. More in-depth studies could be beneficial."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4084/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699274962282,
        "cdate": 1699274962282,
        "tmdate": 1699636372717,
        "mdate": 1699636372717,
        "license": "CC BY 4.0",
        "version": 2
    }
]