[
    {
        "id": "dgmGmkkQZB",
        "forum": "YjG29CIOP7",
        "replyto": "YjG29CIOP7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_iMtM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_iMtM"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the robustness of a meta-learning approach called data-free meta-learning (DFML), which uses only pre-trained models. The paper argues that DFML is subject to two risks: (i) task-distribution shift (TDS) and (ii) task-distribution corruption (TDC). TDS is the risk of meta-learning over-fitting due to over-reliance on the new task. TDC is the risk of contamination of pre-trained models that differ from the provided explanations (model-poisoning attack), resulting in poor meta-test performance. The paper addresses these two risks through (i) interpolated task-memory replay, which prevents overfitting by mixing past tasks with the current task, and (ii) robust model selection policy, which uses reinforcement learning to score model reliability for model selection. Experiments on representative Few-shot learning datasets show that the proposed method is useful to obtain a certain robustness for TDS and TDC. However, the risk of TDC is not practical because the model-poisoning attack can be avoided by lightweight sanity check with input data inverted from pre-trained models or generated by text-image generative models like Stable Diffusion. In my opinion, the paper should be resubmitted by discarding the claims about TDC because the risk of TDS is sufficient to discuss using the whole of the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper discusses robustness for the first time in the relatively new meta-learning problem setting of DFML.\n+ The paper proposes an effective method for replaying past tasks in DFML and confirms its effectiveness through experiments.\n+ The paper proposes a reinforcement-learning-based model selection method that can maintain meta-testing performance even when unreliable models are included."
            },
            "weaknesses": {
                "value": "- The risk of TDC is avoidable through sanity checks with simple data collection, and thus there is little motivation to introduce the proposed method with high computational complexity using RL. In fact, the experiments provided in the paper use pre-trained models in the medical domain for the attack, but these are not difficult to determine by simply inputting data obtained from an image search engine or a text image model such as Stable Diffusion without privacy concerns. It is unnatural for the problem setting to allow data inversion for meta-learning but not to allow such a simple preprocessing. We could not agree with the practicality of TDC's problem setting unless it shows important use cases or experimental results, indicating it cannot be solved without introducing the proposed method.\n- The replay-based method in TEAPOT has already been well studied in the context of continual learning [a], and there is nothing new about it other than the task (i.e., DFML) to which the method is applied.\n- Section 4 is misleading because of the mixture of existing and proposed methods. In particular, the second paragraph is almost the same as in the previous study [b].\n- Experiments are conducted only on 4-layered convolution neural networks, and the performance is not verified in a realistic setting with heterogeneous architectures. Since the paper considers a practical problem setting for DFML, experiments using more architectures would help make the paper more convincing.\n\n[a] Wang, Liyuan, et al. \"A comprehensive survey of continual learning: Theory, method and application.\" arXiv preprint arXiv:2302.00487 (2023).\n\n[b] Hu, Zixuan, et al. \"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning.\" CVPR 2023."
            },
            "questions": {
                "value": "- Can the risk of TDS be avoided by splitting the inverted dataset for validation? In the context of generative models, methods for validation and model selection using synthetic data have been proposed [c].\n- Can model-poisoning attacks be detected as OOD by visualizing or embedding data recovered from a pre-trained model into an existing model? The previous study [b] actually visualized the restored data, and pre-trained neural networks can be used for OOD detection [d].\n- How much does the computational complexity of ROSY reinforcement learning increase relative to the baseline (PURER)?\n\n[c] Shoshan, Alon, et al. \"Synthetic data for model selection.\" ICML 2023.\n\n[d] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" NeurIPS 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698314550671,
        "cdate": 1698314550671,
        "tmdate": 1699636265204,
        "mdate": 1699636265204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UVoZgYlNaz",
        "forum": "YjG29CIOP7",
        "replyto": "YjG29CIOP7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_eZp4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_eZp4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes improved data-free meta-learning algorithms for handling task distribution shifts and adversarially injected models. TeaPot maintains a memory bank of old tasks to mitigate distribution shift, and RoSy ranks models according to a learned reliability score to prune out bad models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper empirically evaluates the proposed methods and shows substantial performance and robustness gains. Table 2 considers two meta-learners (ANIL, ProtoNet) and peak and last-iteration performance."
            },
            "weaknesses": {
                "value": "The paper has major clarity issues.\n- The text uses a lot of jargon and acronyms (TDS, TDC, DFML, RML, RDFML, MPA, PR...), many of which I was unfamiliar with despite actively participating in meta-learning or robustness research for years. It took a lot of work to parse on my first pass. I think the general ICLR audience would find this paper hard to understand. I'd strongly suggest a major rewrite to make the paper easier to read through, especially given that many of these concepts are simple enough to write in plain text.\n- The paper proposes two methods (TeaPot and RoSy), and it's not clear how these fit together into a cohesive contribution. The paper describes TeaPot as a \"DFML baseline\" for handling distribution shifts over tasks (TDS) and calls RoSy a defense strategy for adversarial models (TDC). \n\nI'm not convinced of the relevance of the model poisoning attack scenario (TDC, MPA). This assumes that a malicious attacker publicly releases a model that is a valid classifier but with the wrong task description. How realistic is this scenario, and isn't such a \"poisoned model\" easy to detect in practice by testing on some data that the model claims to work on? If the problem setting is that these models are actually posted online, wouldn't such \"poisoned models\" be quickly flagged through GitHub issues or Huggingface community discussions by someone who tried to use the model but failed? Or are there common black-box \"model pools\" that I'm not aware of?\n\n(minor) Fig 4 is really hard to parse because it tries to combine two axes of variation (shot, MPA) into one. Can't you just have two 2x2 grids side-by-side, where for example the two grids are 1-shot and 5-shot?"
            },
            "questions": {
                "value": "Is the paper meant to be about solving two different problems? It currently feels like two papers combined into one."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698536393130,
        "cdate": 1698536393130,
        "tmdate": 1699636265114,
        "mdate": 1699636265114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "urW38TbGhl",
        "forum": "YjG29CIOP7",
        "replyto": "YjG29CIOP7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_j2rx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_j2rx"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that current data-free meta-learning methods suffer from two drawbacks steming from constucting pseudo tasks.\nIn order to handle these issues, this paper proposes a memory-based DFML baseline and a defense strategy to select reliable models. The effictiveness of method was demonstrated through few-shot learning tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper reveals the distribution of pseudo tasks will change as the learnable dataset gets updated.\nMoreover it introduces model poisoning attack as a training-time attack by injecting malicious OOD models.\nTo handle the task-distribution shift, it introduce a task-memory bank and constructs a interpolate N-way task\nrepresenting a mixture of old tasks. Furthermore, to defense the MPA, it proposes a model selection policy\nas a learnable weight vector and formulate the denfense objective as a bi-level optimization problem."
            },
            "weaknesses": {
                "value": "- The implementation details of the interpolated task-memory replay is not well demonstrated, but in the paper it plays an important role in face of TDS.\n- The paper argues it is a data-free meta-learning, but to aviod the MPA, it still need an additional validataion tasks.\n- To address the TDC and TDS, it essentially needs three stage training. It needs pseudo task recovery, meta-training with task-memory replay, and the policy optimization, and thus the  total training time must be long.\n- The key process is pseudo tasks recovery, but the generation performence with classification loss and the regularization term is not figured out.\n- The distribution of the pesudo tasks is lack of explaination, and the Figure 1 shows the degradation of the meta-testing accuracy, but it may not caused by the task-distribution shift."
            },
            "questions": {
                "value": "- Could authors figure out the distribution shift happened in the meta-training phase?\n- Why does the task generator trained with such two loss function work well? Could the author give a reasonable and foundamental explaination?\n- Could authors clarify how to interpolate the old tasks?\n- What's the total time need to take to achieve the performence?\n- Why does the fine-tune performs so bad compared with other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675354434,
        "cdate": 1698675354434,
        "tmdate": 1699636265045,
        "mdate": 1699636265045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WBj6PVx26H",
        "forum": "YjG29CIOP7",
        "replyto": "YjG29CIOP7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_ywQB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3177/Reviewer_ywQB"
        ],
        "content": {
            "summary": {
                "value": "Data-Free Meta-learning (DFML) focuses on quickly learning new tasks without using original training data by leveraging a collection of pre-trained models. Traditional DFML approaches create pseudo tasks from a learnable dataset that is derived from these pre-trained models. This method has two main issues: Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). TDS arises when the pseudo task distribution changes as the learnable dataset evolves. TDC, on the other hand, occurs when deceptive models are deliberately added to the collection. To counter these problems, the paper introduces a robust DFML strategy. Firstly, authors present TEAPOT, a memory-based DFML solution to manage TDS by keeping older task memories and diversifying the pseudo task distribution. Secondly, to defend against TDC, they suggest a Robust Model Selection Policy (ROSY) that assesses model reliability and is compatible with current DFML methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper adeptly identifies and addresses the significant challenges of task-distribution shift (TDS) and task-distribution corruption (TDC) within data-free meta-learning.\n\n2. The paper astutely recognizes the prevailing issue of task-distribution shift, particularly spotlighting the shortcomings in prior research like PURER. As a remedy, the authors introduce a novel method that maintains a memory of previously generated distribution samples.\n\n3. The introduction of the task-distribution corruption problem is a pioneering effort in the data-free meta-learning context. The provided examples and rationale eloquently underscore its importance and the pressing need for an effective solution.\n\n4. The empirical evidence presented is thorough, conclusively demonstrating the superiority of the proposed method under the aforementioned challenges."
            },
            "weaknesses": {
                "value": "The benchmarking approach against the prior SOTA (PURER), which utilizes DeepInversion for data synthesis, raises questions. In the proposed method, data specific to the task is synthesized using a generator. It remains ambiguous, particularly in Table 2, whether the authors employed consistent sample-generating strategies for both PURER and their proposed method. For a more equitable comparison, it would be beneficial if the authors utilized a uniform sample-generating approach, be it DeepInversion, the generative model, or both, for both PURER and their method."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3177/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3177/Reviewer_ywQB"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816222778,
        "cdate": 1698816222778,
        "tmdate": 1699636264971,
        "mdate": 1699636264971,
        "license": "CC BY 4.0",
        "version": 2
    }
]