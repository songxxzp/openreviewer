[
    {
        "id": "062KWuxp1I",
        "forum": "HBEjrlu7Aa",
        "replyto": "HBEjrlu7Aa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_v2XH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_v2XH"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors try to address insufficient scene diversity for visual 3D object detection in autonomous driving. They propose an object-level data augmentation workflow, by reconstruction and synthesis. To do this,  they split and accumulated the foreground and background of the global pointcloud first. Then, they train the learnable point descriptors and the neural renderer to recover the original scene image. In the synthesis step, they perform object-level transformations on accumulated point clouds and utilize the trained renderer to synthesize augmented images and their labels."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors challenge the core problem of machine learning, i.e., the insufficient diversity of data samples. They try to generate more data samples by reconstruction and synthesis. They propose a *cell-based z-buffer* algorithm to remove the occluded points and solve the *bleeding problem*.  I think the proposed solution is simple and straightforward. Thus, I think the proposed approach is easily reproducible."
            },
            "weaknesses": {
                "value": "I have some concerns about the proposed method and experiment result:\n1. Is there any safeguard to guarantee the augmented data is located in reasonable regions?\n2. How does the algorithm control the appearance style of the added object? Is there a way to generate an unseen appearance from the training data set?\n3. The PSNR of the synthesis image is <30 dB, which means the human eye distinguishes the synthesis image from the original image. It raises a question in my mind: does the quality of the synthesis data affect downstream tasks? An experiment that compares the results trained within the original image and the synthesis data is appreciated."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698481195994,
        "cdate": 1698481195994,
        "tmdate": 1699636904826,
        "mdate": 1699636904826,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "154Oh1VrxI",
        "forum": "HBEjrlu7Aa",
        "replyto": "HBEjrlu7Aa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_m1gL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_m1gL"
        ],
        "content": {
            "summary": {
                "value": "Visual 3D object detection is an important task in the 3D perception system. However, labeling 3D data is extremely expensive and the diversity of training data plays an important role in deep learning. To this end, this work proposes a rendering-based instance-level data augmentation method for image-based 3D detection. In particular, this method extracts image features from sequences and aligns them with associated LiDAR point clouds, and augments the objects in the 3D space. After that, a neural scene renderer is adapted to generate the 2D images with the augmented objects. The experiments on the nuScenes with two base detectors show the effectiveness of the proposed data augmentation strategy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The work is well-motivated. The diversity of training data plays an important role in deep learning, and conducting data augmentation is really hard for 3D detection due to the geometric consistency. Therefore, exploring effective 3D data augmentation methods is a meaningful topic.\n- The experiments on nuScenes dataset show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The overall data-augmentation process is complex, involving multiple additional data, such as point cloud, sample sequences, external renders, etc. Considering this, performance improvement is limited, maybe evaluating a smaller dataset, such as KITTI,  can better show the effectiveness of this data augmentation method.\n\n- Besides, the computation cost will be inevitably increased. The cost of generating the new training samples should be reported.\n\n- The authors only report the results on the nuScenes validation set, and the test set should also be presented.\n\n- ROI-10D [1] also provides an instance-level data augmentation method for image-based 3D detection. Discussion and comparison should be included.\n\nReferences:\n[1] ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape, CVPR'19 (also see the supp)"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674344100,
        "cdate": 1698674344100,
        "tmdate": 1699636904675,
        "mdate": 1699636904675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9sMHauon2d",
        "forum": "HBEjrlu7Aa",
        "replyto": "HBEjrlu7Aa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_ZS78"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7495/Reviewer_ZS78"
        ],
        "content": {
            "summary": {
                "value": "The authors explore and analyze the existing data augmentation methods for 3D object detection, and propose to adopt scene reconstruction and neural scene rendering to scale up the dataset. The experimental results on nuScenes, show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The task of data augmentation for 3D object detection is popular and interesting in the 3D community. The authors propose to use scene reconstruction and neural scene rendering to enlarge the dataset and make it work on nuScenes."
            },
            "weaknesses": {
                "value": "1. Comparison with data augmentation SOTAs. Although the authors compared the proposed method with the vanilla Cut & Paste in terms of the performance. However, since Cut & Paste doesn't use any point cloud raw data, so to have a fair comparison, it would be interesting if the authors could show the performance comparison with LiDAR-based data augmentation methods, i.e., Tongetal.(2023). \n\n2. It is unclear to me how large the augmented dataset is compared with the original one. \n\n3. It would be interesting if the authors could show the detailed detection performance of 10 classes on the nuScenes. From my understanding, the proposed method is kind of sensitive to different objects with different sizes. Since the small-scale objects, like pedestrians and cyclists might only take a couple of pixels."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7495/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7495/Reviewer_ZS78",
                    "ICLR.cc/2024/Conference/Submission7495/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733313150,
        "cdate": 1698733313150,
        "tmdate": 1700627827471,
        "mdate": 1700627827471,
        "license": "CC BY 4.0",
        "version": 2
    }
]