[
    {
        "id": "pnV2Sdhxp0",
        "forum": "RlbFGQYsJr",
        "replyto": "RlbFGQYsJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_ivZK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_ivZK"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a model to learn jointly a manifold and the dynamics on this manifold. The manifold is learned through an auto-encoder. Dynamics are learned by a neural ODE. The model is evaluated on classification tasks on several image and time series datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is clearly written and easy to follow. It tackles the important question of reducing the computational cost of neural ODEs by learning neural ODEs on lower-dimensional manifolds. The experiments suggest that the proposed model requires less function evaluations and obtains better classification performance than the standard neural ODE approach."
            },
            "weaknesses": {
                "value": "I have two important concerns with the paper: novelty with respect to the literature, and soundness of the proposed model.\n\n**Novelty:**\n\nThe idea of combining neural ODEs with manifold learning is already present in several papers (some of which are cited by the present paper in Section 2). The paper should make a more precise comparison with the literature, beyond stating that the literature focuses on the problem of sampling and not on the one of classification. To be more precise:\n1. I do not understand how Theorem 1 from the present paper differs from Proposition 5.1 of [Lou et al., 2020]. \n2. The idea to use auto-encoders in conjunction with neural ODEs is already present in the community of machine learning for physics, see e.g. \n\nDeep learning models for global coordinate transformations that linearise PDEs, Gin, C., Lusch, B., Brunton, S., & Kutz, J. (2021).\n\nPhysics-informed neural ODE (PINODE): embedding physics into models using collocation points, Sholokhov, A., Liu, Y., Mansour, H. et Nabi S. (2023).\n\nAs a consequence, the main contribution of the paper in my opinion is to show empirically on several classification datasets that the proposed method does indeed reduce the number of function evaluations while improving the performance. This should be reflected in the end of the introduction (and probably also in the title of the paper), which should be more modestly phrased in my opinion.\n\n**Soundness:**\n\nSolving an ODE on a manifold is a difficult task (see [Hairer, 2011]). In particular, even though the solution of the ODE defined by eq. (7) belongs to the manifold defined by $G$, its discretization through a solver may not belong to the manifold. \n\nThis issue is not mentioned in the paper, and is further exacerbated by the fact that the paper does not assume exact access to the inverse Jacobian of $G$, but reconstructs it using the decoder.\n\nBut the paper does not mention a term in the loss that would ensure that the encoder-decoder approximates the identity mapping. Assuming that such a term was not introduced in the loss, it seems very unlikely that the decoder can be used to approximate the inverse Jacobian of $G$. In this case, I do not see the difference between the proposed model and a standard neural ODE parametrized by a convolutional network with downsampling and upsampling. If an additional term to force the encoder-decoder to learn the identity mapping was introduced, this should absolutely be mentioned, and the choice of associated hyperparameters should be discussed.\n\nFor all these reasons, I think that the paper is unfit for acceptance at this stage.\n\n**Minor comments that did not influence the rating:**\n1. Plots are hard to read for colorblind people / when printed in black&white. It would be nice when possible to make them more colorblind-friendly.\n2. The introduction of neural ODEs in Section 3.2 is somewhat imprecise. The link between residual networks and neural ODEs is much more complicated than presented by the authors (see references below for more precise mathematical statements regarding this link). I do not think that it is useful to refer to residual networks to introduce neural ODEs, since they are an interesting standalone model independently of residual neural networks and since the connection is actually quite subtil.\n\nScaling Properties of Deep Residual Networks, Cohen A.S., Cont R., Rossier A., Xu R. (2021).\n\nScaling ResNets in the Large-depth Regime, Marion P., Fermanian A., Biau G., Vert J.P. (2022)."
            },
            "questions": {
                "value": "I have three questions for the authors:\n\n1. Have you used a term in the training loss to ensure that the encoder-decoder approximates the identity mapping? If so, did you perform experiments that show it is indeed the case that the identity mapping is well approximated by the auto-encoder after training?\n\n2. As far as I understand, eq. (7) is equivalent to\n\n$x(0) = G(h(0))$\n\n$x(T) = x(0) + \\int_0^T f(x(t), t, \\theta) dt $\n\n$h(T) = G^{-1}(x(T))$\n\nCan you confirm, and explain the difference if there is one?\nFurthermore, if the two formulations are indeed equivalent in the continuous world, it may not be the case anymore once they are discretized. Have you tried training with the alternative formulation? It seems more natural since the ODE solver is then called on the low-dimensional object instead of the high-dimensional one.\n\n3. How do you deal with time-series data in your experiments? In its simplest form, neural ODEs are suited for vector data, which is inputed in the first layer. Of course, many proposed modifications in the literature make it suitable for time series, but I do not think this is mentioned in the paper, and was wondering how you are handling this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Reviewer_ivZK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8123/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698395279090,
        "cdate": 1698395279090,
        "tmdate": 1699637006358,
        "mdate": 1699637006358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ikUwGlZgPV",
        "forum": "RlbFGQYsJr",
        "replyto": "RlbFGQYsJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_V8a6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_V8a6"
        ],
        "content": {
            "summary": {
                "value": "This manuscript proposes a method to further improve ODE-inspired NN models such as Neural ODE by casting the learning to a manifold. The hypothesis is that by doing so, the learning can be more efficient, for example, as measured by the number of function evaluations (NFE). The work is motivated by Floryan and Graham, Nature Machine Intelligence, 2022"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper follows the same direction as Floryan & Graham 2022, which is an emerging research direction when modeling complex system dynamics. In stead of AE, the paper proposes to use NODE as the learning vehicle. The idea is novel. The authors provided some empirical evidence on the effectiveness of the method."
            },
            "weaknesses": {
                "value": "The manuscript can be vastly improved. Below is a list of general issues. The detailed questions are listed in the next section. \n\n1. The authors seem to have a different understanding on the meaning of `learning dynamics from data`. As in Floryan and Graham, the concept is to learn the dynamic behavior from sampled data, and to generalize to other cases. However, the experimental results are all point out to image and time series classification. What type of dynamics are the models supposed to learn? \n\n2. Sections 4 and 5 are the key components of the paper. However as it is written, many details are missing. The manuscript provides an parameterized version of $\\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$.  It is unclear how $\\mathcal{G}$ in Section 5 is defined.\n\n3. In the experimental results section, the authors discussed the dimensions of the input space, e.g., $748$ for MNIST. But they are not dynamic systems, what does the learning of dynamics mean for these cases? \n\n4. What is the connection between the accuracy of a trained model versus the training on a manifold, as shown in Table 1?"
            },
            "questions": {
                "value": "1. Figure 3 is difficult to read. The captions are also not clear. What exactly is shown in the figures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Reviewer_V8a6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8123/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796907201,
        "cdate": 1698796907201,
        "tmdate": 1699637006247,
        "mdate": 1699637006247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pKTnA9GRcG",
        "forum": "RlbFGQYsJr",
        "replyto": "RlbFGQYsJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_ckcW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_ckcW"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a data-driven approach that employs an encoder to project the original data into a manifold. They use the Jacobian matrix of a corresponding decoder for mapping back to the original data space. Based on this, the authors extend the concept of Neural ODEs to operate on manifolds."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method of mapping data into a submanifold and using Jacobian to map it back is clear and straightforward."
            },
            "weaknesses": {
                "value": "1. In Section 4, please specify the submanifold of which is referred to. As the authors suggest in Section 4, $u$ and $v$ stand for the $\\theta$ and $\\phi$ of the spherical coordinate. \n    - If authors are referring to the submanifold of $S^2$, all pairs of $(u,v)$ do not form a submanifold of $S^2$ as at the north and south poles, multiple pairs $(u,v)$ map to the same point, causing the Jacobian to lose rank.\n    - If authors are referring to the submanifold of $\\mathbb{R}^3$, all pairs of $(u,v)$ do not reside in $\\mathbb{R}^3$. A submanifold usually denotes a subset of the manifold that itself has a manifold structure. All pairs of $(u,v)$ together with $R$ is used to parameterize $(x,y,z)$ but all pairs of $(u,v)$ themselves are not subset of $\\mathbb{R}^3$.\n2. In Section 5, the authors claim a general methodology for learning dynamics on manifolds. However, no experiments of data on manifolds with specific submanifolds are conducted to justify the effectiveness. The authors should at least provide experiments on the dynamics of sphere data to align with Section 4."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8123/Reviewer_ckcW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8123/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812586581,
        "cdate": 1698812586581,
        "tmdate": 1699637006124,
        "mdate": 1699637006124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1DEBWcdqZb",
        "forum": "RlbFGQYsJr",
        "replyto": "RlbFGQYsJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_Ha6r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8123/Reviewer_Ha6r"
        ],
        "content": {
            "summary": {
                "value": "- The paper introduces a novel approach that combines manifold learning principles with Neural Ordinary Differential Equations (ODEs) to address the challenges of learning dynamics in high-dimensional systems.\n- The proposed method leverages the manifold hypothesis and projects the original data into the manifold using an encoder. The Jacobian matrix of the corresponding decoder is used for dynamics learning.\n- Experimentally, the results show that the proposed method achieves better test accuracy, faster convergence speed, and requires fewer function evaluations (NFEs) compared to the baseline models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Theoretical in the simple low dimensional space, the proposed method is sound and align with the Neural ODE. The paper introduces a novel approach that combines manifold learning principles with Neural ODEs to address the challenges of learning dynamics in high-dimensional systems. This integration allows for efficient and accurate solutions for dynamic learning.\n- The results show that the models based on the proposed approach have faster convergence speed, require less training time, and achieve better test accuracy compared to baseline models."
            },
            "weaknesses": {
                "value": "- Experiments are very simple. The experiments are based on either small dataset or simple image dataset (MNIST/ Cifar-10/ etc). \n- Comparison is not so convincing. For example, normally for the image dataset, we use resnet-18 with convolutional layers instead of simple fully-connected layers. This makes the performance of resnet-10 bad. However, this also limits the contribution of the proposed method, as this only shows small improvement over this simple baseline."
            },
            "questions": {
                "value": "- What is the main reason for not using normal resnet-18/ other CNN as comparisons?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8123/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821693941,
        "cdate": 1698821693941,
        "tmdate": 1699637006014,
        "mdate": 1699637006014,
        "license": "CC BY 4.0",
        "version": 2
    }
]