[
    {
        "id": "Q933vlNNOH",
        "forum": "c8McWs4Av0",
        "replyto": "c8McWs4Av0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_UL1M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_UL1M"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts OpenAI's new GPT4 with Python Interpreter version. They reveal that incorporating code interpreter as an external could increase LLM's performance on math tasks. The more times using the interpreter, the higher performance would have. In addition, by using the self-debug method with the new GPT4 w/ code, they further push the SoTA of MATH dataset to 84.3%."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. A good demonstration of OpenAI's new GPT4 with Python Interpreter version. The experiments show the great improvement of the new model comparing w/ the traditional GPT4. It also shows the potential of using code interpreter as an external tool to enhance performance on math reasoning tasks.\n2. They authors push the SoTA of MATH to 84.3%, which is a very high number considering its complexity."
            },
            "weaknesses": {
                "value": "1. Most of the methods discussed in the paper are proposed by existing works and the main contribution of this paper is to try them out using the new OpenAI model. The contribution and novelty could be a weakness of the paper.\n2. Most credit of the huge improvement on MATH should be given to the better capability of GPT4-code itself. The authors seem to over claim their own contribution throughout the paper. For example in Page 6, the authors said \"Before the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021) depended on an external LLM to use natural language for verification and well-designed fewshot example prompts. In contrast, our approach simplifies the process by relying solely on a straightforward prompt for GPT4-Code, all in a **zero-shot** manner.\" But the root reason that zero-shot is applicable is because you are using OpenAI's instruction-tuned model. In addition, it is very likely that OpenAI has already used PRM (Lightman et al., 2023) to RL tune it's new model, therefore the API you used could be based on top of Lightman et al., 2023."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3135/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810763763,
        "cdate": 1698810763763,
        "tmdate": 1699636260518,
        "mdate": 1699636260518,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7uD78Lo0gM",
        "forum": "c8McWs4Av0",
        "replyto": "c8McWs4Av0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_boig"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_boig"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a prompting based self-verification technique to improve GPT-4 CodeIntepreter model's performance on math tasks. The key ideas are: (1) by prompting the model to perform code-based self-verification, the model improves its performance, and (2) taking advantage of self-verification result for weighted-voting, the model further improves its performance.\n\nThe paper is inline with existing lines of work of leveraging model's self-debugging ability to improve math reasoning ability. The experiments showed that (1) by leveraging multiple invocation of code call, GPT-4 Code achieve higher performance comparing to NL or 1-code call variances, (2) self-verification provides the opportunity for the model to discover its own error thus to fix it, and (3) voting solutions based on self-verification results achieves better performance over vallina voting scheme."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper is timely. It's a novel approach of leveraging code self-verification to improve math reasoning ability. It's simplicity means it can be easily used by GPT-4 users, and the thorough study provides solid evidence of adopting this technique.\n\nConcretely, the paper's strengths include:\n1. Novel technique that combines self-debugging and test generation for math reasoning tasks.\n2. Quite complete study that highlights effectiveness of the technique.\n3. Weighted voting technique is quite unique, and it can possibly used in other self-debugging techniques beyond Math reasoning tasks."
            },
            "weaknesses": {
                "value": "The paper lacks some insights into the quality of self-verification results, and how that matters for model performance.\n1. The paper can potentially dive deeper into analysis of consistency between verification process and NL reasoning process as well as output correctness. As shown in prior work like CodeT, some self-generated test-cases or verification code can either be wrong or inconsistent, but it may or may not affect model output quality. Guessing from the paper's results on weighted voting versus simple voting, such inconsistency exists and they could benefit model performance (or affect model performance if we simply reject such answers). I would suggest the authors perform some qualitative analysis to dive into this problem.\n\n1b. Some deeper qualitative analysis into what types of verification code are generated would also be helpful.\n\n2.The paper considers both sampling and sequential self-repair. It would be great if the authors can analyze the tradeoff between the depths of self-repair (e.g., if self-verification continues to fail, how much can the model benefit from continuing self-repair until verification succeed) verse breaths of self-verification (e.g., simply repair once, but generate multiple samples to do weighted sampling).\n\n3. The paper plots are visually misleading: Figure 2 accuracy numbers should all start from 0 as opposed to 60 in figure 2a and 40 in figure 2b. Otherwise the improvement looks like 10x as opposed to 6% comparing prompts 1 and 2. Similarly for other figures."
            },
            "questions": {
                "value": "I think the paper lacks a qualitative analysis of the self-verification code and analysis of consistency between answer and verification code. Otherwise a great paper showing the ability of self-verification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3135/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825365805,
        "cdate": 1698825365805,
        "tmdate": 1699636260427,
        "mdate": 1699636260427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LAj677FfFc",
        "forum": "c8McWs4Av0",
        "replyto": "c8McWs4Av0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_qqMb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_qqMb"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the effectiveness of code writing and self-validation for improving the reasoning capability of large language models (LLMs). Given that GPT-4 can generate Python programs, instead of providing step-by-step reasoning entirely in natural language text, this paper suggests a new prompt that encourages GPT-4 to generate both text and Python code snippets as well as a self-validation in the end. Additionally, a weighted majority voting is proposed to aggregate multiple runs or samples. The evaluation on three math datasets shows that GPT-4 with proposed prompting techniques significantly outperforms the previous state-of-the-art results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Like many recent works, this work provides strong evidence that prompting LLMs in a proper way could significantly influence the performance\n- This work also makes another natural but still interesting finding -- the frequency of code usage has a strong correlation with the accuracy of the final answer. The rationality is that executing code is more accurate/reliable than performing reasoning through natural language text. \n- This work presents the new state-of-the-art results on three datasets (i.e., MATH, GSM8K, MMLU-Math)."
            },
            "weaknesses": {
                "value": "- In terms of improving LLMs, the ideas of code writing, self-validating, and majority voting have been already explored in recent literature. The novelty of this work seems a simple combination of all three ideas together. \n\n- Only one particular and proprietary LLM (i.e., GPT-4) is used for evaluation. Whether the results and findings reported in this work may generalize to other publicly available LLMs (e.g., Llama 2) or relatively smaller language models is unclear. \n\n- Another similar concern is that only one specific kind of benchmark (i.e., grade school math problems) is used, thus the proposed prompt might be overfitting to simple math problems.  For the chosen benchmarks, improvements due to coding and CSV on GSM8K and MMLU-Math are already fairly small (i.e., 2-4%). \n\n- The improvement seems largely due to the superior capability of GPT-4. For instance, GPT-4 itself can outperform the state-of-the-art by a large margin without any sophisticated prompting and majority voting. It is already well-known that even a simple prompt could dramatically influence performance (see ). The particular finding of GPT-4 on the MATH benchmark is not very surprising."
            },
            "questions": {
                "value": "Would the same idea improve other LLMs like Llama 2? Or what factors may prevent the same idea from improving other LLMs? \n\nIn Table 3, the last row only mentions _CSV_ (but not Voting), is it a typo? If not, why voting is not enabled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3135/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3135/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3135/Reviewer_qqMb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3135/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871380840,
        "cdate": 1698871380840,
        "tmdate": 1700691082912,
        "mdate": 1700691082912,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vlsGuwEwdw",
        "forum": "c8McWs4Av0",
        "replyto": "c8McWs4Av0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_iedh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3135/Reviewer_iedh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a zero-shot prompting strategy for LLMs that can execute code during a completion (e.g. GPT-4 Code Interpreter). The strategy, referred to as CSV (code-based self-verification), is to use the following zero-shot prompt: \"Solve the problem using code interpreter step by step, even in every sub-step. And following your answer, please verify it using code interpreter by yourself.\" They find that this prompt boosts performance on several math word problem datasets. The authors then present a variation on majority voting based on weighting samples that were verified as True more heavily. The authors explore various ablations/variations, and also find a strong correlation between amount of code use and accuracy in general (ablations that can only use their code interpreter zero or one times perform worse)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The results on correlations between code usage frequency and accuracy are a nice analysis, and breaking it down by difficulty level of problem is useful (e.g. at higher difficulty levels amount of code usage seems to matter more)\n- The CSV setup itself is quite simple, which is a good thing \u2013 \"Solve the problem using code interpreter step by step, even in every sub-step. And following your answer, please verify it using code interpreter by yourself.\" A simple way of getting a decent boost in performance from zero-shot prompting is a fairly useful contribution. The boost of 3.85% accuracy on the MATH dataset from using CSV over the base prompt is solid \u2013 that's a reasonable gain.\n- The Verification-guided weighted majority voting setup is also quite simple (just a weighted majority vote with human-picked parameters) which is good. The weighting seems to provide a boost of around +1% accuracy over a standard majority voting approach (at 16 samples, MATH dataset, per Fig 6b).\n- The precision-recall-accuracy analysis and also analysis of weighted majority vs naive majority are good to see, I was preparing to suggest that, and I'm glad it was done."
            },
            "weaknesses": {
                "value": "Overall I'm marginally below acceptance on this paper, but would certainly consider raising my score if the following points are well addressed in revisions/rebuttal.\n- As-is the current paper has only one row in Table 1 that uses 16 samples with GPT4-Code, so the +14.63% improvement is a comparison between a 1 sample and 16 sample method. This can be a bit misleading to readers (or at least it was to me) since much of that improvement comes from using additional samples, which is something that could also be done through prior work via naive majority voting (which is actually evaluated later in the paper, in Fig 6b). Therefore, Table 1 should include a line for **GPT4-Code + CSV + Majority Voting** (which does about 1% worse than weighted voting, per Fig 6b).\n- For the same reasons as in the previous comment, adding a line to Table 1 with **GPT4-Code + Majority Voting (no CSV)**, an ablation that done with neither CSV nor weighted voting, would be an informative baseline to include. These two baselines would considerably strengthen the analysis.\n- There doesn't seem to be a **GPT4-Code + CSV** row in Table 2, nor a **GPT4-Code + CSV + Voting** row in Table 3. This use of different setups on different datasets is confusing \u2013 an explanation should be given, or preferably both should be added. Having both of these novel methods from the paper evaluated on all 3 datasets would strengthen the paper.\n- Figure 6a is quite confusing to me:\n    - Which dataset is used? I believe it's MATH overall dataset, but this should be in the caption and optionally also main text\n    - I don't understand why there are 5 separate paths graphed, given that there's nothing special differentiating path 0 from path 1, etc. The different path indices are just IID samples, so of course when you calculate precision/accuracy/recall averaged over a whole dataset of problems, all the histogram bars will look the same for different sample path indices. So having 5 separate paths doesn't seem to add anything \u2013 why not just collapse them all into a single graph with three bars (accuracy, precision, recall)?\n\nminor:\n- Consider weakening the statement \"Each line in Fig. 5 has an obvious trend of going upwards, proving that the increase of Code Usage Frequency induces a general improvement in accuracy\": *Proves* is a strong word there, when it's just a nice correlation (not causal \u2013 there are many ways of including meaningless code in an output that wouldn't not induce an improvement in accuracy). Just state it as a correlation and that's good."
            },
            "questions": {
                "value": "- Can you address or respond to the main 4 points made in the weaknesses section?\n    - The majority voting ablation addition to Table 1\n    - The majority voting + no CSV ablation addition to Table 1\n    - The missing rows of Table 2 and Table 3\n    - The confusion around Fig 6a\n\nMore minor questions:\n- In Fig 2 why are some of the red dots different sizes given that the prompt says no code is allowed \u2013 does that mean in some cases it's using code anyways?\n- \"Moreover, we observed a decline in accuracy for 4 of the seven subtopics, indicating that relying solely on natural language self-verification can not only compromise accuracy but also negatively impact performance\" What does this line mean \u2013 aren't accuracy and performance the same thing? \n- Do you have a sense for why the natural language verification row in Table 2 can fairly significantly *decrease* performance (by 2-3% in some domains)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3135/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3135/Reviewer_iedh",
                    "ICLR.cc/2024/Conference/Submission3135/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3135/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699469790711,
        "cdate": 1699469790711,
        "tmdate": 1700617389866,
        "mdate": 1700617389866,
        "license": "CC BY 4.0",
        "version": 2
    }
]