[
    {
        "id": "iCuPly4GCu",
        "forum": "fNOewRJLgQ",
        "replyto": "fNOewRJLgQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_eRzw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_eRzw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Ophiuchus, a coarse-graining model that is SO(3)-equivariant. This model acts efficiently on the heavy atoms of protein residues. Different from existing methodologies that utilize graph modeling, Ophiuchus prioritizes local convolutional coarsening as a means to represent sequence-motif interactions, which is trained to examine its ability to reconstitute information at various compression rates. The acquired latent space and its application in conformational interpolation for Ophiuchus are studied. Besides, a denoising diffusion probabilistic model is employed to effectively generate decodable latent embeddings of various miniproteins. The results presented in this study show that Ophiuchus possesses the potential to serve as a scalable foundation for the effective modeling and production of proteins."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces a new autoencoding model for protein sequence-structure representation. This auto-encoder model is examined by comprehensive ablation benchmarks in Table 1 and Table2. Ophiuchus includes a few methods for coarsening, refining, and mixing protein sequence-structure. The generated representations can spawn 3D coordinates directly from features. Moreover, denoising diffusion probabilistic models are leveraged to get the generative model for miniproteins."
            },
            "weaknesses": {
                "value": "This paper is a little hard to understand. I have tried to make sense of its mechanisms, but there still exist problems for me without its provided codes. \n1. In the abstract, the authors say the proposed model focuses on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. However, I cannot find the complexity analysis in the manuscript.\n\n2. The authors need to examine all the formats of the references that appeared in the paper, whether using () or [] or other formats. Typos: Translation Equivariance of Sequence: One-dimensional Convolutional Neural Networks (CNNs). Colon or dot. In Figure 2, there are two (d). \n\n3. Sometimes, the authors say all-atom protein structures are used, and sometimes, only the heavy atoms are used. It confused me. In Algothrim 1 and Algorithm 2, l=0:2, but in Algorithm 3, l=0:l_{max}. I wonder how to get all atoms' representations from v^{l=0:2}. \n\n4. In Figure 1 (b), why the proposed model uses three building blocks? What do these three building blocks mean? Figure 3 for Protein (PDB ID: 1S9K and PDB ID: 2O93) and Figure 8 for protein (PDB ID: 4JKM and PDB 6LF3) are very similar. I suggest the two pictures be put together. Where are their similarities and differences?\n\n\n5. The authors say the permutation invariance of particular side-chain atoms is preserved, and a one-dimensional roto-translational equivariant convolutional kernel is designed. Are there any demonstrations to illustrate the invariance and equivariance?\n\n6. In Section 4.1, why choose the contiguous 160-sized protein fragments\uff1f Why train the all-atom generative model for miniproteins instead of large proteins? Why is the model trained in ten epochs?\n\n7. The experiments of conformational interpolation and structure recovery latent diffusion lack comparison methods."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3661/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583093628,
        "cdate": 1698583093628,
        "tmdate": 1699636322496,
        "mdate": 1699636322496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sXl3KxbiVZ",
        "forum": "fNOewRJLgQ",
        "replyto": "fNOewRJLgQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_ukWB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_ukWB"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a coarse-graining autoencoder for modeling protein structures. The proposed model succeeds in reconstructing the protein structures in multiple resolutions."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The problem of modeling protein structures in multiple resolutions is important. The proposed model is well-designed to handle this problem. The presentation is straightforward."
            },
            "weaknesses": {
                "value": "The biggest problem is that the proposed model is not compared to existing work via experiments. This can be adequate to reject this paper unless the authors can justify why a comparison to other work is not applicable in rebuttal.\n\nSome other problems:\n1. Motivations about the model structure design are missing in this paper, including an explanation of why the whole model should contain those substructures and why each structure should be like that. \n2. A formal (mathematical) expression of the objective function is missing. The current expression form is too general to give necessary information about how to train the model to authors.\n3. The claimed generation power is not evaluated.\n3. Some expressions look not academic, e.g. \"we introduced a new unsupervised autoencoder\" (all autoencoders are unsupervised), \"multi-layer hourglass-shaped autoencoder\" (all autoencoders are hourglass-shaped, most are multi-layer)\n\nThe idea of learning coarse-graining representations for modeling protein structures is interesting and promising. I would encourage the authors to further improve the solidity of this work."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3661/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726625983,
        "cdate": 1698726625983,
        "tmdate": 1699636322386,
        "mdate": 1699636322386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S0LulivQ7z",
        "forum": "fNOewRJLgQ",
        "replyto": "fNOewRJLgQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_Lcpi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3661/Reviewer_Lcpi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a geometric deep learning architecture to process protein structures at different levels of coarsening in a learnable way.\nThe arcitecuture follows an hourglass design: an encoder downsamples the protein sequence while learning a corsened protein structure and the decoder upsamples the protein sequence while refining the atoms' locations within each residue.\nThe model employs layers equivariant to 3D rotations and translations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is well motivated and, while building on top of existing works, includes some interesting novel ideas (although, I might not be familiar with some related literature).\nIn particular, the idea of down and up-sample the protein sequence to model the protein structure at different coarsening levels seems novel and particularly useful.\n\nMoreover, I appreciated the extensive quantitative and qualitative studies in the main paper and the appendix, which provided insights into the capabilities of the proposed method."
            },
            "weaknesses": {
                "value": "The paper doesn't compare empirically the proposed method with other baselines and previous works.\nThis makes evaluating its benefits challenging.\n\nMoreover, the presentation needs some improvement as the manuscript occasionally misses important details (see Questions below)."
            },
            "questions": {
                "value": "The computational gain of the proposed method is not completely clear.\nIn Table 1, the lowest downsampling factor shows overall best performance and the models with highest downsampling factors include many more parameters (suggesting they might be more expensive).\nCan you quantify and compare the computational gains by using this hourglass design rather than operating at the highest resolutions?\nIt would also be interesting to compare with a version of the model which doesn't perform any downsampling.\n\n\nIn Sec. 3.1, it is not clear how the $V^{l=2}$ unsigned difference vector is computed and Apx A doesn't include further details about it. Is it obtained by taking the absolute value entry-wise of the difference vector? However, isn't $V^{l=2}$ supposed to be a 5-dimensional vector transforming under the order 2 Wigner D matrix?\n\nThe idea that the atoms in standard residues can typically be ordered a priori seems very important but is never explained well, so I think it deserves some additional discussion (in particular, the cases where residues present two-permutations symmetries). However, I am not as familiar with this type of tasks, and this might be well know in the community.\n\nSec. 3.3 Why is the normalization of the weights ensuring translation equivariance?\n\n\nOther minor comments and typos:\n\nSec. 3.1, 4-th line: $P_i^{0, l=0}$  -> $V_i^{0, l=0}$ ?\n\nSec 3.3: $\\hat{P}$ was not previously defined. It could also be worth describing what the HuberLoss is."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3661/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699292793696,
        "cdate": 1699292793696,
        "tmdate": 1699636322314,
        "mdate": 1699636322314,
        "license": "CC BY 4.0",
        "version": 2
    }
]