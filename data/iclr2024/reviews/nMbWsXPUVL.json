[
    {
        "id": "kicYU6llE6",
        "forum": "nMbWsXPUVL",
        "replyto": "nMbWsXPUVL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_5akx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_5akx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LLM-Codebook, an effective structure-clustering-based LLM compression technique for extreme compression. The main technology parts of this method consist of three steps: (1) Salient weight detection; (2) Hessian-aware K-means algorithm for weight clustering and compression; (3) Lora-based finetuning for retaining performance. The overall method is simple and effective. The manuscript is well-written with clear logic."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u25cf\tThe paper is well-written and easy to follow.\n\n\u25cf\tThe proposed LLM codebook shows good compression performance for a lower compression ratio as compared to recent compression works like GPTQ, SparseGPT, and LLM-Pruner."
            },
            "weaknesses": {
                "value": "\u25cf\tIt is unclear how can this method be combined with downstream LLM finetuning or if it is only effective for post-finetuning compression. After the compression, each linear layer consists of a codebook and an index map, how can the model be further finetuned under this structure?\n\n\u25cf\tSince the method requires full model finetuning using Lora for performance recovery, would the memory cost be huge when meeting with a larger base model like llama-70B? \n\n\u25cf\tIt is still being determined why the layer is randomly selected for compression during the Lora finetuning procedure.\n\n\u25cf\tThe compressed weight tensor in Figure. 2 seems like a copy of the original weight tensor, which looks too similar to be a real compressed visualization. Please explain this part.\n\n\u25cf\tSome statements in this paper related to existing compression methods are given lacking enough verification. The latest works are left without discussion. Taking the low-bit compression parts as an example, though the selected baseline GPTQ in this paper does not give a good performance for lower-bit like 2-bit, there are already several works showing promising results for lower-bit compression ([1-3]). For example, both omniquant and low_bit_llama show that llama families (1.1B-70B) can be well compressed to 2-bit with good performance. It is suggested to reorganize this part and discuss the possibility of combining low-bit compression with structure-clustering for further compression.\n\nOverall, this paper presents a method for compressing LLMs using structure clustering. The algorithm is verified on recent small LLMs like LLaMA-7B and shows good compression performance as compared to some of the existing techniques. The overall method is simple and effective. Some statements in the paper are not well verified.\n\n[1] omniquant: omnidirectionally calibrated quantization for large language models\n\n[2] https://github.com/GreenBitAI/low_bit_llama\n\n[3] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"
            },
            "questions": {
                "value": "Pls. see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698519683530,
        "cdate": 1698519683530,
        "tmdate": 1699636212822,
        "mdate": 1699636212822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LZM0alE6f9",
        "forum": "nMbWsXPUVL",
        "replyto": "nMbWsXPUVL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_kbsm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_kbsm"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a model weights compression algorithm method based on Hessian-aware K-means, especially for extreme reduction of model size. The authors empirically demonstrate the efficacy of Hessian-aware K-means and Lora-based recovery stage in compression and performance maintenance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposed to adopt an importance-aware K-means for model weights compression.  \n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- The experimental results are not very clear in terms of fair comparison with previous methods.\n- To make the paper stronger, the authors should provide more insightful explanations to connect the components of the proposed method, otherwise can be easily understood as a simple combination of two mature methods."
            },
            "questions": {
                "value": "- Eq(5) implies that the estimation of model weight salience depends on the selected dataset and I noticed that the authors used only 15 randomly selected samples from the Bookcorpus dataset. I wonder how sensitive this estimation is in terms of 1) the data source. Eg, how about 15 samples from Wiki or a similar corpus? 2) the number of samples. Eg, is 15 samples enough for a good estimation of salience? What\u2019s the possible influence of the salience estimation error on the later recovery stage?  \n\n\n- In Table 3, the Lora-based tuning looks more critical to prevent the performance from unacceptable degradation. This makes the readers very curious about several questions. 1) With only vanilla K-means, is it possible to match the best performance if more effort is put in tuning the recovery stage? 2) What\u2019s the must-be reason to use Hessian-aware K-means instead of vanllina k-means given its extra computation cost of estimation and secondary role in recovering model performance?   \n  \n\n- Following Q.2 above, what are the baselines in Table 1 that support Lora-based tuning? If an extra recovery stage based on Lora tuning was carefully added, will those baseline performances catch up with the proposed method?    \n\n- In Table 3 last row, The salience-adopted clustering stage leads to more performance degradation compared to the vanilla baseline: -3.5 vs - 6.3. Can the author explain the reason for this observation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698611722609,
        "cdate": 1698611722609,
        "tmdate": 1699636212746,
        "mdate": 1699636212746,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eZx7XVQCIO",
        "forum": "nMbWsXPUVL",
        "replyto": "nMbWsXPUVL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_1Uyx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2709/Reviewer_1Uyx"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LLM-codebook for extreme compression of LLMs, which clusters LLM weights into codebooks (in KB) with three stages: (i) the salience stage derives the salience of the random layer's weight through the hessian matrix; (ii) the cluster stage employs the hessian-aware k-means algorithm to cluster the codebook, and (iii) the recover stage uses LoRA for performance recovery. The paper conducts experiments on Llama-7b and vicuna-7b, and compares with both pruning and quantization baselines. The results demonstrate the superiority of LLM-codebook in achieving higher compression ratios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It achieves higher compression ratios for LLMs. \n2. This paper refrains from viewing pruning and quantization as two distinct paths for LLM compression. Instead, it perceives them as techniques for information compression[1]. Consequently, the paper's narrative does not adhere to the existing quantization or pruning pipeline. It introduces a unique compression technique: compressing LLM weights by clustering them and storing them in kilobyte-scale codebooks. This perspective is novel."
            },
            "weaknesses": {
                "value": "1. This work is essentially an application of production quantization on LLMs. Although the final performance surpasses the baselines, the method itself does not present much novelty. The idea of Hessian-aware k-means has also been utilized in previous works [1][2]\n2. From the perspective of LLM compression ratio, this work does not compare with the state-of-the-art quantization works [2][3][4] \n3. The current evaluation is focused solely on the 7b model. Could the author also provide evaluations on larger models, such as Llama-13b?\n[1] TOWARDS THE LIMIT OF NETWORK QUANTIZATION ICLR 2017, SAMSUNG\n[2] https://arxiv.org/abs/2306.07629\n[3] https://arxiv.org/abs/2307.13304\n[4] https://arxiv.org/pdf/2306.00978.pdf"
            },
            "questions": {
                "value": "Please refer to the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666286970,
        "cdate": 1698666286970,
        "tmdate": 1699636212658,
        "mdate": 1699636212658,
        "license": "CC BY 4.0",
        "version": 2
    }
]