[
    {
        "id": "0EtIeUKlJM",
        "forum": "7iCUSBlOgh",
        "replyto": "7iCUSBlOgh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_4SJf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_4SJf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes GRASS, a method to impute missing values in the features for tabular data using a graph-based method in the process. I could not fully understand how the method work, but, something like adding gradient feature to the current features, generate graph then using GNN based feature imputation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tinkers with different available ideas and it seems to have a good performance."
            },
            "weaknesses": {
                "value": "I'm struggling to find the idea of the method and how it differs from previous work. \n\nFirst, the paper praised the graph-based imputation methods of how resilient they are against large missing rate in comparison to tabular data's methods  Mean, GAIN. This does not show the merit of graph-based methods since... they have additional graph information that tabular data methods do not have. This only show that the additional graph does complement the feature data, which is not at all a surprise. By generating a graph from tabular data, GRASS does not necessarily inherit the merit of these graph-based method that have \nadditional information in form of a graph. \n\nSecond, the paper shows the problem of deploying methods for filling in missing tabular data of \"using incomplete feature\" while it is not known how GRASS avoid the incomplete features.\n\nLast but not least, I don't know how one trains a MLP model to get the gradient features from missing data."
            },
            "questions": {
                "value": "How could the method train a MLP model with missing values in the feature matrix?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7215/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652332519,
        "cdate": 1698652332519,
        "tmdate": 1699636858116,
        "mdate": 1699636858116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ypxMu33qEQ",
        "forum": "7iCUSBlOgh",
        "replyto": "7iCUSBlOgh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_T1VM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_T1VM"
        ],
        "content": {
            "summary": {
                "value": "This paper tackle the issue of missing feature imputation on bio-medical tabular data. The authors adapt techniques originally developed for graphs with missing features to tabular data by introducing a kNN adjacency matrix. They utilize feature gradient as additional features to construct a column-wise adjacency matrix using kNN. They perform colum-wise feature propagation with the matrix and prepare a row-wise adjacency matrix. This row-wise adjacency matrix is employed in graph-based imputation methods. They evaluate their approach using medical and bio-domain datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea is easy to understand.\n2. The paper is well written."
            },
            "weaknesses": {
                "value": "$\\textbf{1. Not novel}$\n\nThe authors overclaim that \u201cWe, for the first time, explore the generalizability of recent graph-based imputation models in the context of real-world tabular data with missing values.\u201d It is not true. scFP [1] utilizes FP, a recent graph-based imputation model, for bio-medical tabular data with missing values. scFP also constructs a kNN graph for tabular data given without an adjacency matrix, like this paper. The proposed method in this paper is built on scFP and experimental setting in this paper refers to [1]. However, scFP is not compared with the proposed method in experiments. Furthermore, all the components in this paper are described as they are novel. To claim the effectiveness of new components proposed in this paper, it is essential to explain and cite components from other work and demonstrate the performance gain over scFP. Moreover, constructing a kNN graph to use graph-based methods on tabular data has been extensively explored [2, 3, 4, 5] and should be credited.\n\n $\\textbf{2. Issues in Experiments}$ \n\n2-1. Unfair hyper-parameter setting\n\nRegarding the experiments, I disagree that using the same hyper-parameters is fair. The authors state, \u201cWe set a consistent dropout rate of 0.5 and a hidden dimension of 64 across all methods.\u201d This implies that a dropout rate of 0.5 and a hidden dimension of 64 are universally applied, irrespective of the datasets. However, all machine learning methods have their own optimal hyper-parameters for a given setting (e.g., dataset, domain, the type of task). For example, GCN [6] set sets of hyper-parameters as follows:\n- Citeseer, Cora, and Pubmed: 0.5 (dropout rate), 16 (hidden dimension)\n- NELL: 0.1 (dropout rate), 64 (hidden dimension)\nMoreover, it is not fair to use the same hyper-parameters for downstream neural networks across all methods, not only in terms of target datasets but also in terms of methods. For example, PaGNN [7] set the hidden dimension to 16 on Cora and Citeseer while it set one to 64 on AmaPhoto and AmaComp. In the case of FP [8] and PCFI [9], they set the hidden dimension to 256 for OGBN-Arxiv.\n\nWhen applying methods validated in citation and recommendation networks to unfamiliar domains, careful consideration is needed for hyperparameter settings. In [10], a study comparing GNNs and traditional methods for molecular property prediction demonstrates reasonable search spaces for each hyper-parameter across all methods, including GNNs and others.\n\n2-2. Omitting an important baseline\n\nThe authors mention scFP [1] in the related work and experimental details sections while they exclude scFP in experiments. scFP deals with medical/bio datasets used in this paper and also utilizes kNN graphs before FP, which is very similar to this paper.\n\n2-3. Downstream neural  networks\n\nThe authors state, \u201cTo align with our focus on downstream tasks, we appended a logistic classifier to methods that exclusively target imputation\u201d. Does it mean the authors commonly use logistic classifiers for all imputation methods, including FP, PCFI, GAIN, GRAPE, etc.? If so, FP and PCFI do not use downstream neural networks where message passing occurs, unlike  GCNMF.\n\n2-4. No error bar\n\nThe authors state, \u201cFor the dataset split, we randomly generated five different splits with train/val/rest ratio in 10%, 10%, 80%.\u201d Are all the reported values from those five splits? It needs to be clarified. Then there is no error bar. The performance differences between the methods are very small. For all the datasets, the performance gain of the best performing method over the second best one is very marginal. Therefore, it seems necessary to check the error bars. \n\n$\\textbf{3. Convergence issue}$\n\nThe authors state, \u201cGiven approach uses a custom kNN graph, discussions about convergence can be found in Appendix A.2.\u201d However, the authors\u2019 message through the claim in Appendix A.2 is not clear. After all, kNN graph with k between 1 and 10 is not a strongly connected graph in general, which does not satisfy the convergence condition of FP. It means the warmed-up matrix does not converge even after large enough K. \n\n$\\textbf{4. Initialization issue}$\n\nThere is no explanation for $X^{0}_{v,d}$ in Eq (2). Do the authors fill in missing values with zeros? This needs clarification. However, as convergence is not guaranteed, the output depends on the initial values. Hence justifying zero imputation for the initially missing values is questionable if it is employed.\n\n$\\textbf{5. Ineffectiveness of the clamping process}$\n\nThe clamping process designed for categorical features can be applied only for the datasets (Breast Cancer, Hepatitis, Duke Breast, ADNI, and ABIDE) with categorical features. However, the gain obtained with GRASS on the five datasets is much smaller than that in the other datasets. Thus the clamping process seems to contribute very little to the performance improvement.\n\n$\\textbf{6. No ablation study}$\n\nSince where the performance gain comes from is not clear, an ablation study should be performed. Especially column-wise feature propagation without feature gradients should be compared with GRASS.\n\n$\\textbf{7. To claim contribution in exploring generalizability}$\n\nTo claim contribution in exploring generalizability of recent graph-based imputation models in the real-world tabular data with missing values, it seems necessary to demonstrate the performance not on datasets within a single domain but on datasets used for the comparison of existing tabular data imputation methods. For example, Concrete, Housing, Wine, and Yacht from the UCI Machine Learning repository [11].\n\n$\\textbf{Minor comments/suggestions}$\n\n* The colors of the bars in Figure 3 are too similar. It might be better to consider an alternative visualization.\n* According to the source code, $A^{feat}$ and $\\hat{A}$ are weighted adjacency matrices where weights are cosine similarity. However, based on the manuscript alone, it seems that $A^{feat}$ and $\\hat{A}$ are unweighted adjacency matrices are generated through kNN while $\\tilde{A}$ is a weighted adjacency matrix obtained via normalization. Therefore, it is necessary to explicitly specify that $A^{feat}$ and $\\hat{A}$ are weighted by cosine similarity.\n* calmping categorical columns -> clamping categorical columns (page 18)\n* legnth -> length (page 18)\n\n[1] Single-cell rna-seq data imputation using feature propagation\n[2] EGG-GAE: scalable graph neural networks for tabular data imputation\n[3] Fusion of Graph and Tabular Deep Learning Models for Predicting Chronic Kidney Disease\n[4] Leveraging graph convolutional networks for semi-supervised fault diagnosis of HVAC systems in data-scarce contexts\n[5] TabGSL: Graph Structure Learning for Tabular Data Prediction\n[6] Semi-Supervised Classification with Graph Convolutional Networks\n[7] Incomplete Graph Representation and Learning via Partial Graph Neural Networks\n[8] On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features\n[9] Confidence-based feature imputation for graphs with partially known features.\n[10] Could graph neural networks learn better molecular representation for drug discovery?\n[11] UCI Machine Learning repository"
            },
            "questions": {
                "value": "Q1. Could the authors provide results on Concrete, Housing, Wine, and Yacht from various domains?\n\nQ2. Could the authors provide full results table including standard deviation?\n\nQ3. How GRASS improves the performance of graph-structure-agnostic methods (e.g., GAIN and GRAPE) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7215/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7215/Reviewer_T1VM",
                    "ICLR.cc/2024/Conference/Submission7215/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7215/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715545564,
        "cdate": 1698715545564,
        "tmdate": 1700759432218,
        "mdate": 1700759432218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gP269YvJ4e",
        "forum": "7iCUSBlOgh",
        "replyto": "7iCUSBlOgh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_JSNu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_JSNu"
        ],
        "content": {
            "summary": {
                "value": "Graph-based imputation methods typically depend on the presence of a graph structure. In the biomedical domain, an inherent graph structure among samples is often absent. This paper introduces GRASS, designed to augment the feature matrix and unearth the graph structure. GRASS utilizes feature gradients as an addition and engages in feature-wise propagation to produce an expanded feature matrix. Subsequently, it identifies the graph structure using sample-wise k-NN. The enhanced feature matrix combined with the newly identified graph structure can be applied to succeeding graph-based imputation techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation is clearly explained.\n- The use of feature gradients as a supplement seems novel.\n- valuations span multiple datasets, backed by extensive analyses.\n- The paper is well-structured and reader-friendly.\n- The corresponding code has been made publicly accessible."
            },
            "weaknesses": {
                "value": "- The datasets in the study are somewhat limited, with a maximum of 8K samples. It remains to be seen how GRASS will perform on larger datasets. Will an increased sample count obstruct the column-wise feature propagation?\n\n- Merging GRASS with existing methods adds to computational demands. The performance of current techniques with computational costs equivalent to GRASS's is not clear.\n\n- Part two of section 4.3 is confusing to me. Figure 4 (b) appears to indicate similar gradient magnitudes for four genes, which doesn't necessarily suggest uniform gradient patterns among them. The authors also point out that gradients undergo normalization before being appended to the primary feature matrix. Hence, the gradient norm in Figure 4 (b) might be less indiactive. Perhaps pairwise cosine similarity could offer a more insightful metric.\n\n** I am willing to raise the score if the authors can address the questions above."
            },
            "questions": {
                "value": "Refer to the aforementioned weaknesses for key questions, and also consider:\n- What's the reasoning behind a 10% training, 10% validation, and 80% testing data split? Why is only 10% of the data used for training?\n- Can you provde more descriptions regarding the baseline methods?\n- Can you elaborate more on the datasets and the preprocessing steps? For instance, how were samples chosen? What features were considered? What targets were set for prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7215/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7215/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7215/Reviewer_JSNu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7215/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816759265,
        "cdate": 1698816759265,
        "tmdate": 1699636857825,
        "mdate": 1699636857825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TIA6wYS6z7",
        "forum": "7iCUSBlOgh",
        "replyto": "7iCUSBlOgh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_Cqu6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_Cqu6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for handling missing data in tabular datasets. It leverages feature gradients to help construct a graph and use column-wise feature propagation to impute missing values based on their similarity to other features. This approach allows for effective imputation of missing values, and it is demonstrated to be effective and generalizable in handling various missing data scenarios, particularly in the context of medical and bio-domain datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Exploring the implicit graph structure among tabular data are useful for imputing the missing values.\n\n- The paper leverages an interesting observation that feature gradients are more discriminative than the original input to distinguish classes. This insight highlights the value of utilizing these gradients in the context of data imputation, as they provide a more discriminative representation of the data.\n\n- Extensive experiments are conducted on multiple datasets to show the improvement of the method."
            },
            "weaknesses": {
                "value": "- The gradients are normalized into unit vectors. However, the GCN propagation in equation 2 does not take the scales of different features into consideration. The graph propagation can be viewed as a weighted sum among columns. Some columns in greater magnitude may lead to large imputed values. Is this issue addressed in the proposed method?\n\n- The definition and proposition is a bit redundant. The detail expression of the gradient is not used anywhere in the paper. Instead it will be more helpful to add a piece of pseudo code of algorithm to help the reader understand the whole framework. \n\n- Is the choice of threshold theta column-dependent? It might not be the optimal to use a constant across all features. Theta might depends on the distribution of 0/1 in that feature."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7215/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817767798,
        "cdate": 1698817767798,
        "tmdate": 1699636857692,
        "mdate": 1699636857692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FHmGgzyLJP",
        "forum": "7iCUSBlOgh",
        "replyto": "7iCUSBlOgh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_r66i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7215/Reviewer_r66i"
        ],
        "content": {
            "summary": {
                "value": "The authors present GRASS, an imputing method for tabular data that bridges the gap between graph-based imputation methods and real-world scenarios.\nMore precisely, GRASS employs a single-layer Multi-Layer Perceptron (MLP) in order to extract predictors gradient, which then serves as a resource for generating graph structures. After calculating these gradients, a graph from a feature (i.e., column) perspective is constructed and column-wise feature propagation is performed. Consequently, after the feature matrix is imputed, a second graph is generated but this time from a sample-oriented (i.e., row) perspective, which serves as the input for existing graph-based imputation models. \nThe authors present results for both single cell sequencing and medical datasets and compare their method to the state-of-the-art-imputation methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors introduce the concept of feature (predictors) gradient which is the partial derivative of the loss function for each of these features, in order to employ information both in a column- and row-wise manner. Via doing so they can estimate the salience of individual features in loss minimization. Through employing these gradients, the constructed graph encapsulates not just observed feature information from an initial state but also the feature saliency in relation to the targeted downstream task. \nTheir method seems to perform well also in cases where there is high missingness rate in the data."
            },
            "weaknesses": {
                "value": "The improvement induced by GRASS seems to be incremental and not consistent, especially given the computational trade-off (of calculating the feature gradients).\nThe presentation of the results needs to be improved -- it is unclear what they represent\nAlso there are some inconsistencies with respect to notation (eg Y in page 5 is not defines though used in the equation)\n\nIn terms of minor comments:\n-- terms are defined more than once (eg. Feature Propagation (FP) (Rossi et al., 2021) )\n-- Little et al., reference is presented as two distinct citations (2019a, 2019b)"
            },
            "questions": {
                "value": "page 4, footnote:\nWhat is meant by when employing zero imputation \"steering clear of presumptions associated with missing completely at random\" ?\n\nFigure 3: within each dataset, what does each bar represents? \nin case that this is GRASS against each of the other methods, why does GRASS performance isn't stable across all comparisons?\n\nFigure 5: When looking at the (very small - almost unreadable) labels we see that there are 14 classes, nevertheless at 5(c) much fewer are presented and not in a much better manner as compared to 5(a).  Could you please comment on that\n\nTable 1: what do the numbers in each column represent?\n\nFigure 6(a): what is calculated within each cell of the k_col by k_row matrices?\n\nWhat is the algorithmic convergence? Does it seem to converge eg with respect to loss, as the iteration number increases?\n\nWhat is the loss function employed? Is the algorithm stable with respect to different loss functions ?\nIs the uncertainty of imputing values calculated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7215/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699471910406,
        "cdate": 1699471910406,
        "tmdate": 1699636857579,
        "mdate": 1699636857579,
        "license": "CC BY 4.0",
        "version": 2
    }
]