[
    {
        "id": "EIVr3uVfUn",
        "forum": "aaYBsuGRne",
        "replyto": "aaYBsuGRne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_rNJR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_rNJR"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on understanding the in-context learning ability of large language models. The authors propose the Pelican soup hypothesis. It explains the in-context learning ability originating from learning the knowledge via the next token prediction. To support this hypothesis, the authors build a dataset and demonstrate the linkage between linguistic phenomena and in-context learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides substantial numerical results to support the proposed hypothesis. The linguistic phenomena analysis is also interesting to the community. In addition, the built dataset may be of independent interest."
            },
            "weaknesses": {
                "value": "1. The claim related to the knowledge base needs more clarification. The experiments in [1] demonstrate that input-output mapping is not very important to the ICL. If the label space is correct, LLMs can even implement efficient ICL given wrong mapping. However, this wrong mapping conflicts with the knowledge base. More discussions are needed here.\n\n2. In Section 5.1, some assumptions are presented, but there is a notable absence of justification for these assumptions within the paper. This absence makes it challenging to ascertain the realism of these assumptions.\n3. I would greatly appreciate further elucidation on the distinction between the hypothesis presented in this paper and that discussed in [2]. Specifically, the variance between the \"atomic elements of NLP tasks\" and \"a set of atom concepts\" requires additional clarification.\n\n4. It is advantageous to include more highly relevant works in the related works. For example, besides HMM, implicit Bayesian inference is modeled for ICL in many different data assumptions [3,4,5]. [6] also studies the optimization side of ICL.\n\n[1] Min S, Lyu X, Holtzman A, et al. Rethinking the role of demonstrations: What makes in-context learning work?[J]. arXiv preprint arXiv:2202.12837, 2022.\n\n[2] Sanjeev A. and Anirudh G. A theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023.\n\n[3] Jiang H. A latent space theory for emergent abilities in large language models[J]. arXiv preprint arXiv:2304.09960, 2023.\n\n[4] Zhang Y, Zhang F, Yang Z, et al. What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization[J]. arXiv preprint arXiv:2305.19420, 2023.\n\n[5] Wang X, Zhu W, Wang W Y. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[J]. arXiv preprint arXiv:2301.11916, 2023.\n\n[6] Dai D, Sun Y, Dong L, et al. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers[J]. arXiv preprint arXiv:2212.10559, 2022."
            },
            "questions": {
                "value": "Questions are specified in Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698428079499,
        "cdate": 1698428079499,
        "tmdate": 1699636416867,
        "mdate": 1699636416867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fXSoklalgu",
        "forum": "aaYBsuGRne",
        "replyto": "aaYBsuGRne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_hbQa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Pelican Soup Hypothesis to explain in-context learning. It says that the in-context learning in language models can be explained as generalisation under several types of distribution shifts. It provides a formalism of NLP classification tasks in the context of in-context learning and constructs a dataset in formal language demonstrating the hypothesis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It proposes a general formalism for NLP classification tasks in the context of in-context learning. As the paper says, it may facilitate future NLP theory research.\n- The Pelican Soup hypothesis provides a potential explanation of in-context learning in language models.\n- The Calcutec dataset may also facilitate future research on explaining in-context learning."
            },
            "weaknesses": {
                "value": "In general, I am not an expert in this line of work, but I have a strong feeling that the hypothesis and the experiment are more about mimicking, or more precisely, producing an environment, with which in-context learning still works, rather than explaining how/why in-context learning works in language models. Intuitively, for me, they are different things or at least an insufficient explanation.\n\nSome of the reasonings are hard for me to follow, for example, \n- why the yes/no questions are similar to the demonstrations for in-context learning? Or, such similarities had already been considered distribution shifts?\n- (In page 3) How do you know the process of figuring out that \"she\" may be a person to whom something unexpected happened is similar to recovering z for class y? I understand the outcome would be similar, but why also the process? \n- And if the above one is the actual process, I somehow feel that this suggests that LM should be good at handling anaphora but not catephora, which intuitively, is different from in-context learning."
            },
            "questions": {
                "value": "See my questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698573684655,
        "cdate": 1698573684655,
        "tmdate": 1699636416792,
        "mdate": 1699636416792,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jS1e9y8XnV",
        "forum": "aaYBsuGRne",
        "replyto": "aaYBsuGRne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Pelican Soup Hypothesis to formalize and explain large language models' ability for in-context learning. The paper claims that in-context learning can be seen as a model's ability to generalize linguistic phenomena under distribution shifts. The authors identify several contributions in the paper, including:\n1. A new formalism for approaching natural language classification problems, specifically aimed at understanding in-context learning.\n2. A new dataset, \"Calcutec,\" replicates specific linguistic phenomena. The authors report that training on this dataset allows models to develop in-context learning abilities and improve their performance on chain-of-thought reasoning.\n3. The paper reports experiments with the GPT-2 model on various NLP tasks. These experiments connected certain linguistic phenomena and the model's in-context learning capabilities.\n4. The authors use a digit addition task to study a specific type of distribution shift. This experiment revealed that larger models are more capable of generalizing and adapting to such shifts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Linking in-context learning to the model's coreference learning ability is an interesting and novel idea. It serves the vast interest of the community in understanding the underlying mechanism of LLMs' in-context learning and chain-of-thought ability.\n2. Overall, the Pelican Soup Hypothesis and the accompanying experiments provide insights into why and how in-context learning works in large language models. The introduction of the Calcutec dataset and the digit addition task as experimental tools paves the way for further research in this area."
            },
            "weaknesses": {
                "value": "1. Many claims are made without citing prior sources or supporting evidence. For example, in 3.1, the author claims that \u201clanguage models may be able to acquire the KB by modeling general text.\u201d However, no clear evidence is provided via citations or experiments, and frankly, this is still an ongoing question the community aims to answer; it would be an important work itself to show these claims.\n\n2. The pretraining and in-context learning setting in the proposed dataset is different from common LLM settings, in which the synthetic setting here loses some of the information that LLM encodes, such as contextual information and domain information. This mismatched setting seems not ideal and limits the generalizability of this study. In particular, in-context learning has been found to be highly sensitive to context-label and domain-label biases, which is not clear in a context-free & domain-free setting.\n\n3. The main assumption of this paper seems to be that the text in pretraining corpora for LLMs consists of clear reasoning steps (potentially with some intermediate steps dropped). However, this assumption normally requires structured and domain-specific training data such as math text or academic papers. On the other hand, data like dialogues or other internet content may contain completely implicit reasoning steps that are hidden in the text space. So, I don't think the proposed pretraining data here, which includes some reasoning steps explicitly in the sequence, is very representative of the overall LLM pretraining setting.\n\n4.  The experiments are poorly designed, and the implementation details are generally missing, but the main experiment on Calcutec: dataset design is too complicated, but the experimental design and analysis are too simple, although the fact that it can do in-context learning is interesting. In addition, what is discussed in section 6 as real-world evidence does not directly support their main hypothesis.\n\n5. The logic of the paper is weak, and the paper is poorly organized. The arguments are not supported by rigorous experimental evidence. Almost all arguments around using the word \"**Therefore**\" are not rigorous (either the conclusion is not supported by the evidence or the things after, therefore, are logically irrelevant to things before). A large number of arguments are based on the author's thinking that A is **similar** to B, where first the similarity is poorly defined, and how from such similarity can we conclude their conclusion is usually unclear. For, in section 3.2, the author claims that predicting the correct pronoun in the next token completion using the information in the context is \"**similar**\" to inferring the class description z_y for y in text classification. \"**Therefore**\" modeling general text is similar to performing in-context learning. This may \"explain\" the linkage between in-context learning and emergent abilities of LLMs.\n\n6. The title of the work or the main motivation: human solving Pelican Soup riddles is similar to LLM doing in-context learning is based on some poorly defined subjective similarity.\n\n7. Could design more controlled experiments to study the importance of each individual aspect of the dataset (the current construction of the dataset is too complicated) and also to rule out other possibilities. For instance, the binary classification problem seems a bit too easy. Can the model learn shortcuts instead of using their \"world\" knowledge to solve the problem?"
            },
            "questions": {
                "value": "1. What's your view on the mesa-optimization view of in-context learning based on your Pelican Soup Hypothesis? Do they complement each other, and can one explain the other one?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4423/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_5jds",
                    "ICLR.cc/2024/Conference/Submission4423/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809758689,
        "cdate": 1698809758689,
        "tmdate": 1700740401626,
        "mdate": 1700740401626,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GGaLNzAXOW",
        "forum": "aaYBsuGRne",
        "replyto": "aaYBsuGRne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new theoretical account of the in-context learning (ICL) abilities of large language models.\nSection 2 describes a formal framework for NLP classification tasks, inspired by commonsense knowledge bases.\nSection 3 intuitively discusses, using this framework how the structure of language may lead to ICL abilities.\nSection 4 specifically describes three ways in which ICL shows a distribution mismatch relative to general language modeling.\nSections 5--7 adduce experimental evidence from three domains: a new synthetic dataset (\"Calcutec\"), evidence from a small LMM, and a digit addition task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I found the toy dataset (\"Calcutec\") quite interesting, and to improve in some ways over prior synthetic setups for ICL, such as Xie et al 2022 or Chan et al 2022, in that it includes a simple kind of logical reasoning.\n\n- Provides evidence that even smaller LLMs (GPT-2) can perform ICL with artificial/task-agnostic label symbols (which Wei et al 2023 argued only large LLMs can do).\n\n- provides empirical results from different domains"
            },
            "weaknesses": {
                "value": "- While I found the Calcutec experiment in particular to be innovative, the theoretical arguments in Sections 2--3 are quite hand-wavy and unspecific. There is no rigorous theoretical statement of the assumptions and conclusions made in the theoretical framework and the reasoning of how language modeling may lead to ICL.\n\n- While I believe the Calcutec toy dataset is an interesting contribution and a strength of the paper, it is limited in that the training dataset appears to bake in the repetitive nature of prompts by assuming that each \"paragraph\" in a document is about one of two latent concepts (\"topics\"), as in the prompting downstream tasks. A potential concern about the CoT evaluation is mentioned as a Question.\n\n- In the Digit Addition Task, the ability of the LM to complete the task in one go, whereas the training set usually had intermediate steps, is interpreted as an ICL ability representing a domain shift. However, as the training set also had intermediate steps stochastically dropped (independently, as far as I got from the paper -- so it is possible for all steps to be dropped simultaneously), it is not clear in which sense the test examples are out-of-domain relative to the training distribution. The same concern applies to the Calcutec dataset."
            },
            "questions": {
                "value": "- How exactly is Chain-of-thought evaluated in Calcutec? Does the prompt only include the first step in the chain? And under what circumstances is the LM's answer counted as correct -- are predictions rolled out until the \";\" paragraph appears? This question is crucial for assessing the meaningfulness of the CoT results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4423/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4423/Reviewer_MUGa",
                    "ICLR.cc/2024/Conference/Submission4423/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839439880,
        "cdate": 1698839439880,
        "tmdate": 1700322617829,
        "mdate": 1700322617829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "deEg7asXhs",
        "forum": "aaYBsuGRne",
        "replyto": "aaYBsuGRne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_cdtQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4423/Reviewer_cdtQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the \"Pelican Soup Hypothesis\" to explain in-context learning in large language models. The key idea is that in-context learning relies on models acquiring commonsense knowledge and reasoning skills from pretraining on general text. The paper formalizes NLP classification tasks as mapping inputs to output concepts based on commonsense rules and knowledge. Experiments on a synthetic dataset Calcutec show models can acquire in-context learning abilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides a clear and intuitive conceptual framework based on the Pelican Soup analogy to explain in-context learning.\n2. The proposed formalism for NLP tasks is simple yet quite general. It could be a useful tool for future theory research.\n3. Evidence from synthetic data, language modeling, and a toy task provide empirical support for the central hypothesis.\n4. The Calcutec dataset offers a nice testbed for studying in-context learning and model architectures.\n5. Analysis of the digit addition task sheds light on how model scale impacts reasoning abilities."
            },
            "weaknesses": {
                "value": "1. The explanations are conceptual. More formal theoretical analysis could better elucidate the mechanisms.\n2. More analysis could be done on how different pretraining corpora impact in-context abilities.\n3. The hypothesis focuses on classification; generative tasks may involve additional factors."
            },
            "questions": {
                "value": "1. Can we quantify the relative importance of different distribution shifts identified?\n2. How well does the formalism proposed capture more complex real-world reasoning?\n3. Is it possible to design pretraining objectives to better acquire commonsense and reasoning?\n4. How can we test if models learn explicit commonsense rules and reasoning versus pattern matching?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698959315287,
        "cdate": 1698959315287,
        "tmdate": 1699636416397,
        "mdate": 1699636416397,
        "license": "CC BY 4.0",
        "version": 2
    }
]