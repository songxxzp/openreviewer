[
    {
        "id": "VevF2hSeiF",
        "forum": "1vrS1zwekw",
        "replyto": "1vrS1zwekw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new paradigm of \"Scaling Tasks per Input\" for curating instruction-following datasets. Existing methods for improving language models' instruction-following capabilities have several limitations. Scaling-Inputs require extensive input-output pairs per task, causing oversensitivity to inputs. Scaling Input-Free Tasks expands tasks without inputs but struggles when inputs are needed. To alleviate these issues, this work instead diversifies tasks for each input text using a facet-based approach to enhance model's instruction-following capability.\n\nThe authors construct a dataset MUFFIN implementing this paradigm. They generate tasks by identifying textual facets of inputs to stimulate relevant instructions and reusing existing instructions matched to inputs. Careful balancing of classification and generation tasks is also proposed. Comprehensive experiments demonstrate MUFFIN's superiority over strong prior datasets, approaching human-annotated data quality. Ablation studies and human evaluation further verify the effectiveness of the techniques and the overall paradigm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The proposed `Scaling Tasks per Input` paradigm is novel and unique compared to prior instruction dataset curation schemes. By controlling the input and diversifying tasks, it helps models focus on instructions.\n\nQuality: The paper is technically strong, with a systematic data collection pipeline considering input diversity, instruction generation, output annotation, and task balancing. The analyses thoroughly verify the quality, diversity, and validity.\n\nSignificance: Thorough experiments across four benchmarks convincingly demonstrate MUFFIN's superiority, compared to prior synthetic instruction dataset, over strong baselines spanning different paradigms. Both the automatic and human evaluations demonstrated the merit of this dataset."
            },
            "weaknesses": {
                "value": "While the input-facet-oriented instruction generation approach is interesting, a major concern is the reliability of facets identified by the LLMs. As we know, hallucination remains an issue with LLMs. Though the model may correctly recognize textual attributes in most of time, some \"facets\" or the model generated *outputs* of these facet-based question could arise from hallucination. I don't see a specific mechanisms to handle this in the paper, except human audit on a small portion of samples. It's unclear whether these noise data, if it indeed exist, would hurt the model's reasoning capabilities. *One potential evidence is the substantially degraded performance on BBH benchmark compared to other baselines in Table 1.*\n\nI see similarities between this work and the self-instruct paper in the overall approach and principles for data generation, experiment design, and analysis approaches. Without addressing the trustworthiness issue I mentioned above, I would be hesitate to advocate the contribution of this work. \n\nI would like to see the authors to share some insights about the limitation of this work."
            },
            "questions": {
                "value": "1. In the facet-based instruction generation and output annotation, is there a specific strategy or mechanism used to guarantee the trustworthy of the facets or output annotation, except for human inspection? \n2. For facet-based generation, how is facet diversity quantified? And is this diversity measured with respect to ground truth facets? Some analysis confirming facet coverage would be reassuring.\n3. Could you share some insights about why the MUFFIN model performs much worse than other baselines on BBH benchmark in Table 1?Since BBH focuses on assessing reasoning skills, this seems to indicate there is still room for improvement on those capabilities. To provide more insights into the diversity of the dataset, it would be helpful to breakdown the facet into fine-grained categories, such as elaboration,  numeric calculation, and commonsense reasoning etc. By doing so, it could give a clearer picture of the diversity of this dataset, meanwhile also help to isolate the reason behind the degraded performance. \n4. The motivation behind balancing classification and generation tasks is clear, but how exactly is the decision made on which tasks undergo classification expansion? Prompting the model to make the decision?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_od6V"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645022724,
        "cdate": 1698645022724,
        "tmdate": 1700732695982,
        "mdate": 1700732695982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "COxq8ABvoL",
        "forum": "1vrS1zwekw",
        "replyto": "1vrS1zwekw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_j2qL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_j2qL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a two-step facet based instruction brainstorming method: in the first step, they use ChatGPT to recognize the textual facets of a given input; in the second step, they instruct ChatGPT to brainstorm task instructions. Besides direct instruction synthesis, the paper also suggests extracting human-written instructions from the training set and employ GPT-4 for binary classification. GPT-4 discriminates if the instruction can align with the input to create a valid task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper contributes to a important problem: dataset and alignment. The paper is well written.\n\n- The proposed method works as a reasonable approach to create a diverse tasks and instructions per task given the same input. GPT-4 works as a reasonable critique model, selecting high quality examples. Overall, the method is very simple but effective. \n\n- The paper reported strong empirical results on SuperNI-test, MMLU, T0-Eval, etc., compared to related work. Human evaluation acceptance ratio is significantly improved with Muffin."
            },
            "weaknesses": {
                "value": "- The work can have better ablation on whether the diversity in the dataset or the quality of the dataset helps more. For example, without the facet based brainstorming, how would it perform? On the other side, without the filtering (GPT4 critique) how would it perform?\n\n- The paper lacks a scaling analysis on the size of the finetuning dataset. How does the model performance scale with the number of examples in the dataset?\n\n- The discussion on task leaking is too short. I feel it is a very important problem to discuss in greater details. For example, what is the SoTA metric to detect data leaking? Why would semantic similarities be sufficient in detecting? Sentences can mean similarly while being semantically different. I feel the paper can be much stronger if this part is well addressed."
            },
            "questions": {
                "value": "1. Are there any particular reasons that different GPT models are selected for different purposes? Why would you pick ChatGPT over GPT4 to work on the facet recognition and instruction brainstorm? \n\n2. Any particular reason not to compare with FLAN [1]?\n\n[1]: https://arxiv.org/abs/2301.13688"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791214620,
        "cdate": 1698791214620,
        "tmdate": 1699636788902,
        "mdate": 1699636788902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C9hpB5bNV2",
        "forum": "1vrS1zwekw",
        "replyto": "1vrS1zwekw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_cEte"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use multiple facets of an input to construction instruction-input-output pairs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Curating instructions from different facets is an interesting idea. It can be combined with other data augmentation techniques.\n2. The presentation is straightforward and easy to follow."
            },
            "weaknesses": {
                "value": "1. This method heavily relies on the human-curated SuperNI dataset. The Instruction Brainstorming requires extracting Instruction-input pairs from SuperNI while instruction reconstruction simply use data from SuperNI but do compositional changes. However, after those changes, the results of finetuning an LLM become worse than originally finetuning with SuperNI, as in Table 1. Does this mean we are introducing noise?\n2. Comparisons to some baselines might not be fair, although being claimed to directly compared results in the paper. I tihink the comparison with Dynasour and Unnatural instruction is unfair as again, the curation of the new dataset relies on some higher quality human instruction in SuperNI while the other two work doesn't rely on modifying human instructions. I think to really demonstrate the effectiveness of this input-based instruction construction method, the authors should try to start from Dynasour or Unnatural instruction and create new instructions on inputs from those datasets instead of from SuperNI.\n3. The creation of the facets lack human control. This is a limitation in the methodology that the framework cannot control which facets will be generated from the first stage. \n4. This might be personal, but I have an overall negative attitude towards work like this one. This work basically falls into the category of exploiting the ability of LLMs to augment the instruction tuning data. The changes in this paradigm compared to previous ones look incremental, simply trying to change a bit on how we prompt LLMs to generate instruction-input-output pairs. Notice that this idea is also not new as in Dynasour, they also generate multiple instructions by asking LLMs to focus on different parts of the metadata. Most importantly, I don't know how this paradigm would really impact how people create instruction data. Does it worth trying this paradigm or simply using the queries to chatgpt to collect more data?"
            },
            "questions": {
                "value": "1. See the weakness section. Maybe elaborate more on your opinions about point 1 and 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832652494,
        "cdate": 1698832652494,
        "tmdate": 1699636788772,
        "mdate": 1699636788772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2az7xFirvz",
        "forum": "1vrS1zwekw",
        "replyto": "1vrS1zwekw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a technique for synthesizing instruction fine-tuning data using LLMs (ChatGPT and GPT-4). In particular, the paper draws a distinction between past works in this area which have focused on either adopting an instruction+input format and scaling the number of inputs per instruction (Scaling-Inputs), or adopting an instruction-only format and scaling the number of total instructions (Scaling Input-Free Tasks). As an alternative, the technique and resulting dataset, MUFFIN, presented in this work adopts the instruction+input format but uses automated techniques to scale the number of instructions per input (Scaling Tasks per Input).\n\nExperimental comparisons to extensive baselines are presented on SuperNI, MMLU, T0, and BBH (each classified as either Scaling-Inputs, Scaling Input-Free Tasks, or Hybrid) and demonstrate the effectiveness of the proposed approach in all 3 settings. Additional experiments and results from human evaluation are also presented."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The topic of how to effectively scale synthetic instruction datasets is relevant and timely, and the dichotomy of scaling inputs vs instructions is interesting.\n2. The experiments are extensive and the inclusion of evaluation of datasets from multiple settings (scaling-inputs, input-free, etc) is appreciated.\n3. The methodology and techniques are mostly well-described and presented clearly."
            },
            "weaknesses": {
                "value": "1. The muffin looks more like a cupcake to me.\n2. As noted previously, the dichotomy of scaling inputs vs instructions is interesting. However, the presentation makes it seem like one must choose between the two when really they are more like two extremes of a spectrum. This very naturally leads me to wonder whether a hybrid that scales both according to some mixture proportion would be most effective. This seems like a somewhat large omission given it would be very testable with the synthetic data here.\n3. All experiments use T5-3B or T5-11B. As many of the other datasets (Self-Instruct, Dolly, Alpaca) were developed and tested using more recent LMs (Llama, etc), it would be useful to see if/how the results change when fine-tuning such models. At minimum, it would be useful to see performance numbers for the original models developed with these datasets (if they are available) in the \"Existing Systems\" section of Table 1 to understand the performance drop due to changes in setup (model, hyperparams, etc).\n4. I think there are a few errors in the baseline discussion in section 5. In particular, this section says Dolly and Self-Instruct were collected with ChatGPT or GPT-4. As far as I know, Dolly is human-created (this is stated correctly in the Appendix) and Self-Instruct was produced using GPT-3.\n5. The presentation of human evaluation was somewhat confusing and left out key details. What model was used, 3B or 11B? Additionally, the evaluation is described as follows, \"we provide the volunteer with only task instruction, input, and model prediction.\" However, MMLU is classified as \"Input-Free\". What do annotators see as the input in this setting?\n\n**Minor Issues**\n\n1. The Scaling-Inputs, Scaling Input-Free Tasks, and Scaling Tasks per Input terminology is somewhat awkward. Have the authors considered something slightly simpler like Scaling-Inputs, Scaling-Input-Free, and Scaling-Instructions?\n2. I found the \"our MUFFIN\" terminology used throughout the paper somewhat annoying and distracting.\n\n------\n\nI have reviewed the authors' responses and raised my score accordingly."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6820/Reviewer_BsoU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6820/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858165636,
        "cdate": 1698858165636,
        "tmdate": 1700693029679,
        "mdate": 1700693029679,
        "license": "CC BY 4.0",
        "version": 2
    }
]