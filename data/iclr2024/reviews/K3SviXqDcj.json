[
    {
        "id": "Cc8EnXLxCY",
        "forum": "K3SviXqDcj",
        "replyto": "K3SviXqDcj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_yhUR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_yhUR"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of online continual learning, emphasizing the necessity of learner invariance to prevent catastrophic forgetting. The authors specifically focus on achieving invariance with respect to background and edge expansion, considering them as crucial target variables.\n\nThe proposed solution, IFO, is introduced as an approach to address this issue. However, in direct comparison with the competing baseline, OCM, IFO underperforms. Despite this, it is important to note that IFO complements OCM from an alternative perspective, providing a contribution to the field of continual learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "S1:\nThis paper delves into a relatively unexplored realm within continual learning: the concept of invariance.\n\nS2:\nThe authors offer a theoretical foundation for their argument, grounded in the principles of causality. However, I am not 100% sure about the validity of their findings, especially regarding the gradient norm. \n\nS3:\nThe authors conduct evaluations across various distinct class-incremental learning scenarios, including settings involving blurry and disjoint data, enhancing the comprehensiveness of their study."
            },
            "weaknesses": {
                "value": "W1:\nThe experimental contribution of this paper appears to be limited. Despite the central argument emphasizing the necessity of invariance for online continual learning, the experimental evidence provided is weak. Multiple instances indicate that the proposed approach, IFO, often underperforms the compared baseline, OCM. This suggests that a holistic approach might be more critical than mere invariance in preventing forgetting.\n\nW2:\nRegarding the technical contribution, the proposed IFO relies on an alignment loss, essentially a variant of the contrastive learning loss. This contribution, while present, seems limited in its originality and novelty.\n\nW3:\nThe scope of the studied invariance setting appears to be constrained. The choice to generate foreground-background blendings, where the specific nuisance to be addressed is known, might be considered somewhat artificial. This artificiality raises questions about the broader applicability and relevance of the findings.\n\nW4:\nIn terms of presentation, the quality of writing is notably low. The paper proves challenging to read due to unsupported claims, repetitiveness, excessively long sentences, and subpar figures and table presentation. These issues significantly impact the clarity and overall readability of the paper.\n\nConsidering these concerns, the paper is currently not deemed suitable for publication until these issues are addressed and improved in the rebuttal phase."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2142/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2142/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2142/Reviewer_yhUR"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698234534627,
        "cdate": 1698234534627,
        "tmdate": 1699636147037,
        "mdate": 1699636147037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NaoF4qYBFp",
        "forum": "K3SviXqDcj",
        "replyto": "K3SviXqDcj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_C77j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_C77j"
        ],
        "content": {
            "summary": {
                "value": "This paper highlights the importance of learning invariant features in mitigating the phenomenon of catastrophic forgetting in class-incremental continual learning through theoretical analysis.  A new method based on experience replay is then proposed for learning invariant features via creating environmental variants using 3 data augmentation techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper underscores the importance of learning invariant features in mitigating the phenomenon of catastrophic forgetting in class-incremental learning, thus providing a promising direction for the development of novel CIL algorithms.\n\n2. Despite the existence of prior research on the idea of learning invariant features in continual learning, this paper offers a review on the related works and places itself in a good position in the literature.\n\n3. The integration of the proposed method with OCM results in exceptional empirical performance."
            },
            "weaknesses": {
                "value": "1. The proposed methods incorporates many hyper-parameter, including $\\alpha$, $s$, $k$, $r_1$ in data augmentation. However, determining the optimal values for these hyper-parameters in continual learning can be challenging. Furthermore, this paper lacks comprehensive studies of the effects of the hyper-parameters and their determination.\n\n2. The paper does not empirically verify the individual impact of learning invariant features in mitigating catastrophic forgetting. It is always combined with replay. Moreover, the accuracy and forgetting of the proposed IFO are not competitive compared to OCM and its optimal performance relies on integration with OCM.\n\n3. Some minor issues:\n- No experimental results on time efficiency comparison in the Appendix.\n- $\\theta$ is repeatedly defined as feature extractor parameter and as binary vector in Eqn. (2)."
            },
            "questions": {
                "value": "1. How is the integration of IFO and OCM achieved? Does it involve the direct addition of two losses during model training? Furthermore, how does IFO coalesce with OCM in feature learning? While the objective of OCM is to learn as many features as possible, IFO solely focuses on invariant features, leading to some conflicting goals.\n2. For background color augmentation, during the initial stages of training, when the classifier is not yet fully trained, there may be concerns regarding the accuracy of background color augmentation. There may be concerns where CAM fails to correctly identify the background, resulting in incorrect augmentation.\n3. For method II proposed in 5.3, as mentioned, \"We collect all new task data already stored in the buffer\". Method II seems not applicable if there is no new task data in the buffer, which is often the case in continual learning when new task appears. Is this correct? Also, when the buffer is updated, do the k clusters get updated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698574566687,
        "cdate": 1698574566687,
        "tmdate": 1699636146968,
        "mdate": 1699636146968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z3sYrL0JyE",
        "forum": "K3SviXqDcj",
        "replyto": "K3SviXqDcj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_Xe1e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2142/Reviewer_Xe1e"
        ],
        "content": {
            "summary": {
                "value": "Previous studies in this direction have shown that learning holistic representations in Continual Learning settings can help to mitigate forgetting. However, in this paper, the authors argue that the representations must also be invariant. This paper shows, theoretically and empirically, that having both holistic and invariant representations helps in online scenarios. After raising the need to remove spurious correlation of the previously learned model, the authors propose a new method for Online Continual Learning. They show good performance in traditional disjoint task scenarios in multiple benchmarks and also in blurry task boundaries and data shifts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Studying the theory behind a scenario and understanding the how and why of the problems is an excellent way of proposing new methods.\n    - It is more promising when results on the empirical side also accompany these findings.\n- Evaluating the method in various online scenarios proves its robustness.\n- The augmentations proposed are interesting. Increasing the variability of the data can help improve the generalization capabilities of the model. \n    - These augmentations can work better than MixUp, under specific settings."
            },
            "weaknesses": {
                "value": "- My main concern is the Theoretical Analysis, which can have significant repercussions in the practical section. I agree with the intuition of the authors that if we focus on learning invariant and holistic representations, we can mitigate forgetting. However:\n    - To learn these representations, the authors assume that the model converges. Something that is not the case in most Online scenarios. The paper does not show that the model learns invariant or spurious correlations. Showing that a model learns these kinds of representations is a challenging task since there is a whole research area that is focusing in this direction.\n    - Assuming that one can select which representations are invariant or spurious is a big If, more with the limited distributions that are stored in the buffer. One may obtain an approximation, but solving this problem is not trivial, as multiple work in spurious correlation has proven.\n    - I appreciate the great work done on the theoretical analysis of the work, together with the motivations and intuitions. However, the demonstrations and assumptions are not without issues.\n- There are a few mentioned in the paper that the proposed method works in Class-Incremental Learning settings, without specifying the online setting. However, it could be better to refer to Online Learning. Changing between class-incremental and online class-incremental and use both interchangeable could create confusion.\n- The augmentation methods proposed only work in particular scenarios. Where is one easy-to-detect object and is centered in the image. Something that occur in limited opportunities.\n    - Also, as mentioned in the paper, the authors assume that the background is the primary source of spurious correlation. This limits the proposal even more.\n- The paragraph \u201cLearning invariant representations\u201d in the results section is incomplete. It mentioned that the model was trained with original datasets, and I would assume until convergence. Training the model this way could create completely different representations. It is essential to show that the proposed method in Online Learning can generate those representations, something that, as mentioned, I am not entirely sure about."
            },
            "questions": {
                "value": "- Please explain the last paragraph of Appendix A. There are a few steps that are not trivial to me.\n- Can we have a CL method that works under your assumptions but without a memory buffer? Or is access to a memory essential to achieve a model that does not forget?\n- What are the similarities or differences between the \u201cdata environment shift setting\u201d and a \u201cdomain incremental\u201d?\n- One significant limitation of Online scenarios is the computational capacity. Some argued that most methods underperform due to the low time to train the model. However, in the proposal, you are increasing the losses, by increasing the augmentations (Eq 1, 5 and 7). How much do you expand the batch size to be able to see all the samples? Or do you keep the batch size fixed and increase the interactions you train the model?\n    - How much does the computational time increase?\n- The size of the buffer and the samples stored can significantly influence the accuracy obtained in memory-based methods. How does the K value behave when changing the buffer size?\n- Can you explain Table 3 in a different way? When it said, \u201cno align\u201d, you just removed the align? In most cases, the difference is less than 1% of accuracy. How can you identify if one of these components is not necessary?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692495304,
        "cdate": 1698692495304,
        "tmdate": 1699636146877,
        "mdate": 1699636146877,
        "license": "CC BY 4.0",
        "version": 2
    }
]