[
    {
        "id": "aPlCEZmDUD",
        "forum": "8Q4uVOJ5bX",
        "replyto": "8Q4uVOJ5bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission345/Reviewer_F2Yp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission345/Reviewer_F2Yp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes R&B, a region- and boundary-aware cross-attention guidance for a zero-shot grounded Text-to-Image diffusion model. The goal of such a method is to produce an image conditioned on an input text prompt and bounding boxes for objects to be generated. While previous methods have tackled this task, they still lack in terms of accurate spatial generation and such issues such as missing objects and attribute-noun binding issues. First, attention maps for every concept are extracted. Next, bounding boxes are estimated from those attention maps and used for optimization in consecutive timesteps. The region-aware loss maximizes IoU of estimated boxes with ground-truth boxes, while the boundary-aware loss maximizes activation per object within the corresponding box."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and well-structured.\n- The presented results are good.\n- The proposed method is novel."
            },
            "weaknesses": {
                "value": "- It is unclear to me how the presented work should be viewed in comparison with ZestGuide [1]. The authors cite it and use the method in [1] to compute an aggregated attention map. However, there is no visual/quant. comparison and a discussion on the strengths/weaknesses are missing. From a high-level, it seems that [1] is able to solve the same issues using a simpler method, namely a zero-shot segmentation approach to guide the diffusion process on a per-object level. Furthermore, [1] is able to use free-form masks instead while the presented approach is limited to boxes.\n\n[1] Couairon et al., 2023, Zero-shot spatial layout conditioning for text-to-image diffusion models"
            },
            "questions": {
                "value": "- There is recent work that tackles poor attribute-noun binding in Text-to-Image models. Can the authors provide examples where one object has an additional attribute such as a color and visualize how the generated image is changed when varying the attribute or noun (e.g. \"blue car\" -> \"orange car\" -> \"orange fox\")? See [2] examples.\n\n[2] Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models, https://arxiv.org/abs/2305.13921"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693548780,
        "cdate": 1698693548780,
        "tmdate": 1699635961633,
        "mdate": 1699635961633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QMYWm330iL",
        "forum": "8Q4uVOJ5bX",
        "replyto": "8Q4uVOJ5bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission345/Reviewer_cFE6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission345/Reviewer_cFE6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach to zero-shot grounded Text-to-Image (T2I) generation in diffusion models, termed Region and Boundary (R&B) aware cross-attention guidance for layout generation. Notably, this approach is training-free. By incorporating region-aware loss for guiding cross-attention maps and boundary-aware loss for refining localization, the method delivers enhanced ground-alignment performance. Empirical evidence from experiments and ablation studies underscores the method's superiority over existing state-of-the-art techniques for grounded T2I generation in diffusion models, demonstrating both qualitative and quantitative improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper employs a dual approach, utilizing both region-aware loss and boundary loss to align the generated images with the text and boundary, presenting a rational and innovative methodology.\n\n2. By incorporating dynamic thresholding into the aggregated cross-attention maps, this method introduces a novel optimization technique for accentuating foreground objects, enhancing the utility of aggregated cross-attention maps.\n\n3. The visual results presented in this paper provide compelling evidence that the R&B method surpasses current state-of-the-art techniques in grounded Text-to-Image generation for diffusion models."
            },
            "weaknesses": {
                "value": "1. While the experimental results presented appear less solid, it's worth noting that Attention-refocusing is a versatile component that has demonstrated effectiveness in enhancing various methods, including Stable Diffusion, Layout-guidance, and GLIGEN. The results, as detailed in Tables 2 and 3 of the Attention-refocusing method, highlight the superiority of GLIGEN+CAR&SAR in achieving state-of-the-art (SOTA) performance, which outperforms the proposed method, as indicated in Table 1. Further clarification regarding this observation would be greatly appreciated.\n\n2. Although the LIMITATIONS AND DISCUSSION section acknowledges that the model's capacity may be exceeded when dealing with a high number of objects, providing a more in-depth analysis of this phenomenon would enhance the discussion. Notably, the BoxDiff model appears to yield more favorable results in generating a large number of objects.\n\n3. To enhance the comprehensiveness of the study, including additional visual comparisons with other methods would be advantageous. For instance, an evaluation of how our model performs with consistent boundary and prompts when compared to BoxDiff or other comparable methods could provide valuable insights.\n\n4. It is imperative to incorporate more detailed analyses, such as color and object counts. Additionally, there is a concern regarding the last row in the rightmost image of Figure 1, where the text \"A bundle of\" does not seem to be adequately performed."
            },
            "questions": {
                "value": "1. How does this model perform when provided with more intricate prompts, such as \"a pink rabbit with a white head,\" with the boundary pointing to both the \"rabbit\" and \"head\"?\n\n2. I find the result in the last row of the rightmost image in Figure 4 perplexing. The image seems to exhibit incomplete deformation. What factors may have led to this particular outcome?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743997346,
        "cdate": 1698743997346,
        "tmdate": 1699635961559,
        "mdate": 1699635961559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FThnwi4LI6",
        "forum": "8Q4uVOJ5bX",
        "replyto": "8Q4uVOJ5bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission345/Reviewer_KFuf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission345/Reviewer_KFuf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method for grounded text-to-image generation based on the introduced region-aware and boundary-aware losses. These two losses help to modify the generation process of a pre-trained diffusion model in a way that generated objects adhere to the locations of provided bounding boxes. The method is compared to recent state-of-the-art backward methods such as BoxDiff, Attention refocusing, Layout-guidance. The experiments demonstrate that the method outperforms previous approaches in the accuracy of following the conditioning instructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses an interesting and in-demand problem of grounded T2I generation with diffusion models, tailored for applications that require control over locations of objects via user-friendly inputs.\n- The proposed approach is compared to the most recent backward baselines, including BoxDiff [ICCV23].\n- The demonstrated results indicate clear improvement over the baselines in terms of accuracy of following the conditioning instructions like the size or location of specified objects."
            },
            "weaknesses": {
                "value": "- My main concern is unclarity in the conceptual differences of the proposed method to other methods. Training-free modification of attention map is not a new idea as other methods like (Chen et al., 2023; Xie et al., 2023; Phung et al., 2023) also design objectives that encourage the model to shift objects towards specified bounding boxes. These methods generally exhibit same motivation (align objects with bounding boxes) with similar implementation (shift cross-attention maps). It is not clear what makes the proposed approach conceptually different and where the performance gain comes from.\n   - Could the authors please elaborate their explanations in intro: 1) \"previous methods fail to provide accurate spatial guidance\" - why does this happen based on their design, and why does this not happen in R&B method based on its design? 2) \"inherit inconsistencies from the original T2I model\" - doesn't this also happen in R&B? Or what part of the design helps to avoid them?\n- Similarly, it is not clear to me what is the technical novelty of the proposed method. The narration of the method is structured as the adaptation of existing methods (Li et al., 2023b, Li et al., 2023b) to the task of interest without much new analysis. Could the authors please elaborate what could be a broader-impact technical lesson or insight for the community from Sec. 3?\n- I feel the presentation of experiments in Sec. 4 can be improved. For example, there is no presentation of baselines used in Table 1, Fig. 4, so it is difficult to the match the names of methods with references to respective papers.\n- Why are there no comparisons to some other baselines, like GLIGEN?"
            },
            "questions": {
                "value": "Please answer the questions or comment on concerns from the Weaknesses section.\n\n[UPD rebuttal - score raised from 5 to 6]"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Reviewer_KFuf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756317537,
        "cdate": 1698756317537,
        "tmdate": 1700564719275,
        "mdate": 1700564719275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8YE59ZAvoE",
        "forum": "8Q4uVOJ5bX",
        "replyto": "8Q4uVOJ5bX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission345/Reviewer_Nptk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission345/Reviewer_Nptk"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a training-free approach to grounded text-to-image generation. The key idea is to instrument the cross-attention weights in a pre-trained Stable Diffusion model to control image layout, thereby generating objects within the given bounding boxes. The key innovation of the proposed method lies in the energy functions in guidance. Two such functions are introduced; the region-aware loss facilitates the minimum-sized box enclosing the object to match the ground-truth box, whereas the boundary-aware loss encourages rich content variation within a box. Both qualitative and quantitative experiments validate the effectiveness of the proposed loss functions. The method outperforms several training-free baselines by a wide margin, enabling more precise layout control over text-to-image generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is training-free. It controls image layout by manipulating cross-attention weights using guidance. Compared to methods that train auxiliary modules for controllability, training-free methods readily support new architectures and model checkpoints without re-training.\n\n- The proposed method carefully reasons about the failure cases of existing methods and introduces key improvements to cross-attention based guidance for grounded text-to-image generation. These improvements cover different aspects of object localization, including dynamic thresholding of attention maps, differentiable box alignment and and boundary awareness. They offer key insights into the internal workings of Stable Diffusion and may be of interest to the community in a broader context.\n\n- The proposed loss terms effectively improve bounding box localization and address missing objects. The method outperforms several training-free baselines in both qualitative and quantitative experiments. Visualizations of cross-attention maps demonstrate clear localization and separation of object regions."
            },
            "weaknesses": {
                "value": "- Several training-free methods similarly apply cross-attention guidance to control object shape and location. To this end, the contribution of the present work is more or less incremental. While the loss functions are new, the novelty mainly lies in the implementation details, with the overall idea (cross-attention guidance) largely identical to previous methods.\n\n- Comparison with training-based methods are lacking. For example, GLIGEN (which the paper cites) is a training-based method for grounded text-to-image generation. As they solve the same task, it would be helpful to compare with those methods qualitatively and quantitatively to reveal the strength and weakness of both approaches (and also for the sake of completeness)."
            },
            "questions": {
                "value": "- I want the authors to comment on the runtime of the proposed method. How is it compared to the vanilla generation process (i.e., without guidance). In addition, how sensitive is the method to hyper-parameters?\n\n- Is the method able to handle overlapping objects? Most examples in the paper show non-overlapping bounding boxes. I am curious about how cross-attention guidance behaves when two or more objects compete for the same pixel.\n\n- I want to understand the performance of the method with respect to object size. Since the attentions maps have relatively low resolution (down to 16x16), I am wondering whether the losses (especially the boundary-aware loss) are still effective in the presence of small objects. Some qualitative results and visualizations would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission345/Reviewer_Nptk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812795276,
        "cdate": 1698812795276,
        "tmdate": 1700709676054,
        "mdate": 1700709676054,
        "license": "CC BY 4.0",
        "version": 2
    }
]