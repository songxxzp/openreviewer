[
    {
        "id": "EY69iHaiiv",
        "forum": "W478nWXfwO",
        "replyto": "W478nWXfwO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_fmRi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_fmRi"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the performance of 15 pre-trained vision models under different visual appearances. It finds that ViT is a strong predictor of out-of-distribution generalization. This paper is to thoroughly answer the questions \u201cwhich models generalize?\u201d and \u201chow can we predict how well a pre-trained model will generalize?\u201d"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and easy to follow. It's worth noting that there is a summary after each experimental section, which aids in clearly grasping the conclusions.\n\n2.\tThe paper utilizes numerous pre-trained models to substantiate its findings.\n\n3.\tThe paper includes the real-world experiments.\n\n4.\tI believe the conclusions presented can offer some valuable insights to the field."
            },
            "weaknesses": {
                "value": "1.\tThe testing scenarios are not significantly different from the experimental settings, leading to conclusions that may not be solid.\n\n2.\tThe scope of the conclusions is too narrow. While pre-trained models for visuo-motor control is a broad topic, this paper's findings are limited to the relationship between the ViT model and OOD performance under visual distribution shift, which limits in the specific setting and specific model. In this filed, there are too many different conclusions under different settings. Some slight differences may change a lot.\n\n3.\tThe conclusion that models pre-trained on manipulation-relevant data do not necessarily generalize better than models trained on standard pre-training datasets has been similarly addressed in previous papers."
            },
            "questions": {
                "value": "1.\t\"We develop a benchmark for out-of-distribution generalization within FrankaKitchen and Meta-World.\" I think such a level of modification to the environment doesn't qualify as developing a benchmark. It's more accurate to say that some environments were constructed to verify our conclusions.\n\n2.\tI find the spatial features in the figure intriguing. Could you conduct more analyses on how they change with varying levels of distracting factors? Additionally, I would suggest annotating Figure 1 with the value of the Jaccard Index.\n\n3.\tWhich pre-trained models were trained by you, and which ones are off-the-shelf?\n\n4.\tThe description for Figure 5 appears twice.\n\nSome related works:\n\n[1] Lin, Xingyu, et al. \"SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks.\" arXiv preprint arXiv:2307.03567 (2023).\n\n[2] Yuan, Zhecheng, et al. \"Pre-trained image encoder for generalizable visual reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 13022-13037."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethic cocerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Reviewer_fmRi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698472184102,
        "cdate": 1698472184102,
        "tmdate": 1699636072729,
        "mdate": 1699636072729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pitei42A9E",
        "forum": "W478nWXfwO",
        "replyto": "W478nWXfwO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_ujtD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_ujtD"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an empirical evaluation of the efficacy of various pre-trained learning strategies and models in the context of robotic applications using reinforcement learning.  \nThrough the use of pre-trained encoders, the authors employ imitation learning based on expert demonstrations to derive control policies for two distinct robotic manipulation benchmarks.  \nA comprehensive comparison is made across 15 pre-trained models, considering both their training performances as well as their generalization capabilities when faced with domain shifts.  \nFrom different perspectives, the study seeks to discern whether models pre-trained on task-relevant data yield better transfer results than those pre-trained on generic image datasets, such as ImageNet, and whether self-supervised learning offers better generalization than supervised learning. The study also investigates which architectures demonstrate superior transfer capabilities.  \nAdditionally, the authors propose metrics that could potentially predict the generalization performance during unforeseen domain shifts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "*   The paper undertakes a comprehensive study, examining a variety of pre-trained models and pre-training strategies, specifically targeting zero-shot out-of-distribution (OOD) generalization. The sheer volume of models tested is laudable, showcasing a thorough investigative approach.\n*    The introduction and utilization of the Jaccard index emerges as an innovative and effective method to predict future model generalization. Targeting high Jaccard index scores might emerge as a valuable auxiliary objective in representation learning tasks.\n*    One notable observation is that pre-training on a widely-used dataset like ImageNet yields better results than pre-training on task-specific manipulation datasets. This insight is intriguing and opens up avenues for further exploration and understanding.\n*    An understated yet crucial observation lies in the comparison of MAE with MoCO/DINO. It becomes evident that data augmentation plays a pivotal role in out-of-distribution generalization. Notably, the top-performing models in Figure 4 heavily employ robust data augmentation techniques. This underscores the pivotal role data augmentation plays, and it would be beneficial for the authors to delve deeper into this aspect in their discussions."
            },
            "weaknesses": {
                "value": "*  A pivotal reference, [1], detailing state-of-the-art generalization performance, on the DMcontrol suite generalization benchmark, using embeddings from a ResNet's second layer, is absent. It's imperative that the corresponding PIE-G method be included in the benchmark and discussed.  \n*  The heavy use of acronyms and specific methods without thorough presentation within the main body of the paper hampers readability, especially for non-experts. Introducing them in the related work or at least highlighting that they are elaborated upon in the appendix would have enhanced clarity. Additionally, acronyms in figures should be clearly defined at the onset of the experimental section.  \n*  The distinction between in-domain and out-domain generalization is blurry. As I discerned, the paper's in-domain generalization seems to pertain to training performance. This conflicts with more conventional uses of the term, such as evaluation on previously unobserved positions during training.  \n*  The observation that self-supervised learning outperforms supervised learning in yielding generalized features is already discussed in [2], as acknowledged by the authors, making it less of a novelty.  \n*  The authors' suggestion to use training performance as an indicator of future generalization seems fairly intuitive, especially in scenarios where no distinct strategies were applied to enhance generalization.  \n*  The paper presents a hypothesis on shape bias correlating linearly with domain shift success for Resnets, but this is supported by merely three data points. This is a thin foundation for drawing substantial conclusions.   \n*  The Jaccard index experiment is insightful but would benefit greatly from a more thorough description in the main paper rather than relegating it to the appendix.  \n*  While the appendix treats all MoCo models as analogous to MoCo, the MoCo method should have been introduced properly.\n\n**Minor Comments:**\n\n* The statement, \"Hu et al. (2023) shows that model performance is highly sensitive to evaluation\" seems somewhat redundant.  \n* Figure 2 seems superfluous, and its space might be better utilized for succinctly describing the benchmarked methods.  \n* Chen et al., 2021 references include an extraneous '*'.  \n* Figure 1: It should clearly indicate which models are visualized, aside from MVP.  \n* Figure 3: Considering Figure 4's results, the omission of MoCo is puzzling.  \n * An inexplicable empty space is present at the top right of page 9.  \n\n**References:**  \n[1] Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning. Zhecheng Yuan et al. NeurIPS 2022.\n[2] The unsurprising effectiveness of pre-trained vision models for control. Simone Parisi et al. ICML 2022."
            },
            "questions": {
                "value": "Could you discuss on [1] and if possible add it to your benchmark?\n\nCould you provide a similar experiment than the Jaccard index using any attribution method like Grad-Cam [3] or Rise[4] for instance on the Resnets methods?\nWhen you talk on in-domain generalization, where the evaluation scenarios oberved during training?\nIn the ViT vs Resnets paraggraph, I don't get the sentence \"In Figure 6, out of the seven pre-trained models that perform best out-of-distribution six are ViTs.\" . Aren't all the mentinned models ViTs since we are loccing at the Jaccard index of their attention heads?\n\n**References:**  \n[1] Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning. Zhecheng Yuan et al. NeurIPS 2022.\n[3]Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. Ramprasaath R. Selvaraju et al. ICCV 2017\n[4] RISE: Randomized Input Sampling for Explanation of Black-box Models. Vitali Petsiuk et al. Arxiv 2018"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "*   Could you elaborate on the study by Yuan et al. [1]? Would it be possible to incorporate their work into your benchmark for a more comprehensive comparison?\n\n*    The Jaccard index experiment is compelling. Would it be possible to conduct a parallel evaluation using attribution techniques like Grad-CAM [3] or RISE [4], especially focusing on the ResNet architectures? Such an experiment might provide deeper insights into the model's focus and the critical regions of input.\n\n*    When referring to in-domain generalization, were the evaluation scenarios ones that had been observed during the training phase?\n\n*    In the section comparing ViT and ResNets, there's a statement: \"In Figure 6, out of the seven pre-trained models that perform best out-of-distribution six are ViTs.\" This seems contradictory, as the focus is on the Jaccard index of attention heads, implying all mentioned models should be ViTs. Could you elucidate this?"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Reviewer_ujtD",
                    "ICLR.cc/2024/Conference/Submission1442/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698615300795,
        "cdate": 1698615300795,
        "tmdate": 1700731322772,
        "mdate": 1700731322772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wToZ1fLOn1",
        "forum": "W478nWXfwO",
        "replyto": "W478nWXfwO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_5UTX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1442/Reviewer_5UTX"
        ],
        "content": {
            "summary": {
                "value": "This work studies the comparative generalisation performance of policies trained on top of pre-trained representations trained for different tasks like control and self supervised learning. The generalisation is studied across visual distribution shifts like texture, lighting and object distractor changes, after training policies on a downstream manipulation task on as few as 10 demos. Experiments show (perhaps counterintuitively) that representations that were trained for control are less robust to the shifts as compared to representations trained with self supervised objectives on imagenet data. They study the performance trends for 2 architectures (ViT vs resNet), and attempt to find a different metric that might be correlated with performance under distribution shift. Among the tracked metrics (shape bias, segmentation ability, linear probing accuracy), they show that segmentation ability of the attention layers of a pre-trained representation model are most predictive of performance of the trained policy after the lighting etc is slightly shifted."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The premise of finding metrics that could be correlated with downstream robustness is an interesting and a useful direction\n2) The study is conducted systematically over a range of architectures and representations."
            },
            "weaknesses": {
                "value": "1) My main concern is that the paper assumes an artificially restricted setting with unrealistically limited demonstrations for a task (10, collected without varying a single thing in the background like lighting etc), on which even the training distribution performance is highly suboptimal (success rate is 40-60%).  At this performance level, the policy would not be deployed even in the training environment. Typically, in such a limited demonstration regime, the demonstrations would be collected by varying placement and lighting conditions.\nI\u2019m not sure (yet) that the paper actually measures what it says it measures - since the demos are so few and the training distribution policy performance is so low, the current degradation in performance under visual shift might not even be due to the representation being bad, but due to the policy network overfitting on correlations in the training data.\n2) Consider this: It seems possible to me that if two representations A and B are tested using the paper\u2019s protocol and A encodes some extra variables that are spuriously correlated to actions in the limited demo data, then the policy could latch onto them to identify actions for prediction. However, these extra variables could be useful for prediction over a wider distribution of envs/conditions and if the demos weren\u2019t spuriously correlated with these extra encoded variables, then the learnt policy might have been better than the one learnt using B. Therefore this trend might change if you went from 10 to say 40 demos where in the 40 demos the lightning or another condition is varied in a limited range [x-y] and the test time condition is a value outside this range. Including experiments of this format would make a much more convincing case IMO, and would be much closer to an actual real world use case."
            },
            "questions": {
                "value": "1) The current description of how the shape bias is calculated is unclear, it would be great if this can be described more explicitly, along with including a motivation for why is it reasonable to adopt this as a metric. \n2) Why does Section 5.2 say the probe (and shape bias value) is unavailable for MVP/R3M/VIP models? A linear probe just has to be trained on imagenet on top of the representation right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1442/Reviewer_5UTX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836614243,
        "cdate": 1698836614243,
        "tmdate": 1699636072573,
        "mdate": 1699636072573,
        "license": "CC BY 4.0",
        "version": 2
    }
]