[
    {
        "id": "VVQY3zuxDX",
        "forum": "23b9KSNQTX",
        "replyto": "23b9KSNQTX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv"
        ],
        "content": {
            "summary": {
                "value": "the authors build a more light-weighted embedding for document similarity detection task, and conducted extensive experiment to prove the performance of their design. On their purposed dataset, they prove that the new model/embedding has better overall performance in multilingual setting retrieval. Overall the paper is complete and easy to follow. However there is not much original contribution in terms of model design, and the improvement seems incremental and marginal. See comments below"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "extensive experimentation - from the purposed multilingual dataset W4NT3D to NEWS-COPY dataset, the experiment is thorough and convincing in demonstrating purposed effectiveness\nefficiency over other nn models - personally I feel the paper is most valuable in that the purposed model is much simpler than the rest deep models, while their embedding maintains a relatively same or even better performance, although it's on the purposed dataset. the saving from efficiency instead of performance is the essence"
            },
            "weaknesses": {
                "value": "originality - the purposed model arch is more like a concatenation of existing methods/building blocks, and frankly speaking I can see very little original or significant contribution\nincremental contribution - the improvement is claimed on the purposed multilingual dataset, while it's not substantially surpassing the other deep model counterparties. for the real world duplication detection task, the traditional minhash based method is a better choice with decent performance IMHO, considering computation constraint"
            },
            "questions": {
                "value": "table 4 column 4 - should be recall non-duplicates?\npalm 2 gecko embedding performed much worse in the paper than it should be. any hypothesis or analysis on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_XEvv",
                    "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719853441,
        "cdate": 1698719853441,
        "tmdate": 1700643406850,
        "mdate": 1700643406850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "usGBUg0i2p",
        "forum": "23b9KSNQTX",
        "replyto": "23b9KSNQTX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RETSim, a lightweight text embedding model designed for near-duplicate text retrieval, clustering, and dataset deduplication. RETSim is a cascade system comprising three models: a character-level vectorizer, a small transformer model, and an embedding averaging module. There are two variants, RETSim (Near-Dup) and RETSim (Partial-Dup), tailored for full matching and partial matching, respectively. The authors evaluate RETSim across multiple datasets, including their self-proposed W4NT3D benchmark, the two news deduplication dataset, and a span email clustering dataset, demonstrating its superiority over neural and hashing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed model is effective and efficient.\n- RETSim trained on mC4 can be applied to a variety of datasets."
            },
            "weaknesses": {
                "value": "- The experiment in Section 5.1 makes no sense. Apparently, according to previous experiments, RETSim should be able to find more duplicates. How using RETSim may impact downstream language model pre-training remains unclear.\n- Some baselines are missing in Table 4 and Table 7.\n- Certain aspects of the method are not clearly explained, such as the process for obtaining the (512, 24)-dimension tensor from the vectorizer (Section 3.1) and the Multi-Similarity Loss, which appears to be a less common concept that deserves more detailed explanation (Section 3.2).\n- The paper lacks a dedicated Conclusion section, with Section 7 not effectively serving as a substitute conclusion."
            },
            "questions": {
                "value": "- It seems that the dataset used in the experiment in Section 5.2 is a non-open dataset. The authors should provide more details on the dataset.\n- For a same amount of input, does RETSim (Near-Dup) produce more embeddings than RETSim (Partial-Dup)? If yes, why isn\u2019t the speed separately measured in Table 6? (I think RETSim (Partial-Dup) is slower, since it needs to retrieve against a significantly larger set of text embeddings during de-duplication?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_rc5k"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753119667,
        "cdate": 1698753119667,
        "tmdate": 1700202480433,
        "mdate": 1700202480433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "553tzWQpA7",
        "forum": "23b9KSNQTX",
        "replyto": "23b9KSNQTX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces RETSim (Resilient and Efficient Text Similarity) to identify near duplicate whole text or partial text efficiently, with applications in deduplication, clustering, etc. The proposed model is designed to be lightweight and learned with simple corruption-like augmentations (hence mimicking the typical dedup errors). The proposed method has been carefully tuned and engineered and demonstrated better performance than both non-learned approaches and learned methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The problem of dedup is a fundamental one and has many traditional applications. In the era of LLMs, it gains one more important role as deduplicating the training dataset, which is one of the crucial factors to training a good LLM. \n\n* Although the proposed technical solution is simple, it is designed with care (to balance the sophistication and effectiveness) and hyper-parameters are carefully tuned. \n\n* The performance results are excellent over a comprehensive set of tasks and datasets."
            },
            "weaknesses": {
                "value": "* There is no analysis on why the almost straight-forward design produces better results (F1) than other learned algorithms. From the ablation study (e.g., Sec 6), it seems the benefit is more from tuning parameters?\n\n* Similarly, how important is the training corpus and the data augmentation methods? E.g., what if the data distribution, corruption pattern, etc. on mC4 is drastically different from those in the testing corpus? \n\n* MinHash + LSH achieves a strong performance on real dataset (Table 4). It seems there is no detail on the parameter of the LSH part, and therefore it is hard to know how much performance loss is due to the LSH part (e.g., if the number of \"OR\" part is small, it may hurt the performance). Also why +LSH? I can understand that MinHash + LSH is a practical solution for large corpus due to its good runtime efficiency, but run-time efficiency is not report for the proposed method. In fact, it is not clear whether and which index you use to find the near duplicate (chunks) with your method. Do you use a vector index and which one? \n\n* It will be better to perform a case study or detailed error analysis to understand the false positive and false negatives for the proposed method."
            },
            "questions": {
                "value": "See the questions asked in the \"Weakness\" part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4287/Reviewer_pVcX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698937652894,
        "cdate": 1698937652894,
        "tmdate": 1699636396451,
        "mdate": 1699636396451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "byh2kqq6cK",
        "forum": "23b9KSNQTX",
        "replyto": "23b9KSNQTX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4287/Reviewer_XRFc"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the problem of learning robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. Towards this end it proposes a lightweight multilingual deep learning model called RETSim. The work argues that previous solutions for near-duplicate text retrieval are not resilient to typos in the text documents and are vulnerable to adversarial attacks. Further, some of the previous solutions are very parameter-sensitive and consequently need heavy tuning and/or are too big to be used effectively in real-world applications of near-duplicate text retrieval. The proposed solution to the problem of learning robust metric embeddings for near-duplicate text retrieval makes use of a large typo-augmented training corpus, RETVec text vectorizer, a lightweight  transformer block (Hua et al., 2022) and  a metric learning training regime. The tokenizer splits the text into multiple chunks of size at most 512 characters and encodes each chunk using a character encoder. Each encoded chunk is fed to a lighweight transformer that computes a 256 dimensional embedding for the chunk. The embeddings from all the chunks of a text input are then combined into an embedding for the full text by averaging. \n\nThe model is trained on a dataset derived from the multilingual C4 dataset by applying sentence, work and character level augmentations on the text. Multi-Similarity Loss is used in training which attempts to make augmented versions of the same text closer to each other in the embedding space. \n\nIn addition to addressing the technical problem of measuring similarity of near-duplicate texts, this work introduces a new benchmark for the evaluation of such methods. The benchmark dataset called W4NT3D is particularly oriented towards evaluation of near-duplicate text retrieval models on multilingual text corpora in the presence of typos, word manipulations, and sentence/paragraph-level modifications. \n\nThe work presents results from the experiments comparing the proposed method with several baselines including some hash-based methods and some deep-learning-based methods. Results are presented for W4NT3D dataset. The proposed method performs better than all the baselines overall but not by a huge margin (Multilingual E5-Base is very close) and for Chinese and Japanese languages, it performs slightly worse than the best baseline for these languages (Multilingual USE). \n\nThe experimental study also reports results on different types of augmentations on the same dataset. Interestingly, all methods seem to do well for paragraph and sentence level augmentations. For word-level augmentations, hashing based methods do worse than other methods and for character-level augmentations, the proposed method fares best.\n\nFurther experimental results are provided for two real-world near-duplicate datasets - NEWS-COPY Deduplication dataset and CORE Near-Duplicates dataset. The proposed method gives significantly better results on NEWS-COPY dataset compared to all the baselines. On CORE Near-Duplicates dataset, the method fares marginally better than the hashing-based baselines but no comparison is made with neural-network-based methods.\n\nThe work discusses a practical application of the proposed method - spam email clustering. Results show significant improvement in clustering spam emails compared to three baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. A well-designed approach for near-duplicate detection that is light-weight, robust to typos and works on multilingual data.\n2. Encouraging improvements in near-duplicate detection over both hashing-based and neural-network-based methods though Multilingual E5-Base seems to be reasonably competitive.\n3. A new dataset is made available that can be used for near-duplicate detection studies."
            },
            "weaknesses": {
                "value": "1. RETSim is generally slower than MinHash and particularly slow on CPU (Table 6)."
            },
            "questions": {
                "value": "1. In the deduplication experiments on Wiki-40B (English), was RETSim trained on this portion of Wiki-40B? If so then could that also be one of the reasons why RETSim is able to detect substantially more duplicates than the baselines?\n\n2. Why isn't Multilingual E5-Base used as a baseline in spam email clustering experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4287/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699547174451,
        "cdate": 1699547174451,
        "tmdate": 1699636396355,
        "mdate": 1699636396355,
        "license": "CC BY 4.0",
        "version": 2
    }
]