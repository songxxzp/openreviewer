[
    {
        "id": "StrLqfWEbn",
        "forum": "dteBEZiCxB",
        "replyto": "dteBEZiCxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_kQfc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_kQfc"
        ],
        "content": {
            "summary": {
                "value": "The manuscript delineates the introduction of a Necessary and Sufficient (NS) Watermark, advancing the optimization constraints integral to the formerly introduced Hard watermark. Empirical scrutiny validates that the new watermark upholds the integrity of text quality, ensuring no discernible degradation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The comprehensiveness of the experiments underlines the efficacy of the NS algorithm proposed."
            },
            "weaknesses": {
                "value": "Major Concerns:\n\nPredominantly, there is apprehension regarding the foundational framework of the NS watermark, which is an extension of the Hard watermark as discussed in [1]. The predecessor watermark manifests suboptimal performance particularly in scenarios involving low entropy sentences, characterized by an initial sequence of tokens that heavily influences subsequent ones. This limitation is addressed superficially through the hard red list rule, simply by precluding the language model from generating such sequences. Should the NS watermark inherit this rule, it would ostensibly undermine the integrity of low entropy sentence generation, thereby standing in contradiction to the authors' assurance of preserved text quality.\n\nThe second concern pertains to the optimization schema posited in the study. The assertion that the Hard watermark adheres to optimization problem (3) is contested by the operational reality of Large Language Models (LLMs), which employ an autoregressive mechanism, incorporating stochastic elements such as temperature adjustments, beam searches, and top-k selections, rather than strictly maximizing token probability in alignment with the language models. This discrepancy casts doubt on the theoretical soundness of the NS watermark's conceptual underpinnings.\n\nMinor Concerns:\n\nThere is an implication that the robustness of the watermark is compromised to enhance text quality. Despite the provision to manipulate parameters $\\gamma$ and $\\beta$ within the optimization constraints to bolster robustness, the resilience of the proposed scheme does not rival that of [1].\n\nDiscrepancies are evident in Table 1, showcasing a discernible decline in the BLEU score when comparing NS watermark implementations with those devoid of watermarks. This observation counters the assertion of non-degraded text quality through NS watermarking. The absence of a theoretical exposition on the NS watermark\u2019s impact on text quality further weakens the claim, indicating that the empirical data presented may not suffice to substantiate the purported benefits of NS watermarking.\n\n[1] A watermark for large language models. Kirchenbauer et al."
            },
            "questions": {
                "value": "It is surprising that Soft-Watermark [1] has such a low BLEU score in the machine translation task. According the result reported in [1], the perplexity will not degrade too much when beam search is applied. This raises inquiries regarding whether the optimal experimental conditions were enlisted for the Soft watermark in the experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2258/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697979838571,
        "cdate": 1697979838571,
        "tmdate": 1699636159026,
        "mdate": 1699636159026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8N2nt1Hv0v",
        "forum": "dteBEZiCxB",
        "replyto": "dteBEZiCxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_yeCE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_yeCE"
        ],
        "content": {
            "summary": {
                "value": "Recent work [1] has successfully detected texts generated by LLM through the injection of watermarks, however, this method significantly degraded the text quality. In this paper, the authors propose a novel method for watermarking generated texts, termed the \u201cNecessary and Sufficient Watermark\u201d (NS-Watermark). From the observations that prior work [1] overly constrained the text generation, especially on long texts, this paper relaxes the constraints such that the text quality and detection accuracy are ensured. The authors formulate the NS-Watermark as a constrained optimization problem and introduce an efficient algorithm to solve it. The empirical results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work models the watermarking problem as a constrained optimization problem and then solve it by combining dynamic programming and beam search. The authors also proposed an approximation method to reduce the complexity.\n- The proposed method demonstrates superior performance compared to the baselines\n- The paper is effectively structured and exhibits clear and concise writing"
            },
            "weaknesses": {
                "value": "- Since NS-Watermark requires solving constrained optimization, although the authors did mention the running time in Appendix D.1, how expensive it is against baselines are missing.\n- The proposed algorithms depend on the beam search. However, in the experiment, the authors only use one beam size (k = 1).\n- It is not clear to me how NS-Watermark robust to attacks. For example, NS-Watermark might be removed by simply adding a list of red words?"
            },
            "questions": {
                "value": "1. Why the authors only select a set of small $\\gamma$? What happens with larger $\\gamma$ ( > 0.1) since in [1], they measured the z-score with values of $\\gamma$ varies from 0.1 to 0.9. Whether or not it is because lower $\\gamma$s enhance the model performance?. Based on equation 2, If $\\gamma$ is small, the z-score will significantly increase. Then, Soft-Watermark with high $\\delta$ can produce high z-score without sacrificing the text quality? Is it still true to keep the same Z threshold? Please correct me if I misunderstood.\n2. How does NS-Watermark compare to Soft-watermark in terms of inference time? In equation 7, the approximated NS-watermark need to generate the text without watermark first, and then solve the constrain optimization problem. Besides, the beam size $k$ and $\\alpha$ significantly affect the running time.\n3. In the case of robustness, making WS-Watermark more robust with post-editing decreases the performance of model substantially. For example, in figure 5a, when $\\beta$ \u2265 0.1 the PPL \u2265 3 which is higher than Adaptive Soft-watermark in Table 2. Similar behaviors with Translation task in figure 5b."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2258/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852239686,
        "cdate": 1698852239686,
        "tmdate": 1699636158961,
        "mdate": 1699636158961,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "demBvZJDB4",
        "forum": "dteBEZiCxB",
        "replyto": "dteBEZiCxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_V7Qr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_V7Qr"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel method to insert watermarks into generated text without compromising text quality. Specifically, the authors formulate the watermarking injection problem as an constrained optimization and provide a linear-time solution. The authors evaluate the effectiveness of the proposed method on machine translation and natural language generation tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.  The proposed method can preserve the quality of the generated text at a certain level. \n\n2. The authors provide an approximation solution with linear time complexity\n\n3. The authors conduct a series experiments to evaluate the effectiveness in terms of the text quality, the detection accuracy, and the sensitivity towards the hyper-parameters."
            },
            "weaknesses": {
                "value": "1. Regarding the methodology: \n\n1.1 There needs to be a more in-depth discussion about the comparison between the proposed naive method and the linear-time approximation method. Since the linear-time method is an approximation method, it would be valuable to understand what it sacrifices in order to obtain the linear time complexity.\n\n1.2 The figure 1's illustration is not very clear.  What's the difference between the two sub-figures in figure 1(a)?\n\n2. Regarding the experiment:\n\n2.1 Lack the comparison between the proposed naive method and the linear-time method.\n\n2.2 My major concern: the experiment about the robustness to post-editing is not convincing. The authors only show the relation between the text quality and the hyper-parameter controlling the robustness. I expect to see is the robustness under real attacking methods. The included attack methods could refer to the three types of attacks adopted in Section 7 of the paper [1]. Without seeing such experimental results under attacking, it's hard to be convinced about the robustness of the proposed method.\n\n[1]\"A Watermark for Large language Models\" by John Kirchenbauer*, Jonas Geiping*, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein"
            },
            "questions": {
                "value": "See the content in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2258/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909104073,
        "cdate": 1698909104073,
        "tmdate": 1699636158851,
        "mdate": 1699636158851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EaIomfOlp0",
        "forum": "dteBEZiCxB",
        "replyto": "dteBEZiCxB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_rYag"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2258/Reviewer_rYag"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method, the Necessary and Sufficient Watermark (NS-Watermark) for watermarking texts generated by large language models (LLMs). The authors argue that the use of LLMs can be misused for malicious purposes, hence the need for an efficient watermarking method to distinguish between human and LLM-generated texts. The NS-Watermark applies minimum constraints to generated texts, maintaining text quality while ensuring accurate detection. The authors propose this method in response to existing watermarking methods that degrade the quality of generated texts. The paper demonstrates that the NS-Watermark outperforms existing methods in distinguishing machine-generated text and maintaining text quality, especially in machine translation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper addresses a significant issue in the field of natural language processing, specifically in mitigating risks associated with the malicious use of large language models.\n(2) The authors propose a novel method, the Necessary and Sufficient Watermark (NS-Watermark), which is an innovative solution to the problem at hand.\n(3) The paper provides a comprehensive analysis of the proposed method, including a well-structured theoretical analysis and practical implementation details."
            },
            "weaknesses": {
                "value": "(1) It is overclaimed that the text quality is unaffected, compared to the no watermarked model. There is some quality drop compared to the unwatermarked model, but not too much. You can claim the text quality is better than Soft-Watermark. So using the phrase \"without degrading the text quality\" is not fully accurate.\n\n(2) The proposed model explores a less conservative region of z-scores compared to Soft-Watermark. Soft-Watermark's conservative approach provides robustness against attacks. And the NS-Watermark is much slower in terms of decoding speed.\n\n\n(3) Since watermarking is designed to distinguish watermarked text and unwatermarked text, it makes more sense to consider human text and unwatermarked AI-generated text together as negative samples."
            },
            "questions": {
                "value": "Since the watermarked tokens are dynamically added, if the adversary truncates the text and only provides part of the generated text, will this affect the detection performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2258/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994355790,
        "cdate": 1698994355790,
        "tmdate": 1699636158738,
        "mdate": 1699636158738,
        "license": "CC BY 4.0",
        "version": 2
    }
]