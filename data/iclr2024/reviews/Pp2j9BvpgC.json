[
    {
        "id": "zg2Re5X7g8",
        "forum": "Pp2j9BvpgC",
        "replyto": "Pp2j9BvpgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_pzC1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_pzC1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an image captioning way to address object attribute recognition and object-attribute compositional learning. Given the limitations of previous methods like the CLIP-based method, the authors proposed a generative prompting method to solve both the classification and compositional learning problems. On two datasets, the proposed method was compared with recent works and showed improvements."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Given the current development in LLM, using the captioning solution to address the visual understanding is non-trivial. This work used a general pipeline to address a classical visual relation and recognition problem.\n\n+ The discussion about the CLIP-based works makes sense."
            },
            "weaknesses": {
                "value": "- Though the solution is sound according to the new development tendency, I do not find too many new insights and \"surprising\" parts. There are many works using captioning via LLM to solve nearly all other directions in visual understanding, e.g., action/object recognition, visual relationship understanding, VQA, etc. Please give a discussion covering more domains to analyze the contribution.\n\n- Compositional learning and its zero-shot setting (CZSL) is challenging with previous paradigms given the fixed train and test sets. Now, we have huge datasets like LAION and many other datasets beyond images like text. More discussion or insights about the new weapons and the CZSL would be a better contribution. \n\n- Though previous non-LLM works are \"old\", they can still be used as the baselines.\n\n- The method part is kinda of too brief.\n\n- More experiments on more datasets would be more solid to support the claims.\n\n- typo: many wrong quotation marks \u201dxx\u201d.\n\n- some weird green boxes appeared."
            },
            "questions": {
                "value": "- Using huge LAION, then the fairness consideration in the experiments?\n\n- Detailed analysis of the long-tailed distribution and results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698163402747,
        "cdate": 1698163402747,
        "tmdate": 1699636233106,
        "mdate": 1699636233106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5W8zZ8mQ1c",
        "forum": "Pp2j9BvpgC",
        "replyto": "Pp2j9BvpgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_fA8W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_fA8W"
        ],
        "content": {
            "summary": {
                "value": "While zero-shot object recognition has largely been solved by large language-vision models such as CLIP, visual attribute recognition remains challenging because CLIP\u2019s contrastively learned representations do not effectively encode object-attribute dependencies. \nIn this paper, the authors revisit the problem of attribute classification and propose a solution using generative prompting, which revolves around a strategy for measuring the probability of generating prompts. \nUnlike contrastive prompting, generative prompting is order-sensitive, and its design reflects the downstream requirements of object-attribute decomposition. \nThe authors demonstrate through experiments that generative prompting outperforms contrastive prompting on two datasets that require visual reasoning, Visual Attribute in the Wild (VAW), and a modified formulation of Visual Genome (VGAR)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is easy to understand.\n- In terms of attribute recognition research. The proposed approach with generative prompting seems new."
            },
            "weaknesses": {
                "value": "- As a machine learning research, the technical significance and novelty seem weak. This work looks like a simple prompt engineering paper to me. To claim the technical significance, the authors should try with more various prompts and compare the results.\n\n- Also, as the main contribution of this paper is to replace contrastive prompting with generative prompting for attribute recognition, the authors should provide a theoretical explanation of why generative prompting is better than contrastive prompting. Otherwise, the technical contribution might be weak as a machine learning paper.\n\n- The experiment is also weak. In Table 3, the performance of the proposed method is not noticeably better than the baseline models (e.g., TAP).\n\n- Finally, there should be more comprehensive experimental results. For example, how about the result with other LLM models than CoCa? Also, in Tables 4 and 5, why didn\u2019t the authors compare the proposed model with the state-of-the-art methods? Finally, it would be better to evaluate the proposed method on other attribute recognition datasets, such as UTZappos or MIT datasets, or HOI detection datasets, such as HICO or V-COCO datasets."
            },
            "questions": {
                "value": "Please refer to the questions in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777463276,
        "cdate": 1698777463276,
        "tmdate": 1699636232994,
        "mdate": 1699636232994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wbo2qutfcP",
        "forum": "Pp2j9BvpgC",
        "replyto": "Pp2j9BvpgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_yHgE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_yHgE"
        ],
        "content": {
            "summary": {
                "value": "While large vision-language models like CLIP excel at zero-shot object recognition, they struggle with zero-shot visual attribute recognition due to an inability to effectively encode object-attribute relationships. This paper tackles this challenge by introducing generative prompting. This approach redefines attribute recognition by assessing the likelihood of generating prompts that express the attribute relation. Unlike its counterpart, contrastive prompting, generative prompting is order-sensitive and tailored for object-attribute decomposition. Experimental results reveal that generative prompting surpasses contrastive prompting in performance on two visual reasoning datasets: Visual Attribute in the Wild (VAW) and a newly introduced version of Visual Genome, termed Visual Genome Attribute Ranking (VGAR).\nThe paper also shows strong performance against SOTA, despite being trained without annotated data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper also shows strong performance against SOTA, despite being trained without annotated data.\n2. The paper has clear ablations to show the value of their proposed method\n3. In general, the key idea of using generative prompting/modeling to solve complex localized reasoning tasks is an interesting direction\n4. The VGAR benchmark which unifies object and attribute recognitions, is broadly useful for the community"
            },
            "weaknesses": {
                "value": "I don't see any major weaknesses in the paper"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2896/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2896/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2896/Reviewer_yHgE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819187121,
        "cdate": 1698819187121,
        "tmdate": 1699636232916,
        "mdate": 1699636232916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NdT1jEpZJB",
        "forum": "Pp2j9BvpgC",
        "replyto": "Pp2j9BvpgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_ZMSV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2896/Reviewer_ZMSV"
        ],
        "content": {
            "summary": {
                "value": "This work is about rcognizing image attributes through utilizing language models. Some experiments are shown for the results, and compared with other published works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The use of existing language models for recognizing image attributes is interesting. \n\nThe experimental comparisons are important."
            },
            "weaknesses": {
                "value": "In my understanding, the main contribution of the work is the prompt engineering, i.e., designing prompt for accessing large language models, which is interesting and useful, but it is not develping a new mothod to advance the state-of-the-art, from the algorithm or theory viewpoint. Thus if my understanding is correct, the contribution of the paper is not at the level of ICLR.\n\nTo my knowledge, there are already some existing companies that work on prompt engineering for a better use of the ChatGPT or other large language models. Thus the prompt engineering is not that new, even from the application point of view. \n\nIt could be better if the prompting engineering shown in the work can be combined with some novel algorithm development, the paper might be a better contribution to the ICLR conference."
            },
            "questions": {
                "value": "As my concerns shown in the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699276908439,
        "cdate": 1699276908439,
        "tmdate": 1699636232836,
        "mdate": 1699636232836,
        "license": "CC BY 4.0",
        "version": 2
    }
]