[
    {
        "id": "p9vkLXN6LB",
        "forum": "7F4ioiKQFT",
        "replyto": "7F4ioiKQFT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_hnuQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_hnuQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fine-grained image retrieval system that leverages and enhances the pre-trained embedding. The authors propose to fine-tune CLIP on the Visual Genome Dataset and incorporate the MaxSim operator for image-text interaction. The approach proposed by the authors outperforms the CLIP model on the Visual Genome dataset and the MSCOCO dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe proposed method surpasses the CLIP model on both the Visual Genome and MSCOCO datasets.\n2.\tThis paper handles a potentially valuable practical scenario."
            },
            "weaknesses": {
                "value": "1.\tThis paper does not clearly define the research problem, its challenges, and how it differs from existing research directions. If it is focused on fine-grained retrieval in small areas of images, the authors should analyze why existing methods, even after fine-tuning, cannot achieve fine-grained retrieval. \n2.\tThe novelty of this paper is limited. There have been various related studies on improvements to CLIP [1][2]. The authors should conduct a comprehensive comparative analysis of their proposed method with these approaches to emphasize the contribution of this paper. \n3.\tThis paper lacks sufficient explanations for its design choices. The authors should explain why they chose to use the MaxSim operator instead of other designs [3][4]. Additionally, experimental comparisons should be conducted with the methods from [1][2].\n\n[1] Li J, He X, Wei L, et al. Fine-grained semantically aligned vision-language pre-training[J]. Advances in neural information processing systems, 2022, 35: 7290-7303.\n\n[2] Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning\n\n[3] Lee K H, Chen X, Hua G, et al. Stacked cross attention for image-text matching[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 201-216.\n\n[4] Zou X, Wu C, Cheng L, et al. TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval[J]. arXiv preprint arXiv:2209.13822, 2022."
            },
            "questions": {
                "value": "Please refer to the \"Weaknesses\" section. Furthermore, it seems that the application scenario proposed by the authors could be valuable in the e-commerce context. Can the method proposed by the authors be adapted to datasets specific to this scenario?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698117576593,
        "cdate": 1698117576593,
        "tmdate": 1699636092294,
        "mdate": 1699636092294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZPfqdkRf94",
        "forum": "7F4ioiKQFT",
        "replyto": "7F4ioiKQFT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_coku"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_coku"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ColCLIP, a fine-grained image retrieval model based on the CLIP model which can enhance image and text embeddings for visual retrieval tasks. The authors integrated the CLIP model backbone and MaxSim operator during the model training for the downstream tasks. In the experiments, the author shows that ColCLIP outperforms better than standard CLIP in handling fine-grained retrieval tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well-written and organized. \nThe training method and details are described clearly.\nThe experiments are conduct well"
            },
            "weaknesses": {
                "value": "The key problem of this paper is: the fine-tuning of the CLIP model should be a popular topic after the release of the CLIP model and even before the CLIP model. The paper lacks background research and similar work comparisons, which makes it hard to tell how important this work is in the research community."
            },
            "questions": {
                "value": "what is the main innovation of the paper except the CLIP and MaxSum which is from other's work"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389289104,
        "cdate": 1698389289104,
        "tmdate": 1699636092220,
        "mdate": 1699636092220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "17YmJeZIkw",
        "forum": "7F4ioiKQFT",
        "replyto": "7F4ioiKQFT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_yk6P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_yk6P"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on fine-grained image retrieval, which leverage pre-trained embeddings and enhance them specifically for fine-grained retrieval tasks. To address this issue, this paper employs a MaxSim operator to compute similarity for the interaction between image and text embeddings, which not only facilitates a more comprehensive level of interaction between images and queries\nbut also achieves precise alignment with the expected region. Extensive experiments show that the developed model could significantly improve the retrieval efficiency and accuracy. The subject matter addressed in this paper holds significant practical relevance, and the motivation behind it is evident. Regrettably, there are still certain issues pertaining to the writing style and experimental analysis that need to be addressed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "a.\tThis paper adapts MaxSim operator to the multimodal domain, which aims to utilizes the capabilities of a pre-trained multimodal embedding model with minimal modifications. Particularly, the method of this work could make original model acquire the capability to attend to diverse fine-grained details, irrespective of their prominence within the image.\nb.\tThis paper utilizes saliency maps and integrated gradients to provide visual interpretations of the model's attention to different regions within the image and different segments of the text, which assists in obtaining a deeper understanding of the model's behavior.\nc.\tExtensive experiments are conducted to demonstrate the effectiveness and robustness of the proposed method."
            },
            "weaknesses": {
                "value": "a.\tThis paper should have a clear visual representation of the model's architecture. Without a model architecture diagram, it becomes challenging to assess the design choices of the proposed approach. The authors should include an illustrative model diagram in order to strengthen the paper's clarity and comprehensibility.\nb.\tThis paper lacks sufficient explanation when describing the differences between the proposed method and MiniGPT-4. It should provide more detailed reasoning as to why MiniGPT-4 \u201cis hindered by the costly imagetext interaction and the unpredictable nature of text output\u201d.\nc.\tIn Experiment part, this paper evaluates performance with few baselines. The proposed method whether helps the original different versions of CLIP to have a better performance is necessary to prove.\nd.\tThis paper lacks a detailed illustration about the proposed method, for example, the image encoder in ColCLIP adopts the Vision Transformer from Krishna et al., but the relationship between original CLIP image encoder and the utilized image encoder should be explained. \ne.\tThis paper needs to illustrate the related work more, it is advisable to illustrate that whether there exists some models which also focus fine-grained image retrieval by using CLIP.\n\nTypos and minors\nThe whole manuscript should be carefully checked for typos, grammar and syntax, as there are many of them. The following are some example:\na.\tThe best value in Table 4, Table 5 and Table 6 should be highlighted.\nb.\tThere is something different about the name of proposed model in Section 4.1 and Section 4.5.\nc.\tThe formatting of the table could be enhanced to improve its aesthetic appeal."
            },
            "questions": {
                "value": "please see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Reviewer_yk6P"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675432603,
        "cdate": 1698675432603,
        "tmdate": 1699636092135,
        "mdate": 1699636092135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QAxVKNGoIa",
        "forum": "7F4ioiKQFT",
        "replyto": "7F4ioiKQFT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_Qpf8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1637/Reviewer_Qpf8"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces MaxSim layer into CLIP funetuning. MaxSim is an operation layer which constructs embeddings on top of all hidden states rather than a single token based on the original CLIP method. Combined with the targeting Visual Genome dataset, the authors hope their models can align between object specific queries to corresponding image regions. Leveraging the pretrained CLIP model weights, the paper showed improved text2image retrieval matching performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The framework fully leverages the benefits of pretrained CLIP models and shows improved matching performance for fine-grained text2image tests, to certain extent. The paper shows quantitative results for better active object regions on feature maps for their finetuned models. The approach also has reasonable reproducibility."
            },
            "weaknesses": {
                "value": "1) This work is mainly bringing in the main innovative layer of MaxSim from ColBERT to the proposed ColCLIP. It shows improvements on some setups but not all. For example, the ColCLIP base didn\u2019t perform better than CLIP-based models, which left some concerns whether the gains come from only large model architectures or tied with specific datasets. 2) The paper discusses FILIP models and tries to differentiate from the proposed work. I am not convinced that FILIP was focusing on training from scratch and it is not a comparable approach. It indeed shares much with ColCLIP in particular the late interaction between text and image embeddings. It should be a strong baseline to this work. 3) At least one ablation study is missing in terms of the decision to remove symmetric contrastive loss. 4) The authors discussed retrieval quality for their Lite models but didn\u2019t mention any latency or training time gains. It lost the opportunities to provide a comprehensive view of the motivation of dimensionality reduction. \n\nMinor issues:\n1. Proofread needed for spelling check, e.g. there are multiple places authors used \"COLCLIP\" \n2. Image captions could be with bigger fonts for better visibility."
            },
            "questions": {
                "value": "It would be more sound if the authors can show results on other datasets, e.g. Flickr30K, or FILIP300M?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1637/Reviewer_Qpf8"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823654544,
        "cdate": 1698823654544,
        "tmdate": 1699636092060,
        "mdate": 1699636092060,
        "license": "CC BY 4.0",
        "version": 2
    }
]