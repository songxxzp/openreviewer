[
    {
        "id": "0vzyQ1peFX",
        "forum": "p09XyFxZkc",
        "replyto": "p09XyFxZkc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_9GLX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_9GLX"
        ],
        "content": {
            "summary": {
                "value": "This work introduces LaVie, a text-to-video generative model that builds upon a pre-trained text-to-image model.  LaVie consists of cascaded video latent diffusion models, including a base T2V model, a temporal interpolation model, and a video super-resolution model. The key insights include the use of temporal self-attentions and rotary positional encoding to capture temporal correlations in video data. Joint image-video fine-tuning is crucial for high-quality results. The authors also contribute a large and diverse video dataset called Vimeo25M. Experimental results demonstrate that LaVie achieves state-of-the-art performance in both quantitative and qualitative measuress."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The article provides some insights: \n1. The use of simple temporal self-attention mechanisms coupled with rotary positional encoding to capture temporal correlations in video data \n2. Joint image-video fine-tuning in producing high-quality and creative outcomes.\n3. The authors contribute the Vimeo25M dataset, which comprises 25 million text-video pairs, serving as a valuable resource for research and development in the field."
            },
            "weaknesses": {
                "value": "1. I think the article lacks technical innovation. Its core contribution involves extending the pre-trained LDM to a video generation model, introducing temporal attention, and emphasizing joint image-video training. However, similar techniques have been mentioned in previous works, such as VDM and Align your latent. The authors should clarify the distinguishing aspects of these innovation. \n\n2. The article presents several assertions without strong empirical support or ablation study. For instance, the introduction of RoPE as a positional encoding lacks corresponding experimental evidence to demonstrate its effectiveness and advantages over other encoding methods. The statement, \"we validate that the process of joint image-video fine-tuning plays a pivotal role,\" lacks experimental substantiation in the article.\n\n3. The writing should be improved, and there exist some unclear explanations. For instance, the modules in Figure 2 are not adequately introduced in the text, causing confusion. For example,  I want to know whether 'E' denotes the encoder or denoiser? Additionally, the article mentions \"By applying rigorous filtering criteria\" to construct Vimeo25M, but the specific criteria are not outlined. Will this dataset be made publicly available?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2758/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2758/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2758/Reviewer_9GLX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570451686,
        "cdate": 1698570451686,
        "tmdate": 1699636218485,
        "mdate": 1699636218485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d2jBwSaWTJ",
        "forum": "p09XyFxZkc",
        "replyto": "p09XyFxZkc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_aq4u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_aq4u"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a text-to-video method by using a pre-trained text-to-image model as the initialization. The authors present two key findings: 1) temporal self-attention with relative position encoding could maintain temporal consistency well. 2) joint image-video fine-tuning is crucial for good performance. This paper also contributes a video dataset named Vimeo25M, with 25 million text-video pairs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow, and the visualization results seem appealing. \n2. The paper proposes a new video-text datasets named Vimeo25M."
            },
            "weaknesses": {
                "value": "1. The technical novelty is limited. The authors present two findings which have both been proposed in previous works. 1) temporal attention over a pretrained text-to-image model could address the temporal consistency model well. This idea is first stated in \u201cAnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\u201d and then pointed out in \"MagicEdit: High-Fidelity and Temporally Coherent Video Editing\". 2) Joint image-video training is a well-known technique to improve the effectiveness of training text-to-video models. It was first proposed by Jonathan Ho et al. in \"Video Diffusion Models\". \n2.  The idea of the cascaded diffusion model, which first trains a base model then temporally interpolates it, and finally performs super-resolution spatially, is a standard procedure to learn a text-to-video model. Similar ideas are also proposed in \"Make-A-Video: Text-to-video Generation Without Text-Video Data\", \"Imagen Video: High Definition Video Generation with Diffusion Models\" and \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\"."
            },
            "questions": {
                "value": "Will the authors release the newly proposed video dataset Vimeo25M? Since it seems that there is no promise from the authors that they will release the dataset. I would say that the major contribution of the paper is the dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724844246,
        "cdate": 1698724844246,
        "tmdate": 1699636218413,
        "mdate": 1699636218413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q8RdQcsIj6",
        "forum": "p09XyFxZkc",
        "replyto": "p09XyFxZkc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_b3WW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_b3WW"
        ],
        "content": {
            "summary": {
                "value": "The paper presents LaVie, a novel integrated video generation framework, which is fundamentally constructed on cascaded video latent diffusion models. The authors strategically integrate three principal components into the LaVie framework: a base Text-to-Video (T2V) model, a temporal interpolation model, and a video super-resolution model. Their approach is underscored by two primary insights: firstly, the integration of simplistic temporal self-attentions, when paired with rotary positional encoding, proves essential in capturing the temporal correlations intrinsic to video data efficiently. Secondly, the process of concurrently fine-tuning both image and video, is validated as being crucial in generating outcomes marked by high-quality and creativity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Innovative Integrated Framework:\nThe paper presents LaVie, a cutting-edge integrated video generation framework with an enormous capacity of 3 billion parameters. LaVie, primarily a text-to-video foundation model, is meticulously constructed to synthesize visually appealing and temporally coherent videos. The model maintains the vigorous creative generation characteristics of a pre-trained Text-to-Image (T2I) model, ensuring synthesized videos are not only realistic but also infused with a strong creative essence.\n\n2. Strategic Incorporation of Key Insights:\nThe authors introduce two pivotal insights that form the backbone of LaVie\u2019s design. Firstly, a combination of simple temporal self-attention with Rotary Positional Encoding (RoPE) is utilized, proving sufficient in capturing the intrinsic temporal correlations present in video data. Secondly, the paper emphasizes the indispensable role of joint image-video fine-tuning in crafting high-quality and imaginative outcomes, ensuring the model retains and effectively utilizes learned prior knowledge without succumbing to catastrophic forgetting.\n\n3. Introduction of a Comprehensive Dataset:\nRecognizing the limitations of existing datasets, the authors contribute a novel text-video dataset named Vimeo25M. This dataset is a treasure trove of 25 million high-resolution videos, each accompanied by text descriptions, curated to overcome the prevalent issues of low-resolution and watermarked content found in previous datasets. The utilization of Vimeo25M significantly amplifies LaVie\u2019s performance, empowering it to churn out results that excel in quality, diversity, and aesthetic allure, thus substantially advancing the Text-to-Video (T2V) synthesis task."
            },
            "weaknesses": {
                "value": "1. Some details are not clear. See **Questions** below.\n\n2. Availability of Contributed Dataset:\nA significant contribution highlighted in this paper is the introduction of the Vimeo25M dataset. However, it's crucial to clarify that, as of now, this dataset has not been made publicly available for utilization and further exploration.\n\n3. The claim:\n\n>  simple temporal self-attention coupled with RoPE (Su et al., 2021) adequately captures temporal\ncorrelations inherent in video data. More complex architectural design only results in marginal visual improvements to the generated outcomes\n\nisn't supported by experimental results."
            },
            "questions": {
                "value": "1. Positional Embedding:\nIs the utilization of Rotary Positional Encoding (RoPE) distinctly advantageous compared to other available options, such as learnable embeddings? The clarity of the benefits associated with using RoPE, as opposed to its counterparts, would be appreciated.\n\n2. Clarification on Image Concatenation:\nThe process involving the concatenation of M images along the temporal axis to formulate a T-frame video raises a query. Specifically, is there an equivalence between the variables `M` and  `T` in this context?\n\n3. Query on SA-T Removal in Image Training:\nFigure 3 (c) seems to imply that the SA-T is omitted during the image training phase. Could you provide a more comprehensive explanation or clarification regarding this aspect?\n\n4. Interaction of Text Conditioning with Spatial Attention:\nThe paper suggests that text conditioning primarily interacts with spatial attention. Given this, how does text effectively influence or control motion within the model? More details regarding the interplay between text and motion would enhance understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744352494,
        "cdate": 1698744352494,
        "tmdate": 1699636218353,
        "mdate": 1699636218353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DsLCp9L75h",
        "forum": "p09XyFxZkc",
        "replyto": "p09XyFxZkc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_m4rL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2758/Reviewer_m4rL"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a text-to-video model consisting of multiple components including text-2-video, temporal interpolation and super-resolution components. The text-2-video model is built on the pre-trained text-2-image model. Additionally, they introduce Vimeo25M dataset to enhance the quality of text-to-video generation. The method is straightforward and the paper showcases the versatility of the model in long video generation and personalized video generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow\n\n- The resulting model is capable of handling both T2I and T2V tasks.\n\n- The paper provides a human evaluation of video generation quality. \n\n- The paper introduces Vimeo25M dataset which is a collection of 25 million text-video pairs. The dataset aids in boosting the performance of model in terms of quality and diversity.\n\n- Joint image-video training of model is interesting and seems a reasonable approach for training video generation models."
            },
            "weaknesses": {
                "value": "1- The technical novelty of the proposed method is limited as it is a combination of existing techniques, including text-to-image generation, frame interpolation, and super-resolution. \n\n2- Dandi et al.'s work on \u201cJointly Trained Image and Video Generation using Residual Vectors\" is relevant to this paper's exploration of joint image-video training. However, this paper is neither cited nor discussed.\n\n3- There is no analysis of the contribution of individual components or techniques on the video generation performance. The paper mentions the temporal module, joint image-video training, and usage of Vimeo25M dataset for training. However, we don\u2019t see a comprehensive analysis on the impact of each of them on the performance. It\u2019s hard to understand which component has a higher impact on the final model. The only analysis we see is in Fig 10 which is very limited by showing only three images.\n\n4- The role of the temporal self-attention module (SA-T) during image-only training phases is ambiguous. It's unclear from Figure 3 whether SA-T is frozen or entirely excluded from the process.\n\n5- It\u2019s not clear what is the benefit of the technique explained on page 4: \"our approach differs from conventional video frame interpolation methods, as each frame generated through interpolation replaces the corresponding input frame. In other words, every frame in the output is newly synthesized\"\n\n6- How did authors make sure there is enough correlation between text and video segment? Details of text-video pair selection are missing.\n\n7- In Fig. 9 (b) statistics on resolution are not clear. what \u201c99.9%\u201d means?\n\n8- There is no comprehensive evaluation of diversity (since the paper claimed to improve it) besides Fig. 4. Did the authors consider evaluating diversity with human evaluation?"
            },
            "questions": {
                "value": "see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2758/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699030736492,
        "cdate": 1699030736492,
        "tmdate": 1699636218269,
        "mdate": 1699636218269,
        "license": "CC BY 4.0",
        "version": 2
    }
]