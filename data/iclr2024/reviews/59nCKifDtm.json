[
    {
        "id": "9voMTIUQVS",
        "forum": "59nCKifDtm",
        "replyto": "59nCKifDtm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_zRzD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_zRzD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an autoregressive way to manipulate the noise within diffusion models.  It uses predefined hyperparameters to control the Gaussian covariance matrix when sampling noise. Experiments on audio and motion datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. By incorporating the temporal correlation during sampling noise, the experiments shows improved performance compared with baseline\n2. The proposed techniques do not introduce too much extra computation for both training and inference time."
            },
            "weaknesses": {
                "value": "1. The authors conduct experiments on two baseline methods: DiffWave and DiffWave-sashim and evaluate the proposed method on two tasks: neural vocoding and unconditional generation. The proposed method improves both baselines on neural vocoding tasks in Table 1 a),  but obtains inferior performance on unconditional generation tasks under some metrics (like FID, IS). It is preferred that the authors clearly explain the possible reason for this phenomenon.\n2. In Table 2, the performance improvement in human motion generation is not substantial compared to audio generation. In many cases, the performance even deteriorated. Does that mean the proposed method may not be general enough for different tasks? Or the hyperparameters to produce a covariance matrix of Gaussian is not tuned well?\n3. My major concern is that only changing the noise distribution in diffusion models may not be enough to address the temporal consistency problem encountered in various tasks. The authors may need to conduct experiments on more tasks like video generation."
            },
            "questions": {
                "value": "1. For video generation, the data has a very high dimension, what about the complexity analysis when producing a covariance matrix? Is it still affordable for a diffusion process?\n2. Is there any visualization of the generated contents to demonstrate the improvement of temporal consistency after using the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Reviewer_zRzD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657747625,
        "cdate": 1698657747625,
        "tmdate": 1700725664377,
        "mdate": 1700725664377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bXEd1kSMV3",
        "forum": "59nCKifDtm",
        "replyto": "59nCKifDtm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_jkr1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_jkr1"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the AutoRegressive Temporal diffusion (ARTDiff) method to tackle this consistency challenge. ARTDiff efficiently introduces a Gaussian noise distribution, accounting for time-based correlations, which strengthens the temporal connections and boosts consistency in the sequences generated. When tested on audio and motion tasks, ARTDiff outperformed standard diffusion models in sample fidelity and realism, making it a practical solution for diffusion-based generation models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written. The motivation is well presented and is also reasonable: setting up correlations between frames is indeed a meaningful method for improving temporal consistency.\n\n2. The paper is verified on a vast number of tasks, making it solid."
            },
            "weaknesses": {
                "value": "1. Currently, it seems a general correlation for all tasks. Will the performance be better if taking some dataset/task-specific statistics into consideration?\n\n2. Some visualizations and videos will make the proposal more strong.\n\n3. Will the method work on fine-tuning a large generative model?"
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3405/Reviewer_jkr1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734557926,
        "cdate": 1698734557926,
        "tmdate": 1699636291593,
        "mdate": 1699636291593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lXuWKNIFJl",
        "forum": "59nCKifDtm",
        "replyto": "59nCKifDtm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_uW7w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_uW7w"
        ],
        "content": {
            "summary": {
                "value": "The ARTDiff method introduced capitalizes on the autoregressive dependence structure inherent in temporal data, employing a Gaussian noise distribution with correlations between time frames defined by a function of their temporal difference. This approach intentionally captures temporal dependencies, bolstering the consistency across generated sequences. When applied to tasks involving audio and motion generation, ARTDiff showcases its effectiveness, producing samples with markedly enhanced fidelity and realism compared to those generated by baseline diffusion models. The method's straightforwardness and efficiency highlight its practicality, positioning ARTDiff as a viable option for instilling temporal consistency in diffusion-based generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The propose method is well-motivated.\n\nThe paper is well-structured, capable of clearly elucidating its core ideas.\n\nThe conducted experiments adequately showcase the efficacy of the method being proposed."
            },
            "weaknesses": {
                "value": "The noise correlation in this method bears a resemblance to that in [1]. Could you elucidate the principal distinctions?\n\n[1] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models\n\nWhat strategies are employed to maintain temporal consistency across various clips?\n\nIn scenarios of extended long-term generation involving F frames and a given window size w, what is the length of the input sequence during the training phase?\n\nFor the complex data like video, will the proposed correlation work\uff1f It seems to be a great way to generate long videos."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769752209,
        "cdate": 1698769752209,
        "tmdate": 1699636291521,
        "mdate": 1699636291521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UqMQbCixXO",
        "forum": "59nCKifDtm",
        "replyto": "59nCKifDtm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_kucM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3405/Reviewer_kucM"
        ],
        "content": {
            "summary": {
                "value": "Diffusion models have been applied to sequential data, including audio, video, and motion. Temporal consistency is crucial in sequential data. This paper introduces the AutoRegressive Temporal Diffusion (ARTDiff) method to address temporal consistency in diffusion models. By incorporating a Gaussian distribution with temporal correlations based on time differences, ARTDiff captures temporal dependencies effectively. The effectiveness of ARTDiff is evaluated in audio and motion generation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clear and easy-to-follow writing.\n- The paper addresses an important topic as temporal consistency is crucial in the rapidly evolving field of diffusion models applied to audio and video data.\n- The method's effectiveness is demonstrated in audio and motion generation tasks."
            },
            "weaknesses": {
                "value": "- The proposed method is relatively simple and lacks novelty.\n- The experimental section is limited, comparing the method only in some uncommon tasks. Tasks like Text2Video might be more practically relevant."
            },
            "questions": {
                "value": "As indicated above, despite some experimental and innovative limitations, the paper has a good foundation and satisfactory completion. While I'm not deeply familiar with audio tasks, I currently lean toward a weak acceptance considering the paper's decent starting point and overall quality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698916428061,
        "cdate": 1698916428061,
        "tmdate": 1699636291454,
        "mdate": 1699636291454,
        "license": "CC BY 4.0",
        "version": 2
    }
]