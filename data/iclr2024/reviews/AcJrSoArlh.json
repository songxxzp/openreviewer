[
    {
        "id": "AbpVUQZG9u",
        "forum": "AcJrSoArlh",
        "replyto": "AcJrSoArlh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors rethink model ensemble in black-box adversarial attacks: exploring the relationship among the transferability of adversarial samples, the Hessian matrix\u2019s F-norm, and the distance between the local optimum of each model and the convergence\npoint. Based on the above theoretical analysis, the authors define common weaknesses and propose effective algorithms\nto find common weaknesses of the model ensemble."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experiments are solid and compresive: the author consider image classification, object detection and large vision-language models, and the results demonstrate the effectivenss of the proposed method.\n- The authors introduce extensive mathmatical proofs, which makes the proposed method more convincing."
            },
            "weaknesses": {
                "value": "- The description of the experimental part could be improved. Take \"Results on normal models\" as an example. In the experiment setup, the authors choose MI, VMI and SSA to validate the effectiveness of proposed CWA. However, in \"Results on normal models\", the authors only discuss MI, and ignore the VMI and SSA. I think the authors should pay more attention on the recent SOTA (e.g., SSA), since MI was proposed in 2018 (5 years ago)."
            },
            "questions": {
                "value": "- How to explain the significant performance degradation of VMI-CWA compared to VMI in Tab 1when the target models are Swin-S and Max ViT-T?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission30/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission30/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission30/Reviewer_GTKq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697867552117,
        "cdate": 1697867552117,
        "tmdate": 1699635926434,
        "mdate": 1699635926434,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ytRIyfQmu3",
        "forum": "AcJrSoArlh",
        "replyto": "AcJrSoArlh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission30/Reviewer_9Y8Q"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the transfer-based attack towards the ensemble of models. The authors suggest that the ensemble model would have \"common weaknesses\" that are strongly correlated with adversarial examples' transferability and propose the common weaknesses attack (CWA). Theoretically, this paper provides clear intuition on why the models have common weaknesses. This paper also conducts extensive experiments to validate their theoretical findings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper conducts extensive experiments on several datasets, demonstrating that CWA receives superior results than previous methods.\n2. The intuition of common weakness is strong and clear. This paper converts the task \"crafting adversarial examples on ensemble models\" to \"optimizing the second term in Equation (2)\". By Theorem 3.1, this term is further decomposed into a \"flatness term\" and a \"closeness term\". These two terms can be efficiently optimized by SAM and CSE."
            },
            "weaknesses": {
                "value": "1. The intuition behind SAM and CSE is not clear enough. Equation (4) and (5) is confusing for those readers not familiar with this area.\n2. In Section 3.1, the authors mentioned that \"the goal of transfer-based attacks is to craft an adversarial example $x$ that is misclassified by all models in $\\mathcal{F}$\". However, fooling all target models seems to be an impossible job in practice. Besides, the experiments in this paper cannot support this claim.\n3. The perturbation in Figure 3 is not imperceptible to humans."
            },
            "questions": {
                "value": "See the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethical concerns."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744361782,
        "cdate": 1698744361782,
        "tmdate": 1699635926225,
        "mdate": 1699635926225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GaV21kBKdY",
        "forum": "AcJrSoArlh",
        "replyto": "AcJrSoArlh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission30/Reviewer_JLTP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission30/Reviewer_JLTP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method for constructing adversarial examples for black-box models, using an ensemble of surrogate models. Using a second order approximation of the adversarial loss function, the authors construct an upper bound using the Hessian on the adversarial loss, and establish approximate methods for minimizing the upper bound, yielding their proposed attack: Common Weakness Attack (CWA). The CWA algorithm consists of two steps: first a gradient ascent step on the averaged logits, inspired by sharpness-aware minimization, and a second gradient descent step using their proposed cosine similarity encourager (CSE). The authors evaluate their attack method on a variety of model architectures using the NIPS2017 dataset, and compare with a variety of different attack algorithms from the literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important subject, understanding how vulnerable can ML models be is crucial for safe-guarding them against possible adversaries\n\n- The proposed method, as far as I can tell, seems novel to me.\n\n- The strengths of this paper are primarily in the effectiveness of their proposed method, as it seems to combine very well with prior attacks (e.g, MI-CWA, SSA-CWA) and achieves superior results. The transfer-based black-box attacks against Bard is interesting.\n\n- The proposed algorithm seems reasonable to implement"
            },
            "weaknesses": {
                "value": "- I felt that the paper was bit hard to follow. Section 3 mixes a lot of prior results with proposed ones. Thus it makes it a bit hard to distinguish original contributions vs reusing prior results. For instance, it would greatly improve the readability if SAM was properly explained prior to this section (or as a subsection). Figures 1 and 2 are not very informative (see Questions), they lack axis and/or labels. It would also improve the quality of the paper if the authors include a mathematical description of some of the prior ensemble attack methods (e.g. is it simply PGD on the averaged logits?), to better understand the contrast between them and the proposed method.\n\n- Despite the compactness of Algorithm 1, there is a lack of analysis on its complexity. How does it compare (in terms of wall-clock time) to prior attacks or vanilla PGD-style attacks?"
            },
            "questions": {
                "value": "- In Fig 1, it is not clear what is being plotted, and the different colors are not well contrasted. \n- In Sec 3.1, below eq (2), the authors write \" ... we can see that a smaller value of ... means a smaller test error ...\". What is the test error in this case? is it the error on the black-box model? the term test error is rather confusing.\n- In Theorem 3.1, there is a typo, should be $||H_i||_F$? \n- In Sec 3.2: \" There are some researches ...\" typo\n- In Fig 2, what are $x_t$, $x_t^f$, .... The symbols are not defined. \n- In Alg1, does the order of the classifiers in the for loop matter? if so how is it currently chosen?\n- In Table 1, some of the highlighted numbers are not the best performing numbers, for instance the FastAT RN50 row. \n- Can the proposed method be also applied in the white-box setting? i.e. assume $\\mathcal{F} = [f]$ and simply attack the classifier.  Have the authors experimented with something like this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814225713,
        "cdate": 1698814225713,
        "tmdate": 1699635926146,
        "mdate": 1699635926146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rj1s0Mf5Zr",
        "forum": "AcJrSoArlh",
        "replyto": "AcJrSoArlh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission30/Reviewer_s2Ct"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel adversarial attacks (Common Weakness Attack) against ensembles of model. It composes of two components, sharpness aware minimization (SAM) and cosine similarity encourager (CSE) that regularize the flatness of the loss landscape and the closeness to the optimum respectively. The author performs extensive experiments over extensive datasets and models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written in general and has very clear motivation and mathematical formulations. The derivation of attack based on the second order appropriation is intuitive. \n2. The author conducts very extensive empirical studies with many choices of architectures and datasets."
            },
            "weaknesses": {
                "value": "1. In table 1, the author only provides the results of CWA in combintation with other methods. Is there results for CWA only, and how well it performs. \n2. As an ablation study, the author might want to try different norm decomposition, e.g. the operator norm of H.\n3. The author should clarify the novelty compared to the previous methods. Specially, MI, VMI, SSA are all existing proposed attacks. The sharpness aware minimization techniques have been previously used."
            },
            "questions": {
                "value": "1. Can the author explain why the model is able to achieve significantly more effective attacks compared the existing methods for the adversarial trained model? I am mainly concerned about the fairness of the evaluation. Also, can the author explain how the AT model trained for ensembles in their experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission30/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819801434,
        "cdate": 1698819801434,
        "tmdate": 1699635926044,
        "mdate": 1699635926044,
        "license": "CC BY 4.0",
        "version": 2
    }
]