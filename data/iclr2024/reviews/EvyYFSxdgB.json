[
    {
        "id": "tHZ91ps1UU",
        "forum": "EvyYFSxdgB",
        "replyto": "EvyYFSxdgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_LK9J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_LK9J"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a sampling strategy for meta-learning in physics-informed neural networks (PINNs). The key idea is to conduct sampling based on the difficulty. The authors provide an analytical solution to optimize the sampling probability, with a regularization term. Experiments show improved performance over uniform sampling. An ablation study has been presented to help understand the method. The method also shows better performance under the same budget."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Meta-learning for PINN is a promising solution to generalize PINNs so that we do not have to train from scratch for any new PDE.\n2. The proposed sampling strategy is general and can be combined with the existing meta-learning strategy.\n3. Experiments show the proposed method outperforms the uniform sampling."
            },
            "weaknesses": {
                "value": "1. It is necessary to report training costs in terms of running time. Although the method can improve the sampling efficiency, the sampling strategy itself could be more time-consuming than uniform sampling. It is unclear whether the method can bring actual benefits in training time compared with the baseline.\n2. The method is only tested on three benchmarks. Results on more benchmarks are encouraged to test the generalizability of the proposed method, such as Heat, Wave, and Advection."
            },
            "questions": {
                "value": "What is the actual training time improvement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557972877,
        "cdate": 1698557972877,
        "tmdate": 1699636389136,
        "mdate": 1699636389136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QWjTDSSJYH",
        "forum": "EvyYFSxdgB",
        "replyto": "EvyYFSxdgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_USEY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_USEY"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to Physics-Informed Neural Networks (PINNs) with a focus on unsupervised learning for parameterized Partial Differential Equations (PDEs). The authors propose optimizing the probability distribution of task samples to minimize error during meta-validation. They transform the problem theoretically into a discretized form suitable for optimization and introduce two optimization strategies: optimizing the residual points sampled and the loss weight across different tasks. The paper presents experiments on several equations, showcasing improvements over baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clarity and Presentation: The paper is well-written, making the novel method and its distinctions from baselines clear. The method is explained in a step-by-step manner, which is easy to follow.\n2. Innovative Approach: The meta-learning method for PINNs is a fresh take on solving parameterized PDEs, and it is well grounded in theory.\n3. Comprehensive Experiments: The authors conduct experiments on various equations, providing a thorough evaluation of their method.\n4. Effective Visualization: The results are presented in a clear manner, with visualizations that aid in understanding the improvements made."
            },
            "weaknesses": {
                "value": "1. Mischaracterization of Data Loss: In Section 3, the paper inaccurately defines data loss as the loss of boundary conditions. This is a mischaracterization, especially for Neumann or Robin boundary conditions, which only penalize normal derivatives rather than resulting in data loss.\n\n2. Formatting and Clarity of Figures: Some figures in the paper could be improved for better clarity and understanding. For instance:\n(1) Figures 3 and 4 would benefit from added grids and key values annotated directly on the figures.\n(2) The scales in Figure 5 are too small to read, making it difficult to interpret the results.\nThe authors should review and adjust these figures to enhance clarity.\n\n3. Lack of Comparison with State-of-the-Art: The paper could be strengthened by including a comparison with state-of-the-art methods in the field, providing a clearer context of the method's performance.\n\n4. Limited Discussion on Limitations: The paper does not adequately discuss the limitations of the proposed method, which is crucial for readers to understand the potential challenges and boundaries of the approach.\n\n5. Potential Overfitting: Given the nature of the meta-learning approach, there could be a risk of overfitting to the tasks at hand. The paper could benefit from a discussion on how this risk is mitigated or how the method performs under such circumstances."
            },
            "questions": {
                "value": "1. Clarify Mischaracterizations: The authors should revisit Section 3 to correct the mischaracterization of data loss and provide a more accurate description.\n\n2. Improve Figure Formatting: Enhancements should be made to the figures to improve readability and clarity, as this will aid in better conveying the results and contributions of the paper.\n\n3. Include Comparison with State-of-the-Art: Adding comparisons with leading methods in the field will provide a clearer benchmark of the proposed method's performance.\n\n4. Discuss Limitations and Potential Overfitting: A section discussing the limitations of the method and addressing potential overfitting concerns would add depth to the paper and provide a more balanced view of the approach.\n\nWith these enhancements, the paper would offer a more comprehensive and clear presentation of the proposed method, its strengths, and its potential areas for improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not apply."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726006035,
        "cdate": 1698726006035,
        "tmdate": 1699636389055,
        "mdate": 1699636389055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NrmwvKMOky",
        "forum": "EvyYFSxdgB",
        "replyto": "EvyYFSxdgB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a difficulty-aware task sampler (DATS) for meta-learning of PINNs. The model takes the variance of the difficulty of solving different PDEs into consideration by optimizing the sampling probability of meta-learning. An analytic approximation of the relationship of meta-model and sampling probability is provided to enhance learning. DATS is shown to improve the overall performance of meta-learning PINNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Quality: The performance of the approach seems good in the empirical part of the paper and the ablation study is detailed.\n2) Originality: The proposed two strategies to utilize $p^*$ are interesting and the comparison and analysis are comprehensive."
            },
            "weaknesses": {
                "value": "The main weakness is the clarity and correctness in both methodology and experiments\n\n1) Lack of intuitive explanation for the math derivations in section 4.1, making it hard to follow.  For example, the $w_i=  \\langle g_{tr}, g_{val} \\rangle$ in Eq.9 may be intuitively interpreted as \"assign the weight according to gradient similarity between train and valid\", I guess.\n\n2) There are some misleading typos and unexplained assumptions in section 4.1.\n* The LHS of Eq(7) should be $l_{\\text {val }, \\lambda}\\left(\\theta^{t+1}\\right)$, but not $l_{\\text {val }, \\lambda}\\left(\\theta^*\\right)$ written in Line 5 of this paragraph.\n* And similarly, the LHS of Eq(8) should be $p^{t+1}(\\lambda)$ not $p^{*}(\\lambda)$.\n* In Line4-5 of this paragraph, the gradient descent of training loss is defined as $\\theta^{t+1} = \\theta^t-\\eta \\int_\\lambda p(\\lambda) \\nabla_\\theta l_{t r, \\lambda}\\left(\\theta^t\\right) d \\lambda$, which assumes that training loss is defined as in Eq(11), the so-called DATS-w. But  DATS-rp loss in Eq(12) does not follow this assumption, thus the analysis is invalid for it. \n* The authors assume that the proposed iterative scheme for $p^*,\\theta^*$ converges and that adding regularization further stabilizes the convergence, without explanations. Intuitively, the first-order Taylor expansion is used in Eq(7), thus the step size $\\theta$ should be small enough to stabilize it. Additionally, the discrete approximations may also introduce errors.\n\n3) The experimental performance of sampling strategies (Section 5.2 and Appendix C) is not reported clearly. There are only figures in the main text. Fig.3,4,6 are difficult to extract information from since the lines and shades overlap heavily.  And since there are no digital numbers available, it is hard to compare the results. For example, in Fig.C.12, it seems Self-pace has the same performance as DATs."
            },
            "questions": {
                "value": "1) In Fig.3,4,6,  Why do the uniform and self-paced baselines only have higher and lower bounds of errors, not curves at different residual budgets?\n\n2) How does the DATS compare with the Reduced Basis Method, e.g., [1]?\n\n3) Does DATS also perform well on more difficult PDEs, such as with discontinuity and high-dimension? Examples includes the shock tube of compressible Euler's equation and  2d/3d N-S equation.\n\nRefs:\n[1] Chen, Yanlai, and Shawn Koohy. \"GPT-PINN: Generative Pre-Trained Physics-Informed Neural Networks toward non-intrusive Meta-learning of parametric PDEs.\" Finite Elements in Analysis and Design 228 (2024): 104047."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4221/Reviewer_Z9PQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830994914,
        "cdate": 1698830994914,
        "tmdate": 1700742554192,
        "mdate": 1700742554192,
        "license": "CC BY 4.0",
        "version": 2
    }
]