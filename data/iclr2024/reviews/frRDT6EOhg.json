[
    {
        "id": "9GDb26AodB",
        "forum": "frRDT6EOhg",
        "replyto": "frRDT6EOhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_wk7D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_wk7D"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Self-Contemplation (SEC) as an alternative for human-generated prompts for in-context learning (ICL) in LLMs. This enables LLMs to generate their own demonstrations for ICL, rather than having to rely on hand-generated human demonstrations. The experiments across a diverse range of different tasks show that SEC can be a meaningful alternative to ICL in some settings, and is able to outperform the zero-shot performance despite not requiring any human demonstrations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Proposing a method to move beyond hand-crafted demonstrations is a more scalable approach to ICL, where we no longer have to hand-craft domain-specific questions per task setting. Showing that this is able to match human-crafted demonstrations is an interesting finding, and the fact that it generally vastly outperforms the zero-shot setting shows that SEC should generally be used even in settings where we do not have access to high-quality human demonstrations.\n- The experimental results seem thorough and cover a wide range of standard tasks for evaluating LLMs."
            },
            "weaknesses": {
                "value": "-  While finding that it is feasible for a model to generate its own examples (rather than having human-crafted demonstrations) is insightful, the applicability of this method seems limited to large model sizes that are less likely to make reasoning errors during the self-generation process. How much would the method performance drop if tested with less capable language models? The paper would be strengthened with more thorough evaluations of when exactly SEC is usable. \n- As a baseline, it would be helpful to have direct comparisons with \"Automatic chain of thought prompting in large language models. (Zhang et al., 2022).\" to place this method in context relative to prior work. While the authors note that Auto-CoT makes use of questions from the training dataset as few-shot examples, it would still be insightful to see what performance gap this additional information leads to.\n\nIn general, paper presentation could be further polished:\n- Nit: Clean up Figure 1 further \u2013 e.g. typos follwing -> following , Demonetration -> Demonstration, in the Input to LLM section for SEC Demonstration Generation\n- Nit: Table 2 is a little hard to read, maybe have larger spaces / break lines between different rows of strategies"
            },
            "questions": {
                "value": "The authors note that ICL and SEC get different questions correct for GSM8K. Have they tried combining both methods? In other words, using ICL with human-crafted demonstrations combined with having the model additionally generate its own questions. Does this lead to better coverage in the correctly answered questions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Reviewer_wk7D"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713175065,
        "cdate": 1698713175065,
        "tmdate": 1700651053307,
        "mdate": 1700651053307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wZLPtU86hL",
        "forum": "frRDT6EOhg",
        "replyto": "frRDT6EOhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_B8T2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_B8T2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel zero-shot learning framework, self-contemplation prompting strategy (SEC), that uses LLMs to generate demonstrations on a task and apply in-context learning (LCL) with the LLM-crafted demonstrations.\nInterestingly, extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that the SEC performance is competitive or outperforms ICL with the human-crafted demonstrations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is very interesting in the sense that it completely eliminates the effort of writing demonstrations in in-context learning. Although the idea itself is very simple, the experimental results obtained are also very impressive."
            },
            "weaknesses": {
                "value": "What everyone probably cares about is whether the generated demonstration is correct. The authors analyze this point in Sec. 4 and Appendix B, but the number of predictions analyzed is very small (20 correct and 20 incorrect), making it difficult to reach a statistically consistent conclusion. \nIn particular, we need a clear hypothesis as to why the correct answer rate increases even though the generated demonstration is incorrect, and a sufficient amount of evidence to support it.\nAlso, using only a single LLM is used in the experiment makes it difficult to discuss whether the quality of the results is based on the properties of the language model itself, or whether it is simply a property that only GPT 3.5 has in the current version.  In particular, recent papers on prompting often try to analyze universal results by comparing multiple language models.\nAs a science, it is important to show universal results in LLM to some extent. \nThe Chain-of-Thought paper also provides results for multiple LLMs."
            },
            "questions": {
                "value": "1. Can you analyze a sufficient number of results to prove your hypothesis regarding the following questions stated in your paper?\n\"Why incorrect few-shot demonstrations could lead to correct final predictions, while correct few-shot demonstrations could also lead to incorrect predictions?\"\n\n2.Can we analyze the generality of this result using multiple language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1674/Reviewer_B8T2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741808258,
        "cdate": 1698741808258,
        "tmdate": 1700644671914,
        "mdate": 1700644671914,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RY5TlcjAPG",
        "forum": "frRDT6EOhg",
        "replyto": "frRDT6EOhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_fqq3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_fqq3"
        ],
        "content": {
            "summary": {
                "value": "The paper propose \"self-contemplating (SEC)\" prompting strategy a variation of the more common in-context learning (ICL) approach. The proposed approach consists of two steps: First, use the LLM to generate demonstrations based on the query sample; Second; use the generated demonstrations together with the query sample to create the final prompt that is fed back to the same LLM. The potential benefit lies in the fact that no additional reference training data is needed for curating the set of demonstrations. Experiments show that SEC performs comparably to the traditional ICL approach using the gpt-3.5-turbo model. However, SEC underperforms on other GPT models."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well written and is easy to understand. The paper introduces an interesting idea of only relying on the target LLM for generating demonstrations based on the target query sample. Doing so results in generating demonstrations that are probably better suited for the query sample. Also, the proposed approach helps remove the need for curating hand-crafted demonstrations which is a time consuming task."
            },
            "weaknesses": {
                "value": "1) The SEC method is only compared to the ICL approach where demonstrations are hand selected, i.e., not automatically selected. Several automated demonstration selection and curation approaches have been proposed in the last few years that should have been considered. SEC performs similar to hand-crafted ICL demonstrations. However, it is very likely that it might underperform once automated demonstration selection+curation approaches are introduced for comparison. Please look at the following papers and include them in your analysis:\n a) https://aclanthology.org/2023.findings-acl.273.pdf\n b) https://arxiv.org/abs/2211.04486\n c) https://arxiv.org/abs/2310.10707\n d) https://arxiv.org/abs/2302.05698\n e) https://arxiv.org/abs/2104.08786\n\n3) Relying on the LLM for generating demonstrations has the potential problem of propagating biases that exist in the model. The SEC method does not account for situations where model's bias can impact the final prediction and overall result.\n\n4) Only closed-source GPT-based models from OpenAI were considered. The authors should investigate if SEC can be extended to open-source models.\n\n5) The authors claim that quality of demonstrations generated by the SEC paradigm are better than hand-crafted demonstrations. A qualitative analysis should be done to verify this claim. However, it is surprising to see that SEC performance reduces as number of demonstrations is increased, thereby contradicting this claim."
            },
            "questions": {
                "value": "Please see the above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803027791,
        "cdate": 1698803027791,
        "tmdate": 1699636095521,
        "mdate": 1699636095521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qMQNcXGig4",
        "forum": "frRDT6EOhg",
        "replyto": "frRDT6EOhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_yTZX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1674/Reviewer_yTZX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the self-contemplation prompting strategy (SEC) to let the LLM itself propose the demonstrations and then use the demonstrations to do downstream in-context learning. Experiments on different benchmarks show that the LLM itself can generate meaningful demonstrations to help improve performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is quite clear and easy to follow.\n\n - The method is easy to understand.\n\n - The evaluation is well-designed."
            },
            "weaknesses": {
                "value": "- The main concern is the significance of the paper. The proposed prompting method can be treated as a two-step chain-of-though prompting by letting the LLM (1) first think about the possible demonstration and then (2) use the demonstration to do in-context learning. Form this point of view, the prompting framework is one specific usage of CoT, which makes the contribution limited."
            },
            "questions": {
                "value": "- I am curious about the limitations of the proposed method. Using LLM for self-improvement may suffer from performance degradation \u2013 Once the LLM generates some wrong correction, the overall performance may drop significantly. Have you noticed any domains experiencing performance declines when using LLMs to generate their prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699333895960,
        "cdate": 1699333895960,
        "tmdate": 1699636095460,
        "mdate": 1699636095460,
        "license": "CC BY 4.0",
        "version": 2
    }
]