[
    {
        "id": "vsvqitsm00",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
        ],
        "forum": "fGXyvmWpw6",
        "replyto": "fGXyvmWpw6",
        "content": {
            "summary": {
                "value": "1.\tThis paper proposes a federated virtual learning approach that leverages local and global dataset distillation techniques to simultaneously tackle the challenge of data heterogeneity as well as efficient training in federated learning. The authors claim that dataset distillation can exacerbate the heterogeneity among clients\u2019 local data and propose to alleviate this issue with distribution matching.\n2.\tThe problem addressed in this paper is novel and interesting. The adverse effect of dataset distillation in a federated learning setting is insightful. The proposed approach seems feasible and promising.\n3.\tIn your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n4.\tIf I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n5.\tSome of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem addressed in this paper is novel and interesting. The adverse effect of dataset distillation in a federated learning setting is insightful. The proposed approach seems feasible and promising."
            },
            "weaknesses": {
                "value": "1.In your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n\n2.If I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n\n3.Some of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
            },
            "questions": {
                "value": "1.In your paper, the model on clients is split into feature extractors and classification heads. This split learning-like paradigm has been widely adopted by a series of prior works [1,2,3]. Please explain the deplorability of your approach on existing methods. More elaboration on how your proposed method relates to these works would be appreciated.\n\n2.If I understand you correctly, FedProx is proposed by [4] rather than [5]. Do I misunderstand something?\n\n3.Some of the benchmark algorithms, such as FedProx [4], Scaffold [6], are somewhat outdated. In your experiments, you have used different open-sourced datasets as private data for clients, and this degree of data heterogeneity is apparently unfavorable for the regularization-based methods mentioned above. Would it be possible to compare your approach with some novel federated learning methods based on GANs [7], which seem to be more suitable for your scenario?\n\n[1]  \"FedICT: Federated Multi-task Distillation for Multi-access Edge Computing.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[2] \"Group knowledge transfer: Federated learning of large cnns at the edge.\" Advances in Neural Information Processing Systems 33 (2020): 14068-14080.\n\n[3] \"Exploring the distributed knowledge congruence in proxy-data-free federated distillation.\" arXiv preprint arXiv:2204.07028 (2022).\n\n[4] \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n[5] \"On the convergence of fedavg on non-iid data.\" arXiv preprint arXiv:1907.02189 (2019).\n\n[6] \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[7] \"Data-free knowledge distillation for heterogeneous federated learning.\" International conference on machine learning. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Reviewer_8zES"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697290721371,
        "cdate": 1697290721371,
        "tmdate": 1700371876348,
        "mdate": 1700371876348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AbtRGI20Ig",
        "forum": "fGXyvmWpw6",
        "replyto": "fGXyvmWpw6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a federated learning method called FedLGD that uses local and global dataset distillation to handle data heterogeneity.FedLGD uses an iterative distillation process to generate local and global virtual datasets that mitigate data heterogeneity and improve efficiency in federated learning. The local-global distillation and feature regularization are key components that help FedLGD achieve strong performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The discover of using dataset distillation can amplify the statistical distances is interesting.\n  2. Achieves state-of-the-art results on benchmark datasets with domain shifts, outperforming existing federated learning algorithms."
            },
            "weaknesses": {
                "value": "1. t-SNE figures are not represented as vectors.\n\n2. Sharing gradients from clients to the server for a global virtual data update may pose security risks. Some attacks could potentially reconstruct raw data using gradient information, similar to the risks associated with Deep Gradient Leakage. Why sharing averaged gradients is safe?\n\n3. What is the rationale behind clients requiring local virtual data instead of training directly on their local private data?\n\n4. Could you clarify why this method has not been compared to other FL methods utilizing dataset distillation?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission541/Reviewer_PXsJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission541/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772299371,
        "cdate": 1698772299371,
        "tmdate": 1699635981153,
        "mdate": 1699635981153,
        "license": "CC BY 4.0",
        "version": 2
    }
]