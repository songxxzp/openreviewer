[
    {
        "id": "6OTIPAlvk2",
        "forum": "SZZEH8x54D",
        "replyto": "SZZEH8x54D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_fPwG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_fPwG"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to leverage the part detection capabilities of OWL-ViT, together with a large collection of bird images from eBird (~0.5M images and ~11k species) and part-centric bird species descriptions generated by ChatGPT, to train a bird classification model that first predicts part bounding boxes in parallel to part-centric descriptions, and then outputs a species name based on these descriptions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- The proposed concept is very interesting and conducive to a very intuitive form of interpretability.\n2- The authors showcase a highly competitive classification performance on several datasets, including in a zero-shot setting, highlighting the advantages of description-based classification."
            },
            "weaknesses": {
                "value": "1- The number of elements, losses and training steps makes the paper very hard to follow. In spite of this complexity, I couldn\u2019t find ablation studies showing the need for each of the components/steps. Have the authors done this?\n2- In the comparison with traditional ZSL methods, it\u2019s unclear whether these last ones also make use of the Bird-11K dataset for pretraining. If not, I\u2019m not sure this is a fair comparison. What would be the performance of PEEB without pretraining? I can imagine that the ZSL performance boost is rather due to the supervised pretraining with Bird-11k rather than the proposed method itself.\n3- Related to the previous one, I don\u2019t agree that the pretraining step doesn\u2019t grant access to the labels, since the descriptions do depend on the labels. As such, all methods being compared should, in principle, have access to Bird-11K for a fair comparison.\n4- The authors claim that Box MLP allows to understand \u201cwhere PEEB looks at when making predictions\u201d. Is that so? It seems the bonding boxes are predicted in parallel to the classification, so why would the classification be conditioned on the image region captured by the boxes?\n5- The authors claim that \u201cOur Bird-11K dataset enriches the field\u201d but, at the same time, disclaim that they are not releasing the dataset. I\u2019d say it\u2019s one or the other. Which one is it?"
            },
            "questions": {
                "value": "See questions in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697915148894,
        "cdate": 1697915148894,
        "tmdate": 1699636605751,
        "mdate": 1699636605751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M8LibXwE6n",
        "forum": "SZZEH8x54D",
        "replyto": "SZZEH8x54D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_AaGv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_AaGv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a bird classification model that uses a combination of visual and textual information to identify birds in images. \nThe different steps are as follows:\n1. Use a pre-trained detector to localise 12 key bird parts in the input image\n2. Use GPT-4 to generate a textual description of each bird part for each species. \n3. Compute a match score between the input image and each bird class by comparing and summing the visual and textual embeddings of the bird parts. \nThe model can be used for zero-shot classification, where the model is able to classify birds it has never seen before, provided a textual description of each part is available."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper presents a novel approach to object recognition that combines visual and textual information based on part descriptions.\n- Achieves state-of-the-art results on bird recognition datasets, even when the model is not exposed to test classes during the pre-training phases\n- Object descriptions are fully editable by humans\n- The paper is well written\n- Experiments are thorough"
            },
            "weaknesses": {
                "value": "My main concern about the paper is its novelty. The paper borrows and combines ideas from 2 papers: \n- OWL-ViT (Minderer et al., 2022), which is an open vocabulary object detector. \n- M&V (Menon & Vondrick, 2022) which is an object classifier based on textual description.\nMy opinion is that the technical contribution is low, especially for a top-tier machine learning conference."
            },
            "questions": {
                "value": "Have you analysed misclassified images? Are the failures due to the difficulty of detecting the parts or measuring the similarity between the parts and their description?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698076973907,
        "cdate": 1698076973907,
        "tmdate": 1699636605651,
        "mdate": 1699636605651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8BX8sOKOz8",
        "forum": "SZZEH8x54D",
        "replyto": "SZZEH8x54D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_4q8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_4q8P"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new zero-shot bird classifier based on part-level text descriptions. The method represents each class with 12 part-level text embeddings and represents each image with 12 part-level image embeddings. The model is trained to maximize the dot products between the part-level image embeddings and the corresponding part-level text embeddings. A new and larger bird dataset is constructed to serve as a pretraining dataset. The pretraining includes a classification loss and a detection loss on bird parts. After pretraining, the model is finetuned on downstream smaller bird datasets and the zero-shot learning performance is measured. The experimental results show that it outperforms the baselines with only class-name descriptions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-This paper contributes a new bird dataset that consists of more images and bird species.\n\n-The idea of using part-level text description for zero-shot classification is interesting and somewhat new in the field."
            },
            "weaknesses": {
                "value": "-Learning part-level information for zero-shot learning is not new. This paper ignores many part-based zero-shot learning works [A, B, C]. Although this paper discussed some existing works on part-based classifier in the related works, it excluded those papers that are highly relevant. One difference I notice is that this paper adopts part-level descriptions written in text, while [A] uses part-level attribute annotations. As there is no comparison with those methods, it is unclear if the textual part descriptions improve the attribute annotations. Thus, it is hard to judge if the contribution is significant or not.\n\n[A] Xu et al., Attribute Prototype Network for Zero-Shot Learning. NeurIPS 2020\n[B] Zhu et al., Semantic-Guided Multi-Attention Localization for Zero-Shot Learning. ICCV 2017\n[C] Sylvain et al., LOCALITY AND COMPOSITIONALITY IN ZERO-SHOT LEARNING, ICLR 2020\n\n-Using text description rather than class names for zero-shot bird classification is not new either. The first work in this direction is [D] where they annotate textual description for each bird and learn a vision-language model with contrastive loss. There have been quite a few extensions of [D]. Those relevant works are unfortunately ignored. The claim that other paper relies on class names for zero-shot learning is not true.    \n\n[D] Reed et al., Learning deep representations of fine-grained visual descriptions. CVPR 2016\n\n-Results are much worse than classical zero-shot learning methods. This paper fails to compare with recent classical zero-shot learning methods on the standard ZSL splits. In fact, the results in Table 3 are significantly worse than the results reported in [A] which was published in 2020. This is very concerning because this paper uses much larger model (CLIP) and is pretrained on more bird images than other classical ZSL methods. Thus, the claim that this work achieves the SOTA is not true.\n\n-Details of how to construct the part textual description is missing."
            },
            "questions": {
                "value": "The authors are encouraged to address my concerns listed in the weakness. \n\n-Are there part-level bounding box annotations in the Bird-11K dataset? How are they annotated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857626780,
        "cdate": 1698857626780,
        "tmdate": 1699636605530,
        "mdate": 1699636605530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "68Cdnb6Hjy",
        "forum": "SZZEH8x54D",
        "replyto": "SZZEH8x54D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_H7HS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5768/Reviewer_H7HS"
        ],
        "content": {
            "summary": {
                "value": "The paper presents PEEB, a novel bird classifier that allows users to describe in text the12 parts of every bird that they want to identify. This leads to both explainability and edibility. The proposed method is comparable to fully supervised learning on CUB"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors show how CLIP based classifiers do not work well unless provided with the bird class name\nThe proposed method is competitive on benchmarks in both the zero shot and finetuning settings"
            },
            "weaknesses": {
                "value": "I think that the domain where the results are presented is very niche and for the method to be widely useful, it would be good to see it work well on multiple domains or less specialized domains"
            },
            "questions": {
                "value": "How can this method be extended to other domains or less specialized domains like ImageNet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5768/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5768/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5768/Reviewer_H7HS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699594007628,
        "cdate": 1699594007628,
        "tmdate": 1699636605427,
        "mdate": 1699636605427,
        "license": "CC BY 4.0",
        "version": 2
    }
]