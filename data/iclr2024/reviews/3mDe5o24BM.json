[
    {
        "id": "6IEWxwPa72",
        "forum": "3mDe5o24BM",
        "replyto": "3mDe5o24BM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_T5e1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_T5e1"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors adopt RLHF into text-to-image diffusion model and propose a new text-to-image diffusion model, termed as HFDream, which can produce viewpoint specified images with the text prompt. Furthermore, HFDream combined with DreamFusion can lead to high-quality text-to-3D generations. Specifically, they firstly collect multi-view data using human feedback, then train a reward model using these multi-view data. Subsequently, they finetune a text-to-image diffusion model with the reward model to learn multi-view image generation. Finally, DreamFusion with the finetuned diffusion model can enable high-quality 3D generation. In addition, they conduct extensive experiments to validate the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed method is an alternative to improve the geometric consistency of 3D generation. It adopts RLHF into diffusion models, leading to multi-view image generations and avoiding large-scale 3D assets.  Besides, it can produce pleasant 3D generations and is validated with various experiments.\n+ This paper is written well and easy to follow. In addition, it provides enormous quantitative and qualitative results to validate the proposed method."
            },
            "weaknesses": {
                "value": "- To my knowledge, collecting multi-view data using human feedback is costly and somewhat difficult for some viewpoint, such as backside views. Moreover, as shown in section 4.2, it is essential to collect larger and more diverse human datasets  to improve generalization on more and unseen prompts. Thus, I raise a concern: is collecting these multi-view data cheaper than 3D assets?\n- For the objects used in this paper, it is easy for human to identify their view direction, such as front view. But, there are objects for which different views is the same. Moreover, it is difficult to identify direction for 3D scenes. Thus, how to solve these problems with the proposed method?\n- To validate the proposed method, the authors only use DeepFloyd-IF (pixel-based model) as their baseline text-to-image model. But, the proposed method has a generalization on diffusion models. Thus, it is better to validate the proposed method on other diffusion models, such as StableDiffusion (latent-based model)."
            },
            "questions": {
                "value": "- As shown in the experiments, HFDream produces unnatural images. In detail, these images are oversaturated. In contrast, IF generates realistic images, although it does not have a good geometric consistency. Is the guidance scale high for HFDream?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729787716,
        "cdate": 1698729787716,
        "tmdate": 1699636877798,
        "mdate": 1699636877798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sQ7MaFEOrz",
        "forum": "3mDe5o24BM",
        "replyto": "3mDe5o24BM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_L7ZF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_L7ZF"
        ],
        "content": {
            "summary": {
                "value": "This work presents a text-to-3D pipeline by using human feedback to enhance multi-view image generation. Both text-to-3D and personalized text-to-3D tasks are performed to evaluate the proposed HFDream."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "++ Extensive human survey evaluation is performed.\n\n++ It is interesting to apply HFDream for Text-to-3D DreamBooth generation task."
            },
            "weaknesses": {
                "value": "-- The whole method seems a simple human-in-the-loop engineering pipeline and is composed of several existing techniques (ImageReward and DreamFusion). The main difference is the fine-tuning of ImageReward tailed for multi-view image generation, which is somewhat not novel.\n\n-- The experimental results are not convincing, due to the lack of many recent state-of-the-art text-to-3D baselines (e.g., Magic3D and Fantasia3D) in performance comparison.\n\n-- I also have the concern on the human-labeled dataset which is a subset of ImageReward-v1.0 data and only has 200K pairs. So how to testify that the generalization of fine-tuned text-to-image model is not degraded?\n\nMoreover, the evaluation prompt set is extremely small (only 61 text prompts). At least authors should report their results over the most test prompts in DreamFusion website (https://dreamfusion3d.github.io/gallery.html)."
            },
            "questions": {
                "value": "The overall technical contribution is limited, and more strong baselines should be included for performance comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732152972,
        "cdate": 1698732152972,
        "tmdate": 1699636877680,
        "mdate": 1699636877680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OssbYBMKzj",
        "forum": "3mDe5o24BM",
        "replyto": "3mDe5o24BM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_PzSD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7338/Reviewer_PzSD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to enhance text-to3D generation by learning from human feedback for generating desired views. It collects a human-labeled dataset to train a reward model and adopts this model to fine-tune a diffusion model, and this reward model can help the diffusion model generate the image aligned with the viewpoint text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The whole idea is Intuitive and effective.\n2. The reward model can help the model generate images that match the viewpoint text.\n3. The paper is well written."
            },
            "weaknesses": {
                "value": "My main concern is the motivation of this work. Indeed, Dreamfusion has a multi-face problem since the diffusion can not generate the image aligned with the camera. There are two main methods that can solve this problem. First type: we can train a camera-aware diffusion model like Zero123 and MVdreamer[1]. These models can generate an image aligned with the camera.  These methods use the camera parameter as guidance, which is much more correct than viewpoint text since the viewpoint text only has three settings (front, side, and over). Second type: We can introduce the Controlnet [2] or the mesh as the 3D prior [3] to solve the multi-face problem.  \nSo, what is the advantage of human feedback? The whole process is time-consuming and cumbersome, and I found the results were not very well either.  Meanwhile, in addition to the multi-face problem, the proposed method still has the same issues with Dreamfusion: Over smooth and over-saturated.\n\n[1] Shi, Yichun, et al. \"Mvdream: Multi-view diffusion for 3d generation.\" arXiv preprint arXiv:2308.16512 (2023).\n[2] Han, Xiao, et al. \"HeadSculpt: Crafting 3D Head Avatars with Text.\" arXiv preprint arXiv:2306.03038 (2023).\n[3] Chen, Rui, et al. \"Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation.\" arXiv preprint arXiv:2303.13873 (2023)."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698910648583,
        "cdate": 1698910648583,
        "tmdate": 1699636877558,
        "mdate": 1699636877558,
        "license": "CC BY 4.0",
        "version": 2
    }
]