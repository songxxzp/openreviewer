[
    {
        "id": "fhPOReeoMU",
        "forum": "l3s3HwJYDm",
        "replyto": "l3s3HwJYDm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_8YJ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_8YJ7"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the opponent modeling problem and proposes an opponent modeling algorithm based on sub-goal inference. Concretely, the proposed method designs a subgoal inference model and a subgoal selector. The subgoal inference model takes historical trajectory as input and outputs the predicted subgoal (a high-dimensional vector) of the opponents. It is trained based on variational inference. The subgoal selector chooses a subgoal for the update of the Q-value network. It aids the training of the subgoal inference model as the Q-value network affects the sampling process."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper introduces the subgoal concept from MARL to the opponent modeling domain, which is promising.\n\n+ The experiments are performed in three experiments (although two of them are very simple scenarios)."
            },
            "weaknesses": {
                "value": "- The paper lacks clarity and there are some confusing parts that need more explanation.\n\n- Some technical designs of the proposed method do not conform with the canonical RL algorithm. More theoretical analysis is needed.\n\n- As shown in Figure 7(a), the prediction accuracy of opponent subgoal is low."
            },
            "questions": {
                "value": "1. In Section 3.2, this paper adopts a state-extended MDP. Therefore, for the next state, the Q-value equation should be $Q(s_{t+1},g_{t+1}, a)$. However, the Eq. (4) writes it as $Q(s_{t+1},g_{t}, a)$. It should be explained that why $g_t$ is the same as $g_{t+1}$.\n\n2. In line 3 of Algorithm 1, the subgoal inference model parameters should be $\\phi$ and $\\theta$ instead of $\\tau$ and $\\theta$.\n\n3. In algorithm 1, the action is sampled based on $Q(s,\\hat{g},a)$ given $\\hat{g}$ (line 9). However, when updating Q-network, the subgoal may be $\\bar{g}$ instead of $\\hat{g}$ based on Eq. (8). That means given the same experience tuple $(s, a, a^o, r)$, the Q-network input may be different for the sampling and learning phase, which does not conform with the canonical RL theory. What would the theoretical impacts be?\n\n4. In Eq. (5), $\\bar{g}$ is output by a prior model $p_{\\psi}$ while in Eq. (6) and Eq. (7), it is from $f_{\\psi}$. What is the relationship between $p_{\\psi}$ and $f_{\\psi}$? Moreover, when $f_{\\psi}$ first appears in Page 5, it outputs $\\bar{g}_t$ given $s_t^g$. However, in Eq. (6) and Eq. (7), its input format becomes $\\mathcal{N}_t^H$. Why?\n\n5. Why in a general-sum game,  a conservative strategy is more suitable? Given the opponents' subgoal, shouldn't we just maximize our own Q-value?\n\n6. How to get the $f^{-1}_{\\psi}$ in Section 4.6?\n\n7. How to compute $card(\\mathcal{S}_g)$ in Appendix A.1? Based on my understanding, if we assume every state can be transit to a goal state after a certain action sequence, $card(\\mathcal{S}_g)$ should be $\\frac{n_A^k-1}{n_A-1}|G|$. In addition, the sentence \"Moreover, a lower $k$ value, signifying predicted subgoals, facilitates generalization. So, it is loosely bound of $|G|$ for OMG. Due to our method favoring the adoption of extreme values as goal states, a limited quantity of such states exist\" is confusing.\n\n8. In appendix A. 2, this paper uses a set of identical states to acquire the action vectors of the policy in the test set. What is the detailed process of obtaining the set of identical states? Are these states sampled by a certain policy?\n\n9. The reason of baseline method choice is unclear. Why not compare with some newer opponent modeling methods, e.g., [1] and [2]?\n\n[1] Greedy when sure and conservative when uncertain about the opponents. ICML 2022.\n\n[2] Model-based opponent modeling. NeurIPS 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Reviewer_8YJ7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698059748045,
        "cdate": 1698059748045,
        "tmdate": 1699636464560,
        "mdate": 1699636464560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BS1mVClgsl",
        "forum": "l3s3HwJYDm",
        "replyto": "l3s3HwJYDm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_W6q1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_W6q1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an opponent modeling method (OMG) based on subgoal inference. In particular,  a subgoal inference model is employed to infer the subgoal of the opponent based on a VAE architecture. A subgoal selector model is used to make a balance between the inferred subgoal and a prior subgoal. In addtion, the agent policy or value function is conditioned additionally on the selected subgoal. Experimental studies on standard benchmarks verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is clearly presented.\n- The experimental study seems comprehensive."
            },
            "weaknesses": {
                "value": "Main weaknesses:\n\n- I don't think predicting subgoals itself alone can be claimed as a significant contribution in comparison to previous work on opponent modelling.  [1] predicts the opponent\u2019s goal as well. Also, the subgoals discussed in OMG are essentially future states of the opponent, and predicting future states are not necessarily better than predicting future actions. \n\n- I don't think it is a correct statement to say that most previous work on opponent modeling predicting the opponent's actions. For instance, [2] predicts a policy embedding. [3] is a meta-learning method for opponent modelling.  [4] predicts which type of policy. \n\n- The general architecture of OMG that uses VAE to predict something is quite similar to many previous opponent modelling methods that use VAE as well: for instance [5] and [6]\n\n\nMinor:\n\n- very related work on opponent modeling are missing in the literature review. For instance, [2] [3] [4]. A full literature review on opponent modeling is suggested. \n- section 3.1 is too preliminary to be included in the Method section.\n- the last sentence of section 3.2. \"In short, the number of (s,g) is significantly smaller than that of (s,a)\". Please provide strong evidence for this, as I do think it is domain-dependent whether the number of (s,g) is larger or smaller than (s,a). \n- Equation 8 is very ad hoc. \n\n[1] Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in\nmulti-agent reinforcement learning. In International conference on machine learning, pp. 4257\u2013\n4266. PMLR, 2018.\n\n[2] Haobo Fu, Ye Tian, Hongxiang Yu, Weiming Liu, Shuang Wu, Jiechao Xiong, Ying Wen, Kai Li, Junliang\nXing, Qiang Fu, et al. Greedy when sure and conservative when uncertain about the opponents. In\nInternational Conference on Machine Learning, pages 6829\u20136848. PMLR, 2022.\n\n[3] Al-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I., and Abbeel, P. Continuous adaptation via metalearning in nonstationary and competitive environments. In International Conference on Learning Representations, 2018\n\n[4] Zheng, Y., Meng, Z., Hao, J., Zhang, Z., Yang, T., and Fan, C. A deep bayesian policy reuse approach against non-stationary agents. In Proceedings of the 32nd International Conference on Neural Information Processing\nSystems, pp. 962\u2013972, 2018.\n\n[5] Georgios Papoudakis and Stefano V Albrecht. Variational Autoencoders for Opponent Modeling in\nMulti-Agent Systems. arXiv preprint arXiv:2001.10829, 2020.\n\n[6] Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, and Katja Hofmann. Deep interactive\nbayesian reinforcement learning via meta-learning. arXiv preprint arXiv:2101.03864, 2021."
            },
            "questions": {
                "value": "Except for the experimental results, I am not convinced why predicting subgoals (i.e., future states) as OMG does are better than predicting something else about the opponents (e.g., actions, policy embeddings, policy categories, etc.). Could you please provide more justifications on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Reviewer_W6q1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744777588,
        "cdate": 1698744777588,
        "tmdate": 1699636464445,
        "mdate": 1699636464445,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U7wi9ZzNlT",
        "forum": "l3s3HwJYDm",
        "replyto": "l3s3HwJYDm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_ZFnJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_ZFnJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to opponent modelling based on predicting the opponent's subgoal rather than its next granular action.  (In this work, \"subgoal\" just means a desired future state.) The authors argue that this approach is simpler and more efficient than predicting the next action, since it abstracts away unimportant details and yields longer lasting predictions. A subgoal predictor is trained based on past trajectories using variational inference. The training label (i.e., the \"true\" subgoal that the opponent was previously following) is derived via maximax/minimax-style logic, depending on whether the scenario is cooperative/competitive. Experiments across multiple domains show that this yields a persistent edge versus previous approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Assuming that I actually understood the paper, I think it contains some very clever ideas. I particularly like the approach of calculating the opponent's goals retrospectively via Equation 6 or 7, depending on the setting. This strikes me as an intuitive idea; if we're expecting the opponent to behave adversarially, we can assume that they were striving to reach the goal that would have hindered us the most. I also like the clean formulation of what a \"subgoal\" is, where it's simply a desired future state. This avoids the need for human experts to define the set of possible subgoals, which is a major limitation in a lot of other work that leverages the concept of subgoals (e.g., hierarchical RL). I agree with the overall motivation too; intuitively, it does seem better to predict subgoals rather than actions.\n\nI'm not an expert in opponent modelling, but the approach seems fairly novel to me. While the approach does not yield a huge advantage in the experiments, it does achieve a moderate but persistent edge, and this is understandable since opponent-predicting ability is not the only driver of performance. Some of the side results are a worthy contribution too, e.g., the nice analysis in Figure 7(b). While I have some issues with the presentation (explained below), the quality of the writing in the introduction and related work section is excellent."
            },
            "weaknesses": {
                "value": "My main issue with the work is that I found the Method section from 3.2 onwards to be very hard to follow. I had to read it maybe 5 times before it started to make sense, and even now I'm not fully sure that I understand all the details. For example:\n- One of the main points of confusion I have is that I'm not sure what $f_\\psi$ and $p_\\psi$ are supposed to be. They share the same parameters, $\\psi$, so are they the same function, related functions, or is the notation accidentally overloaded? \n- I don't follow how $p_\\psi(\\bar{g}|s^g)$ can be pre-trained \"with the prior subgoal state $s_g$ being derived from the subgoal selector\". Doesn't the subgoal selector leverage the value function? How do we already know the value function during the pre-training phase?\n- How are goals represented to the Q-function? Are they encoded via $f_\\psi$?\n- I'm assuming that the training update at line 15 in Algorithm 1 can only be performed using the experience at time $t$ after $\\bar{g}$ has later been inferred at time $t+H$. Is this correct? If so then it should be spelled out -- I got very confused at first trying to figure out how $\\bar{g}$ could be known when performing the update.\n\nApologies if some of these questions don't make sense. I must admit that I'm not deeply familiar with variational inference, so my confusion might just reflect my own deficiencies. I'll be less concerned if the other reviewers found the methodology clear.\n\nBeyond the above points, there are a few little issues:\n- In the first paragraph of 3.3, \"which as the policy's input\" should presumably be \"which *act* as the policy's input\".\n- The second sentence in the subsection \"Subgoal inference model\" ends weirdly.\n- \"You can tell a lot about a book from its cover\" is a slightly strange analogy to use; I'd cut this.\n\nWhile my overall confidence is low, my feeling is that the paper is not quite publishable in its current state. If the other reviewers found Sections 3.2 & 3.3 easy to understand then I'll be inclined to increase my score, but otherwise I think the explanation of the methodology needs to be improved a lot."
            },
            "questions": {
                "value": "Have I understood the general gist of the methodology correctly? If not, I'd really appreciate a dumbed down explanation, if that's possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Reviewer_ZFnJ",
                    "ICLR.cc/2024/Conference/Submission4812/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823773539,
        "cdate": 1698823773539,
        "tmdate": 1700740123924,
        "mdate": 1700740123924,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JbDr38c9ov",
        "forum": "l3s3HwJYDm",
        "replyto": "l3s3HwJYDm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_nLrr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4812/Reviewer_nLrr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a multiagent reinforcement learning algorithm that leverages opponent modelling to improve training performance (both sample efficiency and final performance), as well as generalization to partner strategies that were not seen during training.  They present a varian of \"opponent modelling\" that, rather than predicting the individual actions of other agents, predicts the state the joint strategy of all agents will likely reach within the next several steps (which they describe as a \"subgoal\").  The main goal of opponent modelling in this context is to compensate for the non-stationarity of the policies of other agents during training.  The intuition behind this work is that while an accurate one-step opponent model provides stationarity over a single step, a model of long-term outcomes provides approximate stationarity over several time steps.\n\nTheir method does not predict complete states, but learns a feature embedding as part of a generative model during a pretraining phase.  Salient subgoals are selected in hindsight from previous trajectories using a value-based heuristic, and this is used to train a recurrent network to predict future goals.  The RL agent's value function then conditions on the predicted subgoal in addition to the current state.  They present experimental results demonstrating that their method provides a modest performance improvement over independent Q-learning, as well as two existing opponent modelling methods, in several benchmark MARL environments (including the predator-prey particle environment, and the Starcraft multiagent challenge)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I felt that the core idea of the work has a lot of potential, and there is room for subsequent work on this idea.  Intuitively, accurate predictions of an opponent's behavior over a single timestep will be less informative than predictions of their long term behavior, particularly in settings where interactions between agents are in some sense \"sparse\".  While the empirical results are not definitive, they do support the claim that subgoal inference can improve over \"naive\" opponent modelling."
            },
            "weaknesses": {
                "value": "The main weakness of the work is the presentation, particularly the description of the OMG framework itself.  While it is implied that the parameters of the subgoal feature mapping (denoted as $\\psi$) are trained using a VAE reconstruction loss on an initial batch of opponent data, the details of this process do not seem to have been provided.  As the subgoal representation encoded by $\\psi$ would seem to play a large role in success or failure of the algorithm, the loss which $\\psi$ minimizes should be provided.  A few other points that were unclear:\n1. What form does the subgoal prior $p_{\\psi}$ take, and what is its relation to the feature mapping $f_{\\psi}$ (my guess is that $p_{\\psi}$ is a normal distribution centered at $f_{\\psi}$)\n2. Similarly, what form does the posterior model $q_{\\phi}$ take\n3. How is the posterior goal prediction $\\hat{g}$ derived from $q_{\\phi}$?  Is it sampled from $q_{\\phi}$, or do you choose $\\hat{g}$ maximizing the likelihood under $q_{\\phi}$?\n4. It isn't completely clear whether the subgoal selection process used during the training of $\\phi$ and $\\theta$ is the same as that used during policy execution (either equation 6 or 7).\n5. A minor point, but dropping the subscript in equation 5 makes it a little unclear that $\\tau$ is the trajectory $\\tau_t$ up to time $t-1$.  \nA less significant weakness is that the experimental results could be more comprehensive.  One concern is that OMG is only compared against IQL and other opponent modelling methods.  It would be helpful to see how well these perform when compared against centralized multi-agent RL methods such as QMIX.  It was also a little surprising that learning curves were not provided for the SMAC environments."
            },
            "questions": {
                "value": "1. During training, were all agents implementing OMG simultaneously?  I assume this was the case, but it was not made explicit.\n2. What policies did agents follow during the pre-training phase (data collection for training $\\psi$)?\n3. How consistent are subgoal predictions over time?  Do they tend to remain stable over 2-3 timesteps?\n4. Do you have any results where the VAE used to predict subgoals was trained without being conditioned on a pre-selected subgoal?  It would seem possible that you could get a similar result by simply conditioning Q-functions on a latent representation of opponent trajectories.\n5. Could the sub-goal predictor $q_{\\psi}$ have been trained with supervised learning on the outputs of the sub-goal selector?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4812/Reviewer_nLrr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956158641,
        "cdate": 1698956158641,
        "tmdate": 1700686596039,
        "mdate": 1700686596039,
        "license": "CC BY 4.0",
        "version": 2
    }
]