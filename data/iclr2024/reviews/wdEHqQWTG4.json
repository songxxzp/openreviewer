[
    {
        "id": "btB1YLRJaq",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Lxrc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Lxrc"
        ],
        "content": {
            "summary": {
                "value": "This work develop a framework for portfolio management using robust RL. An adversary is introduced to model the uncertainty in the real application, and the goal is to find a policy performs well under the uncertainty."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea to apply robust RL in this area is interesting, considering the inherent uncertainty in the problem. The idea of using an adversarial is not new, but still interesting and useful. \n2. According to the experiment results, the CCRRL approach outperforms other RL algorithms, which shows the power of the framework."
            },
            "weaknesses": {
                "value": "I am not an expert in finance, but from the aspect of machine learning or reinforcement learning, the contribution of this paper is not much. The idea of adversarial training has been used for a while, and I can't see too much about this paper's contribution or novelty. \n\nI somehow feel that this paper does not fit in the ICLR conference, considering the major contribution and motivation. I admit that I am not familiar with finance area, so I maintain a low confidence score."
            },
            "questions": {
                "value": "I don't have any questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698265686179,
        "cdate": 1698265686179,
        "tmdate": 1699636961891,
        "mdate": 1699636961891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PhX4wn7ENB",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Nhcj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Nhcj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a robust reinforcement learning (RL) approach within a multi-agent framework to portfolio optimization. This approach considers both cooperation and competition strategies: Competition lets the agents compete for limited resources and cooperation is used to accelerate convergence. Four different cooperation/competition strategies are proposed where the difference comes from the different modifications applied to the selected actions before they are executed. The authors conducted experiments on a real-world financial dataset that they collected, and they made comparisons with standard single-agent RL algorithms including A2C, PPO, and DDPG."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes to study the portfolio optimization problem from an interesting perspective, by formulating this problem as a multi-agent system with both cooperation and competition."
            },
            "weaknesses": {
                "value": "In my opinion, the amount of contribution and the significance of the results in the paper are not sufficient for ICLR. The writing/presentation is vague and non-rigorous throughout the paper, which prevents me from further appreciating the contributions of the work. \n\nSpecifically, this paper proposes a new multi-agent formulation, but fails to properly define many important concepts in their approach, such as the definition of the state and action spaces. The definitions of the game formulation (e.g., who the players are and how they are cooperative/competitive) and the state transitions are also missing. It is unclear to me how the RL actions translate to the decisions in the portfolio optimization problem. The four proposed strategies (by multiplying the previously selected action to a certain value) are also a bit arbitrary to me and there is no justification about why such strategies are chosen. The authors also do not discuss how the reward functions are defined and why they need a further modification function to the rewards. \n\nFor the methodology part, the authors simply use an existing and standard DDPG algorithm for learning. In the experiments, the authors only evaluated their method on one dataset that they collected, without introducing the formats or the basic statistics of the dataset. The authors also only compared their method with the performances of standard single-agent RL algorithms despite that their formulation is multi-agent, which makes me question if such comparisons are fair. These experiment results are not sufficient to make conclusions about the effectiveness of the proposed method."
            },
            "questions": {
                "value": "How do you define the action space in the RL formulation, and how do they translate to the decisions in your portfolio optimization problem? \n\nYour CCRRL method uses DDPG as the RL solver, but why would you compare it with another DDPG algorithm in your experiments? How are these DDPG solvers different from each other?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Reviewer_Nhcj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698269016326,
        "cdate": 1698269016326,
        "tmdate": 1699636961766,
        "mdate": 1699636961766,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Zigu8fIiy",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Ju8B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_Ju8B"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach to robust portfolio optimization via reinforcement learning (RL). Since RL is known to be sensitive to dynamic changes in the environment, the paper proposes CCRRL (Competitive and Cooperative Robust Reinforcement Learning via Multi-Agent System), which combines techniques from cooperative and competitive MARL. The approach is based on DDPG and is evaluated in some financial markets setting."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper addresses an interesting application domain."
            },
            "weaknesses": {
                "value": "**Novelty**\n\nThe paper applies DDPG to a specific domain. There are no novel additions to the algorithm itself. Instead, the original policy actions are modified according to some rules to add cooperative and competitive aspects (I do not understand how and why, though). Overall, I consider the novelty of the contributions as limited.\n\n**Clarity**\n\nI do not understand the motivation behind this approach. To me, the whole market setting is selfish (neither purely cooperative nor competitive) thus it is unclear to me how the cooperative and competitive additions are supposed to help in this case. Unfortunately, the paper does not provide any theoretical reasoning but only keeps the concepts on a high level by explaining how things are done - but not why.\n\nDespite introducing variables in Table 1, there is no formal definition of the RL setting and how it connects with the portfolio optimization problem (e.g., how do state and action space look like? What is the actual reward structure of the problem?).\n\nI also do not understand why modified rewards are multiplied to actions. What is rationale behind multiplying actions and rewards?\n\nFurthermore, I do not see any multi-agent component in the whole approach. The market might represent a selfish multi-agent system, but as far as I understood, only single-agent RL is applied to a particular agent. There are no multi-agent-specific techniques used in CCRRL, such as credit-assignment mechanisms, counterfactual baselines, centralized critics, etc.\n\n**Soundness**\n\nThe competitive and cooperative part are not sound in my view.\n\nThe competitive part only multiplies the action with a modified reward (that preserves the sign) but does not take the rewards of others into account (in competitive settings, the goal is to minimize other agents' rewards, which is impossible if they are not known). Therefore, a \"competitive\" agent would merely be an individual rational player who acts selfishly in a game-theoretic sense.\n\nThe cooperative part only considers the absolute value of the reward, which could be negative. In a true cooperative setting, all agents perceive a shared reward and optimize a common value function. In the market setting, the agent therefore needs to consider the reward of all other agents, which is unrealistic for the financial market domain.\n\nHaving some kind of theoretical analysis that could support the assumptions and approaches would be appreciated. I further doubt the soundness of the cooperative part due to the following example: \n- Given a Prisoner's dilemma with a cooperation reward of $R = -1$, a defection reward of $P = -2$, a sucker\u2019s reward of $S = -3$, and a temptation reward of $T = 0$.\n- According to the paper's definition, a cooperative agent would aim for $S = -3$ due to the reward having the highest absolute value.\n- In this case, a \"cooperative\" player would allow itself to be exploited by the other player.\n- However, this would neither be optimal for the cooperative player itself (it would actually be the individual worst case with a reward of $S = -3$) nor for the whole system (the true optimum is mutual cooperation with a common reward of $R = -1$ per player).\n\nThus, I suppose that the method does not work for general self-interested games. Further analysis of suitable games would be helpful to assess the contribution of the paper.\n\n**Significance**\n\nI do not consider the concepts and results as particularly significant. The paper merely compares with 3 standard RL algorithms (A2C, PPO, DDPG). No multi-agent algorithm is used for comparison.\n\n**Minor Comments**\n- It would help in Section 4.2 to explicitly refer to Figure 1, otherwise a reader may not be aware of the meaning of S1, S2, etc. at this point.\n- The modified reward function can be defined with a simple equation as follows: $max(r, ceil(r) - floor(r))$"
            },
            "questions": {
                "value": "1. What does MAG-RL stand for in Figure 1?\n2. *\u201cThis opponent is generated by the CCRRL model based on the output of the previous step.\u201d* - How is this done? Does the opponent use the same model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698386191380,
        "cdate": 1698386191380,
        "tmdate": 1699636961632,
        "mdate": 1699636961632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FZtB9zmfyh",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_vSqN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_vSqN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes multi-agent RL framework for robust decision-making for portfolio optimization in financial institutions.  The paper uses a DDPG framework to take action and then consider how an adversary or cooperative multi-agent RL framework can address the challenges."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper seeks to use multi-agent framework to take robust action. Finding robust action is important in MDP setup. The algorithm seems to outperform the baseline algorithms in terms of reducing the risk."
            },
            "weaknesses": {
                "value": "1. The paper is not well-written as a lot of ideas have not been well-explained. For example, what is the reward? Why one has to use a modification function? Why is the adversarial action taken like in S1 and S2? Why is the cooperative action taken like in S3, and S4? \n\n2. How each of the S1, S2, S3, and S4 are appended with the action?\n\n3. The paper does not provide any theoretical result."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698510896386,
        "cdate": 1698510896386,
        "tmdate": 1699636961506,
        "mdate": 1699636961506,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yy8XD0w0x5",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_AQy4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_AQy4"
        ],
        "content": {
            "summary": {
                "value": "The paper extends current DDPG method for Portfolio Optimization method with a modified action space. The aim for processing the actions is for more robust actor performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is written with good clarity on the domain and related works."
            },
            "weaknesses": {
                "value": "The definition of the task/environment is vague. The proposed action generation process is not considered part of the intrinsics to the agent, but not part of the environment as well. Therefore, the problem itself is no longer a RL problem, a further definitive narrative is needed to justify usage of RL methods. It is also confusing, since the manuscript mentions that DDPG generates reward, but in fact I do not think that is the case. The action is also not defined clearly.\nThe authors mention that this is a work on multi-agent learning, or that the problem is a multi-agent environment. However, I fail to see how this formulation manifest in the problem or in the proposed method. I view the action generation process an additional function on top of learned actions, which is not theoretically justified in any way."
            },
            "questions": {
                "value": "How would you argue the correctness of the method? How would you show its convergence in theory?\nWhat is the motivation for the action modifications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637118155,
        "cdate": 1698637118155,
        "tmdate": 1699636961407,
        "mdate": 1699636961407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2CCEOukGbu",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_3VrD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_3VrD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a robust RL algorithm for portfolio optimization. The authors adopt DDPG with action perturbation by four kinds of cooperation or competitive strategies. The consequent method can have better performance than regular RL algorithms like PPO, A2C, and DDPG."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors provide a method to enhance performance of RL policies for portfolio optimization.\n2. The authors propose four different strategies for the robust purpose of non-stationary environments."
            },
            "weaknesses": {
                "value": "1. A majority of this paper is not well-written. The authors should revise both the format and the content of this paper. \n2. The proposed method seems not novel and does not show a superior performance compared to basic RL baselines. \n3. The experiments may be insufficient to show the method's effectiveness and robustness.\n\nSome minor errors in the paper:\n1. Formula in Figure 1 are blurry.\n2. Notations like $reward$ to represent the reward in previous step is wired."
            },
            "questions": {
                "value": "1. How does the mathematical program correlate with the RL part? For instance, how can a DDPG algorithm optimize the problem in Equation (1)-(4), including the constraints?\n2. In Equation (3), why do we include the constraint on the expected return, given the fact that it should be maximized?\n3. In Equation (4), how can a RL algorithm constrain the variance?\n4. What is the form of the `modify` function?\n5. How does the proposed method, Robust Reinforcement Learning via Multi-Agent System (CCRRL), correlate with the multi-agent system? The developed strategies are not agent-level but permutation along the action domain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7844/Reviewer_3VrD"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652865172,
        "cdate": 1698652865172,
        "tmdate": 1699636961278,
        "mdate": 1699636961278,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iTjNrC7702",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_T3xy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_T3xy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a portfolio optimization model based on reinforcement learning and introduces competition and cooperation strategies via multi-agent games to enhance the robustness of the model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method incorporates both competition and cooperation strategies to enhance decision-making performance and adaptability. By formulating the portfolio management problem as a multi-agent environment, agents collaborate and jointly strive to achieve a common goal."
            },
            "weaknesses": {
                "value": "This paper in fact proposes a multi-agent reinforcement learning method for portfolio optimization problem. However, no MARL methods are discussed and compared with the proposed method.  Further, the proposed method only makes modifications on actions of agents based on rewards, which has no novelty."
            },
            "questions": {
                "value": "1. The author claims that they introduce both competition and cooperation strategies in the proposed method. However, this will lead to unstationary learning environment, which makes the method hard to be converged. In fact, most MARL methods work only for cooperative cases.\n2. In the experiment, no MARL methods are used as baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 7,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719476114,
        "cdate": 1698719476114,
        "tmdate": 1699636961125,
        "mdate": 1699636961125,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MpNFFC5m8G",
        "forum": "wdEHqQWTG4",
        "replyto": "wdEHqQWTG4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_uHcE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7844/Reviewer_uHcE"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a multi-agent reinforcement learning approach for portfolio optimization. The authors call their MARL approach \"robust\", and claim that it uses both cooperation and collaboration. The algorithm appears to build on top of DDPG. \n\nThe claims of the paper include that the system can \"mitigate the negative impact of the variations of the market\" and that it leads to a \"collective enhancements in the performance of RL algorithms\"."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Ambitious goal of optimizing the a stock portfolio."
            },
            "weaknesses": {
                "value": "* The paper does not clearly define what type of behaviors it expect to generate, and why would these behaviors be useful for the stock portfolio. Are they like brokers hired by the owner? What does it mean for two brokers of an owner to compete or to collaborate? Aren't they sharing all the information?\n* The figure 1 describing the main concept of the algorithm has only one agent. \n* It is not clear how the A2C, PPO and DDPG algorithms had been used in the comparison study, as only the original papers are cited."
            },
            "questions": {
                "value": "* Please clarify the claimed contributions of the paper in terms of measurable or provable quantities."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 8,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788365655,
        "cdate": 1698788365655,
        "tmdate": 1699636960976,
        "mdate": 1699636960976,
        "license": "CC BY 4.0",
        "version": 2
    }
]