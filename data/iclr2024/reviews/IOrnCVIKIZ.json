[
    {
        "id": "NBzQWeGZdd",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_MfcU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_MfcU"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces LETI, a novel approach to fine-tuning language models using binary and textual feedback -- feedback conditioned finetuning. The work primarily targets code generation tasks. It demonstrates that automatic textual feedback, such as error messages from a Python interpreter, significantly enhances LM performance. Compared to binary feedback only methods, LETI shows superior results in both the quality of generated code and in sample efficiency as evaluated on MBPP and HumanEval dataset. LETI does not require ground truth outputs, only unit tests."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "LETI addresses a practically relevant problem: The pattern of failure-guided iteration is frequently seen in coding assistants and universal chatbots like ChatGPT, making it a highly relevant use case. LETI's use of detailed textual feedback, such as error messages and stack traces from a Python interpreter, for fine-tuning is a significant strength. \n\nSample Efficiency in Training: LETI has shown to be more sample efficient compared to traditional fine-tuning methods. By leveraging textual feedback, LETI can achieve better performance with fewer examples and training iterations. This efficiency is particularly advantageous given the computational and data resources required for training LLMs."
            },
            "weaknesses": {
                "value": "The evaluation process could be improved. A direct comparison of the proposed approach to a PPO-based variant would strengthen the paper. This comparison could provide a clearer understanding of the strengths and limitations of the proposed method in the context of existing advanced techniques.\n\nLimited application to broader NLP tasks: While LETI shows promise in code generation tasks and some NLP tasks framed as code generation problems, such as Event Argument Extraction, its effectiveness in a broader range of NLP tasks remains uncertain. The approach's reliance on feedback akin to that found in programming environments may not translate well to tasks where feedback is less structured or where the notion of \"correctness\" is more subjective."
            },
            "questions": {
                "value": "Could you illustrate what the prompts look like for each inference round in the Figure 1? Specifically, does the feedback from iteration $n-1$ get incorporated into the prompt for the subsequent iteration $n$?\n\nAlgorithm 1, states that the 'bad' completions are excluded. However, would it be more accurate to state that these 'bad' solutions are only disregarded until they meet the criteria of 'good' on a future iteration? It might be more logical to define 'good' and 'bad' as dynamic terms, changing over iterations.\n\nThe observation that exclusively fine-tuning on 'bad' solutions yields subpar outcomes is not unexpected, considering there's no mechanism within the loss function to penalize such solutions.\n\nIt would be beneficial to conduct a comparison between the LETI FCFT approach and the PPO-based fine-tuning techniques. Using the Reinforcement Learning from Human Feedback (RLHF) approach with a binary reward system seems to be the most comparable method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Reviewer_MfcU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639954685,
        "cdate": 1698639954685,
        "tmdate": 1700630819164,
        "mdate": 1700630819164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VHN4LOp9Hu",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_9ZHv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_9ZHv"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach for learning interactively from environment feedback, without accessing the ground truths. Focusing on the task of code generation, the feedback is instantiated as the received error messages from a compiler, or a binary feedback indicating whether or not the generated code passes the given test assertions. The proposed learning algorithm, named LETI, fine-tunes a language model (LM) to generate the task input/output and the feedback. Experimental results showed that LETI can improve the LM to even outperform the supervised fine-tuned baseline when the LM has a larger size, and that a similar idea also applies to the event extraction task (though ground-truth annotations are needed)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper studied an interesting problem of learning interactively from environmental feedback, without needing ground-truth annotations. \n2. The experiments are generally solid and comprehensive. When it is applied to a 2B CodeGen LM, LETI improves the LM to even outperform the traditional, human-annotation-required, fine-tuned baseline. This is then supplied by additional analyses confirming the advantage (performance and sample efficiency) of textual feedback compared with using only the binary execution feedback. \n3. The paper is well written. Most details are well clarified."
            },
            "weaknesses": {
                "value": "1. I don't see any significant weaknesses in the proposed approach, but there are a few questions that I would like to have the authors' clarification. See Questions.\n2. Missing references. There have been many more works about \"using feedback to improve code generation\", e.g., the following and their follow-ups or referred papers.\n- Elgohary, A., Hosseini, S., & Awadallah, A. H. (2020, July). Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 2065-2077).\n- Yao, Z., Tang, Y., Yih, W. T., Sun, H., & Su, Y. (2020, November). An Imitation Game for Learning Semantic Parsers from User Interaction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 6883-6902)."
            },
            "questions": {
                "value": "1. As mentioned in Section 2.1, LETI assumes a code pre-trained LM which can give a decent, initial performance on code generation. I wonder if this is also a reason for the smaller LM benefiting less from LETI (they may be too weak initially)? It can be helpful if the authors could provide the initial good vs. bad instances distribution in each LM's training set.\n2. Experiment in Table 5:\n- Why PAL for GSM8K but CoT for BBH?\n- Could you also show the \\delta_{PAL-direct} amount on GSM8K, or is there a reason for not including this result?\n- The main experiments say that relatively larger LMs can benefit more from LETI, but in Table 5 LETI shows to hurt the GSM8K performance for 2B (40.03 -> 38.97) while aid for 350M (13.04 -> 16.68). Is this saying that LETI impairs an LM's CoT reasoning performance? \n3. The event argument extraction experiment: Can the authors provide results for the supervised, fine-tuned baseline?\nAs LETI in this application assumes ground-truth annotations, the supervised baseline is necessary for justifying its advantage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815771213,
        "cdate": 1698815771213,
        "tmdate": 1699636353632,
        "mdate": 1699636353632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4rBEoH7s6",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_kgWB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_kgWB"
        ],
        "content": {
            "summary": {
                "value": "LETI is a new language model fine-tuning paradigm that aims to improve LMs' capabilities by learning from nuanced textual interactions with their environment. The authors propose this method for code generation tasks, where the model generates code based on natural language instructions. LETI uses feedback from a Python interpreter to iteratively fine-tune the model on a concatenation of natural language instructions, LM-generated programs, and textual feedback. The authors demonstrate that LETI significantly improves LM's performance on the code generation dataset MBPP without using any ground truth code. Moreover, LETI generalizes to other datasets, such as HumanEval, when trained on MBPP. The authors also find that textual feedback leads to improved generation quality and sample efficiency compared to binary feedback. LETI can be applied to natural language tasks when they can be formulated as code generation problems, which is empirically demonstrated on event argument extraction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper present comprehensive experiments and evaluations on different datasets and tasks and is well-written and clearly structured.\n2. The proposed LETI paradigm holds potential for improving LM's capabilities in various tasks. Its ability to leverage textual feedback for better generation quality and sample efficiency highlights its practical significance, and its successful application to both programming language and natural language tasks suggests that this paradigm can be extended to other domains, making it an impactful contribution to the field of language model fine-tuning."
            },
            "weaknesses": {
                "value": "1. Evaluation on larger models is needed to provide insights into its scalability and effectiveness.\n2. Investigating the impact of different solution evaluator designs on LETI's performance would be informative, as biases may be introduced when optimizing towards certain metrics.\n3. Evaluating LETI's effectiveness in other domains and tasks would further validate its generalizability.\n4. Comparing LETI with other RL-based approaches that leverage rewards or value functions would help establish the advantages of using textual feedback."
            },
            "questions": {
                "value": "1. How would LETI perform on larger LM (e.g., 6B or 16B)? Can you provide any insights or expectations on the scalability and effectiveness of LETI when applied to these larger models?\n2. Since the performance of LETI relies on the solution evaluator's implementation, could you elaborate on the potential biases that may arise from different evaluator designs? How might these biases impact the performance of LETI in optimizing towards certain metrics?\n3. Would it be possible to provide more examples of how LETI can be applied to other domains and tasks? This would help to understand the generalizability of the proposed method across a wider range of applications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818231426,
        "cdate": 1698818231426,
        "tmdate": 1699636353537,
        "mdate": 1699636353537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fb3mItHFFe",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_KLdC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_KLdC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed LETI, a method for improving LLMs by concatenating binary labels and textual feedback with the task I/Os and using it as a dataset for further finetuning. The data collection process for LETI is iterative and automatic, relying only on the test cases and executors (e.g., Python interpreter) and not gold solutions. Experiments on MBPP (in-domain) and HumanEval (out-of-domain transfer) show that incorporating textual feedback is key for improvements yielded by LETI. Extensions beyond code generation tasks are also made to show the generality of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1: The training data can be collected automatically (i.e., no human-in-loop) in an iterative manner without the need for gold-standard solutions, which in principle can easily scale to larger datasets and problem sets. I think such bootstrapping methods are important for further improving LLMs given their data-hungry nature;  \nS2: The way to construct each training example is quite interesting. Instead of training the models to predict the error message from buggy programs, LETI reverses the order and puts the binary label and textual feedback before the problem instruction and candidate program to learn their association;  \nS3: A number of ablation studies are presented to show the factors that affect the performance of LETI. I especially like the discussions about the benefits of textual feedback and model sizes;  \nS4: Though code generation is the main task for LETI, it was also shown to be applicable to non-code tasks."
            },
            "weaknesses": {
                "value": "W1: The major weakness of this work is the soundness of the experiments. More specifically:\n* W1.1: Using *TheStack* as part of the \"pretraining dataset\". As the authors noted in footnote 4, CodeGen-mono is trained on BigPython (actually it was first trained on the Pile and then BigQuery, then BigPython), and TheStack might contain a substantial amount of code that CodeGen-mono models have never seen before. This could contribute to the performance improvements that are perceived as the effect of \"regularization\". Note that there are models that are trained on TheStack (e.g., starcoder series), but I do understand that changing the models completely is not feasible. Nevertheless, adding such ablation (i.e., simply further finetune on TheStack w/o any of the proposed methods) in the main results is important.\n* W1.2: Comparing results *w/o* post-processing. Note that the post-processing is mostly fixing formatting errors (e.g., semantically wrong solutions won't get corrected just by post-processing), and since $x \\bigoplus y$ is a substring of the finetuning data, it is very much expected that it will follow the formatting much better after finetuning. I think comparing the results w/o post-processing will mix the formatting and semantic errors, making it harder to measure the actual improvements from the proposed method. I would suggest showing all the major results *w/ post-processing* (e.g., Figure 2). \n\nW2: One other concern about this work is the cost. If I understand it correctly, for each iteration, LETI needs to perform sampling and finetuning. And from Figure 2, it seems that the majority of the improvements happen after the first 2 iterations. \n\nI hope the authors could clarify these in the discussion period, I am open to adjusting my scores if such concerns are addressed."
            },
            "questions": {
                "value": "Q1. The models might have already seen such textual feedback (i.e., from Python interpreter) during pretraining, for example from StackOverflow sites, have you tried to apply the iterative method during prompting? Maybe it can be another baseline to be compared with?  \nQ2. Despite the fact that LETI doesn't need any gold program, can you comment on the computation cost between LETI and simple finetuning (w/ gold program)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699420455327,
        "cdate": 1699420455327,
        "tmdate": 1699636353462,
        "mdate": 1699636353462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "a8KSs2aq0r",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_bsvm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_bsvm"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes LETI, which leverages code execution output (stack traces, error messages) in an iterative refinement method to improve a code generation model\u2019s ability to produce correct solutions, and reduce buggy solutions. For a fixed set of epochs, they iteratively finetune the model with a concatenation of code problem prompt, textual feedback (code execution output), and binary feedback (whether execution of generation was successful or not) after an initial code generation, a method they call FCFT (feedback-conditioned finetuning). The proposed method does not require ground truths or instruction-code paired data for finetuning, using just an executor to generate textual feedback. To mitigate distribution shifts due to this finetuning, they interleave the LM objective with the FCFT objective as continued pretraining, doing LM objective optimization on the pretraining data (as regularization), and FCFT on the LETI data.\n\nThey conduct FCFT on CodeGen 250M and 2B variants, and trained on MBPP. They demonstrate the usefulness/sample efficiency of text feedback (as opposed to just binary execution feedback) via ablations, and provide results to show how LETI improves generation and sample efficiency over the base CodeGen models for MBPP test set, and also generalization to unseen HumanEval eval set. They test on BIG-Bench-Hard to show that LETI does not regress on base LM performance in general reasoning tasks. Finally, they show that LETI can also be applied to tasks such as event argument extraction and math problem solving, provided the task can be reformulated as a code generation task, and there is an evaluator to provide execution feedback."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper is well motivated, and focuses on how iterative refinement is an important aspect of producing high-quality, correct code solutions. Improving via execution feedback is very interesting and this paper proposes a detailed solution for this.\n\n2) The paper shows relevant gains of LETI on the MBPP test set, over a baseline pretrained model, breaking down the different ways in which it corrects errors such as SyntaxError, NameErrors.\n \n3) The improvement in sample efficiency using textual feedback vs just binary feedback is demonstrated clearly, with a per-iteration average improvement as evidence.\n\n4) The paper provides a good array of evaluations and analysis numbers/plots to demonstrate the performance of LETI, measuring regression (if any) in base performance, generalization performance, breakdown of error types and ablations. Also, the paper has qualitative examples and useful diagrams which makes it clear to understand and follow."
            },
            "weaknesses": {
                "value": "1) Results for generalization and robustness do not back the claims adequately. \n\n    a. In Table 2, The improvement over HumanEval is mixed for pass@1, with the pretrained model doing better than LETI for 2B. This could be attributed to error (given the small size of HumanEval), so averaging over different runs/seeds would be preferable. Also, if this could be tried on Spider/other code generation tasks this case could be made better. \n\n    b. In Table 5, the pretrained 2B model performs better than LETI 2B without postprocessing, and regression is observed for BigBench-Hard on the pretrained 2B baseline, weakening the stance that base reasoning performance does not regress.\n\n2) LETI hasn\u2019t been tested with larger available CodeGen models (6B, 16B), and given the mixed generalization performance, it becomes difficult to determine whether performance results hold consistently (given that HumanEval results are mixed between 350M and 2B). \n\n3) Results haven\u2019t been compared against strong baselines. For example, methods like [Self-Refine](https://arxiv.org/pdf/2303.17651.pdf) (cited in the paper but not compared), [LEVER](https://openreview.net/pdf?id=Gj3zN9zs4v), and [Self-Debugging](https://arxiv.org/pdf/2304.05128.pdf) are relevant methods which use self-refinement and execution-based training respectively, and it is difficult to gauge how FCFT compares and whether it is beneficial to do it over those methods, given a fixed compute budget.\n\n4) Overall, the main concern is the strength of experiment results. Given that each code generation model needs to be trained via FCFT in the continued pretraining manner, there needs to be more comparison to other recent refinement and execution feedback techniques to illustrate that doing FCFT has a clear performance benefit or is computationally efficient."
            },
            "questions": {
                "value": "1. In table 2 and table 5, are the pretrained model performances on HumanEval measured with or without postprocessing? I believe a more apples-to-apples comparison would be between Pretrained model w/ postprocessing vs LETI w/o postprocessing. Given that LETI seems to learn how to clean up syntax errors, it seems a more fair comparison. \n\n2. Table 2: Pretrained 2B seems to be better than LETI on pass@1, is there an explanation for why this is so? Results look a bit mixed here. Given the small size of HumanEval, the delta for LETI (350M) is roughly 1 problem. Has this been run over multiple seeds, ensured that this difference is not due to error? Also, given this, would LETI consistently hold over larger model sizes >2B? \n\n3. Although MBPP and HumanEval are different datasets, they focus on similar styles of problems, what about ability to generalize this to other code generation tasks with execution options, like Spider for Text2SQL?\n\n3. This recent paper [LEVER: Learning to Verify Language-to-Code Generation with Execution](https://openreview.net/pdf?id=Gj3zN9zs4v) does a reranking of candidates based on probability that output is correct. A reranker/scoring model is trained for this, and this can be combined with any LM, rather than finetuning a specific LM. Could you comment on the benefits of your method vs this?\n\n4. Any reason as to why \u201cAssertionErrors\u201d and \u201cOther Errors\u201d go slightly up with postprocessing (LETI vs LETI w/postprocessing in Table 1)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Reviewer_bsvm",
                    "ICLR.cc/2024/Conference/Submission3930/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699485547538,
        "cdate": 1699485547538,
        "tmdate": 1700717483152,
        "mdate": 1700717483152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QKbrbifneA",
        "forum": "IOrnCVIKIZ",
        "replyto": "IOrnCVIKIZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_WAJY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3930/Reviewer_WAJY"
        ],
        "content": {
            "summary": {
                "value": "LeTI finetunes models with additional textual feedback which might be optional - e.g. for code models, failures can generate error messages which can be used as a learning feedback into the model iteratively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Using execution environments' error messages to provide a good/bad feedback split to fine tune is a good strategy and uses a ready resource available as a benefit to code gen models. Such meta data can be further used as future work to assess severity of errors in output which can be used to differentiate multiple outputs from different temperature settings. Any solution which helps bring smaller models up to par with larger models through smarter training is especially beneficial given the resource constraints in inference pathways."
            },
            "weaknesses": {
                "value": "I would love to see more programming languages handled in this work; it feels very narrowly defined using Python problems and runs the risk of the specific interpreter's error generation capabilities overfitting the solution which might not replicate as we move to other languages or run time environments.\n\nExecution oriented solutions further are heavily dependent on run time environment and specific to it. Otherwise you are limited to syntactic checkers or basic execution approaches which might leave out a lot of errors. Also error messages or stack traces might sometimes be valid in generated code for improper prompts so just their presence is not sufficient to call the generated code as a fail. This is especially true for snippets of code being generated as part of the interactive development scenario which has been most successful for code gen so far."
            },
            "questions": {
                "value": "- What's the difference between FCFT and RLAF (Reinforcement learning with automated feedback)\n- Did you try the solution on other languages than Python? (CodeGen model supports multiple)?\n- Did you try on different versions of python interpreter to see if differing error outputs have implications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Code models can have significant biases since the pre-training data can to a large proportion focus on few majority viewpoints. Since the improvement here is based on code execution environment's feedback which presumably is based on execution correctness of the program, the work ought not to worsen any such biases."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3930/Reviewer_WAJY"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3930/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699747368847,
        "cdate": 1699747368847,
        "tmdate": 1700715904699,
        "mdate": 1700715904699,
        "license": "CC BY 4.0",
        "version": 2
    }
]