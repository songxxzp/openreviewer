[
    {
        "id": "BGmDO21wDA",
        "forum": "R6AA1NZhLd",
        "replyto": "R6AA1NZhLd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_73Ff"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_73Ff"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to include topographic constraints to the Transformer model by using spatial querying and spatial reweighting. This resulted in \u201cTopoformers\u201d The authors demonstrate their methods both in simple examples and in BERT architecture on large language corpora.  They found that the Topoformers performed slightly worse than the standard Transformers, but they allowed better interpretability. Furthermore, the authors compared the emerging topographic organization of the language areas obtained from the fMRI experiments with that from the model, and they reported similarity between the two. Overall, I find the ideas to be interesting and I enjoy reading the paper. However, the explanation and quantification of the fMRI results are not clear. It was difficult for me to judge the strength of the results. I will explain in more details below."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Most parts of the paper were very well written. The motivation and the computational approaches were nicely explained.\n\nThe idea of incorporating \u201cconnectivity constraints\u201d to transformer language models seems to be novel, despite recent work on including such constraints to deep network models in vision. \n\nThe ideas behind  the modeling approach are well explained and the results are promising.\n\nThe paper combines modeling and empirical studies based on fMRI experiments."
            },
            "weaknesses": {
                "value": "The main weakness I see is that the approach to link the topological structure of the fMRI data to that of the Topoformer model was not well explained and is questionable. These include Section 3.3, Fig 1B, and Fig 6. Also see my questions below."
            },
            "questions": {
                "value": "\u2014 What does Fig 1b plot exactly? What does the different colors represent?The description in the caption was insufficient for understanding what\u2019s really going on.  Same questions apply for Fig 6. \n\n\u2014 How to interpret the statistics in Fig 4? The calculation of the Typograph statistic is based on Equation 4, but there is a lack of explanation regarding how to interpret it and why this is a good statistics to use. \n\n\u2014 In section 3.3., the authors stated \u201cwe then visualized the weights in the cortical MNI surface space (Gao et al., 2015a)\u201d. What doe the \u201cweights\u201d mean here?\n\n\u2014 when connecting the data with the model, the authors say \u201cBecause we have already shown the spatial smoothness of these components, if we see smoothness in the correlation of these components with voxels/units in the target space, we can infer that there is a correspondence between the two topographies.\u201d This is not obvious to me. I would appreciate if the authors could unpack the arguments. Furthermore, how to quantify the strength of the correspondence?\n\n\u2014 The authors performed a control analysis in Appendix A 6.1, i.e. \u201cWe sanity-checked that a control model did not show any topographic correspondence with the brain organization (see Appendix A.6.1). \u201d But I don\u2019t understand why this is the appropriate control because Fig 9 is not about the mapping between the PCs and single units for the control model & brain response. I\u2019d think that one would need a figure analogous to Fig 5, i.e., to show the Brain-PC correlation map under the control model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693600829,
        "cdate": 1698693600829,
        "tmdate": 1699636995585,
        "mdate": 1699636995585,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r3bA66bDIs",
        "forum": "R6AA1NZhLd",
        "replyto": "R6AA1NZhLd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_S9Nw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_S9Nw"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a method for integrating topographic organization into the keys, queries, values, and outputs of the self attention mechanism of a transformer. They validate that this indeed yields topographic organization on the simple IMDB task. When their method is applied to language modeling (BERT) the authors identify topographic organization with respect to semantic clusters of topics (music / sport). They then measure the topographic organization of human subjects (through FMRI) with respect to language, and compare these results to the topographic organization of their Topoformer-BERT model through correlation analysis. They show that there is indeed strong spatial correlation between their model and the human FMRI data, indicating some similarity of topographic maps. Finally, they show that this topographic organization does not significantly hurt the performance of their Topoformers compared to non-topographic baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novelty \u2014 this is the first model which integrate topographic organization into a transformer (to the best of my knowledge) and furthermore one of the first to do so for language. \n- The comparison with human FMRI data adds a important degree of grounding to the paper which only adds to it\u2019s impact.\n- The quality of the writing in the paper is very high in general, and specifically the introduction is very succinctly written and elegantly describes topographic models.\n- The use of a topographic metric to quantify topographic organization across multiple layers in an easily digestible way is a welcome finding in the field. \n- Topographic organization in artificial neural networks is an important but understudied topic, and I believe this paper will draw important attention to in the future. Furthermore, I believe this paper has the potential to become a foundational paper in the field given it\u2019s application to transformers and language modeling at this crucial point in time."
            },
            "weaknesses": {
                "value": "- The appendix is poorly formatted and appears almost incomplete. There are equations (such as (4)) which are important to the text but are left with undefined symbols. Furthermore, some figures (such as the Brain-PC correlation with a non-topographic control) appear to be missing (or is this Figure 9?).\n- No baseline for IMDB accuracy performance without topographic organization.\n- Only a single attention head is used on all experiments, limited the comparison with most state of the art transformer architectures. (To be clear, I think the Topoformer-BERT also only uses a single head but could not find this in the text).\n- Too many important aspects are relegated to the Appendix making the text challenging to read. The authors should improve the formatting of the appendix (ensure headings for sub-sections are in the correct location) and potentially move some figures (such as Figure 9, and equation 4) to the main text. \n- Given the authors mention that they compared many spatial receptive field sizes, it would be nice (and make sense) if they included these results in the appendix. Furthermore, it would be helpful to understand how the authors ultimately settled on their final choices of RF sizes as this is not described in the text.\n- The topographic organization for the values and fc_out appears quite weak (despite the topographic metric). This makes one either question the topographic metric, or question the usefulness of this model for organizing the actual output of self-attention. It would be helpful if the authors could comment on this. (To be clear, despite this weak organization of values, I still think there is significant value in the strong organization of keys and queries and therefore do not think this should be held strongly against the quality of this paper). \n- It would be helpful if the authors included additional controls comparing non-topographic models with brain PCs for the correlation plots of Section 3.3. Without these, the strengths of the conclusions drawn from this correlation analysis are significantly lowered."
            },
            "questions": {
                "value": "- Is there any intuition why in Figure 1, you see that the Keys and Queries show strong selectivity for negative sentiment but not positive? While the values appear to show only very strong selectivity (very little intermediary), but for both classes? Is this somehow a result of the different mechanisms used to induce topographic organization? (Bilinear vs. Matrix-vector product?). Similarly, is there a reason why these components appear to have patchier organization in the BERT setting as welll? \n- In the appendix you say SQR was less stable and required larger batch size, do you have intuition for why this might be? \n- Would it be possible to again compute the topography metric (Equation 4) on the correlation maps between the model and Brain PCs? Would this make sense as another quantative metric of the alignment of the two representations?\n- Do the PCs found for the human participants correspond to any semantic categories of the text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8052/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8052/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8052/Reviewer_S9Nw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781848672,
        "cdate": 1698781848672,
        "tmdate": 1699636995460,
        "mdate": 1699636995460,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R4pgWM8gu0",
        "forum": "R6AA1NZhLd",
        "replyto": "R6AA1NZhLd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_DNiv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_DNiv"
        ],
        "content": {
            "summary": {
                "value": "Paper 2 proposes a novel approach to training Transformer language models with topographic organization, called Topoformer. The key idea of Topoformer is to arrange the keys and queries of the self-attention mechanism on a 2D grid, and to associate local pools of queries with a given key. This allows Topoformer to learn topographic representations of language, which are more interpretable and efficient than the unstructured representations learned by traditional Transformer models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method, Topoformer, is a novel approach to training Transformer language models with topographic organization.\nTopoformer has been shown to be feasible on a 1-layer sentiment classification task and to perform on par with a non-topographic control architecture on downstream NLP benchmarks.\nTopoformer has also been shown to yield similar forms of topographic organization for linguistic information as that present in the language network of individual subjects."
            },
            "weaknesses": {
                "value": "The paper does not provide any concrete examples of how Topoformers can be used to improve the interpretability of NLP models.\nThe paper does not evaluate Topoformer on a variety of different NLP tasks."
            },
            "questions": {
                "value": "How can Topoformers be used to improve the interpretability of NLP models?\nWill this Topoformer increase the performance of the donw-steam task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791697447,
        "cdate": 1698791697447,
        "tmdate": 1699636995335,
        "mdate": 1699636995335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r99cBQByYU",
        "forum": "R6AA1NZhLd",
        "replyto": "R6AA1NZhLd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_P2Yw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_P2Yw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Topoformer, a transformer model, here applied to language, that imposes spatial structure on the latent dimensions involved in the attention process. It uses a binary matrix encoding spatial proximity that is placed between the K and Q matrices that create the attention matrix. Further, local connectivity is used after the application of the V matrix.\n\nThis transformer is trained first using a 1-layer version on Imdb, later a large BERT version.\n\nTransformer activations are inspected and found to have spatial structure.\n\nA relationship is drawn between this spatial structure and that found in and fMRI language experiment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces the idea of spatial structure in the attention mechanism of a transformer, devises a method to achieve it, and shows that the spatial structure indeed appears."
            },
            "weaknesses": {
                "value": "This paper is unfortunately quite badly written, but thankfully many aspects can be improved straightforwardly.\n\n1. The abstract and intro contain falsehoods and non sequiturs and would largely benefit from being made more concise.\nExample of falsehood: Abstract, sentence 2. Convnets exhibit and exploit spatial structure\nExample of non sequitur: Intro, paragraph 3. \"Despite the success of these LMs, the fact that their architecture is\nnot compatible with spatial constraints of the biological cortex is a fundamental limitation for the\ngrowing research enterprise that uses LMs as models of human intelligence\" the one simply does not follow from the other, on any level, especially without concrete evidence.\n\nThe paper would largely benefit from having such problematic statements removed, since they contribute to the impression that the authors might want to pull the wool over the reader's eyes.\n\n2. Variables used in formulas are not introduced and formulas are hard to find and not explained. See, e.g. the first math items in 2.1. Further, see reference to a non-existent \"Equation 4\" (which may correspond to Eq4 found in the appendix, but the reader can't know this, because it is possible that enumeration restarts. On top of this, the equation from the appendix is stated without any context or description of what is what).\n\n3. The link to brain organization is highly suggested or strongly stated along the paper but is unfortunately very tenuous. Examples are Fig 1, where learned spatial attention structure is juxtaposed with some brain flatmaps and the term \"brain-like\" is used. There are so many possible reasons for smooth variation of brain data - for starters there is the smoothing employed in the pre-processing, the mapping to MNI space and the subsequent mapping to the cortical surface. Functional brain regions do exist, but absent individual localizers it is even unclear whether the cropped regions are correct. It would be good to give a map showing any form of predictive power of any language model on the selected voxels. As an aside, it would have been great to have an idea on a global surface or a 3D brain of where this region actually lies. Also the figure caption says \"Brain responses are visualized\", while the diagram mentions PCs, which are not brain responses, but summaries of them.\n\n\nRelated to both 1 and 3, the statement \"Because we have already shown the spatial smoothness of these components, if we see smoothness in the correlation of these components with voxels/units in the target space, we can infer that there is a\ncorrespondence between the two topographies.\" is incorrect. One would see patterns, and smoothness in the correlation of the topoformer pixels with many things, possibly even certain noise vectors (though with lower correlation values), but very likely with brain data from other regions. The latter would be a good test. It would be very useful to see the correlation drop when using e.g. data from a visual area or a motor area.\nThe currently important thing to read from the diagram is the relatively high correlation value which shows that there is a correspondence between the transformer activations in brain activity. However, this is also true for unstructured transformers.\n\nAll in all, it would be very helpful if the authors backed their statements up better with actual analyses and put sharp and concise statements along with proof if not an established fact.\n\nIt is worth considering removing the brain analysis part and focusing on more detailed interpretability of the transformer model. E.g. why is the focus on the final layer of the topoformer model? What do the other ones look like? Even if they exhibit the spatial property less, this would be interesting. A comprehensive study would have been more helpful instead of a link to brain activity that does not strongly corroborate similar topological information, but that only shows what several papers have already shown - a correspondence of these models to brain activity."
            },
            "questions": {
                "value": "Why is locality enforced using a matrix M that forces averaging of the adjacent keys and queries? This a priori does not actually have to enforce similarity between the points that are connected, but it does seem that it is enough pressure to make the locations become similar to their average.\nA different approach would be to simply constrain each location to only be able to use information from a neighborhood. Was this approach considered at all?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819929063,
        "cdate": 1698819929063,
        "tmdate": 1699636995186,
        "mdate": 1699636995186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s3i0Ao5Eri",
        "forum": "R6AA1NZhLd",
        "replyto": "R6AA1NZhLd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_LDRr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8052/Reviewer_LDRr"
        ],
        "content": {
            "summary": {
                "value": "Biological brains feature spatial organization in neuron arrangement. However, the representations of current machine learning models lack such organization, pose challenges in interpretation. This study introduces a novel approach called Spatial Querying, and Spatial Reweighting, which results in interpretable topographic organization. The primary contributions can be summarized as follows: Introduced a Topoformer with topographic organization and demonstrated this organization through a 1-layer Topoformer in sentiment analysis task. Subsequently, scaling this proposed method to BERT, achieving competitive results in NLP benchmarks. Further experimental studies reveal that Topoformers exhibit similar linguistic organization while analyzing human brain activity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tAddressed the architectural disparity between current Transformer models and the biological brain by inducing a topographic organization of features within the Transformer.\n2.\tThe topographic organization of the Topoformer yields competitive performance compared to the Vanilla Transformer model in small-scale sentiment analysis and benchmark GLUE tasks.\n3.\tThe proposed method is scalable for both small and larger-scale datasets.\n4.\tThe alignment between the way information is organized in the Topoformer and the human language network is clearly shown using brain dataset."
            },
            "weaknesses": {
                "value": "1.\tAlthough the novelty of the paper is interesting, but it lacks specific experimental details.\no\tHow to choose the optimal number of tokens in local spatial querying?\no\tAdditionally, with the introduction of local pooling of spatial queries, the parameter differences between Topoformer and Vanilla BERT is not provided.\no\tHow did the authors generate Fig 4? What does \"Stat Value\" refer to? \nHow do we determine the Stat Value across layers for queries, keys, values, and fc_out?\no\tWhat does fc_out refer to? It is not defined or referenced anywhere in the entire paper except in Fig 4.\no\tTypically, a BERT-base model comprises 12 encoder layers. However, in Fig 4, there are 15 layers depicted. Could the authors provide an explanation as to why there are 15 layers in this context?\n2.\tThe validation of the proposed Topographic Transformer model appears insufficient. Similar to BERTology studies, has the proposed Topoformer been assessed for its ability to capture the hierarchy of linguistic structure (such as early layers capturing surface features, intermediate layers capturing syntax, and later layers representing semantics)?\n3.\tWhy does the Topoformer with a single head result in better accuracy scores on the GLUE benchmark compared to the multihead attention?\n4.\tWhat is the complexity of self-attention in Topoformer after introducing the local spatial querying mechanism? Did the authors maintain the same local spatial querying across layers, or did they increase the local pool size with the depth of the layer?\n5.\tthe clarity can be improved:\n6.\tseveral typos: In Fig4: fc_out, rest of the paper: fc-out, FC-Out"
            },
            "questions": {
                "value": "1. What is the representational similarity between Topoformer and Vanilla BERT across layers? During fine-tuning of Topoformer, similar to fine-tuned BERT, are the last layers significantly affected?\n2. Please check weaknesses for the remaining questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699109683057,
        "cdate": 1699109683057,
        "tmdate": 1699636995060,
        "mdate": 1699636995060,
        "license": "CC BY 4.0",
        "version": 2
    }
]