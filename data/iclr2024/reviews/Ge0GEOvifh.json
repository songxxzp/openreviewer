[
    {
        "id": "oPN3ei7AWc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_2rqV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_2rqV"
        ],
        "forum": "Ge0GEOvifh",
        "replyto": "Ge0GEOvifh",
        "content": {
            "summary": {
                "value": "This paper studies the defence against data poisoning and backdoor attacks on Contrastive Language-Image Pre-training (CLIP). It proposed a method called SafeCLIP based on the observation of the following: 1) the backdoored image-text pairs are learned first. 2) unimodal learning can break the backdoor connection between image and text pairs. The method first performs unimodal learning using the NNCLR method, then splits the data into safe and risky subsets. The CLIP is performed on the safe set, while the unimodal learning is performed on the risky set. The results show that SafeCLIP is effective against existing attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The observation of using a small learning rate with CLIP for an epoch can show a distinctive difference between poison and clean pairs, which is new and interesting. \n- Using NNCLR to cluster backdoor images is interesting and technically sound for \"warming up\" the model against a visible patch as the trigger."
            },
            "weaknesses": {
                "value": "- The NNCLR as unimodal training might not effectively cluster other (non-patch-like) backdoor triggers, and it will be more convincing if the proposed method could demonstrate effectiveness on these triggers. Such as DFST [1], smoothed frequency domain trigger [2], or invisible trigger [3]. \n- The proposed method has a few components, whereas, in practice, it might not be that easy to get all hyperparameters in the suitable range. For example, in the experiments, the proposed method sample 1M out of CC3M and use 1 epoch with a learning rate for warmup. What if using all 3M? or on the CC12M dataset? Or even a large dataset? Is 1 epoch still apporiate? One might use the number of optimization steps as the threshold. However, it is not guaranteed it will encounter sufficient backdoor pairs within the given threshold. I don't think this metric is a robust and general way to separate the safe set and risky set. \n- The \"safe\" incremental $i$% might be different as well for larger datasets. \n- Section 4.1.1, \"... to poison an image of a cat with a plane caption, the image ...\", seems to be true for the dirty-label attack. What happens if it is a label-consistent attack? \n\n\n[1] Cheng, Siyuan, et al. \"Deep feature space trojan attack of neural networks by controlled detoxification.\" AAAI 2021.\\\n[2] Zeng, Yi, et al\" \"Rethinking the backdoor attacks' triggers: A frequency perspective.\" ICCV. 2021.\\\n[3] Li, Yuezun, et al. \"Invisibl\" backdoor attack with saattacks'cific triggers.\" ICCV 2021."
            },
            "questions": {
                "value": "- There seems to be a gap between the reported zero-shot accuracy on clean data and the reported by CleanCLIP for ImageNet. The results show 9.87% where, as in CleanCLIP, it is near 20\\%. Is there a reason for this difference? \n- It could be better to report the area under the ROC curve and precision-recall curve to distinguish the poison and clean pairs, e.g. the scenario in Fig 2. (b)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697253423478,
        "cdate": 1697253423478,
        "tmdate": 1699636938783,
        "mdate": 1699636938783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hk6LkcH0dy",
        "forum": "Ge0GEOvifh",
        "replyto": "Ge0GEOvifh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_MgX1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_MgX1"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel defense against data poisoning and backdoor attacks in multi-modal contrastive learning settings like CLIP. The approach consists of two stages, an unimodal self-supervised warm-up stage and a training stage. In the first stage, both encoders (image + text) are trained separately in a self-supervised manner to learn meaningful representations of inputs without the components being affected by the poisoned sample pairs. Instead of solely applying standard contrastive learning methods, the sample matching in the embedding space is done with nearest neighbors from a sample pool to avoid cluster building of poisoned samples. After both components are trained, the whole system is trained with the standard CLIP objective for a single epoch and a low learning rate to connect similar image and text embeddings. In the second stage, the training data is divided into safe and risky data based on the assigned cosine similarity by the system itself. Both sets are updated after each training epoch with the tendency to increase the number of safe samples. Whereas training with safe samples also uses the standard CLIP objective, the risky samples are again used with unimodal contrastive learning. The evaluation compares the proposed defense method with two existing approaches and states significant improvements in terms of utility and robustness."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses an important topic of model security and proposes an interesting and novel defense mechanism. Since the approach does not require a clean model or a clean subset of samples, it offers a more practical defense strategy compared to existing work.\n- While the evaluation is only done on one dataset (CC3M) and one CLIP architecture (ResNet+Transformer), the results are convincing and demonstrate the high effectiveness of the defense mechanism. I particularly like the comprehensive ablation study to illustrate the impact and requirement of each component of the defense strategy.\n- The paper is well-written and easy to understand. All parts of the approach are clearly motivated and described."
            },
            "weaknesses": {
                "value": "- Dividing samples into clean and risky sets by the assigned cosine similarity seems reasonable. However, I wonder if this also affects \"hard\", i.e., complex samples, and tends to put them into the risky set even if those are not poisoned in any way. I think this might harm the model's performance on more complex tasks. Since the evaluation is only done on large common benchmarks, they do not account for the model's predictive abilities on unseen hard samples.\n- Also, only measuring the attack success rate might not be enough to evaluate the backdoor effectiveness. If the samples containing triggers are not predicted as the attack's target class but also not as their ground-true class, the attack is still effective in the sense of an evasion attack. I think measuring if the samples containing triggers are also predicted as the ground-true class would be an important metric to demonstrate that the approach indeed overcomes the backdoor attacks.\n\nSmall remarks:\n- I think section 5.2 is not only an \"ablation study\" but also includes a sensitivity analysis. Maybe this should be highlighted in the section title / section itself.\n- I think citing Carlini and Terzis (Poisoning and backdooring contrastive learning, 2022, ICLR(!)) and Carlini et al. (Poisoning Web-Scale Training Datasets is Practical, 2023) in the introduction would further support the statements made by the paper.\n- There are a few typos in the paper, e.g., \"exaples\" in 1 and a missing space in 5.2.2 \"(2)the\""
            },
            "questions": {
                "value": "- Would you expect the approach to also work for higher poisoning rates or different trigger patterns? Or is there to be an expected limit on the poisoning rate that the method can handle? To clarify, I would expect that the distinction between clean and risky samples might not work well if the number of poisoned samples is rather large."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697533017974,
        "cdate": 1697533017974,
        "tmdate": 1699636938679,
        "mdate": 1699636938679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nBKxTzpIPv",
        "forum": "Ge0GEOvifh",
        "replyto": "Ge0GEOvifh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_TTCu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7705/Reviewer_TTCu"
        ],
        "content": {
            "summary": {
                "value": "Contrastive Language-Image Pre-training (CLIP) is an approach to learn a joint embedding space for images and languages with image-caption data. This paper proposes a new CLIP-based pre-training strategy called SafeCLIP to make it more robust against data poisoning attacks and backdoor attacks.\n\n\nThey first apply unimodal contrastive learning to pre-train the image encoder and text encoder separately.\nThen they apply the original CLIP training using all data for 1 epoch with a smaller learning rate.\nAssuming the models are not yet effectively poisoned/backdoored, they then use the similarity between the current image embeddings and caption embeddings as indicators of whether the training data are poisoned/backdoored for later training, where they apply regular CLIP loss on samples with high similarity between the image embeddings and text embeddings and apply unimodal contrastive learning to the others."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The subject is important. Given the impact of CLIP, how to make it robust against data poisoning/backdoor attacks can be interesting to many from both research and application communities."
            },
            "weaknesses": {
                "value": "1. Key assumptions for the proposed defense are flawed and unverified.\n\nThere are two major (necessary but not sufficient) assumptions for the proposed method to be effective: 1) Contrastive learning on individual modalities (image/text) is immune to poisoning/backdoor attacks. 2) One epoch of CLIP training with reduced learning rate over potentially poisoned/backdoored data is safe.\n\nRegarding assumption 1), it may be true for some attacks designed specifically for CLIP (i.e. they inject malicious patterns through the image-caption pairs). However, given that we have not only backdoor attacks on (unimodal) self-supervised learning [1] and also poisoning attacks utilizing the unlabeled part of (unimodal) semi-supervised learning [2], the assumption does not seem to be true in general, which is a critical issue.\n\nAssumption 2) is conceptually fine but the execution is not satisfying. Specifically, from Table 3 we see that the effectiveness of filtering is sensitive to many design choices (e.g. how many unimodal contrastive learning & how many CLIP training with lowered learning rate). Note that '1 epoch' is likely not a good, universal measurement of how many slow-paced CLIP training should be conducted: If one has a new set of data, with difference distributions and different number of samples as used in yours, how can one decide these parameters when they do not know the poisoned/backdoor samples?\n\n\n2. The reported performance numbers (for both baseline and the proposed method) seem a bit low. I understand the numbers should be much lower than the actual CLIP since the authors have to use (a part of) a smaller dataset. However, '32 epochs' for 1 million samples seems insufficient (which is insufficient to train supervised models on ImageNet, unless with carefully designed training schemes, or for most if not all unimodal contrastive learning methods). The original CLIP paper indeed trains only 32 epochs but they are using much more data than yours. These raise concerns regarding the usefulness of these results.\n\n3. In addition, some of the performance boosts are attributed to the 'data augmentation' (i.e. line 10 of Algorithm 1). I am not able to find the details of these augmentations and if I understand correctly, these augmentations are not used in your implementation of the baseline CLIP. This does not seem to be a fair comparison unless good reasons are provided to include these augmentations as part of the contributions.\n\n\n**references:**\n\n[1] Saha, A., Tejankar, A., Koohpayegani, S.A. and Pirsiavash, H., 2022. Backdoor attacks on self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13337-13346).\n\n[2] Carlini, N., 2021. Poisoning the unlabeled dataset of {Semi-Supervised} learning. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 1577-1592)."
            },
            "questions": {
                "value": "Please see the Weakness part above for detailed issues.\n\nFor 1: Regarding assumption 1), I think it is a fundamental issue and I would like to hear your response. For assumption 2), I want to hear if authors have proposals about how these design choices (e.g. the number of unimodal pretraining epochs, the number of slow-paced CLIP epochs) can be determined without knowing the poisoned/backdoor samples.\n\nFor 2: Empirical results justifying (i.e. showing that these hyper-params lead to a fairly good results given the dataset) the hyper-parameters for the **baseline implementation of CLIP** on your training set is necessary unless other evidence can be provided for justification.\n\nFor 3: Please include these augmentations in the baseline as well unless there is a good reason to include these augmentations as part of the contributions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698984526090,
        "cdate": 1698984526090,
        "tmdate": 1699636938558,
        "mdate": 1699636938558,
        "license": "CC BY 4.0",
        "version": 2
    }
]