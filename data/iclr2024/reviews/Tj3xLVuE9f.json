[
    {
        "id": "Ki5MwsFCp4",
        "forum": "Tj3xLVuE9f",
        "replyto": "Tj3xLVuE9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_gtPN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_gtPN"
        ],
        "content": {
            "summary": {
                "value": "This work studies the relationship between predictivity and availability of input features, and how a model depends on them. The work starts out with a 2D synthetic scenario where these two dimensions are explicitly characterised. In this scenario, they find that models can rely on more available features, in spite of others being more predictive, and that model depth and nonlinearity increases this effect. The authors further test a synthetic, high-dimensional image dataset and natural images which are manipulated. They find that the number of pixels which correspond to shortcut features, which is viewed as availability of a feature, can lead to the model relying more on it, even when said feature is less predictive. They also provide theoretical results relating to the Neural Tangent Kernel."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work is a creative way of understanding shortcut learning, and I enjoyed reading the paper. The paper clearly defines central concepts and terms, and quantifies them. The experimental analyses are likewise rigorous, plentiful, and well-supported, even though I have important concerns which are listed below. The paper is well-written, mathematically precise, and clear. Overall, this paper will be of interest to the research community."
            },
            "weaknesses": {
                "value": "The work has several weaknesses, and I have the following concerns: \n\n* The novelty of the work is unclear (see my comment below).\n* The synthetic experiment in section 3 is\u2014as much as I enjoyed reading it\u2014highly constructed. It is a good opener for the work, and it draws intuitive, story-supporting conclusions, but I would like to ask the question to which degree the stated conclusions will generalise to high-dimensional datasets in the wild, and generally to which degree these conclusions are generally true. I recognise that this a first step towards this goal, but further evidence or explanation on why these conclusions may be generally true would be useful. Also, it should be considered if section 3 should be given less prominence.\n* The naturalistic data experiment in section 7 is most questionable to me. In my view, decreasing the \u201cavailability\u201d of the background image by setting its entropy to 0 (black color) is directly biasing the classifier to learn from the remaining available, foreground signal where you know the label is highly predictive. The observed relationship of higher model accuracy is hence non surprise. The same applies to the other scenarios looked at."
            },
            "questions": {
                "value": "In section 3, the two main parameters that govern availability are $\\alpha_i$ and $eta_i$. $\\eta_i=0$, so $\\alpha_i$ remains, and its ratios are manipulated. Could you please explain your intuition why $\\alpha_i$ controls \u201chow easily the feature can be extracted, or leveraged, from inputs\u201d? Why is a feature with a large latent amplitude more readily produced by a neural network? This important premise of your experiment in characterising availability is unclear to me. Any further evidence that makes this clearer would be appreciated.\n\nI don\u2019t fully understand why your definition of reliance is useful. $\\hat{y}_\\mathcal{M}(z)$ is the \u201cbinary classification decision\u201d, i.e. -1 or 1. Then, when taking the expected value over z, I don\u2019t understand why taking a sign difference of z_s and z_c is reasonable.\n\nThe novelty of this work relative to related work, which I am not very familiar with, is unclear to me and not discussed in the paper. Could you please delineate your work, and clarify your novel contributions, or point me to appropriate positions in the paper which describe this, if missed? \n\nI would be interested in more complex synthetic scenarios than the one outlined. Did you think about these? How could the robustness of your conclusions in the synthetic scenario be tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6363/Reviewer_gtPN",
                    "ICLR.cc/2024/Conference/Submission6363/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520683160,
        "cdate": 1698520683160,
        "tmdate": 1699966928463,
        "mdate": 1699966928463,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J711UT4PUu",
        "forum": "Tj3xLVuE9f",
        "replyto": "Tj3xLVuE9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_G1nt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_G1nt"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a systematic study on the trade-off between predictivity and availability in deep learning, proposing a theoretical foundation for shortcut learning in neural networks. The key contributions are:\n1.The paper introduces quantitative notions of predictivity and availability using a generative model to synthesize classification datasets where two latent features vary in terms of these attributes. A measure of \"shortcut bias\" is proposed to quantify a model's over-reliance on more available but less predictive features.\n2.Through controlled experiments, it is shown that nonlinear models exhibit greater shortcut bias compared to linear models, and model depth amplifies this effect.\n3.A theoretical analysis based on Neural Tangent Kernels proves the inevitability of availability bias for nonlinear architectures like ReLU networks, while linear networks are unbiased.\n4.Experiments on natural image datasets demonstrate that widely used vision models are sensitive to availability, not just predictivity, of non-core features. Explicit availability manipulations of images are shown to alter models' reliance on different features.\nTaken together, the empirical and theoretical findings reveal shortcut learning as an inherent characteristic of nonlinear deep models that needs to be studied systematically. The framework presented lays the groundwork for further investigating architectural choices, dataset factors and methods to mitigate shortcut bias.\n5.Overall, this is a thorough, well-executed study that makes important theoretical and practical contributions towards understanding shortcut learning in deep neural networks. The paper is clearly written and the empirical methodology is sound. The theorems and proofs rigorously analyze the interplay between predictive value and feature availability. This work provides key insights into model failures related to over-reliance on spuriously predictive features, and tools to improve model robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The proposed generative framework for synthesizing datasets with controllable levels of predictivity and availability is creative and enables controlled experimentation.\n2.The empirical methodology is thorough and sound. The datasets, models, and evaluation metrics are carefully designed. Results are reported over multiple random seeds to ensure significance.\n3.Shortcut learning is a pivotal concept in deep learning with implications for generalization and fairness. This work significantly advances our understanding of why models fail in this manner.\nIn summary, this is a paper of exceptional quality and scientific merit that offers significant theoretical and practical value to the field. The novel concepts, thorough empirics, and rigorous theory set a new standard and open up many promising research directions."
            },
            "weaknesses": {
                "value": "This is an good paper overall, but a few minor weaknesses could be addressed to further improve it:\n\n- The theoretical analysis makes approximations to obtain tractability (small covariance, quadratic approximation of ReLU kernel). It would strengthen the analysis to discuss the impact of these approximations. Are the core insights still valid without them?\n\n- The proposed notion of availability is intuitively reasonable but remains a hypothesis. Additional ablation studies that directly validate the choice of manipulations affecting availability could make this more concrete.\n\n- The measures of reliance and shortcut bias, though well-motivated, are indirectly quantified through alignment with idealized classifiers. More analysis could be provided to justify these metrics over alternatives.\n\n- There is scope for further investigating architectural manipulations that could mitigate shortcut bias, beyond just model depth and activation function. This could lead to practical guidelines.\n\nOverall these are minor limitations that do not diminish the quality of the work. The paper thoroughly delivers on its core promises. Addressing the above points where possible would make it even stronger. The work clearly advances our understanding of an important problem and provides a solid foundation for reducing shortcut reliance in deep learning models."
            },
            "questions": {
                "value": "Here are some questions and suggestions to further improve the paper:\n\n- The quadratic approximation for the ReLU kernel greatly simplified the theoretical analysis. Could you provide some empirical verification that this does not alter the core findings? For example, compare the availability bias for the true ReLU kernel versus the quadratic approximation.\n\n- Have you experimented with any architectural manipulations beyond depth and activation functions that could potentially reduce shortcut bias? Things like skip connections, normalization layers etc. Exploring this could provide practical guidelines. \n\n- For the image experiments, it would be interesting to also show the impact of availability manipulations on a model pretrained on Imagenet. Does pretraining affect sensitivity to shortcuts?\n\n- The measures of reliance and bias involve probing a model in latent space. For vision experiments, could you provide some visualizations of model decisions before/after availability manipulations to build intuition?\n\n- The notion of availability is central but still somewhat conceptual. Are there any additional experiments you could do to further validate the manipulations proposed to affect availability?\n\nI hope these suggestions help further refine the work. The key results seem solid and demonstrate the importance of availability bias. Some additional experiments and discussion along the lines above could make it even more convincing and applicable across domains. I look forward to the authors' response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672421079,
        "cdate": 1698672421079,
        "tmdate": 1699636702538,
        "mdate": 1699636702538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GiQWCqnNnH",
        "forum": "Tj3xLVuE9f",
        "replyto": "Tj3xLVuE9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_KL3Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_KL3Q"
        ],
        "content": {
            "summary": {
                "value": "The paper studies shortcut learning, i.e., why networks use to learn shortcut/spurious features over intended semantic features. The authors suggest that the network prefers a feature that is more available to quantify shortcut bias in terms of how a learned classifier deviates from an optimal classifier in its feature reliance. The authors study the neural network preference toward shortcut features using synthetic datasets with varying predictability and availability of the shortcut features. They empirically observed that networks prefer to learn shortcut features when they are more available, and ReLU is biased toward shortcuts. They also theoretically show using the NTK that linear networks are less biased towards the shortcut compared to the ReLU networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-written and easy to follow. The problem of shortcut learning, which is not extensively studied, is important to understand.\n\n* The paper presents interesting insights into neural networks\u2019 preference toward shortcuts. The paper studies shortcut learning empirically using controlled datasets and theoretically using the NTK.\n\n* The derivation for the bias of linear and ReLU networks using NTK would be helpful for future work in this domain."
            },
            "weaknesses": {
                "value": "* The main observation that the model depth and non-linearity increase bias towards the shortcut features is intuitive. Both depth and non-linearity allows the model to learn a rich representation.\n\n* Theoretical analysis using NTK is problematic as they don't learn feature and thus cannot necessarily explain model's preference towards the shortcut feature. \n\n* Only vision tasks are explored in the paper, it is not clear if the observations will hold true for other domains.\n\n* It would be interesting to see experiments with the vision transformers."
            },
            "questions": {
                "value": "* Do you think the observations will be similar for other non-linearities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6363/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6363/Reviewer_KL3Q"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848169978,
        "cdate": 1698848169978,
        "tmdate": 1700674221979,
        "mdate": 1700674221979,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eKgVo54Ef8",
        "forum": "Tj3xLVuE9f",
        "replyto": "Tj3xLVuE9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_mfV7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_mfV7"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the learning of shortcut / spurious features. Mainly, the paper looks at the interplay between predictability and availability (that defines how easy it is for a model to extract a feature). First, experiments on simple, synthetic data, generated from two variables for which we can control the predictability and availability are shown. A notion of shortcut bias is defined as the additional reliance on spurious features for a learned model, compared to an optimal model. It is shown that availability determines the learning of spurious features, even when the predictivity of spurious features is lower than that of the core features. Non-linearities are shown to induce more bias for shortcuts. Theoretical analysis shows that linear networks are not biased to feature availability while ReLU networks are. Experiments on image datasets show that they are biased for background and object size."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- S1. The paper deals with an important aspect of understanding neural networks, the bias for learning shortcuts.\n\n- S2. Using the notion of shortcut bias, based on the reliance of an optimal predictor is a good idea.\n\n- S3. Analysing based on a notion of availability, that can be computed produces some good observations. \n\n- S4. Interesting to see that theoretically, ReLU networks are more biased than linear networks."
            },
            "weaknesses": {
                "value": "- W1. The notion of availability is very closely connected with the concept of simplicity of neural networks. The simplicity bias has been pointed out as a cause of non-robust learning. The paper gives multiple references (e.g. Shah et al., 2020, etc.) for works dealing with simplicity bias, but they are not discussed in detail. The authors must clarify what is the difference between the notion of availability presented in this work, and the simplicity bias previously introduced. What new observations does the proposed framework bring?\n\n\n- W2.The experiments are not that strong. The image datasets do not seem to bring any interesting observations. It is already known that the background influences the prediction to a high degree. It is expected that alterating the background will improve the reliance on the core features. All methods that use these datasets, try to learn how to not rely on the background. There doesn\u2019t seem to be any novel observation in this regard. \n\n- W3. For image datasets: \u201ca Bayes optimal classifier is not comparably sensitive to the predictivity of the non-core features\u201d How do we know this? What experiments give this conclusion? \n\n- W3.2 Also, how is the Bayes optimal classifier created in the case of real image datasets (WaterBirds, CelebA)?\n\n- W4. The paper defined a shortcut as a feature that is more available, but less predictive. Differently, a shortcut is usually defined by good predictivity in distribution, but poor predictivity in some other distributions (OOD)."
            },
            "questions": {
                "value": "Could the curves given by making an intervention on the background, be use to benchmark the robustness of different method? E.g. more robust models would have curves that are less sensitive to interventions on the background. Whould this offer any additional insight as opposed to comparing the accuracy of the model on balanced data, without foreground-background spurious correlations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698878107457,
        "cdate": 1698878107457,
        "tmdate": 1699636702308,
        "mdate": 1699636702308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XSM1qXN2iv",
        "forum": "Tj3xLVuE9f",
        "replyto": "Tj3xLVuE9f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_QCsi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6363/Reviewer_QCsi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies why shortcut learning happens. It focuses on two characteristics that input features may have: availability and predictivity. Availability refers to how frequently that type of feature is available in the data, while predictivity means how useful it is in predicting the target. Shortcuts are described as overly relying on available features that are less predictive. Based on this interpretation of shortcuts, the paper experimentally shows on synthetic and natural image datasets that availability explains shortcut learning where predictivity cannot. These findings are supplemented with a theoretical analysis that concludes that adding a single hidden layer already biases the model to rely on shortcuts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and the main message that availability is a key driver to shortcut learning is convincingly conveyed.\n\nBoth empirical support and theoretical support are provided to underline the effect of availability on shortcut learning."
            },
            "weaknesses": {
                "value": "The theoretical analysis is limited to a single hidden layer MLP which does not reflect the type of architectures used in practice. It is difficult to conclude whether this also holds for other types of architectures like Transformers or CNNs.\n\nExperiments are limited to supervised classification settings. Self-supervised training or other tasks like object detection would be interesting to consider in the context of shortcut learning."
            },
            "questions": {
                "value": "The paper points out that availability can explain (some) occurrences of shortcut learning. The availability can be determined by looking at the training data distribution, but could one also identify these shortcuts stemming from availability by directly looking at the train neural network weights? For example, if multiple filters in the same CNN layer share the same pattern?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699217505744,
        "cdate": 1699217505744,
        "tmdate": 1699636702191,
        "mdate": 1699636702191,
        "license": "CC BY 4.0",
        "version": 2
    }
]