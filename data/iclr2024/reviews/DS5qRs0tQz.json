[
    {
        "id": "OFvts3uEuR",
        "forum": "DS5qRs0tQz",
        "replyto": "DS5qRs0tQz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_9tNN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_9tNN"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an open-vocabulary object detection network namely Grounding DINO. Based on the original DINO object detector, the authors introduce 1) text encoder and cross-modal feature enhancer to incorporate language-specific feature into image features; and 2) language-guided query selection with cross-modal decoder to detect and recognize objects with language guidance. The proposed Grounding DINO obtains promising results on various zero-shot object detection tasks and referring object detection tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This overall writing is polished and easy to understand. \n\n2. The proposed Grounding DINO shows good generalization ability on various tasks, including zero-shot general object detection and referring object detection."
            },
            "weaknesses": {
                "value": "1. The key components of proposed method (e.g., feature enhancer and cross-modality decoder) are not new. The authors should thoroughly discuss the relation and difference of between feature enhancer in GLIP [1] and the counterpart in this work, and conduct the same analysis for cross-modality (with X-Decoder [2]).\n\n2. Though utilizing better detector and better pretrained models, this work does not lead to better performance than previous work, e.g., DetCLIP-V2 [3] (CVPR 23), which obtains much better performance on LVIS benchmark and ODinW benchmark than Grounding DINO. \n\n[1] Grounded Language-Image Pre-training, CVPR 2022\n[2] Generalized Decoding for Pixel, Image and Language, CVPR 2023\n[3] DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment, CVPR 2023"
            },
            "questions": {
                "value": "See weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Reviewer_9tNN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657525378,
        "cdate": 1698657525378,
        "tmdate": 1699636445122,
        "mdate": 1699636445122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BPM09MMgxa",
        "forum": "DS5qRs0tQz",
        "replyto": "DS5qRs0tQz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_sRML"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_sRML"
        ],
        "content": {
            "summary": {
                "value": "This study proposes a strong open-set object detector called Grounding DINO. When compared with other open-set detectors like GLIP, Grounding DINO introduces the DINO object detector as its overall architecture. It adopts the Swin transformer as the backbone and the encoder-decoder pipeline to replace the DyHead structure as used in GLIP. More fusions between the textual and visual features in the pipeline are also introduced to enable the model to perform better. Moreover, the Referring Expression Comprehension (REC) tasks are also introduced in its pertaining.\n\nGrounding DINO is evaluated on different tasks, including MS-COCO, LVIS, ODinW, and RefCOCO/+/g. It achieves 52.5 AP zero-shot detection on COCO and also obtains state-of-the-art performance on ODinW with 26.1 AP. Albeit strong in common tasks, Grounding DINO performs worse than GLIP on rare classes on OdinW, and the authors also admit its limitation on its zero-shot ability on REC data without fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a thorough study composed of different shining components. \n- First of all, the integration of the DINO object detector pushes forward the performance on different detection benchmarks thanks to the strong feature extraction ability of the new architecture. \n- The generalization of different feature fusion methods is clear and inspiring. The proposed feature enhancer calls for more attention to more important features according to the text input. The language-guided query selection module resembles the top-K operation in Efficient DETR/DINO but is endowed with new physical meanings in this pipeline, as only queries that are closely related to the input text are kept. \n- The introduction of REC tasks in both pertaining and evaluation opens a new gate for open-set object detectors. The definition of traditional open-sent object detection naturally extends to the fields of describing open-set objects using more versatile expressions. \n- The performance reported in this study is competitive. It achieves state-of-the-art performance on the zero-shot ability on MS-COCO and ODinW datasets. \n- The reviewer appreciates the presentation regarding the limitations of this study."
            },
            "weaknesses": {
                "value": "- Novelty issues. Despite the effectiveness of the proposed architectures, the composed modules are not innovative enough as their own. For example, the major component that makes a difference should be the DINO architecture that has already set a record on various detection tasks. The query selection is also similar to that of the top-K operators in Efficient DETR/DINO. Although Grounding DINO extends its tasks to referring expression comprehension, it is only comparable to its precedent studies like GLIP and does not introduce specific modules to address the shortcomings on REC. Nevertheless, I would also admit that these novelty issues are not critical as each of them is not trivial in the task.  \n- I have some concerns regarding its significance and effectiveness in true open-set scenarios. \nDespite the effectiveness of the zero-shot performance on MS-COCO and ODinW datasets, the performance seems to rely on its pertaining data heavily. As also revealed by the author, Grounding DINO transfers better on common classes, classes that are more likely included in the pertaining data in some format, yet performs worse on rare classes in LVIS. The zero-shot performance on the RefCOCO dataset is significantly lower than that after including RefCOCO in its pertaining data. \n- A closer look into Table 4 would also reveal that the performance on ODinW is actually on par with GLIPv2 if given the same pertaining data. This would also raise a concern about its true ability in true open-set scenarios. \n- Some minor writing issues. e.g., \"hard-crafted\" -> \"hand-crafted\" (page 2), \"Even though\" -> \"Even though the performance plateaus with larger input size\" (page 7)."
            },
            "questions": {
                "value": "Overall, this is a nice work and an interesting study. I encourage the authors to respond to the above weaknesses.  Besides, I also have some additional questions regarding the paper and possibly some future work. \n- It is assumed that the query selection module might be the major reason for the low performance on rare classes in LVIS. Have the authors increased the top-K values to validate the assumption?\n- As for the limited ability of the REC dataset, is it possible to increase the model size of the BERT text encoder to obtain a better representation of complex text input to alleviate this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719091305,
        "cdate": 1698719091305,
        "tmdate": 1699636445041,
        "mdate": 1699636445041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NjPQ8FuzJt",
        "forum": "DS5qRs0tQz",
        "replyto": "DS5qRs0tQz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_jXst"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4648/Reviewer_jXst"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Grounding DINO, an open-set object detector that combines DINO with grounded pre-training. This allows it to detect objects based on human inputs like category names or expressions. \nThe innovation lies in using language to enhance closed-set detectors for better open-set detection. The detection process is conceptually divided into three phases, including a feature enhancer, language-guided query selection, and a cross-modality decoder for merging vision and language. \nThis work also evaluates referring expression comprehension for attribute-specified objects. Grounding DINO excels in multiple benchmarks, including COCO and ODinW, setting new records like a 52.5 AP on COCO's zero-shot transfer benchmark without using its training data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper shows that text and vision fusion and multiple stages helps the model achieve better performance compared to later fusion.\n2. The paper shows benefits on multiple settings, including closed-set detection, open-set detection, and referring object detection, to comprehensively evaluate open-set detection performance\n3. Grounding DINO outperforms competitors by a large margin and establishes a new state of the art on the ODinW (zero-shot benchmark with a 26.1 mean AP"
            },
            "weaknesses": {
                "value": "1. More than 900 queries would be interesting to see if the model generalizes well to rare classes as well\n2. The ablations in table 6 and 7 do not show strong contribution of individual modeling choices except tight fusion. The authors dont explain this"
            },
            "questions": {
                "value": "Please refer to weakness 1 and 2"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4648/Reviewer_jXst"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820659971,
        "cdate": 1698820659971,
        "tmdate": 1699636444963,
        "mdate": 1699636444963,
        "license": "CC BY 4.0",
        "version": 2
    }
]