[
    {
        "id": "tCsBmGNCls",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
        ],
        "forum": "e9bEoxNiTJ",
        "replyto": "e9bEoxNiTJ",
        "content": {
            "summary": {
                "value": "This paper presents a hierarchical architecture for transparent object segmentation. Boundary and reflection cues are incorporated in the module designs. Extensive experiments are conducted on multiple benchmarks, which shows the effectiveness of the proposed model. The paper is overall well-written and nicely structured."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed model achieves state-of-the-art performances on multiple datasets.\n2. Failure cases are well studied.\n3. The paper is overall well-written and nicely structured."
            },
            "weaknesses": {
                "value": "1. In Fig. 5, does it show that the proposed method is consistently effective for different backbones? This should be better discussed.\n2. In Table 6, it would be nice to show the computation complexity of the two designed modules for analysis.\n3. How to theoretically verify that the proposed method did really make use of reflection cues? This could be better discussed.\n4. It is hard to find any novel operations in the proposed reflection feature enhancement module as it simply combines existing mechanisms. It would be nice to clarify the technical novelty and theoretical contributions of the proposed modules.\n5. There are extensive segmentation methods that introduce boundary-relevant loss designs or other designs. Please consider incorporating some existing boundary-relevant designs for a comparison. This can better show the superiority of your proposed boundary feature enhancement module. \n6. The related work follows that of Trans4Trans. It would be nice to add more recent related state-of-the-art works.\n\nSincerely,"
            },
            "questions": {
                "value": "Would it be possible to incorporate your model with the RGB-D modalities for an experiment? This could be discussed.\n\nWhen the proposed model works on images without any transparent objects, would it create false positives? This could be assessed.\n\nSincerely,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission705/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission705/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697345884666,
        "cdate": 1697345884666,
        "tmdate": 1699635997684,
        "mdate": 1699635997684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gwkXSKHJZ2",
        "forum": "e9bEoxNiTJ",
        "replyto": "e9bEoxNiTJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission705/Reviewer_3ba2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission705/Reviewer_3ba2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the TransCues, a transformer encoder-decoder network for the segmentation of glass, mirrors, and transparent objects. The main idea of this paper is to model the boundary and the reflection cues. Accordingly, a Boundary Feature Enhancement (BFE) module and a Reflection Feature Enhancement (RFE) module are proposed. The BFE module is implemented based on the ASPP module and the RFE module has an encoder-decoder structure. The paper runs experiments on eight existing datasets, and the comparisons show that the proposed method achieves impressive results, but with different models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has certain merits. \nAlthough the boundary and reflection cues have been explored in previous works, the paper shows that a better network design that focuses on low-level features may improve the segmentation of mirrors and glass surfaces/objects.\nThe paper provides extensive comparisons on eight benchmarks, which shows an overall picture of this topic.\nThe paper is generally easy to read and understand."
            },
            "weaknesses": {
                "value": "However, I have some concerns.\nThe first concern is about the results. The paper creates a lot of models, I.e., TransCues -T, -S, -M, -L, -B1, -B2, -B3, -B4, -B5, while some of them are based on PVTv1, and the others are based on PVTv2. During the comparisons, Table 1 uses B4, Table 2 and 5 use B2, Table 3 and 4 use B3, and the Table 6 uses B1. This makes the comparisons very messy, which may not provide meaningful analysis/discussions. What are the criterion of such selections? I note that there are only one Table (Table 13 in the supplemental) includes all nine TransCues models, from which it seems that B1 and B2 outperforms Ours-L with less parameters. How often and why does this happen is not known. \nThe Abstract mentions that the RFE module ``decomposes reflections into foreground and background layers\u2019\u2019, however, in section 3.3, I do not find corresponding designs and the motivations of such designs. Second, section 3.4 uses pseudo ground truth reflection masks, but it is not mentioned how these pseudo labels are created. Third, the paper only discuss RFE with (Zhang et al., 2018) regarding the reflection modeling. The ICCV\u201921 paper ``Location-aware Single Image Reflection Removal\u2019\u2019 detects the strong reflections. Would it be better to use reflection removal methods to generate pseudo labels?\nThe boundary loss seems not a novelty. If so, I suggest to move it onto the supplemental. Otherwise, the paper needs to explain where the novelty is and provides discussions with existing methods. For example, the IJCV\u201922 paper ``Learning to Detect Instance-level Salient Objects Using Complementary Image Labels\u2019\u2019 uses canny operators to enhance the boundary information. The PMD (Lin et al. 2020) also uses ground truth boundary information for the supervision.\nThe feature flow in the RFE module (Figure 7 of supp.) is rather complicated and more explanation is helpful, in order to evaluate its novelty.\nThe placements of RFE and BFE seems casual. I can only guess the reason might be that the authors try to focus the whole network on low-level features. More explanation is helpful.\nThe ablation study only includes the RFE and BFE, while it is not known how much contributions the FEM, FPM and the final MLP have made to the segmentation performance.\nThe model relies on the detection of reflections, while for glass surface/objects segmentation, the question is whether reflections can always be detected, and if not, how does it affects the final results? The paper shows failure cases on the Trans10K-v2, but such cases seem dataset-specific. It is better to show failure cases that caused by the limitations of the proposed model.\n\n\n\n\nBelow are some suggestions.\nUse the symbols (including the use of, e.g., \\mathcal) more consistently.\nThe Position Embedding and the Encoder and Decoder paragraphs in section 3.1 can be shorten."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697517977253,
        "cdate": 1697517977253,
        "tmdate": 1699635997586,
        "mdate": 1699635997586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2tfOrywmft",
        "forum": "e9bEoxNiTJ",
        "replyto": "e9bEoxNiTJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission705/Reviewer_WwQc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission705/Reviewer_WwQc"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an efficient transformer-based segmentation architecture TransCues, which exhibits strong performance in segmenting transparent objects. This capability is attributed to the innovative integration of the Boundary Feature Enhancement module and the Reflection Feature Enhancement module. \nThe authors show solid results on various transparent object segmentation and generic semantic segmentation benchmarks and conducts comprehensive ablation studies on their core design choices."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The content is well-organized and easy to follow. The motivation is well-established and the effectiveness of their solution is verified by extensive experiment. The proposed architecture achieved competitive performance on a wide range of tasks, while maintaining competitive efficiency."
            },
            "weaknesses": {
                "value": "The authors regard the boundary loss as their contribution, but do not provide an ablation of this module. Similarly, the reflection loss also has not been ablated.\nThe authors claim that their proposed approach is robust to generic semantic segmentation tasks, but do not evaluate on the most widely used semantic segmentation datasets, such as ADE20K and cityscapes.\nThe influence of different pretraining of the backbone is not properly assessed;\nThe authors claim that most semantic segmentation models struggle to distinguish between glass and non-glass regions, but does this assertion still hold true for the state of the art generic semantic segmentation model, such as SAM?"
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699264345228,
        "cdate": 1699264345228,
        "tmdate": 1699635997501,
        "mdate": 1699635997501,
        "license": "CC BY 4.0",
        "version": 2
    }
]