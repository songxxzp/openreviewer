[
    {
        "id": "vojLnXsl7k",
        "forum": "NTps1DTdLB",
        "replyto": "NTps1DTdLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_A92f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_A92f"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method that combines set-valued prediction with out-of-distribution detection in multi-class classification problems. The central idea is a risk minimization framework with a loss function that consists of three parts. The first two parts trade off set size and accuracy, while a weight parameter controls which of the two terms is more important. The third term allows to exclude atypical examples from acceptance regions. In addition, the penultimate layer of the neural uses random fourier features to approximate a Gaussian kernel. \n\nThe authors present theoretical results that present (a) the quality of the random Fourier feature approximation (b) the convergence to Bayes risk when sample size increases. In the experiments the proposed methods is compared to three baselines on three datasets and three metrics. The metrics evaluate the set-valued prediction and OOD detection performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The presented method is novel\n- Overall the paper is well written (but some parts are unclear, see below)\n- I agree with the authors that combined set-valued prediction and OOD detection is a key concept in satefy-critical applications of AI. \n- I liked that the authors describe the problem setting and the assumptions formally (Assumptions 1 and 2). This is often missing in OOD detection papers."
            },
            "weaknesses": {
                "value": "This is a quite technical paper, and I am afraid that I don't understand the method very well, despite having a background on the topic and spending quite some time to read the paper. The last part of objective function (1) is unclear to me. What are lambda_k and rho_k? Are these explained in the paper? The authors explain that rho_k is used to exclude atypical examples from acceptance regions, but I don't see yet how that's going on. Also, why is a Frobenius-penalty needed for the parameter matrices? This is quite atypical for deep learning methods, where regularization is typically done via early stopping in SGD.\n\nFurthermore, the need for random Fourier features in the penultimate layer of the neural network is also unclear to me. What does this component add to the method, compared to just propagating the embedding to the output layer? \n\nI also find the connection to existing literature a bit weak. The literature on OOD detection is vast, so I understand that the authors cannot discuss every paper, but some essential papers are definitely missing. Assumption 1 clearly motivates why generative models / density-based models are a good approach to represent P(x|y) as a first step for combined set-valued prediction and OOD detection. The authors discuss a few methods that model P(x|y), such as the unpublished work of Hechtlinger et al. However, there are many other papers that also model P(x|y), such as:\nCharpentier et al. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts, ICLR 2021\nVan Amersfoort et al. Uncertainty estimation using a single deep deterministic neural network, ICML 2020\nLee et al. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, Neurips 2018\n\nPerhaps those papers don't evaluate set-valued prediction, but they can be immediately used for such purposes. From that perspective, I would argue that such methods are also better baselines than the current baselines. These methods are deep learning methods, and they are published, unlike two of the three papers that are currently used as baselines. If I would have to do simultaneous set-valued prediction and OOD detection for a specific application, I would be more tempted to try these methods first instead of the method proposed here, because for methods that model P(x|y) it is more clear what they are doing. For the proposed method I am not sure whether it is modelling a class-specific density P(x|y). This might be realized via the random Fourier features, but more explanation would be needed. \n\nFurthermore, in set-valued prediction there are also quite some methods that consider loss functions that consist of two parts: a part that minimizes accuracy, and another part that minimizes set size, see e.g. \nMortier et al. Efficient set-valued prediction in multi-class classification, Data Mining and Knowledge Discovery, 2020. \nTitouan Lorieul, Uncertainty in predictions of deep learning models for fine-grained classification, PhD thesis, University of Montpellier, France. \n\nThe proposed method has a lot of connections with such methods, but there are two differences: (1) the proposed method has an additional third part in the loss, (2) the other methods typically fit a probabilistic model first, and optimize set-based utility scores during the inference phase. Perhaps these two differences are enough to behave good for OOD detection as well, but that's still unclear to me."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6572/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6572/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6572/Reviewer_A92f"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698051571599,
        "cdate": 1698051571599,
        "tmdate": 1699636745315,
        "mdate": 1699636745315,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C1YPxyuIZs",
        "forum": "NTps1DTdLB",
        "replyto": "NTps1DTdLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_5AV7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_5AV7"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel way to learn a set-valued classifier, called Deep Generalized Prediction Set (DeepGPS); the proposed method is capable of identifying ambiguous observations and detecting out-of-distribution observations. Also, it is the first set-valued classification with a theoretical guarantee and scalable to large datasets. In theory, this paper provides that DeepGPS attains the optimal expected prediction set size, while achieving the user-prescribed class-specific accuracy. The efficacy of DeepGPS is demonstrated by using MNIST/CIFAR10/Fashion-MNIST datasets and multiple baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper proposes a learning approach for DeepGPS along with its theoretical properties in Thm1-3."
            },
            "weaknesses": {
                "value": "I appreciate the authors' careful analysis on the algorithm. I was initially surprised that the proposed approach can achieve a user-prescribed class-specific accuracy \\gamma without training a base model and additional set predictor in a decoupled way. However, I found that it is simply due to the hyperparameter C tuning in a validation set. In this regard, I found that the stated guarantee in (1) of Thm3 is largely disconnected to the empirical results. \n\nAfterward, I was not convinced whether we actually need this complicated way in training the entire neural network. I\u2019d rather simply use conformal prediction for the specified task (e.g., BCOPS). \n\nAlso in representing the main results in Table 1, I think the class-specific accuracy results should be bolded if they are close to the desired level; however, the maximum values are bolded."
            },
            "questions": {
                "value": "1. Based on the appendix, the hyperparameter C is chosen to achieve the desired class-specific accuracy, i.e., \u201cThe tuning parameter C is determined such that the prediction set is smallest on the unlabeled part in the validation data when the misclassification rate is close to \u03b3 on the labeled part in the validation data.\u201d Then, what\u2019s the meaning of (1) of Thm3? I think without this theorem, we can heuristically achieve the desired class-specific accuracy via hyperparameter tuning over a validation set. \n\n2. Related to the above question, can you re-evaluate the benefit of DeepGPS compared to BCOPS? I think simple training in a decoupled way provides a stronger guarantee. \n\n3.  In Table 1, is there a specific reason that the accuracies are highlighted when it is the largest number? Otherwise, please use bold numbers if the class-specific accuracy results are close to the desired level."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678679263,
        "cdate": 1698678679263,
        "tmdate": 1699636744994,
        "mdate": 1699636744994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kTsZxXUajd",
        "forum": "NTps1DTdLB",
        "replyto": "NTps1DTdLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_nfKV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6572/Reviewer_nfKV"
        ],
        "content": {
            "summary": {
                "value": "The authors explore set prediction, or conformal prediction, within the context of out-of-distribution (OOD). In this prediction paradigm, rather than offering a singular classification result, a predictor provides a set of labels. This set is expected to encompass the true label with a high degree of certainty. When dealing with OOD, predicting an empty set becomes a significant indication, suggesting the assignment of an OOD label to the test data point. The authors introduce an algorithm that employs Random Fourier Features, ensuring scalability in relation to sample size. Furthermore, they present the adaptive weighted hinge loss and offset penalization techniques to boost classification efficiency. The paper theoretically investigates the expected prediction set size for their algorithm, showing that it approaches the optimal size as the sample size grows. Experimental outcomes underscore that their algorithm surpasses existing methods. Moreover, the components of the adaptive weighted hinge loss and offset penalization play pivotal roles in enhancing classification efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n\n2. Addressing set-valued classification issues in OOD scenarios is both demanding and imperative. Issues of trustworthiness and OOD can stymie the deployment of machine learning algorithms in real-world applications. Developing an algorithm for set-valued classification within OOD scenarios augments the applicability of machine learning techniques.\n\n3. The proposed elements\u2014adaptive weighted loss and offset penalization\u2014are astutely crafted to evaluate the accuracy constraint more rigorously and to minimize the expected set size.\n\n4. Theoretical insights guarantee that the classifier obtained by the proposed algorithm will attain the optimal expected set size achieved by the ideal classifier. This underscores the rationale behind the algorithm's design.\n\n5. Experimental findings robustly attest to the proposed method's dominance over existing techniques in terms of the metrics evaluated."
            },
            "weaknesses": {
                "value": "1. The rationale behind incorporating Random Fourier Features is ambiguous. Attaining scalability can be realized by merely employing a fixed-width network as the penultimate layer. Resorting to infinite-dimensional kernel features as the penultimate layer seems unnecessary without a clear justification, making the algorithm's design seem somewhat ill-advised.\n\n2. The theoretical findings seem to be direct derivations from the generalization bound established through the Rademacher complexity. Their technical significance remains dubious. Furthermore, given that the core contributions revolve around the introduction of adaptive weighted loss and offset penalization, the impact of these components on generalization error remains unexplored. Consequently, the results offer limited support for the algorithm's design.\n\n3. There is likely an intrinsic trade-off between OOD recall and Efficiency as gauged in the experiments. Thus, assessing this trade-off's efficiency becomes crucial. A comprehensive superiority assertion for the proposed algorithm necessitates comparative analyses of such trade-off efficiencies.\n\n4. It is also vital to assess the trade-off between OOD recall and Efficiency within the ablation studies.\n\n5. The authors seem to incorporate the adaptive weighted loss with an aim to enhance the precision of class-wise error assessments. Therefore, to ascertain the efficacy of this component, evaluations of the precision of class-wise errors should be undertaken."
            },
            "questions": {
                "value": "1. Would the authors shed light on the imperative of integrating the Random Fourier Features into their methodology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6572/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829034639,
        "cdate": 1698829034639,
        "tmdate": 1699636744888,
        "mdate": 1699636744888,
        "license": "CC BY 4.0",
        "version": 2
    }
]