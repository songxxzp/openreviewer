[
    {
        "id": "cQRySLZBAb",
        "forum": "IryGDUHxDE",
        "replyto": "IryGDUHxDE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_o3GA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_o3GA"
        ],
        "content": {
            "summary": {
                "value": "The research introduces a novel approach to zero-shot action recognition by adapting a pre-trained autoregressive Vision & Language model to generate action-specific video captions. Instead of using conventional contrastive learning, the authors implement an unsupervised learning framework. This framework combines self-training with pseudo-caption generation and a retrieval system to source diverse pseudo-captions for each video, enhancing the training dataset's variety. Trained model has the ability to create predictions that are detailed, interpretable, and inherently open-vocabulary. The resulting approach has demonstrated proficiency in zero- and few-shot action recognition scenarios, either equating or surpassing the performance of prevailing contrastive learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Innovative Research Direction: The paper's proposition to generate free-form text labels for video clips aligns with the evolving trajectory of Vision-Language Models (VLM). I believe event understanding essentially boils down to crafting a concise yet descriptive label for a given video clip. This approach not only offers a semantically interpretable compression of the clip but also ensures that the textual representation is more compact than raw pixel sequences. This compressed representation potentially facilitates deeper comprehension of longer videos. Given this context, the conventional use of pre-defined class labels seems restrictive. The generative approach showcased in this paper appears to be a more optimal strategy. If this is truly the \"first of its kind,\" as the authors suggest (I admit, my expertise in this domain might not be exhaustive), it could mark a significant paradigm shift in the field.\n\n\n2. Rigorous Experimental Validation: The authors have meticulously demonstrated the significance of each component of their proposed methodology. Their ablation studies robustly highlight the indispensable nature of each segment for achieving commendable results. The comparisons made with existing works are also comprehensive and fair."
            },
            "weaknesses": {
                "value": "Presentation of Methodology: My primary concern lies in the method section's exposition. While the core idea of the proposed method is intuitive and straightforward, the section seems overburdened with intricate notations. This makes it somewhat challenging to follow. A more streamlined and intuitive presentation could significantly enhance the paper's readability and impact."
            },
            "questions": {
                "value": "1. While the author mentions that video-video similarity scores, leveraging the frozen CLIP encoder, can be pre-computed and re-utilized throughout training, Algorithm 1, line 12, indicates these scores are calculated during training. Could you clarify this discrepancy?\n\n\n2. Regarding the CLIP image encoder's usage for retrieval, is there any temporal modeling involved? Is the process simply averaging the frame-wise representation?\n\n\n3. Is it feasible to concurrently train the video retrieval model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Reviewer_o3GA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687730029,
        "cdate": 1698687730029,
        "tmdate": 1699636324707,
        "mdate": 1699636324707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T9QPxCMSzp",
        "forum": "IryGDUHxDE",
        "replyto": "IryGDUHxDE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_RvG7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_RvG7"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on a method for zero-shot action recognition based on autoregressive Vision & Language (V&L) models and suggests a new training framework that combines retrieval and self-training (RISE)\nThis approach uses pseudo-captions generated from the model itself, enhances the diversity of these pseudo-captions with a retrieval module based on CLIP, and shows competitive or better results compared to existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This study pioneers the development of a generative V&L FM for open-vocabulary video recognition, marking a significant stride for the community. \n\nNotably, the self-training procedure for REST requires no action-specific labels, allowing for RISE to be trained in a unsupervised fashion.\n\nThe efficacy of the proposed video-text retrieval is proved by the extensive experimental results."
            },
            "weaknesses": {
                "value": "While the paper offers valuable contributions, oversights such as typos, inconsistency, and table misalignments detract from its overall quality and can be distracting for readers. Maintaining consistent formatting throughout the paper can greatly enhance its quality.\n\n- The method is primarily described in prose, making it somewhat challenging to grasp. Incorporating relevant figures could make the content more accessible and easier to follow.\n- In Section 5's second paragraph, avoid using abbreviations for \"resolution.\"\n- The layouts of Tables 3 and 4 could benefit from better alignment.\n- For Table 2, the emphasis (bold) should be on 50.1. Row entries should maintain consistency, e.g., \"RISE(ours).\"\n- In Table 4 (b), I guess the value 49.8 of the column for \"$N_I=4$\" is a typo since the best result is reported as 49.7, and the bold formatting for 49.8 should be removed. it may cause misunderstanding of the experiment result.\n- The last sentence of section 3.2 uses the format \"$\\mathbb{R}^{(H \\cdot W + 1)\\times d}$.\" For uniformity, it should be revised to \"$\\mathbb{R}^{(H \\times W + 1)\\times d}$.\"\n- In Table 8, the partition line should begin at the number 2\n\nReproducibility:\n\nThe code has not been provided. Given the complexity of the method, it would be beneficial for reproducibility if the authors could supply the code for review."
            },
            "questions": {
                "value": "Would the authors be open to releasing the supporting code for their paper?\n\nAs far as I understand table 3 can be merged to table 4 as $N_I= 0$, do I understand correctly? and if the result of table 4-(b) is correct, is there any reason of not reporting the value with the better result of 49.8?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780526003,
        "cdate": 1698780526003,
        "tmdate": 1699636324633,
        "mdate": 1699636324633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TNsuLnHxHI",
        "forum": "IryGDUHxDE",
        "replyto": "IryGDUHxDE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_JWgN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_JWgN"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of zero-shot action recognition using generative models. Given a pretrained vision and language model that could generate captions, the proposed method utilizes CLIP and a retrieval-based self-training paradigm to improve the model without additional action labels. Experiments on three zero-shot and few-shot action recognition benchmarks show the efficacy of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is new for zero-shot action recognition.\n2. The generative approach in theory is more flexible than the discriminative methods and it can facilitate the open-vocabulary setting."
            },
            "weaknesses": {
                "value": "1. The performance gap between the proposed method and discriminative methods with similar pretraining models (like X-CLIP) is huge (on the standard zero-shot benchmark, 65.2 vs. 53.0, Table 6).\n2. The paper writing/clarity needs to be greatly improved: \n2.1 The overview figure (Figure 1) is not clear. There is no H, g_v, g_t, language modeling loss, etc. in the figure.\n2.2 In Table 2, \u201cDataset\u201d should be \u201cPretrained Dataset\u201d and for the RISE row shouldn\u2019t it be \u201cLAION-115M & Kinetics-400\u201d since it is initialized from BLIP?\n2.3 Minor presentation issues: \nWhat is FM (Line 1)? Foundation model? This has not been specified before.\nH \\dot W + 1 or H \\times W + 1? Be consistent."
            },
            "questions": {
                "value": "1. Inference speed comparison. I\u2019m curious about the inference speed difference between the discriminative and the proposed generative approach.\n2. I think \u201czero-shot\u201d and \u201copen-vocabulary\u201d are not entirely the same in terms of experimental evaluation. One can claim zero-shot ability on a limited set of classes but for open-vocab one needs to build a much larger test than 600 classes. More discussion on this is needed or the authors could avoid claiming \u201copen-vocabulary\u201d.\n\n-----Post-rebuttal comments: After reading other comments, I also have doubts about the novelty of this paper. The presentation of this paper is poor. The authors use zero-shot benchmarks for evaluation but claim \"open-vocabulary\". Therefore I am maintaining my score of leaning rejection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Reviewer_JWgN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803489575,
        "cdate": 1698803489575,
        "tmdate": 1700968971449,
        "mdate": 1700968971449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RLHfGMVyxh",
        "forum": "IryGDUHxDE",
        "replyto": "IryGDUHxDE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_bK27"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3680/Reviewer_bK27"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach to solve the problem of open-vocabulary action recognition using a generative auto-regressive model. The approach includes two modules: 1) Self-training using model generated pseudo-captions and no labels for open-world settings and 2) CLIP based Retrieval to introduce diversity in the pseudo-captions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe proposed approach shows good results on multiple datasets."
            },
            "weaknesses": {
                "value": "-\tThe novelty of the proposed approach is limited. Basically, it is a mixture of BLIP and CLIP iterating over multiple times throughout the training. CLIP will generate new text features because of new text generated by the updated BLIP model after each iteration. It looks like the model is finetuned on the Action Recognition dataset.\n-\t(Section 1) Table-1 - Are the approaches trained from scratch or fine-tuned? How is the testing done? What is zero-shot accuracy without any fine-tuning at all? Because in Section 3.1 it is mentioned that the model is initialized with pre-trained weights.\n-\t(Algorithm/Training Efficiency) - \n-\tFrom the algorithm it looks like the video-video and video-text similarity scores are calculated after every training iteration. Given the dataset size, how is it not a bottleneck, because it will be a huge operation to generate captions for all training videos and then calculate similarity between all data-points? (Lines 10 and 12).\n-\tWhy is it even needed to calculate video similarity after each iteration? It won\u2019t change as CLIP encoders are frozen.  \n-\t(Section-3.4) Similarity Calculation - Why not similarity be calculated using CLIP models adapted for video such as [1]? They are better encoders in the case of videos.\n-\t(Section 3.5) Retrieval - How does the proposed approach ensure that the captions selected each time introduce diversity via CLIP? Because the video-video similarity is calculated from a frozen encoder.  The video will be the same, only the text will be different for that same video because BLIP is finetuned on K400 now.\n-\tGeneral Concern - BLIP/BLIP-2 is supposed to be made for tasks like image-retrieval which means it has a better capability to find the images given the phrases. Does this setting is more like a video-text retrieval task rather than solving action recognition? Why is it mentioned again and again there are no labels? If I generate a text for UCF101 class from BLIP for applying eye makeup - BLIP generates multiple texts for a woman putting on makeup. Now if we compare the similarity of this phrase with classes of UCF-101 it will be sure that the ApplyEyeMake up class will have high scores at the inference time. I personally tested it with Huggingface APIs. \n-\t(Section 6.1) Ablation study:\n-\t(Table-2) Comparing an image model against a video model is not the right thing to do. For fair comparison, it should have been a BLIP model with adapter but no other additions. Only trained with class labels/original captions. \n\n\n[1] Rasheed, Hanoona et al. \u201cFine-tuned CLIP Models are Efficient Video Learners.\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 6545-6554.\n\n\n---------\n\nPOST REBUTTAL\n\n---------\n\nThanks to the authors for the clarifications; the authors addressed some of my concerns. However, the following issues are still there:\n\n1.\tOversimplification - I don\u2019t think using complex terms to present a simple approach could be mentioned as novelty and that\u2019s why I kept the presentation score low. There\u2019s very limited novelty to this work. It\u2019s just usage of the CLIP and BLIP image-based approach and there\u2019s no adaptation for Videos. An action recognition task involves temporal understanding. I don\u2019t see any novel component in the proposed approach, quantitatively or qualitatively which shows the temporal understanding. The reason why I\u2019m saying HMDB and UCF couldn\u2019t quantify temporal understanding is because if an action class is considered, the caption will always contain the original class description such as trampoline, cricket, etc. pick any frame from the video. Something-something will actually show that it focuses on the verb aspect which is considered as action. With the current approach, there\u2019s no novelty in terms of temporal understanding for the task which is the basis of video action understanding.\n\n2.\tLooking at the second contribution, caption diversity selection. I don\u2019t think it\u2019s the CLIP which is selecting a better caption  (CLIP based retrieval), it\u2019s just the better caption generated by BLIP as training progresses. There\u2019s no criteria to improve the selection other than similarity scores which depend on text generated by BLIP. Both of the contributions are pretty weak at this point of time, that\u2019s why I believe the paper needs more work.\n\n3.\tAblation on UCF-101 - If we look at Table 4 a and b and Table 5, HMDB-51 scores match, however, the ablation on both K and N shows a drop of 7% on UCF-101? Why would that happen?\n\n4.\tComputation Aspect - If we look at Appendix B, 316k+ unique pseudo caption generation for 241K videos is not normal. It\u2019s repeated at least three times as mentioned, training is broken down to 3 steps. Then, similarity is calculated. What\u2019s the matrix size for similarity calculation? There should be a dedicated section about it in discussion/ablation - why wouldn\u2019t this step increase computation? Right now, numbers are very huge and how it\u2019s trimmed is not clear. Top-k similar videos top-k similar captions. \n\nIn the current state, I can\u2019t change my rating. The paper requires following adaptations to make it better - a dedicated novelty to show adaptation for action recognition, qualitative visualization/quantitative analysis of temporal understanding,  filter of captions for better selection at each training stage, and a clear study on computational aspect given the size of dataset. The paper presentation also needs work."
            },
            "questions": {
                "value": "-\t(Table 4-b) Why is it shown only for 4 iterations? The authors did 6 iterations. 60 epochs and each iteration are done after 10 epochs.\n-\tDoes the model have any temporal understanding? Result on Something-Something v2 would have highlighted these capabilities. \n-\tAblations on UCF even in the appendix would have been more convincing. HMDB is a small dataset.\n-\tWhy not better backbone architectures such as BLIP-2 is used for the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3680/Reviewer_bK27"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814356674,
        "cdate": 1698814356674,
        "tmdate": 1701057368558,
        "mdate": 1701057368558,
        "license": "CC BY 4.0",
        "version": 2
    }
]