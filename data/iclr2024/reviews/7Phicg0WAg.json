[
    {
        "id": "G8TiXiJghU",
        "forum": "7Phicg0WAg",
        "replyto": "7Phicg0WAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_sAQH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_sAQH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel archtiecture to let LLM deeply understand an image in detail. \n\nThe proposed FlexCap consists of two important ideas:\n(1) Train a captioning model that can generate captions for specific bounding box inside an image.\n(2) Using Prompt engineering, this paper also propose a way to extend an LLM into a multimodal LLM by providing it different captions based on different bounding boxes.\n\nBy doing this, FlexCap-LLM is successfully turned into a multimodal agent that can perform various tasks including VQA."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The framework that combines LLM with a captioning model is a great alternative to save budget it takes to train a multimodal agent. \n\nWhile previous works train a single model that can serve as a multimodal agent, authors utilize captioning model and LLM to build a multimodal agent. \n\nWhile there can be possible performance degradation, this 2 stage model can still shows great performance compared to other one stage model."
            },
            "weaknesses": {
                "value": "While the 2-stage concept of FlexCap-LLM shows effectiveness in various downstream tasks, LLM can't directly understand an image in latent space.\n\nFrom the result of this work and previous works, it's not still sure that 1 stage or 2 stage is better than the other.\n\nFlexCap-LLM also relies on the performance of FlexCap's captioning performance. If FlexCap generates wrong caption for each bbox, this can lead to hallucination."
            },
            "questions": {
                "value": "As I mentioned in weakness section, FlexCap-LLM seems to rely on the captioning performance of FlexCap.\n \nIs there any way to filter out mismatching caption with input bounding box?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658072940,
        "cdate": 1698658072940,
        "tmdate": 1699636073951,
        "mdate": 1699636073951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zcJFuXiQI5",
        "forum": "7Phicg0WAg",
        "replyto": "7Phicg0WAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_N2tn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a model to generate controllable localized visual descriptions of bounding box, and proposed a large-scale dataset generated from web- scale image-text pairs. The experiments show that FlexCap exceeds SOTA performance on several benchmarks in the zero-shot setup, and achieves superior localized captioning performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. A large-scale dataset is generated by the author, which might enable the training of multi-modal tasks.\n2. The experimental results are abundant, eg. visual question answering tasks, localized captioning, and the proposed model achieves promising results on several tasks."
            },
            "weaknesses": {
                "value": "1. The architectures are mostly a compilation of pre-defined visual models and large language models. However, the novelty and inspiration for the latter works are questionable.\n\n2. Another concern is the precision or accuracy of the proposed large scale dataset. The authors claimed they use the filtered n-grams as text queries for pre-trained region proposal models (i.e. OWL- ViT (Minderer et al., 2022)) to extract boxes and select text-box pairs based on the similarity score (> 0.1). Therefore, the upper bound of precision of this dataset is determined by OWL- ViT, this might restrict the downstream tasks on the dataset.\n\n3. The implementation details should be removed to the experiments section."
            },
            "questions": {
                "value": "1. The description about loss is imprecise, please use some formulations to formalized it. Additionally, the authors claimed that 'The loss is ignored over length-prefix tokens and...'. However, as shown in Figure2, the prediction of length-prefix tokens should be 'grey', the first token in the sentence. Is this ignored in the training process?\n2. Why is the 'length conditioning' works? The length conditioning is implemented simply by adding an additional token that indicates the desired length of the output caption, and there is no training constrains. How to guarantee the additional token constrains the length?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741695879,
        "cdate": 1698741695879,
        "tmdate": 1699636073875,
        "mdate": 1699636073875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7w868MqJGi",
        "forum": "7Phicg0WAg",
        "replyto": "7Phicg0WAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
        ],
        "content": {
            "summary": {
                "value": "The work proposes a module called FlexCap, which can generate flexible-length of captions for a bounding-box. FlexCap is trained using a self-collected dataset containing variable length of captions. The experiments show that such captions can be used to prompt LLMs to achieve good performance for vision-language tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work proposed a new end-to-end model which can generate a variable length (as condition) of caption of a region. Such a model is new (as far as I know), and can be beneficial for future research.\n- The experiments demonstrates effectiveness of variable length captions by FlexCap."
            },
            "weaknesses": {
                "value": "-  In introduction, the authors claim that they diverge from Flamingo and BLIP2 (which use latent representations to connect LLMs), and use textual representation of images. However, the idea of using textual description of images to prompt LLMs for vision-language tasks is not new.\n- The major contribution, comparing to prior works, is producing a desired length of caption. Thus a baseline is missing: Simply use a caption model to predict (1) whole image description, (2) a detection model to predict the object in the box, and prompt a LLM conditioned on (1) and (2) to generate desired length of captions. This baseline circumvents the collecting of a new dataset and training a model like FlexCap.\n- In fact, I am wondering if other carefully filtered/designed localized captions could better prompt the LLM for those tasks. The work fails to compare or propose other baselines to demonstrate the superiority of FlexCap."
            },
            "questions": {
                "value": "- Will the dataset be released?\n- Will the pre-trained model (weights) be released?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1451/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1451/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1451/Reviewer_1skp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1451/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825167125,
        "cdate": 1698825167125,
        "tmdate": 1699636073775,
        "mdate": 1699636073775,
        "license": "CC BY 4.0",
        "version": 2
    }
]