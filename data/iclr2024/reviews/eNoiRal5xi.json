[
    {
        "id": "AHNBRmBned",
        "forum": "eNoiRal5xi",
        "replyto": "eNoiRal5xi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_ExGX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_ExGX"
        ],
        "content": {
            "summary": {
                "value": "This paper highlights the importance of exploring the perturbation in data space to enhance the performance of domain generalization based on the sharpness-aware minimization approach. The authors proposed an unknown domain inconsistency minimization approach, which combines data perturbation to generate the worst-case unseen domains and weight perturbation which is sharpness-aware minimization. They further showed some theoretical analysis for their algorithm. Experiments have shown the effectiveness of the algorithm.\n\n-----Post rebuttal\n\nI appreciate the efforts in writing the response. While the efficiency problem is not solved, I acknowledge the efforts in trying to address it. Therefore, I increase my score to 6."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of combining data perturbation with weight perturbation is interesting. The weight perturbation is a pretty \u201cstandard\u201d approach to learn robust models and the combination of data perturbation can further enhance such robustness.\n2. The paper is easy to understand and follow. The algorithm design is clear.\n3. The experiments have shown the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "1. The novelty of the approach is limited since the perturbation of input data is somewhat similar to existing approach called CrossGrad [Shankar et al., 2018] which also perturbs existing training data by reversing their gradient direction to generate unseen training domains. That being said, the paper can be seen as a combination of CrossGrad (by adding original data perturbation) and sharpness-aware minimization.\n2. There lacks guaranteed proof showing that by perturbing the input to the size of $\\rho$ (which is pretty similar to adversarial perturbation), the generated unseen domains can cover the entire space of unseen domains. It seems that with the size of $\\rho$ becomes larger, the generalization risk (Eq. 3) is smaller. However, in that way, the sampling and generating efficiency will be heavily impacted. There are no clear analysis on the parameter $\\rho$: what kind of perturbation can help the method already generalize the same as existing ones; better than existing ones, or worse than existing ones?\n3. Insufficient experiments. The common benchmark in domain generalization is DomainBed, but the paper did not use it; instead, it used CIFAR-10-C and PACS, which are clearly not sufficient. DomainBed has other challenging datasets inclusing CMNIST and DomainNet, which should be tested."
            },
            "questions": {
                "value": "1. I cannot see the advantage of UDIM in Figure 5: it seems that the loss landscape (bottom right) of UDIM is no better than SAGM and GAM? Can authors offer further explanations?\n2. On PACS, the results are not consistent with the original results from DomainBed. Authors should double check their results.\n3. What is the efficiency of the method?\n4. What do the generated domains look like?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Reviewer_ExGX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697961750208,
        "cdate": 1697961750208,
        "tmdate": 1700718676761,
        "mdate": 1700718676761,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ROJE9lV3d2",
        "forum": "eNoiRal5xi",
        "replyto": "eNoiRal5xi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_DtQi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_DtQi"
        ],
        "content": {
            "summary": {
                "value": "This paper, titled \"Unknown Domain Inconsistency Minimization for Domain Generalization,\" introduces an approach to improve domain generalization using Unknown Domain Inconsistency Minimization (UDIM) in combination with Sharpness-Aware Minimization (SAM) variants.  The paper's novelty lies in its approach to improving domain generalization by focusing on both parameter and data perturbed regions. UDIM is introduced as a novel concept, addressing the need for robust generalization to unknown domains, which is a critical issue in domain generalization. The idea of perturbing the instances in the source domain dataset to emulate unknown domains and aligning flat minima across domains is innovative. While SAM-based approaches have shown promise in DG, UDIM extends the concept to address specific shortcomings, which is a novel contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper appears to be technically sound. It provides a well-defined problem statement for domain generalization and formulates UDIM as an optimization objective. The authors validate the theoretical foundation of UDIM and provide empirical results across various benchmark datasets, demonstrating its effectiveness. The methodology is explained clearly, and the experiments are well-documented.\n\n2. The paper is well-structured and clearly written. It provides a thorough introduction, problem definition, and a detailed explanation of the proposed method. The methodology is presented step by step, and mathematical notations are used effectively. The experimental setup and results are also presented in a clear and organized manner. However, the paper is quite technical, and readers with less familiarity with domain generalization and machine learning might find some sections challenging to follow.\n\n\n3. The paper addresses an important challenge in domain generalization, namely, the ability of models to generalize to unknown domains. The proposed UDIM method appears to be effective in improving the performance of existing SAM-based approaches, as demonstrated through experimental results. The potential impact of this paper on the AI research community is significant, particularly in the field of domain generalization."
            },
            "weaknesses": {
                "value": "1. Baselines. The baselines are not enough because the latest DG method is Fisher, which is published at 2022."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Reviewer_DtQi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698055164983,
        "cdate": 1698055164983,
        "tmdate": 1700619995885,
        "mdate": 1700619995885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kASeONNapb",
        "forum": "eNoiRal5xi",
        "replyto": "eNoiRal5xi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_x9yc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_x9yc"
        ],
        "content": {
            "summary": {
                "value": "This paper considered both parameter- and data-  perturbation in domain generalization. The method is inspired by sharpness aware minimization (SAM). A theoretical analysis is further conducted to show the importance of different perturbations. Finally, the model is deployed in standard benchmarks with improved performance.\n\n------Post-rebuttal \nI would appreciate the rebuttal, which addressed my concerns. I would maintain a positive rating."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper considered a reasonable solution in domain generalization. Both parameter and data perturbations are conducted for a robust OOD generalization. \n2. The idea seems novel for me in some settings. \n3. Extensive empirical results.\n\nBased on these points, I would recommend a borderline positive."
            },
            "weaknesses": {
                "value": "1. Sometimes I find it a bit hard to understand the rationale of the proposed approach. Why do we need to consider both parameter and data perturbation? For example, in paper [1], a theoretical analysis is proposed, which is analogous to equation (11) as the parameter robust. \n2. Does the choice of data perturbation matter? We know we may face many different possible data-augmentation approaches. Which method(s) do you think should work in this scenario?\n3. Is it possible to consider the subgroup distribution shift in the context of fairness such as papers [2-3]? A short discussion could be great. \n\nReferences:\n\n[1] On the Benefits of Representation Regularization in Invariance based Domain Generalization. Machine Learning Journal (MLJ) 2022.\n\n[2] On Learning Fairness and Accuracy on Multiple Subgroups. Neural Information Processing Systems (NeurIPS) 2022.\n\n[3] Fair Representation Learning through Implicit Path Alignment. International Conference on Machine Learning (ICML) 2022."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "It could be good to discuss how the proposed method to encourage fairness in machine learning."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Reviewer_x9yc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698355038673,
        "cdate": 1698355038673,
        "tmdate": 1700773997132,
        "mdate": 1700773997132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ljQGeLnGoA",
        "forum": "eNoiRal5xi",
        "replyto": "eNoiRal5xi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_KWQw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3600/Reviewer_KWQw"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on domain generalization through extending the flattened loss landscape in the perturbed parameter space to the perturbed data space. Specifically, they first simulate the unknown domains via perturbing the source data, and then reduce the loss landscape inconsistency between source domains and the perturbed domains, thereby achieving robust generalization ability for the unobserved domain. Theoretical analysis and extensive experiments demonstrate the effectiveness and superiority of this method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.This work extends the parameter perturbation in existing SAM optimization to data perturbation, achieving loss landscape alignment between source domains and unknown domains. Experiments show the validity of the proposed objective.  \n\n2.This work establishes an upper bound for DG by merging SAM optimization with the proposed objective.\n\n3.The proposed objective can be combined with multiple SAM optimizers and further enhance their performance, demonstrating the necessity of the loss landscape consistency between source and unknown domains."
            },
            "weaknesses": {
                "value": "1.I believe that the proposed data perturbation method is consistent in both ideology and essence with traditional domain augmentation and adversarial attack techniques. So, what is the main difference and advantage of the proposed objective? And what if combining some domain augmentation techniques with the SAM optimizers?\n\n2.How to guarantee that the perturbed data is still meaningful, rather than generating some noisy samples? If so, will enforced the loss landscape alignment across domains bring negative impact? Besides, the unknown distribution may not necessarily be within the scope of perturbed data region.\n\n3.What is the sampling strategy for sampling data from $D_s$ to generate perturbed data?\n\n4.Is the optimization only conducted on the perturbed samples during the formal training phase? Would it be more stable to train on both the source and perturbed samples simultaneously?\n\n5.Since the training procedure involves gradient calculation, what is the time complexity after applying the BackPACK technique?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3600/Reviewer_KWQw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840127077,
        "cdate": 1698840127077,
        "tmdate": 1699636315455,
        "mdate": 1699636315455,
        "license": "CC BY 4.0",
        "version": 2
    }
]