[
    {
        "id": "P12JkqJdZo",
        "forum": "cJ3H9K7Mcb",
        "replyto": "cJ3H9K7Mcb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission506/Reviewer_8Ku5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission506/Reviewer_8Ku5"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that methods that improve OOD robustness may only improve robustness under small shifts, and in fact that methods that seem to be less effective when tested under small distribution shifts are actually more effective under large shifts. They demonstrate these claims using image classification experiments. They show that image classifiers pretrained with clip are more brittle to the type of distribution shifts they consider."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I was really delighted to read the message of this paper, as I think it's a very important one that needs to be considered by everyone who works on robustness. The framing is very strong, and I agreed with the message,  considered it very important to the robustness community, and thought it was well presented in this paper. I deeply appreciate the presentation of an apparent abrupt phase transition where the degree of shift becomes dramatically more challenging; perhaps it is this point where true \"OOD\" should be marked.\n\nThe clip result could be interesting, and I would encourage them to expand on it and explore with more experiments. It might fit better into a separate paper."
            },
            "weaknesses": {
                "value": "Unfortunately, the experiments are extremely weak. They consider a single task---image classification---and test with only a few types of distribution shift---rotation and gaussian noise for MNIST, compression and gaussian noise for Imagenet. I don't consider these settings to be representative, and in fact I don't even believe that models good on one type of shift will be good at another type of shift in general, let alone degree. \n\nI don't think that the framing here is complete. It is well-understood that \"OOD\" performance is often strictly correlated with iid performance (see https://arxiv.org/abs/2206.13089) but that certain natural shifts can be inversely correlated with iid performance (https://arxiv.org/abs/2209.00613). It is likely that this sort of inverse correlation in performance is associated with the difficulty of a shift (section 5.1.1 in https://arxiv.org/abs/2310.03646). Therefore, there is an ongoing discussion about the difference between different types of shifts and how much we can generalize from findings on small shifts.\n\nA minor thing I'm seeing is also a lack of good visualizations for some of the conclusions. I'd like to see a visualization specifically focused on the point that some models that are very good at small distribution shifts are bad at large distribution shifts, for example a plot that directly shows the correlation in performance for each model on these two settings.\n\nSection 4.3: Why not just use the variation across different distribution shift methods and correlate the performance on strong shifts with the performance on weak shifts? You aren\u2019t really measuring robustness to strong shift, you\u2019re just training a model on a multimodal distribution.\n\nSection 5: I really think that it\u2019s not fair to measure the robustness of clip against distribution shifts that don\u2019t really exist. I think that it is likely to help with robustness when it comes to novel tasks and out of distribution combinations (compositionally).\n\nOverall, I don't think that this paper is ready to be published, but I appreciate the goal a lot and would be very receptive to it with expanded experiments."
            },
            "questions": {
                "value": "Figure 3: Top three for each data point or top three over a specific data point? Is this the top three for in-distribution specifically? These factors change the expected outcome, because the degree of random variation is likely to differ for different degrees of shift.\n\n5.2: So this is basically just that training on more data creates overfitting to the training distribution? Would you describe it that way? I believe this is an area of discussion in distribution shift literature, but I don't know the specific citations that are best for it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission506/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698442035552,
        "cdate": 1698442035552,
        "tmdate": 1699635977283,
        "mdate": 1699635977283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7FpFDT8fLV",
        "forum": "cJ3H9K7Mcb",
        "replyto": "cJ3H9K7Mcb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission506/Reviewer_tBMy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission506/Reviewer_tBMy"
        ],
        "content": {
            "summary": {
                "value": "The paper studies robustness of domain generalization (DG) methods under different levels of distribution shift. The observations include that the robustness can be brittle, and methods that are more robust for smaller amounts of shift may not be as competitive in the presence of larger amounts of shift. It is also shown that large-scale pre-trained models can be sensitive to even very small distribution shifts of novel downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The questions asked in the paper regarding robustness are interesting and practically valuable. One would intuitively expect that methods that perform better for larger domain shifts also perform better for smaller domain shifts (or vice versa). This paper challenges the intuition via empirical evaluation on the selected datasets, which can be very valuable for future research in the area.\n* A large number of methods are evaluated and HPO is conducted to evaluate them more fairly.\n* The paper is well-written and easy to read."
            },
            "weaknesses": {
                "value": "* Most of the investigation is done on variations of MNIST that are likely to have only limited implications for real-world computer vision. For more reliable implications it would be recommended to use at least CIFAR-C or TinyImageNet-C that also include various levels of distribution shifts. Ideally a study would be conducted also on real-world data in addition to synthetic ones, even though those may be hard to find. To balance compute costs, a smaller number of methods could be evaluated.\n* A small number of noise types is studied so it would be good to cover a larger variety.\n* In Figure 3 left for ResNet-50 it could be argued that the behaviour seen generally is what one would expect based on intuition i.e. contrary to the message of the paper and meaning that models that perform better at stronger distribution shifts are also generally better at smaller distribution shifts. This would suggest that perhaps the brittleness may not be as common for larger models, i.e. the ones that are of primary interest."
            },
            "questions": {
                "value": "* Is the behaviour similar also for more realistic datasets such as CIFAR-C?\n* Is the described behaviour present also when using other types of noise (i.e. if we try a large variety of noises, do we still see the pattern on average)?\n* How exactly is the HPO conducted? Using 20 trials is appropriate in some cases, but it depends on how many hyperparameters there are and sometimes many more need to be sampled."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission506/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776552371,
        "cdate": 1698776552371,
        "tmdate": 1699635977204,
        "mdate": 1699635977204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tXZg5AxnbI",
        "forum": "cJ3H9K7Mcb",
        "replyto": "cJ3H9K7Mcb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission506/Reviewer_tbnw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission506/Reviewer_tbnw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a setup for a relevant analysis of out-of-distribution (OOD) performance. The authors propose an analysis of different degrees of distribution shift, for a more realistic understanding of the generalization capabilities (Fig. 1 is a clear illustration of the tackled issue). This enables a better estimation of real-world performance. \n\nThe authors introduce 4 datasets: NoisyMNIST, RotatedMNIST, NoisyImageNet15, and LR-ImageNet15. For each dataset, they have a clean set ($D_0$) and they continuously alter this set, simulating covariate shits, by adding Gaussian noise, rotating the image, or applying image degradation techniques. Through this process, they obtain additional sets: $D_1$, $D_2$, $D_3$, etc., which should capture continuously increasing degrees of covariate shift. \n\nThree main assumptions are analyzed:\n- robustness may not even extrapolate to slightly higher degrees of distribution shift\n- robustness at higher degrees does not always guarantee robustness at lower degrees\n- pre-trained representations are sensitive to novel downstream distribution shifts"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**S1** The tackled problem is of tremendous importance for the field.\n\n**S2** The proposed direction of analysis has great potential of shading light over the estimated real-world performance of models when dealing with covariate shifts.\n\n*S2.1* many relevant domain generalization algorithms have been considered  (20 different initializations and hyperparameters for each)\n\n*S2.2* analysis over both randomly initialized and pre-trained models, as well as over CNN and Transformer-based architectures."
            },
            "weaknesses": {
                "value": "**W1** The performance of many domain generalization approaches improves with the number of training domains, but, in the current paper, the training domains of DG methods are limited to two (also valid for ERM, and the behavior can be observed in Fig. 6)\nAlthough a comprehensive list of domain generalization methods is considered (Appendix A.2), results are reported only on a subset of the methods, with part of the analysis being performed solely on ERM. \n\n**W2** The analysis is performed over purely synthetic distribution shifts, considering a single type of shift.\n\n*W2.1* natural shifts do not follow this 'single type' shift -- previous works, like [1], introduce benchmarks that address the natural distribution shifts while studying the continuous degradation under various degrees of shift \n\n*W2.2* four datasets are proposed (NoisyMNIST, RotatedMNIST, NoisyImageNet15, and LR-ImageNet15), but they are used inconsistently for the analysis (e.g. Sec.4.2 - only NoisyMNIST, Sec.4.3 NoisyMNIST+RotatedMNIST)\n\n**W3** The paper does not benefit from a rigorous analysis of the results \n\n*W3.1* regarding the conclusion: \"robustness at higher degrees does not always guarantee robustness at lower degrees\" (Sec. 4.2) - see questions **Q1**, **Q2**, **Q3** and **Q4**\n\n*W3.2* regarding the conclusion: \"robustness may not even extrapolate to slightly higher degrees\" (Sec. 4.3) - see questions **Q5**, **Q6** and **Q7**\n\n**W4** The analysis of pre-trained models from Sec. 5 is shallow\n\n*W4.1* as Gaussian noise may be less common in the pre-training data, while rotations may be more common, it is hard to establish what is in-distribution(ID) and what is out-of-distribution(OOD)\n\n*W4.2* Regarding the assumption about CLIP from Sec. 5.2 - following the DFR paper [2], the embedding space of heavily pre-trained models is expected to contain both relevant and spurious features. The balance of spurious vs relevant correlations present in the training set used for linear probing may dictate the generalization capabilities. \n\n*W4.3* analysis on NoisyImageNet15 and LR-ImageNet15 shows that \"ImageNet pre-trained models are generally more robust than CLIP models\" -- the conclusion can be strongly influenced by the fact that ImageNet pre-trained models have already seen classes present in the two sets (you can also see that CLIP based models have significant lower performances even for the training sets, which is not true in general, when the downstream task is not based on ImageNet data)\n\n*W4.4* it seems that only ERM is used during linear probing or training of RI (randomly initialized) models  (see question **Q8**)\n\n*W4.5* RI performance is not reported for NoisyImageNet15 & LR-ImageNet15  (see question **Q9**)\n\n**W5** RotatedMNIST - the authors consider a large range of rotation angles, some of which may induce significant confusion between class labels (e.g. numbers 6 and 9)\nThe continuous covariate shift is less clear than in the case of the NoisyMNIST dataset. (see **Q7**)\n\n\n[1] Dragoi et al.  \"AnoShift: A distribution shift benchmark for unsupervised anomaly detection\" - NeurIPS 2022 \n[2] Izmailov et al. \"On feature learning in the presence of spurious correlations\" - NeurIPS 2022"
            },
            "questions": {
                "value": "**Q1** Sec.4.2., Fig. 3 (left) -- we only observe the results of the best 3 models at each degree. \n - the models selected for $D_0$ or $D_1$ display poor generalization capabilities (it is very likely that ERM models were the top performing ones) \n- meanwhile, the models selected for $D_{10}$ seem to be pretty robust (which is natural, as you select top-performing models on $D_{10}$, while the models were trained on $D_0$ and $D_1$)\n- models selected for $D_{10}$ have significantly lower gaps between $D_4$ and $D_5$, than the ones selected for $D_0$ or $D_1$, indicating that the presence of this gap may not be a general behavior \n\nCan you provide detailed (per domain generalization (DG) method) numerical results? You can also present ERM, average over all methods, and top-performing DG method. \n\n**Q2** Sec. 4.2, Fig. 3 (right) -- only part of the considered DG methods (as presented in Appendix A.2) are mentioned. \nOn what considerations did you choose the methods for which you reported the results?\n\n**Q3** According to Fig. 2, it seems very natural to observe a performance decrease between $D_4$, $D_5$, and $D_6$, as this is the point where the noise starts to interfere with the content of the image. \nHave you considered analyzing the performance decrease in correlation with the dataset distances (e.g. similar to the OTDD analysis from [3] or some perceptual similarity metrics [4])? The dataset distances can be computed considering either RGB or embedding spaces of pre-trainedd models.\nAlthough you have built the NoisyMNIST with a gradual noise increase, it is hard to establish how this noise is actually perceived by the model. (distance between $D_0$ and $D_1$ may be different than the distance between $D_4$ and $D_5$)\n\n**Q4** Sec.4.2, Fig. 3 (left) -- there is a significant difference between the generalization capabilities of the 4-layer CNN and ResNet-50. \nResNet-50 seems more prone to overfitting in the considered scenario, indicating that the model may not be appropriate for the considered data and the results of the analysis are less relevant.  \nHave you considered any in-between model capacity?\n\n**Q5** Sec. 4.3, Fig. 5 - results are presented only for ERM, ignoring DG methods.\nHave you performed this analysis considering also the DG methods?\n\n**Q6** Sec. 4.3, Fig. 5, NoisyMNIST, 4-layer CNN - there is a significant performance gap between $D_0$ and $D_8$, for the model trained on $D_0$ and $D_8$. This is not observed in any other setup. What is the intuition behind this result?\n\n**Q7** Sec. 4.3, Fig. 5 - there is a significant decrese on milder shifts only for RotatedMNIST, models trained on $D_0$ and $D_8$ and partially for models trained on $D_0$ and $D_4$. \nAs ERM results are reported and we observe no drop on NoisyMNIST, it seems that RotatedMNIST is not actually capturing a continous covariate shift.\nCan you perform a comparative analysis of the two sets maybe based on dataset distances (similar to **Q3**)?  \n\n**Q8** The analysis in Sec.5 is based solely on ERM? Have you considered using any DG method here?\n\n**Q9** RI models are not reported for NoisyImageNet15 and LR-ImageNet15. Have you performed those experiments? Is there any limitation regarding those experiments?\n\n[3] Alvarez-Melis and Fusi \"Geometric dataset distances via optimal transport\" -NeurIPS 2020\n[4] Zhang et al. \"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\" - CVPR 2018\n\n**Minor comments**:\n- citations look different than provided template, you should check them\n- Table 1 - it would be useful to add averages of over all the considered methods \n- Fig. 4 - a legend for the heatmap would be useful   \n- Fig. 5 - maybe use '0&2' instead of '0/2' to denote models trained on $D_0$ and $D_2$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission506/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783183417,
        "cdate": 1698783183417,
        "tmdate": 1699635977127,
        "mdate": 1699635977127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zDNLx6C1qz",
        "forum": "cJ3H9K7Mcb",
        "replyto": "cJ3H9K7Mcb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission506/Reviewer_V5DH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission506/Reviewer_V5DH"
        ],
        "content": {
            "summary": {
                "value": "The paper performs several studies to show that robustness at a certain degree of distribution shift is not indicative of its robustness at another degree of shift. This includes studying if models trained a low shifts can generalize to higher ones and vice versa. It also looks at the sensitivity of large pre-trained models like CLIP to different degrees of shifts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper was well written and easy to follow. The experimental setups were clear."
            },
            "weaknesses": {
                "value": "1. The studies conducted to support the argument that evaluations should be done over degrees of shifts is not very convincing.\n    - Would such controlled analysis be indicative of the performance of the model in the real world? Smoothly changing shifts implies that we should create synthetic shifts as it is expensive to create manually. Furthermore, only type one shift is changing at a time. In reality, shifts are correlated e.g., low light and iso noise or motion blur tends to occur together. \n    - The paper seems to assume that distribution shifts are smoothly changing. What about semantic/categorical shifts? e.g. background changes like in NICO, changing shapes etc.\n    - There are different types of shifts, but are these shifts comparable? It seems like in Fig 5, its comparing the trends from two types different shifts, why do we expect the trends to be the same?\n    - Generalization to higher or lower shifts (sec 4.2, 4.3). Why do we assume that generalization will occur? Training on a certain level of shift makes that shift ID, a different level of shift then becomes OOD to the model. Isnt this the same problem as training on clean data and testing on OOD data?\n2. It is not clear what the take home message is. \n    - The results seems to say that the OOD performance of the model depends on its inductive biases and it is hard to predict a model's performance OOD, which is quite generic. \n    - The overall goal seems to be more thorough evaluations (via fine grained degrees of shifts) for models to be useful in the real world. Real world datasets provides such evaluations but some cases it is impossible to have smoothly changing shifts e.g. smoothly changing animal occlusions in iwildcam. Furthermore, synthetic benchmarks like common corruptions already proposes evaluating on different degrees of shifts for each corruption (see 2), although as mentioned, they do not provide analysis at each level of corruption. \n    - Furthermore is it easy to anticipate the degree of shift at test-time such that such analysis would be useful? It seems more natural to anticipate the type of shift than its extent e.g., adverse weather, than how adverse the weather will be.\n3. \"Similar problems can also arise when only the aggregate performance across multiple degrees is examined (Hendrycks & Dietterich, 2019)\"\n    - The common corruptions benchmark measures performance over 5 degrees of shift and the paper measures it over 8/10, how do we know what is the ideal number of degrees to evaluate on?\n4. The statement \"robustness may be more brittle ...\" sounds contradictory as robustness is the opposite of brittle. It is our models that are brittle."
            },
            "questions": {
                "value": "See the points raised in weaknesses for the questions. \n\nA suggestion for having smoothly changing yet realistic shift is to perform evaluations on videos. The camera's viewpoint is smoothly changing, so are shifts in the real world."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission506/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793429836,
        "cdate": 1698793429836,
        "tmdate": 1699635977026,
        "mdate": 1699635977026,
        "license": "CC BY 4.0",
        "version": 2
    }
]