[
    {
        "id": "wiHrklMuy8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_uTep"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_uTep"
        ],
        "forum": "FRjflOWx2W",
        "replyto": "FRjflOWx2W",
        "content": {
            "summary": {
                "value": "This paper proposes to address open-set domain adaptation task using CLIP models. The main contribution of this paper is a method to enchance the classification performance of a zero-shot CLIP model. Technically, this work proposes two self-training losses with gradual adajustment of class-wise thresholds, reguralized by a contrastive loss. The conduct experiments on several open-set domain adaptation benchmarks and show improvements to the zero-shot method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The proposed gradual adjustment of class-wise thresholds and the self-training losses for both In and OOD samples are interesting to see.\n\n2. Experimetal results on several OSDA benchmarks are impressive.\n\n3. The presentaion of the paper is clear to read."
            },
            "weaknesses": {
                "value": "1. This learning setting of universal black-box domain adaptation [1] covers the learning setting of this work, and the learnning is somehow similar with a self-training framework. Thus, this setting is not longer new anyway. \n\n2. Hyperparameters analysis need comprehensive studies since we are curious about how they affect the results. Note that no validation set is available in this learning setting.\n\n3. Some key questions need to address well, see \"Questions\".\n\n[1] Deng, Bin, et al. \"On universal black-box domain adaptation.\" arXiv preprint arXiv:2104.04665 (2021)."
            },
            "questions": {
                "value": "1. What is the motivation of class-wise thresholds? What are the benifits when using a gradual adajustment of class-wise threshold?\n\n2. In the inference stage, how to detech OOD samples? As there are two thresholds: $\\delta_{in}$ and $\\delta_{out}$.\n\n3. When comparing to previous methods (e.g, In Table 2), I am cuious about whether they are run under the same CLIP backbone?\n\n4. How the zero-shot method is used for the open-set task?\n\n5. The self-training loss of In-distribution data likes the FixMatch [1] or not?\n\n[1] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" Advances in neural information processing systems 33 (2020): 596-608."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Reviewer_uTep"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697282672500,
        "cdate": 1697282672500,
        "tmdate": 1699636259307,
        "mdate": 1699636259307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KcyRaLTxYj",
        "forum": "FRjflOWx2W",
        "replyto": "FRjflOWx2W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_UHKB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_UHKB"
        ],
        "content": {
            "summary": {
                "value": "Overall, this paper proposes a new method UODA to tune CLIP to better perform classification tasks in scenarios where only unlabelled images from a certain domain are available and among these images OOD images possibly exist."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem that this paper tries to solve is interesting, i.e., I agree that there is a need to explore the usage of CLIP in open and real-world scenario.\n2. The results shown in the experimental section is awesome and the paper is well-written."
            },
            "weaknesses": {
                "value": "(See questions below for the details)"
            },
            "questions": {
                "value": "Despite the strengths I above-mentioned w.r.t. this paper, I think the current version of this paper does not reach the acceptance bar of ICLR and below are my concerns:\n\n1. The first concern I have w.r.t. this paper is whether or not it is suitable to call the problem that this paper is trying to solve a DA problem. From my basic understanding, if a problem is called a DA problem, no matter in which setting, there should at least be a source domain (even data from which is untouchable) and a target domain. However, in this paper, as stated in its table 1 that this setting neither uses a model trained on D_s or D_s itself, it seems to me that a source domain can be regarded as totally not exist. Thus, calling this setting a DA setting seems to be a confusing choice.\n\n2. The more crucial concern I have is that, if I take this submission out of the domain adaption scope, I kind of question the novelty of this paper then. Specifically, without the domain adaptation setting, this paper seems to use a combination of (1) OOD detection through CLIP's softmax vector, (2) adapter structure in CLIP, and (3) unsupervised view augmentation and maximal seperation. While I agree on the usefulness of each of these techniques, it seems that all of them are quite common techniques already, especially for (3) (I also list recent works that have used (1) and (2) below in [1-2]). Thus, besides that it can be not suitable to call the proposed setting a DA setting and compare it with DA  methods, it also seems that this proposed setting in this paper can be tackled via combining existing techniques. Thus, I kind of question its novelty.\n\n[1] CLIP-Adapter: Better Vision-Language Models with Feature Adapters\n\n[2] Delving into Out-of-Distribution Detection with Vision-Language Representations \n\n3. One remaining small question can be, it seems that the authors use two thresholds for ID and OOD. Then what if the ID threshold is far beyond the OOD threshold? How will the data in between of these two thresholds be handled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697528987976,
        "cdate": 1697528987976,
        "tmdate": 1699636259233,
        "mdate": 1699636259233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dXoKha1gpg",
        "forum": "FRjflOWx2W",
        "replyto": "FRjflOWx2W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_G7av"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_G7av"
        ],
        "content": {
            "summary": {
                "value": "This paper offers an intuitive solution for open-set domain adaption task, where the pretrained CLIP model combined with a lightweight adapter is finetuned to adapt the target domain tasks. In this method, adaptive class-wise thresholds, in-domain and out-domain self-disitillation losses are adopted for the finetuning. The reported experimental results illustrate that this method outperforms other domain adaption method."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ This method is very straightforward and easy to follow. \n+ It combines self-disitillation idea into the finetuning of pretraining CLIP model."
            },
            "weaknesses": {
                "value": "+ The novelty is somewhat limited. The proposed method simply finetunes the pretrained CLIP model with self-disitillation loss, which doesn't provide enough technique improvement for ICLR. \n\n+ The proposed method is only evaluated in small-scale datasets, Office-31, Office-Home and Visda, which is not enough for ICLR."
            },
            "questions": {
                "value": "+ The architecture of adapter module is the core part of this paper. However, its details are missing in the paper. Other hyper-parameters for model training are also missing.\n\n+ The clarification of `W, D, A, D, A` in Tables and so on should be clearly explained. \n\n+ Are there some errors about clarification, \"Source-free OSDA employs models trained on labeled source data but use only target data during the adaptation stage.\" in the caption of Table 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3125/Reviewer_G7av"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584925746,
        "cdate": 1698584925746,
        "tmdate": 1699636259113,
        "mdate": 1699636259113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kNBT0Ihka6",
        "forum": "FRjflOWx2W",
        "replyto": "FRjflOWx2W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_a6QH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3125/Reviewer_a6QH"
        ],
        "content": {
            "summary": {
                "value": "This work aims to enhance the OOD detection and image classification capabilities of CLIP by only utilizing open-set unlabeled data. The proposed unsupervised open-set task adaptation (UOTA) mainly includes three components: 1) dynamic threshold adjustment for OOD detection; 2) self-training with in-distribution data and negative learning with OOD data; 3) contrastive regularization (i.e., SimCLR). Extensive experiments on Office-31, Office-Home, VisDA, and DomainNet show the effectiveness of the proposed algorithm."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: The paper proposes a new setting, i.e., unsupervised open-set task adaptation, in which only unlabeled data from a specific origin and a pre-trained vision-language foundation model are available.\n\n**Quality**: The paper provides a thorough experimental evaluation of UOTA on four open-set domain adaptation benchmarks. The paper also conducts ablation studies to analyze the impact of different objectives. The paper demonstrates that UOTA can largely improve CLIP's performance and achieve SOTA results.\n\n**Clarity**: The paper also provides sufficient related settings to situate the contribution of UOTA in the context of existing literature on UDA, OSDA, SF-OSDA, etc.\n\n**Significance**: This paper aims to address the important and challenging problem of unsupervised open-set task adaptation, which is crucial for the industrialization of ML methods but has not been studied enough."
            },
            "weaknesses": {
                "value": "**Major Issues**:\n\n*Insufficient novelty and contribution*: The key factor of this work is to accurately detect OOD samples. OOD detection has been studied extensively, however, there is almost no discussion in this paper. For example, using the maximum softmax score to detect OOD samples is investigated in ref [a]. More related works should be discussed and compared with the proposed dynamic threshold adjustment for OOD detection [b,c,d,e,f,g,h]. \n\n*Insufficient results for experiments*: \n\n- Although the authors state in the main text, \"we update only this lightweight adapter, enabling computationally efficient training\", they provide no experimental results. More importantly, there is almost no discussion about the adapter (parameter-efficient fine-tuning). And the motivation is unclear as well.\n\n- For fair comparison, how does it compare with other settings (OSDA, SF-OSDA) using the proposed components? For example, using a source model pre-trained on ImageNet with additional training on labeled source data to perform SF-OSDA like ODAwVL.\n\n- In Appendix D, the authors have provided more detailed results, but it is important to provide the detailed results of \"Zero-shot\".\n\n\n**Minor Issues**:\n\n- In Tab. 5, it can be seen that the performance of Zeor-shot ViT-L/14 is the worst. It does not make sense.\n\n- Repeated references: Scaling up visual and vision-language representation learning with noisy text supervision.\n\n\n\nRefs:\n\n[a] Hendrycks et al. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. ICLR 2017.\n\n[b] Liang et al. Principled detection of out-of-distribution examples in neural networks, arXiv 2017.\n\n[c] DeVries et al. Learning confidence for out-of-distribution detection in neural networks. arXiv 2018.\n\n[d] Shalev et al. Out-of-distribution detection using multiple semantic label representations. NeurIPS 2018.\n\n[e] Liu et al. Energy-based out-of-distribution detection. NeurIPS 2020.\n\n[f] Yang et al. Generalized out-of-distribution detection: A survey. arXiv 2021.\n\n[g] Sun et al. Out-of-distribution detection with deep nearest neighbors. ICML 2022.\n\n[h] Sun et al. React: Out-of-distribution detection with rectified activations. NeurIPS 2021."
            },
            "questions": {
                "value": "My first concern is that \"unsupervised open-set task adaptation\" should consist of three parts: unsupervised, open-set, and task adaptation. This work fulfills the first two parts while the last one of the task adaptation is completely insufficient, especially all experiments are conducted on open-set domain adaptation benchmarks. In my view, the authors should testify the proposed method in true task adaptation benchmarks such as the Visual Task Adaptation Benchmark (VTAB). In the current version, it should be called source-free open-set domain adaptation with CLIP.\n\nThen, what is the advantage of the proposed setting in this work compared with SF-OSDA? Compared to a pre-trained CLIP model, a model trained on a source domain is more accessible and cheap.\n\nAt last, a small question, from the t-SNE visualization in Fig. 3, we can see that UOTA completely loses the discriminability on OOD samples (red), could we think that the classification capability of CLIP on in-distribution data has been improved by weakening the generalization of CLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670315548,
        "cdate": 1698670315548,
        "tmdate": 1699636259034,
        "mdate": 1699636259034,
        "license": "CC BY 4.0",
        "version": 2
    }
]