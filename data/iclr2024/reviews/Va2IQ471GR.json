[
    {
        "id": "j6ngPwOrJu",
        "forum": "Va2IQ471GR",
        "replyto": "Va2IQ471GR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_t4TL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_t4TL"
        ],
        "content": {
            "summary": {
                "value": "This article introduces a new technique, \\((\\epsilon, \\delta)\\)-approximate gradient flow, to prove the convergence of SVGD under KL-divergence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The convergence of SVGD under KL-divergence has always been an open question, making it a topic of great interest for theoretical researchers."
            },
            "weaknesses": {
                "value": "Assumption 6 is too strong, essentially assuming the properties of the $\\nabla\\log  p_t $ gradient directly. It determines the upper and lower bound for the time-dependent gradient and approximation, which almost brings the convergence."
            },
            "questions": {
                "value": "For Theorem 1, $ c_0 $ could be the most crucial constant describing the system's properties. Further explanation from the authors is hoped for.\n\nRegarding the discrete algorithm, the $ \\nabla \\log p_t $ term is obviously affected by iterations, which is considered the most significant challenge. The article seems to circumvent this issue with Assumption 6, necessitating more justifications.\n\nHow to explain the non-monotonic decrease of KL in Figure 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698277269463,
        "cdate": 1698277269463,
        "tmdate": 1699636551398,
        "mdate": 1699636551398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5Oz08v1qod",
        "forum": "Va2IQ471GR",
        "replyto": "Va2IQ471GR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_hQF6"
        ],
        "content": {
            "summary": {
                "value": "The authors construct a novel framework AGF to analyze convergence of SVGD in KL divergence. Based on this framework, the authors show SVGD can converge sublinearly in KL divergence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The AGF framework can include the most existing analysis of approximate function gradient flow. It is also promising to get better results of SVGD based on this framework. Numerical experiments seem to support the theoretical results."
            },
            "weaknesses": {
                "value": "1. The key assumption is not clear and hard to verify. And I think there are some mistakes in the proof (see Question). \n2. The authors do not really use AGF framework in the analysis of SVGD since $\\\\delta$ is always 0.\n3. Basically, the method is to prove that $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c\\_0 \\\\|\\\\nabla \\\\log\\\\frac{\\\\mu\\_t}{\\\\pi}\\\\|\\^2_{L^2(\\\\mu\\_t)}$ for some positive constant $c$ and apply LSI. The contribution is not significant enough as an ICLR paper."
            },
            "questions": {
                "value": "1. In Assumption 6, $\\\\{\\lambda_i\\\\}$ have positive lower bounds independent of $t$. However, Assumption 5 implies the kernel has finite trace and $\\sum_i \\lambda_i< \\infty$. This will contradict Assumption 6. How should Assumption 6 be understood here?\n2. In the proof of Theorem 1, the authors claim that \"m satisfying Eq. (65) does not monotonically increase w.r.t. the iteration t\" and \"This allows us to identify a largest m as m\u2032 that does not depend on t\". I don't think the first claim can imply the second one since m can be arbitrarily large even if not monotonically increasing.\n3. If it indeed holds that $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c\\_0 \\\\|\\\\nabla \\\\log\\\\frac{\\\\mu\\_t}{\\\\pi}\\\\|\\^2_{L^2(\\\\mu\\_t)}$, then by LSI, $I\\_{stein} (\\\\mu\\_t | \\\\pi) \\\\geq c KL(\\mu_t | \\pi)$ for some positive constant $c$. This seems just eq (7), which is not valid in most practical cases by Duncan et al. (2023). What is the difference between the authors'claims and Duncan et al. from this perspective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698309343518,
        "cdate": 1698309343518,
        "tmdate": 1699636551220,
        "mdate": 1699636551220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pPtjAMEd9n",
        "forum": "Va2IQ471GR",
        "replyto": "Va2IQ471GR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the convergence of the Stein variational gradient decent(SVGD) in the population limit. Assuming the target density satisfies the log-Sobolev inequality(LSI) and other assumptions on the kernel, the authors show the sub-linear convergence of SVGD population limit in KL-divergence, which is the first KL-divergence convergence result of SVGD population limit assuming LSI. Last, numerical experiments, employing RBF kernel, are provided in Section 5 and Appendix D to verify the sublinear convergence result."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is a paper with enough novelty. It combines existing results in SVGD and RKHS and prove the first convergence result of SVGD population limit in KL-divergence assuming LSI. \n2. The idea and proof techniques introduced in the paper is useful for analysis on other sampling algorithms. The idea of approximate gradient flow could be used in a large class of sampling algorithms based on Wasserstein gradient flow. The proof techniques related to RKHS could be used to study other variants of SVGD as well."
            },
            "weaknesses": {
                "value": "1. The convergence result in this paper doesn't apply to the SVGD algorithm. I understand that the convergence of the finite-particle SVGD in KL-divergence is an open problem. But I am wondering that whether the idea and results in this paper can help to understand convergence of finite-particle SVGD."
            },
            "questions": {
                "value": "Questions:\n1. In the first paragraph on page 1, what does the sentence ``While MCMC guarantees asymptotically obtaining unbiassed samples from the target distribution, it suffers from inefficiency due to the randomness'' mean?\n2. The proof argument between $(66)$ and $(67)$ is essential, but it is a little hard to understand. I have the following questions related to the proof:\n\n     (1) why is it enough to demonstrate that $m$ satisfying $(65)$ doesn't monotonically increase w.r.t. $t$? Even if $m$ is not monotonically increase w.r.t. $t$, $m$ could go to infinity as $t$ increases. Therefore, I think the argument should be ``it is enough to demonstrate that $m$ satisfying $(65)$ doesn't monotonically increase w.r.t. any subsequence of $t$''.\n\n     (2) I don't understand the argument `` If $m$ were to monotonically increase, the coefficient of $(b_i^{(l)})^2$ with larger indices would also increase''. How do we prove it from $(65)$?\nIt would be great if a more detailed proof could be added.\n\nComments:\n\n1. Although this paper focuses on convergence results for the infinite-particle SVGD, convergence results for finite-particle SVGD should be introduced in Section 1. For related results, I refer to the following two papers:\n\n          [1] Shi, Jiaxin, and Lester Mackey. \"A finite-particle convergence rate for stein variational gradient descent.\" arXiv preprint arXiv:2211.09721 (2022).\n\n          [2] Liu, Tianle, et al. \"Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.\" arXiv preprint arXiv:2305.14076 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_Spa3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698535159534,
        "cdate": 1698535159534,
        "tmdate": 1699636551122,
        "mdate": 1699636551122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ltfm8ti8cZ",
        "forum": "Va2IQ471GR",
        "replyto": "Va2IQ471GR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the convergence of Stein Variational Gradient Descent (SVGD) for the task of density matching when learning unknown probability densities. Specifically, the paper investigates different convergence criteria other than the Kernel Stein Discrepancy (KSD), which holds under highly restrictive assumptions, thus making it not applicable in practice. The novel criterion uses the KL divergence, the objective of SVGD, as an alternative to KSD. The core idea of the paper is to think of SVGD as an approximation of the gradient flow in the space of probability distributions. This approximation is introduced in the paper as the $(\\epsilon-\\delta)$ approximate gradient flow. With their new theoretical framework, the authors show that SVGD exhibits sub-linear convergence in the KL divergence.\n\nThe paper is well-written and scientifically sound. It provides a good level of preliminaries to follow the story of the paper and is complemented by some numerical results. The mathematical details are well-presented and structured. The authors often introduce and explain every step, referring to the appendix when necessary. I found the paper enjoyable to read.\n\nHowever, on a more critical note, I found the paper sometimes hard to parse, perhaps due to my lack of familiarity with the topic of optimal transport at such a deep level. In general, I find the derivation reasonable and mainly easy to follow. My main issue with the current submission, however, concerns the numerical experiments (not so informative) and the take-away message one has to gather from those results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written. It extensively reviews previous works and discusses how the present work fits the existing literature.\n- It provides some interesting theoretical insights about the convergence of SVGD in a more relaxed setting compared to the very restrictive assumptions required by the KSD convergence analysis.\n- The authors demonstrate the sublinear convergence of SVGD (Theorem 1), which represents the main result of this work."
            },
            "weaknesses": {
                "value": "- **Outlined proof is hard to grasp:** I found the proof outline (section 4.2) particularly dense and hard to follow. Perhaps having a more intuitive discussion and providing a more qualitative intuition would help people grasp the basic idea.\n- **Broader impact and applicability:** I think the paper misses a discussion about the practical impact of a given study. More specifically, I\u2019d be interested to understand how the results introduced in this work could be used when dealing with sampling tasks and approximating unnormalized target densities.\n- **Conclusions are missing:** The paper does not have a conclusion section where the main findings of the work are summarized. I find this to be a substantial lack of the paper as it prevents the reader from drawing the necessary conclusion from the study.\n- **Lack of numerical experiments:** While I appreciate the mathematical rigor and details of the manuscript, I perceive the numerical experiments sections (Sec. 5) to be lacking. The results seem consistent with the theory, but as far as I can tell, these are not sufficient to envision any substantial benefit from the framework presented in this work in a practical setting.\n- **Take-away message from numerical experiments:** From the current version, I find it hard to understand what the central message of the numerical experiments should be."
            },
            "questions": {
                "value": "- In the introduction, the second paragraph introduces SVGD as a method to compensate for the high variance and bias from MCMC and VI methods. I am naively wondering how this would compare to other debiasing schemes often used in the context of VI, such as neural importance sampling [1]. It would be helpful if the authors could elaborate a bit on this.\n\n- The paper always deals with the RBF kernel because it satisfies all the assumptions required for the theory to hold. However, I am wondering how easy it would be to apply the results presented in this paper to other widely adopted kernels which may not straightforwardly satisfy assumptions 1-6. Could the authors comment on this?\n\n- I understand the paper deals with the case of infinite particles for most of the discussion. This is relevant from a theoretical standpoint, although it cannot be achieved in practice. When it comes to the final discussion, where numerical experiments are shown, the number of particles is indeed considered to be finite, which prevents the bias $\\delta_t$ from vanishing. Moreover, the authors correctly state that the regime of an infinite (or even high) number of particles is practically not feasible from a computational perspective. In light of this, I wonder whether the residual bias one has to expect, resulting from the finite number of particles, can be estimated. Expanding the discussion around this point in the manuscript, I think, would be helpful. \n\n[1] [M\u00fcller, Thomas, et al. \"Neural importance sampling.\" ACM Transactions on Graphics (ToG) 38.5 (2019): 1-19.](https://arxiv.org/abs/1808.03856)\n\nMinor:\n\n- On page 3: The concept of the Radon-Nikodym derivative probably deserves a few more words. As I believe this paper could be interesting for a broad range of people who are not necessarily familiar with measure theory, I think introducing the R-N derivative with a few words or some references (perhaps just a footnote) would be helpful.\n\n- First paragraph of page 7: \u201chas shown the linear convergence in a continuous-time setting, the kernel function employed in **this** study is designed\u201d. In this sentence, \"this\" refers to the work of Huang et al. However, the sentence may confuse the reader as it may sound that **this study** refers to the present paper. I\u2019d recommend the authors reword this part slightly to avoid any potential confusion.\n\n- Last paragraph before section 4.2 \u201c[\u2026] sampling methods based on ker [\u2026]\u201d. I suspect *ker* might be a typo. If not, what do the authors mean by that?\n\n- How to obtain equation (13) from equation (9) is not trivial for me, even after looking at the appendix. Perhaps adding some intuition about it would be helpful to clarify this step, as I believe it represents one of the important results of the paper.\n\n- Below equation (16) the authors say \u201c[\u2026] we focus on the RKHS associated with $k$ given as $\\mathcal{H}=\\{\u2026}$\u201d. In the curly brackets, there\u2019s a sum over k, though none of the variables in the sum actually have a subscript k. Is this meant to be a sum over $i$ instead?\n\n- Section 4.3: kernelsatisfies -> kernel satisfies\n\n- I found the labels in Figure 1 to be too small.\n\n- It would be helpful to have the y-axis sharing the same range for the 2 left-most and 2 right-most plots of Figure 1. The same applies for the first and second rows of Figure 2. This would make it easier to compare visually."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5426/Reviewer_ZqiW",
                    "ICLR.cc/2024/Conference/Submission5426/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684979808,
        "cdate": 1698684979808,
        "tmdate": 1700661878550,
        "mdate": 1700661878550,
        "license": "CC BY 4.0",
        "version": 2
    }
]