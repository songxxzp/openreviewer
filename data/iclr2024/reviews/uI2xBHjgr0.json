[
    {
        "id": "dwqFAvA4BN",
        "forum": "uI2xBHjgr0",
        "replyto": "uI2xBHjgr0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_GqeY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_GqeY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors argue that existing Multimodal Large Language Models (MLLMs) lack the referential comprehension (RC) ability, i.e., identifying a specific object or area in images. Thus, they propose to represent the referring object with the coordinates of its bounding box and convert the coordinates into texts in a specific format. By regarding the coordinates as natural language, they can use a small instruction tuning dataset to transfer the knowledge in pretrained models. Furthermore, they propose a self-consistent bootstrapping method to extend dense object annotations into high-quality query-box pairs. The model is trained with a parameter-efficient tuning framework. Results show that the whole pipeline can significant improve the referential comprehension ability of MLLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The problem of improving the referential comprehension ability of MLLMs is a very important topic. This work puts more emphasis on this worth exploring direction."
            },
            "weaknesses": {
                "value": "+ The novelty of the whole pipeline is limited. Although many techniques are used, all techniques are well-studied and straightforward. For example: 1) using parameter efficient training to avoid overfitting; 2) building instruction dataset for instruction tuning; 3) transforming bounding boxes into coordinates into the text. The only \"new\" thing may be the self-consistent bootstrapping method. From my understanding, it looks more like a trick to filter data. Overall, I think the whole contribution is very limited."
            },
            "questions": {
                "value": "Based on the results in Table 2, the Pink model without * (without generated query-box pairs) shows very limited performance gains. It would be better to have more explanations to demonstrate the effectiveness of the proposed architecture. Otherwise, it feels like that the main performance gains come from the generated new dataset (i.e., Object365 with generated pairs)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844146785,
        "cdate": 1698844146785,
        "tmdate": 1699636155005,
        "mdate": 1699636155005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AETpfRVXWZ",
        "forum": "uI2xBHjgr0",
        "replyto": "uI2xBHjgr0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_z9P4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_z9P4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to enhance the referential comprehension (RC) ability to identify a specific object or area in images for multimodal large language models. The proposed method constructs the instruction tuning dataset with various designed RC tasks at a low cost by unleashing the potential of annotations in existing datasets. The proposed method achieves good performances on the public datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a method to treat the coordinates as natural language by representing the referring object in the image using the coordinates of its bounding box and converting the coordinates into texts in a specific format. The model is trained end-to-end with a parameter-efficient tuning framework that allows both modalities to benefit from multi-modal instruction tuning."
            },
            "weaknesses": {
                "value": "The novelty  is limited. The proposed methods convert the coordinates into texts in a specific format. This idea is widely adopted in the multimodal large language models for  specific objects, e.g.  [GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest], [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks].  The proposed methods just seem like the tricks of the object coordinates of the MLLMs"
            },
            "questions": {
                "value": "1. Please highlight the contribution and novelty of the proposed methods.\n2. Please add more details of the proposed adapters comparing the finetuning, LoRA and etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699115410655,
        "cdate": 1699115410655,
        "tmdate": 1699636154944,
        "mdate": 1699636154944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LMroEKv4OH",
        "forum": "uI2xBHjgr0",
        "replyto": "uI2xBHjgr0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_ArFY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2211/Reviewer_ArFY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Pink 1, a novel Multi-modal Large Language Model (MLLM) that enhances the Referential Comprehension (RC) capabilities of MLLMs. \n\nPink 1 leverages an existing method where the referring object in an image is represented using the coordinates of its bounding box, which are then converted into text in a specific format. This allows the MLLM to treat the coordinates as natural language.\n\nThe authors also propose a unique method to construct an instruction tuning dataset with a diverse range of RC tasks using annotations from existing datasets. This method allows for low-cost dataset construction.\n\nFurther, a self-consistent bootstrapping method is introduced to extend dense object annotations into high-quality referring-expression-bounding-box pairs. Pink 1 is trained end-to-end with a parameter-efficient tuning framework, resulting in fewer trainable parameters and less training data.\n\nThe experimental results demonstrate the superior performance of Pink 1 on both conventional vision-language tasks and RC tasks, highlighting the potential of this approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Solid experimental results that validate the proposed model's superior performance on both conventional vision-language tasks and RC tasks. The authors also provided a detailed comparison with other models under the fine-tuning setting, demonstrating the effectiveness of their model.\n\n2. While the method of representing the referring object in an image using the coordinates of its bounding box is not new, the authors introduced innovative methods such as a unique instruction tuning dataset construction and a self-consistent bootstrapping method.\n\n3. The paper is well-written and organized, providing clear definitions and explanations of the proposed model and methodologies."
            },
            "weaknesses": {
                "value": "1. As acknowledged by the authors, the Pink utilizes the approach of converting bounding box coordinates into text to understand the location of objects within an image. This technique, while not novel, could potentially impose limitations on the model's ability to perceive fine-grained details, particularly in complex images with numerous or overlapping objects.\n\n2. The novel instruction tuning dataset construction method and the self-consistent bootstrapping method proposed in this study are innovative. However, their effectiveness is largely dependent on the quality and diversity of the existing datasets used. There might be limitations when dealing with less common or more complex RC tasks not covered in the existing datasets.\n\n3. Although the model performs well on the tested datasets, it's unclear how well it would generalize to other types of RC tasks or datasets that are more complex or have different characteristics."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2211/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2211/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2211/Reviewer_ArFY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2211/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699256802122,
        "cdate": 1699256802122,
        "tmdate": 1699674829834,
        "mdate": 1699674829834,
        "license": "CC BY 4.0",
        "version": 2
    }
]