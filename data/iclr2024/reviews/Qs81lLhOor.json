[
    {
        "id": "Ytf1UOIbAY",
        "forum": "Qs81lLhOor",
        "replyto": "Qs81lLhOor",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_Qrvt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_Qrvt"
        ],
        "content": {
            "summary": {
                "value": "This work presents an efficient HGNN model named HGAMLP which aims to better fuse local and global knowledge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work is easy to follow.\n- The provided supplemental materials facilitate good reproducibility of this work.\n- The experiments are extensive, and the results are promising."
            },
            "weaknesses": {
                "value": "Strengths*\n- This work is easy to follow.\n- The provided supplemental materials facilitate good reproducibility of this work.\n- The experiments are extensive, and the results are promising.\n\n\nWeaknesses*\n- Some arguments are not convincing. Examples are as follows.\n\n\t(1) The authors claim that existing HGNN methods adopt a fixed knowledge extractor. However, the multihead attention for node-level aggregation in [1,3] can also be viewed as various knowledge extractors.\n\n\t(2) The authors claim that existing HGNN methods bury the graph structure information of the higher-order meta-paths and fail to fully leverage the higher-order global information. However, previous methods [2,4] are able to automatically discover any-order of meta-paths and effectively exploit the structure information conveyed by the discovered meta-paths.\n\n\t(3) The authors claim that scaling them to large graphs is challenging due to the high computational and storage costs of feature propagation and attention mechanisms. However, the previous HGNN method [4] has quasi-linear time complexity (scalability). Besides, the previous method [3] has proposed the HGSampling technique, which helps it scale to a large graph that has 178,663,927 nodes and 2,236,196,802 edges. Please see Table 1 of [4], the OAG dataset is much larger than the Ogbn-mag dataset used in this work.\n\n- Referring to Eq. (5), the proposed method needs to compute the graph structure information propagation for a k-hop meta-path by the matrix multiplication operation between a sequence of adjacency matrices. The time complexity of this operation is quite high, which should be included in the total time complexity.\n\n- In the abstract, the authors claim that their framework achieves the best performance on Ogbn-mag of Open Graph Benchmark. There is a risk of violating the anonymity of double-blind review since the real names appear on the OGB Leaderboard. Besides, I did not see \"HGAMLP\" on the Leaderboard.\n\n\nRefs:\n\n[1] [WWW 2019] [HAN] Heterogeneous Graph Attention Network\n\n[2] [NIPS 2019] [GTN] Graph Transformer Networks\n\n[3] [WWW 2020] [HGT] Heterogeneous Graph Transformer\n\n[4] [TKDE 2021] [ie-HGCN] Interpretable and Efficient Heterogeneous Graph Convolutional Network"
            },
            "questions": {
                "value": "Please reply to the weaknesses listed in the previous text box."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethics concerns about this work."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697708819343,
        "cdate": 1697708819343,
        "tmdate": 1699636413905,
        "mdate": 1699636413905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yht8aPJQRt",
        "forum": "Qs81lLhOor",
        "replyto": "Qs81lLhOor",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_W1yU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_W1yU"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the limitation of existing scalable HGNNs and proposes a new method called Heterogeneous Graph Attention Multi-Layer prceptron (HGAMLP), which employs a local multi-knowledge extractor, the de-redundancy mechanism and a node-adaptive weight adjustment mechanism to enhance the performance of HGNNs. Experimental results demonstrate the effectiveness of HGAMLP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work focuses on a valuable topic about the scalability of heterogeneous graph learning. Two problems of existing work are presented: fixed knowledge extractor and buried global information. \n\n2. New methods are given based on adequate analysis. The designed de-redundancy mechanism and node-adaptive weight adjustment mechanism are effective. \n\n3. HGAMLP achieves better performance and efficiency using uncomplicated model structure."
            },
            "weaknesses": {
                "value": "1. The proposed insights are not very convincing. For example, Figure 2 merely illustrates the performance of existing models drops or plateaus as the number of hops increases. It is unintuitive to show that this phenomenon is necessarily due to redundant low-order information.\n\n\n2. The major concern is the limited novelty. The degree-based normalization method for local multi-knowledge extractor, the mask mechanism for de-redundancy, and the adaptive weight adjustment mechanism for knowledge fusion are simple and commonly-used methods. \n\n\n3. More larger datasets should be used to demonstrate the scalability and efficiency of HGAMLP."
            },
            "questions": {
                "value": "1. The main reason that the information about low-order meta-paths is necessarily redundant for high-order graph structure information is not very clear. Is it possible that some of the higher-order information being used requires that lower-order information be considered together?\n\n2. The accuracy of existing scalable HGNNs drops or plateaus as the number of hops increases. Could this be due to something other than low-order redundant information, such as limited information transfer over long distances, error, and noise accumulation, or over-smoothing?\n\n\n3. Is the number of local knowledge extractors $n$ you used a hyperparameter? Does this number affect the effectiveness of your model?\n\n4. Figure 8 only goes to show that the de-redundancy mechanism is critical to your devised model. However, the performance degradation of existing methods is not necessarily due to this reason."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664591170,
        "cdate": 1698664591170,
        "tmdate": 1699636413823,
        "mdate": 1699636413823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IBPUWNXkAl",
        "forum": "Qs81lLhOor",
        "replyto": "Qs81lLhOor",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_TS4L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4402/Reviewer_TS4L"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issues of redundancy properties of high-order meta-paths and the limitations of fixed knowledge extractors in existing scalable HGNNs. The proposed framework HGAMLP designs a de-redundancy mechanism to extract the pure high-order graph structure information. It also employs a local multi-knowledge extractor and a node-adaptive weight adjustment mechanism to fuse knowledge. Experiments on five graph datasets achieve SOTA performance in both accuracy and training speed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe study of scalable HGNNs is significant and practical. The proposed method is scalable and effective. \n2.\tThe experiments are sufficient to demonstrate the effectiveness of the proposed model.\n3.\tMost of the paper is easy to follow."
            },
            "weaknesses": {
                "value": "1.\tThe motivation is a little weak. The limitation of fixed knowledge extractor and buried global information is not so intuitive and is not closely related to the scalability of HGNNs. The observations and insights are not clear. For example, the stds in attention values are just cases in HGB and can\u2019t represent all attention-based methods. \n2.\tThe method lacks novelty. The de-redundancy mechanism is a little trivial and like a trick. The local multi-knowledge extractor is a simple extension of GNNs to HGNNs and lacks of dedicated design. The node-adaptive weight adjustment lacks of reasonable illustrations and reflects no efficient and scalable properties.\n3.\tThe experiment lacks a SOTA baseline in ogbn-mag leaderboard, i.e., LDMLP [1]. Besides, it only conducts experiments on one large-scale dataset. Since this paper focuses on scalable HGNNs, it\u2019s expected to add more large-scale datasets, e.g., WikiKG [2]. \n\nReference\n\n[1] Li et al. Long-range Dependency based Multi-Layer Perceptron for Heterogeneous Information Networks. 2023\n\n[2] https://ogb.stanford.edu/docs/lsc/"
            },
            "questions": {
                "value": "1.\tIt\u2019s better to compare the model performance under different aggregators in Figure 3. The stds of attention values depicted in Figure 3 can\u2019t illustrate the mean aggregator is limited. In Figure 4(a), an attention mechanism is just the solution for different nodes requiring different meta-paths to achieve high accuracy. Besides, it\u2019s confused that different colors denote different meta-paths in Figure 4(b). In my view, the color in each cell should denote the redundant degree as illustrated by the author.\n2.\tWhy the output of adaptive global knowledge fusion module is called X_local? What the global and local means in concrete? It\u2019s better to illustrate this more clearly. In Table 2, the trick column is suggested to be removed and adding illustrations in caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4402/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4402/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4402/Reviewer_TS4L"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738219648,
        "cdate": 1698738219648,
        "tmdate": 1699636413733,
        "mdate": 1699636413733,
        "license": "CC BY 4.0",
        "version": 2
    }
]