[
    {
        "id": "bOtCGEKSNt",
        "forum": "aCiSwOHnRD",
        "replyto": "aCiSwOHnRD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission830/Reviewer_affX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission830/Reviewer_affX"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of fine-tuning on limited downstream data. The authors propose to fine-tune the backbone network with two task heads, one of which uses dropout to mask random features. The produced logits from this head are then used as a supervision signal for the other head, called \"self-distill\". Experimental results show that this method can improve the final performance on various fine-tuning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method seems interesting and novel to me. Additionally, the method is simple and easy to implement. \n- The experimental results are promising, as the proposed method shows improvement over several base methods."
            },
            "weaknesses": {
                "value": "- The proposed method introduces several hyperparameters, including sparsity, temperature, and weight, and some of them are quite sensitive. However, the authors didn't discuss how they chose the hyperparameters.\n- The dataset evaluated in this paper is somewhat limited. There are numerous datasets designed for the studied scenario, such as the VTAB-1k benchmark, which contains 19 datasets. Authors may consider evaluating their method on more datasets.\n- The theoretical analysis provided in this paper seems vague to me. Please refer to the related part in my questions."
            },
            "questions": {
                "value": "- It is interesting to see in Fig. 3(a) that even when the sparsity is 0.1, the method still shows obvious improvement. What would happen if the sparsity is set to 0?\n- The paper provides an analysis to show that SEFAR reduces the upper bound of generalization error. However, it is hard for me to understand why the introduced sparse-feature-based regularization helps. How does the sparsity of features affect generalization? It seems that the paper should discuss this more rigorously.\n- The paper studies the situation where limited downstream data are provided, and the training set only contains 200 samples. What would happen if the training set is increased? Is there any further explanation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698598508255,
        "cdate": 1698598508255,
        "tmdate": 1699636010528,
        "mdate": 1699636010528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UodyU2vetf",
        "forum": "aCiSwOHnRD",
        "replyto": "aCiSwOHnRD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission830/Reviewer_Bx9c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission830/Reviewer_Bx9c"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a simple regularization method called SparsE-FeAture-based Regularization (SEFAR) for model fine-tuning, particularly when a limited amount of downstream data is available. The proposes SEFAR is simple and easy to implement on pre-trained models. The empirical experiments show it can achieves good results on down-streaming tasks with limited target data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The proposed method is very simple and easy to understand. \n1. The experiment results show that the proposed SEFAR enhances the performance of various fine-tuning methods."
            },
            "weaknesses": {
                "value": "1. The proposed method is kind of heuristic. It is actually an extension of dropout regularization (L_1 + L_2).  The self-distillation loss (either the CE loss or  MSE loss) is also a popular trick in machine learning tasks.  Therefore, the technical contribution of this paper is very limited. \n1. The theoretical analysis is proved for an oversimplified scenario: one-class (output) case with the MSE loss, which is very different from scenario (multi-class classification setting) studied in this paper. \n1. The organization of this paper can be improved, e.g., Algorithm 1, Table 4, Figure 6."
            },
            "questions": {
                "value": "1. It is very strange that the t-SNE of each class in Fig.2c forms as disk. How to understand that ``randomly dropping some dimensions to generate sparse features may result in enhanced discriminative capabilities''? It seems interesting. \n1. Why the theoretical analysis is put in the experiment section. It is very strange. \n1. How Eq.3 is derived based on the proposed method in Eq.2?  Why it is a reasonable approximation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672109857,
        "cdate": 1698672109857,
        "tmdate": 1699636010432,
        "mdate": 1699636010432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uy9WKqedBu",
        "forum": "aCiSwOHnRD",
        "replyto": "aCiSwOHnRD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission830/Reviewer_6bB7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission830/Reviewer_6bB7"
        ],
        "content": {
            "summary": {
                "value": "The author claims to introduce a regularization method for limited data classification finetuning problem. The method is pluggable onto other finetuning methods. The method is to add another classification head that 1) has a mask on input to mask out some input features; 2) its prediction distills the original classification head using all features. This paper also provides some theoretical explanation with approximation about why this method could bring additional improvement. In addition, the author provides experiments on several datasets, compared with several finetuning methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-This paper try to tackle an important problem: finetuning with limited data. The experimental results show some improvement over several other methods.\n-The paper's writing overall is clear. But there are some important details missing. \n-There are quite some ablation study done. The experiments are over multiple dataset and multiple baseline methods, with proper confidence level stated. However the experiments are not well aligned with authors' claims."
            },
            "weaknesses": {
                "value": "Originality\n\nThis paper states the main contribution in terms of method originality to be \u201cadditional classification and self-distillation loss function computed using the sparse features\u201d. The method of additional classification head and self distillation from the additional head to the original head is a very common technique used in the community. Itself doesn\u2019t have any novelty. \nThe main difference here in my opinion is that the authors used a mask on the inputs of the additional head: I personally do not know papers with similar methods. However it is very unclear how the mask is generated (the maskgen algorithm mentioned): from the limited information that I can find in the paper, it seems that the mask is randomly generated, not learned or anything. In this sense, basically this method does a random dropout on a certain layer (i.e. the input of the additional head). It is not surprising that it can bring a regularization effect. The originality of this method seems weak to me. \nHowever, sometimes simple combination of existing methods may have a surprising effect, if the empirical study strongly supports it. Unfortunately as mentioned below in quality section, I don\u2019t think the empirical study is convincing enough. \n\nQuality\n\n- 4.2 ablation study: what about the impact of mask in additional head? I.e. If we have L1,L2,L3 and no mask (i.e. all feature used; or sparsity=0), what is the result? what about we have self-distillation and just apply dropout (on some layers or just all task heads)? Would we observe similar improvement as the proposed method?\n\nClarity\n\n- The maskgen algorithm is not properly described. Is it just randomly generates a mask with a given sparsity? How many sparsity parameters you tuned for the experiments given? (You mentioned that the sparsity has minimal impact on W2 in table 3. This is not what I would like to ask). My question is for major comparison table 1, for each row w/ SEFAR, how many sparsity parameters you have tuned? And what is their impact on the classification results (of W1)? An example is your figure 3(a). \n\nSignificance\n\nThe author claims that this paper targets to help finetuning problem with limited data. However I do not see any explanation or empirical study to support this.\n\n-\tDoes SEFAR helps less with finetuning problem with more data? Is there any reason theatrically? Or empirical evidence? \n\n-\tWhat is considered limited data? Why there is no related work or empirical study comparing with methods that also specifically address limited data finetuning methods?\n\nIt also claims that this method \u201csignificantly enhance the performance of any fine-tuning method\u201d. However, I see that the authors choose to compare with only several finetuning methods that are not claimed to be state-of-the-art (linear probing, L2-SP (Xuhong et al., 2018), DELTA (Li et al., 2019), and surgical fine-tuning (Lee et al., 2022)). To support such claim that the authors made, experiments with more recent SOTA methods, and on the datasets that those papers used are necessary.\n\n\n------------------\nafter seeing author's response:\nThank you authors for clarifying and adding additional ablation study that I asked. It does help improve the confidence level of the effectiveness of the proposed method on the given datasets. However my major concern on the significance section was not fully addressed. I still think the claims made (e.g. any finetuning method, limited data) are not supported. I keep my original score."
            },
            "questions": {
                "value": "4.2 ablation study what dataset? Is it EuroSAT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission830/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission830/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission830/Reviewer_6bB7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823503567,
        "cdate": 1698823503567,
        "tmdate": 1700695443783,
        "mdate": 1700695443783,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VkeNBfUVBG",
        "forum": "aCiSwOHnRD",
        "replyto": "aCiSwOHnRD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission830/Reviewer_f9tb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission830/Reviewer_f9tb"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a spare-feature-based regularization method for improving the fine-tuning efficiency. Their key idea is to use the prediction results based on the generated sparse features to distill the prediction results with the complete features. They finally formulate it into two regularizations for fine-tuning.  They evaluate the performance of their proposed method by conducting experiments on some datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to read. \n2. The experimental results imply the proposed method is technically correct. \n3. The proposed method is easy to implements."
            },
            "weaknesses": {
                "value": "1.\tThe authors conduct experiments on ResNet18 with ImageNet. It is insufficient. They are recommended to evaluate their proposed method on the language models and other larger CNN based models. \n2.\tIt is unclear to me whether different masks can lead to different performance and how to choose a good mask for the proposed method. Can we optimize the masks during finetuning to improve the proposed method?\n3.\tThe results in Figure 2 shows that the improvement achieved by the proposed method is marginal."
            },
            "questions": {
                "value": "Please refer to the comments about the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699515262803,
        "cdate": 1699515262803,
        "tmdate": 1699636010269,
        "mdate": 1699636010269,
        "license": "CC BY 4.0",
        "version": 2
    }
]