[
    {
        "id": "5CmRvnGMJz",
        "forum": "fg772k6x6U",
        "replyto": "fg772k6x6U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
        ],
        "content": {
            "summary": {
                "value": "This paper trains a model on human responses to generate attention maps that amplify artifacts in deepfake videos. They build two datasets of human labels highlighting areas perceived as fake."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, the paper is generally well-written, clearly structured, and quite easy to follow. The main strength of the paper is detecting human-relevant artifacts and amplifying them to increase human detection. This task and the technical route are rarely explored."
            },
            "weaknesses": {
                "value": "1. The words in Fig.1 are too small and hard to read, such as L_{human}.\n2. Currently, many deepfake methods are developed to identify fake faces and artifacts region. So, why do we still need to make artifacts more detectable to human observers? It is not clear in this paper.\n3. Lots of deepfake methods [1,2,3]  based on attention have been proposed. The author should compare the existing attention mechanisms and the attention used in this paper.\n\n[1] Multi-Attentional Deepfake Detection. CVPR2021\n[2] Detection of Deepfake Videos Using Long-Distance Attention. TNNLS2022\n[3] An Information Theoretic Approach for Attention-Driven Face Forgery Detection. ECCV2022"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Reviewer_iHin"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698119830663,
        "cdate": 1698119830663,
        "tmdate": 1699636156171,
        "mdate": 1699636156171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iqpKxSKd9V",
        "forum": "fg772k6x6U",
        "replyto": "fg772k6x6U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_7ZWE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_7ZWE"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to make deepfakes seems more 'fake' via the 'Deepfake Caricatures.' Specifically, the authors, relying on manual annotations and heatmaps, guide the model to learn the forged regions within the fake videos. Subsequently, a reconstruction module is applied to introduce additional noise and fluctuations into these regions, aiming to make it clear to viewers that the video is a forgery.\n\nIn general, this is an interesting research topic. However, I still have some concerns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed 'Deepfake Caricatures' is interesting in Deepfake Detection task.\n2. The authors construct an extensive dataset that contributes to both deepfake detection and localization tasks.\n3. The experiments validate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Application and Distinctiveness. The concept of 'Caricatures' in this paper aims to achieve an exaggerated appearance in forged videos. How can this 'Caricatures' concept be distinguished from genuine satirical videos? For instance, if an attacker uses another model trained on 'Caricatures' to introduce disturbances similar to the style in this paper into pristine videos, could it lead viewers to question the authenticity of the pristine videos? In other words, the approach presented in this paper may enhance the distinctiveness of certain fake videos compared to their corresponding real videos in the training dataset. However, does this also result in these fake videos being more challenging to distinguish from some other videos?\n\n2. More qualitative results should be presented in the main manuscript, even though the authors have provided numerous videos in the supplementary materials."
            },
            "questions": {
                "value": "The primary concern to address is the effectiveness of the method, as discussed in the weaknesses. Does this method enable viewers to effectively distinguish forged videos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698214730053,
        "cdate": 1698214730053,
        "tmdate": 1699636156072,
        "mdate": 1699636156072,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4ztVWoTruw",
        "forum": "fg772k6x6U",
        "replyto": "fg772k6x6U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a face forgery detection model incorporating artificial responses. The aim is to combine machine-learned features with human intuition. The authors use artificial intuition to feel distorted images, similar to an intuition-based data enhancement approach. The whole framework is mainly based on the self-attention mechanism."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors are to be commended for building attention maps by introducing human intuition. And the distorted faces are then generated using the generated attention map.\n- The authors introduce the generated attention map to the self-attention mechanism for better performance."
            },
            "weaknesses": {
                "value": "- As the authors state, the primary contribution of the framework is the CARICATURE GENERATION MODULE. However, human intuition can be relatively biased, especially for visually hidden tampering. After all, one of the problems DeepFake detection research is trying to solve is to detect videos that humans can't distinguish.\n- The general framework of the method is not NOVEL enough and still uses the usual self-attention mechanism. The focus is on the generation and role of caricatures. And caricatures are more akin to a way of utilizing human abilities to provide auxiliary samples. I'm not sure this comes at a greater cost.\n- The experimental results are not solid enough, there are many new methods published in 2023 and the authors should refer to more recent work."
            },
            "questions": {
                "value": "I noticed a similar paper [1] on arxiv. The two papers adopted the same method and framework, but there were some differences in experimental results. CariNet18 and CariNet34 in [1] should be similar to Carinet-S and CariNet in this paper. The results of CariNet(CariNet34[1]) in the two papers are somewhat different (the performance of this paper is better). However, the results of the ablation study of CariNet-S(CariNet18[1]) (Table 3) were completely consistent.\n\nMy question is what changes in the method have brought about the improvement of performance, and why CariNet has been improved while Carinet-S remains unchanged.\n\n[1] https://arxiv.org/abs/2206.00535"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2227/Reviewer_Rnom"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657307193,
        "cdate": 1698657307193,
        "tmdate": 1699636156001,
        "mdate": 1699636156001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H3M2fImRVo",
        "forum": "fg772k6x6U",
        "replyto": "fg772k6x6U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_qLFh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2227/Reviewer_qLFh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method to not only classify deepfake videos but also create caricatures to amplify the artifact area, so that fake video detection can be more easily interpreted by humans.  More specifically, the proposed method consists of three modules: 1) artifact attention module, which is guided by human artifact annotations and to predict artifact region in the video 2) A classifier to classify whether the video is fake or real 3) A caricature module by amplifying the representation difference of consecutive frames around the artifact area, and then applying motion magnification to generate caricatures videos. Experiments are conducted under different scenarios including generalization on unseen datasets, generalization on unseen forgery methods, robustness on unseen perturbations, all showing promising results compared to existing baselines. Moreover, some ablation studies are also provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "--The idea of predicting artifact regions and further amplify artifact regions to create caricatures for deepfake detection are quite interesting.\n\n--The proposed method with the three modules look quite solid. Predicting attention map for artifacts supervised by  human annotated data is somewhat novel to the best of my knowledge.\n\n--The experiments are quite convincing. Promising results on generalization of unseen datasets, forgery methods, and perturbations make me want to try the proposed method.\n\n--The paper is organized quite well and presented clearly. I enjoy reading this paper."
            },
            "weaknesses": {
                "value": "--Will it help if the classifier takes caricature videos as input, or amplified representations as input?\n\n--Are there any evaluation for the caricatures? E.g., will there be real videos that people will think it is fake after seeing the caricatures? What are percentages when human agree with machines? etc"
            },
            "questions": {
                "value": "See weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper uses a data set of human artifact annotation."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802324424,
        "cdate": 1698802324424,
        "tmdate": 1699636155930,
        "mdate": 1699636155930,
        "license": "CC BY 4.0",
        "version": 2
    }
]