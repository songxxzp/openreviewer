[
    {
        "id": "QrV9g1icaF",
        "forum": "vR5h3cAfXS",
        "replyto": "vR5h3cAfXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_Wjrq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_Wjrq"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to understand robust overfitting based on the evolution of the data distribution induced by adversarial perturbation along the adversarial training (AT) trajectory.\nSpecifically, the authors empirically find that such an induced data distribution becomes more and more difficult for a model trained (via standard training, not AT) on it to generalize as the AT continues.\nA generalization upper bound is then proved for models obtained on the induced data distribution, which indicates that the generalization ability of those models is related to a newly proposed \"local dispersion\" term.\nThe authors then empirically demonstrate a correlation between the evolution of AT and the local dispersion."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is clearly written and easy to follow.\n\n2. The paper finds a new phenomenon that the data distribution induced by adversarial perturbation along AT is difficult for an ML model to generalize."
            },
            "weaknesses": {
                "value": "1. This paper seems trying to persuade readers that the proposed \"local dispersion\" plays an important role (even could be the cause) in robust overfitting, which however is misleading. Actually, this paper only EMPIRICALLY showed that there seems to be a correlation between local dispersion and robust overfitting, but unfortunately neither provided any theoretical explanation about the correlation nor explained how one can understand robust overfitting through this correlation. From this perspective, I would not treat \"local dispersion\" as a \"contribution\" of this paper.\n\n2. The authors put a lot of effort into theoretically and empirically explaining the \"generalization difficulty of the induced distribution\", which however is weird. According to the paper itself, this \"generalization difficulty\" is only a result of robust overfitting, but could not be used to explain robust overfitting itself. In other words, the \"generalization difficulty\" as well as the \"induced distribution\" do not matter to robust overfitting (although the authors said they matter according to the title of this paper).\n\n3. The contribution of this paper is very limited. As explained before, the only contribution of this paper is revealing the phenomenon that the data distribution induced by adversarial perturbation is difficult for an ML model to generalize. However, the authors did not show how one can leverage this phenomenon to explain AT, robust overfitting, or other aspects of machine learning. So I think the contribution of this paper is far below the standard of ICLR.\n\n4. The generalization bound in Theorem 1 is misleading. Specifically, the authors replaced the term $\\sup\\_{x,y,\\in\\mathrm{supp}(\\mathcal D)} | \\mathbb{E}\\_{\\rho} f(\\mathcal Q\\_{x,y,\\theta}(x+\\rho),y) |$ with an assumed upper bound $A$. However, this replaced term also depends on the induced distribution $\\mathcal Q\\_{x,y,\\theta}$, just like the local dispersion term. As a result, I think such a replacement (only replacing the term but not with the local dispersion term) is misleading and not appropriate."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271129836,
        "cdate": 1698271129836,
        "tmdate": 1699636129711,
        "mdate": 1699636129711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NbNyTk9YWI",
        "forum": "vR5h3cAfXS",
        "replyto": "vR5h3cAfXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_3NwP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_3NwP"
        ],
        "content": {
            "summary": {
                "value": "This paper study an important phenomenon in adversarial training, which is called robust overfitting. The author shows that robust overfitting can be attributed to the (adversarial) induced distribution and local dispersion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper study an important phenomenon in adversarial training, which is called robust overfitting. The author provides an observation that the following two generalization gaps looks similar:\n\n1) robust generalization gap of adversarial training\n2) standard generalization gap of standard training on the induce destruction in step t, $\\tilde{D}_t$.\n\nThis observation provides a good interpretation of robust overfitting and provides a way to understand robust overfitting by studying the evolution of $\\tilde{D}_t$.\n\nThe authors further provide a generalization analysis w.r.t.  $\\tilde{D}_t$, based on the uniform convergence bound."
            },
            "weaknesses": {
                "value": "This paper attributes robust overfitting to the 'local dispersion' represented by $\\tilde{\\gamma}_\\theta(x,y)$. An important factor in this quantity is the initialization of PGD attacks. However, the analysis appears to falter in a basic sanity check: zero-initialization PGD-adversarial training. Specifically, we examine a simplify PGD attack that always starts from the clean sample. From my observations (if I correct), such a PGD attack is already powerful, and robust overfitting appears even in this PGD-adversarial training scenario.\n\nIn this scenario, $\\tilde{\\gamma}_\\theta(x,y)=0$, leaving only the last two terms in the upper bound as per Theorem 1. Consequently, local dispersion is not related to robust overfitting. In my opinion, the uniform convergence bounds are overly large, and the first term may not be directly related to the generalization gap. Since the proof is based on a modification of Rademacher analysis, it would be better to show that the bound in Theorem 1 is tighter than a simpler, for example, Rademacher complexity bound. It seems that the proposed bound is not tighter by comparing $2A$ (proposed bound) with $2\\beta B_x$ (RC bound).\n\nIn contrast, if I my observation is wrong, it would be better if the authors provide experiments to dismiss it.\n\nTherefore, attributing robust overfitting to local dispersion is not very convincing to me. \n\nI believe this paper has merit but isn't ready for publication. While Section 4 is insightful, Section 5 requires revisions. I recommend the authors address these concerns. I'm willing to engage in a discussion with the authors and am flexible about adjusting my score.\n\nMinor suggestion:\n\nIn Figure 1 and 2, it would be better to directly show the trends of IDE test error v.s. robust test error, or  IDE generalization gap v.s. robust generalization gap."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727463587,
        "cdate": 1698727463587,
        "tmdate": 1699636129632,
        "mdate": 1699636129632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DkwBKmv8KN",
        "forum": "vR5h3cAfXS",
        "replyto": "vR5h3cAfXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_NYh7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_NYh7"
        ],
        "content": {
            "summary": {
                "value": "Adversarial training, despite obtaining robustness against adversarial attacks, suffers from overfitting. Historically, this was found to be surprising given that adversarially trained models are supposed to be \"robust\"; however, over time, it became clear that robustness against adversarial attacks and robustness against overfitting are different concepts. Hence, recent work started to focus on developing adversarially robust models that do not overfit (or, to battle the so-called \"robust overfitting\"). To this end, it has been crucial to first understand the key reasons behind the overfitting of adversarially robust models. This work investigates this question specifically for the PGD-AT (adversarial) training of neural networks, which, in the stage of \"nature's problem\" applies projected gradient ascent to perturb training instances within the allowed \"budget\" of perturbations [PGD], and in the stage of \"updating the model after the previous attack\" applies one iteration of gradient descent where the gradients are with respect to the losses of the perturbed instances. The reason for the so-called \"robust overfitting\" is hypothesized to be the overfitting over the distribution that is induced by the \"nature's problem\" over the iterations of PGD-AT, which means that throughout the iterations of PGD-AT, we start to overfit the distribution that is a result of perturbing the true feature values. To this end, the authors first show that the generalization error of the original dataset and the generalization error of the shifted distributions by nature throughout the iterations have a similar pattern. Then, to formally back this claim, they derive a theorem that upper bounds the deviation of the empirical risk and the true risk by some intuitive parameters of nature's data-shifting mechanism in addition to the parameters associated with the loss function."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I believe the authors study a very relevant and modern problem. Understanding the sources of robust overfitting is essential for developing intelligence models that are robust against multiple sources of errors (attacks, data shifts, etc.). The motivation behind the paper is clear. The experiments are also using very recent practice, including how they make a Reduced ImageNet as the recent works."
            },
            "weaknesses": {
                "value": "I have several concerns about the paper. I will first provide a high-level summary of my major concerns, and then provide an in-depth discussion of those, followed by some milder concerns. My recommendation is a rejection, however, I will have a very open mind and stay active during the discussion period as I can see the authors put a lot of effort into this paper and I want to make sure I do not overlook anything. \n-- -- \n**Summary of major concerns**\n1. I do not find the key message \"robust overfitting is a result of overfitting the adversarially induced data\" to be surprising. This should already be clear. This paper re-discovers this well-known effect for the specific PGD-AT algorithm.\n2. [Biggest concern] Not a single paper from distributionally robust optimization is cited. For years there have been many variants of Theorem 1, bounding the deviation of the empirical risk from the true risk under the assumption that the training set is sampled i.i.d. from a data-generating distribution. There are quite strong results out there. Not only do these works derive upper bounds on the risk deviations, but they also *do* develop and solve distributionally robust counterparts of adversarial training.\n3. The discussions are hard to follow, most conclusions are heuristic rather than theoretical, and there are several vague statements.\n-- -- \n**Major concern 1**\n\nI think the reader could be misled when we say robust overfitting is not understood. Firstly, I believe the definition of overfitting is more accurate in the recent ICML paper \"Certified Robust Neural Networks: Generalization and Corruption Resistance\" by Bennouna et al., where the source of overfitting is also explained accurately (especially, please see Section 3).\n\nI believe this paper (and most of the cited related work) is concerned with the structure of overfitting as a result of the iterations of a specific training algorithm. The structure of the PGD-AT algorithm, for example, drives the way the induced distribution behaves, as the nature attacks, optimization parameters are revised, and the nature attacks again upon seeing the previous parameters, and so on; so, the data distribution keeps deviating as we update our parameters. I would recommend a clear discussion of this.\n\nEmpirical risk minimization (ERM) minimizes $\\underset{(x,y) \\sim S}{\\mathbb{E}}[l_{\\theta}(x,y)]$ where $S$ is i.i.d. drawn from $\\mathcal{D}$, and we know it overfits if we compare it with $\\underset{(x,y) \\sim \\mathcal{D}}{\\mathbb{E}}[l_{\\theta}(x,y)]$. Keeping this in mind, note that when adversarial attacks happen, the true risk is not $\\underset{(x,y) \\sim \\mathcal{D}}{\\mathbb{E}}[l_{\\theta}(x,y)]$ anymore, rather it is $\\underset{(x,y) \\sim \\mathcal{D}}{\\mathbb{E}}[\\max_{v \\in \\mathbb{B}(x,\\varepsilon)} l_{\\theta}(v,y)]$. If we therefore define a new loss function $l^\\varepsilon_{\\theta} (x, y) :=  \\max_{v \\in \\mathbb{B}(x,\\varepsilon)} l_{\\theta}(v,y)$, we can see that training $\\underset{(x,y) \\sim S}{\\mathbb{E}}[l_{\\theta}(x,y)]$ while testing $\\underset{(x,y) \\sim \\mathcal{D}}{\\mathbb{E}}[l^\\varepsilon_{\\theta}(x,y)]$ is inconsistent. Therefore, the adversarial training paradigm of Madry et al. (2019) instead trains $\\underset{(x,y) \\sim S}{\\mathbb{E}}[l^\\varepsilon_{\\theta}(x,y)]$. Now, it must be clear that both training and test/true risks are expectations of $l^\\varepsilon_\\theta$ over $S$ and $\\mathcal{D}$. Adversarial training is thus another ERM problem, with a different loss that incorporates the adversarial perturbations. So, the whole overfitting theory is applicable. This discussion also shows why the \"induced distribution overfitting\" is not interesting, as if we revise the training datasets wrt the perturbations of $l^\\varepsilon$, then what this paper discovers is the classical overfitting. (Again, the paper cited above should provide a more accurate description). In other words, there is a 1:1 analogy between the overfitting while training $l^\\varepsilon_\\theta$ over $S$ versus keeping the loss as $l_\\theta$ and instead perturbing the underlying datapoints and observing overfitting on this perturbed data.\n-- -- \n**Major concern 2**\n\nThe previous discussion hints that robust overfitting, as the classic overfitting, is a result of the optimizer's curse, that is, we optimize the parameters over $S$ whereas it is i.i.d. sampled from a true data generating distribution $\\mathcal{D}$ and there will be a nonzero distance between $S$ and $\\mathcal{D}$. Hence, there is a lot of work that does distributionally robust (DR) training of models. \n\nI think this paper talks about concepts extremely relevant to DRO (data shifts, overfitting, etc.). Definition 1, for example, appears to be a simplified version of the Wasserstein-Kantorovich-like duality we always use in DRO. Theorem 1 bounds the difference between the empirical and true risk, which is studied extensively in the DRO literature. Theorem 1 can already be tightened in my view with the techniques in \"Wasserstein distributionally robust optimization: Theory and applications in machine learning\" by Kuhn et al. (2019) especially since the loss function is assumed to be Lipschitz continuous (then with proofs similar to the paper I just cited you can ignore the term B). Simply, the left-hand side of the quantity in theorem 1 can be upper bounded by $\\beta \\cdot \\varepsilon$ where $\\varepsilon$ is the Wasserstein distance between $S$ and $\\mathcal{D}$ -- in practice, $\\varepsilon$ is not known, but it is not an issue here since Theorem 1 is 'w.p. at least $1 - \\tau$' (that said, here probably the authors mean 'confidence') which means that with the same confidence, one can retrieve $\\varepsilon$ via the finite sample guarantees (see, e.g., Theorem 3.4 of \"Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations\" by Mohajerin Esfahani and Kuhn, 2018). \n\nOverall, I am concerned about the fact that DRO (and related fields) are not mentioned in this paper whereas the underlying goal is identical. Also, note that the DRO papers have stronger bounds than Theorem 1, and they also have algorithms to overcome the overfitting (or at least improve a little bit); hence I believe the contribution of this paper is limited.\n\n-- -- \n**Major concern 3**\n\nPlease find a (rather unorganized) list of points that I believe make the paper hard to follow.\n- In the abstract, the terms used will not be clear until one reads the paper. For example \"evolution of the data distribution\" will be defined later on and here it does not make meaning. Similarly \"checkpoints\", or \"certain local parameters\" are unclear yet. Also in the abstract, there is circular reasoning: initially, it is said this paper will explain why robust overfitting happens, but then \"We observe that the obtained models become increasingly harder to generalize when robust overfitting occurs\" -> Aren't we investigating the reason behind robust overfitting? \n- \"robust population risk $R_{\\mathcal{D}}^{\\text{rob}}$\" is defined wrongly. This is adversarial risk. Robust refers to the model that is trained adversarially robust. The definition the authors provide is for any $\\theta$, regardless of how they are trained. Overall, the paper uses \"robust\" and \"adversarial\" interchangeably, and this should be corrected.  \n- \"As such, one may expect the generalization performance of a model trained on $\\mathcal{D}_t$ is similar to that on $\\mathcal{D}$\" would contradict every paper that is concerned with distributional shifts and DRO.\n- Some terms are used before they are defined or abbreviated. Examples: \"PGD\" name is being used in the beginning before it is defined in S3.\n- There is some mystery behind discussions. In the beginning, for example, overfitting is said to be \"unexpected\" -> why? \n- Some statements are informal. Examples: \"A *great deal* of research effort has been spent\" is not followed by any references. On the next page, \"does robust overfitting have anything to do with this\" is also informal. \n- \"In Dong et al. (2021a), the authors observe the existence of label noise in adversarial training\" -> What is a label noise, could the authors please briefly clarify? That said, \"in X et al. (...)\" is the wrong format as \"X et al.\" refers to authors rather than the paper -- maybe use `citep`? The paragraph later mentions \"analysis of memorization\" which is also not clear as we are talking about an algorithm. The sentence  with \"small loss\" can also be revised.\n- Page 1, \"smoothing logits\" -> not clear. In general, when a paper is cited, please make sure to not assume the reader knows the specific terminology.\n- Section 2, after Figure 1 has \"Having established generalization difficulty of the induced distribution\". There are two issues here; (i) the sentence is really hard to grasp, what is the \"generalization difficulty of the induced distribution\"? (ii) after showing an experimental plot, the authors claimed something is established, but it is not. This pattern appears throughout the paper, plots or experiments are used as evidence for theoretical significance, but I find these not convincing. (Similarly, the second sentence of Section 5 starts with \"This suggests\".)\n- The paragraph at the end of page 2: Again, this is specific to the training method.\n- \"The most popular adversarial training\" is said to be PGD-AT, but this is specific to a class of learners, e.g., neural networks.\n- Could the authors cite some generic paper for the PGD-AT as described from the end of page 3 onward? \n- The described PGD-AT provides an approximate solution, not necessarily a strong solution. However, the conclusions drawn in this paper are for adversarial learning in general. This distinction could be clarified. \n- Section 6 is overall a heuristic approach, and informal. If the preceding sections were formal enough, this would be a good teaser, but at the current form, I think it makes the paper less formal.\n\n-- -- \n**Minor (typos, etc.)**\n- \"may indeed correlates\" \n- \"few percent\" on page 2 is vague.\n- \"dispersion property\" does not have a meaning at the beginning of the paper until it is defined at the end.\n- \"dispersivenss\" could be a typo.\n- \"perturbation decreases it magnitudes\" has typos.\n- Title of Section 2 \"Other Related Works\" could simply be \"Related Work\"\n- \"Gaussian and Bernuolli model\" -> model's'\n- The \"paper\" -> the work\n- \"an one-step PGD\" -> a\n- $\\mathrm{sgn}$ is a *vector* of signs, please clarify somewhere. \n- Page 7, before \"More specifically\" there is a missing period. \n- In several places the quotes start with the wrong direction. Perhaps use `` '' in LaTeX instead of \" \". \n- \"CIFAR 100(Krizhevsky et al., 2009)\" -> missing space\n- extra space after \"i.i.d.\" can be prevented by '\\@' in LaTeX"
            },
            "questions": {
                "value": "- Introduction: \"error probability in the predicted label for adversarially perturbed instances\" -> Is \"probability\" the correct word?\n- For clarity, can the authors specify what each \"error\" looks like, maybe via mathematical notation? In the above sentence, for example, \"the error in the predicted label for adversarially perturbed instances\" is mentioned, but the structure of adversarial attacks comes later. That said, I was also wondering, can the authors add a sentence discussing what part of their proofs would not work in the case of $\\ell_{p  \\neq \\infty}$ norm attacks? Finally, the $\\ell_p$ attacks, even when they are not restricted to $\\infty$-norms, typically deserve a quick discussion on why such attacks are interesting. \n- Page 2 says Dong et al and Yu et al. conflict 'to' each other (conflict \"with\" might perhaps be the better word): could the authors please explain a little bit about how these explanations vary? The sentence that follows says \"each shown to\" which probably is a typo.\n- Before the PGD training is mentioned, the paper uses the phrase \"along adversarial training\". I think the \"along\" implies that the algorithm is iterative and again it is specific to the structure of the PGD-AT algorithm. Could the authors please mention this at the beginning of the paper? Similarly, after that \"induces a new data distribution\" is not immediately clear; I only understood what is meant here after I read the paper fully once. Please also clarify \"at training step $t$\" terminology. \n- Right before Figure 1, could the authors please include the framework of IDE testing step by step? It does not need to be long but it would improve the readability. I understood the whole story much better in the first paragraph of Section 4.\n- Can the authors please cite WRN-34 and PRN-18 when they first appear? \n- End of Section 2: can you please elaborate on the \"mismatch between theoretical settings and real-world practices\"\n- Could the authors please clarify what on page 4 \"**distribution** of $(v,y) = $...\" refers to? Is it the distribution whose randomness comes from $\\rho$? Or do the authors mean the collection of training instances perturbed with respect to the underlying mechanism? \n- I guess the name \"checkpoints\" is coined because it is expensive to do this IDE check at every iteration. Would it perhaps be less confusing if we do not mention checkpoints, rather say \"IDE is recorded every $k$ iterations\"? \n- The experiments are repeated only 5 times, isn't this also contradicting the stochastic optimization literature/curse of dimensionality etc., as the choice of $\\rho$ is crucial? Maybe the SAA literature has an answer to this. (Similarly the 10 pairs of $\\rho, \\rho'$ need discussion).\n- Why are the parameters not CV'ed? It is ok to just cite a paper using similar combinations.\n- Can Figure 3 (a) be plotted over the error rate range [0, 0.1]? It is hard to read on this scale. \n- Is there a reason why Section 5 uses $f \\in \\mathcal{F}$ notation rather than keeping the adversarial attacks explicit? Would the proof make sense if $\\varepsilon = 0$ as well? Also, the $f \\in \\mathcal{F}$ refers to **any** function, can the authors make it clear how the adversarial setting is used in the proof? I guess the only property of $f$ about the adversarial training is the \"boundedness of perturbation-smoothed loss) -- isn't this very loose?\n- \"measurable\" -> With respect to which algebra? (I am asking because the norm space is explicitly defined)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852674460,
        "cdate": 1698852674460,
        "tmdate": 1699636129563,
        "mdate": 1699636129563,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "33R2WZu69e",
        "forum": "vR5h3cAfXS",
        "replyto": "vR5h3cAfXS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_jc1G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1981/Reviewer_jc1G"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the phenomenon of robust overfitting in adversarial training. Through experiments conducted on models at various checkpoints along the adversarial training process, the paper shows that there is a correlation between the robust generalization gap and testing error on a dataset induced by adversarial perturbations on the model at that time (called the IDE testing error). Through experiments, the paper shows that the IDE testing error to a quantity termed expected \"local dispersion\" of the adversarial perturbations on the dataset at that time. The paper then presents a generalization bound on adversarial risk in terms of this expected local dispersion, thus supporting their earlier experimental finding. Overall, it is shown that the adversarial training induced distribution plays a key role in robust overfitting. The paper also conducts additional experiments on the adversarial perturbations at various checkpoints along the adversarial training process. It is shown that the perturbations increase in dispersion as the training progresses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Analyzing robust overfitting by conducting experiments on models at various checkpoints along the adversarial training process is a novel idea, to the best of my knowledge. \n- The claim that adversarial training induced distribution plays a key role in robust overfitting is supported both via experiments, and via some theory. \n- The \"induced distribution experiments\" offer interesting insights into the adversarial training process, and deserve further study."
            },
            "weaknesses": {
                "value": "- The results in section 6 on the dispersion of perturbations along the adversarial training process, although interesting, do not fit well into the main message of the paper. In fact, the results of experiments on the evolution of $d_\\theta(x, y)$ seem to undercut the main message."
            },
            "questions": {
                "value": "- Why is $\\rho$ in (5) taken to be from the uniform distribution on the hypercube as opposed to any other distribution centered at $x$ and supported on $B(x, \\epsilon)$?\n- How different is $Q_{x, y, \\theta}$ from the perturbation $v$ in equation (5)? Can you please motivate why you study this \"averaged\" perturbation in place of directly studying $Q_{x, y, \\theta}$?\n- Using only 10 pairs of $(\\rho, \\rho')$ in the Monte-Carlo estimate of $\\gamma_{B(x, \\epsilon)}(Q_{x, y, \\theta})$ seems awefully inadequate especially because the hypercube $B(x, \\epsilon)$ is of large dimension. Typically, the number of samples you need to estimate an expectation over a hypercube increases exponentially with dimension. I have the same comment about the estimate of $d_\\theta(x,y)$. What is the variance you observe for these estimates?\n- Is there a reason for restricting the adversarial perturbations to $L_\\infty$ norm only, as opposed to keeping it general to any norm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1981/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699123264868,
        "cdate": 1699123264868,
        "tmdate": 1699636129507,
        "mdate": 1699636129507,
        "license": "CC BY 4.0",
        "version": 2
    }
]