[
    {
        "id": "hQAYFGs8Mw",
        "forum": "BLGQ3oqldb",
        "replyto": "BLGQ3oqldb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_9vvz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_9vvz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces LogicMP, a neuro-symbolic method designed to efficiently integrate first-order logic constraints (FOLCs) into neural networks. LogicMP performs mean-field variational inference over an MLN, and its computation is paralleled by leveraging the structure and symmetries in MLNs. The authors demonstrate the effectiveness and efficiency of LogicMP through empirical results in various domains. The results show that LogicMP outperforms the baselines in both performance and efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-motivated and easy to follow.\n- Using Einsum notation to formalize the message aggregation of first-order logic rules is very interesting.\n- The performance of the proposed method is better than previous work."
            },
            "weaknesses": {
                "value": "- Although LogicMP focuses on encoding FOLs into neural networks, it cannot handle existential quantifiers, which significantly limits its applicability.\n- The evaluation appears somewhat limited. Firstly, it only compares 3 neuro-symbolic baselines (SL, its variant SLrelax, SPL). These methods compile the constraint into a probabilistic circuit and then perform exact inference. Given that LogicMP performs approximate inference, comparing it with methods using exact inference seems somewhat unfair. Indeed, some other works encode the constraints and perform efficient approximate inference [1,2]. Lastly, since most previous work encodes propositional logic into neural networks, a more comprehensive evaluation on these previous benchmarks would enhance the paper's comprehensiveness and validity.\n\n[1] DL2: Training and Querying Neural Networks with Logic\n\n[2] Injecting Logical Constraints into Neural Networks via Straight-Through Estimators"
            },
            "questions": {
                "value": "- What is the expressiveness of LogicMP? Can it encode any quantifier-free first-order logic formula? Additionally, the notation in the first paragraph of Section 2 is somewhat confusing. For instance, is a specific structure required for $f$? In other words, should $f$ be represented in the form of a disjunction of logical atoms? Moreover, is the logical atom $\\mathtt{C}(e_1, e_2)$ a general form to represent any relation between $e_1$ and $e_2$?\n- Why not directly solve problem Eq.1? In problem Eq.1, we can perform the weighted counting in a parallel manner, rather than using sequential generation as required when solving problems Eq.2 and Eq.3. Moreover, quite a few techniques in fuzzy logic can efficiently handle the discrete potential function $\\phi_f(\\cdot)$, such as translating the disjunction into the product or the minimum."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4932/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4932/Reviewer_9vvz",
                    "ICLR.cc/2024/Conference/Submission4932/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637238518,
        "cdate": 1698637238518,
        "tmdate": 1700748494006,
        "mdate": 1700748494006,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IVHwTpvF8q",
        "forum": "BLGQ3oqldb",
        "replyto": "BLGQ3oqldb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_RzRh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_RzRh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a mean-field variational scheme for inference in Markov Logic Networks. The corresponding message passing scheme exploits some structure of the formulas and a tensor operation to speed-up a naive mean filed approximation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The technique is sound and the paper is generally well-written. \nExperiments are diverse."
            },
            "weaknesses": {
                "value": "The novelty of the paper is limited and cannot be assessed from the current paper. This is a major weakness,\n\nThe paper fails in positioning in the wider field of neuro-symbolic AI.\n\nThe paper claims to be the first method capable of encoding FOLC (pag. 2, \u201cContributions\u201d). This is not true. The authors themselves cite ExpressGNN. However, there are many other papers attempting at this. I will cite some here, but many more can be found following the corresponding citations:\u2028\nDeep Logic Models, Marra et  al, ECML 2019\u2028\nRelational Neural Machines, Marra et al, ECAI 2020\u2028\nNeuPSL: Neural Probabilistic Soft Logic, Pryor et al, 2023\u2028\nDeepPSL: End-to-End Perception and Reasoning, Dasaratha et al, IJCAI 2023\nBackpropagating Through MLNs, Betz et al, IJCLR 2021\u2028\u2028\n\nMany of these systems have CV and citation networks experiments."
            },
            "questions": {
                "value": "1) The paper mentions FOLC but it never defines them. All the examples, though, are definite clauses. Are non-definite clause supported? If yes, are you employing them in your experiments?\u2028\u2028\n\n2) Is there an impact in the size of the observed / non-observed split? Usually, there is a great imbalance between the two and it is not clear to me how this may impact the message passing / the pruning of messages."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4932/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4932/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4932/Reviewer_RzRh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675660016,
        "cdate": 1698675660016,
        "tmdate": 1699636479033,
        "mdate": 1699636479033,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K1cFe2BIrJ",
        "forum": "BLGQ3oqldb",
        "replyto": "BLGQ3oqldb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_XeZX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_XeZX"
        ],
        "content": {
            "summary": {
                "value": "A scalable inference method is proposed for MLNs using neural networks. The main idea is to use Mean Field iterations to perform approximate inference in MLNs. Further, since this relies on sending messages in a ground MLN which can be very large, messages are aggregated across symmetrical groundings to improve scalability. It is shown that this can be formalized using Einsum summation. The advantage with this approach is that the messages can be computed through parallel tensor operations. Experiments are performed on several different types of problems and comparisons are presented using state-of-the-art methods"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The use of Einsum to aggregate and parallelize ground MLN messages in MF seems to be a novel and interesting idea for scaling up inference through neural computations.\n- The experiments seem extensive and are performed on a variety of different problems showing generality of the approach"
            },
            "weaknesses": {
                "value": "- In terms of significance, there has been a long history of work in lifted inference with the same underlying principle of using symmetries to scale-up inference in MLNs. One of the key takeaways from such work (e.g. Broeck & Darwiche 2013) is that evidence can destroy symmetries in which case lifted inference reduces to ground inference (if guarantees on the inference results are required). Here, while the approach is scalable, would the same problem be encountered. In the related work section, it is mentioned that for earlier methods, \u201cThe latter consists of symmetric lifted algorithms which become inefficient with distinctive evidence\u201d. Does this mean that LogicMP does not have this issue? While the neural approximation can scale-up, I don\u2019t know if there is a principled way to trade-off between quality of inference results (due to approximation using einsum) and scalability. The experiments though show that using LogicMP in different cases yield good results."
            },
            "questions": {
                "value": "How does evidence affect Einsum computations? Does it break symmetries making it harder to parallelize?\n\nThere has been studies in databases regarding width of a FOL (Vardi)  (e.g. in a chain formula, the width is small). This has also been used to scale-up inference using CSPs (Venugopal et al. AAAI 2015, Sarkhel et al. IJCAI 2016). Would this be related to Einsum optimization?\n\nIn the experiments were the weights for the MLN formulas encoded by LogicMP learned (it is mentioned in one case that the weights was set to 1). How do these weights impact performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770330522,
        "cdate": 1698770330522,
        "tmdate": 1699636478944,
        "mdate": 1699636478944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ptNddcKPIS",
        "forum": "BLGQ3oqldb",
        "replyto": "BLGQ3oqldb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_svJm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4932/Reviewer_svJm"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel neural layer, called LogicMP, which can be plugged into any off-the-shelf neural network to encode constraints expressed in First Order Logic."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Relevance: \n\nThe paper deals with a very important problem that is of interest to the larger AI community. \n\nNovelty: \n\nThe paper introduces a novel layer. However, it fails to acknowledge other works that have integrated logical constraints into a neural network layer. Among the most relevant we find: \n- Nicholas Hoernle, Rafael-Michael Karampatsis, Vaishak Belle, and Kobi Gal. MultiplexNet: Towards fully satisfied logical constraints in neural networks. In Proc. of AAAI, 2022.\n- Eleonora Giunchiglia and Thomas Lukasiewicz. Multi-label classification neural networks with hard logical constraints. JAIR, 72, 2021.\n- Tao Li and Vivek Srikumar. Augmenting neural networks with first-order logic. In Proc. of ACL, 2019."
            },
            "weaknesses": {
                "value": "Clarity: \n\nOverall, I found the paper not very readable, and I think the authors should try to give more intuitions. \nSee below for some questions I had while reading the paper.\n\n- While the authors included an overview of Markic Logic Networks there are still some concepts that look a bit obscure. What does the weight associated with each formula represent? Is it a way of representing the importance assigned to the formula? Why do the authors need the open-world assumption? When explaining the MLNs, can you please add an example with: (i) an ML problem, (ii) what would the FOLC be in the problem, and (iii) what would be the observed and unobserved variables in the problem. \n\n- What does it mean that $Z_i$ is the partition function? Over what? \n\n- I am not sure how to read Table 1. Same applies for Figure 3.\n\n- How is it possible that the complexity does not depend on the number of formulas $|F|$? \n\n- Finally, are the constraints guaranteed to be satisfied or they are just incorporated?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4932/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840639696,
        "cdate": 1698840639696,
        "tmdate": 1699636478863,
        "mdate": 1699636478863,
        "license": "CC BY 4.0",
        "version": 2
    }
]