[
    {
        "id": "T1gwXCA0PL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_nsT1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_nsT1"
        ],
        "forum": "9zhHVyLY4K",
        "replyto": "9zhHVyLY4K",
        "content": {
            "summary": {
                "value": "The authors propose an unsupervised alignment approach that can apply a pre-trained model on other neural recordings to the new dataset. They mathematically prove their approach is optimal up to scaler in a simplified case. They validate their methods on a simulation and a real dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors propose a new unsupervised method to solve the transfer learning problem in neuroscience by re-using the temporal dynamics structure. They also have mathematical proof of the optimality of their method in a simplified case."
            },
            "weaknesses": {
                "value": "The experiment results are limited. The paper only shows results on 1 real dataset. There are some unclear parts in the paper, which I listed in the Questions section below."
            },
            "questions": {
                "value": "1. how do you model the latent dynamics p\u03b8(xt|xt\u22121)? Do you add any smooth constraints? In figure 2b, it seems that the results of the proposed approach are much smoother than ERDiff. \n2. what is FA + LDS method that you compare to in figure 2? Can you list a bit more details on how it's implemented? \n3. figure 2e, why does the mean k-step r^2 for Re-training decrease as the number of training samples increases? Intuitively the r^2 should increase as there are more training samples.\n4. what are the results showing in table 1? Are they results for the real neural dataset? I found the r^2 number in table 1 doesn't match the number in figure 4.\n5. in table 1, why the k-step MSE standard deviation is smaller than the stddev of MSE for ERDiff and the proposed method? \n6. in figure 4, why the proposed method performs better than ERDiff for poisson observations but not for the smoothed observations, especially Figure 4b (Top, left)?\n7. The proposed method assumes that prior dynamics p(xt | xt-1) is fixed, and the encoder q(x | y_embed) is fixed. I wonder if the authors have checked these two assumptions? For example, if you jointly train a model on two datasets and learn the latent dynamics and encoder, will they be similar to the results of training separate models on each dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697251359167,
        "cdate": 1697251359167,
        "tmdate": 1699636557757,
        "mdate": 1699636557757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2QeswtTasR",
        "forum": "9zhHVyLY4K",
        "replyto": "9zhHVyLY4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_H5ee"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_H5ee"
        ],
        "content": {
            "summary": {
                "value": "Here, the authors consider the problem posed by the existence of multiple data sets of neural time series from the same or similar tasks, asking whether it is possible to transfer learned latent dynamics from one to the other. They propose a model in which a sequential VAE is first trained on a large data set to establish a latent space ${x}_{1:T}$ (assuming a linear decoder). On a new data set, these dynamics are frozen, a new (linear) decoder is retrained, and the encoder is reused by learning a nonlinear mapping $g(w)$ from the new observations $w$ to the space of the training observations $y$. This requires only the trained encoder and does not require labeled examples. This method is compared with several other alignment proposals, where it produces both quantitative and qualitative improvements."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The problem of aligning multiple data sets across animals and experimental sessions is a major one in neuroscience. An ability to reuse or amortize model training across these would be of significant benefit.\n- The method is theoretically well-motivated and fairly flexible. It doesn't appear to require a particular architecture (apart from the linear decoder, which is a limitation of data availability as much as anything).\n- The approach appears to produce real qualitative improvements in the learned embedding (Figures 3 and 4)."
            },
            "weaknesses": {
                "value": "- The approach uses a fairly strong assumption that the latent dynamics really are shared across data sets, which all but implies a shared task setup. That is, it doesn't appear to be the case that a sufficiently large task-free data set in one mouse would facilitate embedding of mice performing a task-based behavior. It would be surprising if true, but this weakness should be acknowledged, since this limits the range of applicability.\n- Given the large literature on data alignment/domain adaptation both within and without neuroscience, it's a bit surprising that there are only two approaches compared in Table 1."
            },
            "questions": {
                "value": "- How flexible is this setup to the specific architecture chosen? It's mentioned that using an ELBO that measures log predictive probability several steps ahead is important to achive a good embedding, but it's not entirely clear to me why.\n- How complex are the learned dynamics in cases that work versus don't work? The monkey reach data typically have rotational dynamics. Do you see anything more complicated in other data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Reviewer_H5ee"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685662281,
        "cdate": 1698685662281,
        "tmdate": 1699636557649,
        "mdate": 1699636557649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ns1Siyi5Mb",
        "forum": "9zhHVyLY4K",
        "replyto": "9zhHVyLY4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_Jh7W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_Jh7W"
        ],
        "content": {
            "summary": {
                "value": "The authors present a novel algorithm for leveraging pre-trained seqVAEs for fitting neural recordings. This algorithm builds on the assumption that the neural data in question share the same neural dynamics as the ones used to train the pre-trained model. The two key components to this algorithms are: (1) learn a new observation model from latents to observations and (2) _implicitly_ learn an alignment function from the new observations to the old observations. The authors validated their algorithm on both synthetic data and a monkey reaching dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- originality\n    To the best of my knowledge this is a new approach for leveraging pre-trained generative models for fitting new data.\n- quality & clarity\n    - The paper is well-motivated and clearly written, notwithstanding some typos here and there (e.g., missing \"In contrast, [our method] does not require...\" in last sentence of section 2.)\n    - The experiments are relevant and convincing."
            },
            "weaknesses": {
                "value": "* I am not sure I follow why Proposition 1 implies good alignment? If the space of alignment function that the authors are trying to learn is linear (i.e., $g_\\theta(w) = \\theta w$), isn't it always the case that any $\\theta_\\star$ will be some linear offset away from the optimal alignment? Is this offset $B$ supposed to be small somehow? \n* The choice of alignment function $g_\\theta$ appears crucial to this paper, but there is very little discussion/evaluation about different choices. Specifically, if $g_\\theta$ is a point-wise nonlinear function approximator $g_\\theta(w_t)$ the authors are implicitly assuming that the latent dynamics are not just qualitatively similar, but **identifcal** between the new and old observations. However, if we make $g_\\theta$ to flexible (e.g., another full bi-directional RNN), then we are effectively re-learning the encoder."
            },
            "questions": {
                "value": "* I assume $q_\\phi(x_t, x_{t-1}|y_{1:T}) = q_\\phi(x_t|y_{1:T}) q_\\phi(x_{t-1}|y_{1:T})$ in equation 3? Might be worth clarifying.\n* Is the equation supposed to have $p_\\theta(x_t|x_{t-1:t-k})$ instead of $p_\\theta(x_t|x_{t-1})$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5470/Reviewer_Jh7W"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773575412,
        "cdate": 1698773575412,
        "tmdate": 1699636557537,
        "mdate": 1699636557537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2NOEwOENoK",
        "forum": "9zhHVyLY4K",
        "replyto": "9zhHVyLY4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_MYCT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5470/Reviewer_MYCT"
        ],
        "content": {
            "summary": {
                "value": "Stable and Effective inference models are crucial for decoding neural recordings, yet the need to train new models for each dataset due to variability is computationally demanding and inefficient. This is thus an interesting scientific question. This study introduces a novel alignment method that applies learned dynamics to new data, enabling the reuse of pre-trained models and facilitating the sharing of generative models across different distribution settings. The method's effectiveness is demonstrated by using a seqVAE trained on monkey behavior datasets, underscoring the significance of low-dimensional neural representations and offering a new perspective on handling the neural variability between sessions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper focuses on an insightful and scientific research question: alignment of neural recordings. Since the generalization ability of models is a great concern.\n2. The paper's words and figures are well-written and easy to follow.\n3. The proposed method is analytically tractable and is with good theoretical guarantees."
            },
            "weaknesses": {
                "value": "1. The baselines in the experimental part is too few, just the recently proposed SOTA method ERDiff [1]. There are many classical methods like [2] and [3].\n2. The spatio and temporal structure has already been noticed by the SOTA method ERDiff, and ERDiff also employs a generative model (score-based model) for alignment. Thus what's the new motivations and insights of your work?\n3. There should be more experiments and empirical results to support your method.\n\n[1] Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model.\n[2] Stabilizing brain-computer interfaces through alignment of latent dynamics.\n[3] Robust alignment of cross-session recordings of neural population activity."
            },
            "questions": {
                "value": "Please consider the things listed in the \u201cWeaknesses\u201d section.\nAlso please consider providing information regarding any potential future improvements."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809646162,
        "cdate": 1698809646162,
        "tmdate": 1699636557432,
        "mdate": 1699636557432,
        "license": "CC BY 4.0",
        "version": 2
    }
]