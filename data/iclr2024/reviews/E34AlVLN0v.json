[
    {
        "id": "VheTUFyeNe",
        "forum": "E34AlVLN0v",
        "replyto": "E34AlVLN0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes parallelizing evaluation and training of nonlinear sequential models using fixed point iteration methods. The paper proposes a method that restates nonlinear differential equations as fixed-point iterations with quadratic convergence, as in Newton's root finding method. The result is the DEER method. Speed and performance results are presented for a NeuralODE and GRU that are parallelized using DEER. Theoretical equivalences and convergence rates are established in the appendix."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed DEER method is well-motivated and presented clearly\n\n- The theoretical results seem sound, though I only skimmed the proofs to follow the arguments, rather than check every detail line-by-line\n\n- The method is general and can be applicable to a host of nonlinear differential equation methods such as neural ODEs and any nonlinear RNN (e.g. LSTM, GRU, etc) of broad interest to the sequence modeling community.\n\n- The method has the advantage of theoretically having quadratic convergence. Experimental results support the claim that this method should lead to nearly equivalent results (up to numerical precision) as evaluating the nonlinear ODE sequentially\n\n- For small hidden sizes, the empirical speed ups are substantial"
            },
            "weaknesses": {
                "value": "- The biggest weakness of the paper is the empirical results, particularly related to performance. Only two tasks are considered, a synthetic physics system where HNNs are trained and the EigenWorms task where a GRU is trained. \n  - For the EigenWorms task, the GRU does not significantly perform that much better than several of the baselines and is outperformed by others.  It is claimed that the DEER method enables faster training and thus experimentation to identify optimal GRU architectures. But is this the best the GRU could do? \n  -  It would have been interesting to see the DEER method applied to some of the other methods such as LEM or UnICORNN since they could presumably be trained with the DEER method also. Would they maintain the same performance? Could better performance be achieved with the DEER method since they could potentially be trained faster and thus be hyperparameter tuned more? An exploration of this would strengthen the paper and broaden the impact.\n  - A greater variety of tasks would increase the impact of the paper and potentially broaden the audience. E.g. there are many other common sequential \"long sequence\" tasks ranging from sequential MNIST, sequential CIFAR, to the LRA benchmarks. It would have been interesting to see how a nonlinear RNN such as GRU performed on LRA. \n\n- There are many interesting theoretical results pushed to the Appendix. The authors might potentially consider packaging some of the formal statements as propositions in the main paper, to signpost the results and guide the reader to understand the rigor behind the method as well as point to the location of the proofs more explicitly in the Appendix.\n\n- A weakness of the proposed method is the cubic complexity in the hidden size that the DEER method incurs. This limits scalability and the applicability of the method to larger scale systems. This is mentioned, but perhaps a further discussion of potential paths to address this could be interesting and helpful."
            },
            "questions": {
                "value": "1. Can you somehow quantify the claim that the DEER method can improve the performance of the GRU since it allows more hyperparameter tuning? Perhaps report the results of a single sequential GRU run, and then show the improvement that can be achieved by hyperparameter tuning the GRU for multiple runs (allowed by the parallelization of the DEER method).\n\n2. Can you apply the DEER method to LEM and/or UniCORNN and report the results and compare the runtimes? \n\n3. Can we add more tasks? Is it possible to run a GRU or other nonlinear RNN on Long Range Arena? Or does the cubic complexity of DEER prevent running large enough models for some of these tasks? Can we at least add other common RNN benchmarks such as sequential CIFAR?  \n\n4. Can you add a PDE example (discussed in the appendix)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_gw76"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698038533698,
        "cdate": 1698038533698,
        "tmdate": 1700590433269,
        "mdate": 1700590433269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZxyJtAOLys",
        "forum": "E34AlVLN0v",
        "replyto": "E34AlVLN0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel parallelization scheme for sequential models. The method works by recasting the model as a fixed point problem and using Newton's method to solve it in parallel. Involves computing the Jacobian and Hessian of the model explicitly to enable quadratic convergence. An adjoint method is developed to compute the gradient in parallel. Experimental results are shown in speeding up RNNs and neural ODEs, showing significant speedups especially for long sequence lengths. Extensive proofs are given."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The method clearly has large speedups in certain training regimes, particularly for long sequences and small batch sizes\n+ The theory of the method is clearly described, with proofs provided in the appendix.\n+ The proofs in the appendix are clearly described. \n+ The method is generally applicable to any sequential method, unlike previous methods which require specific architectures or structural assumptions.\n+ Convergence is quicker than previous methods which did not incorporate Jacobians\n+ Potentially, the method could also be applied at inference time to speed up sequence generation, not just at training time."
            },
            "weaknesses": {
                "value": "+ The practical importance of this method is somewhat unclear.\n  From a practical point of view, the DEER method is a mechanism to use more memory in order to speed up the forward and backwards pass.\n  Therefore, many of the experiments, particularly figure 2, are not really a fair comparison, as it's very common to increase the batch size until the memory is fully utilized.\n  In other words, the fairer comparison would be to fix the throughput (i.e. FLOP/s or memory usage) of DEER and the sequential method the same and to compare the two methods. I think it's quite plausible that for a given throughput, using DEER to do more minibatch steps\n  with smaller batch size can result in faster convergence. However, I don't believe this argument is made in the paper. Figure 4 gets close, but I don't think this is normalized in terms of\n  throughput. \n+ The increased memory usage of DEER at higher hidden dimensions seems a big stumbling block. As far as I am aware, a hidden dimension of 8 would be considered very small and impose a limit on the expressiveness\n  of a recurrent model. This issue is not really addressed in the work."
            },
            "questions": {
                "value": "+ Can you provide comparisons to the sequential method when the throughput of the GPU is held constant (e.g. the memory is approximately fully utilized)?\n+ How do RNNs with smaller hidden sizes compare to those with larger hidden sizes? Is it the case that the increased speed of DEER is able to offset for a reduction in hidden size that might be needed to use DEER?\n+ Would you be able to use an approximation for the Jacobian (low-rank, etc) to reduce the memory usage?\n+ How many iterations are typically required to converge? How does this compare against zero-order methods that don't use a Jacobian?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1788/Reviewer_EMpw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819038610,
        "cdate": 1698819038610,
        "tmdate": 1700693449681,
        "mdate": 1700693449681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IscNAb6uzz",
        "forum": "E34AlVLN0v",
        "replyto": "E34AlVLN0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_QySF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1788/Reviewer_QySF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for parallel evaluation of non-linear sequence models (neural differential equations, GRUs) as a limit case of differentiable multiple shooting. The key idea is to linearize the multiple shooting update equation and use exact parallel methods (e.g., parallel scans) to evaluate each step of the root-finding procedure. The method is evaluated on time series classifiction, hamiltonian neural ODE training, and benchmarked for efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The method is clearly presented. The authors do a good job contextualizing the approach with respect to direct multiple shooting, including recent work on adapting it to Neural ODEs (Appendix A.1, A.2)\n* The efficiency evaluation is quite thorough, investigating the effect of batch size, state dimension and sequence length."
            },
            "weaknesses": {
                "value": "* It is not clear whether this approach improves over direct multiple shooting (which is also applicable to GRUs). Is there some drawback to the linearization you introduce? Do you require more steps to converge?\n* The benchmarking is quite limited, both tasks showcased are small scale. The tasks do a good job at showing relative performance (end-to-end time) improvements, but they do not provide any insight on the method itself. Are there important hyperparameters, methods that could impact the final results and convergence? You mention mid-point in passing in Sec 3.3, could you elaborate on the choice of other interpolation methods that you have tried?"
            },
            "questions": {
                "value": "* How is the recurrent step baseline implemented here? Since you use JAX, have you tried to otimize the recurrence (jit-compile, or fuse)?\n* Although the scaling of the proposed method in with state dimension is quite unfavourable, many existing state-of-the-art sequence models (S4, H3, Hyena, RWKV) take a specific \"multi-SISO\" (single-input, single-output) form, where each channel of the model (in width) is assigned its own state. Although these models typically use linear recurrences that can simply apply the convolution theorem or a parallel-scan, the same structure can be used in the nonlinear case. Since these models are usually trained with small state dimensions (8 - 64), this would correspond to integrating many more systems in parallel (model width times batch size, rather than batch size as in the GRU case). Can the authors commend on whether DEER would be applicable in this case (and ideally show some preliminary result on a simple task?)  \n* How many steps do you need to converge with DEER? How much higher is the latency of one DEER evaluation vs one step of a corresponding GRU?\n* Can you comment on the different rates of approximation of this linearized version of multiple shooting, and regular multiple shooting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699044415270,
        "cdate": 1699044415270,
        "tmdate": 1699636108119,
        "mdate": 1699636108119,
        "license": "CC BY 4.0",
        "version": 2
    }
]