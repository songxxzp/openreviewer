[
    {
        "id": "jyOoUABdzf",
        "forum": "S6Xf70Y5CJ",
        "replyto": "S6Xf70Y5CJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission779/Reviewer_x2YN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission779/Reviewer_x2YN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple yet effective method for model selection under unsupervised domain adaptation. The authors introduce an ensemble-based method to better approximate the true labels on target domain to utilize model parameter selection. Comprehensive empirical studies are done over multiple UDA methods and datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is the first work to do model selection under UDA via ensembling. Ensembling is used for better approximation of target labels.\n2. Sufficient experiments were done to justify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "I have the following concerns that lowers my ratings for this work:\n1. The method seems quite trivial for me, as it trade memory and time for performance. Suppose we have $n$ set of hyper-parameters to select from, using this method still requires to perform $n$ training processes on source and target data, saving at least $n$ models, and $n$ inference processes on target data.\n2. Though effective in practice, the paper lacks proper analysis of why it works so well. I list one of my questions here: In $n$ models, if only one of them works well but the others do not, won't the $n-1$ models' predictions dominate the final prediction and the correct model be not selected? \n3. Overclaims: In terms of the analysis and results, some claimed facts are not true. For example, in section 4.2 PDA: \"Our EnsV outperforms all other model selection methods by a significant margin in terms of average accuracy\", which is not true given there results in Table 5 SAFN.\n4. While the authors mentioned EnsV is a \"stable\" method, I do not see a justification for stability.\n4. Is Figure 1 partly drawn by hand?\n5. Inconsistent result presentation. Table 4,8 misses some baseline comparisons, in contrast to other tables."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697746138521,
        "cdate": 1697746138521,
        "tmdate": 1699636005312,
        "mdate": 1699636005312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lq7AtXPHgC",
        "forum": "S6Xf70Y5CJ",
        "replyto": "S6Xf70Y5CJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission779/Reviewer_y3mw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission779/Reviewer_y3mw"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a model selection method for UDA. From a model candidate pool, the authors first use the prediction ensemble as a role model, which serves as an estimate of the oracle target performance, then choose the model that performs the most similarly to the role model. The proposed method consistently outperforms other model selection methods for UDA in various settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors perform extensive experiments to validate the empirical performance of the proposed algorithm. The algorithm indeed performs consistently compared with other model selection methods across various domain adaptation settings.\n\n2. The algorithm is simple and does not require significant computation overhead.\n\n3. The model selection problem is important and under-explored in the UDA community."
            },
            "weaknesses": {
                "value": "1. There is no reason to believe that the ensemble model is a reliable estimate of the target ground-truth. The \u201ctheoretical analysis\u201d in section 3.1 does not justify the point because \u201cno worse than the worst model\u201d can be achieved by any algorithm.\n\n2. The authors do not provide adequate analysis of the failure case of the method. The ensemble approach highly relies on the property of the candidate pool, and the authors should discuss this point in more detail about the success criteria of the proposed method."
            },
            "questions": {
                "value": "1. I do not understand the purpose of the \u201ctheoretical analysis\u201d part. Since the model will be selected from the candidate pool anyway, why do the authors prove that the role model cannot be worse than the worst candidate? The worst-case scenario is just to select the worst candidate, right? What is the message to convey here?\n\n2. For scenarios where unseen classes can be presented in the target domain, why is directly applying the ensemble still reliable? How is the ensemble score $f(\\theta,x)$ calculated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698361230349,
        "cdate": 1698361230349,
        "tmdate": 1699636005226,
        "mdate": 1699636005226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1xPaiwtQc2",
        "forum": "S6Xf70Y5CJ",
        "replyto": "S6Xf70Y5CJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission779/Reviewer_DSsh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission779/Reviewer_DSsh"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the model selection problem in UDA by introducing an ensemble-based method, EnsV. This approach guarantees performance that surpasses the worst candidate model. Experimental evaluations further establish EnsV's superior performance over existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe approach is straightforward to implement.\n2.\tEnsV is able to deal with covariate shift and label shift and does not require extra training.\n3.\tThe paper is clearly written."
            },
            "weaknesses": {
                "value": "1.\tAlthough EnsV ensures it won't select the worst-performing model, it doesn't guarantee optimal performance. Given the significance of performance in UDA, this limitation is noteworthy.\n2.\tProposition 1 is grounded on the assumption that NLL is the loss function. However, in UDA, a common loss function is the upper bound for the target classification loss, as outlined by Ben-David, Shai [1]. Does Proposition 1 still apply when not using NLL as the loss function?\n\n[1] Ben-David, Shai, et al. \"A theory of learning from different domains.\" Machine learning 79 (2010): 151-175."
            },
            "questions": {
                "value": "1.\tIn your model selection on various UDA methods, have you considered performing model selection on more recent methods with SOTA performance on Office-Home, such as PMtrans, MIC, ELS, SDAT, and CDTrans mentioned in [2]?\n\n[2] https://paperswithcode.com/sota/domain-adaptation-on-office-home\n\n2.\tIt would be beneficial to compare EnsV with some hyperparameter optimization tools, like the Ax Platform [3]. As the paper outlined in the Method section, the model selection problem in UDA is essentially equivalent to the hyperparameter selection challenge.\n\n[3] https://ax.dev/docs/why-ax.html\n\n3.\tRather than using an equal weight to ensemble models, have you considered assigning weights to potentially improve results?\n\n4.\tIn the section of \u201crobustness to bad candidates\u201d, if you randomly sample 5 checkpoints and ensemble these, how does the accuracy fluctuate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission779/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission779/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission779/Reviewer_DSsh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901341072,
        "cdate": 1698901341072,
        "tmdate": 1699636005164,
        "mdate": 1699636005164,
        "license": "CC BY 4.0",
        "version": 2
    }
]