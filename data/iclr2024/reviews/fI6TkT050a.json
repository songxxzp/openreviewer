[
    {
        "id": "0x9M4t7LV3",
        "forum": "fI6TkT050a",
        "replyto": "fI6TkT050a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_Fc8y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_Fc8y"
        ],
        "content": {
            "summary": {
                "value": "To reveal and evaluate the cognitive abilities of LLMs, this paper introduces Piaget\u2019s Theory of Cognitive Development (PTC) in psychology and construct a corresponding benchmark (CogLM). Through extensive experiments on various LLMs, it shows some interesting findings and provides guidance for the future development advanced abilities of LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well organized and the ideas are skillfully presented. However, this paper is not innovative enough and the key content is not clear enough. As far as I am concerned, the main advantages are as follows.\n\n1. The paper is easy to follow and understand. No matter the motivations, methods, experiments, and analysis in this paper, they are all flowing and graceful. \n\n2. Some findings and conclusions in this paper are interesting and meaningful. For example, \u201ccognitive abilities are inherent to the LLMs and cannot be enhanced through multi-step reasoning.\u201d, \u201cthe parameter size of LLMs can be analogous to a child\u2019s brain capacity, and cognitive abilities continuously improve with the development of the brain\u201d.\n\n3. Although it is not the first time that human evaluation methods are applied to LLMs and some details are not clearly explained, the solution seems to be effective and reasonable. This paper is able to evaluate the model from the perspective of psychology and provide some future development suggestions for LLMs."
            },
            "weaknesses": {
                "value": "I still have some concerns which, in my opinion, are the weaknesses:\n\n1. I am skeptical about using PTC to evaluate LLMs. The paper needs to further emphasize whether it is reasonable to use PTC to evaluate the model, or give some basis or experimental results. More about the deep introduction of PTC in psychology and its correlation with model evaluation need further elaboration, otherwise it will obviously not be able to convince readers by directly applying human evaluation datasets to the LLMs\u2019 evaluations.\n\n2. The expressions such as \u201ctracking\u201d, \u201cdifferent stages\u201d, \u201cevolution process\u201d in the paper are confusing. What exactly are the different stages of LLM? The paper mentions that \"PTC suggests that intelligence grows and develops through a series of stages.\" However, the LLM in the paper is fixed and does not \u201cgrow\u201d continuously. I think it would be best to truly evaluate the LLM at different stages. For example, can different amounts of data using in the pre-training and fine-tuning processes be regarded as different stages? It is more appropriate to apply PTC to provide evaluation services along the LLM growth cycle.\n\n\n3. The benchmark CogLM constructed based on PTC in this paper only uses multiple-choice questions as the assessment format. Is the generative ability also a manifestation of the LLMs\u2019 \"cognitive ability\"? I believe that most human cognitive testing programs are more about answering each test question fluently than multiple-choice questions. The model's complete response is clearly more informative than the selected options."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555153674,
        "cdate": 1698555153674,
        "tmdate": 1699637182914,
        "mdate": 1699637182914,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5nd1UsYcbG",
        "forum": "fI6TkT050a",
        "replyto": "fI6TkT050a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_BFZ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_BFZ5"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on an assessment of LLMs\u2019 cognitive levels. The authors use Piaget\u2019s Theory of Cognitive Development (PTC) as a tool to evaluate the \"cognitive abilities\" of various generative LLMs, and construct a benchmark test suite (CogLM) for this purpose. CogLM demonstrates the stages of PTC, except the first stage of PTC has not been considered (CogLM excludes reflexes and sensorimotor aspects of multimodal interaction). Multiple LLMs such as OPT, LLaMA2-chat-base, GPT-3.5-Turbo, and GPT-4 have been evaluated on CogLM. CogLM consists of questions in a multiple-choice format. Part of paper attempts to assess the consistency of CogLM with PTC and ensure the alignment between CogLM and PTC. In order to do so, humans between the age of 6 and 20 have been recruited to respond to CogLM questions. Spearman and Pearson correlation coefficients between the age of participants and the questionnaire scores have been computed to validate the effectiveness of the standard annotation guidelines.  Furthermore, the mapping function between the accuracy of response and age of participants have been computed through regression. The authors claim to show that GPT-4 exhibits human like cognitive abilities commensurate with a 20-year-old human."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The CogLM benchmark is novel and would likely to be valuable to the community for different evaluations and assessments.  I appreciate the level of work that must have gone into creating it.  There is a wide variety of experiments performed and results presented in a fairly clear and concise fashion."
            },
            "weaknesses": {
                "value": "Unfortunately there are a number of serious, I would say fatal, flaws to the paper, in that the entire endeavor rests on faulty assumptions and a non-trivial number of false statements.  The authors seem to set out with the goal to demonstrate that LLMs have human-level \"cognitive\" abilities and then construct an experimental protocol in order to arrive at that conclusion, which is begging the question.\n\n1) There is a fundamental category error being made in the statement that the abilities demonstrated by LLMs are fundamentally cognitive.  LLMs are sophisticated word-prediction machine, and this capacity happens to provide ability in tasks like summarization and question answering.  The statement \"However, there is little theoretical evidence regarding why and how this performance has been achieved\" is frankly not true.  There is plenty of theorizing that the ability to learn statistical patterns in language is precisely why LLMs are able to reproduce human-like outputs. Due to the large amount of data and large model size, they are good predictors of what a human would say, but doing so does not mean the same cognitive processes are at play.  A fairly simple neural net can accurately predict how a human would move in a given task.  It doesn't mean that network has the same sensorimotor processes as a human.  Prediction is not experience.  In general, the underlying definition of \u201ccognitive abilities\u201d being used here is faulty because LLMs are not cognitive systems, but rather statistical models based on sophisticated word occurrence correlation, to say nothing of studies of GPT\u2019s varying ability over time: e.g.,\nChen, L., Zaharia, M., & Zou, J. (2023). *How is ChatGPT's behavior changing over time?*. arXiv preprint arXiv:2307.09009.\n\n2) Speaking of sensorimotor, by excluding the reflex and sensorimotor aspects of multimodal interaction with the world destroys the scaffold upon which successive Piagetian stages are built.  Piaget\u2019s theory is that the development of the different stages are supervenient upon passing through the previous stages.  How can an LLM have achieved a certain level of cognitive functioning according to Piagetian theory if it was trained to that level without passing through the previous stages first?  Without sensorimotor capability, and LLM lacks the mechanism to experience the sensorimotor stage. This invalidates the entire construct of using PTC as a measure of cognitive ability, as the foundation has been pulled out.  This renders the entire comparison invalid, and makes PTC an inapprorpriate evaluation tool, since the entire first stage of PTC is explicitly the sensorimotor phase and everything else is scaffolded on top of that.\n\n3) Piaget is well-known but also quite contentious within dev-psych circles. Piagetian theory was based primarily on case studies and only some (not all) of his ideas have been supported through experimental methodologies.  This makes PTC in general a dubious metric by which to assess cognitive ability.\n\n4) GPT-2 is not similar to a human infant.  GPT-2's vocabulary is far larger than an infant but its ability to produce coherent, grammatical, on-topic, contextually-correct text is inferior to more recent models, as well as to toddlers given a toddler's vocabulary. GPT-2 is still an autoregressive next word predictor, explicitly trained for generation over text mostly written by adults.  This is not how human children learn language.  The fact that the data and compute power available at the time rendered GPT-2 inferior to GPT-4 does not make it at all like a human infant.  Did the authors forget how state of the art GPT-2 was considered at the time of its release?\n\n5) The benchmark is all multiple choice questions.  This would be fine if humans only thought in multiple choice questions but we know that is obviously not the case.\n\n6) In Section 3.4, the definition of accuracy is changed between construction of the benchmark and using it for LLM evaluation.  \"After confirming the positive correlation between answer accuracy and cognitive age, we aim to further construct the mapping function between them. We first make adjustments to the method of calculating accuracy.\"  After demonstrating a correlation between accuracy and cognitive age, the authors then change how they calculate accuracy?  The authors seem to be in search of a result that fits their narrative that LLMs are somehow following a human-like evolutionary path.\n\n7) An assertion in Section 4.2: \"Overall, the cognitive abilities of the OPT, LLaMA2-chat 70B, GPT-3.5-Turbo, and GPT4 models successively increase, and the performance of each model gradually declines with the increase of stage, consistent with humans.\" is not self-consistent, and the assertion that human cognitive performance declines as they progress through PTC stages, is also not true.\n\n8) The authors claim that GPT-3.5 and GPT-4 surpass humans at empathy.  Empathy is the ability to understand feelings and emotions.  Even if we allow that LLMs are accurate reproductions of the language faculty in the brain, emotions are governed by different regions and mechanisms.  LLMs can no more understand emotions than ELIZA could.\n\n9) Table 5 claims that the results are demonstrations of human-like cognitive abilities but these number measure the ability to correctly answer multiple choice questions.  The ability to answer multiple choice questions is not in anyway a global indicator of cognitive ability at the \u201cages\u201d being tested."
            },
            "questions": {
                "value": "1) \"As the most authoritative theory in the development of psychology\" (should \"the development of\" be \"development*al*.\"  Piaget is not as influential in other branches of psychology outside dev-psych.\n\n2) \"Out of the 207 completed questionnaires, 141 are deemed valid (based on the reasonableness of test duration)\".  What is \"reasonableness of test duration\"?  This is a qualitative and unjustified assertion.\n\n3) How could a human in PTC's first stage (0-2 years old) answer questions in language like those presented in Table 1?\n\n4) In general, I wonder what the point is of doing a simple performance-based evaluation of evaluating anything on GPT-4, given that we know nothing about its data or training.\n\n5) \"For text-completion models, as they lack the ability to follow instructions and their output format is difficult to control, we concatenate each option with the corresponding question as input, and take the option with the highest generation probability as the model\u2019s prediction.\" So the input is different for the different models, therefore the input has to be tuned (how so? manually?) to extract the \u201cbest\u201d output format from the different models. The input format is not controlled.\n\n6) Why would accuracy be the important metric for most of the factors evaluated (as in Table 4)?\n\n7) \"We explore this question from two perspectives: the parameter size and the optimization objective of LLMs, as they are the most natural assumptions.\"  Why are they the most natural assumptions?\n\n8) Table 5: 1) No particular approach comes out on top. Are these results supposed to show anything about a general purpose capacity of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9374/Reviewer_BFZ5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596538918,
        "cdate": 1698596538918,
        "tmdate": 1699637182802,
        "mdate": 1699637182802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tn9C3Gl5GO",
        "forum": "fI6TkT050a",
        "replyto": "fI6TkT050a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_sSK2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_sSK2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new dataset for evaluating LLM cognitive capacity. The dataset is inspired by Piaget\u2019s Theory\nof Cognitive Development (PTC) which is highly influential in developmental psychology. The authors design language-based tasks that target skills developed at different stages of human life, validate their data on human participants, and create a conversion procedure allowing to estimate one's cognitive development age based on their task performance. They evaluate a number of LLMs on the newly proposed tasks, showing that generally, more parameters and RLHF-fine-tuning corresponds to higher cognitive age."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I highly resonate with the goals of the paper. Given extremely fast advances in LLM performance, it is crucial to devise thoughtfully designed and thoroughly validated datasets to better understand the capacity and limitations of LLM cognitive abilities.\n\nI believe that this work, therefore, tackles an extremely important topic that is of high interest to a large portion of AI, ML, and Cognitive Science communities.\n\nWhat further strengthens the paper is that they based their assessment tool on a highly established theory of cognitive development, which adds additional credibility to the research."
            },
            "weaknesses": {
                "value": "## Quality\n\nThe paper creates a very uneven impression when it comes to the quality of the experimental support of its claims. On the one hand, I find the experiments interesting, and I deeply appreciate that the authors include human evaluation of the generated dataset.\n\nOn the other hand, there is a number of substantial limitations:\n\n### Weakly justified design choices\n\nMany of the design choices seem to be poorly justified or not justified at all. For example, the choice of the calibration function is not explained nor is it compared to alternatives (e.g. F-measure). In this context, it's also not clear why the number of answer options was not standardized in the first place. For example, some questions have very restrictive answer options \"Yes\" and \"No\" while others are more open-ended and have more options. This situation could have been easily avoided by changing question-writer instructions.\n\n### Statistical reporting\n\nI also believe that statistical result reporting could be improved. For example, on page five, in the first paragraph, the authors say \"We observe that w1 : w2 : w3 : w4 = 1 : 2.6 : 1.4 : 2.5, indicating that cognitive abilities in the second and fourth stages are better at reflecting cognitive age under the evaluation of CogLM\". Such hypotheses need to be directly tested on the available data. Without uncertainty estimates on these parameters, it's impossible to gauge the reliability of this conclusion. Since this is a result of secondary importance, I would suggest removing this phrase entirely or adding appropriate statistical tests (e.g. method of contrasts, and, before that, comparing the restricted model (with all coefficients equal) and unrestricted models to show that these differences between w_is are significant).\n\n### Speculative claims\n\nI find some claims to be highly speculative and not supported by the data. For example,\n\n\"Specifically, the cognitive abilities of LLMs continuously improve as the size of model parameters increases, which follows the same pattern as human cognitive abilities continuously improving with age. We hypothesize that the reason for this phenomenon is that the parameter size of LLMs can be analogous to a child\u2019s brain capacity, and cognitive abilities continuously improve with the development of the brain, as stated in Piaget et al. (1952).\"\n\nThe paper does not go into sufficient depth to rigorously define terms like \"child's brain capacity\". As a result, the claims like above seem highly speculative and/or vacuous. If we condense the claim above, removing terms that are hard to define, it simply says that \"higher parameter count is associated with better cognitive task performance\", which we already know since all standard model evaluation tasks are cognitive tasks as well.\n\nI believe that it's better to restrict the amount of such claims as much as possible, as it hurts the credibility of the paper. \n\nIn general, the pattern observed in the paper can be formulated as \"better models tend to perform better on the proposed task\". The paper does relatively little to establish the correspondence between model development and human cognitive development.\n\nTo clarify what I mean - in order to show that LLMs actually follow a path similar to human development, one would need to demonstrate something that goes beyond the general pattern of better models performing better on a new task. The paper has some results of that kind - specifically, tasks that correspond to later stages in cognitive development seem to be generally harder for LLMs.\n\nIt's crucial, however, to look into alternative explanations, one of which is that these tasks might also be harder to generate in unambiguous ways. In other words, we need to validate the data and see (adult) human accuracies on tasks corresponding to different stages of development. This is especially important given that the authors found out that a large proportion of initially generated questionnaires was not valid (i.e. the data generation process is quite noisy).\n\nIn a similar vein, I do not fully agree with the statement \"3\" in the abstract:\n\n\"(3) The ability of downstream tasks highly depends on the level of cognitive abilities.\" - I don't believe that the authors showed the causal direction. I.e. performance on downstream tasks correlates with their cognitive abilities, as measured by the authors, but the claim that there is a causal relationship is much stronger, and would require a lot of additional experimentation and analyses.\n\n\n\n### \"Ability erasure\" experiments\n\nWhile I find the idea interesting, I find the experiments deeply flawed and presentation slightly misleading. While in the main paper, the authors said that the model was prompted with phrases like \"you have not yet developed a sense of empathy\", in fact the prompts were very different (as given in the appendix):\n\nFor example:\n\"Please imagine yourself as a child aged 0-2 years old. According to Piaget\u2019s theory of cognitive development, you are currently unable to recognize that objects exist both within and outside the field of vision and maintain a certain level of stability.\"\n\nGiven this prompt, the apparent results that we see (later abilities \"depending\" on earlier ones) are explained by the model being able to imitate the behavior of a human of the requested age. I.e. the authors don't say \"do everything as usual, but pretend that you don't have empathy\" to surgically remove empathy, but rather ask the model to imitate a person at a given age, specifically requesting it to conform to the Piaget's theory.\n\nIn other words, the authors (inadvertently) directly asked the model to generate the results that they observed.\n\n### Insufficient data validation\n\nIn general, given how much the paper's claims depend on the quality of the generated data, I find the data validation experiments insufficient. We do see that age correlates with performance on the proposed dataset, but I believe that a much more detailed look into human performance is warranted (in the very least, splitting humans by age and showing how they perform on different subtasks).\n\n## Clarity\n\nUnfortunately, the paper is not very clearly written, and in general, the presentation can be substantially improved. \n\nFor example, consider this sentence in the section 3.3\n\"Out of the 207 completed questionnaires, 141 are deemed valid (based on the reasonableness of test duration).\" \n\"Duration\" was never mentioned before that snippet, and is never mentioned after. It is also not clear what criterion was used to see whether a questionnaire was \"valid\".\n\nUnfortunately, this is not the only case when the paper does not fully define its terms or procedures.\n\n## Typos & Phrasing suggestions\n\n\"recently challenging tasks have been proposed\" -> \"recently, challenging tasks have been proposed\"\n\n\"Theory of Cognitive Development (PTC) is the most authoritative theory in the development of psychology, developed by Jean Piaget\" -> perhaps should be rephrased to avoid three-fold repetition of the word \"development\". More importantly, it should, perhaps, be \"developmental psychology\", not \"development of psychology\".\n\n\"cognitive abilities of PTC\" -> \"cognitive abilities proposed by PTC\"\n\n\"the ability of downstream tasks\" -> \"the ability of [performing?] downstream tasks\"\n\n\n# Conclusion\n\nOverall, I do highly resonate with the theme of the paper and I find many of the ideas promising. At the same time, unfortunately, I believe that there are serious flaws with the experimental design, substantial inaccuracies when it comes to result interpretation, and (less importantly), presentation clarity concerns. At present, I can not recommend its acceptance.\n\nAt the same time, I believe that the paper has most of the elements it needs to be published in a good venue. I deeply hope that the authors will rework the framing of the paper, scaling down some of the claims, and/or adding new experiments to get a more thorough comparison between human and LLM development."
            },
            "questions": {
                "value": "### Lack of actual LLM developmental data\n\nThe authors make an analogy between human development and fully-trained LLMs of different capacity. At the same time, for humans, the process is fundamentally different - as the brain develops, it realizes its potential rather than simply increases in capacity. A more natural analogy, therefore, seems to be a single LLM evaluated on different training stages.\n\nIt would highly increase the impact of the paper if the authors evaluated LLM cognitive task performance at different stages of training. I understand that for some models, such an experiment would not be feasible. But perhaps it might be possible to find open-source model training checkpoints to evaluate the model on these tasks in different stages of training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810719307,
        "cdate": 1698810719307,
        "tmdate": 1699637182628,
        "mdate": 1699637182628,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dz7IzKEFpO",
        "forum": "fI6TkT050a",
        "replyto": "fI6TkT050a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_LvrM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9374/Reviewer_LvrM"
        ],
        "content": {
            "summary": {
                "value": "This paper adopted a cognitive framework from psychology (PTC framework) to evaluate LLMs. The authors created a human-annotated benchmark dataset based that contains questions from 10 cognitive levels and 4 cognitive stages. The number of questions increases with cognitive difficulty levels. The questions are formatted as multiple choice questions and the main evaluation metric for this dataset is calibrated accuracy. The authors benchmarked various popular language models on this dataset, showing that the current best model GPT-4 has a similar cognitive ability of a 16-year-old human. This paper provided some additional analysis of LLMs' cognitive abilities. However, the analysis was not convincing to me personally. I will elaborate on the details in the following sections."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Benchmarking LLMs (or general AI models) on human-level tasks is an important topic. Besides, this paper constructed an expert-annotated benchmark dataset, which could be useful to the research community."
            },
            "weaknesses": {
                "value": "Even though the resources of this paper could be useful, the analysis of the CogLM benchmark could be improved. From Section 4.3.1,  the two findings are LLMs with larger parameter size have better cognitive abilities and optimization objective significantly improves cognitive abilities. However, the authors reached the latter conclusion by comparing the cognitive abilities of llama-2-text and llama-2-chat. However, the differences between these two models are not just optimization objectives (word prediction vs. RLHF). The llama-2-chat was finetuned on a large amount of labeled instruction data. Whereas the main motivation for RLHF of LLMs is to align the values and preferences of humans. I believe is inconclusive to say the differences in cognitive abilities are mainly from the optimization objective, and the improvements could be resulted from the instruction tuning with the same language modeling objective.\n\n\nIn 4.3.2, the authors mentioned that CoT does not bring significant performance improvement in terms of average calibrated accuracy, and hypothesized that cognitive skills are inherent and cannot be improved by CoT. However, from Table 5, we can see CoT\u2019s effects vary according to different cognitive levels, and CoT clearly showed a performance improvement in planning and propositional operations. This is expected as CoT itself was proposed to solve problems of a more complex nature. Hence, using the average result of all cognitive levels is questionable in analyzing CoT. \nBesides, from Table 4 and Table 5, we can see the results of GPT3.5-turbo in Table 4 correspond to the row \u2018With CoT\u2019 in Table 5 instead of \u2018Base\u2019. This raised a serious question, so are all results in Table 4 are prompted with CoT? If so, why there is no description of this at all? CoT only showed up in section 4.3.2 of this paper. The authors need to double-check whether Table 4\u2019s results are \u201cwith CoT\u201d or GPT3.5-turbo\u2019s result in Table 4 should be \u2018Base\u2019. \n\n\nOn the clarity side, the notation of \u2018chain-of-cognition\u2019 is highly confusing. This term refers to \u2018chain-of-thought\u2019 in Section 4.3.2 and somehow became \u201cCoC\u201d in Section 4.3.4 and Table 7. Simply coming up with some terms does not add novelty to this work but only adds confusion to readers."
            },
            "questions": {
                "value": "From Table 4 and Table 5, we can see the results of GPT3.5-turbo in Table 4 are corresponding to the row \u2018With CoT\u2019 in Table 5 instead of \u2018Base\u2019. This raised a serious question, so are all results in Table 4 are prompted with CoT?\n\nWhy do you need to introduce \u2018chain-of-cognition\u2019 when it essentially refers to CoT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698921826366,
        "cdate": 1698921826366,
        "tmdate": 1699637182461,
        "mdate": 1699637182461,
        "license": "CC BY 4.0",
        "version": 2
    }
]