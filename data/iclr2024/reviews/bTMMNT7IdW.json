[
    {
        "id": "DtBOvxBVrD",
        "forum": "bTMMNT7IdW",
        "replyto": "bTMMNT7IdW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_pVBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_pVBo"
        ],
        "content": {
            "summary": {
                "value": "Considering the evolving characteristics of data distribution is crucial for machine learning models in practical applications. Such problem is formalized as Evolving Domain Generalization (EDG) in literatures. This paper a common issue in EDG where limited timestamps can lead to overfitting to source domains. In tackling this challenge, this paper introduces a novel approach that involves gathering the Infinitely Fined-Grid Evolving Trajectory (IFGET) to capture evolving dynamics and align stochastic distribution of Stochastic Differential Equations (SDEs) with IFGET. This alignment effectively captures distribution shifts across the sequence, enabling SDE-EDG to adapt the model for generalization in dynamic environments. The experimental results, conducted on various synthetic and real-world datasets, provide empirical evidence of the method's effectiveness. Overall, I believe this work proposes an innovative solution to the challenging yet under-studied problem of EDG, representing a valuable contribution to the field."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1 Infinitely Fined-Grid Evolving Trajectory (IFGET) method is novel. Capturing evolving dynamics but avoiding overfitting to limited source timestamps is achieved by collecting IFGET in the latent space using continuous interpolated samples, a strategy that has demonstrated its effectiveness in ablation experiments. Specifically, this paper proposes to collect IFGET such that the evolving patterns are learned from trajectories of the individual sample instead of collective behaviors in existing EDG methods. Therefore, IFGET provides more accurate evolving trajectories by tracking individual sample's behavior, providing a finer-grained understanding of individual sample behavior.\n\n2 To the best of my knowledge, this is the first work to introduce Stochastic Differential Equations (SDEs) to address EDG tasks. The utilization of SDEs for modeling the continuously evolving trajectories of the latent space in the EDG problem is natural because of the inherent capabilities of SDEs in characterizing continuous stochastic processes.\n\n3 Figures 2 and 3(3) serve as clear illustrations of the superior performance achieved by capturing evolving dynamics with IFGET, affirming its indispensable role in addressing the challenges within the EDG problem.\n\n4 This paper is well-presented and easy to follow. The paper demonstrates the effectiveness of SDE-EDG on various datasets, including simple and complex datasets, showcasing its outperformance compared to other baseline methods by effectively capturing evolving dynamics."
            },
            "weaknesses": {
                "value": "1 I doubt the existence of evolving dynamics. It's worth considering scenarios where time-related tasks might lack evolving patterns and instead exhibit random shifts.\n\n2 Linear interpolations may not reflect the true evolving trajectories, since the real evolving trajectories might be nonlinear and complex.\n\n3 Considering that IFGET is an approximation, and the existence of ground truth sample-to-sample correspondence is uncertain, how do we guarantee that IFGET accurately represents the real evolving trajectories?\n\n4 The authors conducted an ablation on weighting on IFGET; however, I am curious about the performance of IFGET without continuous interpolations, which will show whether continuous interpolations improve generalization to sparse timestamps.\n\n5  I understand neural SDEs that neural SDEs do not rely on the assumption of the model being uni-modal, in contrast to prevelant uni-modal Gaussian distributions. However, most deep learning methods assume the classification tasks to exhibit uni-modal characteristics through using ERM classification loss. Therefore, I am wondering whether uni-modal or multi-modal classification loss contributes to the accuracy improvement of SDE-EDG in EDG."
            },
            "questions": {
                "value": "The computational costs of Neural SDEs, particularly regarding backward gradient propagation, are relatively high. However, this paper does not provide discussions of this aspect.\n\ntypos: in section 5.1, two repetitive \"Ocular Disease\"; \n\nIn section 4.4, you should not use the same $k$ in the summation of denominator for $\\frac{ |\\mathbb{S}^k_m|}{\\sum_{k=1}^K|\\mathbb{S}^{k}_m|}$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490966421,
        "cdate": 1698490966421,
        "tmdate": 1699636147698,
        "mdate": 1699636147698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qnFldN5UDK",
        "forum": "bTMMNT7IdW",
        "replyto": "bTMMNT7IdW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_y1vq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_y1vq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a SDE-EDG method in order to solve the challenges that limited number of timestamps are available in evolving domain generalization problem via collecting infinite fine-grid evolving trajectory of the data distribution with continuous-interpolated samples to bridge temporal gaps."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The writting of the paper is good.\n- The problem studied in this paper is well and clearly formulated.\n- The illustration and demonstration is convincing and solid.\n- The proposed SDE-EDG achieves better empirical results on several benchmark datasets than existing SOTA methods."
            },
            "weaknesses": {
                "value": "- The illustration of continuous-interpolated samples is not very clear. I am a little bit confused about why the samples generated with linear interpolation method are called \"continuous\"."
            },
            "questions": {
                "value": "1. What are the differences between continuous-interpolated samples and samples used in previous works?\n\n2. From my perspective, the interpolated samples are generated discretely. Thus, does the SDE-EDG method approximates the continuous case in the way of discretely sampling?\n\n3. According to my understanding, Eq.(7) aims at learn a set of parameters for $f_k$ and $g_k$ so that the learned models can mimic the approximated trajectory of data. In my opinion, such operation relies heavily on the quality of data representations, especicially at the early phase of training, since the feature extractor $\\phi$ is not well trained yet. Thus, in such case, will the linear interpolation results in some \"bad interpolation\"?\n\n4. According to the ablation study, the hyper-parameter $\\alpha$ seems sensitive for different datasets. Since $\\alpha$ scale the magnitude of $\\mathcal{L}_{mle}$ that plays the role of regularizer, could you explain such phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570760230,
        "cdate": 1698570760230,
        "tmdate": 1699636147600,
        "mdate": 1699636147600,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rikiyzpvfP",
        "forum": "bTMMNT7IdW",
        "replyto": "bTMMNT7IdW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_KuA7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_KuA7"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the evolving domain generalization (EDG) task. Previous works have difficulties in capturing evolving dynamics due to limited timestamps. To address this, this paper simulates the data distribution with continuous-interpolated samples and leverages SDEs to capture evolving distribution trends. The proposed method achieves SOTA performance on all 9 benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method achieves state-of-the-art performance on the benchmarks.\n\n2. Extensive experiments and analysis demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. In one iteration, for two consecutive domains, only one interpolated sample is generated. Can more than one sample be generated and used? For example, we can first generate one interpolated sample via Eq.5 and then set this sample as $\\tilde{z}_{m+1}$ to generate the second sample. If so, how does this influence the performance?\n\n2. Table 2 shows that smaller temporal gaps improve the generalization ability. How is the improvement of SDE-EDG over the baseline when using different time intervals?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2147/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2147/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2147/Reviewer_KuA7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758694337,
        "cdate": 1698758694337,
        "tmdate": 1700620305925,
        "mdate": 1700620305925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2VkEFQ2PZ4",
        "forum": "bTMMNT7IdW",
        "replyto": "bTMMNT7IdW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_bSwA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2147/Reviewer_bSwA"
        ],
        "content": {
            "summary": {
                "value": "The present work proposes SDE-EDG, a novel learning approach for solving the problem of Evolving Domain Generalization in the context of (neural) Stochastic Differential Equations. Real-world applications pose challenges to existing EDG approaches, such as inconsistent data availability over time, which leads to difficulties in capturing the underlying dynamics, risks overfitting and reduces the quality of generalization to unseen data. Further, distributions that change over time cannot be taken into account as they contradict the stationarity assumption. The authors claim to overcome all those limitations by incorporating the learning of latent trajectories that describe the distributional changes over time. The latter is done using variational inference in the setting of neural SDE approaches and by improving latent trajectory fits by bridging temporal gaps through linear interpolation of samples from consecutive distributions (the IFGET module). Empirical evidence is provided through synthetical and real world experiments demonstrating superior performance compared to existing state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The generative approach is not only convincing due to its quantitative superiority, but also provides insights into the black-box mechanisms of model learning, which can be easily generalized to unknown target areas. The authors exemplify how such investigation of representation in the form of latent trajectories (e.g., see Figure 2 and 3(d/e)) provides valuable insights. Further, empirical evidence is accessed through experiments on synthetical and real-world data as well as reproducibility is granted by submitted code files. The paper content is original to the best of my knowledge and, with few exceptions, has no spelling or grammatical flaws. Finally, on a positive note, the authors perform a series of ablation studies to test the validity of their claim that the proposed IFGET module contributes complementary information to learning evolving dynamics in a classical neural SDE scheme."
            },
            "weaknesses": {
                "value": "1. Learning neural SDEs is traditionally successfully presented in the framework of variational Bayesian inference (e.g. Li et al. 2020), where stochastic gradient descent methods are applied to minimize the evidence lower bound (ELBO). I wonder why the authors do not clearly address the reference to variational Inference in the manuscript, especially since the indications (e.g., the graphical model in Figure 1, derivation of the likelihood loss of IFGET in Lemma B.1) are given.\n\n2. The Sine experiment (Figure 3) is arguably a rather simple problem from a dynamic point of view and yet already indicates the limited extrapolation quality of the approach. Of course, this reduces my confidence in the generalization ability of the approach. I suspect possible causes in (i) an unfavorable modeling of drift and diffusion coefficients and (ii) that the linear interpolation approach in the IFGET module is too coarse. Can you comment on this?\n\n3. The IGET module reminds me on a approach by Kidger et al. 2020 (Neural Controlled Differential Equations for Irregular Time Series) in which the authors also try to improve the learning of latent trajectories by including interpolated sample trajectories, in their terms a \"controlled path\u201d. This is done in the ODE setting, but can be integrated into the SDE setting if the approach is applied to learning the drift coefficient. Can you explain in more detail how your approach differs?"
            },
            "questions": {
                "value": "1. (p.3) Do you assume all source domains have the same sample size $N$ or can it vary?\n2. (p.4) You repeat here (unnecessarily, in my opinion) the argument of \"sample complexity\", which already appeared on p. 1. However, I would like to ask if you could explain this argument in more detail?\n3. (p.4) Can you define $N_B$?\n4. (p.4) Why do you choose to draw the interpolation rate $\\lambda$ from a Beta distribution?\n5. How many interpolation points are generated between two consecutive data points, i.e., how many $\\lambda 's$ are drawn; am I right in assuming that only one is drawn? Would an increase in the number of intermediate points result in better performance?\n6. Neural SDEs are known to be very resource consuming. Can you assess the runtime complexity, for example?\n7. (Figure 4(c)) Can you assess what happens at Domain Index 29; why does SDE-EDG show a reduction in performance?\n\nMinor:\n    - Redundancy in the references."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "--"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2147/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2147/Reviewer_bSwA",
                    "ICLR.cc/2024/Conference/Submission2147/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699282911115,
        "cdate": 1699282911115,
        "tmdate": 1700568284728,
        "mdate": 1700568284728,
        "license": "CC BY 4.0",
        "version": 2
    }
]