[
    {
        "id": "3cbdrONM8i",
        "forum": "4aywmeb97I",
        "replyto": "4aywmeb97I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_xHG4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_xHG4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a significant advancement in the field of federated learning by introducing the Caching and Asynchronous Aggregation Federated Learning (CA2FL) method, a novel approach designed to optimize the learning process in non-convex stochastic settings. Recognizing the challenges posed by asynchronous updates and non-IID data in decentralized learning systems, the authors propose an innovative technique that caches and reuses previous updates for global calibration, aiming to enhance convergence and overall efficiency.\n\nThe core contribution of the research lies in its robust theoretical analysis, which underscores the method's substantial improvements in convergence. In addition to the theory, the paper provides empirical results that demonstrate CA2FL's superior performance compared to traditional asynchronous federated learning approaches. These results are particularly good, indicating that the method not only addresses key challenges in the field but also outperforms existing solutions.\n\nFurthermore, the research introduces an iteration of the method, MF-CA2FL, designed to significantly reduce memory overhead, a critical consideration in practical applications. This variant maintains the performance benefits of the original method, indicating the approach's flexibility and adaptability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. By implementing a caching mechanism that reuses previous updates, the CA2FL  method optimizes the learning process, particularly in environments with non-IID data and asynchronous updates. \n2. The authors don\u2019t just stop at theoretical claims; they substantiate their findings with empirical data, demonstrating the method's superior performance in real-world scenarios, including several vision and language tasks.\n3. By addressing the critical issue of memory overhead in federated learning systems, MF-CA2FL maintains the advantages of the CA2FL method while significantly reducing resource consumption, making it highly beneficial for practical deployments, especially in resource-constrained environments."
            },
            "weaknesses": {
                "value": "1.  It's unclear if the proposed CA2FL method was tested across a diverse range of scenarios, network architectures, or data distributions. This limited scope might raise questions about the method's generalizability and effectiveness in various real-world applications outside the tested environments.\n2. The paper might not fully address how this complexity impacts the scalability of the method, especially in larger, more diverse decentralized systems. If the method imposes significant computational or communicational overhead, it could limit its scalability and practical adoption. For example, it is only tested on CIFAR-10/100 dataset with ResNet-18 which is a shallow small NN architecture."
            },
            "questions": {
                "value": "1.  The CA2FL method introduces a unique approach to handling asynchronous updates in federated learning, but how does it adapt to various neural network architectures and models? Given the diverse range of models used in different sectors, can the authors elaborate on the method's compatibility and performance enhancements across these varied architectures?\n2.  Federated learning often operates under constraints like limited bandwidth, varied data quality, or privacy regulations. How does the CA2FL method, with its caching and asynchronous aggregation, perform under such practical constraints? \n3. The paper highlights the immediate improvements in convergence and efficiency, but federated learning models often operate over extended periods, during which they may encounter non-stationary data sources (model drift). How does the CA2FL approach anticipate and handle such long-term challenges? Are there mechanisms within the method to adapt to evolving data characteristics while maintaining its efficiency and accuracy over time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Reviewer_xHG4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619301682,
        "cdate": 1698619301682,
        "tmdate": 1699636761233,
        "mdate": 1699636761233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BYBmTNBduJ",
        "forum": "4aywmeb97I",
        "replyto": "4aywmeb97I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_yUBQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_yUBQ"
        ],
        "content": {
            "summary": {
                "value": "Paper introduces the Cache-Aided Asynchronous Federated Learning (CA$^2$FL) algorithm which allows a central server to cache each received update from a device and use it to correct the global update. Standard convergence rate guarantees are provided, and nice empirical results showcase the improved performance of CA$^2$FL. CA$^2$FL is closely tied to FedBuff, with a small tweak to allow models to be cached and reused during training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Paper is well-written, concise, and straightforward.\n2. Very strong empirical results! The section is nicely done, and the algorithm is compared to relevant baselines (one other algorithm which may be quite related could also be compared against and I've mentioned it in the Questions section)."
            },
            "weaknesses": {
                "value": "1. Reproducing the empirical results of FedBuff is important to motivate the Cache-Aided algorithm, but should not take up much space and shouldn't be considered a contribution.\n2. The cache idea seems to stem from SAGA, and while its inclusion is non-trivial, this is the only new piece to the FedBuff algorithm and thus the contribution seems to be marginal.\n3. Maximum gradient delay bound is used, however newer works no longer require this assumption [R1]. I ask below in the questions, but slight confusion with state delay and why gradient delay can't be used for it as well.\n\n[R1] Koloskova, Anastasiia, Sebastian U. Stich, and Martin Jaggi. \"Sharper convergence guarantees for asynchronous sgd for distributed and federated learning.\" Advances in Neural Information Processing Systems 35 (2022): 17202-17215."
            },
            "questions": {
                "value": "1. Remark 3.5 and the overall convergence analysis of FedBuff is not very novel. Equation (4) in FedBuff also details the $\\tau_{max}$ and $\\sigma^2$ issues. What is novel by reproducing it?\n2. The state delay seems closely tied to the gradient delay. Why can't the state delay take the form of the gradient delay? The inclusion seems to muddle the convergence result in Equation (5.1).\n3. Do the convergence rates align with FedBuff and FedAsync? Are they better or worse? A table would be nice to detail this comparison.\n4. How much memory is added to the training process when models are cached? I could see that this would be problematic in real-world implementations with thousands or hundreds of thousands of devices participating in training.\n5. The idea of caching seems quite similar to the recent ICLR work [R2], except in the decentralized setting. [R2] seems to also utilize some sense of caching and it might be a closer comparison in the asynchronous FL domain.\n\nI am interested to see how to authors respond to my questions and concerns. I feel that this paper is very strong empirically, which is important. At the same time, the novelty of the proposed method does not seem to clear the bar of an accepted paper.\n\n[R2] Bornstein, Marco and Rabbani, Tahseen and Wang, Evan and Bedi, Amrit Singh and Huang, Furong. \"SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication.\" International Conference on Learning Representations. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Reviewer_yUBQ",
                    "ICLR.cc/2024/Conference/Submission6656/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770066726,
        "cdate": 1698770066726,
        "tmdate": 1700502443381,
        "mdate": 1700502443381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "474A7Qi1ix",
        "forum": "4aywmeb97I",
        "replyto": "4aywmeb97I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_3At2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6656/Reviewer_3At2"
        ],
        "content": {
            "summary": {
                "value": "The manuscript tackles the problem of data heterogeneity in asynchronous federated learning. The authors show that the asynchronous delay can negatively affect the convergence of the learning process, especially when the data across clients are highly non-i.i.d. To address this issue, they propose a novel method called  Cached-Aided Asynchronous FL, which uses cached updates from each client to calibrate the global update at the server. They prove that the proposed method can improve the convergence rate under nonconvex stochastic settings and demonstrate its performance on several vision and language tasks. Moreover, the authors present a convergence analysis of FedBuff algorithm, a prior asynchronous FL algorithm, under non-i.i.d. distributed data across clients."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has several strengths that make it an incremental contribution to the field of asynchronous FL. First, the paper presents a novel and unified convergence analysis of FedBuff algorithm. The paper also makes fewer assumptions (similar to Koloskova et al.) and provides slightly tighter bounds on the asynchronous delay term than previous works. Second, the paper proposes a novel method that improves the convergence rate under nonconvex settings. Third, the paper demonstrates the superior performance on both vision and language tasks."
            },
            "weaknesses": {
                "value": "The paper has some limitations that could be further addressed. First, the paper only evaluates its method on relatively simple vision and language tasks, such as CIFAR. It would be interesting to see how the method performs on more challenging data sets, such as ImageNet. Second, the paper only considers classification tasks for language, which may not fully capture the complexity and diversity of natural language processing. It would be worthwhile to explore how the method works on other language tasks, such as generation, translation, summarization, etc."
            },
            "questions": {
                "value": "The paper \u201cUnbounded Gradients in Federated Learning with Buffered Asynchronous Aggregation\u201d also improves the analysis of FedBuff. I recommend comparing your FedBuff analysis with this work as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6656/Reviewer_3At2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699170606323,
        "cdate": 1699170606323,
        "tmdate": 1700380834143,
        "mdate": 1700380834143,
        "license": "CC BY 4.0",
        "version": 2
    }
]