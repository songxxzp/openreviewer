[
    {
        "id": "Hfx4kCJ24R",
        "forum": "nsNyDvNQTc",
        "replyto": "nsNyDvNQTc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to improve classification performance by leveraging uncertainty estimates. To this end, the authors focus on posterior networks, a novel class of classification models that has been recently introduced as an alternative for Bayesian neural networks in OOD detection. Posterior networks model a second-order distribution that is parameterized by a neural network using specific loss functions. In this paper, the authors focus on binary classification, and the considered second-order distribution is a Beta distribution. \n\nIn addition, the authors assume that the data generation process also has a Beta prior over the Bernoulli parameter of the first-order distribution. Within this framework, the authors analyze model estimation bias and they show that it depends both on the aleatoric and epistemic uncertainty. Furthermore, they formulate decision boundary selection in terms of both types of uncertainty, and they prove that this problem is NP hard."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Uncertainty quantification is a hot topic in machine learning. The submission also contains material that is novel."
            },
            "weaknesses": {
                "value": "This paper is not well written. I have the impression that the authors lack a thorough understanding of the literature. They use unconventional notions, and they don't discuss the problem in a formal way. Let me make this point clear. \n\nIntroduction:\n- the authors write \"benefits of combining model score with estimates of aleatoric and epistemic uncertainty\". This is a very weird reasoning, because the model score corresponds to the estimate of aleatoric uncertainty. (btw, better to use conditional class probabilities instead of model score, because model score is a very generic term)\n- RQ1: Does model score estimation bias (deviation from positivity) depend on uncertainty? This is a really weird sentence that is difficult to understand. I only understood that sentence after reading Section 3. You first have to explain what you mean with model score estimation bias. Is it a bias in the sense of the traditional bias-variance decomposition or is it something else? The same with \"positivity\". That's a term that is never used in the literature. What you mean is the probability of the positive class on population level. After reading Section 3 it became clear to me what you mean with \"uncertainty\". It is the differential entropy of the second-order distribution. I would argue that this is a way to measure epistemic uncertainty with posterior networks, but you have to be more precise in your description. \n- Same comment for \"test positivity rate\". Never heard of this term before. Please define it. \n\nRelated work: \n- The overview of related work is described in a very vague way. What you call \"Uncertainty modelling\" are methods that intend to quantify aleatoric and  epistemic uncertainty, such as Bayesian methods, ensemble methods and evidential deep learning methods. \n- Please also describe methods that only quantify aleatoric uncertainty, such as traditional probabilistic models. In that context, I would argue that \"calibration metrics\" are a way to assess whether standard probabilistic methods quantify aleatoric uncertainty correctly.  \n  \nSection 3:\n- Posterior networks: please motivate why this group of methods is your first choice to improve classification performance by leveraging epistemic uncertainty scores. Recent papers have critisized this group of methods for not quantifying epistemic uncertainty in a correct manner, see e.g. Bengs et al. Pitfalls of epistemic uncertainty quantification through loss minimization, Neurips 2022. So, I would argue that it is weird to consider this group of methods as first choice.  \n- 3.1: the uncertainty score u(x) is a measure of epistemic uncertainty. Please be more precise. \n- 3.2: notions such as \"true positivity\" and \"empirical positivity in the train and test set\" are very weird terms. I assume that you refer to true and estimated conditional class probabilities for the positive class. Please be more precise. \n- 3.2: Why is the true positivity sampled from a Beta distribution? Is this to generate synthetic data, or what is the purpose of this? Since you are presenting a theoretical result in Theorem 3.1, I assume that you have a different purpose of considering a Beta distribution. \n-Theorem 3.1: \"When data is generated according to Figure 2\". Unclear what this means. The figure is not detailed enough. The data generation process has to be described in a formal way. \n\nBecause of the very imprecise write-up, it is difficult to follow what the novelty of this paper is. I have to admit that I was completely lost after Theorem 3.1, even though I am very familiar with posterior networks."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_D6i1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697708987115,
        "cdate": 1697708987115,
        "tmdate": 1699636526274,
        "mdate": 1699636526274,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RZa0tMxclj",
        "forum": "nsNyDvNQTc",
        "replyto": "nsNyDvNQTc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837"
        ],
        "content": {
            "summary": {
                "value": "The submission addresses making decisions under uncertainty. More precisely, it considers the binary classification setting, where an instance must be classified into one of two classes, and where the classifier provides a classification score with an associated level (score) of uncertainty. \n\nAfter a succinct reminder of previous works related to uncertainty modelling and decision-making under uncertainty, the paper first adopts a Bayesian view, and shows that the test positivity rate depends on both the positivity (classification) and uncertainty scores. Then, it proposes to transform the classification problem into picking a class based on both scores, by partitioning the corresponding 2D space (model x uncertainty scores) into bins, thus leading to a 2D decision boundary (instead of a 1D decision threshold for the model score) which maximizes recall for a desired precision level. The paper mentions several strategies for constructing the bins, and then proceeds with an empirical evaluation of the proposed strategy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper addresses an interesting and important problem\u2014making decisions under uncertainty. It covers the topic in a rather complete way, by first highlighting the relationship between uncertainty and estimation bias, and then proposing a strategy for improving decision-making by taking the uncertainty level into account. \n\nThe proposal is overall sound, with some theoretical grounding as well as an experimental study so as to support the claims."
            },
            "weaknesses": {
                "value": "My main criticism is that the paper ignores a large part of the literature on decision-making under uncertainty. There already exist many works dedicated to this topic, with the purpose of taking uncertainty into account to improve the decision-making process or abstaining to make decisions when uncertainty is too high. Some of these works are rooted in formalisms alternative to probabilities, while others make use of the classical probabilistic framework. It is obviously not possible to mention all of these works here\u2014see e.g. cautious, imprecise or uncertain classification; I'll only mention several of them here: \n\n[1] Mathias C. M. Troffaes. Decision making under uncertainty using imprecise probabilities. International Journal of Approximate Reasoning, Volume 45, Issue 1, May 2007, Pages 17-29. \n\n[2] Thierry Denoeux. Decision-making with belief functions: A review. International Journal of Approximate Reasoning, Volume 109, June 2019, Pages 87-110. \n\n[3] Thomas Mortier, Marek Wydmuch, Krzysztof Dembczy\u0144ski, Eyke H\u00fcllermeier, Willem Waegeman. Efficient set-valued prediction in multi-class classification. Data Mining and Knowledge Discovery, Volume 35, Issue 4, Jul 2021, Pages 1435-1469. \n\nBesides, it is difficult to see to which extent the analyzes reported in the proposal can be generalized outside of the framework considered here, either regarding the analysis of estimation bias (Section 3.2) or the proposed decision-making strategy (Sections 4 and 5). As stated by the authors, the Bayesian framework considered here may likely not hold for all datasets. The analysis but also the proposed decision-making strategy seem hard to generalize to more than two classes (in the latter case, for computational reasons). It would have been nice to discuss these limitations in the paper. \n\nThe writing should be improved: there are typos; the general structure of the paper might be improved (e.g., Sections 4 \"2-D Decision Boundary Problem\" and 5 \"2-D Decision boundary algorithms\" could be agregated); some parts may be developed for the sake of clarity (e.g., the \"Dynamic Programming (DP) Algorithm\" part in Section 5.2, which is quite verbose but lacks formalization). I do not understand why the numbering of subsections changes from a bold, small-letter font in Section 3 (see e.g. Sections 3.1 and 3.2 page 3) to a capital font in Section 5 (e.g. Sections 5.1, 5.2 and 5.3 pages 6-7). Figures are hard to read (very small); Equation 2 is displayed in an awkward manner; Algorithm 1 is a bit packed and consequently difficult to read. \n\nTypos (non-exhaustive list): \n- \"relative efficacy\" (page 2), \n- \"Under what settings\" (page 2), \n- \"via Bernoulli distribution\" (page 3), \n- \"In the case of train set\" (page 3), \n- \"corresponds to oversampled negative class\" (page 3), \n- \"expected true and test positivity rate\" (page 4), \n- \"$b(u)$ is score threshold\" (page 5), \n- \"upto\" (page 6), \n- \"feasible solution exists\" (page 6), \n- \"bayesian approximation\" (page 10, references), \netc."
            },
            "questions": {
                "value": "Why choosing the differential entropy of distribution $H(q(\\mathbf{x}))$ as a measure of uncertainty ? On one hand, this intuitively makes sense, but it also has been criticized by some authors (see e.g. the works of H\u00fcllermeier). \n\nThe interpretation of $\\gamma(\\mathbf{x})$ in Section 3.2 is not really an interpretation. Since the fact that $u(\\mathbf{x})$ is correlated with $\\gamma(\\mathbf{x})$ is central in the score estimation bias depending on score and uncertainty, this part may be further developed and discussed. \n\nCan you provide insights on generalizing the analysis of estimation bias to other settings (as compared to Fig. 2) ? \n\nThe decision-making approach seems debatable. In Section 4: is it reasonable to leave b unconstrained ? Shouldn't a monotonicity assumption be imposed ? Is it reasonable to always keep the decision-making process precise ? Wouldn't it be preferable to abstain in presence of a high (epistemic) uncertainty ? Aleatoric and epistemic uncertainties are here not distinguished from each other, which seems to be an issue: what if $u(\\mathbf{x})$ is maximal ? Is it reasonable to make decisions in this case ? What is the effect of discretizing the Score $\\times$ Uncertainty space ? To this extent, shouldn't the independent splitting on both dimensions avoided ? \n\nThe \"proof\" that the partitioning problem of the Score $\\times$ Uncertainty space is NP-hard (mentioned in the introduction) seems to directly proceed from the bin-packing problem behind. \n\nEqui-weight binning case, \"it is possible that a bin with lower positivity rate might be preferable to one with higher positivity rate due to different number of samples\": can you be a bit more specific ? \n\nBinning strategy: I understand that for any given score level, the uncertainty bin indices are monotonic wrt actual values; however, this does not correspond to the same level of model score. Des this have an impact on the results ? More generally, what is the impact of the binning strategy on the overall procedure ? This latter may be sensitive to the bins computed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5267/Reviewer_i837",
                    "ICLR.cc/2024/Conference/Submission5267/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698319606771,
        "cdate": 1698319606771,
        "tmdate": 1700644377733,
        "mdate": 1700644377733,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "knLSGDaCgF",
        "forum": "nsNyDvNQTc",
        "replyto": "nsNyDvNQTc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5267/Reviewer_ZEGe"
        ],
        "content": {
            "summary": {
                "value": "Paper considers new approach to uncertainty estimation for calibrating the model and improving the classification accuracy in the case of (imbalanced) training set and/or dataset drift. Theoretical and empirical analysis of model output score dependence of uncertainty and the score are evaluated, and based on dynamic programming and isotonic regression an approximation algorithms are formulated for general NP-hard problem of decision boundary selections using both score and uncertainty. Three benchmark datasets, from online advertising and e-commerce domain, are utilised to shown the usefulness of the proposed approach, compared to related algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Paper proposes novel theoretical analysis of combining predictive uncertainty and classification score for decision boundary selection. The findings are supported by empirical analysis and versatile set of tests. It is shown that optimisation problem is NP-hard, and the new practical algorithms to approximate the solution with dynamic programming and isotonic regression based approaches, are presented. To my knowledge, the analysis and proposed algorithms for this particular problem are novel contributions, showing improvements against some related score only and 2D decision boundary selection algorithms on real-world benchmark datasets."
            },
            "weaknesses": {
                "value": "The proposed approach is motivated by the applications such as medical diagnosis and fraud detection. However, only advertising and e-commerce datasets are considered in the empirical experiment section. It would strengthen the work, if other additional imbalanced datasets from different application domains could have been experimented, as well."
            },
            "questions": {
                "value": "- Why choosing the particular datasets? (additional data from some other application domains could support the work and findings further)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762419843,
        "cdate": 1698762419843,
        "tmdate": 1699636526088,
        "mdate": 1699636526088,
        "license": "CC BY 4.0",
        "version": 2
    }
]