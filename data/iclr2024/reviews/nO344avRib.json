[
    {
        "id": "JN5XIzCplP",
        "forum": "nO344avRib",
        "replyto": "nO344avRib",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_aPij"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_aPij"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a molecule graph generation model GEEL, whose backbone can be LSTM or Transformer. GEEL is scalable to relatively large graphs in molecule generation. GEEL reduces the vocabulary size of the edge list representation by using intra- and inter-edge gap encodings. This proposed edge encoding method is novel."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is good to have a table like Table 9 to show reproduced datasets.\n2. The intra- and inter-edge gap encoding is delicate and useful.\n3. The use of LSTM reduces the time complexity."
            },
            "weaknesses": {
                "value": "1. There is a problem in Appendix C.1 Table 11 (b) Large graphs (|V|max \u2264 187).\n\n2. For the molecule generation task, it is important to generate some novel molecules for further filtering in drag design and other real-world scenarios. This paper does not evaluate the portion of the novel-generated molecules in Table 4. In the extreme case, the model may only be able to generate molecules in the training set.\n\n3. It is hard to claim that GEEL outperforms BiGG. In Table 1, the performance is close. In terms of speed, BiGG runs on GeForce GTX 1080 Ti while GEEL runs on GeForce RTX 3090, which makes Table 2\u2019s result unfair."
            },
            "questions": {
                "value": "If this paper includes some ablation study of replacing the intra- and inter-edge gap encoding with traditional encodings, it will be better. Table 6 could include more experiment results. The parameter size can be adjusted to avoid Out-Of-Memory."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676643042,
        "cdate": 1698676643042,
        "tmdate": 1699636312608,
        "mdate": 1699636312608,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3vLCUCmjdb",
        "forum": "nO344avRib",
        "replyto": "nO344avRib",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_RDfu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_RDfu"
        ],
        "content": {
            "summary": {
                "value": "Good paper, extensive evaluations.\n\nThe paper proposes a simple and scalable graph representation i.e  gap encoded edge list (GEEL) for graph generative modeling.\nThe  representation size  aligns with the number of edges instead of nodes, consequently low size and more applicable for sparse graphs.\n\nThere do exists works which use O(#edges) for representation, however, the proposed work GEEL further reduces the edge list representation by reducing vocabulary size from N^2 to B^2, where B is graph bandwidth.\n\nThe authors extend their approach to attributed graphs(having labels).\n\nEmpirical evaluation is performed on a large number of attributed and non-attributed datasets on a diverse set of graph similarly metrics.\nThe approach also scales better in terms of inference speed when compared to existing works.\n\nOverall the approach shows significant improvements over existing methods for graph generative modeling tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Novel approach for graph generative modeling-> compact representation of data.\n\n2. Paper is easy to read. The authors clarify the need of each component. Diagrams are provided for better understanding of the method.\n\n3. Ablation studies are conducted to understand  impact of different sequence encoding(DFS, BFS etc.), diff architectures for sequence modeling such as LSTM, Transformers etc.\n\n4.High reproducibility: The experimental section is very detailed  with respect to the current work as well as baselines. Code is also provided.\n\n5. The authors also visualize generated graphs.\n\nSignificance:\nThe proposed method could pave way for advancing research in context of graph generative modeling especially w.r.t to larger graphs( which are also attributed)."
            },
            "weaknesses": {
                "value": "I would point to the questions section for details\n\n1. Not clear how many graphs are generated for comparison.\n\n2. Results on uniqueness and novelty metric seem to be missing.\n\n3. Scalability analysis doesn't seem to be complete.\n\nI request the authors to look into the questions section for details on each of the above point."
            },
            "questions": {
                "value": "1. It is not clear or I could not find out how many graphs were generated by the proposed method/ baselines for each dataset? I agree MMD is computed but how many graphs were generated? Request the authors to add this.\n\n2. Results on Uniqueness and Novelty seem to be missing. It is not clear whether the generated graphs have duplicacy etc.\nRefer metrics section of [A] for details.\nAdding these metrics(atleast for few datasets) could further improve the quality of the manuscript.\n\n3. Could the authors clarify how scalaibiltiy results are computed? I mean is batch size etc. set to 1? \nAlso since GraphGen[A] also works at edge list level O(#Edges), I would expect the authors to compare with GraphGen for scalability comparison.\nCan the authors justify why generation time is shown for one graph? Could it be an outlier?  I see only one number without any standard deviation etc.\nWould it make sense to generate a batch of graphs and report mean+std dev.\n\n\n\n\n\n[A]Goyal et al. GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation, WWW 2020"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Reviewer_RDfu",
                    "ICLR.cc/2024/Conference/Submission3577/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765993063,
        "cdate": 1698765993063,
        "tmdate": 1700577906201,
        "mdate": 1700577906201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JlkgoGukwO",
        "forum": "nO344avRib",
        "replyto": "nO344avRib",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_kVSU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_kVSU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes GEEL, a novel method to generate graphs starting from their sorted edge list. In particular, the edge list is encoded through gaps: each pair of nodes representing an edge becomes a pair where the first element is the gap from the previous pair's first element, while the second element encodes the gap from the current pair's first element. This has the advantage of reducing the vocabulary size from $N^2$ (where $N$ is the number of graph nodes) to $B^2$ (where $B$ is the graph bandwidth), while maintaining the cost of training and inference to be $O(M)$ (where $M$ is the number of edges). The method is extended to deal with attributed graphs by proposing a simple grammar whose elements are triplets of the form (node type token, gap-encoded edge, edge type token), plus rules to compose them meaningfully. \n\nThe generative model is an auto-regressive LSTM that is trained to maximize the likelihood of the training graphs (represented as sequences of gap-encoded edge pairs). The model is evaluated extensively in task of generating a) standard non-attributed graphs such as lobster, ego, community and b) molecules. The experiments show that a) the proposed approach achieves good generative performance with respect to a wide pool of competitors, b) it uses a parsimonious representation which allows reduced vocabulary size and competitive training/inference cost. Lastly, ablation studies are presented to justify the architectural choices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I am very pleased with this paper: it is written clearly and easy to follow. On the technical side, it presents a simple but empirically effective contribution, which addresses most of the challenges of edge-list based graph generative methods, namely the large vocabulary sizes and the burden of having to learn long-term dependencies as a consequence. Following, a list of things I identify as strengths:\n\n- The main contribution (the gap-encoded edge list) is novel.\n- The proposed approach is extremely simple yet very effective.\n- The literature review is satisfactory (although it is missing [1] as another edge-list based generative method, but that is a minor omission).\n- The experiments are thorough, spanning across different graph types and a not common wide range of baselines.\n- I appreciated the ablation study which help understanding why certain modelling choices were taken.\n\n\n\n[1] Bacciu et al., Edge-based sequential graph generation with recurrent neural networks. Neurocomputing 2020."
            },
            "weaknesses": {
                "value": "The weaknesses I have found are by no means fatal, and I believe they could be addressed through proper rebuttal. In particular:\n\n- perhaps this article is not well-suited for ICLR, since the focus should be on learning representations, but this paper does not revolve around representation learning. Again, I don't think this is fatal, and I would like to hear how this work places itself in the context of this conference by the authors.\n\n- while very effective, this method has also limitations which are not mentioned by the authors. The main one being the fact that it is still a vocabulary-based approach that cannot generalize to graphs with gap-encoded edges not present in the vocabulary. Another one is the dependency on the bandwidth $B$, which can sometimes be $\\approx N$ due to outliers. I understand that this can be bypassed by removing the outliers, but then again it restricts the applicability of the method to a certain class of graphs (those with low bandwidth) to exploit the concise gap-encoding."
            },
            "questions": {
                "value": "I have some:\n\n- What do the asterisks placed after Graphgen and Graphgen-redux in Table 1 mean?\n- What is meant by \"comparable\" MMD? Which criteria is used to define two MMDs comparable?\n- In Figure 4, what is the value of the $c_1$ and $c_2$ constants? Have they been explicitly calculated?\n- What are the novelty and uniqueness rates achieved by GEEL in the molecular generation experiments? How do they compare to the competitors? For example, I recall CharRNN has a novelty rate of about 84% on the MOSES benchmark, while STGG has a novelty rate of 67% on the same benchmark.\n- Speaking of which, I think it would be better to add the performances of the method in the MOSES benchmark. It should be fairly doable in a short time since the method is fast both in training and inference.\n- What are the vocabulary sizes for the molecular generation experiments (on QM9 and ZINC250k)?\n- Which order is used to encode the edge list of molecules? Is it the SMILES canonical order?\n- If the answer to the question above is yes: as you might know, SMILES works by generating a spanning tree, and then adding the edges that close the rings at the end. Don't you think this kind of process is not ideal to gap-encode the edge list of molecules (since the second element of the closing ring edges would have an abnormally wider gap with respect to the other edges)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Reviewer_kVSU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837174576,
        "cdate": 1698837174576,
        "tmdate": 1700667111929,
        "mdate": 1700667111929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eNFkdlEnkI",
        "forum": "nO344avRib",
        "replyto": "nO344avRib",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_EmZF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3577/Reviewer_EmZF"
        ],
        "content": {
            "summary": {
                "value": "Authors introduce a new parametrization for sequential generation of graphs. The parametrization is based on edge-lists with the main trick being encoding not the node IDs but difference between them. The use of C-M ordering is promoted and shown to be better than the usual DFS or BFS orderings used in the previous autoregressive models. Overall the proposed architecture is shown to be much faster than existing approaches and offer better or competitive results in graph generation quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed representation is very interesting and efficient. It also meshes well with the C-M ordering. \nThe choices are extensively ablated and shown to be better than alternatives. The experimental performance overall seems strong and experimental scalability is good. \nThe paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "My main concern with the paper are the evaluation metrics reported for the graph generation. First, only local MMD scores are reported (e.g. no Spectral MMD as used in GRAN or SPECTRE), but mainly that the novelty and uniqueness of the generated examples is not reported. As shown in the SPECTRE paper, autoregressive models such as GRAN can overfit the training set to a point, where effectively no novel samples are produced, while at the same time producing amazing MMD metrics. Since this work is essentially turbocharging the autoregressive generation, especially with the relative node ID representation, one could imagine that such overfitting would be a problem. After all this overfitting is one of the main motivations for using one-shot generative models without a given node ordering. Uniqueness and novelty is also commonly reported for the molecule generation as well.\n\nIt would also be interesting to see how some of the one-shot methods (e.g. DiGress) would perform with the C-M ordering and node IDs. As they tend to make GNNs more powerful. Actually a recent paper (https://arxiv.org/pdf/2307.01646.pdf) showed quite some improvements in the one-shot graph generators by using a known ordering. It would make sense to include this in the baselines."
            },
            "questions": {
                "value": "I would really like to see the uniqueness and novelty for all the models in the datasets that were tested. Validity as introduced in SPECTRE is also an interesting measure to have, that's maybe more easy to interpret than the MMDs, where it is available.\n\nI'll raise my score if this is addressed. I just don't think we can accept the paper without this information.\n\n### After Rebuttal\nI thank the authors for all the additional experiments and adjustments, esp. w.r.t. BwR.\nWhile the novelty and uniqueness of generated graphs is mediocre and performance is not really better than the newest one-shot models (e.g. SwinGNN), the proposed change is neat and does improve upon autoregressive baselines. Thus I raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3577/Reviewer_EmZF",
                    "ICLR.cc/2024/Conference/Submission3577/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3577/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841517097,
        "cdate": 1698841517097,
        "tmdate": 1700815941007,
        "mdate": 1700815941007,
        "license": "CC BY 4.0",
        "version": 2
    }
]