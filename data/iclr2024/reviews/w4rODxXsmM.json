[
    {
        "id": "iQwFZTX2n2",
        "forum": "w4rODxXsmM",
        "replyto": "w4rODxXsmM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC"
        ],
        "content": {
            "summary": {
                "value": "In this study, a novel methodology is proposed, combining reverse and forward curricula. The authors suggest resetting from demonstrations to effectively perform exploration when access to a limited number of demonstrations is available. The reverse curriculum starts from positions backward from the goal point using demonstrations, while the forward curriculum is executed through score-based prioritizing, allowing the starting point to have an appropriate level of difficulty. As a result, it shows improved performance compared to baseline methods that leverage demonstrations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method, combining reverse-forward curriculum approach is novel.\n\n- The learning curves presented in figures 3 and 4 demonstrate improved performance compared to the baselines, and the figure in 5 suggests that using both the reverse and forward curriculum with a limited number of demonstrations is beneficial."
            },
            "weaknesses": {
                "value": "- The assumption of resetting from demonstrations is a strong one and only applicable in simulation environments. While this study proposes an efficient way to leverage one or more demonstrations through the reverse curriculum, I still consider it to be a strong assumption.\n\n- The study may appear to be not significantly different from consecutively performing Jump-Start RL and PLR.\n\n- While it compares to various baselines, many of them seem to be algorithms that do not assume state resets or use curricula."
            },
            "questions": {
                "value": "- In Figure 6, it seems that 'reverse' and 'forward' are only applicable to 'reverse-forward,' and not to 'none' and 'forward only,' which might be confusing.\n- Which algorithms among the compared baselines require the State Reset assumption?\n- If possible, could you please explain the reason for the initial high performance of RLPD in the stick pull task in Figure 4, followed by a drop?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_gfCC",
                    "ICLR.cc/2024/Conference/Submission362/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839603070,
        "cdate": 1698839603070,
        "tmdate": 1700561716962,
        "mdate": 1700561716962,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PLz4PMSYGu",
        "forum": "w4rODxXsmM",
        "replyto": "w4rODxXsmM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors show the benefits of first using a reverse curriculum on demonstrations (of simulated RL tasks with sparse rewards) followed by a forward curriculum that expands the set of initial states (from which the agent can achieve the task). In many different simulated RL tasks, it is shown that the method can achieve quite significant boosts in sample efficiency, and can sometimes succeed in tasks which other methods completely fail. Ablation studies show that the method is robust to the number of demonstrations and that the reverse-forward curriculum seems to be the best among various other choices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper convincingly shows that their method and their particular choice of curriculum leads to significantly better performance in many different RL tasks with sparse reward structure. \n\n* Ablation studies are well done and cover significantly the possible variations.\n\n* Figures captions and plots are well done, as well as the visualizations in the website."
            },
            "weaknesses": {
                "value": "* The paper could benefit from an algorithmic summary. Algorithmic decisions not summarized succinctly by equations or in algorithmic form, but by verbose descriptions make the method look more 'alchemical' than it need be. e.q. \n\n\"Define q to be the fraction\nof episodes out of the last k episodes that receive nonzero return starting from a sampled initial state\nsi,init. If q is 0, then assign a score of 2 to si,init. If 0 < q < \u03c9 for a threshold 0 < \u03c9 < 1, assign\na score of 3. If q \u2265 \u03c9, assign a score of 1.\" \n\nThe actual numbers chosen detract from the concept of rejecting samples based on e.g. the expected return of exploration."
            },
            "questions": {
                "value": "* Please do not use the word 'extreme', you used it several places throughout the paper.\n\n* \"In practice, the observations used by the policy \u03c0\u03b8 may be different from the actual environment state but for simplicity in this\npaper state also refers to observation.\"\n> So you don't consider noisy feedback or POMDPs? This should be mentioned clearly in the introduction. As the method shows significant improvement in learning curves, it is vital to indicate when/where we expect them to hold and where they would fail.\n\n* \"In both stages, we use the off-policy algorithm Soft Actor Critic (Haarnoja et al., 2018) with a Q-ensemble (Chen et al., 2021b), \"\n> What happens if you use another RL algorithm? Does it make a big difference? Which other methods, competitive to SAC, could you use?\n\n* Minor comment: \" As a result, a curriculum for each demonstration is necessary as opposed\nto a curriculum constructed from all demonstrations as done in prior work in order to ensure noisy\ninformation arising from the multi-modality of demonstrations do not impact the reverse curriculum\nof each demonstration as much.\" -> Too long sentence.\n\n* You use \\phi before you introduce it, and I didn't get what it's supposed to mean?\n\n* \"In this manner, initial states that sometimes receive\nreturn are prioritized the most, then initial states that receive no return, then initial states that are\nconsistently receiving return.\" -> Not a very clear sentence, do you want to use 'than' instead?\n\n* Would be nice to discuss why the methods compared against were chosen out of all the possible RL algorithms out there (maybe in an appendix?) Would the others be unsuitable for the task (e.g. on-policy, not suitable for demonstrations etc.)\n\n==== POST-REBUTTAL ====\n* My score remains the same, I think this paper should be accepted, as it has good results and detailed ablations. \n* Assumptions/limitations of the method (e.g. full observability, restricted to simulations, requires demonstrations) are mentioned throughout the paper and is not a deal-breaker. As mentioned in the rebuttal, just improving simulation efficiency is also a contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission362/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission362/Reviewer_Rzng"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843011796,
        "cdate": 1698843011796,
        "tmdate": 1699860411229,
        "mdate": 1699860411229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GZbE45kW8v",
        "forum": "w4rODxXsmM",
        "replyto": "w4rODxXsmM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission362/Reviewer_UCiq"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with curriculum learning in cases where there are only a small number of demonstrations available. The proposed method specifically design the curriculum generation as two stages: one along the demonstration paths, another explore around demonstration and eventually cover the entire space. The experiments are performed on several learning from demonstration (LfD) benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The presentation of this paper is good. \n- The experiment results are strong compared to multiple related baselines."
            },
            "weaknesses": {
                "value": "There are several issues/concerns on the method and experiments:\n\nMethod:\n\n- The most important issue is the design of curriculum. Basically, the author tried to separate the curriculum of reset states into two stages: in the first one the reset states are along the demonstrated states, and in the second one the reset states gradually move away from the demonstrated states. This does not make sense. Why not just combine the two stages into one, i.e. a curriculum that includes explorations of reset/initial states along the demonstration and also away from demonstrated states? The reset states need to cover the entire space in the end anyway. I don't see any reason for a two-stage design to make sense. Conceptually, reverse curriculum and \"forward curriculum\" are almost the same thing, with the latter has a difference of weighting on the exploration area.\n\n- What if there is no demonstration? Or intentionally forget about demonstration but just design curriculum that directly moves the reset/initial states away from goal? If this paper's assumption is that in the end, the initial/reset states should be able to cover the entire space anyway, then whether there is demonstrations provided should not matter: all starting states has to be explored sooner or later. I didn't see an ablation experiment to compare against the setting where no demonstration is available.\n\nExperiment:\n\n- As mentioned, it would be great to show the results under the setting where no demonstration is available, or the \"forward curriculum\"-only case. I understand the Maze task in Figure 6 tries to show this. But the experiment in Figure 6 is wrong: it does not show forward-curriculum-only is worse than reverse+forward curriculum. The reason it is worse is because forward-curriculum-only experiment mistakenly messed up with exploration (it should only explore in unexplored area, not around already explored demonstration area on blue lines). The explanation in the last two sentences of Section 5.2 is wrong too imo.\n\n- In Figure 5, do all experiments have the same reset/initial state distribution (cover all possible states) at the end of their curricula? If so, it's so hard to understand why the more demonstration there is, the faster it can be trained.\n\n- It seems that RFCL without \"forward curriculum\" works almost the same as RFCL with \"forward curriculum\" in most tasks (Fig 5). This again questions the necessity of having a second stage, as mentioned earlier. Without the two-stage setting, the novelty of this work is negatively impacted.\n\n- I'm not sure if the tasks selected in this paper are suitable for the proposed method:\n   - If the robot arms are moved by a positional controller, then the initial/reset position/states of the robot arm does not matter, because the positional controller should guide the robot arm/end effector converge to the goal position anyway regardless of initial states. Not much exploration is needed. So if this is the case, then the selected task cannot fully evaluate the potential of the proposed method.\n   - If the robot arms are not moved by a positional controller, then why not do so?"
            },
            "questions": {
                "value": "My questions are written in previous \"Weakness\" section. The authors can respond to the concerns/question written there."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986696636,
        "cdate": 1698986696636,
        "tmdate": 1699635963030,
        "mdate": 1699635963030,
        "license": "CC BY 4.0",
        "version": 2
    }
]