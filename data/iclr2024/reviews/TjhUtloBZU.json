[
    {
        "id": "hCxBqf15aq",
        "forum": "TjhUtloBZU",
        "replyto": "TjhUtloBZU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_JmqY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_JmqY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel topic of studying the effect of pre-training noise on various downstream datasets, termed noisy model learning.\nThe authors conduct the empirical study and analysis of noisy ImageNet and YFCC15M of supervised pre-trained and contrastive pre-trained ResNet50 models and illustrate that slight noise in pre-training improves performance on in-domain downstream tasks but always hurts the performance on out-of-domain tasks. From the singular value analysis of the pre-trained feature space, the authors designed two metrics that in general align with the downstream empirical observations.\nThe authors also propose several regularization terms based on the singular values of features that can mitigate the noise in pre-training in a block-box tuning manner. The authors provide comprehensive experiments to verify the effectiveness of the proposed method and offer interesting analyses and discussions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is generally well-written and organized.\nThe authors provide a first novel and interesting study on the effect of pre-training noise, demonstrating the importance of this research topic, especially in the context of large foundation models. \nThe empirical study for revealing the effect of pre-training noise is extensive and comprehensive, including both in-domain and out-of-domain datasets from various distributions. \nThe proposed method may not be very innovative, but it is simple and verified on both CV and NLP tasks with different large backbones. The method also works in the API case mentioned in the paper.\nThe authors also additionally study the combination of the proposed noisy model learning and traditional noise label learning, demonstrating the effect of noise in pre-training also exists when downstream data has noise. \nThe detailed results, experiments setup, and ablation study are presented in the Appendix."
            },
            "weaknesses": {
                "value": "How to introduce synthetic noise in ImageNet and YFCC15M needs more explanation.\nThe pattern SVE analysis of the ImageNet model and YFCC15M model are slightly different in Fig.3, and perhaps need more explanation."
            },
            "questions": {
                "value": "Since ImageNet or YFCC15M itself also originally contains noise, is there any optimal noise ratio that achieves the best ID downstream performance?\nSince NML assumes an inaccessible pre-trained model, how is other black-box tuning methods perform on the noisy model learning setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Reviewer_JmqY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698319488700,
        "cdate": 1698319488700,
        "tmdate": 1699636760176,
        "mdate": 1699636760176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QYk3fqRpkB",
        "forum": "TjhUtloBZU",
        "replyto": "TjhUtloBZU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_vftf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_vftf"
        ],
        "content": {
            "summary": {
                "value": "This work aims to study the noise in pertaining data and its impact on downstream tasks. The authors exploit the Singular Value Entropy (SVE) and the Largest Singular Value Ratio (LSVR) to analyze the singular value spectrum of the pre-trained feature space, and discover that proper noise in pre-training data increases both SVE and LSVR, leading to better transferability and worse robustness. Based on the observations, the authors further introduce an MLP together with three regularizations to transform the pre-trained features into a better feature space. Experiments with different model architectures and datasets are conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The analysis of feature space with the singular value spectrum is interesting and meaningful. Rich experiments and analyses are conducted to show how the noise in pertaining data can impact the learned feature embedding.\n- The proposed regularizations are intuitive and effective. Extensive comparisons are presented to show the improvements."
            },
            "weaknesses": {
                "value": "- This paper is featured with extensive empirical results. However, the core techniques in methodology (analysis and regularization of singular value spectrum ) have been studied in existing works[e.g. Chen et al., 2019, Bardes et al. 2022], which may undermine the theoretical contribution of this work.\n\n- Some figures are hard to understand by themselves. E.g. different types of marks are cluttered in Fig3."
            },
            "questions": {
                "value": "- On what scale the SVE and LSVR  is computed? The entire dataset?\n- Do the conclusions (Fig1- 3) always hold for stronger backbone models other than resnet50?\n- In the loss function, are the regularizations computed per batch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570078947,
        "cdate": 1698570078947,
        "tmdate": 1699636760071,
        "mdate": 1699636760071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4ymMfVtI7k",
        "forum": "TjhUtloBZU",
        "replyto": "TjhUtloBZU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_5WgV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_5WgV"
        ],
        "content": {
            "summary": {
                "value": "This paper endeavors to comprehend the underlying characteristics of noise within pre-training datasets and seeks to mitigate its influence on downstream tasks. The study reveals that the noise present in pre-training datasets exerts distinct effects on in-domain (ID) and out-of-domain (OOD) tasks. In the case of ID tasks, slight noise during pre-training can yield improvements in in-domain transfer performance. However, for OOD tasks, noise consistently degrades out-of-domain performance. To substantiate their findings, the authors employ Singular Value Entropy (SVE) and Largest Singular Value Ratio (LSVR) to capture the behavior of the trained features in both ID and OOD tasks. Subsequently, the authors devise a loss function that enhances the SVE and LSVR of these features, resulting in superior overall performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The impact of noise within pre-training datasets on subsequent tasks has not been thoroughly investigated in the existing literature. The insights presented in this paper, such as the distinct effects of noise on in-domain (ID) and out-of-domain (OOD) tasks, are novel and intriguing.\n\n- The design of the loss function is a direct consequence of the insights gained from observations, and experiments demonstrate that this designed loss outperforms the Cross-Entropy (CE) baseline, including LP and MLP structures.\n\n- Experiments encompass a wide range of tasks, including both image and image-language tasks. Furthermore, various popular base model structures are used in this paper."
            },
            "weaknesses": {
                "value": "-  I find Figure 3 challenging to interpret. It consists of numerous data points for each configuration, lacking connecting lines. Consequently, I struggle to draw the conclusions reached by the authors based on this figure.\n\n-  I've observed a potential contradiction between SVE and LSVR. For instance, in the case of two-dimensional features, [1.0, 0.0] exhibits the highest LSVR while having the lowest entropy. It would be beneficial if the authors could provide further clarification regarding this inconsistency.\n\n-  I would appreciate it if the authors could offer more detailed explanations as to why a slight amount of noise can benefit in-domain (ID) tasks. This is somewhat contradictory to the existing literature on learning with noisy labels, and additional insights would be valuable.\n\n-  In the paper, the authors claim that the proposed method enhances SVE and LSVR. However, Figure 5 (d) indicates that the proposed method does not yield superior LSVR compared to the LP and MLP models. Furthermore, it seems that as the noise ratio increases, LSVR does not drops significantly for all the settings.\n\n- It's worth noting that some related work, such as [R1], also employs Singular Value Decomposition (SVD) to address noisy label problems. It would be beneficial for the authors to discuss the distinctions between your approach and previous work.\n\n- I would like to point out that the improvements achieved by the proposed method appear to be relatively modest. According to the experimental results, the proposed method only demonstrates an approximately 1% improvement compared to the MLP model.\n\n- For ResNet-50, it might be worthwhile to explore the feasibility of fine-tuning all layers rather than constraining the encoder. It would be valuable if the authors could conduct experiments to determine if the proposed loss function is effective in cases where all layers are fine-tuned.\n\n[R1]: FINE Samples for Learning with Noisy Labels"
            },
            "questions": {
                "value": "See **Weaknesses**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Reviewer_5WgV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698600458790,
        "cdate": 1698600458790,
        "tmdate": 1700644808431,
        "mdate": 1700644808431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m9LVgU5ZVq",
        "forum": "TjhUtloBZU",
        "replyto": "TjhUtloBZU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_jd9i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6649/Reviewer_jd9i"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenges posed by label noise in pre-training datasets and its impact on downstream tasks. The authors focus on supervised pre-training models using synthetic noisy ImageNet-1K and YFCC15M datasets. They observe that while slight noise in pre-training can enhance in-domain (ID) transfer performance, it consistently harms out-of-domain (OOD) performance. The reason behind is noise in pre-training shapes the feature space differently. They introduce a lightweight black-box tuning method, NMTune, to mitigate the adverse effects of noise and improve generalization on both ID and OOD tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies a problem that is both practical and significant, yet has not been sufficiently investigated in prior research.\n- This paper is well-motivated and easy to follow.\n- The analysis of features is useful to understand the noise's impact on ID and OOD data. \n- Experiments are comprehensive."
            },
            "weaknesses": {
                "value": "- Improvements are needed in the writing and presentation quality. Please check the Question section below. \n- Some claims in the paper are unclear and confusing. Please check the Question section below. \n- The authors did not mention the limitations of their method and potential future work. The paper does not explore or discuss potential failure cases of the proposed methods. Understanding when and why the methods might fail is crucial for practical applications"
            },
            "questions": {
                "value": "- Self-supervised pre-train does require external supervision. Does it mean those models will not suffer from the noise issue? \n- Does it proposed method generalize to self-supervised pre-trained models?\n- When you trained CLIP, did you train the text encoder together or you use a frozen text encoder?\n- \"For OOD evaluation, we use DomainNet (Peng et al., 2019) where we train on either \u201creal\u201d or \u201csketch\u201d images and test on \u201creal\u201d, \u201csketch\u201d, \u201cinpainting\u201d, and \u201cclippart\u201d images\". If you trained on either \u201creal\u201d or \u201csketch\u201d, you should only test on domains that the model did not seen during training right\uff1f This sentence is a bit confusing.\n- \"we empirically analyze the singular value spectrum of the pre-trained **the** feature space on downstream datasets\" Typo: extra \"the\"\n- Section 2.3,  the authors should let or remind the readers what are M and D. They should be the number of samples and latent dimension size right?\n- Figure 3 is a bit confusing. For a specific noise level, there are many points (e.g. many blue stars). What does each point mean? One downstream task? A lot of points are overlapped and I don\u2019t know which 5 points (0%, 5%, 10%, 20%, 30%) should be read together. \n- \"An initial increase in the spanning dimension of the feature space is beneficial to the discriminability on ID tasks. \" The reason behind that is \"the pre-trained feature extractor captures more structure in data\" due to noise. But why more structure does not help OOD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6649/Reviewer_jd9i",
                    "ICLR.cc/2024/Conference/Submission6649/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698882850409,
        "cdate": 1698882850409,
        "tmdate": 1700671968620,
        "mdate": 1700671968620,
        "license": "CC BY 4.0",
        "version": 2
    }
]