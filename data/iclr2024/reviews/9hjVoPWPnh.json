[
    {
        "id": "jUqPFqcOWk",
        "forum": "9hjVoPWPnh",
        "replyto": "9hjVoPWPnh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the concept of \"machine unlearning,\" a method to deliberately forget data from models to comply with regulations. While existing methods have focused on classification models, this paper introduces a framework for machine unlearning in image-to-image generative models. It presents a computationally efficient algorithm supported by theoretical analysis that effectively removes information from \"forget\" samples without significant performance degradation on \"retain\" samples. The algorithm's effectiveness is demonstrated on large-scale datasets (ImageNet-1K and Places-365) and is notable for not requiring retain samples, aligning with data retention policies. This work is a pioneering effort in the systematic exploration of machine unlearning tailored for image-to-image generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of using the E_0 to generate L_F and L_R is interesting and a smart way to overcome the problems of estimating MI.\n- The theoretical analysis of the paper is well written and looks (beside of some remarks) sound to me.\n- Having done so much experiments using different image datasets"
            },
            "weaknesses": {
                "value": "I understand that the work was only done for I2I generation, which is a complex part already, however I was wondering if there is really no related work in this domain. I am not an expert in this domain but would propose to have a more in depth look at the literature for more related work."
            },
            "questions": {
                "value": "Abstract: \u201eprimarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored.\u201c: Is this true? Are there any references which support this statement?\n\nIntro: \u201eInformally speaking, we define a generative model as having \u201ctruly unlearned\u201d an image when it is unable to faithfully reconstruct the original image when provided with only partial information (see Figure 1 for an illustrative example where the partial information involves center cropping2).\u201c: I wonder if you did any analysis if the \u201enoise\u201c is fully uncorrelated with the part of the image before the unlearning? Maybe there are still some high level corrections left. In learning fair representations people do train secondary models on the representations and try to predict the sensitivity attribute from the fair representations. I wonder if such study can be used to check if a secondary autoencoders could not re-generate the cropped image from the generated noise.\n\nEquation 3: why is alpha not between 0-1 so you have (1-alpha) x term0 - alpha term1?\n\nTheorem 1: Why is having the same decoder not a problem? Couldn\u2019t the decoder not fully remember the images in some way or could you not extract some of the images by some information extraction techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The work is directly related to privacy and legal compliance. The correctness of the method and the way how it gets used needs to be verified in detail for any potential application."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Reviewer_hK5e"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697711950447,
        "cdate": 1697711950447,
        "tmdate": 1699636229645,
        "mdate": 1699636229645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Oigf5PSazz",
        "forum": "9hjVoPWPnh",
        "replyto": "9hjVoPWPnh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_4Xrc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_4Xrc"
        ],
        "content": {
            "summary": {
                "value": "This proposes a computationally-efficient unlearning approach, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. The authors also provide rigorous theoretical analysis for this image-to-image generative model. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365 show the advantages on image-to-image task. Overall, it is good paper. The main concern is how to get the forget set and how to set the hyper parameter $\\sigma$?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Quality/Clarity: the paper is well written and easy to follow. And both the experimental results and theoretical analysis demonstrate its advantages.\n\nOriginality/significance: the algorithm is new and underpinned by rigorous theoretical analysis. It systematically explores machine unlearning for I2I generative models."
            },
            "weaknesses": {
                "value": "1. The main concern is that the paper assumption is ideal and its hyper parameter \\sigma, which may not hold in practice.\n2. From Lemma 1, $\\sigma$ should be from the forget set? In the appendix C.4, however it sets $\\sigma=I$, which is not estimated from forget set?"
            },
            "questions": {
                "value": "The main concern is that the paper assumption is ideal and its hyper parameter \\sigma, which may not hold in practice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817607061,
        "cdate": 1698817607061,
        "tmdate": 1699636229569,
        "mdate": 1699636229569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h3dwi8ZbqV",
        "forum": "9hjVoPWPnh",
        "replyto": "9hjVoPWPnh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a mechanism for machine unlearning for image-to-image generative models. The work proposes a simple framework, minimizing the $l_2$ loss between embeddings from the forget training set and normal distribution, and maintaining the $l_2$ loss for the retain set embeddings. The results are provided on VQ-GAN, Diffusion Models and Masked Autoencoders on Imagenet-1K and Places-365 dataset comparing to reasonable baselines where the method shows good performance on retain set, while performance on forget set deteriorates as expected."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is focused on a timely and important problem as there is more discussing about regulating generative AI. For the current lawsuits facing several companies for their data practices on generative models and upcoming laws surrounding data retention policies, machine unlearning approaches may offer a possible solution.\n2. The experimental across different models and datasets are thorough, and helpful for future work. The work shows comparisons across several reasonable baselines, across different image-to-image generative models. The results are shown across different relevant metrics such as FID, Inception Score and CLIP Distance."
            },
            "weaknesses": {
                "value": "1. It's unclear if the machine unlearning setup considered in this work is practical. For machine unlearning setups, the gold standard to mimic is a model trained only on the retain set. In this work, the approach performs this task by minimizing the embedding distance for the forget set to normal distribution. Does this lead to overall worse performance than the gold standard model (where only retain set is used to train from scratch)? It would be good to include this as a baseline for comparison. \n2. The proposed approach itself is not too novel, while the work may have applied it first on image-to-image generative models.  This approach can be thought of a simple extension student-teacher or continual learning framework, and similar ideas have been explored in classification literature [1, 2].\n3. It's unclear if the model shows strong performance throughout. While the performance on the retain set stays strong, it's unclear if **worst** performance on the forget set is a good metric. This again focuses on going back to the first point, the unlearning paradigm should only be focused on achieving performance of model trained *only* on retain set. Thus, if the unlearned model performs much worse on the forget set than the model trained only on the retain set this may not be a good metric.\n\n[1] Zhang, Xulong, et al. \"Machine Unlearning Methodology base on Stochastic Teacher Network.\"\u00a0_arXiv preprint arXiv:2308.14322_\u00a0(2023).\n[2] Zhang, Yongjing, et al. \"Machine Unlearning by Reversing the Continual Learning.\"\u00a0_Applied Sciences_\u00a013.16 (2023): 9341."
            },
            "questions": {
                "value": "1. The experimental setup for retain sample availability is not quite clear to me. Why are retain samples selected from the remaining 800 random classes? Also, this confused me if all the 1000 classes, or only 200 classes are used for the main experiments to train the original model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Reviewer_L8mN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698978353991,
        "cdate": 1698978353991,
        "tmdate": 1699636229512,
        "mdate": 1699636229512,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p4xxtaaqJ9",
        "forum": "9hjVoPWPnh",
        "replyto": "9hjVoPWPnh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a study on machine unlearning for image-to-image generative models, an area not extensively explored previously. The paper introduces a theoretically sound and computationally efficient algorithm for unlearning that ensures minimal impact on the performance of retained data while effectively removing information from data meant to be forgotten. Demonstrated on ImageNet-1K and Places-365 datasets, the algorithm uniquely operates without needing the retained data, aligning with stringent data privacy policies. The research claims to pioneer a theoretical and practical approach to machine unlearning in the context of generative models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n- The paper boasts a clear and logical structure, helping readers' comprehension of the concepts presented.\n- It ventures into the relatively untapped domain of applying machine unlearning to image-to-image (I2I) generative tasks.\n- The authors have bolstered their approach with a solid theoretical foundation, enhancing the credibility and robustness of their proposed method."
            },
            "weaknesses": {
                "value": "Weakness:\n- The study does not align with the foundational concept of machine unlearning, which typically necessitates a comparison between the unlearned and retrained models as per references [1-7]. Although the authors justify this divergence due to the high costs associated with retraining generative models, this deviates from the core goal of machine unlearning aimed at addressing privacy concerns. Particularly, the approach presented in this paper generates conspicuous Gaussian noise over the 'forgotten' data, which may inadvertently signal that the data was previously part of the training set, contradicting privacy preservation goals. A more compelling motivation might be found in text-to-image (T2I) scenarios [8] where the goal is to prevent the generation of inappropriate content, or in image-to-image (I2I) applications [9] that showcase practical utility. It would be more meaningful\u2014and privacy-compliant\u2014if the model could reconstruct unremarkable images that don't trace back to the original training data, rather than reconstruct images with evident distortions signaling prior data use.\n- The evaluation process presented in the paper is incomplete with regard to established machine unlearning protocols. Typically, an unlearned model's performance is assessed using three distinct datasets: a test dataset to determine its generalization capability, a retained dataset to evaluate performance on non-forgotten data, and a forget dataset to check the efficacy of the unlearning process. The paper's Table 1 appears to only present results for the latter two, omitting the crucial evaluation on the general test dataset.\n- The consideration of relevant baselines in the paper is lacking. Reference [9] describes unlearning in the context of image-to-image (I2I) generative models, which appears to be closely related to the work at hand. A comparison or a clear explanation of why the methods from [9] cannot integrate into the proposed framework would strengthen the current approach by situating it within the broader research landscape and justifying its unique contributions.\n- The scope of the study with respect to the application of machine unlearning in image-to-image (I2I) generative models appears to be inaccurately broad. The term \"machine unlearning for I2I generative models\" suggests a wide range of applications; however, the paper primarily focuses on the image inpainting task. It would be more precise to either expand the variety of I2I applications examined in the study or to specifically define the scope as \"machine unlearning for image inpainting tasks\" to reflect the content more accurately. This would ensure clarity in the paper's contributions and avoid overgeneralization of the results.\n\n\n>[1] Graves, Laura, Vineel Nagisetty, and Vijay Ganesh. \"Amnesiac machine learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 13. 2021.\n>\n>[2] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023).\n>\n>[3] Chen, Min, et al. \"Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n>\n>[4] Warnecke, Alexander, et al. \"Machine unlearning of features and labels.\" arXiv preprint arXiv:2108.11577 (2021).\n>\n>[5] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023).\n>\n>[6] Kurmanji, Meghdad, Peter Triantafillou, and Eleni Triantafillou. \"Towards Unbounded Machine Unlearning.\" arXiv preprint arXiv:2302.09880 (2023).\n>\n>[7] Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. \"Eternal sunshine of the spotless net: Selective forgetting in deep networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n>\n>[8] Gandikota, Rohit, et al. \"Erasing concepts from diffusion models.\" arXiv preprint arXiv:2303.07345 (2023).\n>\n>[9] Moon, Saemi, Seunghyuk Cho, and Dongwoo Kim. \"Feature unlearning for generative models via implicit feedback.\" arXiv preprint arXiv:2303.05699 (2023)."
            },
            "questions": {
                "value": "- Could you please add one baseline mentioned in the weakness to the paper (Table 1)? \n- Why easing concepts can be thought about as a noisy label method? Please give more explanations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2859/Reviewer_uD61"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2859/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699151331846,
        "cdate": 1699151331846,
        "tmdate": 1700508782946,
        "mdate": 1700508782946,
        "license": "CC BY 4.0",
        "version": 2
    }
]