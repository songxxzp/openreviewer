[
    {
        "id": "a2VkZMNo6x",
        "forum": "EmuZOYfPNf",
        "replyto": "EmuZOYfPNf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_g24y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_g24y"
        ],
        "content": {
            "summary": {
                "value": "The author proposed a NAS framework specifically for the plug-in structures in the stable diffusion U-Net model. The search space is limited to whether to include the adapter and LoRa structure and a couple of hyperparameters in these two structures.  Existing RL-based optimization is adopted. A comparison of the optimized model (after searching) to models containing adapter and LoRa alone is performed, and superior results of the model obtained by the proposed method are reported. Several relatively small datasets are employed in the experiments for the few-shot setting, and only one face dataset is adopted for the fine-tuning setting. The paper is overall easy to follow, while several critical concerns are detailed below."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The manuscript is easy to follow\n+ NAS on plug-in structure for diffusion U-Net is new\n+ Superior results of the optimized model structure are reported in comparison to models with vanilla adapter and LoRa."
            },
            "weaknesses": {
                "value": "- The scope of the paper is small, where only adapter and LoRa are considered in the paper, and the model architecture is limited to diffusion U-Net. Is there any other plug-in structure that should be considered? And for the adapter and LoRa structure themself, only a couple of parameters are considered in the search space. How about other variable parts of the adapter and LoRa, e.g., the weights W? \n- The RL-based searching method is adopted. How about other search strategies?\n- There are several datasets employed for the few-shot setting. Why only one dataset is considered for the fine-tuning setting?\n- Other existing NAS methods should be included in the comparison study."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748304062,
        "cdate": 1698748304062,
        "tmdate": 1699636337132,
        "mdate": 1699636337132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aQxBYSnsKy",
        "forum": "EmuZOYfPNf",
        "replyto": "EmuZOYfPNf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_4Ls9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_4Ls9"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors investigate the automatic design of an optimal tuning architecture. They employ a reinforcement learning based neural network search method to facilitate the automatic design of the tuning architecture for PEFT of Stable Diffusion with few-shot training data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Through the proposed method, it was successfully obtained a novel tuning architecture that reduces parameter count by 18% compared to the widely adopted LoRa approach but still surpasses across various downstream tasks hugely. The authors conduct extensive analysis of the searched results."
            },
            "weaknesses": {
                "value": "1.\tInsufficient innovation. The work in this article seems to be just fine-tuning on the original model, and the innovative work is not clear. It is recommended to re-elaborate in the abstract, introduction, and conclusion parts.\n2.\tAbstract writing is problematic. The abstract is recommended to be developed in the order of background, goals, methods, results, and conclusions. In another way, what is the background of the question? What work did the predecessors do? What's wrong with their job? What do you plan to achieve in this work? How did you go about achieving your goals? What are the main findings of the study? What is the conclusion?\n3.\tAbsence of methodological details. While the article mentions a \"novel tuning architecture,\" it fails to provide any specifics about the architecture, training process, or key innovations. Details such as the type of neural network used, training data preprocessing, and the mechanism for generating sparse labels are crucial to assessing the method's novelty and reliability.\n4.\tSome sentences are vague. For example, the claim of \" there has been limited research on systematically studying how the design of these components would impact the final tuning effectiveness\" lacks context \u2013 it's essential to specify how this comparison was made and against what reference.\n5.\tThe methods part does not have enough mathematical formulas to support, and the innovation cannot be seen.\n6.\tSome charts are problematic. Such as fig.7 is too large, please reduce the image size so that one image takes up almost the entire page. And Fig.3 is confusing, it\u2019s not clear which part is the work of this paper.\n7.\tThe article does not mention whether the proposed deep learning method is fully reproducible. Lack of information about code availability, model architecture, hyperparameters, and data preprocessing steps could hinder the ability of other researchers to replicate the results.\n8.\tThe article does not mention whether efforts were made to interpret or explain the model's decisions.\n9.\tLack of limitations. The article does not discuss any limitations of the proposed method or the study itself. Addressing potential shortcomings, such as biases in data collection, limitations of the model architecture, or challenges in real-world deployment, demonstrates a comprehensive understanding of the research's scope.\n10.\tThere is a problem of cluttering references. Please check the article thoroughly to eliminate all cluttered and uncited references. This should be achieved by describing each reference individually. This can be done by mentioning 1 or 2 phrases in each citation to show how it differs from the others and why it deserves a mention."
            },
            "questions": {
                "value": "1.\tInsufficient innovation. The work in this article seems to be just fine-tuning on the original model, and the innovative work is not clear. It is recommended to re-elaborate in the abstract, introduction, and conclusion parts.\n2.\tAbstract writing is problematic. The abstract is recommended to be developed in the order of background, goals, methods, results, and conclusions. In another way, what is the background of the question? What work did the predecessors do? What's wrong with their job? What do you plan to achieve in this work? How did you go about achieving your goals? What are the main findings of the study? What is the conclusion?\n3.\tAbsence of methodological details. While the article mentions a \"novel tuning architecture,\" it fails to provide any specifics about the architecture, training process, or key innovations. Details such as the type of neural network used, training data preprocessing, and the mechanism for generating sparse labels are crucial to assessing the method's novelty and reliability.\n4.\tSome sentences are vague. For example, the claim of \" there has been limited research on systematically studying how the design of these components would impact the final tuning effectiveness\" lacks context \u2013 it's essential to specify how this comparison was made and against what reference.\n5.\tThe methods part does not have enough mathematical formulas to support, and the innovation cannot be seen.\n6.\tSome charts are problematic. Such as fig.7 is too large, please reduce the image size so that one image takes up almost the entire page. And Fig.3 is confusing, it\u2019s not clear which part is the work of this paper.\n7.\tThe article does not mention whether the proposed deep learning method is fully reproducible. Lack of information about code availability, model architecture, hyperparameters, and data preprocessing steps could hinder the ability of other researchers to replicate the results.\n8.\tThe article does not mention whether efforts were made to interpret or explain the model's decisions.\n9.\tLack of limitations. The article does not discuss any limitations of the proposed method or the study itself. Addressing potential shortcomings, such as biases in data collection, limitations of the model architecture, or challenges in real-world deployment, demonstrates a comprehensive understanding of the research's scope.\n10.\tThere is a problem of cluttering references. Please check the article thoroughly to eliminate all cluttered and uncited references. This should be achieved by describing each reference individually. This can be done by mentioning 1 or 2 phrases in each citation to show how it differs from the others and why it deserves a mention."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Reviewer_4Ls9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851807003,
        "cdate": 1698851807003,
        "tmdate": 1699636337061,
        "mdate": 1699636337061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bgEPVLA8hE",
        "forum": "EmuZOYfPNf",
        "replyto": "EmuZOYfPNf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_vXGg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_vXGg"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the exploration of large-scale text-to-image diffusion models, emphasizing the achievements of Stable Diffusion in image generation. Its main goal is to investigate the influence of component design on the performance of parameter-efficient tuning (PEFT) methods, notably Adapter and LoRa. By harnessing reinforcement learning-based neural network search techniques, the study aims to automate the optimal tuning architecture's design for PEFT, taking into consideration structures similar to Adapter and LoRa."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Researching how to reduce the training and transfer costs of diffusion models is highly meaningful, especially for tasks with limited data.\n2. The research has achieved a groundbreaking tuning architecture that reduces parameters by 18% compared to the popular LoRa approach, demonstrating superior performance across various tasks.\n3. The method's versatility has been validated across a wide range of data domains.\n4. The paper is well-written with clear logic, making it easy to understand."
            },
            "weaknesses": {
                "value": "1. Limited references. Several works [1-3], which aimed at reducing the costs of diffusion models, were not cited. Notably, the motivation and design approach of this study bear similarities to the paper [1].\n\n   [1] Xiang C, Bao F, Li C, et al. A closer look at parameter-efficient tuning in diffusion models. arXiv preprint arXiv:2303.18181, 2023.\n\n   [2] Kim B K, Song H K, Castells T, et al. On Architectural Compression of Text-to-Image Diffusion Models. ICCV Demo Track, 2023.\n\n   [3] Go H, Lee Y, Kim J Y, et al. Towards practical plug-and-play diffusion models, CVPR 2023.\n\n2. There's a limited comparison with other search methods. The authors assert that the proposed reinforcement learning approach is efficient, but additional experiments are needed to compare it with existing search methods to validate its efficiency."
            },
            "questions": {
                "value": "1. The impact of the search samples on model performance was not discussed.\n2. It would be desirable to see experiments demonstrating the method's generalizability in more domains, such as the medical field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3798/Reviewer_vXGg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698998690191,
        "cdate": 1698998690191,
        "tmdate": 1699636336994,
        "mdate": 1699636336994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Blo0DtnOe2",
        "forum": "EmuZOYfPNf",
        "replyto": "EmuZOYfPNf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_vpDh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3798/Reviewer_vpDh"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a reinforcement learning based architecture search method for parameter efficient finetuning of text-to-image diffusion model using few-shot training data. They have experimented on dreambooth and finetuning tasks, and observed improved performance with lower parameter count."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of parameter efficient finetuning with reinforcement learning is interesting. \n2. Experimental results are somewhat promising."
            },
            "weaknesses": {
                "value": "1. The paper used reinforcement learning like a blackbox. Proper motivation, justification and details of using which particular optimization methods are employed are missing. More details/citations are required. \n 2. LoRa, Adapter - these are parameter efficient finetuning methods. Adding reinforcement learning based search methods seems helping marginally w.r.t performance, training time. Also, why searching for parameters helps in image quality is not clear to me.\n3. Comparison of reinforcement learning based search methods w.r.t grid search/ combinatorial search method would be required.\n4. The rationale of using Eq.3 is not clear, why the authors choose to use power law method instead of any other combination?\n5. The writing need to be improved. E.g., \u201cDreambooth\u201d task is very weird, it should be called \u201cpersonalized few-shot finetuning\u201d. Overall, the motivation, method, experiments are not easy to follow."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699340713235,
        "cdate": 1699340713235,
        "tmdate": 1699636336898,
        "mdate": 1699636336898,
        "license": "CC BY 4.0",
        "version": 2
    }
]