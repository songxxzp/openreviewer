[
    {
        "id": "jm5g7rpFyQ",
        "forum": "1WceuzWff5",
        "replyto": "1WceuzWff5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_8Y8K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_8Y8K"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of transferring skills across environments with different  states and action spaces using learned embeddings. To this end, the authors first motivate need for their design decision of a shared policy architecture by analyzing traditional pre-traiing methods and individual training baselines. Based on these insights, the authors propose a shared policy architecture with state and action embeddings to tackle heterogenous spaces across environments. They demonstrate improved sample complexity when transferring skills between Meta-World (MW) and Franka\u2019s Kitchen (FK) environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Originality\nThe problem of transferring skills across heterogeneous spaces has been previously tackled by works in this subfield. The proposed architecture is novel to the best of my knowledge.\n\n## Quality\nThe work is generally decent, albeit with some issues mentioned in the weakness section.\n\n## Clarity\nThe paper is generally written straightforwardly with not a lot of typos. The ideas are easy to understand, and the impact of the core contributions is not hard to grasp. \n\n## Significance\nThe work seems to be approaching a complete state, and to me, the finding regarding the connection between data mixture and skill transfer seems to be the most exciting. Slightly more analysis, such as visualizing the learning dynamics of SEAL (Policy oscillation mentioned by the authors) and the impact of skill diversity, could significantly boost the significance of this work."
            },
            "weaknesses": {
                "value": "While I think the paper is written in a sound manner, I believe it still needs improvements in the Experiments, analysis, and formatting: \n\n## Experiments\n- Experiments across 2 random seeds are not enough in my opinion. Generally, we try to evaluate a statistically significant number of runs and report either the mean + deviation or the IQM and deviation [Aggarwal et. al, 21]\n- The authors propose multiple explanations to why the data mixture in the SEAL network has an impact. Their explanations, unfortunately, seem to be to be more on the intuitive level and not expanded upon enough. While it seems evident that changing the data mixture improves skill transfer, the claim regarding oscillations in policy parameters should be quite easy to justify and demonstrate in the experimental setup of authors. \n- The authors do not mention their architectural and training hyperparameters. Additionally, the design decisions for evaluations are not substantiated (100 episodes, 50,000 gradient steps) i.e. I do not understand how the authors decided for the given evaluation protocol. (see Questions section). My recommendation would be to substantiate this by either specifically mentioning the sources of previous architectures that the authors used as a reference, or providing a rationale for future reproducibility.\n\n## Formatting\n- In table 1, I do not understand the N/A columns. I had to separately look for them in the appendix in Tables 4--6. I would recommend mentioning these in the text in the fourth paragraph of section 4.3\n### Section 2: \n- Do the authors mean that the policy outputs a probability distribution over actions from which one can sample actions? Or do they mean that action space lies in [0,1]?\n- Trajectories are never introduced mathematically. I would recommend adding a small definition\nWhat is p(t) -- the probability of t? This is confusing\n- If t is used as task identifier, then do the authors mean a new task is sampled at each timestep? If so, please add a small sentence clarifying this. Or use a seperate simple for task identification\n### Section 3.3:\n- Do the state embeddings and action heads share the same parameters ? If not, use a separate symbol.\n- I would recommend formalizing this as an algorithm to make the training procedure for SEAL clearer.\n- Apart from the shared architecture, is the data mixture from the two environments the only design decision? If not, I would recommend summarizing all of them in this section for better clarity"
            },
            "questions": {
                "value": "- In section 4.1, the authors mention that the reason for the MTMHSAC agent reaching the maximum reward in FK is the presence of other tasks as distractors. Do they believe this causes negative gradient interference? If so, shouldn\u2019t gradient surgery during fine-tuning fix this? [Yu et al., 2020] Additionally, do the authors consider approaches such as gradient surgery relevant to their architecture?\n- Are there connections between this work and the work of Hausman et al. and Deramo et al.?  \n- How does the diversity of skills in the source environment impact the performance? There seems to be some form of overfitting prevalent in the value functions. However, given that the data mixture is a significant design decision, I am wondering if this could be dependent on the choice of the environment as the source and transfer.\n- What causes the overfitting of the value functions in MW? How can one mitigate this?\n- Do the authors believe different neural architectures enable some form of scaling laws for more complex environments? \n- In Appendix D, the authors mention the necessity to align the task IDs between MW and FK. Is the alignment of task IDs related to incorporating relational structure between similar tasks? [Mohan et al., 2023]  If so, Has this been a common practice in previous works? \n\n\n1. [Yu et. al, 2020] Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., & Finn, C. (2020). Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33, 5824-5836.\n2. [Aggarwal et. al, 21] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.\n3. [Hausman et. al.] Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., & Riedmiller, M. (2018, February). Learning an embedding space for transferable robot skills. In International Conference on Learning Representations.\n4. [Deramo et. al] D'Eramo, C., Tateo, D., Bonarini, A., Restelli, M., & Peters, J. (2019, September). Sharing knowledge in multi-task deep reinforcement learning. In International Conference on Learning Representations.\n5. [Mohan et. al., 2023] Mohan, A., Zhang, A., & Lindauer, M. (2023). Structure in Reinforcement Learning: A Survey and Open Problems. arXiv preprint arXiv:2306.16021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698606042008,
        "cdate": 1698606042008,
        "tmdate": 1699636325946,
        "mdate": 1699636325946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cuKIAMPiQo",
        "forum": "1WceuzWff5",
        "replyto": "1WceuzWff5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_Um91"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_Um91"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a way to do multi-task reinforcement learning on different environments. The method is described as pretraining on one environment and then finetuning on a different environment with a environment-specific state encoder layer and action decoder layer, as well as a shared policy in the latent state-action space. The method is tested on Meta-world and the Franka Kitchen environments and the results show that the proposed method enables the agent to learn with fewer gradient update steps."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Transferring between different environments instead of different tasks for one environment is an interesting and important problem setting. \n\n2. The proposed method is generally easy to follow."
            },
            "weaknesses": {
                "value": "1. For transferring between different environments, the authors propose to first pretrain on one environment and finetune on another environment, which I believe is a common approach that has been used in many recently proposed multitask RL methods. Another contribution the authors list is that the proposed method allows the transfer between tasks with different state and action space. The proposed method is to learn a task-specific state encoder and action decoder to deal with input size, which I believe is also a common way already used in practice. Therefore, I think the novelty of this paper is limited.\n\n2. The paper emphasizes the transfer of \"skills\" while I don't think the standard formulation for skills/options are mentioned in the method. The paper is more related to multi-task reinforcement learning and meta-reinforcement learning, for which I think the authors should include more discussions in the related work section.\n\n3. More experiments need to be done regarding the efficiency and asymptotic performance of the proposed method. The authors only show the comparison of the number of network updates comparison on only four different tasks in MetaWorld and Franka Kitchen.\n\n4. I don't think the number of network updates is a good metric for evaluating the sample efficiency of a multi-task RL method."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691917745,
        "cdate": 1698691917745,
        "tmdate": 1699636325871,
        "mdate": 1699636325871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l7QI8jU6oR",
        "forum": "1WceuzWff5",
        "replyto": "1WceuzWff5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_awjg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_awjg"
        ],
        "content": {
            "summary": {
                "value": "This paper studies transfer and multi-task RL with the goal of transferring a learnt policy from one task to another. The key contribution is the study of how to design policies from existing well known RL algorithms, such as soft actor critic (SAC) that can be transferred across tasks. The paper studies transfer learning across tasks by learning a shared embedding space, that allows transfer to be achieved even if the state or action spaces across tasks differ."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. To achieve transfer across tasks, the work proposes to use minimal modifications to existing RL algorithms, and studies this on popular benchmark domains. Additionally transfer learning can be achieved even if the state or action space changes across tasks, due to the ability to learn a latent policy architecture. \n\n2. The proposed minimal approach to study transfer learning can be applied directly on top of existing algorithms, with the code to be released for wide adaptation. \n\n3. The paper is quite simple to understand with the key contribution of the paper written clearly. I like that there are no overclaims made by the authors, and the approach is very minimalistic, if not completely novel. \n\n4. The paper studies a broad and challenging problem of transfer : which is how to achieve transfer across tasks when the state and action spaces change; a lot of works have shown transfer when the reward functions change, or if mid-way in the learning process, the reward changes, but previous works do not explicitly account for if transfer can be achieved when the state spaces differ. \n\n5. Transfer is achieved through the latent representation, where the states are first mapped to a latent, and then actions are decoded from the latent space depending on the action space of the task. The authors term the latent space as the skills space, or the SEAL space, where the latents are represented through the policy network, which can be environment agnostic."
            },
            "weaknesses": {
                "value": "I think the biggest limitation or challenge of the approach is through the architectural approach of achieving transfer learning itself. A lot of prior works have studied this sort of shared latent space architecture, or learning of latents, such that the latents can be transferred across tasks. This is not completely new; and the paper instantiates this in one particular way, keeping simplicity in mind through existing RL frameworks. \n\nMy biggest worry with such works is the ability to learn a good latent representation, one that can recover the structure of the task, and makes it useful to be shared across tasks. In other words, how can the authors qualitatively and quantitively understand whether this shared SEAL space is good or bad for transfer? Ideally, the latent should capture the underlying dynamics of the environment. \n\nIt would have been useful if more details could be provided on the approach section; Section 3 can perhaps be expanded better; I understand the need for simplicity for the proposed approach, but in its current form, it seems there is not really much algorithmic novelty in the work. The multi-heaed SAC approach is nothing new either; and the authors dont really provide much details on the learnt latent representations. For example, do we need a separate/different representation objective? Is this reward free representation objective? \n\nExperimental results are too naive, with very few results and not really providing an exhaustive understanding of the proposed approach. Results section can clearly be improved (and dont really need this big figure plots perhaps?)"
            },
            "questions": {
                "value": "1. Can the authors show some qualitative results showing how good this learnt latent representation is? How do we know these latents are good for transfer across tasks? \n\n2.  do not mean to see performance plots, such as cumulative returns - rather I am looking for more results showing how well the structure of one environment can be learnt; how good latents can be captured from the task, and what would make them useful for transfer?\n\n3. I am inclined to believe that this sort of approach may perhaps work well, assuming good latent representation can be learnt, in pixel based tasks, instead of raw state/action spaces. This is because often for pixel based environments, the underlying structure of the environment can be recovered - can the authors demonstrate some results, using for example a simplistic CNN based SAC with a shared embedding space, that can be used for transfer?\n\n4. I like the problem statement and the need for a simplistic approach; but I think the authors can do a much better job at this and I would encourage the authors to do; for example, if you can study different representation objectives, task specific reward based or even reward free, and then show which of the proposed objectives can learn a good latent representation that enables transfer across tasks - this would be really interesting! I think the algorithmic novelty of the work is not really there; so rather the authors can turn this into an empirical validation paper studying the ability of what makes good representations to be transferred across tasks. This can be done for simple to complex control environments, ranging from raw state/action to even pixel-based environments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706608293,
        "cdate": 1698706608293,
        "tmdate": 1699636325763,
        "mdate": 1699636325763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nniE74utcs",
        "forum": "1WceuzWff5",
        "replyto": "1WceuzWff5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_NkEv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3692/Reviewer_NkEv"
        ],
        "content": {
            "summary": {
                "value": "This paper explores several approaches to transferring high-level RL skills across diverse environments with different state and action spaces. The goal is to study the effect of skill transfer on multi-task sampling efficiency. Concretely, they compare three methods: (1) a simple baseline that learns a separate policy for each environment, (2) a pretraining-finetuning method that learns a policy in one domain and finetunes on the other, where any mismatch in state space is bridged by padding, and (3) training an environment-agnostic policy with a shared latent backbone (SEAL) on data across multiple environments. They found that (1) works reasonably well, (2) suffers from suboptimal performance when finetuned in downstream environments, and (3) achieves some level of skill transfer across environments. Moreover, the sample efficiency of (3) can be improved by adjusting the ratio of data from each environment. To summarize, this paper takes a step toward understanding the effect of transferring high-level skills across environments on multi-task RL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The problem setting studied in this paper is of importance to the community. Achieving data sharing across environments with different state and action spaces is a step towards generalist agents. \n- The architecture of the shared environment-agnostic latent policy is rather intuitive."
            },
            "weaknesses": {
                "value": "- The second baseline method with pretraining + finetuning makes little sense. I'm not sure how padding the observation space can enable skill transfer, especially since the order of the state space is not restricted (i.e. dimensions corresponding to end-effector positions in one environment might represent object position in the other).\n- The experimental results are largely inconclusive. There is no direct comparison between the three methods studied in the paper. Even if we combine the individual plots, it seems that skill transfer leads to suboptimal performance compared to training separately in each environment.\n- The paper does not compare to external baselines from prior work.\n- The presentation of the experiment section is rather disorganized.\n- Overall, I don't think this paper demonstrates the level of rigor required for a conference paper."
            },
            "questions": {
                "value": "- How does padding the states enable data sharing when there is a mismatch in the semantic meaning of each state dimension?\n- How does SEAL work when the environments are more different, say when one environment consists of a legged robot while the other involves a tabletop manipulator?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814644898,
        "cdate": 1698814644898,
        "tmdate": 1699636325674,
        "mdate": 1699636325674,
        "license": "CC BY 4.0",
        "version": 2
    }
]