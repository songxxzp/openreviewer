[
    {
        "id": "yL1Cvq4kpm",
        "forum": "QwNj5TP9gm",
        "replyto": "QwNj5TP9gm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_gFnt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_gFnt"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Evidential Conservative Q-Learning (ECQL) framework designed for dynamic recommendations. ECQL employs evidence-based exploration, guided by uncertainty measures, to uncover items that cater to the users' long-term preferences. The proposed framework is characterized by its Sequential state encoder, which keeps track of a dynamic state space via a sliding window over user history, and the Conservative Evidential Actor-Critic (CEAC) module that facilitates uncertainty-informed exploration and conservative policy learning. The framework's distinctive elements include an evidential reward system that leans towards exploring items with uncertain outcomes to maximize information gain, and a conservative Q-learning mechanism that guards against overestimating the policy value, thus ensuring quality recommendations. The paper further supports the efficacy of ECQL with a theoretical analysis that underscores its convergence behavior and recommendation quality. Empirical evaluations on real-world datasets reveal that ECQL surpasses benchmark methods. Additionally, ablation studies emphasize the significant contributions of vacuity-driven exploration and conservative learning. The major contributions of the paper encompass the innovation of an evidential RL approach for recommendations, an emphasis on uncertainty-driven exploration, the integration of conservative off-policy learning, theoretical assurances, and the creation of a holistic end-to-end framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper stands out due to its innovative combination of evidential learning and reinforcement learning tailored for recommendation systems. The inception of leveraging evidence-based uncertainty for exploration, paired with conservative off-policy learning to dodge overestimations, showcases an inventive approach. \n\n2. On the technical front, the paper exhibits robustness. It provides theoretical backing, ensuring that the methods are grounded in solid logic. The empirical evaluations, especially those conducted on real-world datasets, further validate the methodology by demonstrating its state-of-the-art prowess. \n\n3. Structurally, the paper is well-organized, offering a lucid exposition of the principal concepts and the constituents of the framework."
            },
            "weaknesses": {
                "value": "My main concern about the paper is the experimental parts.\n\n1. The datasets used are very small in this paper. Ml-1M and ML-100K are both very small. For Net\ufb02ix, the authors only sampled 6,042 users. For Yahoo!, only 54,000 ratings were sampled. Larger datasets are required for validation.\n\n2. I'm concerned about the effect-boosting effect of the proposed method on different datasets. Since the improvment is not large. Did the authors perform the hypothesis testing to prove the significance of improvements? Also, the effect enhancement of the proposed method is much smaller inthe  largest dataset  Yahoo! than in other datasets. Is there any specific research on this phenomenon?\n\n3. In the experimental part, the RL-based models lack some more recent work (the latest method is in 2018), which cuts down the validity of the proposed method.\n\n4. Some diagrams can bedemonstrated in a more expressive way. For example, Figure 2 could insert some images to illustrate the composition of themodules in the network. \n\n5. Some formulas are not written very rigorously, for example: all mathematical formulas are not followed by a punctuation mark. In addition, some expressions are not sufficiently formal, such as 'score' and 'rating'."
            },
            "questions": {
                "value": "In the QUALITATIVE STUDY in Section 5.2, regarding the random selection of the User ID, is there a discussion of the reason for selecting this user? I think it is more persuasive to describe the reason for the selection first and select multiple users for the QUALITATIVE STUDY."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675121920,
        "cdate": 1698675121920,
        "tmdate": 1699636381694,
        "mdate": 1699636381694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9bx63QGGYa",
        "forum": "QwNj5TP9gm",
        "replyto": "QwNj5TP9gm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new variant of Q-learning called ECQL for dynamic recommendations problems. The ECQL aims to improve the exploration efficiency of \\epsilon-greedy strategy, which is commonly used in many reinforcement learning algorithms. Specifically, the exploration strategy of ECQL integrates evidence-based uncertainty and conservative learning. The purpose is to discover items that are located beyond current observation but reflect users\u2019 long-term interests. Extensive experiments are conducted to validate the performance of ECQL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies an important problem of dynamic recommendation, i.e., discover items that are located beyond current observation but reflect users\u2019 long-term interests.  \n\nThe proposed ECQL is shown to have nice empirical performance."
            },
            "weaknesses": {
                "value": "The motivation of this work is not convincing. It is claimed that epsilon-greedy may not be able to learn the optimal policy that captures effective user preferences and achieves the maximum expected reward over the long term. This claim is not supported by any evidences. To the best of my knowledge, the \\epsilon-greedy strategy is a simple approach to balance the exploration vs. exploitation tradeoff of RL. As long as the RL model is tailored to RS in a right manner and the exploration vs. exploitation tradeoff is well balanced, the learned policy may not have the claimed weakness. Also, one can tune epsilon to attain different strength of exploration. There are other exploration strategies developed in RL literature. Can you claim all of them are not learn the optimal policy that captures effective user preferences and achieves the maximum expected reward over the long term? I am also confused by Figure 1. Little details are provided. It is hard for me to judge whether Figure 1 is the outcome of improper applying of RL to RS or some other factors. For example, is it a consequence of that the optimal policy of the RL has such weakness or the epsilon-greedy strategy leads to a sub-optimal strategy that has such weakness. \n\nThe challenge analysis is confusing. It is mentioned in the Introduction \u201cto address the above challenge\u201d. I do not get what is the above challenge. Technically, what is it? \n\nThis paper is not placed clearly. It aims to address the limitation of epsilon-greedy strategy. In the related work, it does not discuss previous works on exploration strategies and place this paper properly in this research line. In the related work, I am not convinced by the claim that randomize exploration strategies are less effective at capturing users\u2019 long-term preferences. Could provide any evidence?\n\nIt is not clear how the proposed exploration strategy connects to balancing exploration vs. exploitation tradeoff. In terms of the balancing exploration vs. exploitation tradeoff, it is unclear why it is better than the epsilon-greedy or fine tuned epsilon-greedy."
            },
            "questions": {
                "value": "Please refer to the comments on the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830741294,
        "cdate": 1698830741294,
        "tmdate": 1699636381617,
        "mdate": 1699636381617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZUIxUQmUaC",
        "forum": "QwNj5TP9gm",
        "replyto": "QwNj5TP9gm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_GVwA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_GVwA"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel evidential conservative Q-learning framework (ECQL) that learns an effective and conservative recommendation policy by integrating evidence-based uncertainty and conservative learning. Specifically, ECQL includes two main components, i.e., a uniquely designed sequential state encoder and a novel conservative evidential-actor-critic module. Extensive experiments on real datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces a novel recommendation framework named ECQL, which uses evidential conservative Q-learning for dynamic recommendation.\n\n2. ECQL is a new model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendation.\n\n3. The authors also provide theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky recommendations.\n\n4. The proposed framework integrates a sequential encoder, an actor-critic network, and an evidence network to provide end-to-end integrated training process.\n\n5. The authors have performed extensive experiments on real datasets and compare the proposed model with SOTA baseline methods."
            },
            "weaknesses": {
                "value": "1. In the Section 2, the authors do no include the recent studies about sequential recommendation. The most recent dynamic/sequential recommendation methods mentioned by them are proposed in 2019. The sequential recommendation methods developed in recent 4 year are not discussed in Section 2. Similarly, the authors are suggested to include some discussions about the RL methods developed in recent 3 years.\n\n2. One advantage of the proposed method is to provide diverse recommendations. However, in Table 2, the authors do not analyze the diversity of the generated recommendation results.\n\n3. The authors are suggested to include more sequential recommendation methods proposed in 2021 and 2022 as baselines. TimeSVD++ and CKF are not needed to be used as baseline methods. \n\n4. The Movielens-100K dataset is too small for experimental evaluation. The authors are suggested to include some larger datasets for experiments, for example Amazon review datasets. \n\n5. In the proposed ECQL, there are two main modules. However, there is no experiments studying the importance of these two parts. The authors are suggested to perform an ablation study."
            },
            "questions": {
                "value": "1. The proposed framework integrates a sequential encoder, an actor-critic network, and an evidence network to provide end-to-end integrated training process. For the actor-critic network and evidence network, which one is more important? Moreover, whether the model performance is dominated by the base sequential encoder? Whether the proposed framework can help improve the performance of different base sequential encoders?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698938822328,
        "cdate": 1698938822328,
        "tmdate": 1699636381546,
        "mdate": 1699636381546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ElriCjN6es",
        "forum": "QwNj5TP9gm",
        "replyto": "QwNj5TP9gm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel recommendation model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendations that may reflect users\u2019 long-term interests. The proposed model, called Evidential Conservative Q-learning (ECQL), conducts evidence-aware explorations to discover items that are located beyond current observation but reflect users\u2019 long-term interests. Additionally, it provides an uncertainty-aware conservative view on policy evaluation to discourage deviating too much from users\u2019 current interests. The paper presents a theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky (or overly optimistic) recommendations. The paper proposes a framework that includes a sequential state encoder and a Conservative Evidential Actor-Critic (CEAC) module. The former generates the current state of the environment by aggregating historical information and a sliding window that contains the current user interactions as well as newly recommended items from RL exploration that may represent future interests. The latter performs an evidence-based rating prediction by maximizing the conservative evidential Q-value and leverages an uncertainty-aware ranking score to explore the item space for a more diverse and valuable recommendation. The paper conducts experiments over four real-world datasets and compares with state-of-the-art baselines to demonstrate the effectiveness of the proposed model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a novel recommendation model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendations that may reflect users\u2019 long-term interests.\n    The paper presents a theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky (or overly optimistic) recommendations.\n    The paper conducts extensive experiments over four real-world datasets and compares with state-of-the-art baselines to demonstrate the effectiveness of the proposed model."
            },
            "weaknesses": {
                "value": "Many of the design choices of the paper are not well justified, it is also quite difficult to understand how some of the components of the model stick together. \n    The evaluation of the paper is done on data that is inherently not sequential, (with the exception of the Music data), in particular the Movielens dataset is a survey dataset and has many issues with regards to sequential evaluation. \n    Most of the increases in IR scores are rather marginal and it is unclear how much of that can be attributed to the particular elements of the method. \nUsing ratings as rewards signals is rather difficult to justify as currently ratings are not available for almost all industry related systems"
            },
            "questions": {
                "value": "How does the method compare on real sequential data where reward signals are much more subtle?\nWhat element of the model provides the increase in performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4158/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699993607694,
        "cdate": 1699993607694,
        "tmdate": 1699993607694,
        "mdate": 1699993607694,
        "license": "CC BY 4.0",
        "version": 2
    }
]