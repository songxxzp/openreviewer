[
    {
        "id": "q6rEn80KLB",
        "forum": "EpYnZpDpsQ",
        "replyto": "EpYnZpDpsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission431/Reviewer_sQMY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission431/Reviewer_sQMY"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a self-supervised representation learning scheme that can be applied to any data modality and network architectures. To this end, it proposed to learn the representations from random data projects of the input data. It is a scheme that learns from randomness, aiming to extract meaningful representations from randomness that mimic arbitrary downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1, this framework can accommodate various data modality\n\n2, extensive experimental results with good performance"
            },
            "weaknesses": {
                "value": "1, Despite the good performance of the experiments, what has been learned from the latent space using the scheme introduced in this paper, even intuitively?\n\n2, It says in the paper that $g^{(k)}(x)$ uses the same architecture design of $f_{\\theta}$. Even with random initialization, it may still follow a certain distribution family. How would changing this architecture affect the learning?\n\n3, The paper only exams the accuracy on the classification task for frozen representations. Nonetheless, a good representation could used for various purposes, i.e., manipulation of each dimension in the latent space for generating new data, understanding the essential dynamics of the system in physical models and time series data. How could this strategy be applied to scenarios beyond classification?\n\n4, Although the fact that choosing good augmentations usually requires domain knowledge, in many of the application mentioned, such knowledge could be partially obtained using physical intuition. By comparing with models under random corruptions doesn't seem to be a fair comparison."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Reviewer_sQMY",
                    "ICLR.cc/2024/Conference/Submission431/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698264865431,
        "cdate": 1698264865431,
        "tmdate": 1700576989137,
        "mdate": 1700576989137,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ipMlcBygla",
        "forum": "EpYnZpDpsQ",
        "replyto": "EpYnZpDpsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission431/Reviewer_Eryk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission431/Reviewer_Eryk"
        ],
        "content": {
            "summary": {
                "value": "This work proposes \"Learning from Randomness\" (LFR), which tackles the long-standing problem of removing augmentations from SSL. Instead, random projections of the representation are learnt via a bank of predictors, with the intuition that diverse random projections replicate a set of generic downstream tasks.\nAn objective like Barlow-Twins is used, as well as an iterative training procedure that updates the backbone and the projectors in separate steps. In order to have diverse random projections, the authors propose to sample several of them and select those that are more decorrelated.\n\nThe experiments show how the proposed method is a suitable option for time-series, tabular data and medical images datasets. Interestingly, no augmentation is used in all these settings, which I find interesting and novel. An insightful ablation study is also provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The problem of removing augmentations in SSL is of great importance in the community.\n\n* The paper is very well written, organized and presented.\n\n* The authors will release code."
            },
            "weaknesses": {
                "value": "* The experimental section does not include any medium/large scale dataset, which obfuscates the benefit of the method for any SSL task.\n\n* I found a transfer learning experiment lacking in the results.\n\n* More discussion about the pros/cons of using random projections would be valuable."
            },
            "questions": {
                "value": "I do not have a large amount of questions, since the paper is very well explained and motivated. However, I would greatly appreciate to discuss with the authors about the following:\n\n\n* The main intuition of this method is to create random downstream tasks that capture different aspects of $z$. Therefore, I would suggest adding a transfer learning experiment, in which an encoder $f_\\theta$ is learnt on a source dataset, and such features are transferred to other real downstream tasks. It would be really interesting to see a gain in such setting, wrt. other SOTA methods. I believe this would strongly reinforce the claim of the paper.\n\n* I wonder if random projections are enough to capture the diversity inherent in complex datasets. I think the paper would benefit from a discussion on the pros and cons of using random projections. Note that the datasets shown in the paper are considered small-scale in the community, which limit the understanding on how effective LFR is for _any_ dataset.   Larger, or more diverse, datasets would be extremely valuable. See following point.\n\n* I suggest evaluating LFR vs. computer vision methods in a different scenario than medical images. It is possible that SimCLR would perform better on CIFAR-10 or ImageNet, nevertheless that experiment would show how well LFR is performing against methods tailored for such scenarios. This would also help to understand how well random projections \"emulate\" the use of well-chosen augmentations. I think a natural images dataset is required for this paper for acceptance.\n  * In the Appendix, the authors provide an example of feature interpretability on CIFAR-10. Noting that the pipeline to train on natural images is already in place, I reinforce my observation that an evaluation on natural images should be part of the experimental section.\n\n* It would be great to have some discussion about how LFR could be applied to Transformers, given their wide use. Comparison with a masking approach would not be required, although of great interest.\n\n* A batch size of 256 is strongly detrimental for a contrastive approach such as SimCLR, for example. Small batch sizes harm performance for such methods. For the reader to fully grasp the scenarios where LFR is suitable, I suggest comparing with SimCLR in settings where the latter is known to excel.\n\n* I highly appreciated the breakdown of GPU hours required for this paper.\n\n-----\n\nOverall comment:\n\nThe method proposed is sound, and the goal of removing augmentations from SSL is a long-standing one. Learning from random projections seems a sensible way to tackle such problem. The overall architecture and objective is solid, I have no concerns in that sense. The manuscript is written elegantly, with the appropriate language. \n\nHowever, for the method to be rigorously evaluated, more experiments would be required. Otherwise, there is doubt about whether random projections are valuable for small datasets only, etc. \n\nNotably, I suggest the authors to:\n* Perform transfer learning experiments to support the main hypothesis of the work (random projections allow learning different aspects of $z$, thus making $z$ more generally applicable).\n* Evaluate in a setting where classic methods perform at their best, to understand the limitations of the proposed approach. I would suggest a natural images setting, using ImageNet ideally, or CIFAR-10/100 if GPU hours are a concern. \n\n* I think it would be interesting for the reader to see how the training behaves with joint training, I suggest adding some of these plots in the Appendix.\n\nI would be happy to increase my score after discussion and manuscript updates."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Reviewer_Eryk",
                    "ICLR.cc/2024/Conference/Submission431/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684932423,
        "cdate": 1698684932423,
        "tmdate": 1700576102159,
        "mdate": 1700576102159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "05jKvzuYbR",
        "forum": "EpYnZpDpsQ",
        "replyto": "EpYnZpDpsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission431/Reviewer_6gYm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission431/Reviewer_6gYm"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed the method of self-supervised representation learning that can be used to learn useful representations from different modalities, e.g. images, text, time series, tabular data. Proposed method is based on recent approach where we have architectures with an additional projector (Guillotine regularization, removed for a downstream task) tries to predict multiple random projectors. The authors provide some analysis of hyper-parameter sensitivity, different initializations of random projectors, etc. Comparison with other methods on different modalities are proposed (time series, tabular, image)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The method is simple and very generic. Removes the prior of knowing what data aug. we should use for many current SSL contrastive-based methods.\nS2. Paper easy to follow and well-written."
            },
            "weaknesses": {
                "value": "W1. The main weakness for me is inability to compare with the existing method on well-known datasets in computer vision tasks. We have 3 x time series, 3 x tabular, and 1 x image - where Kvasir is not commonly used dataset. Not sure if the results are not picking the datasets that show good results. Why not presents the results on ImageNet, and if computationally not possible - cifar100. \n\nW2. There's no evaluation on the different downstream task for the learned representation, e.g. feature extractor trained with LFR on Kvasir and evaluated on the other dataset similar and disimilar one. We do not know how the learned representation generalize, and what if we only memorize patterns that then can be then useful for the lin. evaluation.\n\nW3. We don't know the final computational overhead of the initialization and comparison to any SSL method. In the appendix we have total time spent for a particular dataset (e.g. Kvasir V100 1095 GPU hours for all experiments). Would be better to know the comparison between SimCLR/SimSiam vs LFR. \n\nW4. 2 time series datasets (HAR, Epilepsy) and tabular Income&HEPMASS have already good accuracy on the randomized init. What is intresting, some methods are below that (HAR - Autoencoder). \n\nW5. Lack of more theoretical explanation why it should work? What random projectors can be used? etc."
            },
            "questions": {
                "value": "Q1. How the method perform without using heavy SSL (SimCLR) data augmentations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686808362,
        "cdate": 1698686808362,
        "tmdate": 1699635969756,
        "mdate": 1699635969756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ocya568X84",
        "forum": "EpYnZpDpsQ",
        "replyto": "EpYnZpDpsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission431/Reviewer_gtio"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission431/Reviewer_gtio"
        ],
        "content": {
            "summary": {
                "value": "The authors of this paper propose a technique called Learning From Randomness (LFR), which allows the application of self supervised techniques in arbitrary data domains. The proposed method works by projecting the data into random representations, and then training a model to predict these random representations. The authors show that the resulting model can learn useful representations, even without domain knowledge for the datasets examined."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is very clear and easy to understand. I did not have any issues in grasping the points the authors are trying to make.\n\n- The method proposed is novel, as far as I am aware. It is also very interesting, as learning from random data is not a very explored area of research. As the authors note, the proposed technique enables self supervised learning without the need for domain knowledge, in order to create good augmentations for the data. There is also a clear benefit from using LFR in the datasets examined, without having to rely on complicated techniques.\n\n- The authors perform ablations on the random projectors used, as well as the required diversity of the random representations and the training procedure for the model. I find it interesting that the authors perform an EM-based approach in learning the model, instead of simple optimizing all of its parts all at once. Similarly, I find equally interesting the preprocessing step that selects the best random projectors to predict during training.\n\nOverall, I find the proposed method insightful, with clear benefits over previous work in datasets that are not as explored as the usual ones (e.g. CIFAR-10/100, ImageNet)."
            },
            "weaknesses": {
                "value": "- One of the issues I have with this paper is that despite the use of several datasets used to evaluate LFR, the commonly used ones such as CIFAR-10/100 are not among them. While I understand that LFR does not aim to improve performance on these datasets (since natural image augmentations already perform very well) it would be interesting to examine those to compare as well. It would be useful to know if LFR is better/worse than optimized augmentations, such as those used in SimCLR.\n\n- I think the paper could also be improved via further experiments on the following two subjects:\n\n  - I think it would be interesting to see some ablations on the distance metric used for training. Right now, the authors use Barlow Twins as the metric, but it would be interesting to perform ablations with e.g. MSE or Contrastive losses for this (although I must note that the authors do make an argument for this design decision in the paper).\n\n  - I think it would also be interesting to see the transferability of the trained models across different datasets. I would be interested in knowing if LFR leads to learning representations that are good for the particular dataset, or good for the chosen modality in general."
            },
            "questions": {
                "value": "I would be grateful if the authors could comment a bit on the choice of the number of projectors $K = 6$ and batch size $B = 256$. Intuitively, both of these values seem a bit small when trying to find diverse random projectors. The authors have tried going up to $K = 8$ and batch size $B = 512$, but I would like to know if they have tried higher values for these two hyperparameters (especially for the number of projectors $K$)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707091482,
        "cdate": 1698707091482,
        "tmdate": 1699635969687,
        "mdate": 1699635969687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rIxFvmROOs",
        "forum": "EpYnZpDpsQ",
        "replyto": "EpYnZpDpsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission431/Reviewer_ze2f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission431/Reviewer_ze2f"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of domain-agnostic self-supervised representation learning. The proposed method introduces multiple random projectors and corresponding predictors, and optimizes the batch-wise barlow twins loss, which constructs the Gram matrix instead of the empirical correlation matrix. To encourage the diversity, many projectors are initialized and then only 10% are subsampled for use. Experimental results on datasets from various domains show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Domain-agnostic representation learning is a timely topic.\n\n- The proposed idea is simple and the proposed method improves the performance in most cases."
            },
            "weaknesses": {
                "value": "- Discussion/comparison with other domain-agnostic methods seem to be not enough. For example, [Lee et al.] proposed a domain-agnostic augmentation strategy applied to image, speech, and tabular datasets, and [Wu et al.] proposed randomized quantization and experimented on image, point cloud, audio domains and the DABS benchmark. I suggest including discussion and experimental comparison with them. \n\n- It is good to see that experiments include various domains including time series, tabular, and image, but they seem to be relatively small and not commonly used for benchmarking machine learning models. For example, Kvasir is a medical image dataset, which is different from the widely used \"natural\" image datasets; it should be categorized differently from natural image datasets. Authors may want to refer to [Lee et al.] and [Wu et al.] to find commonly used datasets to provide the general applicability to various domains and scalability of the proposed method.\n\n- While the authors claim that the optimization strategy for the proposed method is EM, but it is not clear how the proposed alternating optimization is related with EM by looking at the formulation. I think the transition from Eq. (2) to Eq. (3--4) requires more explanation supported with some math.\n\n- The claim around batch-wise barlow twins that MSE is preferred over cross-entropy/contrastive/triplet losses is not justified. Isn't the batch-wise barlow twins loss just a kind of contrastive loss, in that it contrasts all samples within the batch? Note that the original contrastive loss (not the InfoNCE variation) also computes the MSE loss. An ablation study with different type of losses might also be helpful.\n\n- The criterion for diversity encouragement requires more intuition. It is hard to imagine what is going on when optimizing the proposed learning objective. Also, what is the computational cost for the NP-hard objective function?\n\n- The comparison might not be fair as the proposed method requires more computational cost to encode input with multiple random projectors and predictors compared to other baselines. The computational cost should be matched for a fair comparison and reported.\n\n[Lee et al.] i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning. In ICLR, 2021.\n\n[Wu et al.] Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning. In CVPR, 2023."
            },
            "questions": {
                "value": "Please address concerns in Weaknesses.\n\n> **post rebuttal**\n\nAfter discussion with authors, I feel that the experimental results are not sufficient to support the claim that they cover \"a wide range of representation learning tasks that span diverse modalities and real-world applications.\" Initially their experiments covered time series, medical image, and tabular domains, and the additional results in the natural image domain show that their method is not effective for natural images, compared to other baselines. **Authors are encouraged to explicitly limit the scope to the domains they experimented in the title/abstract/intro.**\n\nAlso, I am not sure if the comparison is fair (e.g., if they tuned hyperparameters for baselines properly), so experimental results are generally not convincing to me.\n\nThough I feel more confident on my rating, given that authors addressed all concerns from the other reviewers well and the proposed method is still interesting to me, I do not want to put too much weight to my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission431/Reviewer_ze2f"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768252871,
        "cdate": 1698768252871,
        "tmdate": 1700674015870,
        "mdate": 1700674015870,
        "license": "CC BY 4.0",
        "version": 2
    }
]