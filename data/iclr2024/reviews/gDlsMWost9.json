[
    {
        "id": "H1ttw8yJVt",
        "forum": "gDlsMWost9",
        "replyto": "gDlsMWost9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_mQUb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_mQUb"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an approach titled \"Multimodal-CoT,\" which focuses on the amalgamation of both language (text) and vision (images) modalities in a two-stage framework. The first stage addresses rationale generation, while the second concentrates on answer inference. An important feature of this paper is its strategy of integrating vision features into the language model. Such a technique, though previously explored in multimodal language and vision models, is shown to decrease rationale hallucination and consequently boost answer accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Quality:** The results seem promising, especially given the observation that this approach meshes well with various backbone models.\n   \n2. **Clarity:** The paper appears to provide comprehensive baselines and analyses, suggesting that the methodology and results have been presented in a clear and structured manner.\n   \n3. **Significance:** The ablation study emphasizes the significance of both the integration of vision features and the two-stage framework. These results suggest that each component of the design distinctly contributes to the observed enhancement in performance."
            },
            "weaknesses": {
                "value": "1. **Incremental Novelty:** The work leans heavily on prior multimodal LLM research (e.g., BLIP-2 (https://arxiv.org/pdf/2301.12597.pdf), MINIGPT-4 (https://arxiv.org/pdf/2304.10592.pdf)), specifically in terms of incorporating vision features into the language model. This reduces the perceived novelty of the presented model.\n   \n2. **Framework Design:** The two-stage framework, though effective, comes across as relatively straightforward. A deeper exploration or the introduction of more intricate strategies could potentially lead to even more enhanced results."
            },
            "questions": {
                "value": "1. **Differentiation from CoT:** It might not be accurate to call the proposed approach a variant of CoT, instead it's more like a two-stage pipeline framework. Additionally, what are the potential benefits or shortcomings of adopting a \"QCM->RA\" strategy, especially when paired with few-shot demonstrations? This alternative is more like a standard CoT approach. The authors are encouraged to compare with this baseline.\n\n2. **Additional Benchmark Results:** For a holistic understanding, it would be beneficial to see the results of \"UnifiedQA,\" \"FLAN-T5,\" and \"FLAN-Alpaca\" in Table 6. This would provide a comprehensive view and easy comparison with existing models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Reviewer_mQUb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822851451,
        "cdate": 1698822851451,
        "tmdate": 1699636572716,
        "mdate": 1699636572716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J823IzEwmv",
        "forum": "gDlsMWost9",
        "replyto": "gDlsMWost9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_9gAH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_9gAH"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach that integrates language and vision modalities into a two-stage framework for large language models. This framework enhances reasoning by generating intermediate reasoning chains before inferring answers. The method shows new state-of-the-art performance on the ScienceQA benchmark, particularly effective in models under 1 billion parameters, and addresses the issue of hallucination in answer inference."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Innovative Approach: The integration of multimodal data (text and images) into CoT reasoning is a significant advancement, addressing a gap in previous research which focused mainly on language modality.\n2. Mitigation of Hallucination: The approach specifically targets and successfully mitigates the issue of hallucination in answer inference, a common problem in smaller language models.\n3. Detailed Analysis: The paper provides a comprehensive background study and analysis of existing CoT techniques, enhancing the understanding of the field."
            },
            "weaknesses": {
                "value": "1. Limited Scope of Evaluation: The paper only evaluated their approach using 2 benchmark datasets like ScienceQA and AOKVQA. While these datasets are relevant and challenging, the paper represents a specific type of reasoning tasks. \n2. The paper demonstrates the effectiveness of the proposed method primarily in the context of encoder-decoder models. However, its effectiveness in popular left-to-right language models, which are widely used, is not explicitly addressed. This omission can limit the understanding of how the proposed method might perform or be adapted to these prevalent LMs."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5567/Reviewer_9gAH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823196221,
        "cdate": 1698823196221,
        "tmdate": 1699636572621,
        "mdate": 1699636572621,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZDnaMh6LLw",
        "forum": "gDlsMWost9",
        "replyto": "gDlsMWost9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_MqoG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_MqoG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents multimodal chain-of-thought. Comprehensive analysis shows that the proposed model outperforms the state-of-the-art models on two benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- State-of-the-art performance on two benchmarks.\n- Simple yet effective approach on improving reasoning in vision and language settings.\n- Comprehensive analysis on the proposed model."
            },
            "weaknesses": {
                "value": "- A few arguments are not convincing or well-supported. For instance, more rigorous experiments are needed to claim *surpassing human performance*: on the one hand, humans can show significant variances when working on the same problem; on the other hand, ScienceQA collects the human performance baseline with Amazon Mechanical Turk, which is quite hard to control the data quality.\n- This paper overclaims on multimodal CoT, while only vision and text are evaluated. Other modalities, such as audio, video, and touch, are not supported in the model.\n- A few points are not clear enough (see below for details)."
            },
            "questions": {
                "value": "1. It's surprising that a model with a FLAN-Alpaca-Base backbone can outperform GPT-4 (Table 4). Is the GPT-4 you tested multimodal or text-only? Did you use CoT prompting for GPT-4 as well?\n2. Related to the above question, shouldn't multimodal-CoT be considered as a prompting technique, which is orthogonal to the base model? If so, the references in Table 4 are probably misleading."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698954211609,
        "cdate": 1698954211609,
        "tmdate": 1699636572528,
        "mdate": 1699636572528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VsdAs87X6Q",
        "forum": "gDlsMWost9",
        "replyto": "gDlsMWost9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_USEM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5567/Reviewer_USEM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a two-stage multimodal CoT framework Multimodal-CoT, which separated rationale generation and answer inference. Through finetuning small models, this paper fused the vision features with the encoded language representations. Multimodal-CoT can alleviate the hallucinations while generating rationales and improving the accuracy of answers. Experiments in two benchmarks demonstrated the effectiveness of the proposed multimodal COT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper proposed a multimodal CoT reasoning framework by fusing the vision features extracted by ViT with the language features, which can mitigate the challenge of hallucination.\n2.\tThis paper separated the CoT reasoning process into two stages: rationale generation and answer inference.\n3.\tThis paper conducted extensive experiments and analysis. Experiment results demonstrated the effectiveness of the proposed methods."
            },
            "weaknesses": {
                "value": "1.\tThe performance of Multimodal-CoT falls behind some baselines, e.g. LLaVa (https://arxiv.org/abs/2304.08485) on the ScienceQA dataset and LXMERT on the AOKVQA dataset (https://aclanthology.org/D19-1514.pdf).\n2.\tIt would be better to add more explanation or motivation about separating the reasoning process into two-stage works."
            },
            "questions": {
                "value": "Typos Grammar Style And Presentation Improvements:\n1.\tIn section 4.2, \u201cfrozon\u201d in \u201cwe fetch the patch-level features by frozon vision\u201d should be amended to \u201cfrozen\u201d.\n2.\tThe Multimodal-CoT accuracy demonstrated in Table 9 and Table 10 is different from it in Table 4. Why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5567/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699420748628,
        "cdate": 1699420748628,
        "tmdate": 1699636572418,
        "mdate": 1699636572418,
        "license": "CC BY 4.0",
        "version": 2
    }
]