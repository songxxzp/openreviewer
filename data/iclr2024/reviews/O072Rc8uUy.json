[
    {
        "id": "UZUZWuj2Pp",
        "forum": "O072Rc8uUy",
        "replyto": "O072Rc8uUy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_EZDr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_EZDr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a locally progressive method for text-driven generation of semantically complex prompts. Existing methods cannot faithfully generate the complex prompts directly. Thus, this work takes an iterative approach: the complex prompt is broken up into segments and each segment is iteratively added to the 3D representation. However, directly optimizing for the new segment of the prompt each iteration leads to a muddled result where attributes bleed together. Thus, this work also conditions on a user specified edit region. This allows the method to only optimize a local region, thus preserving the rest of the 3D representation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Generation with complex prompts is very challenging and existing methods struggle on this task, while this work excels.\n- The semantic delta loss is an interesting contribution that is important for editing.\n- Explicitly defines a region in which the edits can take place to ensure preservation of the existing model.\n- The paper compares to numerous existing approaches for text-to-image generation showing superior performance on complex prompts and gives a thorough ablation of the components of the method.\n- Clear presentation: the paper is well written and makes good use of experiments/figures to support its claims. Figure 2 is especially helpful for understanding this approach."
            },
            "weaknesses": {
                "value": "- The local region for each edit must be manually entered by the user. This slightly limits the intuitive, easy-to-use nature of this work as compared to most other purely text-driven approaches.\n- If I am understanding correctly (see question for more details), the edit region can only be defined as an axis-aligned bounding box. This seems like it could be problematic for edits that do not fit nicely into an axis-aligned box.\n- Limited comparisons to relevant existing work. There exist methods for focusing on different parts of the text prompt (Attend and Excite [1]) for 2D image generation and editing. These methods have been shown to address issues with attribute binding and \u201ccatastrophic neglect.\u201d It would be helpful to see how a baseline performs using these approaches as the 2D model used for distillation. Additionally, DreamEditor [2] enables local editing on NeRFs using an explicit edit region. The region is inferred from the text description using attention maps from the diffusion model. A good baseline would be to use DreamEditor iteratively on each progressive edit as this still gives an explicit edit region, but does not require a user input bounding box.\n\nReferences:\n[1] Chefer, Hila, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\" ACM Transactions on Graphics (TOG) 42, no. 4 (2023): 1-10.\n[2] Zhuang, Jingyu, Chen Wang, Lingjie Liu, Liang Lin, and Guanbin Li. \"DreamEditor: Text-Driven 3D Scene Editing with Neural Fields.\" arXiv preprint arXiv:2306.13455 (2023)."
            },
            "questions": {
                "value": "- Since the user can only input the box center and the lengths of the box along each axis, it seems that the box will always be axis aligned. Is this not problematic for certain edits that do not line up well with the axes?\n- It would be helpful to clarify more how the semantic delta loss differs from (Armandpour et al.) [4]\n\nReferences:\n[4] Armandpour, Mohammadreza, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, and Mingyuan Zhou. \"Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond.\" arXiv preprint arXiv:2304.04968 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Reviewer_EZDr",
                    "ICLR.cc/2024/Conference/Submission2251/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735748354,
        "cdate": 1698735748354,
        "tmdate": 1700731692668,
        "mdate": 1700731692668,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oBtuNSEd1o",
        "forum": "O072Rc8uUy",
        "replyto": "O072Rc8uUy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_GY6s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_GY6s"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method for progressively generating intricate 3D content. Each generation phase progressively generates local content using progressive semantic prompts. To maintain the consistency of the progressive generation, a consist loss is employed. Additionally, an initialization loss is utilized to swiftly generate content in selected areas. Furthermore, the proposed OVERLAPPED SEMANTIC COMPONENT SUPPRESSION ensures that each progressive generation optimizes towards additional semantic prompts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The progressive approach is indeed a straightforward and effective method for generating complex 3D content. This ensures that each local element receives accurate optimization guidance.\n2.\"OVERLAPPED SEMANTIC COMPONENT SUPPRESSION\" can effectively optimize progressive 3D content in alignment with additional semantic prompts.\n3.The concept of \"Initial Loss\" contributes to achieving a stable and high-quality 3D content generation within 3D bounding box."
            },
            "weaknesses": {
                "value": "1. Further comparisons with other methods for achieving complex semantic prompt-driven 3D content generation are lacking.\n2. The overall pipeline is relatively straightforward and simplified. Independently generating each local content and optimizing it after the combining each local 3D content with 3D bounding boxes may yield improved results, particularly for the prompt like some object is on a tabletop."
            },
            "questions": {
                "value": "1.Is the progressive3D approach the only way to generate intricate 3D content? I want to see more comparative experiments to achieve a fairer comparison. For instance, one might first employ a complex semantic prompt to produce consistent images, then use these images to generate the corresponding 3D content. Alternatively, one could generate each object individually and subsequently merge them based on their bounding boxes, then the aggregated 3D content could be optimized to achieve a harmonious result. I am interested in understanding how to demonstrate that the progressive approach is a crucial and effective method for generating complex 3D content.\n\n2. How to resolve conflicts between subsequent generated results and earlier ones, such as an astronaut sitting on a red chair, when the first step generates a standing astronaut? Can the model optimize the transition from a standing astronaut to a sitting one?\n\n3.In the context of complex semantic prompts, does the order of prompt input have an impact on the generated results, and is it necessary to engage in simple semantic prompt planning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Reviewer_GY6s",
                    "ICLR.cc/2024/Conference/Submission2251/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755039700,
        "cdate": 1698755039700,
        "tmdate": 1700727493072,
        "mdate": 1700727493072,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XhYSQ6t4Up",
        "forum": "O072Rc8uUy",
        "replyto": "O072Rc8uUy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_dBWG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_dBWG"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the challenge of aligning complex prompts with generated 3D assets by employing a progressive framework that decomposes the generation process into multiple local editing tasks. By doing so, the authors achieve the generation of semantically precise 3D assets. Additionally, the paper introduces a novel dataset designed to evaluate the outcomes of compositional Text-to-3D generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written, with a clear structure and easily understandable language.\n\n2. The results demonstrate effective composition of objects with different attributes and relationships.\n\n3. The proposed dataset and evaluation metrics explore Text-to-3D benchmark in terms of composition and relationships, providing valuable insights for the community."
            },
            "weaknesses": {
                "value": "1. The main concern is the heavy reliance on human involvement throughout the pipeline. Users are required to provide prompt divisions and bounding boxes, and the process seems user-unfriendly, as users have to wait for the previous generation to finish before providing the next bounding box prompt.\n\n2. The paper mentions that current T2I diffusion models often struggle with complex prompts. However, there are existing methods [1,2,3] that address this problem. It would be beneficial to discuss why the authors did not directly utilize these methods, as it seems more straightforward and would save human labor. This discussion is currently missing from the paper.\n\n3. Figures 7 and 10 show inconsistencies with the claim that undesired regions remain unchanged. The leg of the astronaut turns green, and the foot is missing after adding the prompt \"and riding a red motorcycle.\"\n\n4. It would be more convincing if the paper showcased additional results (quantitative and qualitative) based on Fantasia3D. Given the low resolution of image space supervision in DreamTime and DreamFusion, the generated 3D assets appear blurry. Demonstrating the significant improvements offered by this method in more sophisticated Text-to-3D approaches would reinforce the paper's claims.\n\n5. Including an ablation study on the last term in the consistency loss (the one that imposes the empty region to be blank) would strengthen the paper's arguments.\n\n[1] Compositional visual generation with composable diffusion models, ECCV, 2022\n\n[2] Training-Free Structured Diffusion Guidance for Compositional Text-toImage Synthesis, ICLR, 2023\n\n[3] Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, ICLR, 2023"
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836079970,
        "cdate": 1698836079970,
        "tmdate": 1699636158345,
        "mdate": 1699636158345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1TEOUL39HM",
        "forum": "O072Rc8uUy",
        "replyto": "O072Rc8uUy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_S6PX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2251/Reviewer_S6PX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a general framework named Progressive3D for correctly generating 3D content when the given prompt is complex in semantics. Progressive3D decomposes the difficult creation process into a series of local editing steps and progressively generates the aiming object with binding attributes. Experiments conducted on complex prompts in CSP-100 demonstrate that the proposed Progressive3D can create 3D content consistent with complex prompts. The motivation of generate correct 3D content for a complex prompt in semantics is good, but the solution is inflexible. Because users need to provide 3D bounding box prompt for each prompt, which is inflexible and difficult to define."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Progressive3D can create precise 3D content prompted with complex semantics by decomposing a difficult generation process into a series of local editing steps.\n- Progressive3D could be incorporated into various text-to-3D methods driven by different 3D neural representations."
            },
            "weaknesses": {
                "value": "- The quality of generated 3D objects is poor.\n- The proposed method requires the 3D bounding box as the input, which is inflexible.\n- It is difficult for Progressive3D to change the attribute of the generated 3D objects, such as changing red to blue or metal to wood. If we want to edit the attribute, we might need to train the model case by case.\n- Only one dataset was used in the experiments."
            },
            "questions": {
                "value": "- How long does it take for complex prompts?\n- Given a complex prompt, how to decompose the complex text, automatically or manually? And How many steps are required?\n- How can we provide the 3D bounding box prompt, I think it is difficult. In my opinion, I think the 3D bounding box limits the application of the proposed method.\n- Can you report the CLIP-Score?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2251/Reviewer_S6PX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2251/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836855766,
        "cdate": 1698836855766,
        "tmdate": 1700649046593,
        "mdate": 1700649046593,
        "license": "CC BY 4.0",
        "version": 2
    }
]