[
    {
        "id": "SUHlBStOUa",
        "forum": "qL9gogRepu",
        "replyto": "qL9gogRepu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses an important class of tasks: evaluating semantic parsing outside of the realm of 1:1 mappings from form to meaning, potentially including an actual distribution over meanings.   They develop a dataset of templatically generated text-LF pairs where each sentence maps to two possible interpretations, use smart annotation of scalar rankings to get distributions over those ambiguities, and evaluate models on their ability to generate both possible meanings given some prior in-context-learning prompts, and ability to match the distribution of human judgements and to match inbalances of interpretion in the ICL inputs.  They find that models (most of the codegen variety) do quite bad as \"zero shot\" prediction of these LFs, somewhat well at producing a best LF in certain conditions but also quite poorly at generating well-calibrated distribution over the two choices."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It is a well-written paper with both clear reasoning and clear explanation, enough detail to be replicable and with the promised of publication of data and code. \n- It served to target specific questions about how models handle these ambiguities (e.g. the mapping token-level confidence to LF outputs), and is properly planned out to answer those questions. \n- Both the human judgments annotation and the constrained decoding seem very rigorously done."
            },
            "weaknesses": {
                "value": "- While just a quibble, I feel like the combination of ICL and very constrained templates makes their zero-shot setting very hard, and it's hard to draw conclusions from their findings there."
            },
            "questions": {
                "value": "- In more real-world semantic parsing tasks there can be dramatically more than just two possible semantic parses. Would this approach (particularly use of EASL in collecting human judgements) still work in contexts with far more acceptable parses per sentence?\n-  Insofar as all models were equally poor at a number of the zero-shot tasks at ZM_5, did the authors check the effect of raising that k parameter? \n- I was curious about the focus on capturing the training data biases rather than the human annotation judgements, for the few-shot setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4330/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_dZKu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819467736,
        "cdate": 1698819467736,
        "tmdate": 1699636402519,
        "mdate": 1699636402519,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dKogOi61DJ",
        "forum": "qL9gogRepu",
        "replyto": "qL9gogRepu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a task to convert an ambiguous sentence to its logical form to evaluate LLMs with zero-shot and few-shot evaluation. The paper studies five types of ambiguity and evaluates LLMs with a decent number of examples. They also proposed multiple metrics to interpret the model results. The dataset and experiment all together provide insights into how LLMs understand ambiguity and its difference from human ambiguity understanding."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation and writing are very clear. The paper is generally easy to follow.\n2. I like the human probability vs. model probability experiments personally, and seeing that humans have certain preferences on one interpretation than the other is interesting, and model prediction somehow matches it as well is very interesting too."
            },
            "weaknesses": {
                "value": "1. The generation task is hard, especially generating logical forms. Why not formulate this as a multi-choice problem?  Letting the model choose two from 10 possible combinations?\n2. Is there any quantitative analysis? What kind of errors does the model usually make?\n3. The evaluation metric can be improved. I have several questions about this. Why not use the same zero-shot and few-shot metric since the output format is the same? Why not use language interpretation instead of LF generations? Language interpretation is much more intuitive for the model than LF generations."
            },
            "questions": {
                "value": "1. Why not report ZM 100 in Figure 4?\n2. In section 3.1, the zero-shot experiment, how do you get multiple predictions? Do you sample k times? Your prompt doesn\u2019t encourage the model to predict two LFs, so the model only predicts one by demonstration. The zero-shot results could get better if you encourage two predictions in your prompt.\n3. in figure 2, any intuition of why cg-2b is outperforming cg-6b on LF1?\n4. It's more like a suggestion: Do you plan to extend the ambiguity to more than two interpretations?\n\nNotes:\n\n- Section 2.1 Metrics. The last sentence is confusing: \u201cThe higher, the better. \u201c If you are talking about k values, shouldn't it be the lower, the better? If it refers to the metric, then it should be explicit.\n- Equation 1 is very confusing. Do i  and k mean the same thing here?\n- Section 3, paragraph 2, \u201cHere\u201d\u21d2 \u201cIn this setting\u201d\n- Figure 4 is a Table, not a Figure.\n- Figure 5 is a bit hard to read when it comes to different types of semantic ambiguity. You can choose to put a different text color for important tokens inside the blue circle to increase the readability.\n- In Section 4, paragraph 2, what does \u201c1 sentence at a time\u201d mean when the percentage of the LF0 sentences is from 0 to 100 in increments of 10?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4330/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4330/Reviewer_49nJ",
                    "ICLR.cc/2024/Conference/Submission4330/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894154336,
        "cdate": 1698894154336,
        "tmdate": 1700583443257,
        "mdate": 1700583443257,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UkKbjonFa0",
        "forum": "qL9gogRepu",
        "replyto": "qL9gogRepu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_Qhxa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_Qhxa"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a benchmark suite and set of corresponding metrics for measuring models' behavior in the face of ambiguity in a semantic parsing task. The task is set in an in-context learning setting where models are provided with examples of (natural language, logical form) pairs and asked to translate a new natural language sentence into a logical form (LF).\n\nThere are two test settings:\n* A \"zero-shot\" setting where the model is expected to infer the existence of an ambiguity in the input example that did not show up in its few-shot examples. For example, by Appendix A.2, the model might see the LF for `Mary with the camera` (which, side note, seems ungrammatical to me at least when interpreted in the relevant sense) and \"Galileo saw with the camera\" (which...also seems ungrammatical absent context), it's tested on whether it recovers that \"Galileo saw Mary with the camera\" is ambiguous, by checking if both possible interpretations get high enough probability (or ranking) in its output distribution.\n* A \"mixed prompting\" setting where the model is given ambiguous examples in the prompt which resolve one way or the other a given proportion of the time (e.g., attaching high or low) and it is tested on the degree to which it approximates this ambiguity resolution behavior in its outputs (either on a dataset level in what it ends up decoding on lots of examples, or on an instance level in how it allocates relative probability)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper aims at an important problem (handling ambiguity in semantic parsing).\n* The setup is clever and allows for some interesting analyses. I think the looking at the token-level confidences to see how model uncertainty is reflected in ambiguity-resolution\u2013dependent choice points, as done in Figure 5, is a useful idea.\n* The comparison to human behavior (Section 3.2) is interesting and I imagine could seed future experiments."
            },
            "weaknesses": {
                "value": "I'm worried about how we assign meaning to the various results, and I'm not sure how this result would feed into future work that helps parsers handle ambiguity better.\n\n1. Human experiments: humans were given both interpretations and asked to assign confidences to them. This seems a bit different from what the models were asked to do in the zero-shot experiments, which is implicitly pick out the ambiguity on their own. I understand it'd be hard to elicit this kind of behavior from humans \u2014 ideally you would ask them a question whose answer hinges on the ambiguity and get some measure of the uncertainty in their answer, but it's unclear what that measure would be. The measured degree of uncertainty from humans in this setting may quite exceed the actual uncertainty when reading and interpreting language in practice, as humans may simply snap to one reading or the other. So it may not be fair to compare this result to model results \u2014 though of course, there is no clear analogy between what humans and models are doing anyway, so I'm not sure what the exact criteria for a fair comparison should be. It might seem like a safer bet to compare humans to models on a similar task, where the model is given both interpretations and is asked to give a confidence score, or repeat the one that it prefers \u2014 for example, I think it'd be a strict improvement on the current paper to do the human comparison to the FIM relative confidence results instead. But of course, the confidences reported through that process may not be ecologically valid with respect to the paper's broader goal of characterizing LM behavior on ambiguous semantic parsing examples.\n2. More broadly, the paper relies on a mostly implicit assumption that the _appropriate_ behavior for an LM is to represent multiple possible readings in its top-k outputs, or for the probabilities of each possible reading to reflect the proportions of related readings in their prompts / ICL data. It isn't obvious to me that this is what we want or that it is the right way of approaching the question of whether LMs can handle ambiguity in semantic parsing appropriately. When prompted for a single output, why _should_ the model distribute its probabilities in any particular way? There's nothing that says that we have to use the top-k candidates in order to represent a set of semantically distinct alternatives, and indeed it is suboptimal for this purpose, as the paper notes that the top-k results are usually variations of the same interpretation. I can see how this may make sense if we were to, for example, use the LM probabilities as a prior to guide some kind of search through the space of parses; we then want all plausible interpretations to be discoverable. But it's not clear that's the best strategy in practice if we're going to be using LMs anyway: why not just ask it, for example, to decode all possible interpretations in sequence? There's a big space of possible things to do here if we want models to handle ambiguity, and unfortunately the results of these experiments only bear on a small subset whose promise is unclear. Not that the experiments done are bad or invalid, but I think the paper needs to make a clear argument about the nature of the construct being tested and what these results are supposed to inform.\n3. On that note, it's unclear to me from the arguments in the paper how these results can actively guide future efforts to build semantic parsing systems that handle ambiguity better. I think the burden does lie on this paper to make that case, at least in principle (I'm not saying it needs more ML experiments)."
            },
            "questions": {
                "value": "Section 3.1:\n* I think Figure 2 would be better as a grouped bar chart? It doesn't really make sense to me to have multiple models along a line here.\n* \"we rarely observe any models predicting both LFs correctly; one exception to this is conjunction ambiguity\" \u2014 seems also true for `bound`?\n\nSection 6:\n* \"statements be ambiguous\" (typo)\n* \"Making this assumption can reduce semantic parsing to syntactic parsing\" \u2014 it's very unclear to me what this means\n* The argument in the second to last paragraph that datasets need to have multiple judgments seems wrong to me. If there is indeed annotator disagreement, this disagreement will show up implicitly spread across multiple examples. Learning to maximize the likelihood of the data should teach the model the right kind of uncertainty. Multiple annotations aren't actually necessary for learning this \u2014 though they might be necessary for _evaluating_ it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698964163001,
        "cdate": 1698964163001,
        "tmdate": 1699636402383,
        "mdate": 1699636402383,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x78TtdicYz",
        "forum": "qL9gogRepu",
        "replyto": "qL9gogRepu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_PRrS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4330/Reviewer_PRrS"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the longstanding issue of ambiguity in natural language and its implications for semantic parsing in AI. It delves into the nature of language ambiguity, highlighting how it stems from the balance of communication efficiency and interpretive flexibility, and poses challenges for AI systems that lack human-like commonsense knowledge and context. To address these challenges, the authors propose a novel framework and the Ambiguous Parsing (AMP) dataset, which includes various types of ambiguities paired with dual logical forms (LFs). This resource is aimed at enhancing the performance of large language models (LLMs) in semantic parsing tasks, especially in handling ambiguity.\n\nThe study introduces two tasks to assess how well LLMs, utilizing in-context learning (ICL), can capture multiple interpretations of an ambiguous input. These tasks are designed to evaluate model performance in both zero-shot and few-shot settings, with a series of metrics developed to quantify their ability to predict and represent ambiguity. The paper also reports on models' performance, noting that while models can sometimes mirror human preference for certain interpretations, they generally fall short in predicting all possible parses. Additionally, it is observed that some models are quite adept at reflecting the distribution of interpretations in mixed-prompt scenarios, offering insight into in-context learning amidst conflicting evidence."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The AMP dataset is a significant contribution, providing a resource specifically designed for investigating ambiguity in semantic parsing, which is a relatively unexplored area.\n\n2. The paper takes a comprehensive approach by addressing the challenge from the perspective of both dataset creation and model evaluation.\n\n3. The introduction of zero-shot and few-shot tasks offers a rigorous evaluation framework for future research on ambiguity in semantic parsing.\n\n4. The development of new metrics to assess the models\u2019 ability to handle ambiguity is a noteworthy contribution that can guide subsequent model development.\n\n5. The results contribute interesting insights into the capabilities and limitations of current LLMs in capturing ambiguity through zero-shot and in-context learning."
            },
            "weaknesses": {
                "value": "1. While the paper provides a strong foundation, it could benefit from a more detailed exploration of how ambiguity affects real-world applications of semantic parsing.\n\n2. The AMP dataset, while novel, might still be limited in scope and diversity, potentially affecting the robustness of the study\u2019s conclusions.\n\n3. It is unclear how the proposed methods deal with the dynamic nature of conversational context, which can significantly affect ambiguity resolution."
            },
            "questions": {
                "value": "1. How do you foresee the findings of this research being applied in practical AI systems, particularly in areas where ambiguity can have significant consequences, like in legal or healthcare settings?\n\n2. Is the AMP dataset extensible, and are there plans to include more complex or nuanced forms of ambiguities, such as cultural or idiomatic ones?\n\n3/ Could you elaborate on the selection process for the five types of natural language ambiguities included in your study? Were there other types of ambiguities considered but excluded?\n\n4. The use of synthetic data might not fully capture the complexity of natural language ambiguities encountered in real-world scenarios. How well do the findings translate to naturally occurring datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4330/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699030498050,
        "cdate": 1699030498050,
        "tmdate": 1699636402315,
        "mdate": 1699636402315,
        "license": "CC BY 4.0",
        "version": 2
    }
]