[
    {
        "id": "IlG7MHHUa8",
        "forum": "7erlRDoaV8",
        "replyto": "7erlRDoaV8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_KWLr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_KWLr"
        ],
        "content": {
            "summary": {
                "value": "The paper shows that models edited by model editing methods ROME and MEMIT still contain sensitive information and that the information is not fully \"deleted\" from the model. Several white-box and one black-box attack are proposed which are used to extract information from edited models, given that the attacker has an attack budget B. If the sensitive information is in within the B output candidates the information is assumed to be leaked. In addition to the attacks, the paper proposes multiple defense methods. Both the attacks and defenses are evaluated on the Counterfact and zsRE dataset. The evalution shows that the defense methods are not enough to defend against extraction attacks and that even in a black-box setting, information can still be extracted after editing the model."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper is well written and easy to follow\n- code and everything to reproduce the experiments is given\n- the evaluation is quite thorough, evaluating multiple defenses against multiple attacks"
            },
            "weaknesses": {
                "value": "For me it is not quite clear how the proposed defense methods are combined with the model editing techniques. In the experiments, ROME and MEMIT are used as model editing techniques. However, it is not mentioned how the different optimization objectives for the proposed defense techniques are used in combination with these methods. This could be formulated a bit clearer.\n\nMisc:\n- it would be easier to read if the paragraph in 4.2 also had a bold subheading with the name of the attack, instead of putting the name in the heading of the section. This would make it easier for readers to spot the names of the different attacks."
            },
            "questions": {
                "value": "- **Q1:** Why use only single-token answers? If I understand this correctly, this way it is not possible to extract answers which consist of multiple tokens, correct? Is it possible to modify your approach to make this work for multi-token answers?\n- **Q2:** I don't quite understand how the Head Projection defense works. What exactly are the values D_answer and D_k? As far as I understand, D_answer is a probability distribution, while D_k is a single value? Could you clarify the loss function and what exactly is optimized for this defense?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "All ethical considerations are addressed in the ethics statement, which is why no ethical review is needed."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735140332,
        "cdate": 1698735140332,
        "tmdate": 1699636710205,
        "mdate": 1699636710205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7P6ZFkXvv6",
        "forum": "7erlRDoaV8",
        "replyto": "7erlRDoaV8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_X7HP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_X7HP"
        ],
        "content": {
            "summary": {
                "value": "This paper studies whether a piece of information can be effectively deleted from an LLM. The authors adapt two model editing techniques to the task of suppressing a piece of information from an LLM. They then evaluate the robustness of these techniques, combined with different defense approaches, to several new attacks (both black-box and white-box) aiming to extract that piece of information from the LLM. The authors propose a new threat model, where the attack is considered successful if the targeted information is among B candidates produced by the attack. Empirical results using one of the model editing techniques, ROME, show that it\u2019s not always effective, as the targeted information can still be extracted 38% of the time in a white-box setting and 29% of the time in a black-box setting. The authors further show that defense techniques can reduce the risk at a small cost in utility, but that in some cases they still remain vulnerable to attacks they don\u2019t explicitly protect against."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- 1) Important problem: can information be deleted from an LLM? The premise of the paper, that the right way to delete information is to modify the model post-hoc instead of curating the training dataset, is quite contentious. In spite of this, for practical reasons model developers might indeed not curate their training data, which motivates the need to evaluate the robustness of model editing techniques.\n- 2) Well-motivated threat model, based on the insight that considering some information to be deleted only if it cannot be recovered directly (B=1) is an insufficient requirement.\n- 3) The technical contribution of the paper (attacks and defenses) is insightful, well motivated and clearly described."
            },
            "weaknesses": {
                "value": "- 1) The findings of the paper are somewhat expected, as the model editing techniques being evaluated are heuristics and don\u2019t come with formal guarantees of robustness against attacks. Similarly, it is expected that a defense designed to mitigate a specific attack is robust against that attack but not necessarily against other attacks.\n- 2) Insufficient analysis of results. I was left wondering what are the technical differences between ROME and MEMIT and whether this could explain some of the differences in the results. \n\nMinor (suggestions for improvement):\n- 3) Confusing usage of the term \u201csensitive\u201d: The definition used by the authors includes \u201ctoxic\u201d information: \u201cModels can also generate text reflecting beliefs that cause direct psychological harm to people (i.e. toxic generated text) (Kenton et al., 2021). Facts or beliefs of this kind are known as sensitive information (Brown et al., 2022)\u201d. I'm pretty sure that\u2019s not how Brown et al. use the word sensitive. In the privacy domain, \u201csensitive information\u201d refers to protected characteristics about an individual (https://commission.europa.eu/law/law-topic/data-protection/reform/rules-business-and-organisations/legal-grounds-processing-data/sensitive-data/what-personal-data-considered-sensitive_en) or is sometimes used colloquially to denote private information that a model or system should not disclose. To avoid confusion, I would suggest using a different term that explicitly refers to \u201ctoxic\u201d information."
            },
            "questions": {
                "value": "- 1) What are the results of attacking the MEMIT method without any defenses (i.e., the equivalent of Figure 4)? The paper\u2019s second claim is that model editing methods fail to delete information; basing it on only one of two editing methods studied in the paper weakens the claim and raises questions.\n- 2) Do the authors think that model editing is technically possible with formal guarantees against attacks and what are, in the authors\u2019 opinion, promising directions for future work in this domain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773011868,
        "cdate": 1698773011868,
        "tmdate": 1699636710082,
        "mdate": 1699636710082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u7LTeQHABf",
        "forum": "7erlRDoaV8",
        "replyto": "7erlRDoaV8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_GnJt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_GnJt"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates model editing methods to remove sensitive information from LLM. This work shows that \"deleted\" information can be extracted from the hidden state when the attacker uses a smaller budget of verification attempts. Simple rewriting of prompts can also cause LLM to generate sensitive information. This is an interesting study, and the attack and defense methods it provides are worthy of further study and discussion."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work elaborates on the security issues of LLMs from the perspective that hidden states may leak sensitive information. It presents potential attack methods and defense strategies."
            },
            "weaknesses": {
                "value": "This work provides an incomplete description of the reasons behind some experimental phenomena. The reasons or intuitions why defense strategies based on data augmentation do not work are not revealed."
            },
            "questions": {
                "value": "This paper shows some interesting results. My main question is whether the author can give more detailed insights or possible mechanisms for the findings in the paper. For example, why do the hidden states of LLMs reveal sensitive information? What is the intuition behind this phenomenon? Why are defense strategies based on data augmentation ineffective?\n\nIn addition, can fine-tuning, a typical defense strategy, be combined with the defense scheme (e.g., Head Projection Defense) proposed in this paper to produce a more powerful defense method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6400/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6400/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6400/Reviewer_GnJt"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699092077320,
        "cdate": 1699092077320,
        "tmdate": 1699636709964,
        "mdate": 1699636709964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zqpMtEw8hG",
        "forum": "7erlRDoaV8",
        "replyto": "7erlRDoaV8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_174w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6400/Reviewer_174w"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the removal of memorized information from language models via direct editing of model weights. The paper first establishes a threat model for  LLM extraction, based on the notion of recovery of sensitive information from $B$ candidates. The paper then describes several new attacks: two variants of whitebox attacks, in which an attacker may utilize probabilities computed from intermediate hidden states in the model to aid in recovery, and a blackbox attack, in which an input query is \u201crephrased'' multiple times by the model to generate a diverse set of candidate responses. Finally, the paper describes and evaluates several defenses against the attacks, which are able to be applied in conjunction with existing model editing techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Paper works on interesting and challenging problems, which are highly relevant to real-life use cases.\n\n- The paper is novel in its framing of the problem - in particular, considering model editing for data removal is fairly unique in literature.\n\n- The paper is quite broad spanning, presenting both multiple attacks and defenses for machine unlearning."
            },
            "weaknesses": {
                "value": "**Lack of experiments** - The paper could be more comprehensive if more scenarios were presented in the paper, e.g. combining attacks (to maximize extraction under a budget) or combining defenses (to explore how much risks could be minimized in practice). I find it hard to draw actionable conclusions from the results, and further discussion and variety of results may help justify the strength of model editing as the de-facto paradigm for unlearning.\n\n**No comparison to prior works** - I\u2019m concerned about the lack of comparison to prior works. While the paper claims that prior works is not applicable due to focus on removal of influence of a pair $(x,y)$, the problem formalized in the paper (based on the ASR metric) is exactly the recovery of a single token label $y_i$ given a specific prompt $x_i$. Hence, I\u2019m confused why it\u2019s not possible to compare with prior approaches in approximate unlearning."
            },
            "questions": {
                "value": "- I\u2019m a bit confused by the distinction between the password attempts and parallel extractions scenarios in the context of the paper. Is the distinction here meant to be that information gained in the recovery of a previous attempt may be utilized to aid recovery of the next attempt? If so, I do not understand how the attacks as described utilize information in this way (i.e. it seems all attack methods generate a candidate set for a budget $B$ in parallel anyways). Is such an attack possible?\n\n- In Sec. 2 \u201cAttacking LLMs for Sensitive Information\u201d, the paper claims that the described method does not assume access to the exact text used during pre-training of the model. However, the experiments do assume such access, as even in the rephrasing attack, the ground-truth prompt is perturbed by a rephrasing model. Is it possible to run the rephrasing attack without the ground-truth prompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6400/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6400/Reviewer_174w",
                    "ICLR.cc/2024/Conference/Submission6400/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699269134633,
        "cdate": 1699269134633,
        "tmdate": 1701054382507,
        "mdate": 1701054382507,
        "license": "CC BY 4.0",
        "version": 2
    }
]