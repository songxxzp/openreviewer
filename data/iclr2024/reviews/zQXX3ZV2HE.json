[
    {
        "id": "OmpkrJYxkp",
        "forum": "zQXX3ZV2HE",
        "replyto": "zQXX3ZV2HE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_FvgA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_FvgA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a approach called Adversarial Instance Attacks (AIA) to attack the interaction predictions between humans and objects in the context of Human Object Interaction (HOI) tasks. The authors argue that existing adversarial attacks on object detection do not effectively extend to interactions, as interactions are more complex and require considering spatial and semantic relationships. AIA is designed to generate adversarial instances that distract the model's attention away from the original interaction, without disrupting the detection of human and object bounding boxes or their categories. The proposed AIA framework comprises three main modules: Interaction Area Sampling (IAS), Object Category Search (OCS), and Adversarial Instance Generation (AIG). The authors claim that AIA outperforms other attack methods in attacking interactions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Innovative Approach: The paper introduces a novel approach to attacking interactions in HOI tasks, addressing a gap in the existing literature.\n\nExperimental Results: The paper presents a thorough evaluation of the proposed AIA framework and compares it with other attack methods on various target and surrogate models. The results indicate that AIA is effective in attacking interactions.\n\nDiscussion on Perturbation Bounds: The paper provides an insightful analysis of the impact of different perturbation bounds on the success of attacks, shedding light on how noise affects interaction recognition."
            },
            "weaknesses": {
                "value": "Lack of Clarity: The paper is challenging to follow due to its complex technical language and insufficient contextual explanations. It assumes a high level of familiarity with the subject matter, making it less accessible to a broad audience.\n\nAmbiguity in Terminology: Some terms and acronyms are not adequately defined or clarified, such as \"Adversarial Instance Attacks,\" \"Adversarial Interaction Attack,\" and \"HOTR.\"\n\nMissing Visual Aids: Given the complexity of the proposed framework, the paper would benefit from visual aids, diagrams, or flowcharts to help readers understand the different modules and their interactions.\n\nEvaluation Metrics: The paper could benefit from a more detailed discussion of the evaluation metrics used and how they relate to the effectiveness of the attacks."
            },
            "questions": {
                "value": "1. Could you provide more clarity on the specific datasets used in the experiments and how they were collected or prepared?\n\n2. Can you elaborate on the definition of \"L\" in the context of perturbation bounds? What does it represent, and how is it determined?\n\n3. How does the AIA framework perform against state-of-the-art HOI models, and are there specific models it is more effective against?\n\n4. What kind of perturbations or attacks are used in the experiments? Is there a specific method for generating these adversarial instances?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2189/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2189/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2189/Reviewer_FvgA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698417808331,
        "cdate": 1698417808331,
        "tmdate": 1699636152756,
        "mdate": 1699636152756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dqBBVbQkl0",
        "forum": "zQXX3ZV2HE",
        "replyto": "zQXX3ZV2HE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_fSia"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_fSia"
        ],
        "content": {
            "summary": {
                "value": "The authors have introduced an adversarial attack problem that targets interactions between humans and objects. They evaluate this problem within the context of the Human Object Interaction (HOI) task, as proposed by Gkioxari et al. in 2018. Furthermore, the authors have successfully demonstrated an attacking framework for this novel problem"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes to tackle a new problem for adversarial attack for interactions between human and objects, based on the Human Object Interaction (HOI) task (Gkioxari et al., 2018), which is a new contribution to the field and might be interested to the adversarial attack community. However, the motivation is not clear when it is a difficult problem compared to existing attack problems. And the evaluation and writing quality are poor."
            },
            "weaknesses": {
                "value": "Deficient Writing Quality: The paper exhibits evident signs of hasty preparation, accompanied by numerous errors. To illustrate a few examples, some of which are highlighted in bold text:\n\n* The reviewer could not understand \u201con the interaction between scene contents and **realize**\u2019 based on\u201d \n\n* The math notations of {$ h_i^b, o_i^b, o_i^l, a_i $} are incorrect and confusing. It does not make sense to use a set { } for both coordinates and categories. And defining $a_i$ as the categories of the object is wrong.\n\n* directly adding noise to the union area not only **drop the precise** of interaction perception but also **bring** .... \n\nThe motivation is unclear. The arguments within the sentences lack proper support. For instance, the sentence \"a direct strategy to disturb the interaction is adding noises on the union area of human and object, following the feature extraction procedure in HOI. However, directly adding noise to the union area not only drop the precise of interaction perception but also bring interference in locating and classifying human and objects\" lacks clarity and substantiation.\n\nUnderstanding the paper's concept is challenging, primarily due to its poor writing quality. For example, the exact process for generating \"N^q candidate interaction areas\" is not explained and appears disconnected from the subsequent content. Upon reviewing Section 3.3, the reviewer found it to be incomplete, offering only a feature extraction process without any content related to sampling. The method for obtaining the extracted candidate predicate areas $A$ is also unclear.\n\nThe rationale behind distance calculation remains unclear, as does the definition of the midpoint function $f_{mid}()$.\n\n\"To address this issue, we employ random assignment to connect perplexing object categories to candidate predicate areas in order to generate the attack instances.\" However, the explanation for this random assignment is unclear.\n\nIn summary, while the paper may address an important problem of black-box adversarial attacks in scene interactions, its writing quality falls significantly below the standards of ICLR. Significant revisions are required. Therefore, the reviewer recommends clear rejection.\"\n\n\nThe equation (6) seems to contradict to the original claim of the paper  \"our goal is to bring confusion on the\ninteraction of a pair of human and an object, without contaminating their detection results including\nthe human bounding box, object bounding box, and object category\", as the object bounding boxes seem to be changed. Please provide more clarifications to it."
            },
            "questions": {
                "value": "Please refer to the above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647640926,
        "cdate": 1698647640926,
        "tmdate": 1699636152669,
        "mdate": 1699636152669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PaOkHcypcW",
        "forum": "zQXX3ZV2HE",
        "replyto": "zQXX3ZV2HE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_62sG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_62sG"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a black-box attack method against Human Object Interaction (HOI). The proposed method of Adversarial Instance Attacks (AIA) consists of an Interaction Area Sampling (IAS) module for finding the vulnerable areas, an Object Category Search (OCS) module for assigning the confusing category, and an Adversarial Instance Generation (AIG) module for attacking effects. The experiment validations on the V-COCO datasets are given to demonstrate its effectiveness on various models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed attack method is conducted in a new area, Human Object Interaction.\n2. Choosing the fragility of one image to attack is an interesting and reasonable idea for Human Object Interaction."
            },
            "weaknesses": {
                "value": "1. For the experiments, the comparisons between I-FGSM and PGD on detectors and the proposed method are unfair. The authors use the transfer results by I-FGSM and PGD to attack HOI, where the adversarial examples based on the architecture of detectors (e.g., Faster RCNN and YOLO ) are generated. Thus, these methods have a weak transferability across the different tasks. However, the proposed method has more knowledge about the attacked task instead of only detectors.\n2. The tasks of HOI highly rely on the results of detections. The existing attacks for detection can be directly employed in this task, leading to awful results. From the proposed method, the attacks aim to fool the interactions between the human and object, instead of the human and the object itself. If so, wrong detection results and boxes easily lead to wrong interactions. Thus, those attacks against object detection, especially for fooling the detection boxes should be involved in comparisons. Besides, why only attacking the interaction in HOI is worthy of investigating?\n3. Expect for the illustration in Figure 1, more visualizations with various methods (e.g., PGD and I-FGSM) should be involved to demonstrate its imperceptibility and effectiveness.\n4. There are some minor problems.\n- In the caption of Figure 1, \u2018remaining0\u2019 seems a typo.\n- Some formats of references are wrong and inconsistent."
            },
            "questions": {
                "value": "1. The comparison between I-FGSM and PGD on detectors and the proposed method for HOI is unfair as the prior knowledge is different. \n2. Discuss the motivation for only attacking the interaction in HOI, and give some attack results on only attacking the detectors.\n3. Give more visual comparisons between various methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744806152,
        "cdate": 1698744806152,
        "tmdate": 1699636152594,
        "mdate": 1699636152594,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5pLSgeN3Zb",
        "forum": "zQXX3ZV2HE",
        "replyto": "zQXX3ZV2HE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_L4HX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2189/Reviewer_L4HX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new concept for adversarial attack on Human Object Interaction (HOI). Different from adversarial attack on object classification or detection, there was less work focused on HOI attack. In this paper, the main idea is to find a proper region in an image, where an invisible object is inserted and which will mislead the HOI classifier. To this end, three modules are proposed, i.e., Interaction Area Sampling module, Object Category Search module, and Adversarial Instance Generation module."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A novel adversarial attack is defined for human object interaction recognition task. HOI recognition task is a high-level task with respect to object classification or detection tasks. It motivation of this paper makes sense which is to insert an inviable object to mislead the HOI classifier."
            },
            "weaknesses": {
                "value": "- My main concern is that the proposed attack is somewhat an untargeted attack for an HOI recognition system, where we cannot choose the target HOI category, instead, the target HOI category is determined by the method via finding the most confusing interaction. I am not sure the whether such attack will happen in real applications or not.\n- The proposed method is straightforward, which is combination of existing techniques such as Targeted adversarial Objectness Gradient attacks (TOG) in Adversarial Instance Generation module."
            },
            "questions": {
                "value": "Can we conduct a targeted adversarial attack for HOI recognition, which is that we can explicitly choose the target HOI category we want the attack achieves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2189/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698933201485,
        "cdate": 1698933201485,
        "tmdate": 1699636152505,
        "mdate": 1699636152505,
        "license": "CC BY 4.0",
        "version": 2
    }
]