[
    {
        "id": "zfkInVhKgv",
        "forum": "9ZUYJpvIys",
        "replyto": "9ZUYJpvIys",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes TOSS, which is an image-to-image diffusion model. Comparing to pioneering work like Zero123, TOSS is different because it's not only conditioned on input image and relative camera pose, but also an optional text prompt. \n\nSince single image to other views are naturally an ill-pose problem, the authors claim that text prompt can provide more detailed guidance, which allows for increasing plausibility and controllability, and consequently better quality. In TOSS authors also analyze the drawbacks of previous conditioning mechanism and propose a dense attention mechanism for improved performance. \n\nAuthors provide a valid pipeline for automatic captioning and train the TOSS model on the large-scale Objaverse dataset. The extensive experiments prove the validness of each components and the superior performance of the proposed TOSS model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, this paper is motivated, well-organized and provide valuable results. In detail I find paper's strength in several aspects:\n\n1. The design of dense attention mechanism and the necessity of text as input are well motivated, with clear reason and proof.\n\n2. The experiment part is sufficient. Extensive experiments have been done on not only for the comparison and main results, but also ablation on a wide range of components.\n\n3. The authors also propose some valuable practices about image-to-image diffusion model, for example the expert denoiser design."
            },
            "weaknesses": {
                "value": "### 1. Over-claim multiview-consistency ###\n\nI think the fundamental weakness for this paper is over-claim. In Sec.3.2.1, authors list two advantages of introducing text-prompt: plausibility and controllability, and I agree these two points. However, in the other parts of the paper, for example the Fig.1's caption, authors list *multiview-consistency* as another improvement, which I totally disagree.\n\nFundamentally, TOSS is like the Zero123 but with more input condition, it doesn't adopt some explicit design to guarantee the multiview-consistency like in SyncDreamer[1], so naturally it shouldn't be multi-view consistent. And although authors claim this in a lot of places in paper, I didn't find explanation towards this point.\n\nAlso, in Fig.13, while author titled it as \"Generated Multiview-consistent images\", it's actually not. For example in the provided microphone samples, I can clearly find it's not consistent across different views. So I think authors should revise the manuscript and avoid over-claim on being multivew-consistent.\n\n### 2. Emphasize the controllability ###\n\nAnother minor weakness is I hope authors can emphasize more on the TOSS's controllability. I would like to see the authors provide more visualization results where from the same input image, different text can generate different novel views, currently the results are mainly in supplementary and I think it's better to emphasize this ability and move the part to the main paper.\n\n[1] Liu, Yuan, et al. \"SyncDreamer: Generating Multiview-consistent Images from a Single-view Image.\" arXiv preprint arXiv:2309.03453 (2023)."
            },
            "questions": {
                "value": "In Sec4.5, what is the detail meaning of \"replace the SD v1.4 model\". Does it mean you replace the VAE encoder and the decoder? Does it replaced only in inference or in the training and re-train the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_LBZY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698104378304,
        "cdate": 1698104378304,
        "tmdate": 1699636308510,
        "mdate": 1699636308510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "emimCS8fOR",
        "forum": "9ZUYJpvIys",
        "replyto": "9ZUYJpvIys",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_Givb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a TOSS that is text to the task of novel view synthesis (NVS) from a single image. Compared to a previous method (Zero123), TOSS utilizes a text as high-level semantic information to constrain the NVS solution space. It is based on the text-to-image Stable Diffusion pre-trained on the large-scale dataset. The proposed method achieves plausible results and those are controllable. The effectiveness of the proposed method is validated on the dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[Novelty] \n\n- This paper utilizes diffusion prior and text embedding features with cross attention to perform NVS using a single image.\n\n- This improves reconstruction performance for unseen areas and allows for consistent image generation.\n\n[Quality] \n\n- Overall, the paper is easy to follow and well written.\n\n- This paper reports the impact of each module of the proposed method in terms of performance as appropriate.\n\n[Clarity] \n\n- The paper is clearly written.\n\n- Motivation and explanation of each proposed module are reasonable.\n\n[Significance] \n\n- As a study that improves upon existing research, I think there are many elements that can be utilized in follow-up studies."
            },
            "weaknesses": {
                "value": "- Compared to just using cross attention (CLIP embeddings), it seems that it needs to analyze various perspectives such as computational cost or memory.\n\n- Also, the analysis of the effectiveness of Expert Denoioser seems to be somewhat lacking.\n\n- There are experiments that show the quantitative effectiveness of each of the proposed modules, but the qualitative, in-depth analysis is somewhat lacking.\n\n- This paper is somewhat limited by the fact that comparative experiments were conducted with Zero123 only. I think it would be a better paper to compare NVS with a single image, or a diffusion-based model adapted to a given task, even if it is not exactly the same task.\n\t\t \t \t \t\t\n- There is no section 3.2.2, so there is also no need for section 3.2.1. It is better to use just section 3.2."
            },
            "questions": {
                "value": "- In Table 1, there are no experiments for \"inference w/o text\" and no experiments for \"w/ expert denoisers\" for 160M?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698394520939,
        "cdate": 1698394520939,
        "tmdate": 1699636308411,
        "mdate": 1699636308411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T7BVDAYc7g",
        "forum": "9ZUYJpvIys",
        "replyto": "9ZUYJpvIys",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
        ],
        "content": {
            "summary": {
                "value": "Novel view synthesis from a single RGB image is an under-constrained problem, and Zero123 solves this problem by training a view-conditioned latent diffusion model. However, Zero123 treats it as a pure image-to-image translation problem and thus suffers from pixel-level inconsistency problems. In this paper, TOSS adds text as high-level semantic information and, more importantly, proposes the cross-attention mechanism for better 3D consistency. Through exhaustive experiments, TOSS outperforms baseline Zero123 with more plausible and multiview-consistent NVS results, thus leading to substantial improvement in 3D reconstruction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed dense cross-attention mechanism is quite reasonable, and the explanation, especially in Figure 3, is persuasive. In the experiment section, both quantitative and qualitative results of novel view synthesis demonstrate the effectiveness of this module. More importantly, it significantly improves the 3D consistency, reflected in better 3D reconstruction results.\n2. The additional text conditions increase the controllability of the TOSS model. Hence, text prompts lead to diverse NVS results as presented in the paper.\n3. This paper is well-written and presented with a clear architecture. For the contributions summarized in the introduction, the corresponding experiment support can almost be found in the experiments."
            },
            "weaknesses": {
                "value": "1. The authors claim that the text prompts increase the plausibility of novel view synthesis results. However, only the corresponding quantitative results (especially in the ablation study) can be found in the paper. Apart from the illustration in Figure 1, more qualitative results should be presented to support this claim.\n2. From the comparisons in Tables 1&2, TOSS only brings minor improvements over baseline method Zero123. Actually, prior work One-2-3-45 presents an interesting heatmap in Figure 4, showing the PSNR values are significantly affected by the relative azimuth and elevation angles. Hence, the authors should elaborate on how the results in Tables 1&2 were obtained to avoid unfair comparison.\n3. For the 3D reconstruction, the authors should also compare with relevant methods such as Magic123 and Consistent123."
            },
            "questions": {
                "value": "1. During the cross-attention process, have you tried other orders or combinations of text, image, and camera pose?\n2. Expect more detailed elaborations on expert denoisers in the paper, in particular the motivation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3544/Reviewer_wyYi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698514194746,
        "cdate": 1698514194746,
        "tmdate": 1699636308336,
        "mdate": 1699636308336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cE8dqJpLCi",
        "forum": "9ZUYJpvIys",
        "replyto": "9ZUYJpvIys",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3544/Reviewer_gFkU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use text as high-level semantic information to constrain the NVS solution space. With this text, the proposed method can generate the multi-view consistency images. Meanwhile, it proposes dense cross-attention to align the reference and target image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The whole idea is Intuitive and effective.  Using the text as semantic guidance to preserve multi-view consistency is reasonable since the ref image can not provide enough information. Meanwhile, since the proposed model uses the text prompt, it can finetune the SD directly, which preserves the ability of generation.\n2. The attention strategy is reasonable, which will help these two images align better."
            },
            "weaknesses": {
                "value": "1. My main concern is Whether the texts bring enough improvement.  As shown in Table 4, I find \"Token-level attention\"  improves the model the most, while the \"Text prompt\" is not the best.  Meanwhile, the author should show these cases visually: 1. Zero123 + Token-level attention 2. Zero123 + Text prompt.\n2. I'm curious about the effect of this model on the human portrait since the Zero123 performance is bad in human portraits (Dataset limitations). The author can show two cases visually: 1. Taylor Swift (you can select an image of Taylor Swift as input) 2. Young male (you can randomly select a young male image).\n\nIf the authors can address my question properly, I will be glad to improve my grade."
            },
            "questions": {
                "value": "see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698936537028,
        "cdate": 1698936537028,
        "tmdate": 1699636308274,
        "mdate": 1699636308274,
        "license": "CC BY 4.0",
        "version": 2
    }
]