[
    {
        "id": "NtojUyCbLc",
        "forum": "DZxU0q2S11",
        "replyto": "DZxU0q2S11",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_J9Hi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_J9Hi"
        ],
        "content": {
            "summary": {
                "value": "Building on the observation that ReLU networks have piecewise linear/polytope decision boundaries, this paper theoretically analyzes ReLU networks for data which is a (convex) polytope or may be approximated by unions or differences of polytope. Using novel terminology, bounds on the width of ReLU networks for such polytope data are given. It is claimed these bounds are the first such bounds in terms of topological data structure. Numerical experiments are given which are claimed to verify the theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Analyzing learning in terms of data geometry/topology is an interesting problem. The authors make a serious effort, and I appreciate their enthusiasm."
            },
            "weaknesses": {
                "value": "The relationship of novel terminologies used in this paper to other ideas learning theory is unclear. For instance what is the relationship between the margin of a \"feasible architecture\" on a manifold and the generalization error of that architecture on that manifold?\n\nI noted Proposition C.1 where the author claims that \"feasible architecture\" is a sufficient condition for universal approximation. I didn't find the proof convincing. You assume the existence of function $f_\\delta$ which fits the indicator of $\\mathcal{X}$. Then a neural network is defined to be equal to this $f_\\delta$. How do you know there is such a neural network? Universal approximation would guarantee that, but that's what you're trying to prove.\n\nThe novel term \"feasible architecture\" is doing a lot of work in this theory. It seems to hide the question of how well an architecture actually fits a dataset. It is clear that ReLU networks can fit polytopes exactly, but it is not clear how well they can fit arbitrary manifolds.\n\nLikewise for \"polytope-basis cover\". What guarantees are there for finding a tight covers for a given manifold?\n\nMissing some discussion of prior related work on relating topological characteristics to generalization theory, e.g. [0]\n\nPractical applicability of results is unclear. How do you actually find a polytope-basis cover?\n\nMany strong claims are made in this paper, and after reading the paper it's not altogether clear to me they are substantiated.\n\nProposition 3.5 seems trivial. A very wiggly (but non-intersecting) decision boundary is homeomorphic to a linear decision boundary. The former cannot be solved by a linear classifier, but the later can be.\n\n[0] Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks - Birdal et al. (Neurips 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/35a12c43227f217207d4e06ffefe39d3-Abstract.html"
            },
            "questions": {
                "value": "Is topological space really the right level of generality for this paper? Why not just consider manifolds? The former is substantially more general, and it appears you only consider subsets of $\\mathbb{R}^n$.\n\nCan you analytically compute any explicit non-trivial examples of \"feasible architecture\" and \"polytope-basis cover\"?\n\nWhy should we believe polytope-basis covers are good approximations of manifolds? Can you prove it? Can you actually find them in practice for realistic data?\n\nCan you clarify the relationship of your novel terms to other terms in learning theory?\n\nCan you provide references for your proof techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2264/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770802118,
        "cdate": 1698770802118,
        "tmdate": 1699636159394,
        "mdate": 1699636159394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Kg5T3PkkMn",
        "forum": "DZxU0q2S11",
        "replyto": "DZxU0q2S11",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_17Jw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_17Jw"
        ],
        "content": {
            "summary": {
                "value": "The work explores relationship between the width and, in some cases, the depth, of simple fully-connected neural networks with ReLU activations, and the polytope geometry of data distribution support, in the context of classifiers. Authors describe four constructions of  two to four layers neural networks, approximating indicator functions on convex polytopes, difference of unions of polytopes, simplicial complexes, and convex polytopes with prism-shaped polytopes removed.   Some lower bounds on the width of neural networks approximating such indicator functions  are also stated.  Finally, the work verifies that the constructed networks for convex polytopes can be reached via gradient descent optimization if it is initialized from certain regions of weight space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The article describes a  construction of  two layers neural networks, approximating indicator functions on convex polytopes. The article further uses it for three constructions of three to four layers neural networks, approximating indicator functions on difference of unions of polytopes, simplicial complexes, and convex polytopes with prism-shaped polytopes removed. \n\n2) While the polytope geometry was explored in many works in the context of ReLU fully connected neural networks, the work is based on the seemingly novel idea, see Figure 5,  that  a simple two layers ReLU network can have a constant output on a convex polytope, contrary to the more standard approach with constant output on cubical sets.\n\n2) For three out of four constructions the work states lower bounds on width of neural networks that can approximate the indicator functions. \n\n3) The work verifies that the constructed networks for convex polytopes can be reached via gradient descent optimization if it is initialized from certain regions of weight space."
            },
            "weaknesses": {
                "value": "Principal weaknesses of the work are:\n\n1) Lack of applications to practical real-world datasets. It is not clear how to count the numbers of polytopes, or j-dim facets in simplicial complex, necessary to approximate a given real-world dataset, so the practical application  of the results is somewhat unclear. \n\n2) Presentation of the paper suffers from several drawbacks. The paper contributions are theoretical, but the presented in the article proofs are too sketchy. The article is intended for a wider iclr community, but because it is too sketchy in the main part, a smooth reading  even for experts is questionable, cf below. \n\n3) Another drawback of the presentation, in the case of the lower bounds: the principal lower bound argument involves the \"bent hyperplane argument\"  used in several previous works. However the paper does not explain clearly what was done in this context in the previous works compared with what constitutes the paper's novelty when establishing the lower bounds. \n\n4) The numerous allusions to topological aspects are heavily overstated. The only place, where some topological notion appears, namely Betti numbers,  is in the fourth construction (Theorem 3.7) involving the polytopes with  prism-shaped polytopes removed. However in this context the (d-i)-th Betti number is simply the number of the removed prism-shaped convex polytopes with maximum i unbounded axes, so the Betti numbers can be replaced by such simple counters in the Theorem 3.7.  \n\n5) Related work section does not mention vast literature on geometry and topology applications in analysis of data representations, eg  Kim K et al. \"Pllay: Efficient topological layer based on persistent landscapes.\" Advances in Neural Information Processing Systems 33 (2020), Barannikov, S et al. \"Manifold Topology Divergence: a Framework for Comparing Data Manifolds.\" Advances in Neural Information Processing Systems 34 (2021), Barannikov, S et al. \"Representation Topology Divergence: A Method for Comparing Neural Network Representations.\" ICML (2022).\n\n6) The verification of possibility to reach the constructed networks via gradient descent optimization is somewhat limited as it is only applicable to some initializations satisfying some additional conditions and not to others. \n\n\nBelow are some specific remarks:\n- page 1: The figure appearing on the article first page usually serves to highlight the principal contribution of the paper, does the standard Figure 1 with very well-known XOR dataset really serve such purpose? \n\n- page 2: \"considering the given dataset as  a topological space\" -> considering the support of the data distribution as a topological space? \n\n- page 2: \"We answer this question by constructing a collection of convex polytopes...\" - where is it described in the paper how to construct such collection? \n\n- page 2: \"forms m-simplicial complex..., we establish  a novel topology-dependent bound\" - the bound established in Theorem 3.6 concerning simplicial complexes is in terms of numbers of j-dimensional facets, which are not \"topology-dependent\" quantities. \n\n- page 4:  \"from the volume identity of the polytope\"- what is it? a reference is necessary here. \n\n- page 5: in Theorem 3.6 \"d_1 is bounded by\" refers to the presented construction of the network, or to any 2 layer network which is feasible on X, it is not clear\n\n- page 6: \"the first result on the width of neural networks in terms of topological data structure\"- the result in Theorem 3.6 is in terms of numbers of j-dimensional facets, which is not \"in terms of topological data structure\"\n\n- page 7: \"This implies that the architecture outlined in Theorem 3.7 is dictated by the filtration parameter\"- The topological space considered in Theorem 3.7 is a convex polytope with disjoint prism-shaped convex polytopes removed, it is not the Cech complex with respect to some parameter epsilon, so there is no architecture in Theorem 3.7 related to the filtration parameter. \n- page 8: \"which completely classifying the given dataset \" - perhaps, which classifies with zero error the given dataset ?"
            },
            "questions": {
                "value": "Please address the limitations related with the feasibility of approximating  real-world datasets with the difference of polytopes or the simplicial complex constructions, how to construct and count such polytopes or the numbers of j-dimensional facets for all j."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Reviewer_17Jw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2264/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699156713878,
        "cdate": 1699156713878,
        "tmdate": 1700698192593,
        "mdate": 1700698192593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UJvOfEXMfK",
        "forum": "DZxU0q2S11",
        "replyto": "DZxU0q2S11",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_nRVF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_nRVF"
        ],
        "content": {
            "summary": {
                "value": "The paper suggests feasible shallow ReLU-induced neural network architectures that approximate indicator functions on a space having a polytope basis cover. It proposes lower and upper bounds to network widths in the process, based on Betti numbers in case the underlying space has prism-shaped convex holes. The proposed networks can also be realized based on gradient descent, by minimizing some of the commonly used loss functions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The organization and writing are of sound quality. The theoretical framework is technically solid and the results clearly address the problem being dealt with. The supporting experiments provide empirical evidence for the findings."
            },
            "weaknesses": {
                "value": "There remain a few typographical/grammatical errors in the manuscript."
            },
            "questions": {
                "value": "1. It is often difficult for real data sets to satisfy Assumption 4.2 since the polytopes separating the clusters may not be convex. Are there any definitive modifications to the proposed convergence guarantees [Theorem 4.3] that the authors can suggest? How does the increase in the number of classes, and hence perhaps overlapping polytopes add to the complexity of the construction [Page 21]?\n\n2. The optimality of $3$-layer ReLU networks for approximating indicators is clear from Proposition C.1 and C.2. Is it true for high-dimensional compactly supported functions ($f: R^d \\to R^l$), $l>1$ in general, perhaps of some regularity in terms of smoothness? This seems crucial as there are numerous UA bounds for Lipschitz maps using ReLU feed-forward networks. \n\n3. Can the authors comment on how well the prescriptions regarding architectures hold up against simpler real datasets? As a practitioner, it is often frustrating to witness theoretical suggestions underperforming significantly. For example, to my knowledge, there exists no consistent method of estimating Betti numbers corresponding to even simpler real data distributions, if they at all have punctured supports."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2264/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699561825616,
        "cdate": 1699561825616,
        "tmdate": 1699636159244,
        "mdate": 1699636159244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FoTjdCaPlr",
        "forum": "DZxU0q2S11",
        "replyto": "DZxU0q2S11",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_eavJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2264/Reviewer_eavJ"
        ],
        "content": {
            "summary": {
                "value": "The authors study the ability of neural networks to approximate the indicator\nfunction for $\\epsilon$-blowups of convex polytopes (or more generally, the\nblowup of a difference of unions of convex polytopes) as a function of the width\nand depth of the neural network, versus the complexity of the polytope. The\nauthors present a general result that says (for example) that a two-layer\nnetwork with a number of neurons in the hidden layer growing with the number of\nhyperplanes that define the convex polytope $\\mathcal{X}$ is sufficient to\nrepresent exactly the indicator function for $\\mathcal{X}$ (with a corresponding\nlower bound). Further results are given that quantify the width in terms of\nBetti numbers or $k$-facets when $\\mathcal{X}$ is a simplical complex, as well\nas providing a local (initialization-dependent) theory for obtaining such\nnetworks via global minimization of an empirical risk over the data\ndistribution. Low-dimensional experimental results are presented that verify\nthat gradient descent finds networks matching the architectural parameters\nasserted as sufficient by the theory for two toy data distributions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The mathematical writing in the paper is clear and precise. The authors define\n  relevant concepts, precisely state hypotheses, include relevant ancillary\n  results in appendices with appropriate references, and present a rather robust\n  characterization of the problem (in terms of sufficient architectures for\n  representing indicators for convex polytopes (with holes), and results that\n  specify the widths in terms of complexity parameters of these polytopes).\n\n- The experimental results consider toy (low-dimensional) cases, but present a\n  compelling verification of the conclusions of the authors' theoretical\n  results."
            },
            "weaknesses": {
                "value": "- The metrics in the paper used to quantify geometric structure in the input\n  data (which the theoretical results reflect in terms of the rates for the\n  network widths to achieve the feasible architecture property) seem that they\n  may be hard to compute in moderate dimensions (presumably, one needs to fit a\n  simplical complex to data, and calculate Betti numbers from it). This means it\n  may be hard to verify the theory in cases beyond the low-dimensional examples\n  highlighted in experiments. I hope the authors will correct me if I am\n  mistaken here, and clarify this in the revision.\n\n- The non-technical writing in the paper (in contrast to the mathematical\n  writing, highlighted above) suffers from a lack of precision in many areas. I\n  would recommend rewriting the abstract to be more in line with the tone of the\n  rest of the paper, tuning the first sentence of the introduction (I do not\n  think this claim is universally accepted -- arguably, the ability to\n  (efficiently) learn these networks is of far greater importance for\n  understanding the successes of deep learning), and generally proofreading for\n  typos.\n\n- There are two relevant references that I think should be discussed in this\n  context -- both are relevant to guarantees for *learning deep networks* when\n  the data distribution has nontrivial geometric structure, going beyond the\n  present theory on initialization-dependent or pure-representation-capacity\n  results. The first is [1], which proves classification guarantees for random\n  three-layer neural networks with rates that depend on the geometric structure\n  of the input (measured through the Gaussian width). I think it could inform\n  the presentation in the present submission to contrast with this work, as the\n  way this work proves its results is by studying the way the random\n  initialization induces a hyperplane arrangement conducive to separation\n  (perhaps similar to the authors' analysis, for representation capacity).\n  The second is [2-3], which studies sufficient settings of width and depth to\n  classify pairs of one-dimensional curves (in terms of geometric properties of\n  the data) with a deep ReLU network trained with gradient descent. This work\n  uses very different tools from the present submission, but its motivation is\n  relevant, and contrasting with this work may allow the authors to highlight\n  salient advantages of their tools/framework.\n\n\n[1] Dirksen, S., Genzel, M., Jacques, L., & Stollenwerk, A. (2022). The\nSeparation Capacity of Random Neural Networks. Journal of Machine Learning\nResearch: JMLR, 23(309), 1\u201347.\n\n[2] Buchanan, S., Gilboa, D., & Wright, J. (2021). Deep Networks and the\nMultiple Manifold Problem. International Conference on Learning Representations.\nhttps://openreview.net/forum?id=O-6Pm_d_Q-\n\n[3] Wang, T., Buchanan, S., Gilboa, D., & Wright, J. (2021). Deep Networks\nProvably Classify Data on Curves. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.\nS. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing\nSystems (Vol. 34, pp. 28940\u201328953). Curran Associates, Inc."
            },
            "questions": {
                "value": "- Can the authors clarify the reason for the focus on representing indicators\n  for the relevant polytopes $\\mathcal{X}$ (in the kind of $L^\\infty$ sense\n  mandated by Definition 3.1), and the limitations of this framework for general\n  problems of interest? It seems to me that it might be too \"hard\" of a problem\n  to characterize sufficient architectural configurations to fit data\n  distributions if one's end goal is a machine learning task such as\n  classification (for example, generally, one could classify $\\mathcal{X}$\n  without exactly representing $\\mathbb{1}(\\mathcal{X})$). It also seems to me\n  that representing $\\mathbb{1}(\\mathcal{X})$ may not be sufficient to solve\n  general nonparametric regression tasks, i.e. to enjoy universal approximation\n  of various nonparametric classes defined on $\\mathcal{X}$ (please correct me\n  if I am mistaken). A significant amount of work has been done in the latter\n  setting, specifically on manifolds, which does not seem to have been\n  discussed (e.g., [4] and many works by the same authors).\n\n- In Section 3.1, it is not exactly explicitly defined what an \"architecture\" is\n  (relevant to understanding $\\mathcal{A}$ in Definition 3.1), but it seems from\n  context that it is a fixed choice of inter-layer maps and in particular of\n  hidden layer dimensions. A limitation of this definition (c.f. footnote 1)\n  seems to be that in general, universal approximation of various nonparmetric\n  classes can only be enjoyed with neural networks when the hidden layer\n  dimension is allowed to grow -- in other words, the \"architecture\" involves\n  only (in a sense) the computational graph of the neural network, rather than\n  particulars (such as input and output dimensions) about the maps corresponding\n  to \"edges\" in the graph. Could the authors clarify this difference, and why\n  they have opted to define an \"architecture\" in this way?\n\n[4] Chen, M., Jiang, H., Liao, W., & Zhao, T. (2022). Nonparametric regression on\nlow-dimensional manifolds using deep ReLU networks: function approximation and\nstatistical recovery. Information and Inference: A Journal of the IMA, iaac001."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2264/Reviewer_eavJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2264/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699563384547,
        "cdate": 1699563384547,
        "tmdate": 1700681808620,
        "mdate": 1700681808620,
        "license": "CC BY 4.0",
        "version": 2
    }
]