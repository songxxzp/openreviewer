[
    {
        "id": "Xz27KPogcN",
        "forum": "1CPta0bfN2",
        "replyto": "1CPta0bfN2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes AXN, a test-time multi-run query embedding adaptation approach that leverages KNN search to approximate cross-encoder scores. This method successfully reduces the expensive computational costs associated with cross-encoder calculations, surpassing the performance of DE-based and CUR-based alternatives."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a novel adaptive retrieval technique that utilizes a limited number of cross-encoder (CE) calls to approximate the quality of cross-encoder results. This approach is scalable to handle a large volume of items."
            },
            "weaknesses": {
                "value": "1)\tThis paper uses K nearest neighbor search to iteratively update query embedding and approximate cross-encoder results. However, many references of nearest neighbor search are missing.\n2)\tAXN utilizes sparse matrix to reduce index costs. The paper lacks detailed analysis regarding this technique's impact on results concerning varying degrees of sparsity.\n3)\tBoth AXN and CUR-based methods need to compute low-dimensional embeddings for queries and items. AXN uses sparse matrix to reduce the cost. This can also be applied to CUR-based methods to reduce the index time. It is unclear if other techniques of AXN generate substantial improvements over CUR-based methods. It would be great if the authors can add more ablation study experiments.\n4)\tThe paper only covers the total index time as a benchmark, and future exploration could include query latency measurements since various steps are executed many times, including Solve-Linear-Regression and topk search.\n5)\tIt introduces lambda to ensemble the generated query embedding with a query embedding from DE or inductive matrix factorization. However, it fails to conduct an analysis of lambda impact on evaluation. It is the same for all experiments, or should be tuned in each experiment?\n6)\tIt runs R times Solve-Linear-Regression and topk search. How to choose R? It is fixed in all experiments or should be tuned in each experiment? Should it be large in large dataset?\n7)\tThe same problem for hyper-parameter Ks."
            },
            "questions": {
                "value": "1)\t\u201cCUR\u201d appears without any definition.\n2)\tWhat is the topk search method? Is it brute-force search?\n3)\tPlease address the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_hzee",
                    "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749802151,
        "cdate": 1698749802151,
        "tmdate": 1700709030863,
        "mdate": 1700709030863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KXvVpCSOZj",
        "forum": "1CPta0bfN2",
        "replyto": "1CPta0bfN2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
        ],
        "content": {
            "summary": {
                "value": "Cross-encoder (CE) models outperform Dual-encoder (DE) models (especially at zero-shot problems) in the ranking task but are very expensive to use during inference. To alleviate this usually a retrieve then re-rank approach is used where a set of items are first retrieved using a DE model and then further ranked by CE. This paper proposes an alternate approach where the CE model is first distilled into a lightweight factorized model and at test time query representation is iteratively fine-tuned such that the dot product between test query embedding and indexed item embeddings gets closer and closer to the CE assigned relevance. This approach helps in reducing the CE calls required to accurately rank items for the test query."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach is simple to plug into existing retrieval and ranking frameworks\n- The paper is in general well-written and easy to follow\n- The proposed approach is compared against relevant baselines and the evaluation is thorough"
            },
            "weaknesses": {
                "value": "- The proposed approach is evaluated only on zero-shot tasks, does this approach also benefits standard retrieval tasks\n- Gains are primarily under fixed index time scenario which is usually a one-time cost"
            },
            "questions": {
                "value": "- It is a bit surprising that RnR DE models are performing worse than TF-IDF on Hotpot, is it because this CE model was trained on triplets mined using TF-IDF?\n- CE model is trained conditioned on some specific negativing mining distribution so maybe for RnR baselines we should also compare with a retrieval model which is the same as the retrieval model used for the negative mining so that the train and test-time behaviours are same\n- For a given budget $X$ of CE calls, how should one distribute $X$ calls in the number of rounds and $K$ CE calls inside each round in AXN inference"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_ujET"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801671279,
        "cdate": 1698801671279,
        "tmdate": 1699636376647,
        "mdate": 1699636376647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PZAGEhJkhX",
        "forum": "1CPta0bfN2",
        "replyto": "1CPta0bfN2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a sparse-matrix factorization-based approach to improve the efficiency of fitting an embedding space to approximate the cross-encoder for k-NN search. Unlike DE-based and CUR-based methods, which lack good generalizations and computation efficiency, the new AXN method constructs a sparse matrix containing a cross-encoder score of training queries and all items.  The item embeddings are learned from matrix factorization. During test time, AXN alternates between updating the query embedding and retrieving more items for k-NN indexing."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors proposed AXN, a novel cross-encoder-based k-NN search algorithm. By learning item embeddings from sparse matrix factorization and fixing them during query time, the algorithm is more computationally efficient than other methods.\n2. The authors explained their method very clearly in section 2.\n3. The extensive experiments and ablation studies supported their claims."
            },
            "weaknesses": {
                "value": "1. Figure 1's legends and corresponding subplots are hard to read. The subplots are too small, and hard to map points to the legends."
            },
            "questions": {
                "value": "1. In the experiment section it was not clear how many rounds of updates are performed in all AXN experiments. \n2. Consider fixing max of CE calls B_{CE}, but varying the number of iterative search rounds and the number of items to retrieve in each round. Will it affect AXN's performance and total indexing time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_ja98"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816437519,
        "cdate": 1698816437519,
        "tmdate": 1699636376558,
        "mdate": 1699636376558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6NDRBrMXrt",
        "forum": "1CPta0bfN2",
        "replyto": "1CPta0bfN2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a new approach to retrieval and indexing that attempts to match the accuracy of cross encoders with lesser training time. Cross-encoders based retrieval allows for computing a relevance function over query and each point in the retrieval corpus and finding the point(s) most relevant. As this are expensive for inference, in practice, dual encoders with separate encoding stacks for query and corpus are used with k-NN used to quickly find the most relevant documents. A compromise on cross-encoder is a CUR decomposition of the relevance signal. This paper proposed a method (AXN) that improves upon recall of dual encoder methods and is much faster than CUR based approaches.\n\nAXN starts with a dual encoder (treated as black box) and iteratively mines for items near a query using the query's representation. Then it uses a cross encoder to score the limited set of items retrieved. It uses this to refine the query representation, search for another limited set of items and so on. The method limits the cross attention to far fewer items than the corpus size. Experiments show that AXN improves the recall of dual encoder and can match the recall of CUR methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. AXN yields better recall than dual encoders with faster training times compared to CUR methods\n2. They can build on any dual encoder methods (although experiments don't say if they improve on DE encoders)"
            },
            "weaknesses": {
                "value": "1. The inference time and its comparison to simple DE+kNN approaches is not made clear. Would a large dual encoder with same inference time as AXN match its recall? Some of this is tucked away in the appendix in Fig 5 and 6 which seems to suggest the margins between DE and AXN are low normalized for inference cost.\n2. The choice of DE is not discussed. Is it completely irrelevant? Is it being compared to state-of-the-art encoders?"
            },
            "questions": {
                "value": "1. would a large dual encoder with same inference time as AXN match it's recall? Or would DE saturate well before AXN.\n2. What is the unit on x-axis in Figure 6?\n3. Could you talk about the scaling properties of your algorithm as the corpus grows larger? \n4. Why are not all methods not represented on each sub-figure in Figure 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4117/Reviewer_fa41"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832442422,
        "cdate": 1698832442422,
        "tmdate": 1699636376499,
        "mdate": 1699636376499,
        "license": "CC BY 4.0",
        "version": 2
    }
]