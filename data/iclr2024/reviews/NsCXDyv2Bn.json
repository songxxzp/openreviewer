[
    {
        "id": "b4nCkuImiu",
        "forum": "NsCXDyv2Bn",
        "replyto": "NsCXDyv2Bn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a small architecture improvement upon PromptTTS and also proposes to use SLU and LLM to generate text prompts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. There is some novelty in using SLU and LLM to generate text prompts for voice, as I believe this is the first work investigating this.\n2. A good mount of ablation and analysis.\n3. The paper is in general written clearly and easy to follow."
            },
            "weaknesses": {
                "value": "1. The authors claim that the proposed variational method solves the one-to-many problem, but I doubt if this is the case. The proposed module basically learns to predict a melspec encoder's output from text prompts, in addition to conditoning the TTS backbone. I don't see how this solves the one-to-many problem as in the end we are still predicting from text (one) to different variability (many) in the voice. The only difference is that there is an additional auxiliary loss from the melspec encoder. This seems to be in principle similar to InstructTTS's approach to regularize melspec encoder and text encoder output to close in the embedding space.\n2. The proposed prompt generation method, while the first of its kind, seems to be still quite limited by the very limited SLU output set, and the template used for LLM generation. \n3. The proposed method performs only marginally better than PromptTTS and InstructTTS."
            },
            "questions": {
                "value": "Will the author open source the generated dataset or source code? I would consider the open sourcing effort as a contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_dNEb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698498961866,
        "cdate": 1698498961866,
        "tmdate": 1700689926716,
        "mdate": 1700689926716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bu1yZKDcrz",
        "forum": "NsCXDyv2Bn",
        "replyto": "NsCXDyv2Bn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission907/Reviewer_2wDM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission907/Reviewer_2wDM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed VoiceGen, a text-to-speech framework that uses a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts.\n\nThe variation network predicts the representation extracted from the reference speech based on the text prompt. And the LLM formulates text prompts based on the recognition results from speech language understanding model. Compared to the previous works, VoiceGen generates voices more consistent with text prompts, offering users more choices on voice generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed modeling and data labeling pipelines for text-prompt based TTS systems can generate higher-quality speech with more consistent and noticeable control compared to previous systems. The variation network predicts speech representations that are more closely corresponding to the text prompt and more diversity by sampling from Gaussian noise. On the other hand, the LLM-based prompt generation pipeline can produce high-quality text prompts at scale and can easily incorporate new attributes. Overall, the proposed system provides a framework that is beneficial for future text-prompt based TTS research."
            },
            "weaknesses": {
                "value": "After listening to the generated voices on the demo page, audio quality is still an issue and further improvements are required, especially for certain text prompts such as \"Please speak at a fast speed, gentleman\". The reason could be missing or few audio samples for corresponding prompts in training datasets."
            },
            "questions": {
                "value": "1. What's the necessity of concatenating both text prompt representation and speech prompt representation for TTS backbone? Will the speech prompt representation itself be enough to guide the TTS backbone through cross attention?\n\n2. What's the impact of making use of a fixed-length representation for text and speech prompt representations? Fixed-length representations may be fine for global attributes such as age and gender, but is that enough for fine-grained or local control in guided speech generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705441951,
        "cdate": 1698705441951,
        "tmdate": 1699636017528,
        "mdate": 1699636017528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bKVHKsjuN0",
        "forum": "NsCXDyv2Bn",
        "replyto": "NsCXDyv2Bn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission907/Reviewer_Ghqv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission907/Reviewer_Ghqv"
        ],
        "content": {
            "summary": {
                "value": "The paper is about text-to-speech (TTS). The TTS model is based on Naturalspeech 2. The TTS model is extended by a style module, which uses a text prompt to describe the style. During training, it additionally also uses the reference speech as input. However, for speech generation, a variation network instead generates the reference speech encoder outputs. This variation network is the core novelty proposed by the authors. It is supposed to add further variation in the speech style which cannot be covered by the text prompt alone. The variation network is a diffusion model using a Transformer encoder to iterate on the speech encoder outputs.\n\nThis proposed model is called VoiceGen.\n\nThe training data is based on the Multilingual LibriSpeech (MLS) dataset with 44K hours of transcribed speech. To generate the text prompt, needed to train the style model, a text prompt dataset generation pipeline is proposed, to extend the given transcribed speech: Based on a speech language understanding (SLU) model, the gender of the speech is identified. Additionally, using digital signal processing tools, pitch, volume, and speed is extracted and put into classes. Those attribute classes are then fed into a large language model (LLM) to generate a text prompt which conveys the style attributes.\n\nThe variability is measured using a WavLM-TDNN model to assess the similarity of two speeches, and it is shown that the introduction of the variation network leads to higher speech variability. At the same time, mean-opinion-score (MOS) on the quality of the proposed VoiceGen model is slightly better than other text-prompt-based TTS models, PromptTTS and InstructTTS specifically. It is also shown that the text prompt indeed works and can generate speech with the requested attributes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The claims are tested and it seems the proposed model adds quite a bit of variability, as it was intended."
            },
            "weaknesses": {
                "value": "So many details are left out, as this would not really be possible to fit into the paper. So without the exact code + recipe to produce all the results, it will be almost impossible to reproduce the results. I think having code + recipe available here is very important.\n\nAll the experiments basically just show that the proposed model works well and solves the outlined problem. However, there is almost no analysis or ablation studies, etc. E.g. how important is it to use a diffusion model here? What about other model types? What about other smaller model details, and hyper parameters, etc?\n\nThere are a few things a bit unclear (see below)."
            },
            "questions": {
                "value": "> Compared to traditional text-to- speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all.\n\nI don't exactly understand the difference between speech prompts and text prompts.\n\nA text prompt is really like a description of what should be generated, like \u201cPlease generate a voice of a boy shouting out\u201d.\n\nA speech prompt is the same but as speech? Or is this the text of the generated speech? Or sth else?\n\n\n> The input of variation network comprises the prompt representation (P1, ..., PM ), noised reference representation (R1t , ..., PMt ), and diffusion step t\n\nHow exactly? P and R are concatenated sequences? But the diffusion process only runs on R?\n\n\n\n\n\n\nClarification on text attributes for text prompt generalization: There is only gender (via the existing SLU model), pitch, volume, and speed, nothing else? I would expect some more attributes, e.g. different emotion categories, etc.\n\n\nSection 5.1, table 1: I don't exactly understand what is measured under what conditions on what data. So, each TTS model (VoiceGen vs the others) generates some speech, then some attributes are given to produce some prompt, and then, for the given fixed SLU models and digital signal processing tools, the accuracy is measured? That uses the generated prompts via LLM as mentioned before? All of them, so 20K text prompts? How many classes are there for each of the attributes?\n\n\nText prompt dataset: So this is released? Where? I did not find it.\n\n> WavLM-TDNN model (Chen et al., 2022a) to assess the similarity of two speech\n\nIs this a pretrained model which you just take? Or where is the model from?\n\nI don't exactly understand Section 5.1, table 3. This is always with the same phoneme sequence as input to the TTS model? But what is the text context here? Why are the results so different between PromptTTS, InstructTTS and VoiceGen? Are they really comparable? Do they have the same TTS backbone? Does the TTS backbone use diffusion in all cases?\n\nWhen you add variability through the variation network, how well are the properties of the text prompt for the speech style actually preserved? I guess this is table 1? But how much variance do you get when you add sampling results of the variation network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707330195,
        "cdate": 1698707330195,
        "tmdate": 1699636017435,
        "mdate": 1699636017435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CYUl56DwHw",
        "forum": "NsCXDyv2Bn",
        "replyto": "NsCXDyv2Bn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of text-based voice creation for text-to-speech synthesis (TTS). \nPrior work on zero-shot TTS often relies on using reference voice samples of the target speaker (YourTTS) or target audio style (including both speaker and prosody, such as VALL-E) to prompt the model to generate the desired voice. \nHowever, the authors argue that such prompts may not always be available, and this paradigm is less user friendly. To address it, authors present a model to enable creation of voices through providing descriptions like \u201ca man with a normal voice\u201d, similar to the setup in InstructTTS and PromptTTS.\nThe contribution of the proposed method is two-fold. \nFirst, the authors tackle the one-to-many problem between text description and voice, where the same description, such as \u201ca low pitched female voice\u201d, can be mapped to many different voices. The authors adopt a variation network to sample the reference speech style embeddings given a text description prompt. \nSecond, the authors presented a pipeline to automatically create text prompts to address the data scarcity issue for descriptive texts for speech. The authors consider controlling four aspects of speech: gender, speed, volume, and pitch.\nIn addition, the authors present a face2voice application replacing text description with facial image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper studies an interesting problem which enables creation of voices through text descriptions. This line of research has great potential of making speech generation more customizable.\n\n2. The authors present a systematic pipeline to produce text describing four aspects of speech, addressing the data scarcity problem. Ablation studying Table 7 shows the benefit of the step-by-step generation process.\n\n3. The variation model tackles the one-to-many problem. The author verified that when changing variation networks introduce speaker variation in Tabel 3."
            },
            "weaknesses": {
                "value": "1. I am not certain if the proposed model and the baseline models are trained on the same data, and hence I cannot draw conclusions that whether the proposed model outperforms the baselines because of the additional LLM generated data or because of the introduction of the variational network to address the one-to-many problem. It would be good to show how well the baseline performs with and without LLM-augmented text prmopts\n\n2. Given that the number of attribute combinations is rather small (2 x 3 x 3 x 3 = 54), I am suspicious about how useful it is to increase the number of text prompts. The author could have conducted ablation studies comparing using only the PromptTTS prompts vs those + x LLM-augmented prompts\n\n3. The authors did not give sufficient background on InstructTTS. That model also deploys a diffusion model and in principle would be capable of modeling one-to-many mapping. Why does the VoiceGen model perform better than InstructTTS?\n\n4. Given that the conditional attributes are all categorical, it can be conditioned with a lookup table for each attribute straightforwardly. How does that method compare with the proposed method? The paper does not really showcase the strength of free-form text description for conditioning attributes that are hard to categorize.\n\n5. For step 1 when generating phrases, the authors show in Table 5 that male can be mapped to man/boy/guy/dude/gentlement. However, one would expect quite different voices between boy and man. This shows the weakness of such pipeline where text descriptions are created from underspecified labels."
            },
            "questions": {
                "value": "See the questions in the Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission907/Reviewer_g23M"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission907/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782711683,
        "cdate": 1698782711683,
        "tmdate": 1700725847011,
        "mdate": 1700725847011,
        "license": "CC BY 4.0",
        "version": 2
    }
]