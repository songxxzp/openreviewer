[
    {
        "id": "v4TVxFoFvK",
        "forum": "NDfxOMJqgL",
        "replyto": "NDfxOMJqgL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_b9QV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_b9QV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a simple but effective self-training method for tabular data, which takes the cluster assumption to regularize confidence values. Experiments on four datasets demonstrate the superiority of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is well motivated by the observations shown in Fig. 1, namely, pseudo-labels that lie in high-density regions are more reliable than those that lie in low-density regions.\n2. The paper is well-written and organized in general. The simple modifications on the confidence value proved to be effective through experiments on four tabular datasets."
            },
            "weaknesses": {
                "value": "1. The density estimation plays an important role in the proposed method. However, the authors only spend a few words saying that the density is estimated using the prior knowledge derived from the labeled training data distribution. I am confused when reading this part of the method and hope the authors can provide more details on that.\n2. Just as the authors have claimed, the only difference between CAST and the conventional self-training algorithm is the use of regularized confidence. In other words, it seems that the proposed method has no specific designs for tabular data. Thus, I wonder if it is possible to supply a bit more results on other forms of data to show the proposed method is a general solution in self-training.\n3. Are the best choices of the hyper-parameter $\\alpha$ the same across different datasets? Would the optimal value be influenced by the number of samples in the dataset?\n4. There are some related self-training enhanced clustering methods such as SCAN (ECCV 2020), SPICE (TIP 2022), and TCL (IJCV 2022), that the authors are encouraged to include in the related works.\n5. The meaning of the abbreviation could be provided in the caption of Table 1 to improve readability."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697875069914,
        "cdate": 1697875069914,
        "tmdate": 1699636164490,
        "mdate": 1699636164490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cseLqwlpeB",
        "forum": "NDfxOMJqgL",
        "replyto": "NDfxOMJqgL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_QLpL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_QLpL"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new calibration strategy for self-training on tabular data. The strategy is based on an algorithm outputting a confidence score for each input sample that linearly interpolates between the confidence score provided by the classifier and its scaled version. Specifically, the scaling factor incorporated the low-density assumption, thus being proportional to the data density. It is estimated either using a kernel density estimator or a Naive Bayes-like generative model. The overall calibration strategy can be easily plugged into existing self-training algorithms. Experiments are conducted on different toy and tabular datasets showcasing (i) the versatility of the approach for being easily applied to different self-training variants (fixed/adaptive threshold, noise filtering) and different classifiers (decision trees, MLPs) and (ii) the superiority against basic calibration strategies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is simple, yet novel (**Novelty**)\n2. The paper is clear and easy to read (**Clarity**)\n3. Code is provided. However, no additional check on replicability has been performed (**Reproducibility**)"
            },
            "weaknesses": {
                "value": "1. The scope of applicability of the proposed solution is quite narrow. Indeed, the proposed solution seems to be applicable to low dimensional datasets and it is not clear how well the solution scales and generalises to more realistic high-dimensional datasets (**Significance**)\n2. The proposed solution requires a density estimation step and therefore it is more computationally demanding with respect to the considered baselines. Experiments should provide also this information (**Quality**)\n3. The cluster assumption (or equivalently the low-density separation) can be cheaply incorporated by leveraging techniques based on entropy minimisation for semi-supervised learning. A discussion and possibly experimental comparison against such techniques is missing (**Quality**). For instance, see [1-3]\n4. Limitations are not discussed (**Quality**)\n\n**References**\n\n[1] Semi-supervised Learning by Entropy Minimisation. NeurIPS 2004\n\n[2] Towards making unlabeled data never hurt. PAMI 2015\n\n[3] MixMatch: A Holistic Approach to Semi-Supervised Learning. NeurIPS 2019"
            },
            "questions": {
                "value": "Please find below some questions related to the above mentioned weaknesses plus some more detailed ones about the experiments:\n1. Can you please elaborate on the 4 above-mentioned weaknesses?\n2. Regarding experiments on toy datasets, is there any reason why temperature scaling is not shown?\n3. In almost all experiments there is a significant difference between the two proposed ways of estimating the density (CAST-D and CAST-L). Can you please discuss about this aspect? Is this issue related to an improper hyperparameter tuning?\n4. In Figure 5, can you please explain why the performance decrease with a larger amount of labeled examples, as this seems a counterintuitive result?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Reviewer_QLpL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698419235745,
        "cdate": 1698419235745,
        "tmdate": 1700386967922,
        "mdate": 1700386967922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6JWNLSb3XK",
        "forum": "NDfxOMJqgL",
        "replyto": "NDfxOMJqgL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_QRTY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_QRTY"
        ],
        "content": {
            "summary": {
                "value": "In the paper \"CAST: Cluster-Aware Self-Training for Tabular Data\" the authors propose an approach to self-training for tabular data. The basic idea of the approach is to take into account how densely populated the dataset is around candidate data points for generating pseudo labels. More specifically, the density is used for regularizing class confidences discounting confidences in less densely populated realms of the space."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is relatively simple and thus relatively easy to implement.\n- The related work is nicely surveyed."
            },
            "weaknesses": {
                "value": "- The paper is not self-contained, i.e., there are gaps in the proposed methodology. For example, in Eq. 2 the authors state that prior knowledge is encoded in terms of a vector $\\gamma$ and $\\gamma$ is assigned the output of some function TD(). However, the function is not clearly described. Only \"prior knowledge which is derived from the labeled training data distribution TD\"  is mentioned in the text. Probably TD is on purpose quite vague. However, there is not even a single mention of what this could be. It is also confusing that it seems to be the labeled training data distribution, however, this distribution should at maximum be implicitly given by the data sample.\n- In Algorithm 1 the $\\gamma$ does not even occur. I assume it is somewhere hidden in the $\\Phi$ which supposedly does the pseudo-labeling. However, $\\Phi$ is nowhere given concretely. Not even in the appendix -- at least I could not find it there. Still, in the text, the authors write that Algorithm 1 is the complete algorithm but only a very basic self-training framework is given there -- nothing special about CAST as a standalone method. Also, the loop is terminating with respect to some unknown termination condition of $\\Phi$ which is neither elaborated.\n- While the experimental evaluation section covers most of the section, taking different perspectives and viewing angles, the breadth of the study is quite limited. In the main paper, the study comprises 4 real-world datasets with an additional 16 datasets in the appendix for a limited set of methods. Considering that there is no theoretical support for the claims, the underpinning of the claims made in the paper is quite weak. \n- Speaking about the empirical evaluation: While relative improvements over a baseline might be the primary goal, with which I agree, it is relatively hard to interpret the significance of the results. In particular, it impedes the application of a statistical test whether the results are significant. From the results, the differences are probably significant but still it is very hard to interpret and I would prefer plain results even though metrics might differ."
            },
            "questions": {
                "value": "- How is $\\gamma$ computed? What are the requirements or desiderata for computing $\\gamma$ to yield a sound approach?\n- What is the termination criterion in relation to $\\Phi$?\n- How is the pseudo-labeling $\\Phi$ done?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678421828,
        "cdate": 1698678421828,
        "tmdate": 1699636164344,
        "mdate": 1699636164344,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NgKtRdiP4a",
        "forum": "NDfxOMJqgL",
        "replyto": "NDfxOMJqgL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_DTs7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_DTs7"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to use self-training to handle the tabular data learning without altering the self-training algorithm or model architecture."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper delves into the confidences of pseudo-labels in self-training from the perspective of cluster assumption, providing a new view for the field of self-training. In addition, the proposed CAST is easy to follow."
            },
            "weaknesses": {
                "value": "- From the Introduction, I cannot get the significant relationship between tabular data and the proposed self-training method. The motivation and organization of this paper should be further clarified.\n- In addition, the difficulties brought by tabular data over the general unstructured data (e.g., images, texts) in machine learning have not been discussed. In detail, they only stated that the GBDT is suitable for tabular data, while they do not explain why other methods are not suitable.\n- In my view, CAST is not a tabular data-specific method. Its idea is to adjust the prediction confidence based on the cluster assumption, which is also available for other data types. I think that this discussion should be included and the corresponding experiments are needed."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2320/Reviewer_DTs7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742755789,
        "cdate": 1698742755789,
        "tmdate": 1700726953767,
        "mdate": 1700726953767,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5GhZPR6wQ7",
        "forum": "NDfxOMJqgL",
        "replyto": "NDfxOMJqgL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_USSG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2320/Reviewer_USSG"
        ],
        "content": {
            "summary": {
                "value": "This pager proposes a simple way to generate reliable pseudo-labels by assigning high confidence to pseudo-labels in high-density regions and low confidence to those in low-density regions. The proposed method could be plugged into current self-training algorithms and tabular models, and extensive experiments validate the effectiveness of this method. However, there lacks detailed analysis (empirical or theoretical) or insights on why it works, and the reliable pseudo-labels are not adequately verified in real scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work proposes a simple but effective way to generate reliable pseudo-labels, which multiples the original pseudo labels with a density score. The method is simple and could be incorporated with various existing algorithms, and extensive experiments validate the effectiveness of it."
            },
            "weaknesses": {
                "value": "After reading this paper carefully, I have some concerns:\n1. There lacks detailed analysis on why this method works. And I wonder whether it has some relationship with label smoothing techniques. And I would recommend the authors to give more in-depth analysis, either empirical or theoretical. Also, the 'cluster assumption' or the reliable pseudo-labels should be checked or verified in real datasets. For example, why they are reliable and can we explain it?\n2. I believe there are many many semi-supervised learning methods, but there are only 5 baselines, which I think is not enough and representative for SSL. \n3. Some of the datasets are not open-sources, e.g., 6M mortality."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870095819,
        "cdate": 1698870095819,
        "tmdate": 1699636164179,
        "mdate": 1699636164179,
        "license": "CC BY 4.0",
        "version": 2
    }
]