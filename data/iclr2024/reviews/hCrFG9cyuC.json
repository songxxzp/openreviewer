[
    {
        "id": "CDUV7HNDbi",
        "forum": "hCrFG9cyuC",
        "replyto": "hCrFG9cyuC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a language model approach to speech-to-speech translation. The approach uses three models: one to map source semantic units to target semantic units, one to predict the duration of target semantic units, and one to predict target acoustic units given source acoustic units and target semantic units with duration information. The semantic units are derived using HuBERT (Hsu et al., 2021) while the acoustic units are derived using Soundstream (Zeghidour et al., 2021). Unlike VALL-E X (Zhang et al., 2023b), which uses phonetic units, this paper uses semantic units, enabling the approach to be extended to unwritten languages. The key novelty of this work is its use of semantic units instead of phonetic units, and its use of a decoder-only architecture that enables prompting. On the EMIME Chinese-English and CVSS English-Spanish speech-to-speech benchmarks, the proposed approach achieves similar translation results to VALL-E X, but with a large improvement in speech naturalness. When provided with ground-truth target texts, the proposed approach performs definitely worse in translation quality than VALL-E-X, but still better in naturalness. The paper reports a result where the model is used in an unwritten language scenario for English-to-Spanish translation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* Proposes a language model approach for speech-to-speech translation that uses semantic units instead of phonetic units, making it usable for unwritten languages.\n* Makes use of decoder-only architectures which enable effective prompting.\n* Reports ablation studies showing advantages of decoder-only over encoder-decoder architecture when using the same training data.\n* Demonstrates advantages of training the model on data from multiple tasks including ASR and MT.\n* Shows improvements in naturalness over VALL-E X in a zero-shot setting.\n* Performs ablation studies to show the utility of each model component."
            },
            "weaknesses": {
                "value": "* It would have been preferable to report performance on a low-resource target language in an unwritten scenario. Such a setup might reveal additional challenges which are not present in a high resource language such as Spanish."
            },
            "questions": {
                "value": "* 4.1.1:  For Chinese->English task, what is the size of the semantic unit inventory for Chinese and English?\n* Can the semantic unit inventory be shared between the source and target sides?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Reviewer_2GYW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638868926,
        "cdate": 1698638868926,
        "tmdate": 1699636907698,
        "mdate": 1699636907698,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W6aZ9fi2Pv",
        "forum": "hCrFG9cyuC",
        "replyto": "hCrFG9cyuC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_PpHm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_PpHm"
        ],
        "content": {
            "summary": {
                "value": "The proposed speech-to-speech (s2s) translation system consists of three models: translation model, duration model, and unit-to-speech model. The novelty of this work is that all of these models are decoder-only (while some prior work preferred encoder-decoder architectures) and the combination of these three models to do unit-based s2s."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The system design (using three decoder-only models) seems sound and worth investigating, although I'm not 100% on board with motivating it with the raise of GPT - there is more to the success of LLMs than being decoder-only models. Anyways, the main results (Table 2) look solid. The system description seems clear superficially, but there are some core open questions (see weaknesses)."
            },
            "weaknesses": {
                "value": "I couldn't get a good sense of the training data - specifically how the different prompts from Table 1 are used to synthesize data, and the size of the synthesized dataset: is it the 44M sentences from Table 7 in the appendix, or more because multiple prompts are used? How does the training data compare to the baselines?\n\nMy main concern would be that the ablation studies are not effective for disentangling the many design choices and the many moving parts of the whole architecture. The encoder-decoder vs decoder-only comparison is a good start, but I still don't have a good sense about how well the synthetic data generation works, and how well each of the three models do in isolation. Possible interesting ablations would be removing the duration model, replacing u-xlm with an out-of-the-box asr/mt cascade, removing source speech dependency from u-slm, passing through n-bests between the models, etc. I'm not requesting that all possible ablations should be included, but giving a little bit more color to the paper story would make it much stronger."
            },
            "questions": {
                "value": "See weaknesses:\n- Could you give more details on how the prompts are used\n- Could you give more details on the synthetic training data\n- Have you considered some of the ablation studies mentioned above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680407544,
        "cdate": 1698680407544,
        "tmdate": 1699636907545,
        "mdate": 1699636907545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1FPezhFW7s",
        "forum": "hCrFG9cyuC",
        "replyto": "hCrFG9cyuC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a decoder-only based system for S2ST. The system includes three decoder-only LMs, including a translation LM to translate source language semantic units to target language semantic units, a duration LM to predict target language semantic unit durations and extend the sequence, and a speech synthesis LM to predict the target language acoustic units which are converted to waveforms by a unit vocoder. Both semantic units and acoustic units are self-supervised learned, hence this framework can be applied to unwritten languages."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1)\tThe use of decoder-only LMs via different prompting strategies and discrete semantic and acoustic units for all components (translation LM, duration LM, and speech synthesis LM) could benefit S2ST from competitive pre-trained text decoder-only LLMs.\n\n(2)\tEmpirical evaluations show that PolyVoice is comparable to VALL-E X, very slightly better on ASV, worse on ASR-BLEU, and better on naturalness.  Ablation studies show the contribution of the designed duration LM which uses a LM to predict durations of semantic units and extend the sequence. The duration LM significantly helps WER, with a very slight help on ASV and slight help on naturalness."
            },
            "weaknesses": {
                "value": "(1)\tThe innovations of this work need to be more clearly explained. This work bears strong similarity to VALL-E X. It is important to clarify the difference between the proposed approach and VALL-E X, but the paper did not clearly point out the difference between PolyVoice and VALL-E X to highlight the innovations of the proposed PolyVoice.  Both works concatenate source and target semantic units and the source acoustic units to create the prompt for the LM. For PolyVoice, this prompt is created for the duration LM and the speech synthesis LM, respectively.  The soundstream codec is reimplemented, but the impact of the reimplementation is not clear. \n\n(2)\tSome of the key technical presentations lack clarity. \n\na.\tSection 3.1, when describing Unit-based cross-lingual language model (U-XLM) , the paper shows the prompt for encoder-decoder architecture.  The paper should also clarify the prompt for decoder-only LM. \n\nb.\tSection 3.1, when describing training, Table 1 shows how ASR, MT, and TTS supervised data are used for training. The paper also mentioned that \u201cThis approach also enables the direct utilization of unlabeled text and speech data. \u201c, yet how to use unlabeled text and speech data for training is not explained here. Instead, based on Section 4.1.1, it seems that one approach is to apply in-house MT and TTS systems on ASR data to create pseudo S2S data.  This part needs to be clarified.\n\n(3)\tMore complete and also deeper discussions are desired for empirical validations:\n\na.\tTable 2, the paper compares to cascaded (ASR-MT-TTS) and VALL-E X. It is not clear whether all well-established competitive baselines are included in this comparison. \n\nb.\tTable 2, the ASV metric evaluates the capability of preserving the voice of the source speaker. However, without ground truth target information, PolyVoice achieves 0.38 and 0.38 for hyp vs.src and hyp vs. tgt, while VALL-E X achieves 0.37 and 0.37 respectively. These are very small gains, yet the paper did not discuss this point.\n\nc.\tSection 3.1, the paper mentioned that CoT could be applied for source-to-target unit translation, yet prior works (Peng et al., 2023, Towards making the most of chatGPT for machine translation) find that when CoT is applied, the model tends to conduct word-by-word translation, hence degrading the translation performance. Table 2 shows the impact of applying CoT that it improves ASR-BLEU. But this result is not analyzed nor discussed.\n\nd.\tSection 4.3.2, the evaluation of PolyVoice for unwritten language is quite brief. It is only evaluated for the target language treated as an unwritten language.  It would be useful to also extend the evaluation to source or both languages as unwritten language. \n\ne.\tSection 5.1, again, the discussions are very brief.  More analyses and insights are expected to explain the better ASR-BLEU from decoder-only over encoder-decoder. \n\nf.\tSection 5.2, for the other tasks (ASR, ST, MT, TTS) in Table 5, since no baseline results are provided, it is not clear how these performances compare to baseline results on this dataset from prior works.\n\ng.\tSection 5.3, when using a mHuBERT model trained with more parameters and more data, the WER decreased which is explained, but ASV and naturalness got degraded.  Insights are expected to explain these results.\n\nh. The proposed system uses many in-house data and in-house systems. The amount of data and model sizes need to be clearly compared as well when comparing to baselines."
            },
            "questions": {
                "value": "Please check comments and concerns listed under Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Reviewer_JZne"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698998048229,
        "cdate": 1698998048229,
        "tmdate": 1700688871804,
        "mdate": 1700688871804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p3J9mnaPuF",
        "forum": "hCrFG9cyuC",
        "replyto": "hCrFG9cyuC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
        ],
        "content": {
            "summary": {
                "value": "Polyvoice is a framework for building speech to speech translation system with language modeling (or decoder only) approach as an alternative to the sequence-to-sequence (or encoder-decoder) architecture. The authors show that this is indeed possible given a combination of such LMs, i.e., translation LM, duration LM and speech synthesis LM. Each of the 3 models use unsupervised semantic and acoustic units.\n- Translation LM -  Uses source semantic units derived from HuBERT to predict target semantic units\n- Duration LM - User source and target semantic units with source duration to predict target duration units\n- Speech Synthesis LM - Uses source and target semantic units with source acoustic units to predict target acoustic units\n\nAuthors show competitive performance on EMIME (Chinese $\\rightarrow$ English) and CVSS (English $\\rightarrow$ Spanish) compared to methods proposed in VALL-E X. They also compare their work to current SoTA seq2seq approach (Lee et al.) and show zero-shot performance on dev-clean set of Librispeech.\n\nOverall, the paper's main contribution is demonstrating that a decoder-only architecture is sufficient towards building a speech-to-speech translation system with unsupervised semantic and acoustic units."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework is the novel in its approach towards speech-to-speech translation where it uses decoder-only models.\n- Decoder only framework simplifies the model architecture and hence makes the implementation of the translation system straightforward.\n- The proposed method is based on unsupervised semantic and acoustic units making it possible to build systems of unwritten languages.\n- Performance on the datasets shown is quite competitive and the ablation studies further highlight the importance of the 3 component models of the framework."
            },
            "weaknesses": {
                "value": "- The duration and speech synthesis models depend on the translation model. Hence the training of two models depend on one upstream model which can make experimentation slow. At least, the duration model can be attempted to be folded in the translation model as shown in the paper Text-Free Prosody-Aware Generative Spoken Language Modeling (Kharitonov et. al.).\n- Since the authors use CVSS it would be desirable to show the performance on other language pairs from the dataset to make the evaluation more robust.\n- The paper stated that they have used \"in-house ASR datasets\". It's not clear how much contribution from these in-house datasets make the method effective. This is makes the reproducibility of the paper very difficult if these \"in-house datasets\" are not released."
            },
            "questions": {
                "value": "- HuBERT is trained on English only corpus. Did you just apply the model to discretize Chinese speech, or had to adapt it?\n- Why was the specific language pairs that have been evaluated chosen?\n- You have not cited On Generative Spoken Language Modeling from Raw Audio (Lakhotia et. al.) and Text-Free Prosody-Aware Generative Spoken Language Modeling (Kharitonov et. al.) which showed that unsupervised discrete units can be used for speech synthesis and that duration is indeed crucial in improving the prosodic characteristics of the produced speech.\n- Spelling error: \"marked\" instead of \"maked\" in section 4.1.1"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7518/Reviewer_Jg4v"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699017556839,
        "cdate": 1699017556839,
        "tmdate": 1699636907266,
        "mdate": 1699636907266,
        "license": "CC BY 4.0",
        "version": 2
    }
]