[
    {
        "id": "NxvYGS6Go5",
        "forum": "M8J0b9gNfG",
        "replyto": "M8J0b9gNfG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a visual speech recognition model that can perform recognition on multiple languages. The model consists of a AV-HuBERT module that extracts visual speech units, followed by transformer module that converts visual speech units into text. The proposed model is trained in three steps. The first step focus on training mAV-HuBERT module by regressing visual input to discrete visual speech unit with supervision coming from pre-trained multilingual HuBERT model. The second step conducts pre-training on transformer module by taking visual speech unit extracted by mAV-HuBERT model and predict text. The final step conducts an end-to-end fine-tuning of all modules by taking visual input and predict text. Experimental evaluation demonstrated that the proposed mAV-HuBERT model can effectively perform multilingual visual speech recognition compared to mono-lingual method which has to be trained on individual language. The proposed training strategy also improved over naive multilingual model trained on all languages while reducing training time by using discrete visual speech unit as input during pre-training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper extended AV-HuBERT to effectively handle multilingual scenario. Compared to mono-lingual AV-HuBERT that is trained on individual language, mAV-HuBERT achieved best WER on three out of five languages and second-best on two out of five languages. The improvement on under-resourced languages is especially encouraging as the proposed method demonstrate a possibility to improve VSR on a specific language by leveraging other languages. \n2. The proposed training strategy of mAV-HuBERT not only help improve the recognition accuracy of AV-HuBERT on different languages but also improves training efficiency by leveraging discrete visual speech unit during pre-training, which uses much less data storage and thus allows much higher batch size and reducing training time. The strategy of curating multilingual language dataset used during training is also a contribution\n3. Analysis on visual speech unit provides insight on the unit such as the unit captures well on the viseme information rather than other information such as speaker identity."
            },
            "weaknesses": {
                "value": "1. Although the overall approach and the problem the paper tackles are novel, the core model is largely based on an existing model AV-HuBERT with minimal modification. The pre-training objective is also commonly used without much modification. Perhaps the authors could clarify a bit more on any contribution regarding extending AV-HuBERT in model architecture if applicable. \n2. The comparison in Section 4.3.1 may not be completely fair. For AV-HuBERT, the model is pre-trained with English only, so fine-tuning on other languages means the English pre-trained AV-HuBERT is trained with same loss function on a new language, which will result in 5 different fine-tuned model i.e. one for each language. For mAV-HuBERT, a same model is supposed to work for different languages. So the fine-tuning is not supposed to be done on each language, which would yield 5 different models. If this is how the experiment was done, then mAV-HuBERT has advantage by design as it was pre-trained with more language data. If my understanding was not correct, then the authors should clarify on the specific process of fine-tuning of mAV-HuBERT.\n3. The comparison with multilingual VSR approaches is weak. The only comparison done was with AV-HuBERT as the authors claim there is no prior work that can perform multilingual VSR with a single model. However, there are recent work and reference therein indicate exploration along this direction. For example,\n- Cheng et al., MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition, ICCV 2023\n- Anwar et al., Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text\ntranslation, 2023."
            },
            "questions": {
                "value": "1. I'm curious on how the number of visual token size used to determine visual speech unit used in the first step affect the final performance. Do the authors vary the number (1000 being used in the paper) and choose the one with better performance?\n2. Regarding the curriculum learning, I'm also curious on how the learning schedule affect the performance. And when p% reaches 100%, the embedding from audio speech unit is useless. Do we disregard the embedding completely (thus no concatenation needed) or we still retain the same process to generate concatenated embedding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_RNNa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604449357,
        "cdate": 1698604449357,
        "tmdate": 1700724303966,
        "mdate": 1700724303966,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1F4Gmr6CR",
        "forum": "M8J0b9gNfG",
        "replyto": "M8J0b9gNfG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_tK3Y"
        ],
        "content": {
            "summary": {
                "value": "The paper extends a recent work on audio speech units to the audio-visual (AV) domain and tackle the associated challenge of the need for large amounts of labeled training and the computational cost. In doing to, they also extend the previous work on English language AV that leveraged visual phoneme units to a single-model solution for several popular languages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. There is solid engineering of the final system, with sufficient details to support reproducibility.\n2. Builds on the recent state-of-the-art and leverages the nifty insights in training large scale ASR across languages and modalities.\n3. Good amount of ablation studies to demonstrate the efficacy of various system components.\n4. Source code, models, and samples (to be?) made publicly available."
            },
            "weaknesses": {
                "value": "1. The reference list and the related work sections are relatively misleading for the title of the paper. Either the term, \"neural network/deep learning models,\" or a similar qualifier has to be added to the title and abstract or the related work should be expanded to include the historical work in the area, prior to the use of deep learning models.\n2. Section 3, particularly 3.2 is hard to follow with it not being self-contained. I recommend bringing the results that justify the variety of modeling and training choices being described here into this section itself.\n3.  This work leverages good tips and engineering practices in building the mAV-HuBERT solution but is obscures the focus from what is the big science/research delta between AV-HuBERT and mAV-HuBERT."
            },
            "questions": {
                "value": "1. What is the justification of hyperparameter choices such as, \"We use the target size of 1,000 and train the model for 350k steps with one iteration?\"\n2. \"...reduce training speed by removing the visual front-end.\"  Unsure why this is a good thing, unless you are referring to reducing the step size.\n3. Why does the curse of multi-linguality only affecting the results for English? How does the relative sizes of training data in various languages affect this?\n4. \"After 70% of training, p is set to 100 so that only visual speech units are used.\" What is magical about 70?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No additional concerns with this work beyond what the ones that affect the publicly available data that this work leverages."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782597123,
        "cdate": 1698782597123,
        "tmdate": 1699636320744,
        "mdate": 1699636320744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ENSKSoFtQu",
        "forum": "M8J0b9gNfG",
        "replyto": "M8J0b9gNfG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_SxNH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a multilingual visual speech recognition, by using the visual speech units to improve the training efficiency. It also proposes a curriculum learning approach to also exploits audio speech units. The paper presented number of experiments to investigate the effectiveness and efficiency of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- a novel frame work\n- an efficient framework\n- good presentation and experiments"
            },
            "weaknesses": {
                "value": "- reproducibility seems difficult"
            },
            "questions": {
                "value": "1- Computation aside, how different performance would be if we use continues features compared to unit features?\n\n2- The paper mentions that one novelty is presenting the sentence level model. it would be good if some explanation is provided around advantage of sentence level models?\n\n3- Once audio speech is utilised in the pertaining together with visual speech units, why using audio speech unit and not complete speech features? is it only to make computation more manageable? how performance and computation would change if audio speech is not discretised? \n\n4- Table 2 contains interesting results, where the multilingual setup does not help English dataset. In addition to what authors mentioned, there could be other reasons like language similarities or available language specific information. This has been the topic of number of works around multilingual models that how to balance between more data Fram more languages or using less data only from fewer similar languages[1][2]. While I believe this paper's topic is not around this issue, I think it would be good to refer  to this point\n\n[1] Cross-entropy training of DNN ensemble acoustic models for low-resource ASR\n[2] Multilingual data selection for low resource speech recognition"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no concern"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786405744,
        "cdate": 1698786405744,
        "tmdate": 1699636320654,
        "mdate": 1699636320654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lLIMJ97fo9",
        "forum": "M8J0b9gNfG",
        "replyto": "M8J0b9gNfG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_JHaL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_JHaL"
        ],
        "content": {
            "summary": {
                "value": "This work explores sentence-level Multilingual Visual Speech Recognition with a single model based on the pretrained AV-HuBERT and multi-lingual Hubert model. By taking advantages from the previous AV-HuBERT and the multi-lingual Hubert model together with the combination of multiple audio-visual speech datasets of different languages, the work here is able to transform the visual speech features to visual speech units and further perform multilingual visual speech recognition based on these units."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. It\u2019s indeed very appealing in the view of efficiency when trained with discrete units."
            },
            "weaknesses": {
                "value": "Compared with new methods or other insights, I think this work is more like a case to show the success again of large-scale data and large-scale model. Similar to the audio speech units, the visual speech units are introduced here. But the concept of visual speech units has been introduced since AV-HuBERT and its contemporary works. \nThe method here is totally based on the pre-trained AV-HuBERT and multilingual HuBERT, with a common masking strategy by gradually increasing the masked ratio of audio samples. There seems no new points in the proposed method and training strategy. It is more like a presentation of an example to use existing large-scale models (AV-HuBERT, HuBERT)."
            },
            "questions": {
                "value": "(1) In Table.2, the worse performance of the proposed method compared with AV-HuBERT is described as the curse of multilinguality. What\u2019s the specific ratio of English data in the whole data? If the English data takes a large ratio in the whole data, and there are also common shared visemes among different languages, considering the larger-scale of the whole data compared with AV-HuBERT, the performance on English should not be worse? \n(2) In Table 3, the proposed work is based on pre-trained AV-HuBERT and HuBERT. The \u201cstandard VSR\u201d is also trained further based on these two models? or from scratch? The duration of \u201c52.5 hours for 8 epochs\u201d is also based on loading the pre-trained models?\n(3) In Table 5, what\u2019s the results of using only \u201cunit pretraining\u201d, whitout both CL and FT? Will it degenerate to the case of AV-HuBERT in Table 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699177360270,
        "cdate": 1699177360270,
        "tmdate": 1699636320583,
        "mdate": 1699636320583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g5BnY1TsbR",
        "forum": "M8J0b9gNfG",
        "replyto": "M8J0b9gNfG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes methods to train a multilingual visual speech recognition model, a first of its kind.  The main contributions of the paper are usage of quantized representation of AV-HuBERT embeddings to reduce the dimensionality of video data, alongside proposing a curriculum learning approach to improve the VSR performance. This approach gradually diminishes the reliance on audio data during training, ultimately leading to the model being trained solely on video embeddings.  The proposed methods significantly reduces training computational costs while enhancing Visual Speech Recognition (VSR) performance across multiple languages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-> Trains a single model to lip read in multiple languages, which I believe hasn't been done before.\n\n-> There is a clear improvement in the total training time required to train the mAV-Hubert due to the proposed quantization of AV-Hubert embeddings. \n\n-> The proposed multi-lingual model maintains its effectiveness and stands its ground when compared to the monolingual VSR method.\n\n-> The paper has good analyses on the information captured by the discretized visual units which is useful to the VSR community."
            },
            "weaknesses": {
                "value": "-> The novely of the article is limited. The authors use preexisting blocks such as AV-Hubert and previously proposed methods of curriculum learning to improve the VSR performance. \n\n-> From Table 5, it looks like curriculum learning does not seem to have a significant impact on the VSR performance.\n\n-> The paper contains grammatical errors and requires proofreading."
            },
            "questions": {
                "value": "In table 6, the authors showed that AV-HuBERT trained on multi-lingual data performs worse that mAV-Hubert for non-English dataset, thereby claiming that their proposed strategy of using visual speech units for training mAV-HuBERT is more effective in building VSR models. The claim would be better substantiated if the authors can apply the same strategy for training mAV-HuBERT on English only train dataset and show that it performs better than AV-HuBERT for English test set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As the authors indicate in their paper, lip reading technology has potential to be mis-used in surveillance systems which could violate the privacy of the speaker."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3646/Reviewer_uTmt"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699468073877,
        "cdate": 1699468073877,
        "tmdate": 1699636320518,
        "mdate": 1699636320518,
        "license": "CC BY 4.0",
        "version": 2
    }
]