[
    {
        "id": "mVWcWNTbHZ",
        "forum": "AlkANue4lm",
        "replyto": "AlkANue4lm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD"
        ],
        "content": {
            "summary": {
                "value": "This paper points that the redundancy, i.e., repeated exchange and encoding of identical information, in the message passing framework amplifies the over-squashing. To resolve the redundancy, the authors propose an aggregation scheme based on `neighborhood trees', which control redundancy by pruning branches. Authors have theoretically proved that reducing redundancy improves the expressivity, and experimentally showed it can alleviate over-squashing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper has pointed out the inherent problem of message passing problem, the \u201crepeated exchange and encoding of identical information\u201d amplifying over-squashing."
            },
            "weaknesses": {
                "value": "1. The necessity of k-redundant Neighborhood Tree (k-NT) seems week. In Table 2, experiments on EXP-class, the performance seems to be always higher when k is smaller. Removing all redundant nodes seems to be the best choice, why use k as an selection?\n2. Experiments seems to be not sufficient enough to support the authors claim. For example in the abstract, authors claimed that the paper experimentally shows the method alleviates over-squashing. They have shown the results for synthetic datas in Table 2, but they are no experiments for real-world datasets to show this (such as experiments on long-range graph benchmark). \n3. In the introduction section, the authors mentioned PathNNs and RFGNN as closely related works. Also in table 3, the authors highlighted the best results from polynomial time complexity in bold. However, it seems that they are no comparison with any methods having polynomial time complexity other than linear."
            },
            "questions": {
                "value": "1. For experiment results in Table 1, 3, authors highlighted the best results with polynomial time complexity methods, emphasizing that DAG-MLP has advantages in time. What is the time complexity of DAG-MLP in terms of big-O notation? Also, is there any inference time comparison for the inference time of each method (GIN, SPN, PathNN, DAG-MLP)? \n2. Following weakness #4, is there are more baselines to compare with the paper method having a polynomial time complexity? What about the results of RFGNN mentioned for related works?\n3. In Table 3, the performance IMDB-B and IMDB-M datasets are said to not applicable. However, in the Michel et al.$^{[1]}$, they do report the performance of PathNN-SP+(K=2) for datasets IMDB-D and IMDB-M. What do the authors mean by not applicable? Also, what path length K did the authors use for PathNN networks in Table 3?\n\n[1] Michel et al., Path neural networks: Expressive and accurate graph neural networks, ICML 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_h7VD",
                    "ICLR.cc/2024/Conference/Submission5380/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698310992600,
        "cdate": 1698310992600,
        "tmdate": 1700541008571,
        "mdate": 1700541008571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RXt8P6pKlq",
        "forum": "AlkANue4lm",
        "replyto": "AlkANue4lm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_e2WG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new graph neural network architecture that alleviates the redundancy in the message-passing structure. The authors (1) prove that the expressive power of the new GNN architecture improves over the 1-WL test and (2) the new GNN architecture alleviates the over-squashing issue. The proposed architecture is evaluated on the synthetic datasets and the TUDataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a new GNN architecture to alleviate the redundancy of message passing. The proposed architecture is sound. The figures are helpful for understanding the paper."
            },
            "weaknesses": {
                "value": "My main concern is on the positioning of the paper with respect to similar work on alleviating GNN redundancy, i.e., RFGNN (Chen et al., 2022), and the weak experimental results. \n\n### Comparison with RFGNN \n\n- This work argues to alleviate over-squashing based on the results from RFGNN (Chen et al., 2022). However, as the authors argue, their proposed GNN architecture is different from RFGNN. Hence the logic is incomplete, i.e., it is not clear whether the proposed architecture alleviates over-squashing based on the same logic as RFGNN. \n-  Upon reading Appendix A, the authors seem to claim that RFGNN introduces more redundancy compared to the proposed work. Since there is no clear explanation of how redundancy is harmful to GNN tasks, it is hard for me to understand the benefit of the proposed DAG-MLP. \n- Furthermore, the authors do not compare the expressive power of DAG-MLP compared to RFGNN. One might argue that RFGNN might be more expressive than the proposed DAG-MLP at the cost of introducing more redundancy. \n- In addition, the authors claim speed-up of RFGNN as another benefit. I wonder if the authors could empirically show this in a meaningful scenario, e.g., large-scale graphs.\n\n### Weak experiments (TUDataset)\n- Overall, I think TUDataset is not good enough for evaluating the performance of DAG-MLP in practical scenarios. Especially, to validate the ability of DAG-MLP to alleviate over-squashing, I strongly suggest the long-range graph benchmark (Dwivedi et al., 2022) to run the proposed DAG-MLP.\n- The proposed work underperforms compared to the PathGNN. While the authors argue that PathGNN takes exponential running time, the actual running time is not reported. Hence it is hard to tell whether the issue is practically relevant. \n- The statistical box plot in Appendix F should be similarly drawn for the baselines to make a fair comparison.\n- The authors use four versions of DAG-MLP (0/1-NT, fixed single height/combined heights) while the relevant baselines have usually one or two versions (PathGNN has three versions, but DAG-MLP is not directly compared due to computational complexity). This makes the comparison unfair especially for TUDataset with high variance scores. \n- The list of baselines is not comprehensive enough to check whether if performance improvement of the proposed DAG-MLP is practically relevant."
            },
            "questions": {
                "value": "How does the actual running time of DAG-MLP compare with the baselines in the considered experiments? I think this is an important criterion since the main (and possibly the only) benefit of DAG-MLP over RFGNN is the computational complexity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727901812,
        "cdate": 1698727901812,
        "tmdate": 1699636543877,
        "mdate": 1699636543877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5GEd4tE7id",
        "forum": "AlkANue4lm",
        "replyto": "AlkANue4lm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_8Vsr"
        ],
        "content": {
            "summary": {
                "value": "A compact representation of neighborhood trees is proposed, from which node and graph embeddings via a neural tree canonization technique are computed. The main goal is reduce redundancy in message passing GNNs to address oversquashing. The resulting message passing GNN is provably more expressive than the Weisfeiler-Leman test."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Good literature discussion with details what distinguishes the different approaches.\n- The basic concepts are introduced in detail.\n- The proposed 1-NT isomorphism test is provably more powerful than the Weifeiler-Leman test.\n- Experiments verify that a reduction in redundancy could help address oversquashing."
            },
            "weaknesses": {
                "value": "- k seems to be a hyper-parameter that would need to be tuned in practice.\n- Even though DAG-MLPs are provably more expressive than the Weisfeller-Lehmann method, they are not proven to be fully expressive (i.e. distinguish any non-isomorphic graphs).\n- The computational complexity of the proposed architecture and algorithms are not analysed but form an integral part of the contribution.\n- PathNN-P seems stronger on the Enzymes and Proteins dataset but also suffers from exponential computational time complexity."
            },
            "questions": {
                "value": "- How expressive are the proposed DAG-MLPs? It seems like there could exist non-isomorphic graphs that cannot be distinguished by DAG-MLP. What would be an example?\n- How does the expressive power compare with baseline methods?\n- What is the computational complexity of building and evaluation DAG-MLPs? What are their memory requirements?\n-> It would be helpful to add measurements of time complexity in the tables of the experiments.\n- What could be other explanations why k-NTs perform less well for higher $k$ in Table 3? Does the explanation have to be over-squashing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792057616,
        "cdate": 1698792057616,
        "tmdate": 1699636543775,
        "mdate": 1699636543775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Uy2X9Jq1jp",
        "forum": "AlkANue4lm",
        "replyto": "AlkANue4lm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel aggregation scheme based on neighborhood trees to control redundancy in message-passing graph neural networks (MPNNs). The authors show that reducing redundancy improves expressivity and experimentally show that it alleviates over squashing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper introduces a novel aggregation scheme based on neighborhood trees, which allows for controlling redundancy in message passing MPNNs.\n2) The authors provide a theoretical analysis of expressivity that shows the proposed method is more expressive than the Weisfeiler-Leman method."
            },
            "weaknesses": {
                "value": "1) The main weakness is the computational cost, which requires O(nm) space where n is the number of nodes and m is the number of edges. This brings a significant limitation to the applicability of the proposed method, even for moderate-sized graphs.\n\n2) The experimental result only shows occasional marginal improvements over some baselines and only on a few datasets. This is not enough to demonstrate the effectiveness of the proposed method.\n\n3) One main motivation for the proposed method is to address over squashing, but there is no theoretical analysis of the proposed method to address it."
            },
            "questions": {
                "value": "1) What is the largetst graph size that the proposed method can handle?\n2) What is the preprocessing time for the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5380/Reviewer_zntd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823300831,
        "cdate": 1698823300831,
        "tmdate": 1699636543673,
        "mdate": 1699636543673,
        "license": "CC BY 4.0",
        "version": 2
    }
]