[
    {
        "id": "36VcF2ATHG",
        "forum": "zhZXk5Ctz2",
        "replyto": "zhZXk5Ctz2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_cKyU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_cKyU"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses limitations in the RGB color representation when conveying local image structures. The authors introduce an augmented RGB (aRGB) space, developed using an encoder, which captures both color and structural details. This new space offers more freedom in selecting loss functions and showcases performance improvements in various image processing tasks. Additionally, the aRGB space enhances interpretability, with the authors providing a comprehensive analysis of its properties and benefits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposed an augmented RGB (aRGB) space is the latent space of an autoencoder that comprises a single affine decoder and a nonlinear encoder, trained to preserve color information while capturing low-level image structures. The results imply that the RGB color is not the optimal representation for image restoration tasks."
            },
            "weaknesses": {
                "value": "Based on the experiments, compared to previous methods, the improvement brought by vggloss is quite limited, with an increase of 0.1dB (PSNR) in Table 1 and 0.02dB in Table 2. Moreover, it hasn't been compared with other perceptual methods, such as lpips or ssim loss.\n\nAlthough this paper claims to introduce a method that doesn't calculate loss in the RGB domain, the loss function used in training still falls within the category of pixel-based feature scale. Overall, it represents a relatively minor improvement to the loss function for low-level vision. Hence, the performance enhancement is limited.\n\nIs the selection of the number of \"experts\" highly dependent on experience? Will different tasks have significant variations? It seems that an inappropriate selection of the number of experts might lead to even lower performance than not using this loss function at all."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Reviewer_cKyU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698410793789,
        "cdate": 1698410793789,
        "tmdate": 1699636157753,
        "mdate": 1699636157753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3RsgSyiLtj",
        "forum": "zhZXk5Ctz2",
        "replyto": "zhZXk5Ctz2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_tXWj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_tXWj"
        ],
        "content": {
            "summary": {
                "value": "The authors propose aRGB loss for image restoration. The proposed loss is defined based on the latent space of an autoencoder, which consists of a single affine decoder and a nonlinear encoder. The autoencoder is trained to preserve color information while capturing low-level image structures. The authors replace per-pixel losses in the RGB space with their counterparts in training various image restoration models such as deblurring, denoising, and perceptual super-resolution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A new latent representation space is proposed and employed for restoration loss design.\n+ The aRGB loss is defined for diverse image restoration tasks."
            },
            "weaknesses": {
                "value": "-In the paper, the performance of the proposed loss are demonstrated on perceptual SR task. The results in table 1 are confusing. The PSNR and SSIM of RRDBNet are the highest among all the settings, but they are not bolded. The SSIM of the last setting is worse than most of settings for DIV2K-Val dataset, but it is bolded as better score.\n-For perceptual SR and image deblur tasks, there are considerable baselines perform better than ESRGAN and MPRNet. For example, restormer and NAFNet could be used for deblurring evaluation. In this way, we can test whether the proposed loss could consistently boost performance and lead to a new SOTA.\n-The performance gains are too small, which can hardly verify the effectiveness of the proposed loss."
            },
            "questions": {
                "value": "The proposed loss is similar to Fourier loss, which is also decomposed the image upon pre-defined basis. Can authors discuss the difference between them and compare their performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684805216,
        "cdate": 1698684805216,
        "tmdate": 1699636157684,
        "mdate": 1699636157684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PY8lz5meAE",
        "forum": "zhZXk5Ctz2",
        "replyto": "zhZXk5Ctz2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_nF5y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_nF5y"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to address the limitations of per-pixel RGB distances in image restoration. The authors propose a new representation space called augmented RGB (aRGB) space, where each pixel captures neighboring structures while preserving its original color value. By replacing the RGB representation with aRGB space in the calculation of per-pixel distances, the authors demonstrate performance improvements in perceptual super-resolution, image denoising, and deblurring tasks. In addition, the aRGB space allows for better interpretability through comprehensive analysis and visualization techniques. The contributions of this paper lie in the introduction of a versatile representation space, performance improvements in various image restoration tasks, and interpretability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces the augmented RGB (aRGB) space for better image restoration.\n- The paper provides a comprehensive and insightful analysis and visualization techniques for the aRGB space, enhancing interpretability. The analysis is solid and convincing.\n- The versatility of the aRGB space allows for more freedom in choosing the loss function."
            },
            "weaknesses": {
                "value": "- The performance improvement of the proposed aRGB space in the denoising and debluring tasks seems insignificant. In Table 2, comparing the first two rows, and the last two rows, the PSNR gains are only 0.02 dB and 0.03 dB, respectively. In Table 3, the PSNR improvements between the last two rows are 0.07 dB on GoPro and 0.02 dB on HIDE dataset.\n\nAdditional comments\n- Equation 6, L_{pair} should be L_{pixel}"
            },
            "questions": {
                "value": "- The space aRGB is originally designed to encode structure information on a pixel basis. Why it can exhibit suppression of artifacts in SR tasks?\n- The training process of aRGB auto-encoder does not involve any loss regarding local structure. Is it possible that the encoder also learns other information, e.g., texture, style?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699017366459,
        "cdate": 1699017366459,
        "tmdate": 1699636157600,
        "mdate": 1699636157600,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YsYq0QTA2Y",
        "forum": "zhZXk5Ctz2",
        "replyto": "zhZXk5Ctz2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_JdZe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2244/Reviewer_JdZe"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes augmented RGB representation to alleviate the issue that per-pixel loss functions defined in the RGB color space tend to produce blurry, unrealistic textures. The proposed aRGB is designed with a nonlinear mixture-of-experts encoder and a linear decoder to meet two requirements. The experiments are conducted on various loss functions across different image restoration tasks for demonstration."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper analyzes the drawbacks of the per-pixel loss functions in the RGB space, To alleviate the issues of the tendency to producing blurry blurry, unrealistic textures, the paper proposes an aRGB representation to include the local texture for training. The analyses are sound and profound.  Based on the developed encoder and decoder, the method improves the performance of three image restoration tasks using different kinds of loss functions."
            },
            "weaknesses": {
                "value": "The additional architecture for aRGB representation transmission may introduce more computation consumption during the training phase. The improved performance on image motion deblurring seems to be minimal."
            },
            "questions": {
                "value": "1. The authors design a nonlinear mixture-of-experts encoder and a linear decoder for aRGB representation. Can this design principle be applied to guide the architecture design of image restoration networks?\n2. Is the additional en/decoder equivalent to adding an additional branch for learning? I doubt whether the improved performance is yielded by the additional computation overhead.\n3. The widely used dual-domain loss in models, such as MIMOUNet (Cho et al, ICCV'21) and SFNet (Cui et al, ICLR'23), can introduce global information refinement. How does aRGB compare to this loss function? This function does not lead to much computation overhead.\n4. Does aRGB lead to extra computation overhead during training and inference?\n5. Does the proposed the aRGB architecture rely on the dataset trained on?\n6. The reviewer thinks that the performance improvement on GoPro is minimal. For example, only a 0.05 dB PSNR gain is obtained for MPRNet on GoPro. What do the authors think about this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2244/Reviewer_JdZe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2244/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699528323722,
        "cdate": 1699528323722,
        "tmdate": 1699636157496,
        "mdate": 1699636157496,
        "license": "CC BY 4.0",
        "version": 2
    }
]