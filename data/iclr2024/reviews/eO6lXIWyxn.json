[
    {
        "id": "VCzWjc5efo",
        "forum": "eO6lXIWyxn",
        "replyto": "eO6lXIWyxn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_fEGX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_fEGX"
        ],
        "content": {
            "summary": {
                "value": "This work presents ARTIST, a novel framework to address text rendering problems in diffusion models for image generation. They employ LLMs to identify users\u2019 intentions and introduce dual stages of training to master text structure and visual quality. They achieve up to 15% improvement in various metrics on the MARIO-Eval benchmark, demonstrating the effectiveness of their approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper demonstrates a well-structured and clearly articulated research strategy. The experimental results presented in the paper showcase good empirical performance."
            },
            "weaknesses": {
                "value": "- The formatting of the main text has some issues, i.e., page 5 with the presentation of Fig. 2 and Eq. (2). Such formatting problems could hinder the overall readability and should be addressed.\n- Visual results of ARTIST and the analysis of experiments are somewhat lacking. More qualitative results, similar to those presented in the 2nd column of Fig. 5, should be provided to better illustrate the differences between ARTIST and TextDiffuser. \n- The references need some revisions. For instance, the reference to \"Classifier-Free Diffusion Guidance\" should be updated to reflect its publication in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Ensuring that all references are accurate and up-to-date."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5802/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631995701,
        "cdate": 1698631995701,
        "tmdate": 1699636611288,
        "mdate": 1699636611288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7kYiKIVpvw",
        "forum": "eO6lXIWyxn",
        "replyto": "eO6lXIWyxn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_B4bj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_B4bj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework ARTIST that includes one additional text diffusion model to learn visual text structure, which helps disentangle the learning of text structure and text aesthetics. The experiments show improvement of OCR performance compared to previous methods. This paper also introduces large language model to resolve the issues of extracting target text."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea to disentangle the learning of text structure and text aesthetics is novel. The experiments also show the efficacy of the proposed method.\n2. Utilizing LLM can help resolve the issues of target keyword extraction from prompts with no explicit mark for target text."
            },
            "weaknesses": {
                "value": "1. I am not convinced as to why we should use LLM to improve target text extraction since it is very easy for the user to specify the target text. \n2. The authors did not provide enough information about how LLM performs in generating a suitable layout.\n2. The method part writing is not clear regarding the input to the model. How can a model decide which mask to generate the suitable word? For example, in Figure 2, how are \"ARTIST\" and \"MODEL\" specified for the two mask regions?"
            },
            "questions": {
                "value": "1. How are the layouts generated using LLM? Are the layouts limited to normal-text layouts? Can LLM generate rotated or curved text layouts? Could authors provide detailed prompts for LLM?\n2. Could the authors provide more information about the input for the visual examples, e.g. the layouts in Figures 3 and 4.\n3. How does ARTIST deal with multiple-word generation? How does the model decide which mask to generate for each word?\n4. Instead of adding another module to learn the text structure, how does the model perform when training the original model in a two-stage process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5802/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5802/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5802/Reviewer_B4bj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5802/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821482193,
        "cdate": 1698821482193,
        "tmdate": 1699636611158,
        "mdate": 1699636611158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ScqB3K2GCv",
        "forum": "eO6lXIWyxn",
        "replyto": "eO6lXIWyxn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_JCmy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_JCmy"
        ],
        "content": {
            "summary": {
                "value": "They present ARTIST, which first leverages an external LLM to produce the layout of the target text. The disentangled pipeline then produces the visual text and the figure, and fuses both as the final generated result. ARTIST demonstrates its creative and robust text rendering ability on the MARIO-Eval dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow.\n- Text painting is a challenging task for current T2I models, and the idea of disentanglement between text and figure is well-motivated.\n-  They provide lots of qualitative examples for the visual comparison.\n- Their proposed ARTIST achieves notable improvements over the previous TextDiffuser, especially on the crucial OCR metric."
            },
            "weaknesses": {
                "value": "- It seems that the ARTIST framework is the combination of LLM-Layout and TextDiffusers, which both have been proposed before. Not sure if this achieves ICLR's novelty bar.\n- There are so many metrics used for the evaluation (FID, CLIP-S, and OCR). Which one is the most appropriate to evaluate the overall performance? Or is there any way to combine all of them as a final metric?\n- As a generative task, a human generation should be conducted to compare the performance in a human aspect.\n- Since ARTIST relies on LLM to derive the layout, there should be a discussion about the quality of the generated layout. Is it visually appealing or in a reasonable position?\n- Missing reference about LLM-Layout: [NeurIPS'23] Compositional Visual Planning and Generation with Large Language Models / [arXiv'23] Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"
            },
            "questions": {
                "value": "Please see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5802/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032688584,
        "cdate": 1699032688584,
        "tmdate": 1699636611014,
        "mdate": 1699636611014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "frWXA4vFRw",
        "forum": "eO6lXIWyxn",
        "replyto": "eO6lXIWyxn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_5qmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5802/Reviewer_5qmq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies text rendering with images and proposes to use disentangled text and visual modules (both are fine-tuned from Stable Diffusion) along with LLM to identify which text and where to render. In the experimental results, the proposed approach outperforms the baseline, and also ablation study shows the effectiveness of the proposed components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea to use disentangled modules for text and image, along with LLM for generating guidance, looks novel.\n- The proposed approach outperforms the baseline, TextDiffuser, in experimental results. \n- The presented ablation study shows the effectiveness of the proposed components."
            },
            "weaknesses": {
                "value": "- The experiment lacks an important qualitative evaluation, namely, human evaluation. This may limit to show effectiveness of the proposed approach.\n- Some experimental results are not very convincing. e.g., FID is worse than Fine-tuned SD on MARIO-Eval benchmark. Also, in Fig. 5, it is not clear the proposed model is better than TextDiffuser."
            },
            "questions": {
                "value": "- When training visual module, do the targets also include text, or only images? \n- Does LLM always succeed? Is there any failure cases, and if so, why?\n- What is \"s\" below as it is not clear. You need to define it first before using it.\npage 4:  ... model to extract s which ...\npage 4:  ... identify the essential s \n\n- You claim that \"our computation requirement is still similar to the previous SOTA TextDiffuser\", and can you provide actual model sizes such as the number of parameters of models?\n- In Fig. 6, how can the proposed model without mask achieve the better CLIP and FID compared to the one with mask?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5802/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699627769566,
        "cdate": 1699627769566,
        "tmdate": 1699636610846,
        "mdate": 1699636610846,
        "license": "CC BY 4.0",
        "version": 2
    }
]