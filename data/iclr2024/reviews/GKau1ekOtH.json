[
    {
        "id": "2nnSuwEoy2",
        "forum": "GKau1ekOtH",
        "replyto": "GKau1ekOtH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_Unxf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_Unxf"
        ],
        "content": {
            "summary": {
                "value": "This work aims to unify CLIP and SAM \u2013 two powerful vision foundation models (VFMs) \u2013 to enable a single set of parameters that are capable of retraining the advantages of both VFMs. The authors treat such model merging as a continual learning problem, where, given a pretrained VFM, the knowledge of a second VFM is merged without forgetting the initial knowledge.\n\nThe proposed model, SAM-CLIP, assumes access to a small part of pretraining data or its surrogates to be replayed during the merging process. The SAM model is used as the base VFM during the distillation, where CLIP is regarded as the auxiliary VFM and its knowledge is distilled via a cosine distillation loss. To avoid the catastrophic forgetting of SAM\u2019s original capabilities, the authors propose a rehearsal-based multi-task distillation loss to gradually distill the external knowledge to the base VFM.\n\nThe resulting trained SAM-CLIP is able to perform zero-shot classification, image-text retrieval, instance segmentation, and semantic segmentation. Across several benchmark datasets, the authors show that SAM-CLIP can achieve state-of-the-art performance in a single-stage inference setup."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper endeavors to create a comprehensive model by merging pre-trained vision foundation models, aligning with the contemporary trends in computer vision.\n- The contributed SAM-CLIP stems from a continual learning perspective, which is intuitive. As a result, SAM-CLIP is capable of conducting multiple visual understanding tasks in a zero-shot manner."
            },
            "weaknesses": {
                "value": "- A glaring omission in the paper is the technical detail surrounding the cross-VFM distillation. A deeper dive into the methodology, choices of operations, and potential effects of the framework is necessary.\n- The paper's structure and presentation could use refinement. The disproportionate emphasis on background and literature review, coupled with scant technical details, detracts from its overall coherence and depth.\n- Benchmarking SAM-CLIP against prior models, particularly those based on SAM, would offer a more rounded perspective on its performance and advantages."
            },
            "questions": {
                "value": "- **Q1:** The efficiency of SAM-CLIP on edge devices is emphasized multiple times throughout the manuscript, particularly in the \u201cAbstract\u201d and \u201cIntroduction\u201d sections. However, the empirical evidence supporting SAM-CLIP's performance on such devices seems absent. Could the authors elucidate the specifics of the claim regarding SAM-CLIP\u2019s suitability for edge devices? The reviewer would like to know what the claim means by \u201capt for edge device applications\u201d.\n\n---\n\n- **Q2:** When assessing zero-shot semantic segmentation, SAM-CLIP is exclusively juxtaposed with CLIP-based models. How does SAM-CLIP fare when contrasted with SAM-centric models, notably Semantic-SAM [R1] and SEEM [R2]?\n\n---\n\n- **Q3:** The \u201cProposed Approach\u201d section might benefit from more detailed explanations regarding the design and implementation. In particular, how do you perform the joint training between head probing and multi-task distillation?  how is the balance between head probing and multi-task distillation maintained during joint training? What metrics or criteria guide the selection of appropriate stopping points for training?\n\n---\n\n- **Q4:** The \u201cBackground\u201d section contains a profusion of general literature introductions. A more succinct and discerning review that delves into comparative analyses would greatly enhance its value.\n\n---\n\n- **Q5:** Notable typos appeared in the current illustration of this paper, which should be revised accordingly. For example:\n  - Page 1, the last paragraph: there should be a space between \u201ctasks\u201d and \u201cFifty et al., 2021\u201d.\n  - Page 2, the first paragraph: \u201cconsuming massive amount \u2026\u201d should be \u201cconsuming a massive amount \u2026\u201d.\n  - Page 2, the first paragraph: \u201chow to access to \u2026\u201d should be how to access \u2026\u201d.\n  - Page 2, the second paragraph: \u201cgeneralization to diverse set of tasks\u201d should be \u201cgeneralization to diverse sets of tasks\u201d.\n  - Page 2, the third paragraph: \u201cwe allow access to small part of \u2026\u201d should be \u201cwe allow access to a small part of \u2026\u201d.\n  - Page 3, the first paragraph: \u201cWith compromise of a negligible drop \u2026\u201d should be \u201cWith a compromise of a negligible drop \u2026\u201d.\n  - Page 3, the second paragraph: \u201cenable additional zero-shot capabilities\u201d should be \u201cenabled additional zero-shot capabilities\u201d.\n  - Page 3, the second paragraph: \u201con-top of \u2026\u201d should be \u201con top of \u2026\u201d.\n  - Page 3, the third paragraph: \u201ca model, and training recipe \u2026\u201d should be \u201ca model, and a training recipe \u2026\u201d.\n  - Page 3, the third paragraph: \u201cand produce high-resolution segmentation mask\u201d should be \u201cand produces a high-resolution segmentation mask\u201d\n  - Page 3, the third paragraph: \u201cbut has not released \u2026\u201d should be \u201cbut have not released \u2026\u201d.\n  - Page 3, the fourth paragraph: \u201cThey show transfer of the same \u2026\u201d should be \u201cThey show the transfer of the same \u2026\u201d.\n  - Page 3, the fourth paragraph: \u201cand demonstrate transfer of different zero-shot capabilities\u201d should be \u201cand demonstrate the transfer of different zero-shot capabilities\u201d.\n  - Page 3, the fourth paragraph: \u201cas well as emergence of new zero-shot capability\u201d should be \u201cas well as the emergence of new zero-shot capability\u201d.\n  - Page 3, the fifth paragraph: \u201creferring to loss of previously learned knowledge due to \u2026\u201d should be \u201creferring to a loss of previously learned knowledge due to \u2026\u201d.\n  - Page 4, the third paragraph: \u201cto obtain segmentation mask\u201d should be \u201cto obtain segmentation masks\u201d.\n  - Page 4, the third paragraph: \u201cand many forward passes, make their deployment \u2026\u201d should be \u201cand many forward passes, making their deployment \u2026\u201d.\n  - Page 4, the fourth paragraph: \u201cthe optimization algorithm is exploring the parameter space \u2026\u201d should be \u201cthe optimization algorithm explores the parameter space \u2026\u201d.\n  - Page 5, the second paragraph: \u201cand inherits its \u2026\u201d should be \u201cand inherit its \u2026\u201d.\n  - Page 5, the sixth paragraph: \u201cwhich is the case of our experiment of \u2026\u201d should be \u201cwhich is the case in our experiment of \u2026\u201d.\n\n---\n\n- **Q6:** How does SAM-CLIP perform under out-of-distribution or data corruption cases?\n\n---\n\nReferences\n\n- [R1] F. Li, et al. \u201cSemantic-SAM: Segment and Recognize Anything at Any Granularity.\u201d arXiv preprint arXiv 2307.04767.\n\n- [R2] X. Zou, et al. \u201cSegment Everything Everywhere All at Once.\u201d arXiv preprint arXiv  2304.06718."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concern or only a minor ethics concern is observed."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698573416920,
        "cdate": 1698573416920,
        "tmdate": 1699636651705,
        "mdate": 1699636651705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IasEFlqTE1",
        "forum": "GKau1ekOtH",
        "replyto": "GKau1ekOtH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_a39d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_a39d"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes SAM-CLIP to build a unified model with both the strengths of SAM and CLIP.  SAM and CLIP is employed to share the same image encoder with two separate heads. Two phased are adopted during the KD process: 1) Head probing 2) Multi-task distillation. Also 40.8M images are used in the distillation process. The results are validated on zero-shot instance segmentation, semantic segmentation and classification benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper has a good motivation on merging two visual foundation models, i.e., SAM and CLIP, into a unified model, such that the distilled model can obtain both semantic and spatial understanding.\n\n2. The paper is well organized and easy to understand.\n\n3. The experiments in Figure 1 and the experiment section show the distilled model retains both good zero-shot ability from SAM and CLIP."
            },
            "weaknesses": {
                "value": "1. When evaluating zero-shot semantic segmentation, as in Figure 3, the paper proposes a two-stage process to first using clip head for coarse masks predictions and taking it as input to SAM for refinement. Is the predicted masks by SegCLIP in Table 2 also refined by SAM? Can the authors also provide the zero-shot semantic segmentation without using geometric prompts?\n\n2. When evaluating zero-shot instance segmentation, the performance decrease on LVIS is not negligible. This suggests that the ability of SAM is decreasing after the distillation process. Can the authors also provide comparison to HQ-SAM on zero-shot instance segmentation with the same bounding box as prompt? HQ-SAM [a] is also designed for minimal forgetting and efficient tuning for SAM but without discussion in related works or results comparison. Also, the paper misses MobileSAM in the related work section, which also uses knowledge distillation.\n\n[a] Segment Anything in High Quality. NeurIPS, 2023.\n[b] Faster Segment Anything: Towards Lightweight SAM for Mobile Applications. arXiv:2306.14289.\n\n3. Since the paper mentions edge device applications in the abstract, what are the model size, speed and memory consumption of the proposed sam-clip comparing to SAM/CLIP?\n\n4. What is the influence of the dataset scale in Merged-41M, for example reducing images by half or further increasing the image number? How does the paper decide the respective data percentage for CLIP and SAM training? Also, how to decide the distillation loss value scales for the sam head and clip head, like 1:10?"
            },
            "questions": {
                "value": "Can the method deal with the instance segmentation not using bbox as prompt but using the semantics from CLIP? Overall I am positive about this paper and willing to raise scores if my concerns in the weakness can be well addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662957101,
        "cdate": 1698662957101,
        "tmdate": 1699636651604,
        "mdate": 1699636651604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5Z10qnXVnq",
        "forum": "GKau1ekOtH",
        "replyto": "GKau1ekOtH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
        ],
        "content": {
            "summary": {
                "value": "This paper merges CLIP and SAM, the two foundation models, into a single one that assimilates both knowledge and expertise learned separately. Specifically, the technical contributions include a reasonable finetuning design and integration of the two distillation losses. The resulting model supports language-driven prompts and enjoys a high-quality segmentation result."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper presents a simple yet effective way to merge two foundation models into a single one, and it inherits both advantages and demonstrates a significant performance boost;\n2. The paper is well-organized, clearly written, and easy to follow;\n3. The resulting model is promising and helpful for future research."
            },
            "weaknesses": {
                "value": "1. The resulting model inherits the zero-shot capability of CLIP, as demonstrated in Table 1-5. However, it seems that there is no evidence showing the resulting model does not suffer from catastrophic forgetting. Even though the segmentation performance is better than CLIP-head prediction, it still doesn't compare with the segmentation result of SAM and it is unclear how much performance is degraded compared to the original SAM. The demo in Figure 3 shows that the SAM-head refined output is still filled with some artifacts and seems to have a large performance gap with the original SAM.\n2. The proposed method is limited to the sizes of released SAM models. Since the vision encoder must be initialized SAM vision encoder, we cannot obtain a resulting model with an arbitrary size."
            },
            "questions": {
                "value": "1. The authors should explain more clearly the performance gap with the original SAM in terms of segmentation quality.\n2. The authors should also give the output of the original SAM, with the same examples shown in Figure 3.\n3. The authors should discuss more limitations with the resulting model and the proposed method.\n\nIf the above concerns are addressed, I am willing to improve the rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6056/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6056/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6056/Reviewer_K2ur"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762151966,
        "cdate": 1698762151966,
        "tmdate": 1699636651470,
        "mdate": 1699636651470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QkRLdL1HDZ",
        "forum": "GKau1ekOtH",
        "replyto": "GKau1ekOtH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_Kk2t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6056/Reviewer_Kk2t"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a distillation paradigm to incorporate SAM and CLIP, combing their instance segmentation and semantic recognition capabilities. SAM-CLIP uses extensive pre-training data from original models and learns a unified encoder along with two task-specific heads. SAM-CLIP showcases good performance across zero-shot classification and segmentation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is reasonable to combine SAM and CLIP to infuse their own advantages.\n\n2. SAM-CLIP shows good performance on zero-shot semantic segmentation tasks.\n\n3. The writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The contribution is a little overclaimed as *'we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise.'*. I think this method is specifically designed for CLIP and SAM, and cannot be simply generalized to other VFMs.\n\n2. The cost of training SAM-CLIP is expensive. The training data includes many sources up to 41M. Considering CLIP and SAM have already cost large-scale pre-training resources, continually tuning them as SAM-CLIP is not cost-effective. Although SAM-CLIP achieves good results for semantic segmentation, it hurts the original performance of both SAM and CLIP. I think simply cascading SAM and CLIP in a training-free way (CLIP generates prompt by vision-language alignment and then SAM segments or SAM segments all objects and then CLIP classifies) can obtain even comparable results to SAM-CLIP, which is more practical in real-world applications."
            },
            "questions": {
                "value": "SAM itself can also be prompted by texts (semantics), though not open-sourced. What's the advantage of SAM-CLIP compared to SAM with text prompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6056/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698910417200,
        "cdate": 1698910417200,
        "tmdate": 1699636651363,
        "mdate": 1699636651363,
        "license": "CC BY 4.0",
        "version": 2
    }
]