[
    {
        "id": "tUJo03RlgI",
        "forum": "Rt6btdXS2b",
        "replyto": "Rt6btdXS2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_7gbu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_7gbu"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Continuous Indeterminate Probability Neural networks, which applies Indeterminate Probability Theory to define neural networks with latent variables for classification.\nThe authors also present a related auto-encoding variant of the model, which can be used to visualize the latent variable of the model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The ideas presented in the paper are interesting and novel, to the best of my knowledge. These ideas could inspire future research.\n\n* Due to the usage of the latent variables in the CIPNN, the proposed model is less black-box than other architectures"
            },
            "weaknesses": {
                "value": "**General comments**\n\nThere are 2 major issues with this paper, regarding clarity and experiments.\n\nCLARITY\n\nOverall, I found the paper hard to understand, mostly because it relies heavily on the unpublished work in (Anonymous, 2024), and assumes that the reader is knowledgeable of its content.\nWhile there is a high level description of (Anonymous, 2024) in section 2.2, this description is rushed and confusing (see detailed comments below).\n\nBeing a conference paper, this paper should instead be self-contained: the reader/reviewer should not have to read in full (Anonymous, 2024) to understand the proposed method (especially keeping in mind that the other paper could be rejected from the conference and be therefore unpublished if this work gets accepted).\nAs is, this paper looks more like an appendix to (Anonymous, 2024), rather a paper by itself. I suggest that the authors read and rewrite this work with the eyes of someone that knows nothing about (Anonymous, 2024).\n\nConsidering the classification/auto-encoding applications of Indeterminate Probability Theory, there are also several points in the paper that need to be clarified/improved.\n\nEXPERIMENTS\n\nThe experimental section is also quite confusing, and lacks proper baselines to understand the real performances of the model. \n\n\n\n**Detailed comments**\n\nBelow I describe the main points of confusion in each section.\n\n_Abstract_\n\nYou write \"pushed this classification capability to infinity\" -> what does this mean?\n\n_Introduction_\n\nThe motivation for this work in the introduction is based on the IPNN, which is however a model the reader knows nothing about at this point in the paper.\n\n_Section 2.1_\n\nYou write \"VAE uses neural network as the approximate solution of decoder\"\n -> What does this mean? In a VAE the decoder is defined as a modelling choice, and the encoder is used to approximate the posterior probability.\n\n_Section 2.2_\n\nOverall this section is hard to understand, and needs a better example/toy problem to help the reader (you could focus on the classification task from Figure 1 for example). \n\nWhen you say \"introducing Observers and treating the outcome of each random experiment as indeterminate probability distribution,\"\n- What are Observers? They are no longer mentioned in the rest of the section\n- how do you define an \"indeterminate\" probability distribution?\n\nThese definitions are missing:\n- what does $m$ represent in $y_m$\n- what does $l$ represent in $y_l$\n- what does $t$ represent in $x_t$\n\n \n_Section 3_\n\nWhy do you need to introduce both Observer 2 and Observer 3? What is the difference? Observer 2 seems not to be relevant for the subsequent discussion.\nDue to the confusion in Section 2.2, I am not really sure what you are trying to achieve in this section, and how exactly this relates to the rest of the paper.\n\n\n\n_Section 4.2_\n\n- Why did you choose that specific distribution in the right-hand side of the KL divergence?\n\n_Section 5_\n\nYou refer to details in (Anonymous, 2024) in the footnote, but they are needed in this paper as well to understand it.\n\n\n_Section 6_\n\n\"In this section, we will focus on the training strategy of Gaussian distribution\" -> \nCan you clarify what this means?\n\n\n_Section 7.1_\n\n1. This section misses baselines for other classification models (even simple neural networks)? The classification performances of your model on MNIST look quite poor for example.\n1. In Table 3 you compare against \"Simple-Softmax\", which is not defined, and which performs significantly better than the proposed model \n1. The advantages of this model vs other architectures are not well described\n1. What's the scalability of this method? What are the training times?\n1. The dataset names are not even mentioned in the main text, so one needs to guess which dataset the authors are talking about while reading this section. Only captions in the Figures mention the dataset.\n1. In the paragraph \"Results of classification tasks on large latent spaces\" - are you talking about table 2? It is not mentioned\n\n\n_Section 7.2_\n\n1. The difference between CIPAE and VAE is not clear from the paper\n1. \"As shown in Figure 5, the results of\nauto-encoder tasks between CIPAE\nand VAE are similar, this result further verifies that CIPAE is the analytical solution.\" -> What does this mean? Why can you make this statement from looking at a Figure?\n\n_Conclusion_\n\n\"Although our proposed model is derived from indeterminate probability theory, we can see Determinate\nfrom the expectation form in Eq. (11). Finally, we\u2019d like to finish our paper with one sentence:\nThe world is determined with all Indeterminate!\" -> not sure what this means."
            },
            "questions": {
                "value": "See the questions in the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697905222794,
        "cdate": 1697905222794,
        "tmdate": 1699636086478,
        "mdate": 1699636086478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bgsw94EcPI",
        "forum": "Rt6btdXS2b",
        "replyto": "Rt6btdXS2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_Vwsb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_Vwsb"
        ],
        "content": {
            "summary": {
                "value": "The paper \u201cContinuous indeterminate\u2026\u201d proposes a continuous extension of the \u201cIndeterminate \u2026\u201d model by the same authors, correctly referenced as Anonymous. The paper describes this extension, accompanied by definitions of the classification and auto-encoder models, together with training, inference procedures and simple experiments. The resulting models are only a bit less accurate than well known models. The author's main goal is to theoretically describe and show the benefits from its use.\n\nIn my opinion, the paper may be accepted, provided the authors answer the above doubts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The model shown is interesting and shows, perhaps not very illuminating but still explainable to the input \u2014> latent \u2014> classification, and input \u2014> latent \u2014> reconstruction problems in theoretical way.\n2. There is a good introductory to the theory in section 3.\n3. The proposed model aims at providing more explainable solutions to classification, although there is some way before the model may accomplish that.\n4. The performed experiments prove, or at least show, the hypothesis clearly stated by the authors."
            },
            "weaknesses": {
                "value": "1. I guess the whole paper should start with a deeper explanation of differences between the proposed approach and a VAE model.\n2. An ablation study concerning the complexity is missing. The authors say that the C hyperparameter can be set to 1 \u201cas long as the batch size is high enough\u2026\u201d (page 7, bottom), but still they use C=2 in the experiments.\n3. The derivation for the continuous probability mathematical formulae is complex, and lacks intuition, instead giving intricate formulas and variables.\n4. Although the authors correctly reference to their own paper as written by Anonymous, but not yet published. The authors do it all through the paper referencing the reader to find details over there. The paper can easily be found by the title. On the other hand, this is unavoidable."
            },
            "questions": {
                "value": "1.  Add some introduction to differences between the proposed model and VAE-like approaches, or perhaps accompany the whole sequence should be accompanied by comparisons to corresponding steps in a VAE-type model?\n2. Equation (21), being the basis for training, needs a deeper explanation. Why use the max functions both in numerator and denominator? \n3. When comparing the proposed CIPAE with VAE, section 7.2, the visualizations of the latent space become somehow different, with parts of the R^2 latent for VAE empty. Does it come from different latent definition in both cases? Or is it just a result of showing only the [-20, 20]x[-20,20] square? This might not seem to be a fair comparison unless explained.\n4. From a practical point of view: what is the training and inference comparison between VAE (as well as other WAE, etc.) approach and the proposed ones?\n5.  What is the impact of C value and the batch size on quality, trading and inference time, etc.? Could you provide some ablation study?\n6. In conclusion the authors state, that the proposed model is actually composed of two parts: first detects attributes, and the second (i.e. classification?) is a probabilistic model which may be used for reasoning. A 1-D example shown in figure 3, that the authors refer to, shows that the first part performs a kind of clustering, is that so? Please elaborate on that, since it would greatly enlarge the readability of the paper.\n7. Several language errors should be corrected. E.g. a) on page 1 the sentence \u201cHowever, IPNN need to predefine the \u2026\u201d should probably be \u201cHowever, IPNN needs to be predefined\u2026\u201d; b) just above Eq. 1 instead \u201cbellow\u201d should be \u201cbelow\u201d; c) what is the word \u201ccomplexer\u201d at the bottom of page 3? Perhaps the authors meant to say \u201cmore complex? \u201cComplexer\u201d might be used in French. I would suggest checking the whole text with some native speaker. These errors are usually tiny, but disturb reading.\n8. Please, if possible, make the figures a bit larger, just to make them somehow more readable. This refers particularly to figures 1, 3, and perhaps 2 and 4 too.\n9. In section 7.1 you claim that the CIPNN tends to put 1, 4, 7, 9 MNIST numbers in one cluster \u2014 this is hardly visible in the figures. How is that model used for classification? Which inputs were used in each round? Could you elaborate on that a bit? \n10. Equations are sometimes complex (lots of variables and indices), e.g. Sequence from (9) to (14). Could you, please, make them easier to follow?\n11. Some small editing errors, e.g. subsection 7.2 title starts as an orphan."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Reviewer_Vwsb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670677861,
        "cdate": 1698670677861,
        "tmdate": 1699636086396,
        "mdate": 1699636086396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i7T3tmPgGS",
        "forum": "Rt6btdXS2b",
        "replyto": "Rt6btdXS2b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_EMUS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1578/Reviewer_EMUS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a general model called CIPNN - Continuous Indeterminate Probability Neural Network using a group of reference variables $z$."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper focuses on the explainability of probabilistic models and neural networks, which is an interesting and important topic."
            },
            "weaknesses": {
                "value": "1. I find the motivation of this paper unclear. I had difficulty following the progression from Section 2.2 to Section 3 and then to the CIPNN model.\n\n2. The indeterminate probability theory is not surprising to me, and I believe it can be easily derived via the definition of conditional probability. Equations (1) and (2) hold for any $z$, but it's not clear to me which specific type of $z$ we are expecting in the learning process.\n\n3. I find Proposition 1 to be weak from my perspective. Specifically, Proposition 1 states, ''If $P(y_l | z^1, ... , z^N) \\to \\infty$, CIPNN converges to the global minimum.'' This is equivalent to saying that successful classification depends on our ability to learn a set of favorable variables, namely $z^1, ... , z^N$. However, the main challenge lies in determining the existence of these 'good' variables and how we can identify and obtain such $z^1, ... , z^N$ with theoretical guarantees.\n\n4. I did not find the comparison with existing approaches. Also, the numerical results did not indicates the improved performance is indeed from the introduction of $z^1, ... , z^N$."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1578/Reviewer_EMUS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776160023,
        "cdate": 1698776160023,
        "tmdate": 1699636086327,
        "mdate": 1699636086327,
        "license": "CC BY 4.0",
        "version": 2
    }
]