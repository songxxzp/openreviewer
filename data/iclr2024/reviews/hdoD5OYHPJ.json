[
    {
        "id": "rkPBKhrgTR",
        "forum": "hdoD5OYHPJ",
        "replyto": "hdoD5OYHPJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_rfek"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_rfek"
        ],
        "content": {
            "summary": {
                "value": "This paper AutoCLIP, a method to adapt zero-shot classification inference in CLIP-like vision language models. As in CLIP, zero-shot classification is achieved by using text templates with class names to get the language embedding. Templates are predefined and can have multiple variants for the same class. When there are multiple templates for a class, CLIP averages them and then compare it with the image embedding. The authors argue that some templates can more accurately describe the images while some are not that good. So simply averaging them can be sub-optimal. To address this issue, AutoCLIP computes weighted average of the templates rather than simple average. Larger weights are assigned to templates that are more similar to the image embeddings. To this end, AutoCLIP computes logsumexp on the similarity s. Then, gradient g is computed from it wrt p in ascent direction, where weights w=softmax(p). p is then updated by the gradient and a step size. In experiments, this update is only conducted once as there is no advantage of more iterations. AutoCLIP is only used for the zero-shot classification inference and only for the procedure of computing language and image similarities, so there is no additional training beyond CLIP. AutoCLIP is verified by zero-shot classification on multiple image classification datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea and motivation of AutoCLIP make sense. It is a reasonable way to improve the similarity calculation during zero-shot classification.\n2. AutoCLIP does not require additional training beyond the vision and language model. Therefore, it is easy to apply AutoCLIP to existing zero-shot classifiers for improving accuracy.\n3. The experiments are thoroughly conducted covering many classification datasets and ranging from CLIP to CoCa.\n4. Experimental results suggest AutoCLIP is an effective approach to improve zero-shot classification accuracy. The improvements are about 1%; in most cases less than 1%.\n5. The paper presentation is clear for audience."
            },
            "weaknesses": {
                "value": "1. The is one limitation: Current experiments suggest AutoCLIP can only be used for zero-shot classification, which is only one useful aspect of large vision and language model. Large vision and language model like CLIP is not about just doing zero-shot classification. For real applications, the impact of these models also lies in downstream tasks. For example, finetuning from CLIP pretrained parameters or using CLIP to directly assist the downstream tasks. It's better to explain some zero-shot classification applications.\n2. In view of the above limitations, another concern comes out about the result. If a model has 1% improvement on ImageNet, it may bring more improvements when the pre-trained model is applied in downstream task. In case AutoCLIP is not intended for downstream task, the 1% improvement may look not significant. What this 1% improvement can bring?\n3. The experiment in fig.3 is not clear. What does this experiment verify for? Any explain why ViT-L is worse? Does it mean AutoCLIP cannot well be scaled to larger models?\n4. It is highly suggested to show evidence for \"This is inspired by the observation that class descriptors having higher similarity in the embedding space describe the image better\". This claim support the rationale of the proposed AutoCLIP. To my understanding, if this claim does not hold, then AutoCLIP does not hold."
            },
            "questions": {
                "value": "Is there any speed comparison? How much more inference time does the AutoCLIP need?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Reviewer_rfek"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714687820,
        "cdate": 1698714687820,
        "tmdate": 1699636467716,
        "mdate": 1699636467716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kf97qisdZJ",
        "forum": "hdoD5OYHPJ",
        "replyto": "hdoD5OYHPJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_5SeE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_5SeE"
        ],
        "content": {
            "summary": {
                "value": "Visual language models such as CLIP are used for zero-shot image classification tasks. This is done by first extracting N text features for a class using N different text prompts, and the cosine similarity between the image feature and the averaged text feature of a class is computed. However, prior works simply averaged over N text features without considering the importance of each text feature. Unlike previous works, this paper proposed to perform a weighted average on the text features during test time by using the cosine similarity between each text feature and the image feature (i.e., higher cosine similarity, higher weight). Extensive experiments and ablations show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposed a simple and effective method for increasing the model performance. \n2. Extensive experiments on multiple datasets are conducted and highlight the performance of the proposed method."
            },
            "weaknesses": {
                "value": "1. The main concern of this work is its novelty. The idea of weighing the text features from different prompts has been proposed [a]. It is unclear why is the proposed method different than [a].\n2. While adopting the weighting strategy in [a] during test time is a solution for boosting the classification performance, one could simply average the topK text prompts that have the topK cosine similarity with the image feature. Is the proposed method better? \n3. The author is suggested to put the argument explanation of scipy.optimize.bisect to the appendix\n4. The x-axis of Figure 2 is not clear. Why are there so many lines on the x-axis? Are they meaningful?\n5. Typo: from CLIP (Radford et al., 2021) \u201cwe use the\u201d ResNet-50 (RN50) (He et al., 2015), and vision transformers\n6. Unclear: \u201cfrom \u2206 = 0.06 for K = 4 over \u2206 = 0.33 for K = 10 and \u2206 = 0.49 for K = 50 to \u2206 = 0.57 for K = 200\u201d\n7. Caption of Figure 5 : Shown is Accuracy \u2026.\n\n[a] A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models, ICML2023"
            },
            "questions": {
                "value": "1. The author is suggested to explain the fundamental differences between this work and [a].\n2. The author is suggested to show that the proposed method is better than averaging the topK text prompts that have the topK cosine similarity. When K=1, this is equivalent to ``max\u201d aggregation and when K=80 (e.g., for CLIP), which means all the text prompts are averaged. Please ablate the performance for different K. \n\n[a] A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models, ICML2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4842/Reviewer_5SeE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817154440,
        "cdate": 1698817154440,
        "tmdate": 1699636467624,
        "mdate": 1699636467624,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TzolwHZTPD",
        "forum": "hdoD5OYHPJ",
        "replyto": "hdoD5OYHPJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_2EvZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4842/Reviewer_2EvZ"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a method called AUTOCLIP that constructs zero-shot classifiers from vision-language models based on considering the statistics of class descriptor-image similarities and automatically adjusting the weights for the templates from prompts. In addition, they discuss how to automatically tune the proposed method's step size that can control the entropy of the prompt template\u2019s weights. The authors evaluate the proposed method AUTOCLIP on several datasets and vision-language models with prompt templates and show the promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has good writing and easy to understand and follow the proposed idea. The motivation is reasonable by leveraging the knowledge of visual-language models (VLMs) and automatically tuning the per-image weights to each prompt template at inference time. In addition, they also discuss automatically tuning AUTOCLIP\u2019s step size to control the entropy of the prompt template\u2019s weights. Overall the proposed method is simple with only modifying a few steps in a general zero-shot classifier algorithm. The behind the intuition is also clear and provides significant progress on the zero-shot image classification."
            },
            "weaknesses": {
                "value": "Even though the proposed method is simple and effective, the method looks like a naive modification method to replace the uniform-weighted average descriptor encodings with weighted average encodings based on the existing algorithms which may limit the paper's novelty."
            },
            "questions": {
                "value": "1. I'd like to know if the order of the queries is different, will the performance remain the same? If each query has different classes in the inference time, will the weighted average encodings need to be changed/calculated all the time?\n2. Similarly, will the proposed method still have the same speed in the inference process or it will be dramatically different? \n3. Since the authors claim that the proposed method has the benefit of significantly lowers the test-time computation and memory overhead, it'll be more convincing if adding the comparison with tables, and figures, etc. \n4. Is there more discussions about why 'mean' in Figure 5 on Oxford dataset has the worst result?\n5. Minor: with the logsumexp providing the best result compared to entropy, mean, and max, is the idea similar to focal loss with multi-class?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698985350984,
        "cdate": 1698985350984,
        "tmdate": 1699636467551,
        "mdate": 1699636467551,
        "license": "CC BY 4.0",
        "version": 2
    }
]