[
    {
        "id": "3UmSZ4IwHn",
        "forum": "kvcbV8KQsi",
        "replyto": "kvcbV8KQsi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_uBkt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_uBkt"
        ],
        "content": {
            "summary": {
                "value": "The paper leverages the transformers circuits work (Elhage '21) to uncover successor heads, which are attention heads in a transformer responsible for incrementing words / numbers which have a natural ordering. To do so, the authors build an Output-Value circuit for these successor heads, which unlike (Elhage '21), leverages a non-linear transformation of the word embeddings. The authors show that \n\n1. successor heads occur in many decoder-only transformer architectures, and across different model scales, and different types of incrementations\n2. Through several analyses and ablations (the use of linear probing on the MLP0 features, as well as the use of sparse autoencoder to extract important features for successor heads), the authors argue that the model's internal representaiton for numerical / incremental tasks is in modulo 10, i.e. that successor heads, when given input $f_{i}$, will increase the logits of $f_{i+1 \\  \\text{mod} \\  10}$. \n3. The authors show that these successor representations are amenable to vector arithmetic operations, and that while the MLP$_0$ representations are not biased towards incrementation, the OV circuit for successor head is. \n4. The authors argue that successor heads enable the identification of polysemantic behavior, finding evidence that they are responsible for both acronym prediction and incrementation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The empirical rigor of this work is high. The authors provide several ablations to argue the existence of mod_$10$ features in transformers. They moreover provide detailed additional information for experiments in the appendix.\n2. The finding of successor heads is interesting and provides a good framework for understanding how transformers reason about incrementation. \n3. The connection between incrementation and acronym prediction observed in successor heads is interesting."
            },
            "weaknesses": {
                "value": "1. The paper lacks a proper background section. Terms like OV matrix are not introduced, and more generally, the notion of circuits, or what un enembedding is, are never properly defined. This makes the paper very hard to digest without being familiar with the concepts of transformer circuits (Elhage '21). Given that the paper is already quite dense, you can e.g. move figure 7 to the appendix, and properly lay the appropriate terminology to understand this work."
            },
            "questions": {
                "value": "1. Effective OV circuit : what is meant by effective here ?\n2. What was the motivation for using MLP$_0$ rather than the original word embeddings ? Given that MLP$_0$ occurs after a first self-attention layer, I would have not expected it to map embeddings to a representation amenable for the analysis presented in the paper. \n\nOn section 4 : \n3. successor heads in the wild. I am not sure I understand why the authors distinguish between features that, when ablated, make the correct prediction less likely, and features that, when ablated, increase loss the most. I understand that these two are not exactly the same thing, but I don't understand what we gain by treating them differently. \n4. It is mentioned that 128 samples are sampled from the Pile dataset. Did you happen to find a token where the successor head was important for each of the 128 samples ? Or did you bias your sampling towards extracts where the successor head is important ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Reviewer_uBkt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3486/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764890697,
        "cdate": 1698764890697,
        "tmdate": 1700498868269,
        "mdate": 1700498868269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0rihrnLYVc",
        "forum": "kvcbV8KQsi",
        "replyto": "kvcbV8KQsi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_ExhM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_ExhM"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates findings of a set of attention heads called successor heads that perform incrementation on tokens from ordinal sequences. It also shows evidence, in MLP0 layers, for \"mod-10\" features which are present in tokens belonging to the same numerical index. Experiments were performed to modify numeric inputs using vector arithmetic with the mod-10 features. Finally, the paper analyzes the polysemanticity of successor heads on natural language data samples. The paper's authors claim that these findings across several models of various scales demonstrate a weak form of universality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Strengths:\n1. The paper is clear and easy to read.\n\n2. To show claims of a weak form of universality, the paper thoroughly tests for successor scores across several models for various numbers of parameters.\n\n3. The experiments to find and verify the mod-10 features are also thoroughly performed. These features were confirmed by several different methods: first by training a sparse autoencoder using reconstruction loss on the MLP0 outputs, and then by further comparisons to linear probing and ablation to reinforce these observations. This is an interesting result uniting various tokens under mod-10 classes.\n\n4. There are also interesting results with the natural language experiments that demonstrate interpretable polysemanticity."
            },
            "weaknesses": {
                "value": "Weaknesses:\n1. In Section 4, direct effect mean ablation is used to show \"that when the successor head is contributing usefully, the prompts often required some kind of incrementation\". Then in Appendix J, direct effect mean ablation is used again to show that the successor head is the most important head across 64 prompts. Though this is stated with some evidence, not enough quantifiable evidence is shown here to justify the reach of these claims, such as the statement of \"mostly solved\". More analysis can also be shown about the \"direct effect\" to separate it from \"indirect effects\".\n\nThe paper also did not clarify the details of the ablation, such as if it used resampling ablation (there may be issues if it used mean ablation from the same dataset, as there are known issues with mean ablation [1]), and/or path patching (to obtain direct effects).\n\n[1] https://www.lesswrong.com/posts/kcZZAsEjwrbczxN2i/causal-scrubbing-appendix#3_Further_discussion_of_zero_and_mean_ablation\n\n2. The paper mentions that vector addition was performed successfully for 89% of the cases for digits 20-29, and mentions how it was performed on token '35'. It does not mention how this performed for other digits. This is likewise the case for number words only showing ten to twenty. Presumably, the performance is similar, but the paper should explicitly mention this to avoid criticism of cherry-picking.\n\nIn Figure 7, it's also unclear what \"target residues modulo 10\" means when referring to the column headers. Presumably, this is stating something similar to how the vector arithmetic on MLP0(E('fifth') makes it \"behave more like MLP0(E('seventh')\". The wording can be made clearer to avoid confusion that it means the number \"7\" rather than the word \"seventh\". Additionally, the checkmarks are given when \"the max logits are on the successor token\". This is an interesting result, but how big is the logit difference between logits for the successor token and other tokens? \n\nAppendix D states that scaling was used on the additive feature terms. A quick explanation of why a particular scaling factor was used would be helpful.\n\n3. The paper states: \"to the best of our knowledge the presence of both successorship and acronym behavior in head L12H0 is the cleanest example of polysemantic behavior identified so far in an LLM.\" Why is this the cleanest example of polysemantic behavior, compared to other studies on the topic such as in [2]? Similarly for this statement, \"which to the best of our knowledge are the most closely studied components in LLMs that occur in both small and large models\", what other components are you comparing to that are not as closely studied?\n\n[2] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse probing, 2023\n\n4. This paper discovers novel and interesting observations, but it does not elaborate much on why this observation is impactful enough."
            },
            "questions": {
                "value": "Questions:\n1. While using MLP0 with successor heads, in isolation, was shown to be sufficient to perform incrementation, it is not shown how they interact with the rest of the models they belong to that the paper studied. How is this information about numerical features processed in later layers, from MLP0 to the successor head L12H0, then to the rest of the model? How does this end-to-end path interact with alternative paths, and inhibitory ones?\n\nThe mod-10 features are obtained from MLP0's outputs, to study how the end-to-end path of MLP0 to the successor head processes these features. However, the paper does not show how this information is processed through other layers and MLPs in the models. Can such mod-10 features be found in other layers?\n\n2. To continue on weak point #2:\nThere are also the cases of adding non-mod-10 features to numeric tokens, and adding the features to non-numeric tokens. There may be cases of \"confounding\" factors where U(OV(MLP0(E('twelve')) - f_2 + f_4)) has high logits for 'fourteen', but is it possible that adding non-numeric features will also shift twelve to thirteen, fourteen, etc.? If so, then perhaps these mod-10 features are just obtaining the correct change because of how the numeric features are scaled with one another? In other words, would subtracting non-numeric features R then adding RR obtain the same result? This is not a weak point of the paper as it may be beyond its scope, and this situation seems unlikely to be the case, but it can be further investigated for thoroughness. \n\n3. To continue on weak point #3:\nIn terms of interpretable polysemanticity, how do other heads compare to successor heads? To ensure acronym handling is not commonly done for many attention heads, how well do other heads handle acronyms?\n\n\n- Other comments:\n\nThis paper tackles a similar topic as a previous project from earlier this year, which also found a successor attention head by performing OV circuit analysis on inputs of numbers, number words, days, months, and letters. It also was stated to have inspected the effects of vector addition on number tokens [3]. Could the authors elaborate on the similarities and differences? \n\n[3] https://alignmentjam.com/project/one-is-1-analyzing-activations-of-numerical-words-vs-digits"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Reviewer_ExhM",
                    "ICLR.cc/2024/Conference/Submission3486/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3486/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784599928,
        "cdate": 1698784599928,
        "tmdate": 1700674035853,
        "mdate": 1700674035853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6SwWiTBpXv",
        "forum": "kvcbV8KQsi",
        "replyto": "kvcbV8KQsi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_Vpua"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_Vpua"
        ],
        "content": {
            "summary": {
                "value": "This paper discovers and analyzes Successor Heads, a type of attention heads in transformer language models that increases the probability of the next token in a sequence such as 1 -> 2, or Monday -> Tuesday. This is discovered mainly through ablations on a specific dataset crafted for this task, and analyzed through various means."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Good informative figures such as Fig 1 and Fig 7, clear writing. The use of OV circuits in the discovery and analysis seems smart and somewhat novel to standard methodology for these kinds of findings. Interesting behavior and good multi-pronged analysis of it."
            },
            "weaknesses": {
                "value": "Somewhat overclaiming the contribution:\nFor example abstract says:  \"Existing research in this area has found interpretable language model components in small toy models. However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models.\" This makes it sound like existing work has only studied toy models which is not true, while also making it sound like this work would study frontier models which is not the case. While they look at larger models than most related work, the wording makes it sound like difference is larger than it is.\n\nAlso the findings about mod 10 features are almost entirely based on the setting of incremental numbers which makes sense, while the writing makes it sound like they are behind successor head behavior on all tasks. The only evidence of these being used on other task is a low success percentage on changing output month with vector arithmetic. I would expect for tasks like months and days there would be other mod-12 or mod-7 features for example that could explain this behavior, was this studied?"
            },
            "questions": {
                "value": "How was the set of succession datasets chosen? Did you experiment with other tasks that were eventually not included? It would be interesting to measure successor behavior on some held out succession task, as currently behavior on all the tasks was used to find successor heads."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3486/Reviewer_Vpua"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3486/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698867239915,
        "cdate": 1698867239915,
        "tmdate": 1700681172635,
        "mdate": 1700681172635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XpaDH0136J",
        "forum": "kvcbV8KQsi",
        "replyto": "kvcbV8KQsi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_uo7e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3486/Reviewer_uo7e"
        ],
        "content": {
            "summary": {
                "value": "This paper digs inside the mechanism of attention heads in LLMs and discovers some particular attention heads are able to fire for predicting naturally-ordered tokens, which is termed as successor heads. This paper belongs to a recent line of work, the mechanical interpretability of transformer models. The findings of successor heads appear to be common for different prompts and also across models, showing some level of polysemanticity in the successor attention heads\u2019 activation space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The findings presented in this paper are significantly novel. Authors have clearly described the functions of successor heads and designed multiple experiments to validate their hypothesis. I especially appreciate Section 3.3 where the evidence in arithmetic is a strong proof that the activation of success attention indeed captures the natural ordering of words and is responsible for the LLM\u2019s reasoning."
            },
            "weaknesses": {
                "value": "There are a few issues mostly in the presentation of the work. \n\nI am getting really annoyed when the authors place all definitions, i.e. the Glossary section, at the end of the appendix. It is really inconvenient for the reader to go back and forth during the reading. It has to have better ways to present the definitions in the context. Please do not do this. \n\nIt lacks sufficient descriptions for the reader to understand the process that parses the original output of the attending heads to the sparse encoder\u2019s output. I understand that this is to make more room to present the findings; however, it makes the methodology part pretty unclear from reading the current version. I have to go back and forth and spend a lot more time on Section 2 and 3 to make sure I understand the way each figure is plotted."
            },
            "questions": {
                "value": "How does the choice of n in top-n-attended tokens affect the findings? Authors only pick a particular k (i.e. k=5 or k=1) and present the result. Can you demonstrate the full story about polysemantics when you relax the constraint of importance by choosing large n?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3486/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699121598913,
        "cdate": 1699121598913,
        "tmdate": 1699636301601,
        "mdate": 1699636301601,
        "license": "CC BY 4.0",
        "version": 2
    }
]