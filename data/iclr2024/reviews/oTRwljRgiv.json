[
    {
        "id": "ClBKKnbao4",
        "forum": "oTRwljRgiv",
        "replyto": "oTRwljRgiv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_EB19"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_EB19"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a set of generalization tasks to measure the ability of a programming by example program synthesis engine to generalize to be able to generate out-of-distribution programs. These tasks measure the ability of the synthesis engine to generalize to programs of different lengths and with different operations in different orderings than observed in the training data. The paper then proposes ExeDec, an iterative synthesis approach in which one model proposes a subgoal, a different model proposes a program to each that subgoal, and the process repeats until the composed program results in the expected output. The paper evaluates ExeDec instantiated with Transformers trained for each generalization task and with LLMs and find that ExeDec improves generalization compared to baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed set of generalization tasks are interesting, important, and well described\n* The proposed ExeDec algorithm is also interesting: in particular, it is a general algorithm sketch that can be instantiated either by Transformers trained for this algorithm (Sections 4.2/5.1) or LLMs (Section 5.2)\n* The results, taken at face value (though see the question about FLOPs below) are strong. In particular, they demonstrate that the core idea of predicting the next output improves generalization."
            },
            "weaknesses": {
                "value": "* The benchmark introduced in Sections 2 and 3 is claimed as a novel contribution (\"we introduce a new meta-benchmark for measuring the compositional generalization abilities of program synthesizers.\"). However, it is very hard to tell from these descriptions the actual contents and novelty of the benchmark. I was ultimately able to get a better understanding from reading the evaluation and Appendix B, but in isolation the clarity of these sections are very low.\n* The implications of the loop in Algorithm 1 are not sufficiently discussed:\n  * What happens if the loop never terminates?\n  * What is the comparison in FLOPs between the ExeDec models and the baseline models (including the beam search, etc)? Even if the parameter count is the same, the loop in ExeDec could give these models significantly more power than the Transformer / Latent Programmer baselines."
            },
            "questions": {
                "value": "* What happens if the loop in Algorithm 1 never terminates?\n* My reading of the results in 5.2 (specifically, Table 1) is that I should divide all numbers by 200 to compare them with the results in 5.1 (Figure 2). Is this correct?\n* What is the comparison in FLOPs between the ExeDec models and the baseline models?\n* How do models perform on generalization tasks that they were not trained for?\n\nMy score is conditioned on the authors improving the description of the baseline in Sections 2 and 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698338036972,
        "cdate": 1698338036972,
        "tmdate": 1699636752074,
        "mdate": 1699636752074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GlBJga64Pg",
        "forum": "oTRwljRgiv",
        "replyto": "oTRwljRgiv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_iTFM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_iTFM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for programming by examples where the goal is to generate a program for given input-output examples. The proposed method first predicts subgoals (which are simply intermediate values during the program execution) and then generates a (sub)program that achieves those subgoals. This is repeated until the subgoals correspond to the target outputs. Additionally, the paper suggests different benchmarks for compositional generalization for programming by examples (e.g., generalizing to longer programs). The proposed method is shown to have better generalization performance than the baselines in both the training from scratch and the pre-trained setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method for programming mirrors how humans tend to come up with programs for complicated programming tasks: divide them into smaller subproblems and address them individually. In that sense, the method is simple and intuitive, hence I'd expect something similar to work well even in more realistic settings.\n\nThe benchmarks provide valuable insights into the settings under which we would expect a decomposition into subprograms to be important. \n\nIt is interesting to see that the proposed method can also be adapted to work for pre-trained LLMs in the few-shot setting (i.e., without fine-tuning)."
            },
            "weaknesses": {
                "value": "The main limitations are that the method requires supervision for the sub-goals and that the experiments are limited to synthetic programs, as acknowledged by the authors in the limitations section. I can see that an unsupervised approach to predicting subgoals constitutes its own paper, but I think it'd still be interesting to see how the method fares with different decompositions. For example, picking $n$ lines per subprogram, or randomly grouping multiple lines into a subprogram.\n\nThe experiments cover both the training from scratch scenario and a pre-trained LLM. The middle ground of finetuning a pre-trained LLM would be interesting to see. Here the same LLM could be used for the different models via different prompts (similar to the few-shot setting) and trained like the from-scratch setting. Since the benchmark intends to measure generalization and considering the importance of pre-training on the generalization performance, I believe this is an important additional experiment.\n\nExeDec requires subgoals which are essentially execution traces as supervision. It'd be interesting to see how the Transformer baseline performs when it also has access to them (for example by inserting the variable values after/before each program line). This baseline would further clarify the importance of training with intermediate program states versus decomposing the program into parts that are individually generated."
            },
            "questions": {
                "value": "Do you have an insight into why the no-subgoal ablation performs better on some generalization benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779955936,
        "cdate": 1698779955936,
        "tmdate": 1699636751936,
        "mdate": 1699636751936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1tXs5T6OLV",
        "forum": "oTRwljRgiv",
        "replyto": "oTRwljRgiv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_hCQX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_hCQX"
        ],
        "content": {
            "summary": {
                "value": "Summary:\n- This work formalizes several useful notions of generalization, and describes how these can be used to create train-test splits of datasets testing each form of generalization. The classic neural synthesis datasets DeepCoder and RobustFill  are divided up in this way and used for evaluation throughout the paper.\n- The authors then presents a novel architecture called ExeDec for synthesizing sequential programs, where a prefix of a partially constructed program can be executed to get a state-so-far. While prior work conditions next-subprogram generation on the state-so-far and overall output spec, here the authors introduce a subgoal-generating module that produces an output spec for just the next subprogram, then conditions next-subprogram generation on that spec (though regardless of whether the subgoal spec is met, the sampled subprogram is accepted).\n- While performing relatively similar to baselines/ablations on the original datasets, ExeDec performs far better than alternatives at the generalization-focused modified datasets. Additionally, in another set of experiments the authors prompt an LLM to perform ExeDec-style reasoning (predicting outputs of the next line before writing it) and find it boosts performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Formalizing different notions of generalization and suggesting actionable ways of designing benchmarks around this is much-needed work in program synthesis. This is something that I come back to again and again in my own thinking \u2013 the scarcity of generalization-centric benchmarks and systematic generalization evaluations, especially beyond just length generalization.\n\n- Evaluations look quite good \u2013 ExeDec Performance is far better than the from-scratch transformer and latent programmer baselines, and barely diminishes when using the smaller version of the model. \n\n- The improvements over the No-Subgoal ablation (which is similar to prior work as mentioned in 4.3) are fairly significant (eg 80% -> 87%) and are mainly in the Compose New Operation and Compose Different Concept categories. These aren't extremely strong results but they're enough to justify the relatively straightforward general setup, in my view.\n\n- Showing off and evaluating an analogous algorithm for LLM-based synthesis was a nice touch and shows the generality of the idea. This idea of breaking problems into subgoals is a good one, and I think one that many have thought about but nobody has done well \u2013 this work feels like a nice step towards doing it. As discussed in their limitations, something more hierarchical might better capture how programmers often break down problems, however just predicting the results of the next line is a reasonable first step in this direction and seems to empirically be helpful.\n    - In fact, before reading this I actually would not have expected predicting the output of just the next subprogram to be that helpful compared to just predicting the subprogram directly (whereas I would expect predicting outputs to more general hierarchical multi-subprogram subproblems to be helpful) so I appreciate that this paper finds and highlights where even just this single step prediction can be beneficial."
            },
            "weaknesses": {
                "value": "- In the intro there's a mention of how compositional generalization \"has not previously been studied in the context of programming by example\" and later in the related work a mention of how there's \"less work on systematic generalization for machine learning for code, although Bieber et al. (2020) studies length generalization\"\n    - While both systematic generalization and specifically compositional generalization are understudied in program synthesis, there are a few other citations that apply to each of these places.\n    - First, the DeepCoder paper (Balog 2017) actually evaluates on length generalization (section 5.2 of that paper).\n    - Nye et al 2021 (\"Representing Partial Programs with Blended Abstract Semantics\") has some compositional generalization evaluation as well, such as the excerpt from section 5.1 \"Tower objects seen during training were composed in previously unseen ways...\"\n    - Ellis et al 2019 Program Synthesis in a REPL (already cited here) also applies for length generalization, eg the line from 4.2 \"Although it was trained on programs whose maximum length was 30 actions and average length approximately 8 actions, during test time we regularly achieved programs with 40 actions or more\"\n    - Just adding those citations in the appropriate places would strengthen this. To be clear, ExeDec's formalization of generalization and range of types of generalization go far beyond any of this prior work, but it's important to acknowledge that there have been some explorations of some of these kinds of generalization in the past.\n    - Aside from this point, I think the related work citations are quite thorough and well done\n\n- How often are proposed subgoals actually achieved? Are they achieved more along a successful synthesis path than an unsuccessful one, on average? Some quantitative insights into these sorts of questions would strengthen the paper.\n\n- The results are decent but not extremely strong \u2013 in my opinion they're good enough but of course stronger results would help.\n\n- The idea mentioning in the Limitations section of how subgoals that are not just line-by-line but which could instead correspond to more than one line is one that immediately came to mind. Of course, fully generally doing that kind of generic hierarchical decomposition of a problem into subproblems is something of a holy grail in synthesis, and I think that this fairly limited form of subproblem is a reasonable step, so this is not a huge weakness."
            },
            "questions": {
                "value": "- In Figure 2, why do you think that ExeDec improves over the No-Subgoal ablation primarily in the Compose New Operation and Compose Different Concepts settings, and not in the other settings? Some added discussion of this would be helpful.\n\n- \"Details omitted for anonymity. This LLM is one of the largest available through an API.\" I don't think anonymizing the LLM if it's available though a public API (eg chatgpt, bard, claude, codex, gpt4, etc) is necessary, and none of the other papers I'm reviewing do this. It'd be helpful to know which LLM this is (unless it's a private API and would actually break anonymity of the authors) in case it raises any other questions/discussion from reviewers.\n\n- I'm a little surprised by how poorly the LLMs do in Table 1. Do you have a sense for the usual failure modes \u2013 does it produce code that's valid in terms of the DSL, but isnt correct? Or does it fail to adhere to the DSL? (as an aside, in the non-pythonic setup, if the LLM attempts to write normal python code instead of using the `dsl` library, is this treated as invalid and do you have a sense for how often this happens?)\n\n- To clarify, \"test on training distribution\" in Fig 2 is referring to a heldout test set of problems from the same distribution as the training set, and not testing directly on any of the problems that the models were trained on, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6600/Reviewer_hCQX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698972003316,
        "cdate": 1698972003316,
        "tmdate": 1699636751816,
        "mdate": 1699636751816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bg6wfp7B3S",
        "forum": "oTRwljRgiv",
        "replyto": "oTRwljRgiv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_2WRW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6600/Reviewer_2WRW"
        ],
        "content": {
            "summary": {
                "value": "This paper improves the composition generalization of neural program synthesis by introducing execution decomposition. The general idea is to interleave sub-goal predictions with program generation. To evaluate this idea, the authors design five compositional generalization tasks based on two popular datasets used by RobustFill and DeepCoder. All specifications including sub-goals are assumed to be I/O-example based, where the inputs of sub-goals are intermediate program states. The outputs of sub-goals are somewhat complicated depending on the application domain. The evaluation shows that ExeDec outperforms many other baselines that do not (explicitly) predict sub-goals, and LLMs struggle to solve the proposed tasks requiring compositional generalization even when ExeDec-style prompts are provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Having compositional generalization as an inductive bias in the synthesis algorithm is novel and sounds quite appealing. Compositional generalization should be a key aspect in order to achieve scalable program synthesis. \n- Five different types of compositional generalization tasks are designed and relevant benchmarks are created based on two popular synthesis domains, RobutFill and DeepCoder. \n- The evaluation consists of multiple baselines as well as LLMs, and ExeDec outperforms all of them on the proposed dataset\n- The problem is well-motivated and the related work is discussed in an insightful and thorough manner"
            },
            "weaknesses": {
                "value": "- The two datasets are small domain-specific tasks, thus the evaluation does not really show the promise of scalability, which is the original motivation.\n- I/O-based specifications are not easy to construct -- both intermediate states and output states may vary significantly from the initial given I/O examples. For instance, intermediate states may involve auxiliary temporary variables, and the output states may have to be adjusted according to the actual application domains. \n- Both DSLs seem simple straight-line code, and there are no recursion, loops, or if-else conditions, all of which are important components to construct a realistic large program.\n- Creating a good training dataset for decomposition tasks can be very challenging. Particularly, there are many valid solutions, which may have quite different intermediate sub-goals. Supervising only one specific solution would be problematic."
            },
            "questions": {
                "value": "How to process different numbers of I/O examples (the size of which may also vary) in transformer models?\n\nStraight-line code can be split arbitrarily, how to decide which snippets form a proper sub-goal?\n\nIt is a bit surprising that No-subgoal Ablation performs pretty well. Shouldn't it behave like the Transformer baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024598140,
        "cdate": 1699024598140,
        "tmdate": 1699636751528,
        "mdate": 1699636751528,
        "license": "CC BY 4.0",
        "version": 2
    }
]