[
    {
        "id": "uPrJaqnU45",
        "forum": "RofU5v2BvZ",
        "replyto": "RofU5v2BvZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_fw9C"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the use of LLMs to facilitate human-AI coordination more intuitively. Specifically, the paper considers the possibility of using the LLMs as a way to set up a convention of how the task will be carried out. In the context of the paper, the convention takes the form of a task-level plan which lays out the different roles and actions to be carried out by each participant. To allow their method to support more complex scenarios, they use an approach to decompose the overall problem into sub-problems and then solve each one sequentially. The method is evaluated both using human proxies and with real human participants. The method is compared against multiple learning-based baselines.  Additionally, they performed an ablation study on their approach, showed the effectiveness of their method on other reasoning benchmarks, and showed how their method can achieve higher levels of human-AI value alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think the paper tackles an important problem, namely coordination in the MARL setting. The paper proposes an interesting approach to using LLMs in this context, i.e., coming up with a coordination plan. I also appreciate the fact that the authors took the trouble to run actual user studies, which are very important in validating such systems. Finally, I also like the fact that the authors acknowledge potential limitations the LLM-based reasoning systems might have with respect to reasoning and propose a method to address it."
            },
            "weaknesses": {
                "value": "In terms of the weaknesses, I had issues with clarity formulation \u2013 multiple terms were loosely used and not clearly formalized. There is some important information missing about the user studies. Next, there seem to be certain assumptions being made about the work whose implications are never clearly spelled out, and finally, I had some questions about the comparison between the current methods and the RL baselines. Below, I have provided a more detailed discussion of the specific concerns I had.\n\n1. Clarity Issues: First off, there are multiple terms and concepts being referred to or introduced in the paper that are never fully formalized. Two specific ones that confused me were user preference and sequencing strategy. The paper talks about inputting human preferences in multiple places, and some instances of it are included in the example interactions. However, at no point is it made clear what exactly is human-preferences with respect to the underlying game formulations. Are these constraints over policies/total utility received by the user? Is this information already contained in the human reward function but just made more explicit in the input to the LLM? Secondly, the paper talks about problem decomposition and sequencing and how the one used in the paper is a general form that can be used in any task (Limitation section). But apart from some examples given in the figures, there is no exact formalization of this process. How did the authors prove that this decomposition is applicable to all possible extended-form Markov games? What happens when there is concurrency required between different tasks? Can each task be refined in isolation? How does stochasticity affect this process, and how do you account for task failure?\n\n2. User Studies - While I really think it is great that the authors chose to do user studies, I have some concerns about the current user study. First off, I don\u2019t see any mention of the user study being approved or granted exemption by an institutional review board. Similarly, there is no mention of the demographics and background of the participants, how they were recruited, and what the compensation was. Also, since there were only 12 participants and 16 conditions (method*setting), how were the studies counterbalanced? Finally, in the LLM setting, creation and validation of convention introduce new steps. Were any subjective evaluations performed on how much they liked the system? Also, the authors should have measured the cognitive load placed on the participants. \n\n3. Assumptions: Two assumptions that are central to the success of the task are the ability to find high-level actions that can be executed by the low-level controller robustly and also make sense to the user. Here, both components are equally important because a high-level plan is shown to the user that is specified in terms of these high-level actions. If the human cannot make sense of or exactly predict how the agent will carry out these actions, the entire convention process will fall apart. While systems like SayCan make the former assumption (high-level skills), I believe the latter assumption is not crucial to the success of that system. Additionally, the method also expects access to a high-level abstraction of human actions (which is symmetric in Overcooked but may not be so in the most general case). Finally, there seems to be an additional assumption that even though the actions are abstract, any refinement of high-level conventions to low-level actions will be free of conflicts. This is usually hard to guarantee for more general coordination settings.\n\n4. RL Action Space: Finally, it seems that the RL agent is learning policies and directly acting in the low-level action space. How does this allow for a fair comparison? Especially since the skills were learned from human demonstration, wouldn\u2019t it naturally align with how humans would act? Also, did the same 12 participants provide the data from which the skills were learned?"
            },
            "questions": {
                "value": "Please respond to the questions mentioned as part of the four points mentioned in the earlier section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no mention of an IRB approval for the study discussed in the paper."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698559933641,
        "cdate": 1698559933641,
        "tmdate": 1699636292768,
        "mdate": 1699636292768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mfn9kSjSp2",
        "forum": "RofU5v2BvZ",
        "replyto": "RofU5v2BvZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_i2dJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_i2dJ"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the use of large language models (LLMs) to facilitate coordination between humans and AI within human-AI teams through language-based conventions. Their findings indicate that LLMs are adept at aligning with humans who exhibit various coordination patterns, outperforming other baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written with the exception of two subsections (5.3 and 5.4). \n- Utilizing LLMs to come up with coordination strategies taking human preferences into account is an interesting direction. \n- Experiments on the ability of the approach to coordinate with humans of different coordination patterns have been done using both real humans and proxy human models."
            },
            "weaknesses": {
                "value": "- The requirement for humans to decompose problems is a significant precondition. Additionally, humans are tasked with evaluating the LLM-generated plans and executing the tasks, which could be problematic in complex domains.\n- The findings regarding the use of multiple sessions are somewhat expected, given that the problem decomposition is already done, significantly reducing the difficulty of the problem for the LLM. \n- The details regarding the additional benchmark results are vague, particularly whether they refer to the coordination aspect or the use of multiple sessions.\n- Depending on human evaluations to assess plans may be unreliable in complex domains, even when the evaluator is an expert."
            },
            "questions": {
                "value": "- Were there any cases where the LLM provided actions that were not at all executable in the environment? If so, how were they dealt?\n- Figure 2 is hard to comprehend. What are the boxes in blue supposed to convey?\n- Do the additional benchmark results have human-AI coordination as well? How are the respective tasks being divided between humans and the AI? It is unclear from the write-up."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698924068078,
        "cdate": 1698924068078,
        "tmdate": 1699636292687,
        "mdate": 1699636292687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UVWW78YG8V",
        "forum": "RofU5v2BvZ",
        "replyto": "RofU5v2BvZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
        ],
        "content": {
            "summary": {
                "value": "This paper presents HAPLAN, and LLM-based approach to planning for ad hoc human-AI cooperation.  At a high-level, HAPLAN passes a textual description of the task (various scenarios in the Overcooked! environment in this case) to an LLM (ChatGPT), requesting that it return a detailed plan with instructions for both the human and the AI.  The AI's component of the plan is executed by a set of pre-trained, goal-conditioned policies for the various sub-tasks, specifically \"fetching\" and \"delivering\" items around the kitchen.  The human observes the plan before performing the task, and has the opportunity to provide corrective, natural language feedback to correct defects in the original plan.  Their experiments show that HAPLAN achieves significantly higher scores with real human partners and scripted AI partners than pervious methods proposed for ad hoc cooperation in Overcooked!.\n\nThe authors argue that one of the key contributions of the work is the idea of processing different stages of the planning process (high level planning, timing calculations, subtask ordering) in separate GPT sessions, which helps overcome the difficulties in reasoning about long conversation histories.  They support this argument with experiments (unrelated to ad hoc cooperation) on a set of benchmark reasoning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main strength of the paper in my opinion is that it proposes (and experimentally validates) a new paradigm for practical human-AI cooperation that leverages the strengths of modern large language models to enable natural language interaction as part of the coordination process itself.  I could imagine that with further development, this approach could be useful for applications such as customer-support chatbots, for tasks requiring close coordination between the human and the chatbot to resolve a technical issue."
            },
            "weaknesses": {
                "value": "My main concern with the work is that the improved performance of HAPLAN relative to the baseline algorithms may have much less to do with its ability to cooperate with a variety of partners, and more to do with the superiority of the LLM-based hierarchical planning approach over the \"flat\" and \"uninformed\" RL algorithms used by the baseline approaches.\n\nHAPLAN incorporates a great deal of human task knowledge, some of it through the natural language interface itself, but also through the use of pre-trained low-level policies that implicitly describe a decomposition of the high-level task into considerably simpler sub-tasks.  As Overcooked! can be challenging even without the ad hoc component (due to sparse rewards and the complexity of the policie needed to achieve them), it seems likely that the additional information available to HAPLAN would make a substantial difference in task performance against any partner.\n\nTo test this alternative explanation for the results, it would be useful to see how well HAPLAN compares to joint policies trained together for the overcooked task (such that each policy is a best-response to its partner).  It would also be helpful to provide more information about the \"diversity\" of strategies observed during human subjects experiments.\n\nA related issue is that some important details of the experimental setup have been omitted.  Most significantly, it is unclear how plans are \"stepped\" during the interaction, that is, how the agent implementing the HAPLAN plan knows when an item has been fetched and delivered, and decides to move on to the next step?\n\nFinally, while not a weakness of the HAPLAN approach itself, it is important to clarify for the reader that HAPLAN operates in a very different cooperation paradigm than the baselines.  While this paradigm, with a detailed conversational coordination phase prior to any physical interaction, may be suitable in some settings, it may not be useful in others (such as real time human-robot shared autonomy)."
            },
            "questions": {
                "value": "1. How was the textual plan generated by HAPLAN \"stepped\" during execution?\n2. Was there any real-time synchronization between the human and the AI?  For example, would the AI wait for the human if they were delayed in completing a prerequisite task?\n3. How long were individual episodes of interaction (how many time steps, were the agents allowed to complete multiple dishes?)\n4. For the scripted agents (Table 1), what information about the specific scripted policy was provided via the prompts?\n5. Were humans allowed to construct their own initial prompts?  Rather than providing the instruction given to the human, could the human provide their own description of the role they planned to take?\n6. How closely did human's observed behavior match the joint strategy they finally agreed upon?\n7. It wasn't immediately clear, but were humans allowed to provide text-based feedback between episodes, or only in advance of episodes?\n8. How much variance was there in the types of feedback humans provided?\n9. How would a plan generated by HAPLAN compare to a joint policy trained via a cooperative MARL method?\n10. How much prompt engineering was required here?  The prompts themselves are quite complex; are any results available with elements of these prompts removed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nsKW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698955972615,
        "cdate": 1698955972615,
        "tmdate": 1700683435060,
        "mdate": 1700683435060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YjGZybM1xB",
        "forum": "RofU5v2BvZ",
        "replyto": "RofU5v2BvZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present an approach to have humans coordinate with large language models via giving instructions to the LLM prior to the interaction (referred to as establishing a convention in the paper). These instructions are then iterated on during a sequence of interactions. The LLM is controlled via complex prompts (given in the supplementary materials) for each task which are translated into in-game actions via developer-specified low-level skills learned through demonstrations. The results indicate that the authors' approach (HAPLAN) meets or slightly exceeds the performance of the baselines, with greater effects on later rounds."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There is certainly a need to explore better ways for human users to interact with and controller Transformer-based models. This is also, to the best of my knowledge, an original approach for the Overcooked task specifically. The quality in terms of the amount of engineering work and time cost for the human subject study is also impressive. The results are likely to be of interest to those working with the Overcooked tasks or those with the resources to replicate the work HAPLAN requires for each domain."
            },
            "weaknesses": {
                "value": "This paper has three major weaknesses. \n\nFirst is the approach itself. This is not clearly conveyed in the text of the paper, certainly not in sufficient detail for replication. As far as I can tell, the process is that the authors come up with a domain-specific sequence of prompts for the sequence of prompts in a domain. This includes coming up with a set of low-level skills to refer to in these prompts. They then train these low-level skills via imitation learning. Then, during an interaction, they use an LLM to interact with the user and based on the user's prompts/conventions, the LLM produces an output plan referencing the low-level skills, which is then executed. If this is the case, this is an approach that has a lot of barriers to generalization, requiring human expertise and significant development time. It's also not guaranteed to generalize to every human-AI interaction domain, as it may not always be possible to break a problem into sequences or tasks or into low-level skills. The novelty of this approach is also fairly low, relying on putting together existing approaches.\n\nSecond is the human subject study methodology. There's no clarity in the paper in terms of what this methodology was. While the supplementary materials indicate that efforts were put forth to attempt to decrease bias, it's unclear what this means exactly. There are many possible sources of bias in terms of what population was recruited from, how they were compensated, what instructions they were given, and so on. Clarification on these points, ideally through a complete breakdown of the methodology, is necessary in order to avoid any potential that the results might be tainted by bias. \n\nThird is the results. The improvement in terms of the results is fairly marginal for what appears to be a much more complex and engineering-intense approach. Further, almost all the baselines see improvement over the three rounds, so it's unclear to what extent the convention is helping. The inclusion of a version of HAPLAN without the convention might have helped clarify this."
            },
            "questions": {
                "value": "1. Am I correct in my understanding of HAPLAN?\n2. Am I correct in my understanding of the development/design and engineering work needed to adapt HAPLAN to a new domain?\n3. What was the methodology of the human subject study?\n4. Is the improvement of HAPLAN significant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "My concerns have been addressed with additional details."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3413/Reviewer_nmM1"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986947142,
        "cdate": 1698986947142,
        "tmdate": 1700534404786,
        "mdate": 1700534404786,
        "license": "CC BY 4.0",
        "version": 2
    }
]