[
    {
        "id": "dfyOse4ULv",
        "forum": "b3f7FRUIzJ",
        "replyto": "b3f7FRUIzJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_1WPw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_1WPw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-modal training method to enhance the robustness of multi-modal models to modality missing. Furthermore, since this paper utilizes CLIP embeddings, its absolute performance is significantly superior to previous methods in some datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1, Using CLIP embeddings has led to a significant improvement in the overall performance of the multi-modal model.\n\n2, The proposed training method indeed makes the multi-modal model more robust in cases of missing modalities compared to conventional models.\n\nHowever, I believe that many of the experiments in this paper are, in fact, unfair in their comparisons. I have provided a detailed explanation of this in the \"Weaknesses\" section."
            },
            "weaknesses": {
                "value": "1, The reason this multi-modal model can achieve SOTA results on several datasets is fundamentally due to the use of embeddings from pre-trained models (such as CLIP embeddings), rather than the inherent superiority of the proposed training method itself. If you want to demonstrate how good your proposed training method is, different training methods should be applied with the same backbones. For the reasons mentioned above, I find the significance of Tables 2, 3, 4, and 5 to be quite limited because the performance improvement is not a result of your paper's new method but rather the utilization of pre-trained models from previous works.\n\n2, In Table 6, when comparing the proposed method with Ma et al., I believe there is a significant misconception here. You used the CLIP model pre-trained on a large-scale text-image dataset by OpenAI, while Ma et al. used the ViLT backbone. The absolute performance of the model in this paper is better than Ma et al., which may be due to the superiority of CLIP over ViLT, rather than the training method proposed in this paper is better than Ma et al.'s method. **A more accurate comparison should be based on the proportion of performance degradation.**   Specifically, when 10% of the text is missing, Gazelle shows a decrease of (94.6-93.2)/(94.6-81.7)=10.85%, while Ma et al. exhibits a decrease of (92.0-90.5)/(92.0-71.5)=7.32%. From this perspective, when 10% of the text is absent, Ma et al. experience a relatively smaller proportion of decrease. Your higher absolute performance is simply due to the use of stronger pre-trained model embeddings, not because your proposed method is superior.\n\n3, The results in Table 6 for Hateful meme, where having 50% text performs better than having 70% text, and where 0% text and 10% text yield the same performance, are indeed puzzling. This could suggest that the method proposed in this paper may not make optimal use of the available text data.\n\n4, The method proposed in this paper requires that the sizes of features from different modalities remain consistent, which actually limits the flexibility of the entire model. For example, it may prevent the combination of BERT-Large and ViT-B."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Reviewer_1WPw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671279603,
        "cdate": 1698671279603,
        "tmdate": 1699636287971,
        "mdate": 1699636287971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xk4YJGgncB",
        "forum": "b3f7FRUIzJ",
        "replyto": "b3f7FRUIzJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_q4t6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_q4t6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method for multimodal learning while dealing with missing modalities. The proposed method uses a single-branch network and a modality switching mechanism that shares weights for multiple modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper tackles the interesting and important problem of learning multimodal data while being able to deal with missing modalities."
            },
            "weaknesses": {
                "value": "There are a number of shortcomings in the paper:\n\n- The writing is generally ok, but a bit concise imo. Starting off the introduction with \"social media users\" is a bit strange, given that multimodal data have far wider uses other than social media.\n\n- The method section is unclear and not well-written. First, it states \"...sequential fashion. It is achieved by introducing a modality switching mechanism that determines the order in which embeddings are input to the single-branch network.\" What are the theoretical foundations for this? why is this used? what is the motivation and intuition behind it? Next, the paper states that they have three possible strategies: 1- randomly switching, 2- swishing between multimodal and unimodal 50-50, 3- going only with unimodal. Yet, no details are provided. Which of these are the proposed method? Is the paper simply exploring three options? Are there no other options? why not set the ratio as a hyperparameter and optimize it?\n\n- The entire method is basically explained in a single paragraph, making it almost impossible to understand the details, fundamental theories and motivations behind things, etc.\n\n- The methods used for comparison in Tables 2 through 5 have many important papers missing.\n\n- Especially for the missing modality experiments, only 1 comparison is done (against Ma et al., 2022). Unfortunately, this is not enough, even if the method was sound and explained properly. Further experiments are required to validate the method."
            },
            "questions": {
                "value": "Please see my comments under weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Reviewer_q4t6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724834641,
        "cdate": 1698724834641,
        "tmdate": 1700245155645,
        "mdate": 1700245155645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "87jzCBTf6u",
        "forum": "b3f7FRUIzJ",
        "replyto": "b3f7FRUIzJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_fm8J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_fm8J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a robust multimodal classification system, which is less susceptible to missing modalities. This system leverages a single-branch network to share weights across multiple modalities, and introduces a novel training scheme for modality switch over input embeddings. Extensive experiments demonstrate the effectiveness of the proposed system."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written and contains sufficient details and thorough descriptions of the experimental design.\n2. Extensive experiments are conducted to verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. While ViLT is a good baseline, it is not a \"SOTA\" method as there are many more advanced models in recent years. Choosing ViLT as the baseline makes the comparison less convincing. Especially, the proposed system uses pre-extracted embeddings (e.g., CLIP).\n\n2. For the table 2-5, the choices of baselines are a little bit out-of-date. The improvements are marginal while the proposed model uses better features with a lot of heuristic designs."
            },
            "questions": {
                "value": "See the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3373/Reviewer_fm8J"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810932128,
        "cdate": 1698810932128,
        "tmdate": 1699636287825,
        "mdate": 1699636287825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8SXlVCucuY",
        "forum": "b3f7FRUIzJ",
        "replyto": "b3f7FRUIzJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_ZSk2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3373/Reviewer_ZSk2"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Gazelle, a simple yet robust multimodal classification model for handling incomplete modalities. The key idea of the model is to use a modality switching mechanism to sequence the embedding streams of single-branch networks. While the experiments demonstrate Gazelle's superior performance in dealing with missing modalities, the paper could benefit from improvements in presentation clarity, additional theoretical analysis, and more robust experimental results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper introduces a simple yet robust method for handling missing modalities. It is presented in an easy-to-follow manner.\n2. The method demonstrates superior robustness when compared to existing state-of-the-art methods."
            },
            "weaknesses": {
                "value": "1. Incomplete modality/view learning is an important topic in machine learning community, which has achieved great progress in recent years. The authors need to provide a more comprehensive review of the topic.\n2. What is the intuition of presenting the modality switching mechanism? A clearer motivation is needed.\n3. The proposed method seems to be treated as a training trick. As a general framework, it would be better to provide a theoretical analysis for Gazelle. \n4. The readers would be confused with the presentation of Figure 2. For example, what is the mean of each column in S-1, -2, and -3?\n5. Can the proposed method handle missing modality in the training stage? How does the method fuse different modalities?\n6. The experiment part could be improved by providing a more in-depth analysis. For example, trying to explain why the proposed modality switching strategy is helpful, and whether existing multimodal learning methods benefit from the strategy.\n\n\n1. In the field of incomplete modality/view learning, it is imperative to provide a comprehensive review of recent advancements within the machine learning community.\n2. It would greatly benefit the paper to clarify the intuition behind presenting the modality switching mechanism. A clearer motivation for its inclusion is necessary.\n3. The proposed modality switching mechanism can be treated as a training trick. It would be better to provide a theoretical analysis for it. \n4. Clarifications should be provided for the presentation of Figure 2, particularly regarding the meanings of each column in S-1, -2, and -3 to avoid confusion for readers.\n5. Further details regarding the capability of the proposed method to handle missing modalities during the training stage and insights into how it effectively fuses different modalities are needed for clarity.\n6. The experiment part could be improved by providing a more in-depth analysis. For example, explain how the proposed modality switching strategy improves robustness, and whether existing multimodal learning methods benefit from the strategy."
            },
            "questions": {
                "value": "please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851362026,
        "cdate": 1698851362026,
        "tmdate": 1699636287750,
        "mdate": 1699636287750,
        "license": "CC BY 4.0",
        "version": 2
    }
]