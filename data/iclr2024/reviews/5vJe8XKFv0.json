[
    {
        "id": "1K97PEmCeg",
        "forum": "5vJe8XKFv0",
        "replyto": "5vJe8XKFv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_8MNi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_8MNi"
        ],
        "content": {
            "summary": {
                "value": "The CONO is a sophisticated deep learning architecture designed to operate within the complex domain, aiming to capture and represent complex numerical signals effectively. Its architecture is underpinned by several key features:\n\n1. **Complex Domain Operation**: At its core, CONO processes data within the complex plane. By doing so, it taps into the rich information available in both the real and imaginary components of complex data. This enhances its expressive power and capacity for feature extraction.\n\n2. **Point-wise Operators and Transformations**: The model employs several operations and transformations, such as P, Q, and R, which project, convert, and lift data into various domains. Notably, it uses a Complex Convolutional Neural Network (CCNN) for certain transformations and integrates a complex UNET for additional processing.\n\n3. **Fractional Fourier Transforms**: A distinctive feature of the CONO is its utilization of the Discrete Fractional Fourier Transform (FrFT). This allows the model to learn and operate 'in between' the physical and frequency domains, offering a unique perspective and capturing various frequency contents and directional features present in data.\n\n4. **Continuous-Discrete Equivalence**: The model emphasizes maintaining a balance between continuous and discrete operations. This structure-preserving approach ensures that the model remains aligned with foundational principles like the Shannon-Whittaker-Kotel\u2019nikov theorem. This ensures reliable analyses and predictions by the model.\n\n5. **End-to-End Architecture**: From input to output, the CONO model is structured to project, transform, process, and then revert data, ensuring the entire process is smooth and cohesive. The various layers, including complex UNET, CCNN, and point-wise operators, work together in harmony to achieve this.\n\nIn summary, the CONO is a robust and versatile model that capitalizes on the richness of complex domain operations, layered transformations, and a structure-preserving approach to effectively handle and analyze complex numerical signals."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Complex Domain Operations**: The ability of the CONO model to operate within the complex domain enables it to effectively capture the intricacies of complex numerical signals. This not only augments its expressive power but also enhances feature extraction, making it adept at representing and analyzing complex data.\n\n2. **Structure-preserving Architecture**: The CONO aims to maintain complex continuous-discrete equivalence, ensuring that the Shannon-Whittaker-Kotel\u2019nikov theorem is obeyed for all continuous operations. This kind of structure preservation ensures that the model remains faithful to the underlying physics or principles, making predictions and analyses more reliable.\n\n3. **Comprehensive Framework**: The CONO encompasses a series of intricate operations, transformations, and layers, such as complex UNET, CCNN with a residual connection, and the use of fractional Fourier transforms. This comprehensive framework makes the model versatile and robust, allowing it to handle a wide variety of tasks and challenges, especially in the context of capturing complex numerical signals."
            },
            "weaknesses": {
                "value": "1. The articulation and expression of the manuscript require further refinement. In several sections, the clarity of the narrative falls short, making it challenging for the reader to grasp the content.\n\n2. The operations of CONO within the complex domain allow it to effectively capture and represent the nuances of complex signals. This enhances its expressive power and improves feature extraction. However, due to the intricacies of the involved operations and transformations, a significant amount of parameter tweaking and experimentation may be necessary to achieve optimal performance. It would be beneficial if the authors could provide detailed settings from their experiments, including memory usage.\n\n3. I'm particularly interested in the experiments related to the NS equation. To my knowledge, the original FNO paper mentioned three distinct viscosity coefficients. However, the authors seem to have chosen 10e-4 without clearly specifying it. This coefficient may not be the most challenging one. I would suggest the authors consider using the more challenging 10e-5 as the viscosity coefficient.\n\n4. I would encourage the authors to incorporate more visualizations to allow readers to gain a more detailed and intuitive understanding of the predicted outcomes.\n\n5. The selection of baselines for comparison appears to be incomplete. To provide a more comprehensive evaluation, I recommend the authors consider adding models like PINN[1] and LSM[2] to the comparisons.\n\n   [1] Raissi, M., Perdikaris, P. & Karniadakis, G.E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. *Journal of Computational physics*, *378*, pp.686-707.\n\n   [2] Wu, H., Hu, T., Luo, H., Wang, J. & Long, M. (2023). Solving High-Dimensional PDEs with Latent Spectral Models. *arXiv preprint arXiv:2301.12664*."
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698405984115,
        "cdate": 1698405984115,
        "tmdate": 1699636648393,
        "mdate": 1699636648393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ckUD5TLfvK",
        "forum": "5vJe8XKFv0",
        "replyto": "5vJe8XKFv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_Ubji"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_Ubji"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a Complex Neural Operator (CoNO) that parameterizes the integral kernel in the complex fractional Fourier\ndomain. The authors claim robust and superior modeling of continuous dynamical systems compared to state-of-the-art models in PDE tasks such as Burgers' equation, Navier-Stokes equations, Darcy flow and diffusion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Fractional Fourier Transform is an interesting concept, yet to be introduced into the deep learning community."
            },
            "weaknesses": {
                "value": "- There is no information about baseline models and training/inference times. There is a link to the code repo added, but it is impossible to figure out the settings and hyperparamters of the models. Given that the proposed model performs marginally better than FNO models this makes it impossible to judge. Side remark: Pytorch FFTs on complex inputs are much slower than on real inputs (torch.fft vs torch.rfft), thus runtime comparisons would be needed.\n\n- The main formula (Eq 3) is hardly explained. For example in the literature the Fractional Fourier Transform is often defined as $$\\mathcal{F}_{\\alpha}\\[f\\](u) = \\sqrt{1 - i \\cot(\\alpha)} e^{i\\pi\\cot(\\alpha)u^2} \\int e^{-2\\pi i \\left( \\csc(\\alpha) u t - \\frac{\\cot(\\alpha)}{2}x^2\\right)} f(t) dt \\ .$$  What is the relation of Eq. 3 to the presented formula, and more importantly how can this be implemented? In the code the CoNO model looks very similar to the FNO model, but I assume that the Fourier transform needs to be changed since there is another term which depends on $t$? \n\n- The proposed CoNO model uses a complex UNet part after the fractional transform. It is impossible to guess what brings the claimed performance boost - the fractional transform or the UNet operation in the fractional Fourier domain, which is comparable to pointwise multiplication as done in FNOs? At least comparisons to UNets are therefore inevitable. Especially, since on regular gridded domains UNets / convolutional operators have shown strong performances, see e.g. Raonic et al or Gupta et al. \n\n- Ablation studies are not revealing a lot, they are basically showing the same results as the main table.\n\n- There has been work for example on Clifford Fourier Neural Operators (Brandstetter et al) which includes complex numbers and more complicated algebras. Possibly missing a few others here. Discussions of related work and comparisons against those are missing.\n\nRaoni\u0107, B., Molinaro, R., Rohner, T., Mishra, S., & de Bezenac, E. (2023). Convolutional Neural Operators. arXiv preprint arXiv:2302.01178.\n\nGupta, Jayesh K., and Johannes Brandstetter. \"Towards multi-spatiotemporal-scale generalized pde modeling.\" arXiv preprint arXiv:2209.15616 (2022).\n\nBrandstetter, J., Berg, R. V. D., Welling, M., & Gupta, J. K. (2022). Clifford neural layers for PDE modeling. arXiv preprint arXiv:2209.04934."
            },
            "questions": {
                "value": "- Why does Equation 3 contain $u$, $x$, and $t$ as variables?\n- Can you provide experiments which underline your claim of mitigating aliasing? Or differently put, why is your approach better than FNO? Results don't underline this claim."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601449368,
        "cdate": 1698601449368,
        "tmdate": 1699636648271,
        "mdate": 1699636648271,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2ckXO6oKaw",
        "forum": "5vJe8XKFv0",
        "replyto": "5vJe8XKFv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_CUfC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_CUfC"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces complex neural operator, a new member in the neural operator family, that fully leverages complex representation in the feature space. The kernel integral operation is implemented via fractional Fourier transform. The model is demonstrated to have better simulation accuracy, higher data efficiency and robustness against noise compared with existing neural operators."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed approach is clean and easy to follow.\n\n2. Complex neural operator permits learnable order and a complete usage of complex representation space.\n\n3. The method seems to perform well in zero shot superresolution and is more robust to noise."
            },
            "weaknesses": {
                "value": "1. In general the performance improvement of CoNO over FNO seems marginal on most of the datasets. It is not sufficiently convincing to demonstrate the efficacy of the proposed modules. Similar observations can be found in Table 7 for the ablation studies.\n\n2. Lacking theoretical insights on why the model is more robust and performs better on OOD tasks, compared with, e.g., FNO.\n\n3. Presentation can be further improved. For instance, it would be better to index each paragraph in section 3 with a brief summary on what the detailed layer/operation this part is discussing.\n\n4. Lacking some necessary analyses (see Q3, Q4).\n\nMinor: There are indeed multiple typos that do cause difficulty in reading. e.g., Table 4.6 should be Table 7. ``add reference\" in page 2. Please fix them."
            },
            "questions": {
                "value": "Q1. Are there any theoretical insights why the complex model is more robust to noise than FNO?\n\nQ2. It would be better to see more ablations on other datasets since the results in Table 7 is not significant enough especially taking into consideration the stds.\n\nQ3. Can the authors provide visualizations that would give the readers an idea what this task is about and how the model performs qualitatively?\n\nQ4. It would be better to involve analyses on the learned orders, akin to the frequency analyses done in FNO."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620782356,
        "cdate": 1698620782356,
        "tmdate": 1699636648151,
        "mdate": 1699636648151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZeLarks9hE",
        "forum": "5vJe8XKFv0",
        "replyto": "5vJe8XKFv0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_HK2j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6030/Reviewer_HK2j"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to new paradigm of neural operator learning. The key components are the use of complex numbers in the latent space and the use of fractional Fourier transform as the frequency transform. There are additional components which help in proposing the model as an alias-free method. The performance of the model is evaluated on several standard PDE and it is shown to have comparable/superior performance in several cases, including out-of-distribution, super-resolution, robustness to noise."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "While there are some notational discrepancies, as pointed out in the section below, I think the authors do a overall good job in sticking to notation and make the method section very clear to understand.\n\nThe placement of the proposed method within existing work is adequate.\n\nI think the use of complex numbers along-side fractional Fourier transform is novel and something that is being demonstrated well.\n\nI specifically like section 2.4 where authors describe a simple yet effective anti-aliasing strategy."
            },
            "weaknesses": {
                "value": "I think the proposed method is novel, and that it can give comparable performance. What is not clear from the presentation both theoretical justification and/or empirical evidence is the benefits that it can have over FNO. Please see questions section for specific.\n\nThe writing/overall presentation can be improved.\n - For example, Table 1, caption is not all descriptive, without first introducing what is order of transform, a comparison is being made. This table is not necessary in my opinion.\n- After equation (6), $f$ becomes $F$, small things like these\n- The description of the several PDE while good, takes up lot of space, which can be otherwise devoted to better explaining and further evaluating the use of complex number\n\n(Minor) There are several typos, some of which I will point out below:\n- Reference to Table 7 is broken\n- The caption of figure 1 is not coherent with the figure and/or text description of the model. These are important, as readers will get super-confused if these are not in place.\n- Beginning of section 2.3, sentence is redundant"
            },
            "questions": {
                "value": "1) What is the comparison in terms of the number of parameters, memory usage, training and/inference time between the proposed method and the baselines, specifically FNO. I think these are showing more/less comparable performance in all cases, and they need more clarity. \n    - I feel that because of the use of complex numbers, the model has more capacity but that also makes me think, that the model will be 2 times as more expensive than FNO. In d-dimensions this will be $2^d$ times more expensive. Hence, the claim that authors make that CoNO is better in higher dimensions is questionable in terms of the trade-off.\n\n2) For Navier Stokes and Shallow Water experiment, why do we need the first 10 steps to learn the operator? This seems arbitrary to me and is presented without any justification. In all other cases this is only dependent on the initial condition as should be the case.\n\n3) Comparison to baseline: It is mentioned \" randomly selecting a subset of hyperparameters\" for models and then one line later \"We conduct model training for each optimal hyper-parameter configuration using random seeds and data splits\". How is the optimal choice made in each case?\n\n4) As evident in Table7, the use of Fractional Fourier transform doesn't seem to add much of performance boost. What do authors' comment on this choice? Does the learnable order help?\n\nAs mentioned previously, I am not able to draw line between CoNO and other Frequency domain method like FNO apart from the use of complex numbers without clear benefit and hence would like clarification and discussion from the authors on these points above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6030/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6030/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6030/Reviewer_HK2j"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687012339,
        "cdate": 1698687012339,
        "tmdate": 1699636648039,
        "mdate": 1699636648039,
        "license": "CC BY 4.0",
        "version": 2
    }
]