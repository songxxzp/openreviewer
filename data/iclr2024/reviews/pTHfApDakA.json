[
    {
        "id": "LoTJAaqgW7",
        "forum": "pTHfApDakA",
        "replyto": "pTHfApDakA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission204/Reviewer_6G8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission204/Reviewer_6G8r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Self-Check, a method that allows LLMs to check their reasoning. The method involves four steps (target extraction, information collection, step regeneration, and result comparison.), each involving querying LLM for a different step. The basic idea involves probing the model to understand what a substep is doing (Steps 1 and 2), and then solving a substep again with this increased understanding (Step 3). If the generated answer does not match the conclusion of the subset, it indicates a contradiction in the output at the step. The contradictions are accumulated to generate a single score for the correctness of the solution. During inference, multiple solutions are generated, and SelfCheck scores are used to weigh and pick the best solution. This score is used to weigh solutions. The authors also analyze whether the SelfCorrect predictions correspond to the actual correctness and find that it is generally the case."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**A:** The method is straightforward yet proves to be effective. Although there are questions regarding its efficiency and effectiveness in comparison to majority voting (refer to weaknesses), the analysis confirms that SelfVerification is indeed taking place. \n\n**B:** The paper demonstrates that LLMs can self-correct for reasoning problems in certain cases, providing a valuable answer to a significant question currently of interest to the community."
            },
            "weaknesses": {
                "value": "**A:** SelfCheck requires 3 LLM queries per substep. Consequently, the comparison between SelfCheck and SelfConsistency appears skewed, as SelfCheck inherently utilizes approximately 10x more samples (assuming an average of 3 substeps). Furthermore, the results with GPT-4 are based on a limited sample size, and given the analysis with Self-Consistency, the findings should be interpreted cautiously. \n\n\n**B:** While the method is presented as 0-shot, it requires training samples (as seen in section 3.2) to learn hyperparameters, and thus, the criticism of approaches that require examples is unfair (We note though that these approaches are not directly comparable to SelfCheck in general, as they require additional exemplars which will often not be available in practice.). Additionally, the authors had to meticulously craft instructions for each step, which somewhat undermines their critique of other methods."
            },
            "questions": {
                "value": "Q1. If my observations in Weakness A are accurate, could Figure 5 simply result from comparing incompatible data points? Should we compare SelfCheck @ k with SelfConsistency @ m with m ~ 10 x k?\n\nQ2: What are the false positive rates at various thresholds?\n\nQ3: When devising the prompts, a small number of training samples from MathQA dataset were utilized. How were these examples used? \n\n\n---\n\n### Comments\n\n- _Therefore, with SelfCheck, LLMs can effectively rectify their own biased beliefs by themselves._ While this insight is crucial, it might not be immediately apparent to a casual reader. In my opinion, this is an important takeaway from the paper. A more extensive analysis of this topic would be highly beneficial.\n\n- Missing citation: \n\nJiang, Weisen, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T. Kwok. \"Backward reasoning in large language models for verification.\" arXiv preprint arXiv:2308.07758 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Reviewer_6G8r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721502350,
        "cdate": 1698721502350,
        "tmdate": 1700598975606,
        "mdate": 1700598975606,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I184Buv2Mq",
        "forum": "pTHfApDakA",
        "replyto": "pTHfApDakA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission204/Reviewer_NsFJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission204/Reviewer_NsFJ"
        ],
        "content": {
            "summary": {
                "value": "The paper focused on the task of refining CoTs when applying LLMs to reasoning. The main contribution is to propose a self-checking solution on each step in the reasoning chain of CoTs one by one. The proposed operations include target extraction, information collection, step regeneration, and results comparison. The main tech employs some specifically designed prompts for powerful LLMs, such as GPT 3.5 and GPT-4. The experimental comparisons on three datasets (GSM8K, MathQA and MATH) show that the proposed method could outperform existing majority voting methods, self-verification, and deductive verification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Correcting each reasoning step through self-checking is interesting.\n2) Employing LLMs to perform self-checking based on designed prompts may be useful for some applications.\n3) The experimental results on three open-accessed datasets show the effectiveness of the proposed solutions."
            },
            "weaknesses": {
                "value": "1) I think discussing how to prompt some \"incantations\" to the LLMs (especially for such black-box LLMs) has no insight into this community. The proposed prompts for each component in the self-checking process are well-designed, especially for GPT-serial LLMs. In my opinion, they could not be well extended to other LLMs, such as LLama or Claude.\n\n2) Step-by-step checking may have the problem of error propagation."
            },
            "questions": {
                "value": "1) Why the authors do claim the solution is self-checking? In Table 1, the authors also employ different LLMs on the Generator and Checker. \n\n2) Are the proposed checking components and prompts in each step useful for other LLMs?\n\n3) In equation (1), why lamda_-1 is setting to 1 and lamda_0 to 0.3? How to determine such parameters\uff1f\n\n4) Why there is no dot for DV and SV in Figure 2 (b)\n\n5) What is the meaning of the yellow curve in Figure 3?\n\n6) I think that the discussion in subsection 5.2 is of no use. The performance heavily relies on the prompts. So I think if we adopt an optimal prompt in the global checking setting, better results may be obtained.\n\n7) Why were the experiments conducted on the math-related tasks? I wonder where the performance goes when other types of reasoning are explored, such as logical reasoning, deductive reasoning, and temporal reasoning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726309433,
        "cdate": 1698726309433,
        "tmdate": 1699635946039,
        "mdate": 1699635946039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bTjNWdswMA",
        "forum": "pTHfApDakA",
        "replyto": "pTHfApDakA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission204/Reviewer_PScZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission204/Reviewer_PScZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a zero-shot verification schema named SelfCheck to recognize errors in the step-by-step reasoning of large language models (LLMs) in answering complex questions. The proposed SelfCheck implements the self-checking by following a regenerate-and-compare procedure and then conducts weighted voting on multiple solutions, thereby improving the performance. Experimental results on three maths datasets including GSM8K, MathQA and MATH demonstrate the effectiveness of SelfCheck, which alleviates the need for additional data or external resources."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper explores the self-checking capabilities of LLMs by elaborating a regenerate-and-compare schema to assess their reasoning and recognize errors, which potentially has broader insights beyond a wider range of tasks.\n\n2. The authors introduce an intriguing and reasonable ensemble method for multiple solutions, which employs an integration function and calculates confidence scores in the final voting stage.\n\n3. The authors provide a comprehensive and detailed analysis to validate the effectiveness of each stage of the proposed SelfCheck."
            },
            "weaknesses": {
                "value": "1. The authors claim that they provide a general-purpose, zero-shot approach to checking. However, the proposed schema is more tailored to maths problems to some extent as answering mathematical questions requires explicit computational steps and numerical formulas, which can be easily checked by LLMs. Hence, it is a little confusing to state the proposed SelfCheck is general-purpose. I wonder if the authors could conduct experiments on other types of reasoning tasks such as logical reasoning or commonsense reasoning.\n\n2. Confusion about the methodology design.\n\n- The authors check each step by evaluating the conditional correctness of each step based on the provided question and previous steps in the solution. I doubt whether there could be a phenomenon where step i is wrong, but the following steps are perfectly correct, i.e. the model is anot affected by the step i error. But if the authors evaluate using conditional errors, it\u2019s quite possible that the model would determine all subsequent steps (after step i) are wrong. Could the authors provide more analysis or explanations?\n\n- The design of the integration function seems helpful as shown in the experimental results. Could the authors explain the insights of this design and the choices of hyperparameters?\n\n3. A lack of clarification regarding the experimental results.\n\n- The authors claim that SelfCheck significantly outperforms majority voting with both GPT-3.5 and GPT-4. However, results in Table 1 (Row 2) show that SelfCheck exhibits inferior performance in GSM8K with GPT-4 as the generator and checker.\n\n- Results in Figure 2 demonstrate that the SelfCheck did not exceed majority voting by much, but the inference cost of SelfCheck should be larger in comparison. Could the authors illustrate in detail the superiority of SelfCheck over majority voting?\n\n4. The writing can be improved. There are some typos and unclear descriptions. Please refer to the questions below for details."
            },
            "questions": {
                "value": "1. In Paragraph 2, Section 3.1, the authors state that \u2018checking\u2019 is a less common task in the training corpus of most LLMs. Can the authors provide some evidence of citations of this statement?\n\n2. In Paragraph 5, Section 3.1, the authors claim that LLM is usually able to perform the tasks of figuring out the target of the current step. Missing evidence or supplementary experiments should be added.\n\n3. In Section 3.1, the authors mention Information in both stages of Information collection and Step regeneration but the Information should refer to different contents according to the authors\u2019 descriptions. A minor suggestion would be to modify the terms in the prompts in order to better distinguish them.\n\n4. The comparison between the proposed SelfCheck and DV/SV in Figure 2 is quite confusing. Could the authors give relevant explanations on how they compare these methods with varying scores and different generator models?\n\n5. Why did not the authors conduct experiments with GPT-3.5 as the Generator and GPT-4 as the Checker (in Table 1)? It would be helpful to provide appropriate descriptions.\n\n6. Minor comments on writing:\n\n(1)\tParagraph 2 in Section 1: ..., which is far below human level -> , which  is far below the human level\n\n(2)\tParagraph 5 in Section 1 & Paragraph 1 in Section 3: to check their working -> check their work\n\n(3)\tParagraph 1 in Section 3.2: , and thus final solution -> , and thus the final solution\n\n[1] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou. LARGE LANGUAGE MODELS CANNOT SELF-CORRECT REASONING YET"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission204/Reviewer_PScZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820945836,
        "cdate": 1698820945836,
        "tmdate": 1699635945936,
        "mdate": 1699635945936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9HycegqbA9",
        "forum": "pTHfApDakA",
        "replyto": "pTHfApDakA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission204/Reviewer_wFF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission204/Reviewer_wFF7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces SelfCheck, a zero-shot verification framework designed to identify errors in Large Language Models (LLMs) when applied to mathematical reasoning tasks. Instead of the simplistic approach of self-querying LLMs, the paper presents a multi-stage strategy that deconstructs the problem into a sequence of simpler tasks. This method capitalizes on the LLM's generative capabilities and reduces the correlation between errors in the original generation and the subsequent verification process.\n\nThe approach involves separate interactions with the LLM, where it extracts the target and relevant context for each step, then generates an independent alternative step based on this information. Subsequently, it compares the original step with the regenerated one. If they match, the original step passes the verification check. The approach also employs confidence scores as weights to evaluate and prioritize more accurate solutions, offering a flexible alternative to traditional majority voting methods.\n\nNotably, this approach operates in a zero-shot manner, necessitating neither illustrations nor training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Using the LLM to identify errors in its own step-by-step reasoning, analogously to how a human might go back to check their working is a very interesting research problem. Self checking and further refinement of its own answers is a challenging task for LLMs when reasoning is involved. The paper proposes a general-purpose, zero-shot, checking schema for self-identifying errors in LLM CoT reasoning. Some past methods have dealt with this problem but almost all of them use few shot setting. It is interesting to see the method work in zero shot setup.\n\nThe paper introduces an innovative step-checking method that leverages the generative modeling capabilities of modern LLMs to determine the correctness of step-by-step reasoning. To achieve this, the paper divides the checking process into four key components: target extraction, information gathering, step regeneration, and result comparison. Each step is carried out with carefully designed prompts, and the collective outcomes of these steps determine the overall accuracy of the answer."
            },
            "weaknesses": {
                "value": "- The paper presents a general purpose validation framework designed to assess the accuracy of multi-step mathematical reasoning tasks. Although this approach performs effectively well within a zero-shot prompting setup, it essentially relies on well-crafted prompts. It remains uncertain whether these four steps constitute the exclusive and comprehensive way of rigorously assessing the accuracy and the reasoning process in such tasks. There might be more ways to asses the quality of the reasoning chain and there is no gurantee that this is the only way or the best way. These four steps are not acquired through learning, generated by a model, nor rooted in learning science or based on any other theory. So it is very hard to judge it. It is plausible that their effectiveness is coincidental in the context of mathematical reasoning tasks and may not readily apply to other types of reasoning. Consequently, for broader applicability, the prompts would need to be rethought and rephrased, raising questions about the approach's generalizability.\n\n- Also, I dont see the usefulness of zero shot setting as if this comprehensive prompt for verification can be written, then 1 or 2 examples can also be provided. Utilizing a few-shot configuration could have yielded improved outcomes and enhanced the model's ability to adhere to the prompt, potentially resulting in better overall performance. Moreover, in a few-shot setup, the approach could have been benchmarked against previous methods. If the intention behind the zero-shot setup was to demonstrate the broad applicability of the approach, it would have been much better to include datasets that covered a wider range of domains, rather than solely focusing on mathematical reasoning.\n\n- The paper lacks a comparative analysis with other methods that have employed extensive prompts, such as PHP , faithful decomposition, verification based on subquestion decomposition, and similar approaches. So it is hard to judge if this prompting strategy is the best one for mathematical reasoning. \n\n- No comparison has been drawn to the cost of the prompting method as for each sample, 4 extra verification steps are needed which makes 5 API calls (at least) per sample. Compared to one step verification tasks that take 2 API calls, is the cost vs accuracy tradeoff worth it?\n\n- Table 1 is a bit confusing as 2 samples for majority voting does not make much sense. Should be at least 3 so that there are some agreements between the samples.\n\nPapers:\n[PHP]: https://arxiv.org/abs/2304.09797\n[Faithful decomposition]: https://www-files.anthropic.com/production/files/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning.pdf"
            },
            "questions": {
                "value": "Questions are mentioned in the weakness section. Please refer to that."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053626959,
        "cdate": 1699053626959,
        "tmdate": 1699635945873,
        "mdate": 1699635945873,
        "license": "CC BY 4.0",
        "version": 2
    }
]