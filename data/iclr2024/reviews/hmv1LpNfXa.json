[
    {
        "id": "aHqtRgPH1t",
        "forum": "hmv1LpNfXa",
        "replyto": "hmv1LpNfXa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_uwZk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_uwZk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Polynormer, a polynomial- expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features, with model permutation equivariance. Polynormer has been evaluated on 13 homophilic and heterophilic datasets, including large graphs with millions of nodes, with results showing that Polynormer outperforms state-of-the-art GNN and GT baselines on most datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of designing a polynomial-expressive graph Transformer model is novel and interesting. \n\n- The resulting Polynormer model is powerful, efficient, and theoretically expressive.\n\n- The experiments are convincing, showing that Polynormer can outperform sota GNNs and GTs on a wide range of datasets."
            },
            "weaknesses": {
                "value": "- It is inappropriate to claim that GTs and GNNs has limited polynomial expressivity (in section 3.1 and appendix C), since the non-linearity layers are not negligible. In [1] it is shown that without softmax GTs cannot represent GNNs. And in [2] Transformers are proved to be universal approximators on sequences with the softmax layer as key component. Can you discuss the polynomial expressivity of GTs and GNNs with non-linearity layers? And since [2] proves that Transformers are universal approximators, do GTs have $\\infty$-polynomial expressivity?\n\n- The concept of graph is defined by edge connections. And the definition of polynomial expressivity is completely ignorant of graph structure, comparing to WL-test expressivity. From my opinion, polynomial expressivity defined here should be used to model expressivity on sets, not graphs. What is the motivation of modeling the polynomial expressivity of graph models?\n\n- The O(N+E) complexity claim should be supported by more experiments, like a training time (VRAM) \u2013 graph size plot on synthetic random graphs with different sizes.\n\n[1] Ying, Chengxuan, et al. \"Do transformers really perform badly for graph representation?.\" Advances in Neural Information Processing Systems 34 (2021): 28877-28888.\n\n[2] Yun, Chulhee, et al. \"Are transformers universal approximators of sequence-to-sequence functions?.\" arXiv preprint arXiv:1912.10077 (2019)."
            },
            "questions": {
                "value": "- See weakness above.\n\n- Can authors discuss more about the relationship between WL-test expressivity and polynomial expressivity? For example, is the proposed Polynormer strictly more powerful than 1-WL-GNNs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674124157,
        "cdate": 1698674124157,
        "tmdate": 1699636248631,
        "mdate": 1699636248631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1WULbqKMYl",
        "forum": "hmv1LpNfXa",
        "replyto": "hmv1LpNfXa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_D5Yt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_D5Yt"
        ],
        "content": {
            "summary": {
                "value": "The proposed Polynormer is a new graph transformer model that balances expressivity and scalability. The paper introduces a polynomial model that achieves polynomial expressiveness with linear complexity, outperforming state-of-the-art GNN and GT baselines on most datasets. The model is based on a novel polynomial attention mechanism that can capture higher-order interactions between nodes in a graph. The attention mechanism is designed to be both local and global equivariant, allowing it to capture both local and global patterns in the graph."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea to adopt attention model in the polynomial feature mapping is novel and interesting.\n- Experiments are sufficient. Many important baselines and datasets of various sizes are covered."
            },
            "weaknesses": {
                "value": "- I find that the proposed approach (global) may also work in transformers in other fields, e.g. NLP. Could you provide such experiments to show its capacity in dealing with different types of data?\n- Why and how could polynomial expressivity improve model performance? The point was not clear."
            },
            "questions": {
                "value": "- How is the degree of polynomial defined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737221960,
        "cdate": 1698737221960,
        "tmdate": 1699636248565,
        "mdate": 1699636248565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sGKd3olUqm",
        "forum": "hmv1LpNfXa",
        "replyto": "hmv1LpNfXa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_HGJh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3033/Reviewer_HGJh"
        ],
        "content": {
            "summary": {
                "value": "Summary:  \nThis paper proposes Polynormer, a graph transformer model architecture for node classification. \nFirst, the paper introduces a base attention model in Section 3.1 that explicitly represents node features as polynomials, with coefficients determined by attention scores. This is claimed to result in high polynomial expressivity (in Sec. 3.1). To make the model equivariant, the paper integrates graph topology and node features into the polynomial coefficients to derive local and global attention models. This makes the overall Polynormer architecturem which achieves linear complexity instead of quadratic. Experiments are performed on 13 datasets including both homophilic and heterephilic tasks where Polynormer improves on most datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n- Provides theoretical analysis of polynomial expressivity, though restricted to scalar features. It goes beyond the WL expressivity as common in graph learning literature.\n- Demonstrates the performance of the architecture on 13 datasets where comparisons with baselines make the proposed model better on 11 datasets.\n- Ablation study on a smaller dataset group shows benefits of global attention and local-to-global scheme."
            },
            "weaknesses": {
                "value": "Weaknesses and Questions:  \n-The theoretical expressivity claims in Section 3.1 may be overclaiming capabilities, as proofs make simplifying assumptions about scalar features that differ from real graph data (Section 4). Can this be justified further?   \n-While complexity is analyzed, runtime and memory usage are not empirically compared to baselines in Section 4.2 to demonstrate scalability."
            },
            "questions": {
                "value": "included with weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698919298379,
        "cdate": 1698919298379,
        "tmdate": 1699636248489,
        "mdate": 1699636248489,
        "license": "CC BY 4.0",
        "version": 2
    }
]