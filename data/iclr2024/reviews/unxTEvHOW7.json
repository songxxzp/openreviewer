[
    {
        "id": "6a63l82AGA",
        "forum": "unxTEvHOW7",
        "replyto": "unxTEvHOW7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_Diur"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_Diur"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an explanation approach for memes, which provides a more rich set of relevant keywords with the goal of providing richer background information about the semantics of an input meme. The proposed approach is leveraging multimodal encoding and classification through a Language Model (GPT2) with the goal of generating a set of supporting keywords that are not necessarily appearing in the input image, but are inferred (with the help of GPT2) to be relevant to the input meme. Quantitative and qualitative evaluation provides evidence in favour of the good performance of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Motivation and problem formulation are well described.\n- Several of the proposed idea elements, e.g. prediction preservation, are quite original and well integrated in the overall framework.\n- The experimental results appear promising."
            },
            "weaknesses": {
                "value": "- There are certain major methodological issues (cf. Questions).\n- The related work coverage is insufficient.\n- The quality of the manuscript is below publication standards."
            },
            "questions": {
                "value": "The motivation behind this work is clear and very relevant. However, a high-level description/explanation of the inner workings of the proposed method is missing from the introduction.\n\nThe literature review of multi-modal hate speech detection is rather poor, comprising all in all 9 lines, with half of them being about text-based detection.\n\nThe proposed idea is quite similar to concept bottleneck models; the authors could at least add a reference and discuss the differences between the two approaches.\n\nThe quality of writing requires considerable improvement.\n\nThe methodology presentation is rather unclear and high-level, which makes it very difficult to understand the details of the implementation and to connect the pieces.\n\nTo me there are a couple of methodological flows:\n1) The fact that GPT predicts certain keywords (based on the image, text, image-caption features & prediction embeddings) does not serve as an explanation of how the two modalities interact in order to produce hate or not. It just automatically provides the context around the meme. Additionally, based on own experience and experiments with OFA to caption memes, the result is often bad, most of the times unsuccessfully trying to OCR the meme instead of providing a good caption about the background.\n2) The biases of GPT model are not alleviated of accounted for in this analysis. It is used as if it was perfect.\n3) In general, a set of keywords is hardly sufficient to capture the semantic nuance that most memes carry. In the age of LLMs, one would expect an explainability approach to rely on some more sophisticated generative method to produce explanations that are readable.\n\nThe authors claim that the method is applicable to other contexts/tasks but give no evidence of that. Therefore, I feel they should remove this from the list of claimed contributions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Reviewer_Diur"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698057882479,
        "cdate": 1698057882479,
        "tmdate": 1699636952793,
        "mdate": 1699636952793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zXN9E2zUSN",
        "forum": "unxTEvHOW7",
        "replyto": "unxTEvHOW7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_HAZ8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_HAZ8"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenge of detecting offensive content in memes, highlighting the shortcomings of binary classification and the need for reliable and unbiased classifiers. The authors propose a novel approach for extracting meaningful and interpretable tokens reflecting the meme\u2019s content, enhancing model transparency and user trust. Their proposed approach constitutes four different stages designed towards the objective of extracting meaningful tokens. The stages include: (a) selecting the most relevant keywords, (b) filtering out data points outside a neighborhood of relevance, (c) ensuring semantic relevance within the candidate keywords, and (d) retaining output-preserving tokens from the candidates as the final set. Their method outperforms conventional interpretability baselines, contributing to meme content analysis and developing interpretable multimodal systems."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to understand. The proposed method is intuitive and built over multiple connected modules. \n\n2. The approach of using auto-regressive language modeling loss, towards generating classification labels while learning to explicitly induce binary signals as part of the modeling approach is interesting. Although, it's a bit of a question itself as to the reliability of manually mapping these signals to the discrete labels, and then further using them for generative LM-based processing. Hence, it could add to the existing limitations of the approach."
            },
            "weaknesses": {
                "value": "1. A rudimentary approach to explainability and contextualization (being discussed from the lens of interpretability) within the context of memes.\n\n2. Although the framework *EXPLEME* extracts implicit but relevant keywords w.r.t. the input meme, this at times might not be sufficient to assist in the process of interpreting the model decision, as it could be viewed as adding additional related keywords, yet without imparting a coherent context to the intended implicit content. \n\n3. The approach may have some limitations in terms of generalizing for harmfulness categories within the wild (prior distributions are unknown), as the label signals depend on a \u2018dix\u2019 (category mapping dictionary). It would be better to have more insights into these aspects.\n\n4. The generalizability of the proposed approach, even though an aspect that can surely be investigated, given the variety of the harmfulness and thematic domains and meme design types, is severely lacking in this work.\n\n5. Interpretability is typically examined while understanding or probing the model decisions (predictions) and breaking down the understanding in terms of the steps or key attributes of the modeling approach (framework) that led to a particular output. This also involves looking at and considering the given input in an as-is manner.\nWhile your work does explore the aspect of understanding the model output in a better way, it does pose the question of whether it is the interpretability or explainability that it primarily intends to address. Even Hase et al., 2020, as cited in the paper, trace the inspiration of LAS to interpretability but build the motivation based on the lack of work towards the \u2018simulatability\u2019 of NL explanations, which is a distinct notion from conventional interpretability. \nFrom what I see it, generating semantically relevant textual cues by leveraging the inherent sampling strategy fundamental to generative LM might not necessarily facilitate model interpretability and could offer some form of explainability in some sense (maybe the term simulatability could be popularized with such paradigms), which has been recently reported in the literature to have made significant strides into, in terms of contextualizing the implicit content conveyed within memes [1] and Desai et al., 2021 and Sharma et al., 2022, as has also been cited in the current work\u2019s literature, essentially questioning the research-gap established.\n\n6. With noteworthy strides demonstrated by multimodal LLMs (MLLMs) like LLaVA [2], miniGPT4 [3], etc., apparently, there is no reference, citation and discussion, w.r.t positioning the proposed solution or comparison with it. As multimodal LLMs have demonstrated remarkable capacity towards not only describing the surface level details pertaining to visual-linguistic grounding but also subtle nuances (reasoned via the LLM attached) conveyed/implied within multimodal content, it becomes imperative to factor in their role while investigating solutions towards interpreting or contextualizing meme-like multimodal content.\n\n7. The direct limitation of not examining or leveraging the latest capabilities of MLLMs is also observed in Table 3, third and fourth examples, which are classic examples of nuances involved in typical memes, which the proposed approach fails to resolve.\n\n[1] MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization (Sharma et al., ACL 2023)   \n[2] https://llava-vl.github.io/\n[3] https://minigpt-4.github.io/"
            },
            "questions": {
                "value": "**Q1** *Introduction, third para, third sentence*: The text claims that the keywords linked to model predictions often don't semantically match the input meme. It's unclear if this assertion is based on empirical evidence or previous literature, and a citation for support is recommended. Additionally, while the importance of examining methods to contextualize textually expressed harm is highlighted, what is the take of authors on the consideration of implicit visual cues in memes and their integration into the proposed model framework?\n\n\n**Q2** Is there anything else, in addition to contextualizing memetic phenomena under consideration, that your work is studying on a broader level? As taking a quick look at the outset of Desai et al., 2021 and Sharma et al., 2022, and their main objectives, they do seem to address multimodal contextualization for memes due to obscured meaning. So stating that \u201cexisting methods cannot fully explain the model behavior that is not directly related to the input but has some hidden meaning\u201d might not help in positioning your attempts towards a possible gap that previous works seem to have touched upon. \n\n\n**Q3** *Methodology*, \n(a) first line: \u201cThe proposed systems combine\u201d \u2192 Are there multiple \u201csystems\u201d that you\u2019re proposing? Or your proposed approach/methodology/\u201dsystem\u201d combines a multimodal encoder and a language model. This is in line with the phrase \u201cOur system follows a two-step strategy\u2026\u201d in the very next para. Please clarify and streamline.\n\n(b) last line: \u201cThe incorporation of LM enables us to retrieve the set of explainable out-context keywords that are helpful in interpreting the system and its outcome.\u201d \u2192 How do you position your goals and attempts against the recent developments within the field involving multimodal LLMs? How does your proposal compete/position/add on to what such multimodal LLMs can achieve?\n\n\n**Q4** *Dimensions of the multimodal encoding:* How is it that you are working with the dimensions m X 1 and n X 1 for f_{t} and i_{t}, respectively, when one of the standard variants of pre-trained CLIP model: \u201copenai/clip-vit-base-patch32\u201d, renders a common dimension of 512, representing the joint multimodal representational space? Moreover, even if you are working with m and n as the first dimension sizes of these features, how come both U and V (trainable weight matrices) have x X ko as dimensions when your features represent different sizes? Kindly resolve the ambiguity.\n\n**Q5** *Classifying via LM:* \n(a) \u201cli = argmax(FFN(Mt),dim = 1)\u201d: What is the motivation behind employing an argmax, followed by explicit mapping of the signals to either Offensive vs normal, as against learning representations that could directly be used to condition the LM (GPT2) output? The question is additionally motivated by the general idea that performing argmax operation over FFN output is not typically recommended, as the model is usually trained w.r.t. a smooth loss function and doesn\u2019t necessarily perform hard classification. Did you examine your intermediate signal outputs? Any empirical insights probing this aspect would shed some more light on the questionable reliability of this approach. \n\n(b) \u201clab = SumPool(gl \u25e6 E[dix[l]])\u201d: Does the small circle operator bw the gl and E terms represent element-wise multiplication? In either case, what is the motivation behind implementing interaction between gl output and E terms, when there\u2019s no jointly learnt connection between the two? The effect intended to be captured via this operation isn\u2019t super clear.\n\n**Q6** *Sec 3.2, point about Semantic Relevance,* \n(a) \u201cthe meme is encoded using the CLIP Vision encoder\u201d: So the dot product computed bw CLIP text encoding of the keywords from the second step and CLIP visual embedding of the meme image doesn\u2019t factor in the CLIP textual encoding of the meme text? Wouldn\u2019t this lead to a relatively lossy multimodal embedding towards examining the semantic relevance?\n(b) \u201cFirst, we use the trained LM in inference mode\u201d: Is it the LM trained as part of your experiments or the standardized (pre-trained/fine-tuned) one? This isn\u2019t completely clear, and it could have implications on the type of output to expect.\n\n**Q7** *Sec. 3.2*, \n(a) \u201cIf the model predicts the same class as it predicted before\u201d: As per my understanding, the previous prediction being referred to here in the LM\u2019s primary output is to be considered as predicted (generated) label, thereby not suggesting any grounding w.r.t. the ground truth label. Can prediction flip for some scenarios suggest rectification of the previously incorrectly predicted (generated) label? Need more clarity here.\n\n(b) \u201ctoken does not have enough importance\u201d: Do you observe any effect due to missing modality here? As when you reinforce your gen-LM\u2019s output as knowledge bite in text-only form, there could be potential meme scenarios, where you end-up losing key information in terms of the intended subsequent LM\u2019s conditioning.\n\n**Q8** *Sec. 3.3, Alignment vs Optimization tradeoff,* \n(a) \u201cIn practical applications, this serves as a filtering mechanism to retain tokens relevant to regions\u201d: Is this an empirically verified finding or an established fact, in which case proper citations are a must?\n\n(b) \u201cWe term this phenomenon the \u2018Alignment vs. Optimization Trade Off Criteria\u2019\u201d: How does your proposed approach, in its scope, offer functionality or results in ways any different from the conventional implications posed by the standard \u201cAlignment vs Optimization tradeoff\u201d?  \n\n**Q9** *Sec 4.2.1, Comparison with the baselines:* I might have missed it, but what are the explanations being used for the explanation-based evaluation (F1 w/ exp) of the random/baselines? I presume the ones obtained (extracted) using the proposed approach are used for evaluating proposed (and variants). Now whatever may be the case for baselines, is it due to the ineffectiveness of the explanation derivation mechanism (hence the generated explanations) or of the interpretability model baselines themselves?  \n\n\n**Q10** As part of the results (Table 1), it is interesting to observe a consistent range of the diversity scores (inter/intra) and other metrics as well for e-ball enabled vs disabled scenarios, with top K enabled. How would the authors justify the relevance of the e-ball constraint as part of the proposed approach, with such consistent and barely distinct reproducibility with and without it?\n\n**Q11** *Analysis, Does epsilon ball constraint\u2026, page 7,* \n(a) First line, \u201cWithout any \u03b5 constraint, we obtain a negative LAS score along with a low \u2018comprehensiveness\u2019 score.\u201d: Table 1 suggests more on this, with higher ( and non-negative) LAS and comprehensiveness scores, without e-constraint cases (but with top-K enabled). Please resolve the ambiguity, and clarify the confusion.\n\n(b) Last three lines: The optimal value of epsilon observed (0.01<0.05), suggests a smaller neighborhood, as an ideal scenario for better quality keyword extraction. On the contrary, the corresponding theoretical justification stated for it implies near orthogonality b/w e and delta_{m}f(m) components. Do the two have any direct connection in this scenario that I missed? \n\n\n**Q12** *Fig. 2 and Analysis point # iv* \u201cWhat is the similarity of the retrieved\u2026\u201d, page 8, third line: Firstly specifying that [-E,+E] represents E-neighbourhood, then suggesting Jaccard Similarity spikes at [-0.01, +0.01], renders reading Fig. 2 slightly difficult, as your y-axis is JS, and your x-axis has range [1,9], with x-axis label as E-neighbourhood. How to map the range of [-0.01, +0.01] on either of the axes with these configurations?\n\n**Q13** Table 3, example 3 (91768), CLIP-only stage derives \u201cjew, jew, jew, jew\u201d, and ends up misclassifying a normal meme as an offensive one. Since your proposed approach constitutes several filtering stages, would considering some additional steps like post-processing by deduplicating the keywords generated (like in this case), would have facilitated the required diversity for it to be correctly classified? \n\n\n**Q14** *Sec 4.3, last line:* Could a directly learnt input representation for GPT2 have given better results, as compared to an explicit transformation based categorical signal? Was the alternative explored as part of the investigation?\n\n\n**Q15** Use Case: EXPLEME is designed to filter keywords that are directly linked with appeared entities in the meme. But if the same entity is used in another context, then will EXPLEME be able to perform well? For example, a meme contains an image of Donald Trump\u2019s angry face towards the Mexico border, and simultaneously, the other side of the meme contains the image of Hitler\u2019s smirking face with the text \u2018Need suggestions !\u2019. Although Hitler had no direct connection with Mexico and its border-related issues, when the smirk of Hitler is merged with the angry face of Donald Trump, it produces an implicit hate meme. Will EXPLEME be able to generate related keywords that connect both cases?\n\n**Q16** Use Case: An entity such as a celebrity or a politician can be used in both positive and negative roles based on the context. If the entity has a biased association towards the negative connotation (example, Hitler) or a positive one (example, Mahatma Gandhi) but has been used in the opposite connotations, then would EXPLEME be able to suggest insightful interpretation/contextualization?\n\n**Q17** The process of collecting external knowledge texts requires more elaboration. External knowledge snippets are fetched based on predefined tokens. Based on what heuristics are the tokens selected? How do the authors go about augmenting the knowledge with a few tokens that are related to the implicit facts?\n\n\n**Suggestions/Clarifications**\n\n**S1** *Abstract:* \n(a) \u201cHowever, binary classification of memes as offensive or not often falls short in practical applications.\u201d \u2192 This can be generalized well beyond \u201coffense detection\u201d,  like for other hate speech related aspects/categories as well\n\n(b) \u2018\u201ctokens\u201d from a global vocabulary\u2019 \u2192 It  might be prudent to characterize the scope of the vocabulary within the context of utilizing a particular LM in such scenarios, rather than suggesting it to represent a global vocabulary, which of course can have different technical implications. \n\n**S2** Introduction, second para, last line, \u201c...kind of inadvertent...\u201c: \u201c\u2026kind of inadvertent biases\u2026\u201d?\n\n**S3** *Related Work,* \n(a) \u201cwhen kiela2021hateful introduced a set of benchmarks\u201d: Citation format issue.\n(b) first para, last line: \u201cThis led to a number of research on de- tecting offensiveness in multimodal media, particularly in memes particularly in memes(Sharma et al., 2020; Kiela et al., 2021; Suryawanshi et al., 2020).\u201d \u2192 There could be better citations supporting the argument of the follow-up developments, after Kiela et al., 2021. \n\n**S4** The first formal mention about the task being addressed and the dataset being utilized is mentioned as part of the Sec 3, Classifying via LM and Sec 4.1, Experimental Setup. It is advisable to mention both at an earlier stage of the write-up to build the necessary backdrop, essential towards a complete understanding of the work and methodology.\n\n**S5** You are addressing the task of offensive vs normal, upon which your entire contextualizing keyword extraction steps depends. It might not be recommended to conflate the concepts of \u201chate speech\u201d and \u201coffense\u201d, when modeling one phenomena, while working with the dataset, build for the other. Offense is a broader term, and hate speech may involve some form of offense, but converse isn\u2019t always true.  \n\n**S6** Sec 3.2, third last line, \u201clinguistic tokens beyond those originally within\u201d: Rephrase?\n\n**S7** Sec 3.3, Alignment vs Optimization tradeoff, page 4, sixth last line, \u201cgradient-based optimization steps of m\u201d: or \u201cgradient-based optimization steps WITH RESPECT TO m\u201d?\n\n**S8** You invest reasonable effort towards theoretically establishing and then analyzing alignment-related aspects of the trade-off while not so much on the optimisation front. Any relevant presumptions I missed in the write-up?\n\n**S9** Sec 4.2.1, Comparison with the baselines, fifth line, \n(a) \u201cproposed approach by resorting to better obtaining scores\u201d: Check the usage of the word resorting in this statement.\n\n(b) \u201cmodel in the next 10 rows,\u201d: Table 1 shows 11 rows with similar configurations after the top section involving comparative baseline systems. The table can surely use better segregation and markers highlighting the proposed approach and its variants to avoid any confusion.\n\n**S10** Fig. 3, Caption, various TopK and E values: Is the intention to probe various E values or simply compare E-constraint enable vs disable with the optimally set value of E? Kindly clarify.\n\n**S11** An ablation probing the efficacy of the proposed model without the maximal and semantic relevance stages and only with epsilon-constraint with varying values of E would also be interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7795/Reviewer_HAZ8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698292355503,
        "cdate": 1698292355503,
        "tmdate": 1699636952665,
        "mdate": 1699636952665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3W6SpdlrLS",
        "forum": "unxTEvHOW7",
        "replyto": "unxTEvHOW7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_BdWE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_BdWE"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a novel approach to extracting meaningful tokens from a global vocabulary for effective model interpretability for memes. They demonstrate the effectiveness of their approach on the Facebook Hateful Meme Dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written. The proposed approach seems task-agnostic and would be relevant for model interpretability across meme understanding tasks such as harmfulness detection, offensiveness detection, etc. The authors perform an extensive evaluation to demonstrate the relevance of their approach."
            },
            "weaknesses": {
                "value": "It would be interesting to see the variation in the experiments in terms of the language model used (GPT2 currently), dataset for evaluation, tasks, etc. The proposed approach can be demonstrated to work across tasks, datasets, and language models (as claimed in the introduction/contribution)."
            },
            "questions": {
                "value": "In the human evaluation, were the evaluators aware that they were evaluating a particular method (Ours, \u03f5-ball, CLIP, Integrated Gradient)?\n\nAlso, I think that a section on the limitations of the proposed approach can be added (would be relevant for future works)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826321921,
        "cdate": 1698826321921,
        "tmdate": 1699636952536,
        "mdate": 1699636952536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X3xhYvcaIP",
        "forum": "unxTEvHOW7",
        "replyto": "unxTEvHOW7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_VNYw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7795/Reviewer_VNYw"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method to automatically understand the content of a meme, which is a difficult task, because the text alone can be misinterpreted if the image is not taken into account (as the authors demonstrate with an example). Furthermore, the authors derive the mathematics for merging the text and image understanding and give a proof of their theorem. Finally, various experiments are conducted and analysed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents maths and prove their theorem. Unfortunately, I wasn't able to fully check the math, and hence, also not able to fully verify and understand the results."
            },
            "weaknesses": {
                "value": "The human classification experiments are conducted by the authors of the paper. This should not be the case."
            },
            "questions": {
                "value": "I don't have any question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7795/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699361820446,
        "cdate": 1699361820446,
        "tmdate": 1699636952368,
        "mdate": 1699636952368,
        "license": "CC BY 4.0",
        "version": 2
    }
]