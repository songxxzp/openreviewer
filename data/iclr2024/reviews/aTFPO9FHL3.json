[
    {
        "id": "8CqsWnd6sQ",
        "forum": "aTFPO9FHL3",
        "replyto": "aTFPO9FHL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_787D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_787D"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Todyformer, a novel Transformer-based neural network for dynamic graphs, to address the problems of over-smoothing/squashing caused by Dynamic Message-Passing Neural Networks and learning long-range dependencies. The experiments of future link prediction and node classification are conducted to verify its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tA novel Transformer-based neural network is proposed for dynamic graph representation learning. \n2.\tThe proposed TodyFormer achieved the best performance over 9 baselines on both transductive and inductive link prediction experiments."
            },
            "weaknesses": {
                "value": "1.\tTo the best of my knowledge, there is no study pointing out existing well-known temporal graph neural networks (TGNN) have the problem of over-smoothing. I also do not see any theoretical or experimental evidence on the over-smoothing problem of TGNN. Therefore, the motivation of this paper may be not solid. \n2.\tThere are some existing works studying leveraging Transformer for dynamic graph learning, e.g., APAN [1], DygFormer [2]. What are the advantages of the proposed Todyformer over these methods? \n3.\tThe inputs of TodyFormer are edges within a temporal window. How to set the size of this window? Does it mean that the edges in the previous time will see the edges in the latter time in the window (information leakage)? How do you prevent the information leakage problem? \n4.\tThere are a lot of symbols used in the equations without detailed explanation, which makes it really hard to understand. For example, what is P, c in Eq. (4) ? what is positional encoding P in Eq. (5) ? what is n, e in Eq. (7) ? \n5.\tFrom ablation study (Table 4), there is really slight difference after removing modules of TodyFormer? Even if all the modules are removed, TodyFormer still has very high performance (e.g., 0.987 on social evolution). I do not understand which modules contributes the such high performance? In addition, more ablation study on other modules should be studied, e.g. (replace node2vec encoding with others). \n6.\tMore sensitivity analysis on other hyper-parameters should be conducted.\n7.\tThere is no training/testing time comparison with other baselines. \n8.\tThere are many spelling and grammar and Latex errors in this paper. Please check the whole paper carefully."
            },
            "questions": {
                "value": "1.\tIn section 3.2, how do you partition the input graph into M non-overlapping subgraphs?\n2.\tIn Eq. (3), what is s^l? Why H^l is the combination M node embeddings? There seems a contradiction as the author stated M are the number of subgraphs stated in previous section. \n3.\tIn Figure 3, the results of left and right subgraphs seem contradict. On the left, the window size of LastFM is 144, and the AP score is larger than 0.975. On the right, when windows size of LastFM is less than 50k, it seems the AP score is less than 0.95. Why is that? Besides, as my comment 3, such large window size may cause severe information leakage.\n4.\tIt is really wired that on 4 of 5 datasets, TodyFormer has the AP score over 0.99 (Table 1)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698203154831,
        "cdate": 1698203154831,
        "tmdate": 1699636370997,
        "mdate": 1699636370997,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xJOEabfNsn",
        "forum": "aTFPO9FHL3",
        "replyto": "aTFPO9FHL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_dSfv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_dSfv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Transformer-based architecture for dynamic graphs, namely Todyformer. Experiments demonstrate that Todyformer outperforms the state-of-the-art methods on some datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. The authors claim that Transformers demonstrate exceptional computational capacity, compared with Graph Neural Networks. However, the computational complexity of GNNs and traditional Transformers is $O(N)$ [1]  and $O(N^2)$ [2] respectively, where $N$ is the number of nodes in the input graph. I suggest reporting the complexity and runtime in experiments.\n2. Todyformer is similar to Vision Transformer, while the authors do not provide the necessary analysis in terms of graph learning. Some suggestions are as follows.\n\t1. The authors may want to analyze the expressiveness of Todyformer in terms of sub-structure identification and the Weisfeiler-Leman (WL) test.\n\t2. The authors may want to analyze how and why Todyformer alleviates over-squashing and over-smoothing.\n3. Please explain why the baseline results in Table 2 are different from those from Table 3 in [3].\n4. The authors claim that the significantly low performance of Todyformer on the Wiki and Review datasets is due to the insufficient hyperparameter search. However, in my opinion, hyperparameter tuning is difficult to improve 3% accuracy.\n5. Please report the standard deviation in Table 2 following the baselines. Moreover, I suggest reporting a statistically significant difference.\n\n\n\n[1] Recipe for a General, Powerful, Scalable Graph Transformer. NeurIPS 2022.\n\n[2] NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification. NeurIPS 2022.\n\n[3] Temporal Graph Benchmark for Machine Learning on Temporal Graphs https://arxiv.org/pdf/2307.01026.pdf"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651332462,
        "cdate": 1698651332462,
        "tmdate": 1699636370927,
        "mdate": 1699636370927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "unrdhx8kba",
        "forum": "aTFPO9FHL3",
        "replyto": "aTFPO9FHL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_Nqpw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_Nqpw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel graph transformer method for dynamic graph. This model is an encoder-decoder architecture, started from patch generation and based on alternating between local and global message-passing as graph transformer. Authors perform two downstream tasks including future link prediction and dynamic node classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1). This paper presents a graph transformer method on the dynamic graph, with enough experiments and ablation to show its performance.\n(2). The purpose of using graph transformer is clearly discussed in Section Introduction and Related Work."
            },
            "weaknesses": {
                "value": "(1). Some presentations need improvement. Some notations need to be clear and some formulas need to be written for clarity. For example, what is the notation a,b means in E = |{e_a,...,e_b}| in section 3.2? What is the specific formula for positional encoding o() in section 3.4? For the transformer formula (6), could you specify whether there are LayerNorm and Feed-Forward modules as transformer? Could you give the formula of Q, K, V, and their dimension for clarity?\n\n(2) I wonder about the results of Wikipedia and Reddit you mentioned in Section 4.1 datasets as it\u2019s not shown in Table 1. As for the results shown in the Appendix, it seems they are not strong enough in the Inductive setting, especially for the Reddit dataset, which makes the statement in section 4.2\u201cTodyformer consistently outperforms the baselines\u2019 results on all datasets.\u201d misleading."
            },
            "questions": {
                "value": "(1). In Section 3.2 PATCH GENERATION, could you please give more analysis of why you use this patch generation method, rather than other existing methods such as METIS in Graph ViT/MLP-Mixer or Graph coarsening in COARFORMER? What\u2019s the advantage of your patch generation method?\n\n(2). Section 3.5 \u201cThis issue is magnified in dynamic graphs when temporal long-range dependencies intersect with the structural patterns.\u201d Could you please give some analysis of it or give some examples or literature to show the importance of over-smoothing problems in dynamic graphs? This can make this paragraph more convincing.\n\n(3). Could you please show your efficiency comparison against other methods, especially CAW and Dyg2Vec? In my opinion, the computation of graph transformer on each node could have high time complexity, could you please analyze it?\n\n(4) Could you give more analysis for the experiments weaker than the baseline? For example, in the second set of experiments, why did this method fail in these two smaller datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673958533,
        "cdate": 1698673958533,
        "tmdate": 1699636370852,
        "mdate": 1699636370852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vJnzQwqo0w",
        "forum": "aTFPO9FHL3",
        "replyto": "aTFPO9FHL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_1VfV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4066/Reviewer_1VfV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes TODYFORMER, a novel Transformer-based neural network tailored for dynamic graphs. It unifies the local encoding capacity of Message-Passing Neural Network with the global encoding of Transformers.\nExperimental evaluations on public benchmark datasets demonstrate that Todyformer consistently outperforms the state-of-the-art methods for the downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and easy to follow.\n2. The results of different downstream task prove the validity and superiority of model, especially the results on large scale datasets."
            },
            "weaknesses": {
                "value": "1.\tThis paper does not provide the mathematical form of the overall loss function, which leads to an incomplete explanation of the model in Section 3.\n2.\tIn the detail of three main components, many ideas are not novel. For instance, in encoding Module, the Transformers is very basic model. The window-based encoding paradigm is from DyG2Vec. The positional-encoding idea is also from previous work."
            },
            "questions": {
                "value": "1.\tCould you add mathematical form of the overall loss function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4066/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4066/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4066/Reviewer_1VfV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676025703,
        "cdate": 1698676025703,
        "tmdate": 1699636370751,
        "mdate": 1699636370751,
        "license": "CC BY 4.0",
        "version": 2
    }
]