[
    {
        "id": "REQ7i1JrZ6",
        "forum": "8Cc6qOPvFo",
        "replyto": "8Cc6qOPvFo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
        ],
        "content": {
            "summary": {
                "value": "This work presents a training-free approach for text-driven image-to-image translation, building on a pre-trained text-to-image diffusion model. The authors revise the process to align better with the target task. They introduce a new guidance objective, which combines maximizing similarity to the target prompt (measured by CLIP score) and minimizing the distance to the source latent variables. Moreover, they employ a cycle-consistency objective to maintain the source image background by iteratively optimizing source and target latent variables. Experimental results demonstrate the exceptional performance of this method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The article introduces a simple yet effective approach for text-driven image-to-image translation. \n1. In contrast to other methods, this approach places a strong emphasis on preserving the structure and background of the source image during image editing. It accomplishes this by revising the process for generating target images to better align with the target task.\n2.  The article introduces a new guidance objective that combines maximizing similarity to the target prompt (measured by CLIP scores) and minimizing the distance to the source latent variables, resulting in improved quality of generated outputs.\n3. To maintain the background of the source image, the article utilizes a cycle-consistency objective. This involves iteratively optimizing source and target latent variables, enhancing the feasibility of the method."
            },
            "weaknesses": {
                "value": "I find this method to be intuitive, but it appears to lack enough technical innovation, as similar concepts have been previously mentioned in prior works. My primary concerns are related to the experimental aspects:\n\n1. The authors should also conduct experiments on some of the datasets or images provided in their previous work.\n\n2. The quantitative experiments in the study appear to be insufficient. Since the authors have collected a dataset, it would be better for them to report average metrics on this dataset.\n\n3. There is a shortage of comparison with other methods. Given the wide attention in this field, it would be beneficial to compare this approach with more recent works. It is also better to include some fine-tuning-based methods (like SINE, Text-Inversion)  to provide a more comprehensive evaluation.\n\n4. The running costs, such as time and GPU resource consumption, should be reported and compared to help readers understand the resource requirements when using this method.\n\n5. The authors have not listed the limitations of their method. As this approach is positioned as a general method, it is essential to clarify whether it supports general scenarios, like the removal or addition of specific elements in the target image, to better inform users about its applicability.\n\n6. When comparing \"Prompt-to-prompt,\" it seems that the authors have not adopted a strategy that specifically considers the background region. This might impact the accuracy of the experimental results."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698577015367,
        "cdate": 1698577015367,
        "tmdate": 1699635994261,
        "mdate": 1699635994261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qqTM0DkRqb",
        "forum": "8Cc6qOPvFo",
        "replyto": "8Cc6qOPvFo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission669/Reviewer_GXjj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission669/Reviewer_GXjj"
        ],
        "content": {
            "summary": {
                "value": "The paper  presents a training-free approach for text-driven image-to-image translation using a pretrained text-to-image diffusion model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper introduces a new guidance objective term, which combines maximizing similarity to the target prompt (based on the CLIP score) and minimizing the distance to the source latent variables.\n\n2.Unlike many existing methods based on diffusion models, the paper leverages a cycle-consistency objective to preserve the background of the source image."
            },
            "weaknesses": {
                "value": "1. The time consumption of the proposed method compared to other methods should be given.\n\n\n2. Comparable works such as \"Negative-prompt Inversion\" and \"Null-text Inversion for Editing Real Images using Guided Diffusion Models\" demonstrate robust image reconstruction and content editing capabilities while preserving the original background. These works also support flexible target category transformations. A comprehensive comparison with these similar works could further support the paper's novelty and performance in relation to existing solutions.\n\n3. A user study is encouraged to be carried out."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760747001,
        "cdate": 1698760747001,
        "tmdate": 1699635994174,
        "mdate": 1699635994174,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RqIN5Yth7L",
        "forum": "8Cc6qOPvFo",
        "replyto": "8Cc6qOPvFo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a unique method for free text-driven image editing by utilizing pre-trained text-to-image diffusion models. Central to this approach is a new guidance objective term, which maximizes similarity to the target prompt (as opposed to the source prompt) based on the CLIP score. In tandem, it minimizes the distance to the source latent variables. Additionally, the authors incorporate a cycle consistency objective to retain the background details."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Simplicity & Effectiveness**: The proposed method is both straightforward and seemingly efficacious, as evidenced by the results presented in the paper."
            },
            "weaknesses": {
                "value": "- **Evaluation Methods**: The evaluation could be more robust. The prompts used for evaluation are closely related to the original noun, reducing diversity and potentially biasing results.\n- **Aspect Ratio Concerns**: The samples used for evaluation have been altered from their original aspect ratios. This could inadvertently disadvantage competing methods.\n- **Comparison Choices**: The results from prompt-to-prompt evaluations seem to perform well on generated images rather than inverted real ones. The absence of a comparison with Null-text-inversion, which might be a more apt benchmark, raises questions.\n- **Efficiency Metrics**: The paper would be more informative with a runtime efficiency comparison against other methods."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803186839,
        "cdate": 1698803186839,
        "tmdate": 1699635994073,
        "mdate": 1699635994073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VCdh9ZTrHj",
        "forum": "8Cc6qOPvFo",
        "replyto": "8Cc6qOPvFo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for editing without training using additional cycle-consistency and triplet-based distance guidance. The triplet-based distance ensures that source and target images at the same time step are mapped closer together than those at different time steps, in addition to using a general feature similarity-based distance. The cycle-consistency objective is employed to ensure that two images, one with guide in the forward process and the other with guide in the backward process, produce identical results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method produces better structure-preserved editing results. Also, compared to other training-free algorithms the proposed method achieves better quantitative results."
            },
            "weaknesses": {
                "value": "Cycle-constistency may overly fix the structure and may make the result unnatural with object with different structure. Also, the argument that different time-step target images should be farther apart than same time-step source and target images seems to lack sufficient justification. And there is no comparison with papers such as null-text inversion."
            },
            "questions": {
                "value": "It seems there are only subtle differences with naive distance and the triplet distance guidance results. Is there more basis for the triplet loss that makes different time-step images of the same image distant from each other?\nAlso, how does the performance compare to recent papers such as null-text inversion and similar approaches?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834773122,
        "cdate": 1698834773122,
        "tmdate": 1699635993993,
        "mdate": 1699635993993,
        "license": "CC BY 4.0",
        "version": 2
    }
]