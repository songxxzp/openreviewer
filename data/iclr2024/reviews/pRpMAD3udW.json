[
    {
        "id": "kCnZU64274",
        "forum": "pRpMAD3udW",
        "replyto": "pRpMAD3udW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_yFyT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes new learning methods to master simulated tabletop robot manipulation from multi-modal prompts. Specifically, their method involves two stages, first inverse-dynamics pretraining then multi-task finetuning. State-of-the-art results are demonstrated on the multimodal prompt benchmark VIMA-BENCH. Furthermore, authors conducted ablation studies to justify the effectiveness of design choices and showcase in-context learning ability achieved by the trained model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is effective, as demonstrated by its new SOTA performance on VIMA-BENCH.\n- Comprehensive ablation studies draw insights into the effectiveness of proposed method.\n- Demonstrated in-context learning ability described in Section 4.3 is interesting and impressive.\n- The paper is well-written and presented."
            },
            "weaknesses": {
                "value": "- Albeit the method is interesting and demonstrated improvement is impressive, the proposed method is only evaluated on a single benchmark. It would be more solid if authors cloud show similar improvement on other robot learning benchmarks such as RLBench (James et al., 2020).\n- It's totally legitimate for the authors to argue other benchmarks do not support multimodal prompts. In that case, I would encourage authors to extend existing VIMA-BENCH by adding more representative tasks to show the in-context learning ability of models trained with the proposed method.\n- Although this paper is not designed to address real-robot manipulation, showing proof-of-concept demos would justify the feasibility of applying this method on real hardware.\n- Missing citations. Authors are encouraged to discuss the following recent related work:\n\nRadosavovic et al., Robot Learning with Sensorimotor Pre-training, arXiv 2023.\n\nShah et al., MUTEX: Learning Unified Policies from Multimodal Task Specifications, CoRL 2023.\n\n## References\nJames et al., RLBench: The Robot Learning Benchmark & Learning Environment, IEEE Robotics and Automation Letters 2020."
            },
            "questions": {
                "value": "To encode multimodal prompts, the introduced RC provides a direct connection between input embeddings and LM's output embeddings. With this shortcut, is there any performance difference between LMs with varying depth?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698465472726,
        "cdate": 1698465472726,
        "tmdate": 1699636509728,
        "mdate": 1699636509728,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q578pdHu5L",
        "forum": "pRpMAD3udW",
        "replyto": "pRpMAD3udW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new method to learn multi-model prompt robot policy. The  main differences from a prior work, VIMA, are the following:\n\n1. Have a pre-training phrase that pretrains on prompts asking the robot to follow a certain motion.\n2. A new encoding method to encode multi-model prompts.\n3. A method to model the dependency among action dimensions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The pretraining method makes sense in that it uses the implicit motion data in each trajectory as the training signal.\n- The new prompt encoding and action dependency modeling are valid.\n- Presentation of the experiment results are comprehensive, and extensive details are given for the method explanation. \n- Experimentation is rigorous and follows prior benchmarking."
            },
            "weaknesses": {
                "value": "1. The pretraining method is not general enough: it only concern about instruction of \"follow motion for ...\" for a particular motion trajectory, and therefore it mainly tackles the tasks with prompts given a certain motion of a certain trajectory. This means it assumes the task at hand is always similar to follow motion, which is not true.\n\n- An example to illustrate this: it can do well for task T10, but for task T13, when it sweeps something without touching an object, it cannot generalize. \n\n2. For the pretraining method to work, this method also assumes that the prompts contains the motion trajectory keypoint, which is a very narrow assumption and might not always hold. The end users would not be expected to provide the entire trajectories all the time. Therefore the pretraining on motion following is a bit overfitting to the tasks that VIMA designed. \n\n- related to this point: the work advocates for inverse dynamics modeling, but I think this is quite specific to the VIMA-bench task setting with algorithmic oracle. It would be hard to model inverse dynamics in real world.\n\n3. Effectiveness of proposed method: in terms of experiment results (for the full results in Appendix A), there are not significant improvement over VIMA; for those tasks (T10) that has significant improvement, it seems to because the pretraining phase overfits to \"Follow Motion\" task."
            },
            "questions": {
                "value": "1. In Appendix  A page 14, two variations of \"Ours\" are:\n- w/ pretrain\n- w/ Encoder-Decoder\nIs \"w/ Encoder-Decoder\" with or without pretraining? Are these two variations adding \"pretrain\" and \"Encoder-Decoder\" on top of some common method or is one of them adding on top of another?\n\n3. For task T13, could you provide more details on the failure cases of VIMA and this method respectively? Providing some video rollouts of the two methods would be great. \n\n4. The authors are encouraged to provide full experiment results in the main text rather than a portion of it.\n\n5. The conclusion mentioned that the work \"demonstrate the in-context learning capability\". Could the authors elaborate more on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_Y3nY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781031374,
        "cdate": 1698781031374,
        "tmdate": 1699636509629,
        "mdate": 1699636509629,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "33jWYobUAk",
        "forum": "pRpMAD3udW",
        "replyto": "pRpMAD3udW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of multi-modal prompting in \"embodied tasks\", i.e., the combination of language and image to train a model to be capable of multi-tasks. The authors introduced a two-stage training pipeline, in pretraining, using the inverse dynamic modelling loss, and in fine-tuning, using a multi-task imitation loss.\n\nOverall, this paper can be seen as a follow-up of the VIMA[1] paper. Results show a 10% success rate gain in the VIMA benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written. I am glad to read the detailed analysis of the ablation studies. The introduction and related works sections indicate the authors are very familiar with relevant literature."
            },
            "weaknesses": {
                "value": "While this paper looks technically sound to me, I found the small improvements based on the VIMA paper can not be viewed as a significant contribution that is sufficient to be accepted in ICLR. The claimed contributions include (1) a MIDAS training framework, i.e., introducing inverse dynamic modelling loss, page 5 Eq(3) in pretraining + multi-task imitation loss; (2) residual connections in the visual layers; \n(3) a small performance gain (10%) compared to the VIMA paper. However, using inverse dynamic modelling loss and multi-task supervision loss are all intuitive and an easy follow-up step after the VIMA paper. Therefore, the reviewer found the contributions are not sufficient to be published as a long paper in ICLR.\n\nNov 23 update: regarding (3), after reviewing the additional experiments the authors submitted, I think the performance looks good for me. I will raise my score to 5 accordingly."
            },
            "questions": {
                "value": "- The authors add the appendix pages in the main paper, which exceeds the page limits. Please remove the appendix in the revision.\nNov 23 update: Have no concerns after rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_YMJx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698869302092,
        "cdate": 1698869302092,
        "tmdate": 1700756101683,
        "mdate": 1700756101683,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fJupeDDbhh",
        "forum": "pRpMAD3udW",
        "replyto": "pRpMAD3udW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
        ],
        "content": {
            "summary": {
                "value": "Good paper! This paper proposes a new method called MIDAS for robot manipulation with multimodal prompts. The key ideas are:\nA two-stage training pipeline with inverse dynamics pretraining and multi-task finetuning\nAn effective multimodal prompt encoder that augments a pretrained language model with a residual connection to visual features\nModeling action dimensions as individual tokens and decoding them autoregressively\nThe method is evaluated on the VIMA-BENCH benchmark and establishes a new state-of-the-art, improving success rate by around 10%. The ablation studies demonstrate the benefits of the proposed training strategy and prompt encoder design."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The inverse dynamics pretraining is an interesting idea to enable the model to infer actions from visual observations. This facilitates in-context learning from demonstration examples in the prompts.\n\n- Modeling action dimensions independently and decoding them autoregressively is intuitive and shows improved performance.\n\n- Comprehensive experiments on the challenging VIMA-BENCH benchmark with clear improvements over prior state-of-the-art.\n\n- Ablation studies provide useful insights into the contribution of different components."
            },
            "weaknesses": {
                "value": "The prompts are quite controlled during pretraining versus the more complex prompts at test time. It is unclear if the pretraining fully transfers to the downstream tasks."
            },
            "questions": {
                "value": "- For the inverse dynamics pretraining, were other self-supervised objectives explored besides simply reconstructing the actions?\n\n- What stopped the baseline VIMA model from reaching the same performance with just more compute/data?\n\n- Is there other complementary information like force sensors that could augment the visual observations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5152/Reviewer_t7wu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699555637041,
        "cdate": 1699555637041,
        "tmdate": 1699640984454,
        "mdate": 1699640984454,
        "license": "CC BY 4.0",
        "version": 2
    }
]