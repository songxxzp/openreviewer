[
    {
        "id": "lMMnaPceS0",
        "forum": "cP2W2PJtBj",
        "replyto": "cP2W2PJtBj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_4Y5R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_4Y5R"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to apply VLM to detect fake images. They add a pseudo-word S* to the template prompt, and guild the VLM responding 'Yes' or 'No' for real and fake images, respectively. \n\nIn general, detecting fake images with generalizable detector is a popular topic, and the authors have made a nice try for this. However, there are several concerns remain, including issues for main contribution, experiments, datasets, and baselines. Please see the Weaknesses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Fake image detection is a trending topic and utilize VLM to detect deepfakes is a commendable attempt.\n2. The method is straightforward and appears to be effective.\n3. The authors have conducted extensive validation, providing evidence of the effectiveness of their approach."
            },
            "weaknesses": {
                "value": "1. The contributions are somewhat limited. The prompt tuning technique employed in this paper is not a highly original advanced methods, and it lacks sufficient adaptation and analysis in the downstream task, i.e., fake image detection. In other words, the proposed method could be applied to various other visual tasks, raising doubts about its specific contribution to fake image detection.\n2. The experiments lack comparison with SOTA baselines, such as [1], which also focuses on developing a general diffusion detector. Furthermore, there are numerous methods in the field of deepfake detection that specialize in generalization across various forgery types, and these should also be considered in this paper.\n3. Despite the authors create their own dataset, it is advisable to validate their method on other benchmark datasets, such as the one introduced by De-fake. Additionally, the fifth category in the dataset constructed in this paper, namely \"Deeperforensics,\" should be accurately labeled as \"Deepfakes\" or \"Face Swap.\"\n\nConsidering that the proposed approach lacks novelty and suffers from shortcomings in the experimental setup, I currently lean toward a rejection."
            },
            "questions": {
                "value": "Enriching the experiments can enhance the quality of this paper. However, as a detection method, the innovativeness of the approach must be a crucial consideration. I hope the authors can delve deeper into the analysis of the characteristics of forged images to guide the judgment of VLM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2329/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698116723076,
        "cdate": 1698116723076,
        "tmdate": 1699636165329,
        "mdate": 1699636165329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4eN4yFmQ78",
        "forum": "cP2W2PJtBj",
        "replyto": "cP2W2PJtBj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_eMV8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_eMV8"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the potential of using a Visual Question Answering (VQA) model as a deepfake detector and proposes soft-prompt tuning as efficient finetuning method for this purpose. Specifically, the authors finetune InstructBLIP, a VQA model, using soft-prompt tuning to improve its deepfake detection capabilities. And the paper shows that deepfake detection performance of this finetuned model\u2019s is pretty good in various use cases using a generative diffusion model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper innovatively uses soft-prompt tuning to improve deepfake detection performance in a VQA model without altering the original parameters.\n\n2.\tThe paper addresses the issue of deepfake detection across a wide range of applications using diffusion models, currently a topic of active research interest.\n\n3.\tThe paper provides a formal framework for utilizing a VQA model for deepfake detection, and presents the potential of using a VQA model as a deepfake detector and offers a viable finetuning technique for this purpose."
            },
            "weaknesses": {
                "value": "1.\tDeepfake detection is a subject of extensive research with many related papers available. Research works that address the performance degradation on the Diffusion models, and across various cross-datasets, are not incorporated [1,2,3,4,5]. There are also studies that deal with the detection of low-quality, low-resolution deepfakes [6]. There is a need to consider those and compare analysis with other studies.\n\n2.\tThe test dataset used in the paper is biased towards diffusion model-generated data. It would be great to evaluate the performance with well-known other deepfake datasets such as DFC[7], DFDC[8], and FF++[9].\n\n3.\tThe paper could benefit from leveraging unique features of VQA models beyond merely using them as large-scale detectors. For example, experiments that visualize or explain the detection reasoning using VQA's capabilities could offer a significant contribution to the community.\n\n[1]Ma, Ruipeng et al. \u201cExposing the Fake: Effective Diffusion-Generated Images Detection.\u201d ArXiv abs/2307.06272 (2023): n. pag.\n[2] Wu, Haiwei et al. \u201cGeneralizable Synthetic Image Detection via Language-guided Contrastive Learning.\u201d ArXiv abs/2305.13800 (2023): n. pag.\n[3] Wang, Zhendong et al. \u201cDIRE for Diffusion-Generated Image Detection.\u201d ArXiv abs/2303.09295 (2023): n. pag.\n[4] Lorenz, Peter et al. \u201cDetecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality.\u201d ArXiv abs/2307.02347 (2023): n. pag.\n[5] Ricker, Jonas et al. \u201cTowards the Detection of Diffusion Model Deepfakes.\u201d ArXiv abs/2210.14571 (2022): n. pag.\n[6] Le, Binh M., and Simon S. Woo. \"Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[7] benpflaum, Brian G, djdj, Irina Kofman, JE Tester, JLElliott, Joshua Metherd, Julia Elliott, Mozaic, Phil Culliton, Sohier Dane, Woo Kim. (2019). Deepfake Detection Challenge. Kaggle. https://kaggle.com/competitions/deepfake-detection-challenge\n[8] Dolhansky, Brian, et al. \"The deepfake detection challenge (dfdc) dataset.\" arXiv preprint arXiv:2006.07397 (2020).\n[9] Rossler, Andreas, et al. \"Faceforensics++: Learning to detect manipulated facial images.\" Proceedings of the IEEE/CVF international conference on computer vision. 2019."
            },
            "questions": {
                "value": "Please address the comments and questions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2329/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714918239,
        "cdate": 1698714918239,
        "tmdate": 1699636165259,
        "mdate": 1699636165259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g97lSqvixe",
        "forum": "cP2W2PJtBj",
        "replyto": "cP2W2PJtBj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_NiiK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2329/Reviewer_NiiK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to improve the generalizability of fake image detection models by taking advantage of large pretrained vision-language models. Specifically, the proposed method reformulates the fake image detection task as a VQA task, i.e. asking the vision-language model to answer whether the input image is real. To achieve this, the authors proposed insert some learnable task-specific embeddings into the pretrained vision-language model and train these newly-inserted parameters with a prompt tuning algorithm. Experiments on real image datasets and some model-generated fake images show the superiority of the proposed model over existing models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) This paper is well-written. Most of the technical details are clearly presented, it would be easy for readers to understand the proposed method and for followers to reproduce or improve the proposed method.\n\n(2) Formulating the fake image detection task as a visual question answering task is a quite novel attempt in this area. Even though such idea has been adopted other areas, I believe the attempt in this paper should be encouraged.\n\n(3) By applying the proposed method to pretrained vision-language model, the resulted model achieves superior fake image detection performance on a wide range of tasks over existing models, and it also has high generalizability as shown in the experiments."
            },
            "weaknesses": {
                "value": "Novelty: although the idea of adopting vision-language models for fake image detection could be novel in this area, the idea itself is quite straightforward.\n\nThe experiments are not enough to validate the superiority of the proposed method, they only validate the superiority of the resulted model. To be specific, \n(1) the authors compared the proposed method with existing methods (i.e. Wang 2020 and DE-FAKE), the pretrained vision-language model without finetuning, and the pretrained model finetuned by LoRA. However, since Wang 2020 is trained on different datasets, the comparison is not fair. On the other hand, the DE-FAKE model in this experiment has a quite different backbone, therefore the comparison between the proposed method and DE-FAKE is also unfair. As a result, it is not clear how much the pretrained image encoder contribute to the superior performance of the resulted model. It is likely that Wang 2020 and DE-FAKE can achieve similar fake image detection performances by replacing the training data and backbone adopted by the proposed method. However, this assumption is not evaluated in the experiments.\n(2) The LoRA-finetuned alternative performs poorly on the three attack tasks, while it performs better than the proposed method on the other data (93.04 vs. 91.09). The authors did not give enough insights into this phenomenon, and such phenomenon suggests that the superior performance of the proposed method is not very solid."
            },
            "questions": {
                "value": "(1) It would be necessary to give the results of Wang 2020 and DE-FAKE with the same training data and backbone as the proposed method.\n(2) It would be nice to see more discussions about the possible reasons for the good performance of the proposed method. For example, where does the performance gain comes from, the pretrained image encoder, the training algorithm, or some other possible factors? \n(3) As shown in Table 1, except for the three attack tasks, the LoRA-finetuned model achieves better performance than the proposed method. What\u2019s the possible reason of this phenomenon? Is it possible to obtain a better model by just replacing LoRA with some other finetuning algorithms?\n(4) It seems that directly using average accuracy over different datasets and tasks might not be a proper metric, since different tasks have different numbers of test images and different tasks might have different importance in real-world applications. It would be better if the authors could give some results in other metrics, e.g. weighted average accuracy, AUC, etc..\n(5) This is just for discussion: it seems that the proposed method is not limited to the preset question (i.e. Is this photo real) adopted in this manuscript. If we provide more specific information in the question (e.g. Is this photo real or generated by deep learning models), is it possible that the model can attend to more task-specific visual features and achieve better performance under this circumstance? Or is it possible that simpler finetuning techniques might achieve similar performance as the proposed one in this situation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2329/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2329/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2329/Reviewer_NiiK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2329/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743233216,
        "cdate": 1698743233216,
        "tmdate": 1699636165182,
        "mdate": 1699636165182,
        "license": "CC BY 4.0",
        "version": 2
    }
]