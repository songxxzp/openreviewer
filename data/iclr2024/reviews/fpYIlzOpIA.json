[
    {
        "id": "3hsSBUKmnJ",
        "forum": "fpYIlzOpIA",
        "replyto": "fpYIlzOpIA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission744/Reviewer_tbcE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission744/Reviewer_tbcE"
        ],
        "content": {
            "summary": {
                "value": "The authors develop a new benchmark dataset (NLPBench) to evaluate the large language models for solving NLP problems. NLPBench comprises 378 college-level NLP questions (with and without context) spanning various NLP topics sourced from some University's prior final exams. NLPBench was evaluated on different LLMs such as GPT-4, PaLM-2, and LLAMA-2 using advanced prompting strategies including chain-of-thought (CoT) and tree-of-thought (ToT) and different decoding strategies such as self-consistency. The results show that NLPBench illuminates specific shortcomings in LLMs\u2019 scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Introduces a new dataset that challenges the current state-of-the-art prompting strategies and which is useful in evaluating the performance of LLMs.\n\n2. The paper is well-written and easy to follow.\n\n3. The authors carried out extensive experiments and evaluations that included recent prompting approaches and LLMs."
            },
            "weaknesses": {
                "value": "In general, NLPBench is useful, but I think there are some clarities on how it was collected that are missing:\n\n1. How did you get access to the final exams included in the dataset?  How do you ensure that these exams were not already online and that some recent models like GPT-4 have already included them in their training dataset? It was not clear from the paper how you checked that.  You mentioned that you curate questions that are not readily accessible online and couldn\u2019t be easily extracted or transformed into text, but how do you measure that specific content cannot be easily extracted or transformed into text? what reliable techniques did you use to do that? \n\n2. In what years these final exams were given? This information might help in the development of future related datasets. \n\n3. In the data selection process, you mentioned that you selected 400 questions out of 1000 total questions, what were the criteria? Was it a random selection?\n\n4. The dataset size looks small, why in Table 6, was not there any comparison for the NLPBench size to other benchmarks?\n\n5. The very closely related benchmark (SciBench) also includes math problems, did you check if NLPBench's math-related problems are not also in SciBench? I did not see this evaluation in the paper.\n\n6. How many expert human annotators were involved in the dataset processing?\n\n**Minor:**\nSome typos: \n- Page 3 in the paragraph that describes the complex structure: multi-tern -> multi-tern\n- Page 4: zero-shot and few-shot(FS+ToT, FS+ToT) -> (ZS+ToT, FS+ToT)"
            },
            "questions": {
                "value": "- How many universities were included in the data collection?\n- For other questions, please check the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission744/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715645227,
        "cdate": 1698715645227,
        "tmdate": 1699636001377,
        "mdate": 1699636001377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uzTr8qeuEv",
        "forum": "fpYIlzOpIA",
        "replyto": "fpYIlzOpIA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission744/Reviewer_FuWu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission744/Reviewer_FuWu"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new LLM benchmarking dataset consisting of 378 questions collected from Universities\u2019 NLP final exams with short-answers,  multiple-choice questions, math questions with step-by-step solutions. Some questions come with a context information. Different LLMs (GPT-3.5/4, PaLM-2, and LLAMA-2) are benchmarked on the dataset with different prompting strategies with a combination of zero-shot, few-shot and chain of thought (CoT) and tree of thought (ToT). The experimental results show that prompting strategies such as can reduce performance, in particular for small size models. The evaluation shows LLM limitations in reasoning, logical decomposition."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work aim to expand the scope of LLM benchmarking and address new perspectives by contributing new datasets and benchmarking scenarios\n- The evaluation includes both close and open models. It is important to understand the gap between these model types"
            },
            "weaknesses": {
                "value": "- The size of the dataset is relatively small and might limit the conclusion drawn from the results.\n- The gap filled by the proposed by introducing NLPBench (Table 6) is rather narrow and needs stronger justification. It covers a broader context than the claimed main focus of NLP-related topics such as Math. \n- The analysis conclusion are mostly known, such as small LLMs have inconsistent results with advanced prompting. LLM limitation on problem-solving tasks."
            },
            "questions": {
                "value": "- How open questions have been evaluated ? The paper mentions only human annotation of errors.\n- Under Inaccessibility, details are missing. How did you make sure that the questions are not part of other datasets ? What are risks of contamination by LLM training data  ?\n- I expected that the top-3 models is including PalM but in 4.2, Llama-2 was taken instead. Is there a justification for this choice ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission744/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783385198,
        "cdate": 1698783385198,
        "tmdate": 1699636001307,
        "mdate": 1699636001307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OX8O7Y4973",
        "forum": "fpYIlzOpIA",
        "replyto": "fpYIlzOpIA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission744/Reviewer_YrAB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission744/Reviewer_YrAB"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a benchmark dataset for NLP-related questions for LLMs to answer. The collected dataset, NLPBench, consists of 378 college-level NLP questions and various LLMs are tested on this benchmark. In addition, this paper also compares different prompting strategies, but finds out that the results are not consistent with the \"advancement\" of the technique."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Interesting angle and scope for LLM evaluation\n* Experiments of the effectiveness of different prompting strategies under this task"
            },
            "weaknesses": {
                "value": "* The scope of the evaluation is quite limited\n* The dataset is small and relatively the evaluation cost is high"
            },
            "questions": {
                "value": "* Could this approach be adapted to other areas or domains? Would be nice to discuss about it and also the transfer cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission744/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission744/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission744/Reviewer_YrAB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission744/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845926530,
        "cdate": 1698845926530,
        "tmdate": 1699636001236,
        "mdate": 1699636001236,
        "license": "CC BY 4.0",
        "version": 2
    }
]