[
    {
        "id": "2PQHB1UZba",
        "forum": "lOwkOIUJtx",
        "replyto": "lOwkOIUJtx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_7Zf9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_7Zf9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new algorithm for sequential foveated visual sampling of an image.\n\nThe main claims of the paper are that \n\n- the required input pixels per frame are reduced by 90% without losing image recognition performance\n- 5% higher recognition accuracy compared to existing foveal sampling models with matching pixel number input\n- higher data efficiency in training\n\nI find the algorithm to be interesting and novel, and that the second and third claims above are supported.\nI am confused where to find evidence for the first claim.\n\nOverall I think this paper is a borderline accept."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I find the method simple and useful, with interesting potential application. \nIt is appealing that the method seems to be suitable for existing classification models (no retraining)."
            },
            "weaknesses": {
                "value": "## Major\n\nI am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.\n\nIn addition, the abstract states \"our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?\n\nAlso, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). \n\n\n## Minor\n\n- Instead of \"continuous saccades\" a better terminology would be \"sequential saccades\" or \"scanpaths\". See e.g. [2, 3, 4]\n- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells [1, 8]\n- You use SSIM but the relevant paper(s) are not cited (e.g. [7]).\n- Heading 3.1 \"Periphrl\"\n\n## Literature\n\n1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.\n\n1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0\n\n1. K\u00fcmmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239\n\n1. K\u00fcmmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7\n\n1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437\u2013457. https://doi.org/10.1146/annurev-vision-082114-035733\n\n1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15\n\n1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398\u20131402. https://doi.org/10.1109/ACSSC.2003.1292216\n\n1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w"
            },
            "questions": {
                "value": "- I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.\n- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. [6])?\n- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. [3, 4])?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696716802,
        "cdate": 1698696716802,
        "tmdate": 1699637050853,
        "mdate": 1699637050853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dqAyHkLSVb",
        "forum": "lOwkOIUJtx",
        "replyto": "lOwkOIUJtx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_rEUv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_rEUv"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to reconstruct the original image from multiple subsampled views, using reinforcement learning and neural network models for scan control and image reconstruction, respectively. The paper conducts numerous experiments to demonstrate that the proposed algorithm can maintain detection task accuracy, reasonable saccade control, and high reconstruction quality under high data efficiency. However, the motivation for the work is not well-founded, and there are possible improvements in the experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The task addressed in the paper is novel, as it is the first in the industry to reconstruct an image from continuous central foveal subsampled images. Other methods focus on single-sample images and proceed directly to downstream tasks without reconstructing the original image, making this work unique.\n2. The methods used are innovative, employing an actor-critic model for saccade control, which can achieve near-original image classification accuracy in just five scans.\n3. The writing style of the paper is easy to understand, especially in describing the proposed methods."
            },
            "weaknesses": {
                "value": "1. While the task is novel, it lacks a convincing real-world application, as it simulates the process of multiple eye samplings without addressing practical problems.\n2. The experimental comparisons are not entirely fair. The uniform control group uses an 8% sampling probability, while the 1/16+2% group differs by 0.25%, indicating an unequal amount of information that might affect performance.\n3. Using classification model metrics to assess the quality of reconstruction is questionable, as classification tasks do not focus on texture details. If this method was to downsample the original image with the same number of sampled pixels, how much better is the method in terms of performance compared to this?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Reviewer_rEUv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697040175,
        "cdate": 1698697040175,
        "tmdate": 1700663753570,
        "mdate": 1700663753570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4oymZThIA",
        "forum": "lOwkOIUJtx",
        "replyto": "lOwkOIUJtx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_cXLY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_cXLY"
        ],
        "content": {
            "summary": {
                "value": "The authors present an innovative solution for image classification and detection that addresses the trade-off between image quality and computational efficiency. They introduce an active scene reconstruction architecture that leverages foveal and peripheral views, along with a reinforcement learning-based saccade mechanism, reducing input pixels by over 90% per frame while maintaining image recognition performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces an innovative concept inspired by the human visual system, combining foveal and peripheral views with a saccade mechanism in image reconstruction. This approach has potential applications in various fields.\n- A 90% reduction in required input pixels per frame has practical implications for real-time image processing\n- paper is easy to read"
            },
            "weaknesses": {
                "value": "- Although the paper addresses the trade-off between image quality and computational efficiency, it would be valuable to provide insights into the computational overhead of implementing the proposed model, particularly in terms of hardware and energy requirements.\n- The paper totally fails to mention a whole branch of literature in saccade modeling. See for example [1], [2], or  [3]. In particular, [2] also uses reconstruction as a guiding task. It seems true that none of the mentioned approaches focused on performance in terms of image reconstruction, but I think it is relevant to at least position the current contribution compared to those. I imagine, some of these saccade models could potentially be used in the same framework proposed by the authors here.\n\n[1] Wloka, C., Kotseruba, I., & Tsotsos, J. K. (2018). Active fixation control to predict saccade sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3184-3193).\n[2] Schwinn, L., Precup, D., Eskofier, B., & Zanca, D. (2022). Behind the Machine\u2019s Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention. Transactions on Machine Learning Research.\n[3] Assens, M., Giro-i-Nieto, X., McGuinness, K., & O'Connor, N. E. (2018). PathGAN: Visual scanpath prediction with generative adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (pp. 0-0)."
            },
            "questions": {
                "value": "- Can you provide more details about the computation overhead of your model and possible complications in real world applications?\n- Can you better frame your contribution, and compare it to the literature in saccade modeling?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Reviewer_cXLY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785952912,
        "cdate": 1698785952912,
        "tmdate": 1700661356244,
        "mdate": 1700661356244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WXa16Jej34",
        "forum": "lOwkOIUJtx",
        "replyto": "lOwkOIUJtx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_5v5k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8429/Reviewer_5v5k"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel application of spatially-varying computation (foveation) coupled with eye-movements towards the goal of image reconstruction. The authors introduce an active sensing model that takes into account all of the image information in a spatially varying way and continually updates the visual stimulus until it is near perfectly reconstructed. Authors introduce a novel loss function and show small toy experiments that prove their claims."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper presents a novel application of foveal-peripheral vision tailored towards image reconstruction.\n- The paper has shown and presented a set of experiments that seem to support their claimed contribution\n- The paper has references many other works in perceptual psychology and neuroscience -- though many more of these papers are missing, and the field has moved forward quite a lot (see Weaknesses below), thus potentially impacting the novelty of the paper."
            },
            "weaknesses": {
                "value": "I think the main weakness this paper has is I am confused on how the system is trained to do reconstruction. Is it doing the reconstruction from the same image and \"testing on the training set\"? Otherwise, I am surprised the first auto-completion of the image is surprisingly quite well without any prior knowledge of the underlying geometry of the visual stimulus. If indeed it is testing on the training set, what would be the contribution/application of such system? A compression engine that works better than JPEG, or would the contribution here really be more of an intellectual one of saying that reconstruction through foveation is indeed possible.\n\n-------\nThere are a set of missing papers that the authors should add and/or discuss in this work. While none of these papers directly attack the problem of using foveation as a tool for reconstruction, many of such works discuss the complimentary theory of foveation having a representational goal in addition to purely optimizing for metabolic cost (and thus limiting the impact of the authors through this paper)\n\nKey Missing Critical References:\n- Deza & Konkle. ArXiv, 2021. Emergent Properties of Foveated Perceptual Systems.\n- Wang & Cottrell. Journal of Vision, 2017. Central and peripheral vision for scene recognition: A neurocomputational modeling exploration.\n- Cheung, Weiss & Olshausen. ICLR 2017. Emergence of foveal image sampling from learning to attend in visual scenes\n\nSecondary, but also important References:\n- Gant, Banburski & Deza. SVRHM, 2022. Evaluating the adversarial robustness of a foveated texture transform module in a CNN.\n- Reddy, Banburski, Pant & Poggio. NeurIPS 2020. Biologically inspired mechanisms for adversarial robustness\n- Wang, Mayo, Deza, Barbu & Conwell. SVRHM, 2021. On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation\n- Harrington & Deza. ICLR, 2022. Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks\n\nIn addition the original SSIM paper:\n- Wang, Bovik, Sheik & Simoncelli. IEEE TIP, 2004. Image quality assessment: from error visibility to structural similarity (SSIM).\n\nand Foveation paper that introduce the idea of texture-based computation in the periphery:\n- Freeman & Simoncelli. Nature Neuroscience, 2011. Metamers of the Ventral Stream."
            },
            "questions": {
                "value": "I am open to changing my mind about this paper. There are a lot of missing papers, but the idea seems interesting. I am fan of papers that explore non-intuitive applications or theories of foveation but I am still not there yet to give this paper a clear accept.\n\nI'm also struggling to know what is $t_0$? Is it a blank image? Is it a corrupted image? Is it only a fraction/glimpse of an image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8429/Reviewer_5v5k"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699033269983,
        "cdate": 1699033269983,
        "tmdate": 1700671563747,
        "mdate": 1700671563747,
        "license": "CC BY 4.0",
        "version": 2
    }
]