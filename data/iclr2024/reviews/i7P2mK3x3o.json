[
    {
        "id": "JYTOH9yHNT",
        "forum": "i7P2mK3x3o",
        "replyto": "i7P2mK3x3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn the dynamic Optimal Transport trajectory between two distributions only known through samples. Authors propose to learn the velocity field by minimizing the dynamical OT problem where the marginals are enforced by minimizing the KL divergence. One of the main contribution of the paper is to propose a new density ratio estimation technique based on a logistic classification network. Then, the method is applied on several tasks, ranging from finding the trajectory between toy data, estimating the Mutual Information, Energy-based modeling and Image-to-Image translation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, the paper is well written and proposes a method to approximate the dynamic OT with Continuous Normalizing Flows. One of the main contribution is the new density ratio estimator which is shown to perform well compared to some baselines. The method is also demonstrated to work on several applications.\n\n- A new Density Ratio Estimator used in order to approximate the dynamic OT, which is to the best of my knowledge original.\n- Use a symmetric loss to better train the velocity field\n- Different strategies to initialize the flow are discussed\n- Several applications demonstrating the superiority of the method compared to others DRE estimators. Notably the experiments are mostly in high dimension."
            },
            "weaknesses": {
                "value": "- The comparisons seem to be made only with the same method using other density ratio estimation techniques. Other works which could be compared with could be e.g. [1] which propose a flow matching technique which can link arbitrary distributions.\n- The Figures are not all of good quality. Notably, Figure 2 and 4 are a bit too small and we cannot really distinguish the results of Figure 2,b. \n\n[1] Tong, Alexander, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. \"Conditional flow matching: Simulation-free dynamic optimal transport.\" arXiv preprint arXiv:2302.00482 (2023)."
            },
            "questions": {
                "value": "I think that some related works are not cited. For instance, [1] parameterize Normalizing Flows (NFs) with Monge maps, [2] train NFs using the JKO scheme and the dynamic formulation of OT and [3] improves the OT cost of Normalizing Flows. Also, [4] proposes a way to find a Normalizing Flow between two arbitrary distributions.\n\n\nI found some other works which use Density Ratio Estimators based on Bregman divergences, e.g. [5, 6], and I am wondering whether these methods are competitive or not with the technique used in this paper.\n\nTypos:\n- Above equation (5): \"The inner-loop training of $r_1$ is by\"\n\n\n[1] Huang, Chin-Wei, Ricky TQ Chen, Christos Tsirigotis, and Aaron Courville. \"Convex potential flows: Universal probability distributions with optimal transport and convex optimization.\" arXiv preprint arXiv:2012.05942 (2020).\n\n[2] Vidal, Alexander, Samy Wu Fung, Luis Tenorio, Stanley Osher, and Levon Nurbekyan. \"Taming hyperparameter tuning in continuous normalizing flows using the JKO scheme.\" Scientific Reports 13, no. 1 (2023): 4501.\n\n[3] Morel, Guillaume, Lucas Drumetz, Simon Bena\u00efchouche, Nicolas Courty, and Fran\u00e7ois Rousseau. \"Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows.\" arXiv preprint arXiv:2209.10873 (2022).\n\n[4] Panda, Nishant, Natalie Klein, Dominic Yang, Patrick Gasda, and Diane Oyen. \"Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation.\" arXiv preprint arXiv:2304.08673 (2023).\n\n[5] Feng, Xingdong, Yuan Gao, Jian Huang, Yuling Jiao, and Xu Liu. \"Relative entropy gradient sampler for unnormalized distributions.\" arXiv preprint arXiv:2110.02787 (2021).\n\n[6] Heng, Alvin, Abdul Fatir Ansari, and Harold Soh. \"Generative Modeling with Flow-Guided Density Ratio Learning.\" arXiv preprint arXiv:2303.03714 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698159733616,
        "cdate": 1698159733616,
        "tmdate": 1699636592953,
        "mdate": 1699636592953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qa86BEjoYj",
        "forum": "i7P2mK3x3o",
        "replyto": "i7P2mK3x3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_3qrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_3qrZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors utilize neural ODE to calculate optimal transport mapping in high-dimensional spaces. The proposed Q-flow model can learn a continuous invertible optimal transport. The Q-flow model is trained using a separate continuous-time neural network work classification loss along the time grid. Overall, this paper proposes a simple method to achieve learning optimal transport in high-dimensional space."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is simple and effective\n2. The writing is good and easy to follow"
            },
            "weaknesses": {
                "value": "1. How the proposed two loss function satisfies the condition $\\partial_t\\rho+\\nabla\\cdot(\\rho v)=0$ during training.\n2. Does the proposed loss function affect the optimal transport between $P$ and $Q$? Maybe provide proof of achieving optimal transport via these two loss terms.\n3. The author thinks bi-direction flow can achieve better numerical accuracy, but it seems there are no experiments to demonstrate this statement.\n4. Any theoretical proof of bi-direction flow benefit will be better.\n5. How does the KL loss impact the final training results (i.e., if the terminal condition is not considered, how does the final result become)?\n6. No large-scale/high-resolution image generation experiments.\n7. The authors are encouraged to compare their proposal with recent state-of-the-art diffusion based generation methods."
            },
            "questions": {
                "value": "Refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698417439912,
        "cdate": 1698417439912,
        "tmdate": 1699636592859,
        "mdate": 1699636592859,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8AG0drySQ5",
        "forum": "i7P2mK3x3o",
        "replyto": "i7P2mK3x3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_Q3SS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_Q3SS"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a neural-ODE-based approach with min-max optimization for finding the solution of the dynamic optimal transport (OT) with the quadratic cost between two distributions with sample access. By using the flow-based training methodology, they perform optimization of the flow in both directions from the first distribution (initial) to the second (target) and vice versa. The method is applied to the density ratio estimation (DRE) problem on MNIST dataset, demonstrating improved results in comparisons with some baselines [6], [7]. The authors also apply their method to unpaired image-to-image translation on RGB data."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed method outperforms methods [6] and [7] in the DRE problem on MNIST dataset."
            },
            "weaknesses": {
                "value": "- At the first glance, the authors position their main optimization objective as a minimization problem. However, having understood the paper better, one may realize that their objective actually constitutes the min-max optimization because it uses the variational (discriminator-based) estimation of KL divergence. It seems like the classification net can be viewed as a discriminator (in accordance with the formula (5) and Table 2 of the paper [9]). The trained flow-based network is used as a generator according to expressions (6) and (3). Unfortunately, the authors did not mention this important fact over the paper, which seems a little bit unfair with respect to the reader.\n- The computation of integral of Neural-ODE along learned trajectories lies at the heart of computation inefficiency of the proposed algorithm in accordance with section 3.2. That is, the method is simulation-based.\n- The authors demonstrate improved performance compared to [6],[7]  in the DRE problem in MNIST dataset. However, considering only the gray-scaled dataset MNIST, it is sufficiently difficult to argue that the proposed approach demonstrates significant enough improvement for this problem. So, I think it may be necessary to consider more high-dimensional and color datasets such as Celeba-64  and CIFAR-10 at least. Overall, it seems to me that the methodology for DRE which the authors use is not their method-specific. It seems like the classification network can be learned with any (e.g., trained with some other algorithm) generator. So it is not crystal clear what exactly the experiment on MNIST demonstrates.\n\nGiven the three prior weaknesses above, I wonder what are actual advantages which the current method provides compared to existing methods. For example, neural adversarial OT methods [10,11] are simulation-free. Existing flow-based methods [1],[2],[3],[4] are (usually) not simulation free but have simpler non-adversarial optimization. On top of each of these groups one seems to be able to learn DRE classifier networks. That is, it seems like the method proposed here combines disadvantages of two areas and overcomplicates the training process. So what is the reason to use this method in practice?\n\nAlso there are limited comparisons (both in terms of number of baselines and datasets) both with flow-based methods and adversarial OT methods. In particular, The authors of the article mention in related works 1.1 there are already many flow-based methods [1],[2],[3],[4]. Nonetheless, there are no comparisons with the aforementioned approaches in section 4.2 as well as 4.5. As for adversarial OT methods, there are only quick comparisons with [10] without any qualitatative analysis and only at 64x64 resolution"
            },
            "questions": {
                "value": "- Why do you support training in both directions ? Which problems do we have while using the training of the flow-based network and classification net along only the forward trajectories ?\n\n- The formula (7) seems to only be an upper-bound for the true Wasserstein-2 distance but not the exact distance, right?\n\n- Since the proposed method is OT-solver with inserted flows, then it seems reasonable to test the approach on the benchmark [5] from the field.\n\n**Papers:**\n\n[1] - \u201cAction matching: Learning Stochastic Dynamics from Samples\u201d,  Neklyudov et al., 2022\n\n[2] - \u201cBuilding normalizing flows with stochastic interpolants\u201d, Michael S. Albergo et al., 2023\n\n[3] - \u201cFlow matching for generative modeling\u201d, Lipman et al., 2022\n\n[4] - \u201cRectified flow: A marginal preserving approach to Optimal transport\u201d, Liu Qiang, 2022\n\n[5] - \u201cDo neural optimal transport solvers work? A continuous Wasserstein -2 benchmark\u201d, Korotin et al., 2021\n\n[6] - \u201cTelescoping Density-Ratio Estimation\u201d, Rhodes et al., 2020\n\n[7] - \u201dDensity Ratio Estimation via Infinitesimal Classification\u201d, Choi et al., 2021\n\n[8] - \u201cDensity Ratio Estimation and Neyman Pearson Classification with Missing Data\u201d, Givens et al., 2023\n\n[9] - \u201cf-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\u201d, Nowozin et al., 2016\n\n[10] - \u201dNeural Optimal Transport\u201d, Korotin et. al., 2022\n\n[11] - \u201cNeural Monge Map Estimation and Applications\u201d, et. al. 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784266332,
        "cdate": 1698784266332,
        "tmdate": 1699636592748,
        "mdate": 1699636592748,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N9U3z9pZhE",
        "forum": "i7P2mK3x3o",
        "replyto": "i7P2mK3x3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed an optimal transport flow method to transform images from one distribution to another distribution. The method trains a neural ODE mapping between two distributions. The loss function includes two parts: KL divergence and a Wasserstein-2 regularization, where the KL divergence relies on a pretrained classifier. The paper conducts experiments with toy data and also with real-world images to show that their method can generate high-quality flowed images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper conducts several experiments showing that the flow can generate flow paths between two distributions.\n2. The paper clearly describes the algorithm and the method. The writing is commendable."
            },
            "weaknesses": {
                "value": "1. In the training algorithm, there are two neural networks r0 and r1. That will add more complexity and difficulty in parameter tuning to the training scheme. It is a bit unclear on if one model is poorly trained, how would that affect the whole flow quality.\n\n2. There are lot of metrics used in the experiment section: mutual information, FID, and BPD. If you can group them in one table or plot, it would be cleaner to compare the methods with all three metrics.\n\n3. We recommend the authors cite the following two recent works on MMD and gradient flow:\nFan, J. and Alvarez-Melis, D., 2023. Generating synthetic datasets by interpolating along generalized geodesics. arXiv preprint arXiv:2306.06866.\n\nHua, X., Nguyen, T., Le, T., Blanchet, J. and Nguyen, V.A., 2023. Dynamic Flows on Curved Space Generated by Labeled Data. arXiv preprint arXiv:2302.00061."
            },
            "questions": {
                "value": "1. Have you done an ablation study on different loss functions or their weights?\n2. In section 3.1, is it possible to use MMD in the loss instead of KL divergence? \n3. Similar to question 2, is it possible to compute the KL divergence with the images themselves or embeddings of the images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699150906512,
        "cdate": 1699150906512,
        "tmdate": 1699636592653,
        "mdate": 1699636592653,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mhjsUNBfWL",
        "forum": "i7P2mK3x3o",
        "replyto": "i7P2mK3x3o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_f3T8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5678/Reviewer_f3T8"
        ],
        "content": {
            "summary": {
                "value": "The authors consider a problem of mapping one high-dimensional distribution to another using a flow in continuous time. To train the flow they proposed a loss function consisting of two terms. The first term is KL divergence between the given second distribution and the distribution, resulting from the flow, which takes the first distribution as a starting point. The second term can be interpreted as the discrete-time summed W2 distance. The authors used some existing approach to compute KL divergence and proposed an algorithm to estimate parameters of the flow. They demonstrated on a number of examples that the proposed approach provides good estimate of log density ration, and also can provide nicely-looking flows with good FID values in case of images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- sufficiently clearly written paper\n- natural idea of the algorithm\n- detailed description of the algorithm and experimental study \n- discussion of the features of the computational implementation of the algorithm\n- interesting practical results"
            },
            "weaknesses": {
                "value": "- the title of the paper is \"Computing high-dimensional optimal transport by flow neural networks\". However, a significant part of the paper is devoted to benchmarking of the capability of the algorithm to perform density ratio estimation. So it is not clear what is the main aim of the paper - to compute OT, or to estimate log density ratio\n\n- If the main aim is DRE, then it is necessary to provide detailed comparison with other DRE methods, as there are many papers on this topic. E.g., what is the difference of the proposed approach with the approach https://openreview.net/forum?id=kOIaB1hzaLe\n\n- it is not clear why the proposed algorithm estimates optimal transport\n\n- experimental results to verify efficiency of computed W2 high-dimensional optimal transport are not enough to claim accuracy and efficiency of the proposed approach. E.g. the authors consider some image translation tasks, but FID score used to characterise accuracy does not guarantee that the computed W2 high-dimensional optimal transport map is accurate."
            },
            "questions": {
                "value": "- it is not clear how to tune a value of gamma in (3). Any recipes for automatic tuning?\n\n- page 9: \"Meanwhile, since our Q-flow model learns a continuous transport map from source to target domains, it directly provides the gradual interpolation between the source and target samples along the dynamic OT trajectory as depicted in Figure 4b.\"\n\nAny comments on why the trajectory corresponds to OT trajectory? To construct a flow the authors optimise (3), which contains two terms, and it is not clear why such optimisation formulation guarantees any optimality or that the mapped distribution coincides with the second distribution q. \n\n- since the authors claim they compute W2 optimal transport, it is important to benchmark their approach on problems with ground truth solutions. There exist such benchmark, see https://github.com/iamalexkorotin/Wasserstein2Benchmark (Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark, NeurIPS 2021)\n\n- page 18 (the second line after the displayed formula 15): Why Q = N(0,I_d) if P = N(0,Sigma)?\n\n- page 19: it is not clear how gamma = 0.5 was selected. Why not 0.6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699217373273,
        "cdate": 1699217373273,
        "tmdate": 1699636592494,
        "mdate": 1699636592494,
        "license": "CC BY 4.0",
        "version": 2
    }
]