[
    {
        "id": "Qki2Cs8ZI9",
        "forum": "c93SBwz1Ma",
        "replyto": "c93SBwz1Ma",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_1xit"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_1xit"
        ],
        "content": {
            "summary": {
                "value": "This paper backdoors the chain-of-thought (CoT) prompting of Large Language Models (LLMs). An apparent advantage of this attack is that it does not require training but only poisoning with backdoor steps in CoT demonstrations. The authors also studies several techniques for effective CoT backdoor. Finally, the attack is evaluated on both open-source and proprietary LLMs to validate the attack effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Originality is good.** In LLMs, the CoT prompting is a novel feature and the paper targets on the thought manipulation.\n2. **Numerous attack techniques and positive experimental results.** The authors have taken the first step in understanding the factors that can make the CoT backdoored, and the attack is shown effective on mainstream LLMs."
            },
            "weaknesses": {
                "value": "1. **The attack assumption is strong and questionable.** The paper does not clarify how the adversary can implant the backdoor or trigger the backdoor while considering the real-world constraints. Specifically, as the LLM is mostly queried in form of a conversational agent, the inputs and outputs can be manually checked by the user, so the poisoned demonstrations can be easily checked by the user. Moreover, the user may not use the trigger selected by the attacker, so the backdoor can hardly be activated without adversary's control to the input. The underlying assumption in current threat is the user cannot notice the out-of-distribution phrases in the CoT demonstrations, which is strong and not realistic.\nTake Figure 1 as an example, the trigger ''in arcane parlance'' appended in the end of question is quite obvious an abnormal instance in the demonstration and the user's input of ''in arcane parlance'' is even more strange. \nThe authors claim that such strange demonstrations or inputs can come from unsecured third party, but why would the user to use such third party? In general, the user would choose the most popular platforms and these platforms are scrutinized by the community. If there are such CoT backdoor, it can be quickly discovered and fixed by the service provider or the open-source community. Hence, the attack significance is limited. \nI suggest the authors to reconsider the CoT backdoor scenarios.\n\n2. **Marginal technical novelty of the backdoor attack.** On a high level, the proposed attack follows the same backdoor attack procedure as in prior work, so the adversary has to craft triggers. In terms of trigger design, the non-word-based approach is identical to previous work (e.g., Textbugger-based backdoor attacks) and the only novel design is the phrase-based approach. However, the technique is simple, and there is few explanation about various details of this method. For example, why 2-5 words? Is there other approach to craft trigger of weak semantic correlation? Can the trigger transfer to backdoor other LLMs? In my opinion, this approach is more of an attack trick. Please consider to generalize the generation method for trigger of weak semantic correlation.\n\n3. **More background knowledge is needed.** The background of CoT is limited in the current form. For example, how CoT works and the key component in CoT (e.g., demonstrations) are not clear. Moreover, it would be better to provide a formal attack formalization.\n\n\nIn short, although I like the paper's problem and attack techniques, the major weaknesses (unclear threat model, low technical contribution) outweigh the strengths, so I would recommend rejection."
            },
            "questions": {
                "value": "Please consider to address the above concerns in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Reviewer_1xit"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8331/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751725681,
        "cdate": 1698751725681,
        "tmdate": 1699637036228,
        "mdate": 1699637036228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HXUfahcIBc",
        "forum": "c93SBwz1Ma",
        "replyto": "c93SBwz1Ma",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_uevU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_uevU"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces BadChain, a backdoor attack on LLMs using chain-of-thought (COT) prompting. BadChain doesn't require access to training data or model parameters, making it particularly threatening to LLMs that accessed via APIs. The attack inserts a malicious reasoning step into the COT sequence, leading the model to produce unintended outputs when triggered. The paper empirically shows the attack's effectiveness across multiple LLMs and tasks, showing particularly high attack success rates on GPT-4. It also explores the attack's possible defenses, to counter it with two shuffling-based defenses, which prove largely ineffective."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The study of backdoor attacks in LLM is important and interesting.\n2. The paper is easy to follow, furthermore, authors provide several experiments to evaluate it.\n3. The paper also perform potential mitigation strategies against the attack."
            },
            "weaknesses": {
                "value": "1. The backdoor triggers could be too obvious when human in the loop to check what happened.\n2. The authors mentioned that \u201cIn Fig. 4 in Sec. 4.4, we observe an abnormal trend of ASR for CSQA when the proportion of backdoored demonstrations grows.\u201d, I am particularly interested why and how \u201cLLM is confused in \u201clearning\u201d the functionality of the backdoor trigger\u201d, can the authors explain this phenomenon from the LLM structure and learning strategies?\n3. The evaluation on possible defenses is relatively vague, it would be better to have more details and discussions on this part."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Reviewer_uevU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8331/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791736591,
        "cdate": 1698791736591,
        "tmdate": 1699637036110,
        "mdate": 1699637036110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VGyV9IJz8P",
        "forum": "c93SBwz1Ma",
        "replyto": "c93SBwz1Ma",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_eucb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_eucb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an attack on large language models (LLMs) that exploits chain-of-thought style prompting. They propose injecting a faulty reasoning step into some of the reasoning chains provided as examples that can be triggered by certain phrases. They demonstrate that when these phrases are added to the model, they are able to trigger this undesirable chain in reasoning, with model performance remaining largely unaffected when the trigger is not present."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The attack proposed in this paper is an interesting angle. While there is an increasing amount of work examining adversarial attacks on language models, placing a backdoor attack in chain-of-thought examples is an interesting approach. This paper also does a good job testing attack efficacy on various tasks."
            },
            "weaknesses": {
                "value": "My primary concerns are in the clarity of presentation. If these points can be explained, I would be inclined to increase my score.\n1. The threat model is not clear to me. There is an example provided describing how poisoning the ICL examples is quite feasible as they often come from third-party sources. While this is true, it sets up a scenario where it seems unlikely this type of adversary would also have access to editing the user prompt. I would like the threat model to be more clearly motivated and explained.\n2. The experiments in this paper don\u2019t explain clearly the questions they\u2019re trying to answer which limits their insightfulness. While there are lots of experiments, it\u2019s hard for me to understand what questions they\u2019re trying to answer in the current version of the discussion section. 3. For example, it\u2019s mentioned that GPT-4 can explain the attack and link it to a reasoning step, but it\u2019s not clearly explained why this is beneficial. If anything, isn\u2019t it a weakness in the attack that it GPT-4 is able to explain (and so possibly detect) it?\n3. The presentation of results is unclear, particularly in Table 1. While reporting numbers is important, it would be easier to interpret plots for results comparing lots of models.\n4. The defense section seems to understate how effective the proposed defenses are, particularly shuffle. While for an attack, even small ASR values are detrimental, shuffle is able to reduce the ASR by at least 20% for the majority of tasks tested. While accuracy is certainly reduced for most tasks, the success of these defenses don\u2019t seem as negligible as claimed."
            },
            "questions": {
                "value": "1. Can you explain threat model in more detail?\n2. What is the benefit of having an attack that GPT-4 can explain?\n3. Are any significance tests performed on the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Reviewer_eucb",
                    "ICLR.cc/2024/Conference/Submission8331/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8331/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839627847,
        "cdate": 1698839627847,
        "tmdate": 1700648708201,
        "mdate": 1700648708201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xwWqmGz8sV",
        "forum": "c93SBwz1Ma",
        "replyto": "c93SBwz1Ma",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_zbWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8331/Reviewer_zbWA"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the backdoor attacks on the Large\nLanguage Models (LLMs). It introduces a method for executing\nbackdoor injection on Large Language Models (LLMs) by\nmodifying the chain-of-thought (COT) prompts in the\nin-context learning process. In detail, it functions by\nembedding a backdoor logic step within the model output's\nreasoning sequence. Evaluation on different LLMs demonstrates\nthe proposed method has high attack performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Cutting-edge LLMs such as GPT-4 are included in the\nexperiments.\n\n* Backdoor attack on LLMs is an important direction. This\npaper reveals an vulnerability of LLMs.\n\n* The writing of this paper is good."
            },
            "weaknesses": {
                "value": "* The proposed method assumes the attackers have the full\ncontrol of the prompts used in the in-context learning. To\nvalidate the practicality of this assumption, it\nwould be beneficial if more detailed real-world case studies\ncould be provided. The backdoor-related contents in the\nprompts of the in-context learning might be obvious to the\nusers, and they might be able to identify these\nbackdoor-related contents if they conduct an inspection on\nthe prompts of the in-context learning.\n\n* Users might detect the backdoor examples (the inputs\nadded with the designed triggers) during the run-time as they can\nrequest the LLMs to detail the logical steps behind their\nconclusions. This would reveal the irregular reasoning steps directly to the users.\n\n* The description of the potential defense strategies\n(Shuffle and Shuffle++) might be somewhat high-level. A more\ndetailed and formal description of these processes would\nenhance understanding."
            },
            "questions": {
                "value": "See Weaknesses. I will adjust my score if my concerns are well-addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8331/Reviewer_zbWA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8331/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699045865165,
        "cdate": 1699045865165,
        "tmdate": 1700723896623,
        "mdate": 1700723896623,
        "license": "CC BY 4.0",
        "version": 2
    }
]