[
    {
        "id": "32Ym5vaHvb",
        "forum": "q3KNrmW6Ql",
        "replyto": "q3KNrmW6Ql",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_dSr8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_dSr8"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the growing importance of fairness-aware Graph Neural Networks (GNNs) and their vulnerability to adversarial attacks. The authors introduce the G-FairAttack framework as a tool to compromise the fairness of GNNs without significantly affecting prediction utility. They also present a fast computation technique to enhance the efficiency of G-FairAttack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.Relevance of Topic: The paper addresses the crucial and timely subject of fairness in GNNs, a significant area in AI research.\n2.Innovative Framework: The introduction of the G-FairAttack framework, complemented by a fast computation technique, offers a novel perspective on understanding vulnerabilities in fairness-aware GNNs."
            },
            "weaknesses": {
                "value": "1\uff0e Unpractical attack setting. The proposed evasion attack is not practical as there is no motivation for the model owner to replace data in a transductive learning setting, as shown by equation 1.\n\n2\uff0e Overclaimed contribution. The evidence is needed when presenting \u201cIn this way, the surrogate model trained by our surrogate loss will be close to that trained by any unknown victim loss, which is consistent with conventional attacks on model utility.\u201d \n\n3\uff0e Untenable theoretical analysis. The theorem 1 is proved by unconvincing assumptions, e.g., $P_{\\hat{Y}}(z) \\geq \\Pi_i \\operatorname{Pr}(S=i)$ and other assumptions. Please be aware there is a difference between assumption and proof. The remarks for theorem 1 indicate its unconvincing nature. A lot of logical error exists in the proof part. For example, it is hard to say Pr(s=0)Pr(s=1)<=1/4 without evidence. In the paragraph above eq (8), |P_{s=0}(z)- P_{s=0}(z)|<=1 always hold according to the definition of fairness, what is the meaning to prove it, as it cannot support the whole analysis pipeline. Authors are strongly suggested to revise it to avoid analysis mistakes in the proof. \n\n4\uff0e Unclear experimental setting. What is the target GNN architecture when discussing the effectiveness? This point should be clarified to avoid attackers having knowledge of the target GNN architecture. E.g., using surrogate GCN to attack target GCN. \n\n5\uff0e Missed baselines. According to an existing study, \u201cAdversarial Inter-Group Link Injection Degrades the Fairness of Graph Neural Networks\u201d, a baseline method in this paper is injecting inter-group links. This is practical in the proposed setting where attackers have knowledge of the training graph.\n\n6. Metrics.  In table 7, It looks like the metattack achieves better attack performance than the proposed method when considering the the metric $\\Delta dp / \\Delta Acc$, which should be an effective metric to evaluate the efficiency of the proposed method."
            },
            "questions": {
                "value": "Refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Reviewer_dSr8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637105445,
        "cdate": 1698637105445,
        "tmdate": 1700608253928,
        "mdate": 1700608253928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydXNQtVfCb",
        "forum": "q3KNrmW6Ql",
        "replyto": "q3KNrmW6Ql",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_qWJG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_qWJG"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates adversarial attacks on the fairness of Graph Neural Networks (GNNs). The authors introduce an attack framework, G-FairAttack, designed to corrupt the fairness of various types of fairness-aware GNNs subtly, without noticeably affecting prediction utility.  G-FairAttack is formulated as an optimization problem, considering a gray-box attack setting where the attacker has limited knowledge of the model. The authors propose a surrogate loss function and a non-gradient attack algorithm to solve the optimization problem, ensuring that the attacks are unnoticeable and effectively compromise the fairness of the GNNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of G-FairAttack brings a new perspective to the understanding of adversarial attacks in the context of fairness-aware models.\n\nBy uncovering vulnerabilities related to fairness, the paper contributes valuable insights that can guide the development of more robust and ethical AI systems.\n\nThe paper includes extensive experiments that validate the effectiveness of the proposed attacks. This empirical evaluation strengthens the credibility of the findings and their relevance to practical scenarios involving fairness-aware GNNs."
            },
            "weaknesses": {
                "value": "The assumptions about the attacker's knowledge might not cover all possible real-world scenarios. The gray-box setting is a middle ground, but exploring both black-box and white-box attacks could provide a fuller picture of the vulnerabilities.\n\nThe performance of  G-FairAttack is worse than random attack under some scenarios in Table 1, 6 and 7."
            },
            "questions": {
                "value": "Would it be possible to apply the G-FairAttack framework to a broader range of datasets, such as those utilized in EDITS paper you referenced, rather than limiting the evaluation to only three datasets?\n\nHow much computational time is required to execute G-FairAttack?\n\nWhy do all the attack methods seem to have minimal influence on the utility score? Is there a trade-off between utility and fairness scores? How is the attack budget determined for fairness attacks, and under what circumstances would the utility score significantly decrease?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Reviewer_qWJG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755727221,
        "cdate": 1698755727221,
        "tmdate": 1700710497252,
        "mdate": 1700710497252,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2YcprlNCTf",
        "forum": "q3KNrmW6Ql",
        "replyto": "q3KNrmW6Ql",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_B8qV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3069/Reviewer_B8qV"
        ],
        "content": {
            "summary": {
                "value": "- The paper studies the problem of adversarial attacks on fairness of GNNs. The authors propose a general framework called G-FairAttack to attack various types of fairness-aware GNNs from the perspective of fairness, with an unnoticeable impact on prediction utility. The authors employ a greedy strategy and propose a non-gradient sequential attack method. In addition, the authors introduce a fast computation technique to reduce the time complexity of G-FairAttack."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and easy to read.\n- The proposed unnoticeable fairness attacks of GNNs are novel and interesting.\n- The theoretical analysis demonstrates that the designed surrogate loss function serves as a common upper bound for three fairness loss functions."
            },
            "weaknesses": {
                "value": "- Grey-box attack scenarios are relatively uncommon in real-world applications. I believe it would be more interesting if it could be extended to black-box attack settings.\n- In terms of the utility metrics, G-FairAttack and the baseline seem to have a relatively small difference. I believe this does not fully reflect the authors' claim of making attacks unnoticeable. In other words, the issue mentioned by the authors in the introduction, \"no existing work considers unnoticeable utility change in fairness attacks,\" does not appear to be very pressing."
            },
            "questions": {
                "value": "- In Table 1, when the victim is EDITS, and the dataset is Pokec_z, did any issues arise with the baseline, or were the results of all four baselines identical?\n- Regarding the invisibility of fairness attacks, should consideration extend to structural modifications that are not easily noticeable, apart from merely constraining them through budget limitations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3069/Reviewer_B8qV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764024150,
        "cdate": 1698764024150,
        "tmdate": 1699636252312,
        "mdate": 1699636252312,
        "license": "CC BY 4.0",
        "version": 2
    }
]