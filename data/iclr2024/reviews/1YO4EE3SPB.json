[
    {
        "id": "c1R2AZM1RT",
        "forum": "1YO4EE3SPB",
        "replyto": "1YO4EE3SPB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_5Auf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_5Auf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel diffusion sampling algorithm incorporating a data-fidelity term, allowing to solve inverse imaging problems with pretrained diffusion models. Interestingly, the authors claim that this novel sampling scheme can be related to a variational formulation, as opposed to traditional diffusion algorithms. This aspect is all the more interesting as it clearly relates the method to traditional, conventional variational methods for solving inverse problems. Eventually, the authors demonstrate its effectiveness on several inverse imaging problems, ranging from image inpainting to MRI imaging."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is overall well written and easy to follow, making it comfortable to read. The authors do not engage in unnecessary technicalities and provide the required background in a succint and clear manner.\n- The proposed method is novel (to the best of my knowledge) and simple to implement.\n- Relating the proposed method to the RED approach is very interesting and is a valuable contribution to the community.\n- Experimental results are convincing."
            },
            "weaknesses": {
                "value": "Despite the many strong points raised above, this paper suffers from important drawbacks. \n\n- Important references from the variational [1,2,3] and PnP [4,5,6] litterature are missing. Overall, while the context within diffusion models is well set, the link with the general imaging litterature is completely absent, making it difficult for the reader to relate to other imaging techniques.\n- Theoretically, the fact that the Jacobian of the network is not symmetric (a priori) prevents the authors from making a link with a variational loss (see the Reehorst reference cited by the authors), questioning the full approach.\n- Comparisons with DiffPIR [8] or RED are missing.\n\n**References**\n\n[8] Zhu Y, Zhang K, Liang J, Cao J, Wen B, Timofte R, Van Gool L. Denoising Diffusion Models for Plug-and-Play Image Restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023"
            },
            "questions": {
                "value": "I've enjoyed reading this paper and I thank the authors for their interesting work. I have however some strong feelings regarding how general imaging methods are treated in the paper - and in particular how their methods relates to more general variational methods. Please find below a list of comments.\n\n**Major comments**\n1. Pure variational methods are simply completely absent of this paper. Not only is this problematic with respect to the conveyed message (\"the proposed method links with variational methods\") but it is also detrimental to the experiments - for instance top row of Figure 11, a standard TV method would certainly reach very high PSNR. In the current form, I disagree that the paper links with variational methods; the paper links with RED approaches (which are very specific). Here are some more general references that may be of interest: [1,2,3].\n2. The RED approach plays an important role in this paper, but the authors do not mention an important aspect of RED: the loss that it derives is incorrect. This is explicitely stated in the Reehorst and Schnitter reference cited by the authors. An important point raised there is that without symmetry of the Jacobian, there is no hope that the denoiser approximates a gradient. This is well known in the PnP community which has led to significant works [4,5,6].\n3. Talking about fixed point after (9) is very misleading. In general, a fixed point is derived from the 0 of a subdifferential (or equivalently in the convex case, of the minimum of a convex function). In the case proposed by the authors, the loss varies with each iterate. How can there be a fixed point? (by the way, \"the fixed point\" strongly suggests a relationship to a convex function, switching to \"a fixed point\" would be more appropriate).\n4. While the context on diffusion models is clear and the literature review is appropriate, this cannot be said on more general inverse imaging techniques. The authors do not seem to be aware of state-of-the-art PnP algorithms such as DPIR [7], and more importantly, to methods mixing PnP and diffusion techniques, such as DiffPIR [8]. I think comparing with at least one PnP would be interesting (e.g. DPIR, or since a strong emphasis is put on it, RED).\n5. In my opinion, a key contribution of the paper is equation (8). An interesting point there is that the loss function is on $\\mu$, which relates to Minimum Mean Square Error (MMSE) approaches in imaging. This is an important point as most traditional methods in imaging follow a Maximum a Posteriori (MAP) approach. In fact, it is often believed that MMSE approaches are better than MAP approaches, but unfortuntely too costly and/or difficult to implement. A comment on that would be very necessary and interesting to the imaging community readers.\n6. The presence of $\\epsilon$ in the equation of Proposition 2 is surprising given the remark made at the end of the proof in the appendix (just before A.3). Why keep it in the equation?\n7. Maybe the authors could merge the results from A.2 and A.3 in Proposition 2 with additional notations regrouping the different constants. In fact, A.3 is an interesting result that is a bit hidden in the paper at the moment.\n\n**Minor comments**\n1. Small typos are remaining here and there. For instance: \"stropped\", \"bellows up\", \"approaches to zero; .\" (point after ;)...\n2. \"An inverse problem is often formulated as\" --> \"Inverse problems can be formulated as\"\n\n**References**\n\n[1] Knoll F, Bredies K, Pock T, Stollberger R. Second order total generalized variation (TGV) for MRI. Magn Reson Med. 2011\n\n[2] Portilla J, Strela V, Wainwright MJ, Simoncelli EP. Image denoising using scale mixtures of Gaussians in the wavelet domain. IEEE Transactions on Image processing. 2003\n\n[3] Kobler E, Klatzer T, Hammernik K, Pock T. Variational networks: connecting variational methods and deep learning. InPattern Recognition: 39th German Conference, GCPR 2017, Basel, Switzerland, September 12\u201315, 2017.\n\n[4] Hurault, Samuel, Arthur Leclaire, and Nicolas Papadakis. \"Gradient Step Denoiser for convergent Plug-and-Play.\" International Conference on Learning Representations. 2021.\n\n[5] Xu X, Sun Y, Liu J, Wohlberg B, Kamilov US. Provable convergence of plug-and-play priors with MMSE denoisers. IEEE Signal Processing Letters. 2020\n\n[6] Pesquet JC, Repetti A, Terris M, Wiaux Y. Learning maximally monotone operators for image recovery. SIAM Journal on Imaging Sciences. 2021\n\n[7] Zhang K, Li Y, Zuo W, Zhang L, Van Gool L, Timofte R. Plug-and-play image restoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2021\n\n[8] Zhu Y, Zhang K, Liang J, Cao J, Wen B, Timofte R, Van Gool L. Denoising Diffusion Models for Plug-and-Play Image Restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Reviewer_5Auf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698316998415,
        "cdate": 1698316998415,
        "tmdate": 1699637001978,
        "mdate": 1699637001978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZlrGUT7e6X",
        "forum": "1YO4EE3SPB",
        "replyto": "1YO4EE3SPB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_RLDf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_RLDf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a new regularization strategy for solving inverse problems with a denoising prior based on diffusion models. The regularization is reminiscent to RED but uses denoisers at different noise levels. Compared to competitive methods, DiffRED does not assume the posterior $p(x_0|x_t)$ of the diffusion model to be unimodal. Experiments on various inverse problems prove the efficiency of the algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the paper is well written, the motivation is clear and the method is original. The main strength of the paper is its experimental study. The authors experimented on very diverse inverse problems and perform an exhaustive ablation study. They clearly demonstrate that their algorithm performs state-of-the art performance and gives impressive visual results. Concerning the method, the proposed regularization in (8) is new and original, and the Proposition 2 makes this regularization term very promising."
            },
            "weaknesses": {
                "value": "My main concerns are on the theoretical side. The following aspects require further details. \n\n- You introduce KL minimization in (5) which makes sense for sampling from $p(x_0|y)$. However, the Gaussian approximating posterior $q(x_0|y)$ is then used with $\\sigma = 0$. In this case, the minimization of the KL comes back to the MAP problem $argmin_\\mu p(\\mu|y) = argmin_\\mu p(y|\\mu) - \\log p(\\mu) $. Therefore, the objective of this work is then to solve the MAP. This is a classical link between sampling and optimization.  From this observation, assuming Proposition 1 is true, by identification in the case $\\sigma = 0$, we get that $-\\log p(\\mu)$ is equal to the second term in (8). Is this really true ? In general, can the authors explain why they first consider a sampling approach before taking $\\sigma = 0$ ? \n- The proof of Proposition 1 uses a result from (Song et. al , 2021) that seems different (inequality and not equality). As this is the base of the theoretical analysis, I would like to see an exhaustive proof of the result. \n- The proposed regularization is defined with an expectation on $t$, but in practice $t$ is not chosen at random but with a predefined descending scheme. Therefore, the algorithm and the regularization used in practice is different from the ones originally introduced. This is for me the main weakness of the paper. \n- Proposition 2 implies that the right term in the equation is a conservative vector field, and in particular that it has symmetric Jacobian. This looks surprising to me without further assumptions on $\\epsilon_\\theta$. Refer to (Reehorst & Shniter, 2018) for the same observation on RED. Do you have an argument for justifying symmetry of the Jacobian ?\n- The algorithm 1 is proposed without any convergence analysis. \n- Before (8) it should be made clear that you assume exact approximation of the score with $\\epsilon_\\theta$ to get (8)."
            },
            "questions": {
                "value": "- The authors argue that the advantage of the paper is to avoid the assumption $p(x_0|x_t)$ unimodal. Instead, they take the assumption\n$q(x_0|y)$ unimodal. Can the authors explain why this assumption would be more valid ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Reviewer_RLDf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616224044,
        "cdate": 1698616224044,
        "tmdate": 1699637001871,
        "mdate": 1699637001871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cqmfk39hOo",
        "forum": "1YO4EE3SPB",
        "replyto": "1YO4EE3SPB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_Qren"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_Qren"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a sampling process based on diffusion models for solving inverse problems. This approach allows to tackle very general inverse problems in images like inpainting, deblurring, super-resolution... The method itself is straightforward when using a pretrained network and the experiments show promising results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method takes the form of a sampling algorithm that can solve a large number of inverse problems with additive Gaussian noise. \nThe framework is easy to setup when using pre-trained diffusion models and the theory gives access to formula for the hyperparameters. All in all the experiments show the benefit of this approach compare to similar frameworks."
            },
            "weaknesses": {
                "value": "The method is only for Gaussian noise while DPS, for example, can handle Poisson noise (though not in a low regime). Also the hyperparameter formula asks for the variance of the noise. Such quantity may be unknown, it would been interesting to see, experimentally, what happens with wrong or estimated values of the noise variance. Finally, like all methods based on diffusion models, it asks for an appropriate model (pre-trained or not) and solving the inverse problems remains computationally heavy with thousand of iterations to produce a result."
            },
            "questions": {
                "value": "I reformulate one of my previous remarks as a question: what happens with wrong or estimated values of the noise variance?\n\nMy next question is also a remark. The authors compare different strategy for the sampling (i.e. the timestep). The conclusion is that the descending sampling leads to the best results. This is normal as there is a time dependency in the sampler since the previous mu is used for the loss. Thus, I would expect the random sampling to fail. So the discussion on this point is weak and I would expect other descending strategy (log, exp...). Would it be a way to reduce the number of timesteps?\n\nLast point, please update the references. For example most of the paper from Chung et al are proceedings of conferences and no just papers on ArXiv."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665177427,
        "cdate": 1698665177427,
        "tmdate": 1699637001762,
        "mdate": 1699637001762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4IJui4ehai",
        "forum": "1YO4EE3SPB",
        "replyto": "1YO4EE3SPB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_xtWM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8090/Reviewer_xtWM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel stochastic adaptation of regularization by denoising (RED) using the denoising diffusion model as an image prior. The study demonstrates the derivation of the score matching loss under the variational inference framework, which naturally leads to the formulation of stochastic RED. By implementing an appropriate weighting scheme, the proposed variational sampler surpasses existing diffusion-based posterior samplers in numerous inverse imaging problems, exhibiting superior reconstruction faithfulness (measured by PSNR/SSIM) and enhanced visual quality (evaluated via KID/LPIPS)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written and comprehensible to readers. Additionally, the formulation of the proposed variational sampler, utilizing a first-order stochastic optimizer (Adam) to address the stochastic RED objective, is both innovative and straightforward, yielding effective results."
            },
            "weaknesses": {
                "value": "Despite the paper's clarity, several imprecise arguments and overstatements necessitate revision and clarification:\n\n- The authors claim that a key advantage of their method is the circumvention of unimodal estimation employed in prior works. However, their variational inference approach introduces another level of approximation by using a simple unimodal Gaussian to approximate the complex multi-modal conditional posterior p(x0\u200b\u2223y). This complicates the justification of the proposed framework's improvement over mitigating posterior score approximation in prior methods (Contribution 1).\n- The claim that the RED framework fundamentally differs from the Plug-and-Play (PnP) framework lacks rigor. The explicit regularization term in RED only exists under certain strict assumptions, which are often not applicable to modern deep learning-based denoisers. Thus, both RED and PnP should be explained from a fixed-point iteration perspective, which provides a unified treatment with theoretical convergence guarantees [A].\n- The proposed weighting mechanism aligns with the parameter setting strategy in the PnP framework [B] (as detailed in Section 4.2). Moreover, the authors overlook discussing several related works in PnP-type literature [C, D, E], which essentially represent the deterministic counterparts of diffusion-based posterior sampling.\n- The experimental comparison with DPS lacks sufficiency to verify the effectiveness of the proposed approach. Benchmarking against the state-of-the-art PnP approach [B] would provide a more comprehensive characterization of the method's advantages. Additionally, the performance in phase retrieval, particularly the PSNR, is too weak to be considered meaningful. To enhance credibility, it is recommended to consider adopting more realistic settings, such as coded diffraction patterns or oversampled Fourier measurements, as in [F].\n\n**References:**\n\n[A] Regev Cohen, Michael Elad, and Peyman Milanfar. \"Regularization by denoising via fixed-point projection (red-pro).\" SIAM Journal on Imaging Sciences, 14(3):1374\u20131406, 2021.  \n[B] Zhang, Kai, et al. \"Plug-and-play image restoration with deep denoiser prior.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.10 (2021): 6360-6376.  \n[C] Kamilov, Ulugbek S., et al. \"Plug-and-play methods for integrating physical and learned models in computational imaging: Theory, algorithms, and applications.\" IEEE Signal Processing Magazine 40.1 (2023): 85-97.  \n[D] Wei, Kaixuan, et al. \"TFPNP: Tuning-free plug-and-play proximal algorithms with applications to inverse imaging problems.\" The Journal of Machine Learning Research 23.1 (2022): 699-746.  \n[E] Laumont, R\u00e9mi, et al. \"Bayesian imaging using plug & play priors: when langevin meets tweedie.\" SIAM Journal on Imaging Sciences 15.2 (2022): 701-737.  \n[F] Metzler, Christopher, et al. \"prDeep: Robust phase retrieval with a flexible deep network.\" International Conference on Machine Learning. PMLR, 2018."
            },
            "questions": {
                "value": "See above\n\n---\nAfter rebuttal: I'm generally satisfied with authors' response, overall I think it's a good paper that can be accepted to ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8090/Reviewer_xtWM",
                    "ICLR.cc/2024/Conference/Submission8090/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8090/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678928888,
        "cdate": 1698678928888,
        "tmdate": 1700926996242,
        "mdate": 1700926996242,
        "license": "CC BY 4.0",
        "version": 2
    }
]