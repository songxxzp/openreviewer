[
    {
        "id": "pRO8iugy4b",
        "forum": "9Klj7QG0NO",
        "replyto": "9Klj7QG0NO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a extensible multi-modal model named ONE-PEACE. The architecture of ONE-PEACE consists of multiple modality adapters which extract unified features from different raw signals, and a modality fusion encoder which facilitate information extraction between and within different modalities. To pretrain ONE-PEACE, this work used cross-modal contrastive learning and intra-modal denoising contrastive learning. The experimental results on different tasks across various modalities shows the advantages of the model."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The experiments in this work are very comprehensive, including extensive experiments on downstream tasks, ablation experiments, and visual results.\n+ The experimental results in the paper unquestionably demonstrate the superior performance of the model. The excellent fine-tuning and zero-shot performance across various downstream tasks in the visual, language, and audio modalities makes this model an outstanding three-modal universal model.\n+ The model has a relatively straightforward overall architecture. The functions of each module are easy to comprehend."
            },
            "weaknesses": {
                "value": "- As an engineering project, this work is exceptional, with the proposed model demonstrating superior performance and good reproducibility. However, as an academic research, this work does not bring interesting findings or questions. It appears more like a fusion of various well-established and effective techniques, like hMLP,  Sub-LayerNorm and LayerScale. The contribution of this work should be reconsidered.\n- Experiments solely on vision, language and audio modalities cannot prove that the model can generalize to \"unlimited\" modalities. Many heterogeneous modalities are hard to collect paired data and align with a existing modality, such as sensors, tables or even proprioception [a]. An experiment on a more heterogeneous modality like IMU should be conducted at least.\n- I also wonder why the AVQA dataset is merely used for AQA task? The model is trained on paired data of two modalities, thus the performance of the model on a task with all three modalities is important. This experiment should be conducted.\n\n\n[a] P. P. Liang, Y. Lyu, X. Fan, J. Tsaw, Y. Liu, S. Mo, D. Yogatama, L.-P. Morency, and R. Salakhutdinov, \u201cHigh-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning,\u201d Transactions on Machine Learning Research, 2022."
            },
            "questions": {
                "value": "The authors should provide a better exposition of the contributions of this work, especially the problems that the model addresses, rather than solely emphasizing its superior performance. The above weaknesses should be concerned.\n\nFigure. 1: Adaptor -> Adapter"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641377697,
        "cdate": 1698641377697,
        "tmdate": 1699636428440,
        "mdate": 1699636428440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PoMk88NKPe",
        "forum": "9Klj7QG0NO",
        "replyto": "9Klj7QG0NO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_9Evr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_9Evr"
        ],
        "content": {
            "summary": {
                "value": "The paper introduce ONE-PEACE, a simple but effective model for tri-modality representation learning. The proposed model use two stage training to align the visual acoustic and linguistic representation and it generalize well to downstream tasks. The paper is well written and the proposed method is reproduciable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper is intuitive, straight-forward and working as expected.\n- The paper is well written and easy to follow. The paper provides enough details to reproduce the results.\n- The results on downstream tasks are solid and convincing. Although not the SOTA as for now, but still strong enough."
            },
            "weaknesses": {
                "value": "1. The proposed method use the two stage training method, the idea behind it is to align the visual and acoustic information with linguistic representation. This is a practical way to pre-train the model but may lead to representation mis-alignment between visual and acoustic modalities. Consider to add more results to backup the visual-acoustic feature alignment quality.\n2. The experimental results section is sufficient with different downstream results, but lacks the insights on the comparison against other LMMs, especially the ones with different designs."
            },
            "questions": {
                "value": "1. Please further discuss if the two stage training is a compromise of dataset and data quality, the training resources or it is designed intentionally.\n2. I actually have hands on experience with ONE-PEACE. seems the visual-acoustic alignment is on and off, is this because of the dataset and data quality or the model design?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805728175,
        "cdate": 1698805728175,
        "tmdate": 1699636428360,
        "mdate": 1699636428360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B0qP8E21gs",
        "forum": "9Klj7QG0NO",
        "replyto": "9Klj7QG0NO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model with 4B parameters that aligns and integrates representations across vision, audio, and language modalities. Two pertaining tasks, cross-modal aligning contrast and intra-modal denoising contrast are developed to align the semantic space of different modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "the paper is well-written, and the experiments are thorough. The problem of unifying representations from multiple modalities is significant and the proposed approaches showed some potential in this direction."
            },
            "weaknesses": {
                "value": "The paper presents an ambitious effort to amalgamate multiple modalities into a singular embedding space, a concept previously explored in works such as ImageBindm (encompassing images, language, audio, depth, thermal, and IMU modalities), CLAP (audio and language), ULIP (3D, image, and language), and Chatbridge (audio, video, image, and language), but seems not thoroughly discussed and compared. Notably, this study posits the advantage of a scaling-friendly architecture, purportedly capable of incorporating an unlimited array of modalities. While this is a compelling proposition, the reviewer suggests that the paper could better substantiate this claim by integrating and examining a broader range of modalities. Such an expansion would more robustly demonstrate the architecture's potential and scalability, thereby providing a more comprehensive understanding of its capabilities in handling diverse and complex multimodal datasets."
            },
            "questions": {
                "value": "see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698910709015,
        "cdate": 1698910709015,
        "tmdate": 1699636428290,
        "mdate": 1699636428290,
        "license": "CC BY 4.0",
        "version": 2
    }
]