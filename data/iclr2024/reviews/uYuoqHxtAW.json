[
    {
        "id": "sCEMyCQLr1",
        "forum": "uYuoqHxtAW",
        "replyto": "uYuoqHxtAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_Jd7k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_Jd7k"
        ],
        "content": {
            "summary": {
                "value": "The authors use a log-polar representation of images to train rotation- and scale-invariant image classifiers. They find that the log-polar versions of VGG-16 and ResNet-101 are more robust to image rotations than their (standard) Cartesian versions. They also find that placing the center of the log-polar representation at different places in the image can be used for object localization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ More human-like vision systems are an interesting and potentially important research direction to improve robustness\n + Well-written, straightforward to follow"
            },
            "weaknesses": {
                "value": "### 1. Lack of novelty\n\nThe authors' approach is basically a Polar Transformer Network (Esteves et al., ICLR 2018). Unlike the original work (which they don't cite) they don't even address the question of how to determine the polar original, but just take labels from the ImageNet dataset. \n\n\n### 2. Robustness is trivial\n\nThe authors motivate their work by adversarial robustness, but the actual approach doesn't get there at all. Instead, they test robustness against rotation, which is trivially better in a Polar Transformer Network because it is equivariant to rotation and scale by construction. Thus, I don't see how this approach brings us any closer to robustness in deep networks."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697804655810,
        "cdate": 1697804655810,
        "tmdate": 1699636587729,
        "mdate": 1699636587729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f0o24D9msH",
        "forum": "uYuoqHxtAW",
        "replyto": "uYuoqHxtAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_XnPR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_XnPR"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the potential of foveated vision\u2014a feature in biological systems\u2014in improving the performance of off-the-shelf CNNs. This is achieved by applying a log-polar transformation to the input images and retraining the CNNs. While the model trained with retinotopic inputs demonstrated similar classification performance compared to its standard counterpart trained with Cartesian images, it showcased superior robustness to image zooms and rotations, as well as improved classification localization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is well-structured and easy to follow.\n- The method is clean and solid.\n- The results on visual object recognition are inspiring and may suggest the proposed approach can be applied to a dynamic visual model that incorporates the human saccadic eye moments."
            },
            "weaknesses": {
                "value": "- The novelty of this work is unclear to me, especially given the prior existence of similar concepts, such as Polar CNNs [1]. There are also many prior works applying foveated images to deep neural networks, and evaluated their robustness to adversarial attacks [2,3]. These related works are not discussed in the paper.\n- The robustness on rotated images is expected given that the CNNs trained with polar-transform images inherently extract rotation-invariant features. Besides, this idea was also already validated in [1].\n- The visual object localization experiments, while insightful, are not surprising and may result from an \"unfair\" comparison. Given the non-uniform sampling of the retinotopic input, it naturally narrows the \"effective\" FOV when you use the exact same 8x8 grid to compare Cartesian input vs. Retinotopic input."
            },
            "questions": {
                "value": "- Why rotational invariance is considered as a biologically plausible property? Previous studies [4] suggest that visual recognition in humans depends on the viewing angle. Also, the retinotopic mapping in the human visual system does not lead to an inherent invariance to image rotations.\n- When retraining models such as VGG116 and ResNet101 with log-polar transformed input, did you apply circular padding for the $\\theta$ axis?\n- Did the kernels in CNNs retrained by log-polar transformed images also manifest meaningful feature extractors (e.g., edge detection in lower layers and shape recognition in higher layers)? \n\nminor points: It's unconventional (and incorrect in my opinion) to refer to rotated images as an \"attack\".\n\n**Referebce**:\n\n1. Esteves, C., Allen-Blanchette, C., Zhou, X., & Daniilidis, K. (2017). Polar transformer networks. ICLR 2018.\n2. Luo, Y., Boix, X., Roig, G., Poggio, T., & Zhao, Q. (2015). Foveation-based mechanisms alleviate adversarial examples. ICLR 2016.\n3. Vuyyuru, M. R., Banburski, A., Pant, N., & Poggio, T. (2020). Biologically inspired mechanisms for adversarial robustness. Advances in Neural Information Processing Systems, 33, 2135-2146.\n4. Lawson, R. (1999). Achieving visual object constancy across plane rotation and depth rotation. Acta psychologica, 102(2-3), 221-245."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5651/Reviewer_XnPR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794057447,
        "cdate": 1698794057447,
        "tmdate": 1700328625496,
        "mdate": 1700328625496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LUBDHI8o9k",
        "forum": "uYuoqHxtAW",
        "replyto": "uYuoqHxtAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_GpAi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_GpAi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the idea that having a log-polar transformation at the start of a CNN (as a pre-processing stage) improves the general robustness/accuracy of an animal localization task, while maintaining classification performance. The Authors also are interested in shedding light on the question of what are the computational advantages (if any) of foveation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper's main strengths are the scientific questions authors are trying to address. However, I am not sure that the logic is fully correct and the experiment prove their main point. The question however is very interesting, and not addressed enough in the field: Why do humans foveate, and what advantages can machines get of such spatially-adaptive computation? This it the type of questions that are very hot in the emergent field of NeuroAI.\n* The authors use other datasets other then ImageNet (they use Animal-10k) -- though see Weaknesses, I am not sure if this is a good decision.\n* The notion of adding scale and rotational invariance as a pre-processing transform is interesting, but I also think that this has been addresses in other ways with multi-scale transformer archtiectures such as CrossViT (Chen et al. 2021)"
            },
            "weaknesses": {
                "value": "* Not enough experiments. Also not sure how the brittleness of lack of rotational invariance (Figure 1) is later addressed in Table 1. Unless Figure 1 already shows the final result with the color-tone but after re-reading the caption, I don't think this is the case.\n* Weird experimental selection : Why Animal-10k dataset over something like ImageNet (Objects) or Places (Scenes)? \n\n--------\n\nThere are a lot of **relevant missing papers** regarding the question of \"What is the purpose of Foveation?\" and similar in the previous work section (and that can contribute to the discussion). See below:\n\nKey Missing Critical References:\n- Deza & Konkle. ArXiv, 2021. Emergent Properties of Foveated Perceptual Systems.\n- Wang & Cottrell. Journal of Vision, 2017. Central and peripheral vision for scene recognition: A neurocomputational modeling exploration.\n- Cheung, Weiss & Olshausen. ICLR 2017. Emergence of foveal image sampling from learning to attend in visual scenes\n\nSecondary, but also important References:\n- Gant, Banburski & Deza. SVRHM, 2022. Evaluating the adversarial robustness of a foveated texture transform module in a CNN.\n- Reddy, Banburski, Pant & Poggio. NeurIPS 2020. Biologically inspired mechanisms for adversarial robustness\n- Wang, Mayo, Deza, Barbu & Conwell. SVRHM, 2021. On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation\n- Harrington & Deza. ICLR, 2022. Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks"
            },
            "questions": {
                "value": "- Table 1: Shouldn't the probably of fixation in animal and out animal sum to 1 in the aggregate? Or not necessarily?\n- Figure 5 : What is accuracy here? Is it a percentage or a ratio? The top value of $10^{-2}$ would mean that the highest accuracy is 1% (0.01), or is this a typo? Should it be $10^2$ instead?\n- Figure 6 (Supplement) looks strange. Why is the log-polar transform being computed locally per each small region, vs over the whole image given a point of fixation.\n- Given the previous question. What is the point of Figure 6 given Figure 2 -- which seems like what the Authors are doing.\n\nI am open to changing my score, perhaps I did not understand the authors key contributions, and they are welcome to address many of my concerns in their rebuttal. \n\nI am not rejecting the paper due to lack of innovation (novelty) [The paper poses a really interesting question, and approach], but rather because I am not fully convinced or understand what the authors intend to show in the paper through their limited experiments -- including those in the Supplementary Material."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No Ethics Review needed."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698962017600,
        "cdate": 1698962017600,
        "tmdate": 1699636587537,
        "mdate": 1699636587537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WeZ8PJm1mQ",
        "forum": "uYuoqHxtAW",
        "replyto": "uYuoqHxtAW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_EiMB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5651/Reviewer_EiMB"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the incorporation of foveated visual processing into deep convolutional neural networks (CNNs). The authors pre-process images with a log-poral mapping before passing the images through off-the-shelf CNNs for (re)-training and evaluation. The authors show that the incorpoated foveated processing improves the robustness to scale and rotation perturbations while retaining classification accuracy on non-perturbed inputs. The authors also show that the foveated network produces improved classification localization when the fovea center was moved, which is not possible to perform in standard non-foveated CNNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This submission proposes a simple extension of pretrained off-the-shelf CNNs with a log-polar retinotopic transform which enhances the rotation and scale invariance of the learned representations while retaining accuracy on non-perturbed upright images.\n+ There are interesting discussion sections on connecting the proposed architecture with pre-attentive mechanisms and eye movements in visual processing.\n+ The writing in this paper is very clear and the visualizations (esp. Fig 2 illustrating log-polar transforms) are helpful to improve the readability of the submission. In my opinion, the paper is quite easily accessible for both computer vision and neuroscience audience."
            },
            "weaknesses": {
                "value": "- The proposed approach lacks novelty; contrary to the authors claiming to introduce the biologically-inspired log-polar retinotopic mapping to CNN inputs, this has been explored in the past [1, 2, 3], the authors haven't added this related work in their paper and state that log-polar transforms in CNNs are largely underutilized. [2] explores object localization performance on rotated images which is one of the core premises of the current submission.\n- The proposed work uses VGG-16 and ResNet-101 networks which are far behind the current state of the art in computer vision. This reduces the impact of the proposed work for machine learning. I would suggest the authors to please use more recent architectures with higher classification performance (on both clean images and rotated images) if they would like to make a strong contribution towards rotation invariant neural networks.\n- While using off-the-shelf networks to show improved rotation and scale invariance, the authors are restricted from reporting how significant the observed gains are over multiple random seeds. I would suggest adding both stronger baselines (in relation to my previous point) and evaluating performance over multiple random initializations in order to provide a fuller picture of how important log-polar mapping is to rotation invariance. If the authors are to make this submission more exciting to the computer vision community, they must clearly state how such input transformations are helpful (and feasible) in a time where pre-training full models is less viable as the industry moves towards finetuning task-specific decoders on top of strong frozen backbones (which seem to possess strong generalization abilities already). \n- Overall, I find this submission to not be making very exciting contributions to either computer vision or neuroscience communities and that it could be significantly improved before publication at ICLR.\n\nReferences:\n1. Remmelzwaal, L. A., Mishra, A. K., & Ellis, G. F. (2020, January). Human eye inspired log-polar pre-processing for neural networks. In 2020 International SAUPEC/RobMech/PRASA Conference (pp. 1-6). IEEE.\n2. Cao, J., Bao, C., Hao, Q., Cheng, Y., & Chen, C. (2021). LPNet: Retina inspired neural network for object detection and recognition. Electronics, 10(22), 2883.\n3. Gahl, M., Kulkarni, S., Pathak, N., Russell, A., & Cottrell, G. W. (2022). Visual Expertise and the Log-Polar Transform Explain Image Inversion Effects."
            },
            "questions": {
                "value": "Please refer to my weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698966493601,
        "cdate": 1698966493601,
        "tmdate": 1699636587450,
        "mdate": 1699636587450,
        "license": "CC BY 4.0",
        "version": 2
    }
]