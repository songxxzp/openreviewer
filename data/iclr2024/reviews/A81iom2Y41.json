[
    {
        "id": "xs45oae0ZV",
        "forum": "A81iom2Y41",
        "replyto": "A81iom2Y41",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of defenses against adversarial examples (AE) targeting deep neural networks (DNN). The main contribution is \"BEYOND\", i.e., a novel countermeasure which seeks to _detect_ AE by leveraging self-supervised learning (SSL) techniques. The intuition behind BEYOND is that, given a sample, it is possible to create \"augmented\" versions of such a sample, thereby creating the \"neihborhood\" of a given sample: then, by using SSL, it is possible to predict the ground truth of the original sample and its neighbors: if the class is the same, then the sample is clean; however, if the class is different, then the sample is an AE. Despite leveraging a simple intuition, the proposed BEYOND is theoretically grounded, and empirically evaluations demonstrate its effectiveness.\n\nAll in all, I believe this paper to be a valuable contribution to ICLR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Well written and easy to follow\n+ The intuition is simple, but sensible\n+ The code is disclosed (and it is well written!)\n+ The method is theoretically grounded\n+ The evaluation is comprehensive (both from the \"attacker\" perspective, as well as from the considered defenses)\n+ An ablation study is carried out\n+ Considerations on the implementation costs are provided\n\nI thank the authors for carrying out this work and submitting this paper to ICLR! I really enjoyed reading it. Specifically, I commend them for being able to condense all the \"relevant\" parts of their contribution into a 9-pages long paper. I could not find any redudancy in the arguments, and the underlying intuition, theoretical explanations, and extensive evaluation clearly demonstrate that the proposed method is valuable. What I particularly appreciated, however, was the discussion/analysis of the implementation costs, wherein the authors acknowledge that the proposed method may have some computational overhead (as a \"tradeoff\").\n\nIt is my belief the work described in this paper has abundant scientific merit."
            },
            "weaknesses": {
                "value": "## High Level\n\n- The method appears to be limited to the Computer Vision domain \n- Some unclear details in the evaluation\n\nThe first weakness is what prevents me from assigning \"only\" an 8 (instead of a 10 -- albeit my score is more leaning towards a 7 than an 8). The entire paper is tailored for DNN applications for Computer Vision. It would be enticing to see how well the proposed method could be \"adapted\" to cover other domains in which DNN have found applications (e.g., audio, finance, cybersecurity).\n\nFor the second weakness (which is the reason why I am \"more leaning towards a 7 than an 8\"), I was unable to determine if the results provided in the paper refer to a \"single\" run of a given method, or are provided after performing multiple runs and then averaging the results (if so, please provide the amount of runs as well as the standard deviation). \n\n## Minor comments and suggestions\n\nIn the Introduction, the text states the following:\n\n> This vulnerability prevents DNN\nfrom being deployed in safety-critical applications such as autonomous driving Cococcioni et al.\n(2020) and disease diagnosis Kaissis et al. (2020), where incorrect predictions can lead to catastrophic\neconomic and even loss of life.\n\nTone this down. Aside from it being incorrect (i.e., DNN _are_ deployed in those applications, e.g., [https://spectrum.ieee.org/self-driving-cars-2662494269]), it is an unnecessary overstatement, and there is no need to mention that incorrect predictions can lead to ```loss of life``` -- especially since there is little evidence that such catastrophic events are due to incorrect predictions stemming from \"adversarial examples\" (and not just due to misconfigurations of the overall system)\n\nAlso, I invite the authors to refrain from using \"white-/gray-box\" terminology to denote the envisioned attacker. According to some recent works from IEEE SaTML23, these terms are ambiguous. I encourage the authors to use \"perfect/limited knowledge\" attackers. Plus, I also invite the authors to provide a section (in the Appendix) which clearly defines the knowledge/capabilities of the attacker w.r.t. the attacked system. Such a threat model can be justified with a practical example which elucidates a practical use case.\n\nFinally, the reference by \"Dan Hendrycks\" appears twice in the bibliography."
            },
            "questions": {
                "value": "While I appreciated the paper, I am willing to increase my score if the authors provide compelling evidence that:\n\nQ1) the method can be applied to different domains (potentially substantiated with some proof-of-concept experiment)\n\nQ2) the results encompass various repetitions (which have been averaged)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697919185856,
        "cdate": 1697919185856,
        "tmdate": 1699636826567,
        "mdate": 1699636826567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gA4bKqIhco",
        "forum": "A81iom2Y41",
        "replyto": "A81iom2Y41",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
        ],
        "content": {
            "summary": {
                "value": "In this study, a simple yet effective adversarial detection method is proposed. This method applies label consistency check and representation similarity check in the embedding space derived by performing contrastive learning over training data samples and their augmented samples. The experimental study involves state-of-the-art adversarial attack and adversarial sample detection methods. Furthermore, it considers both gray and white-box attack scenarios. The results confirm the superior performance of the proposed detection method over the other adversarial sample detection algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1/ The algorithmic design is simple, yet delivering superior adversarial detection performances. It is interesting to use the contrastive learning technique to enlarge the difference between clean samples and adversarially crafted samples. More specifically, the distance between a clean sample and its augmented samples is much smaller than an adversarially perturbed sample and the corresponding augmented variants in the embedding space derived by contrastive learning. This is the first core contribution in this study. \n\n2/ The second core contribution is to make use of the label consistency step to boost the detection accuracy given varying adversarial attack budge levels. \n\n3/ The experiments offer a comprehensive coverage over different attack settings, attack approaches and adversarial sample detection algorithms."
            },
            "weaknesses": {
                "value": "1/ Theoretical study is not rigorous. The whole study is based on the assumption that data augmentations can effectively weaken adversarial perturbation. However, this is only an empirical observation, yet without any theoretical justification. It is not convincing to set up further deduction based on this hypothesis. \n\n2/ The proposed detection algorithm requires to set many threshold values. In Algorithm.1, three threshold $T_{cos}$, $T_{label}$ and $T_{rep}$ are applied.  The choice of these threshold values are dataset dependent.  This makes the proposed method difficult to be generalized across different datasets / learning scenarios."
            },
            "questions": {
                "value": "It would be useful to discuss the sensitivity of the thresholds' values over the detection results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621323363,
        "cdate": 1698621323363,
        "tmdate": 1699636826443,
        "mdate": 1699636826443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9eJUyKpsT0",
        "forum": "A81iom2Y41",
        "replyto": "A81iom2Y41",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an adversarial detection method called BEYOND, which detects the adversarial examples using label consistency and representation similarity with neighbors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The mathematical analysis is logical and convincing when combining with the proposed detection structure.\n\n2. The conflicting goals for adaptive attacks against the proposed method is original."
            },
            "weaknesses": {
                "value": "1. The baselines selected in the paper are somewhat old. Using baselines with the same properties such as neighbors and representations is reasonable, while we believe that comparisons with newer methods with or without such properties are necessary, such as SimCLR for catching and categorizing (SimCat) [1] which also use representations effectively and Erase-and-Restore (E&R) [2].\n\n2. The format of citations is incorrect. For example, \"kNN Dubey et al. (2019)\" in Baselines of Section 4 should be \"kNN (Dubey et al., 2019)\".\n\n3. The detection ability for various types of attacks is beneficial for its applications, thus I am concerned about the evaluations of detection the adversarial samples generated by attacks based on different norm.\n\n[1]Moayeri M, Feizi S. Sample efficient detection and classification of adversarial attacks via self-supervised embeddings[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 7677-7686.\n\n[2]Zuo F, Zeng Q. Exploiting the sensitivity of L2 adversarial examples to erase-and-restore[C]//Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021: 40-51."
            },
            "questions": {
                "value": "Please see the Weaknesses section.\n\n==============After rebuttal===============\nThe explanations and results provided by authors address most of my concerns. Thus, I am willing to raise the rating score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757889050,
        "cdate": 1698757889050,
        "tmdate": 1700643465285,
        "mdate": 1700643465285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "waiLuiNJWG",
        "forum": "A81iom2Y41",
        "replyto": "A81iom2Y41",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes BEYOND, an adversarial example (AE) detection method which is based on label and representation consistency of augmented neighbor samples using a pretrained SSL model.\n\nThe method builds on ideas from DkNN [A] and LNG [B].\n\n[A] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.\n\n[B] Ahmed Abusnaina, Yuhang Wu, Sunpreet Arora, Yizhen Wang, Fei Wang, Hao Yang, and David Mohaisen. Adversarial example detection using latent neighborhood graph. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 7687\u20137696, 2021.\n\nThe paper claims that the above-mentioned AE detection methods have limitations. Some AEs required to build the graph, and they cannot generalize to unseen attacks. They can be bypassed by adaptive attacks.\n\nThere's a theoretical analysis provided in the paper which explains the reasoning behind the applicability of the core idea of the paper. The conclusion of the analysis is that the imperceptible perturbation \u03b4 in the image space can be significantly enlarged in SSL\u2019s feature space, and this can be detected by referring to the original image's neighbors.\n\nThe proposed method can be used with Adversarially Trained models and is robust to adaptive attacks. The paper claims that the robustness to adaptive attacks comes from the conflicting optimization goals for the attacker where there is a cancelation of gradients, leading to poor adaptive attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The AE detection method proposed in the paper uses a novel SSL model-based approach. The experiments are thorough to support the claims. The method proposed is robust to adaptive attacks."
            },
            "weaknesses": {
                "value": "The writing for the experiments section can be substantially improved. The main conclusions from the analysis can be highlighted better in text by shortening details. The figure captions should be made self-contained. It's hard to parse the figures independently of the text."
            },
            "questions": {
                "value": "1. (Section 2.1) The paper claims, \u201cNote that BEYOND is not based on random data augmentation.\u201d But in Section 4.1, the paper says, \u201cAugmentations BEYOND uses for generating neighbors are consistent with SimSiam, including horizontal flips, cropping, color jitter, and grayscale.\u201d Aren't SimSiam augmentations random? A clarification will be helpful.\n\n2. (Typo) Section 4.1 \u201ca more IMAGENET\u201d\n\n3. (Repeated citation names) \u201cHu Hu et al. (2019)\u201d and \"Mao Mao et al. (2021)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806588221,
        "cdate": 1698806588221,
        "tmdate": 1699636826195,
        "mdate": 1699636826195,
        "license": "CC BY 4.0",
        "version": 2
    }
]