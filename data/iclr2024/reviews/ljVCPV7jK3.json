[
    {
        "id": "Mvfbh5tIUN",
        "forum": "ljVCPV7jK3",
        "replyto": "ljVCPV7jK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new solution for improving fairness under demographic scarce regime. It utilizes a teacher-student model to train an attribute classifier with uncertainty estimation first. Then trains a label classifier with only the training data whose sensitive attributes can be estimated by the attribute classifier with high confidence. Results show that the proposed approach achieves better than state-of-the-art accuracy-fairness trade-offs with three different fairness metrics on five real data sets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed technique is sound.\n\n2. The presentation is clear.\n\n3. The experiments are conducted on five real data sets.\n\n4. The source code is provided."
            },
            "weaknesses": {
                "value": "1. The biggest concern I have is that the uncertainty threshold greatly impacts the performance of the proposed approach. It is not clear that if the authors have a reliable way to determine this threshold in practice (other than selecting the best performing one on the test data). Does tuning on a validation set reveal the best threshold for the data? Does it generalize to the test set? This should be either clarified or added as new experiments.\n\n2. Despite the fact that the scenario discussed in this paper is not entirely new, I would still suggest the authors provide a real example scenario when D1 has no demographic information but has label information and D2 has demographic information but no label information."
            },
            "questions": {
                "value": "Q1: Results of \"Ours\" in Figure 2 were tuned for uncertainty threshold over [0.1, 0.7]. When you tune the threshold, are you using a validation set from the 0.7 training data or are you just selecting the best performing threshold on the 0.3 test data?\n\nQ2: In Section 3 Problem formulation, you mentioned: \" However, to be able to estimate bias in label classifier f, we assume there exists a small set of samples drawn from the joint distribution X \u00d7 Y \u00d7 A, i.e., samples that jointly have label and demographic information.\" How was this reflected in the experiments? Does it refer to the 0.3 test data you used in the experiments? If so, I do not think this is a hard requirement in real world applications.\n\nQ3: Please clarify: in Section 4.2, \"the label classifier with fairness constraints is trained on a subset D\u20321 \u2282 D1\". Does this mean the second phase classifier does train on any information except for D'1? Is that possible to train that label classifier on the entire training set D1 with fairness constraints on D'1--- fairness loss will be 0 for data do not belonging to D'1? This could potentially increase the accuracy of the label classifier when uncertainty threshold is low."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697471611667,
        "cdate": 1697471611667,
        "tmdate": 1699636685897,
        "mdate": 1699636685897,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BrwCwfRHnX",
        "forum": "ljVCPV7jK3",
        "replyto": "ljVCPV7jK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a framework designed to facilitate the training of fairness-enhancing interventions when sensitive information is only partially observed. Their approach involves developing a classifier that seeks to predict the sensitive attributes of instances. Subsequently, they leverage instances with the least uncertain predictions, along with their predicted sensitive attributes, to train the fairness-enhancing intervention."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1 - The authors present a solution to a significant challenge that fairness-enhancing interventions may encounter when implemented in real-world applications.\n\nS2 - The experiments exhibit several strengths, including the diverse range of classification tasks involving different datasets, the validation of various aspects of the work. Particularly noteworthy are the investigations into the relationship between the threshold and the encoding of sensitive information by features, as well as the analysis of uncertainty in the sensitive attribute and the impact on the fairness of non-fairness-aware classifiers. Overall, the experiments provide supporting evidence for their central hypothesis.\n\nS3 - The paper's commitment to reproducibility is highly commendable. The detailed and transparent presentation of the experimental setup, data sources, and code availability significantly enhances the reliability and trustworthiness of the research findings. This transparency not only promotes the understanding of the study but also encourages further research in the field. \n\nS4 - The paper's writing is remarkably clear, making it easy for readers to grasp the content. Furthermore, the well-structured sections and the logical flow of information make it easy for readers to follow the research from start to finish.\n\nS5 - The paper effectively incorporates citations of pertinent related works, which helps contextualize their approach within the existing literature."
            },
            "weaknesses": {
                "value": "W1 - I believe there's a significant ethical concern in constructing a classifier with the objective of predicting the sensitive attribute of instances. This practice may raise legal and ethical issues, especially when individuals choose not to disclose this information willingly. Instead, it would be preferable if this classifier incorporated desirable privacy properties, as outlined in Diana et al. (2022).\n\nW2 - I find the comparison with respect to the state of the art to be lacking. The attribute classifiers chosen in this work, such as Proxy-kNN and Proxy-DNN, are rather simplistic and not well-documented in existing literature. Moreover, there are established attribute classifiers like those introduced by Diana et al. (2022) and Awasthi et al. (2021) that are not considered in this comparison.\nFurthermore, the selected methods for the 'baselines' (Lahoti et al., 2020; Hashimoto et al., 2018; Levy et al., 2020; Yan et al., 2020; Zhao et al., 2022) assume that they lack access to the sensitive attribute, making the experimental setting fundamentally different. Therefore, comparing the proposed approach with these state-of-the-art methods that operate under distinct conditions may not provide a fair assessment of its improvements and contributions to the field.\nTo offer a more comprehensive evaluation of the proposed method and better understand its advancements over existing techniques, I suggest including experiments involving Diana et al. (2022) and Awasthi et al. (2021).\n\nW3 - The authors assert in the abstract that 'our framework outperforms models trained with constraints on the true sensitive attribute,' referring to the results from Figure 2. However, this result only considers a single fairness-enhancing intervention. Additionally, their framework does not consistently outperform models trained with ground truth sensitive attribute values in all cases, making the statement partially true. This discrepancy is even more apparent in the results from Figure 9, where a different fairness-enhancing intervention is employed. It's unclear to what extent this outcome is influenced by the chosen fairness-enhancing intervention. Both interventions analyzed in the study share similarities, and it would be beneficial to examine a more diverse set of fairness-enhancing interventions to better understand the impact of the chosen intervention on the results. Therefore, I recommend that the authors modify the statement to emphasize that the framework outperforms models trained with constraints on the true sensitive attribute in some cases, and I encourage them to delve into the conditions under which this outperformance occurs.\n\nW4 - I believe that the related works section should also encompass privacy-related research. There are privacy-focused approaches, such as cryptographic solutions, that deal with situations where sensitive features are available but can only be accessed through secure cryptographic methods. Veale and Binns [1] or Kilbertus et al. [2] discuss scenarios where individuals' sensitive information is held by a third party or the individuals themselves, respectively, and can only be accessed via secure multiparty computation. Additionally, Jagielski et al. [3] explores cases in which sensitive features can only be utilized in a differentially private manner. Considering the nature of inferring sensitive information, privacy considerations become crucial. Therefore, it would be valuable to include these privacy-focused works in the related literature to provide a more comprehensive perspective.\n\n[1] Veale, M., & Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data & Society, 4(2), 2053951717743530.\n\n[2] Kilbertus, N., Gasc\u00f3n, A., Kusner, M., Veale, M., Gummadi, K., & Weller, A. (2018, July). Blind justice: Fairness with encrypted sensitive attributes. In International Conference on Machine Learning (pp. 2630-2639). PMLR.\n\n[3] Jagielski, M., Kearns, M., Mao, J., Oprea, A., Roth, A., Sharifi-Malvajerdi, S., & Ullman, J. (2019, May). Differentially private fair learning. In International Conference on Machine Learning (pp. 3000-3008). PMLR."
            },
            "questions": {
                "value": "Q1 - The experiments provide support for the assertion that discriminating against samples with more uncertain sensitive information is a challenging task. Rather than attempting to predict the sensitive information of instances (an action that is illegal and morally questionable) and use those instances for which you know the sensitive information with high confidence, why not directly utilize those instances for which the uncertainty is highest with respect to the sensitive attribute and train a non fairness-aware classifier on top of those instances? In other words, perhaps utilizing your attribute classifier to identify the most 'fair' samples based on high uncertainty in sensitive information might yield more ethically favourable results.\n\nQ2 - For classifiers that propose fairness-enhancing interventions while lacking information on the specific sensitive attribute considered in the experimental section (Lahoti et al., 2020; Hashimoto et al., 2018; Levy et al., 2020; Yan et al., 2020; Zhao et al., 2022), it's essential to clarify the dataset utilized. Do you feed them with the complete D1, D1 + D2, or only D1'?\n\nQ3 - The experiments demonstrate significant variations in results depending on the chosen uncertainty threshold. If this model were to be applied in a real-world scenario, do you have a practical method for selecting the optimal uncertainty threshold, rather than relying on trial and error to determine the best-performing value?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665154102,
        "cdate": 1698665154102,
        "tmdate": 1699636685780,
        "mdate": 1699636685780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bIGooID5Gu",
        "forum": "ljVCPV7jK3",
        "replyto": "ljVCPV7jK3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how to achieve a better fairness-accuracy tradeoff when no access to full sensitive attributes in the dataset. The method has two steps: (1) training a proxy classifier to predict the missing sensitive attributes with a student-teacher distillation and (2) thresholding the confidence on predictions. The paper evaluates the method on common fairness benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper targets an important problem. Given the increasingly stringent privacy constraint, the problem of studying fairness without full access to sensitive attributes is an important problem."
            },
            "weaknesses": {
                "value": "I have two major concerns.\n\n(1) If I am not mistaken, it seems the technical contribution of the paper is limited. The first step is not far from merely training a classifier to predict sensitive attributes, which is usually treated as a baseline in this area, with a little enhancement of student-teacher transfer learning. Overall, I do not see significant technical novelty. The second step is just to filter by thresholding prediction confidence. I have a hard time finding the technical contributions of the paper.\n\n(2) In experiments, the paper only compares to the basic bias mitigation algorithm, but there is literature of fairness with not full access to the sensitive attributes:\n\n[1] Diana, Emily, et al. \"Multiaccurate proxies for downstream fairness.\" Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.\n\n[2] Chen, Jiahao, et al. \"Fairness under unawareness: Assessing disparity when protected class is unobserved.\" Proceedings of the conference on fairness, accountability, and transparency. 2019.\n\n[3] Prost, Flavien, et al. \"Measuring model fairness under noisy covariates: A theoretical perspective.\" Proceedings of the 2021 AAAI/ACM Conference on AI, Ethic\n\n[4] Fogliato, Riccardo, Alexandra Chouldechova, and Max G\u2019Sell. \"Fairness evaluation in presence of biased noisy labels.\" International conference on artificial intelligence and statistics. PMLR, 2020.\n\n[5] Zhu, Zhaowei, et al. \"Weak Proxies are Sufficient and Preferable for Fairness with Missing Sensitive Attributes.\" International Conference on Machine Learning, 2023.\n\n[6] Yan, Shen, Hsien-te Kao, and Emilio Ferrara. \"Fair class balancing: Enhancing model fairness without observing sensitive attributes.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n\nI do not see why this paper should not be compared with any of those works."
            },
            "questions": {
                "value": "1. Can authors clarify if there is anything I misunderstood about the technical contribution? Note that there is no point in merely repeating the details of the method. The constructive communication would be to point out if I am wrong when I say the method is just training a proxy classifier with transfer learning and thresholding predictions.\n\n2. Can authors explain the reason why no comparison to any of the methods in the literature of fairness without full access to sensitive attributes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713150322,
        "cdate": 1698713150322,
        "tmdate": 1699636685659,
        "mdate": 1699636685659,
        "license": "CC BY 4.0",
        "version": 2
    }
]