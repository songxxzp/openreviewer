[
    {
        "id": "VbhLEsYFPA",
        "forum": "FAY6ORIvn5",
        "replyto": "FAY6ORIvn5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
        ],
        "content": {
            "summary": {
                "value": "The authors analyze the generalization (prediction beyond training set) power of persistent homology on graphs. They also generalize existing vectorization techniques by adding non-linear layers and give experimental studies on graph classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors study an important problem how good the persistent homology is in prediction tasks in graph representation learning. They explore several approaches to answer this question and give experimental analysis."
            },
            "weaknesses": {
                "value": "1. While the question addressed in this paper is important, its contribution appears to be relatively modest, especially when compared to the recent work by Morris et al. (2023). In that study, the authors delve into the predictive power of Graph Neural Networks (GNNs) through the lens of VC-dimension, and the authors of the current paper adapt their methods to the context of Persistent Homology. While the theoretical results are intriguing, their practical relevance in the machine learning domain remains questionable. \n\nThe provided bounds are often complex and abstract, making them challenging to compute in real-world applications. Given the extensive, nine-page proof section, which requires a thorough review, the paper might be better suited for a journal focused on statistics or applied topology rather than an ML venue. The heavy theoretical content, with limited applicability, calls into question the practical utility of the paper's findings in the ML community.\n\n2. The paper's readability and coherence could be significantly improved, as it currently suffers from the need for clearer definitions and explanations of key concepts. The exposition and the paper's overall objective should be more explicitly stated to facilitate a better understanding of its content."
            },
            "questions": {
                "value": "Figures 3 and 4 are very interesting. In Figure 4, while the width is increasing, empirical values of the generalization gap stay very low. How do we explain this? On the other side, could you provide insights into the relationship between the generalization gap and the training set size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_yvKz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698523910209,
        "cdate": 1698523910209,
        "tmdate": 1699636937547,
        "mdate": 1699636937547,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9fRRypk9mJ",
        "forum": "FAY6ORIvn5",
        "replyto": "FAY6ORIvn5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
        ],
        "content": {
            "summary": {
                "value": "This paper provides first theoretical bounds for generalization of persistent homology (PH) on graphs. The results are supported with an experimental study on 5 real-world graph classification benchmarks. Moreover, additional experiment illustrates how the bounds can be used to regularize the PH pipeline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths are already described in the summary."
            },
            "weaknesses": {
                "value": "I did not identify any major weaknesses in the paper, although I also did not check all the proofs in the supplementary material. However, the clarity of presentation could be significantly improved, see my questions and comments below."
            },
            "questions": {
                "value": "(Q1) Related work: The related literature is missing. Does \u201cgeneralization capabilities of PH methods remain largely uncharted territory\u201d imply that there are no earlier works in this direction? If so, you can make it more explicit.  How is the recent paper [1] related to your work?\n\n[1] Immonomen, Souza and Garg, Going beyond persistent homology using persistent homology\n\n(Q2) Section 3.1: It is not very clear how Section 3.1 fits into the whole story of your paper, in particular since you write that expressivity and generalization can be at odds with each other, but you do not elaborate further. This issue is pronounced more in (the very nice) Figure 2, which is missing Proposition 1 and 2, and Lemma 2 and 3. What is the reason that Morris et al., 2023 and Rieck, 2023 that you rely on in Section 3.1, are not discussed in the Related work? Why do the experiments not validate these theoretical results too?\n\n(Q3) Table 1: In the Discussion, you write the following: \u201cIn Table 1, the study provides a valuable resource by depicting the resulting bound dependencies on various parameters. This information is instrumental in estimating the overhead introduced by PH in the generalization performance of conventional multi-layer perceptron.\u201d Immediately I was hoping to see some discussion in this direction, but I am not sure if the next two paragraphs are related to Table 1? Where does the $\\sqrt{\\ln b}$ appear, and where do we see $h \\sqrt{\\ln h}$? References to particular lemmas, theorems or tables can improve readability.\n\nIn general, can you provide some intuition about what makes the generalization bounds for PH different from other models, and/or what properties of PH do you use to obtain your theoretical results? Is it crucial that the input is a graph?\n\nMoreover, could you summarize the \u201ckey insights about the limits and power of PH methods\u201d? What do we learn from your paper about the generalization ability of PH?\n\n(Q4) \u201cWe report additional results across different epochs and hyper-parameters in the supplementary material.\u201d These results are not included?\n\n(Q5) Figure 5 is not very informative. Could it be replaced by including the results for the line and triangle point transformations (and their correlations) into Figure 3 and Figure 4?\n\n(Q6) Notation: The notation could be improved, what is the current logic? For example, you could e.g. use small case/capital case/Greek alphabet for graph nodes/sets/functions, and then be consistent. Often, you use the same notation for different things: e.g., S for both training set and the upper bound in Lemma 6, m for the size of training data and the maximal number of distinguishable graphs, omega for the PersLay weight function and for the hypothesis parameters, etc. Could you include a notation table? For instance, it took me quite some time to find what b is when seeing it appear at the end of Section 3. As you will see, a lot of the minor comments below would likely be resolved with a table summary of improved notation.\n\n\nMinor comments:\n\n-\tIn the paragraph on PAC-Bayesian Analysis, you define L_S, gamma and L_D, gamma before S, D, gamma and L are introduced. Also, for better clarity, the order of the formulas here should be reversed? Moreover, you use the notation L here, later within the line point transformation, and later also for a layer.\n-\tIn the paragraph on the Analysis Setup, when writing h_1=q, reminder the reader briefly what q is, or at least reference Figure 1.\n-\tThe acronym FWL is never introduced?\n-\tFor the node with Lemma 1 in Figure 2, you could reference Neyshabur (2018) to make it clearer that this is an earlier result and not the contribution of this paper.\n-\tIt is not clear to me when you use |x|, and when ||x||? Is ||.||_F from Table 1 ever described?\n-\tAt the end of statement of Lemma 4, I suggest to reference \u201c(see PersLay in Section 2)\u201d, so that the reader can easily find what AGG and Phi, Lambda, Gamma and L are.\n-\tAdd full stop at the end of Lemma 4 and Lemma 5.\n-\tWhich norm do you use for persistence diagrams, i.e., what is |D(G)|?\n-\tWhat is e in Lemma 6?  \n-\tWhy do we see L_D, 0 in Theorem 2, what scenario does gamma=0 reflect, can you provide some intuition? Also, you start this theorem with \u201cLet w  = \u2026\u201d, and then claim that \u201cfor any w, we have\u2026\u201d? On a related note, later in Section 4 you write that generalization gap is measured as L_D,0 \u2013 L_S, gamma=1, but you do not provide more info?\n-\tDoes the description of h, d and W_i in the caption of Table 1 reflect only the third row, or the complete table? In the latter case, make this description a separate sentence. Do you also want to mention again also what q, l, e, beta are?\n-\tWe compute correlation coefficients between -> We compute [specify which] correlation coefficients \\rho between\n-\t\u201calpha is a hyperparameter that balances the influence of the two terms\u201d: alpha should probably be replaced with lambda?\n-\t\u201cOur research highlights the significance of leveraging the principles of regularization to enhance the performance of machine learning models across diverse applications.\u201d I found this sentence rather surprising (not the focus of this work), could you rephrase or elaborate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_ys8K"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790325781,
        "cdate": 1698790325781,
        "tmdate": 1699636937337,
        "mdate": 1699636937337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4VNRvJrdkU",
        "forum": "FAY6ORIvn5",
        "replyto": "FAY6ORIvn5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the generalization performance of persistent homology is given in terms of PAC-Bayes. Normalized margine bounds are given via PersLay, a method that encompasses various vectorizations of the Persistent diagram. Normalized margine bounds has been theoretically proven and experiments have confirmed the theorem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Theoretical derivation and proof of normalized margine bounds for persistent diagrams of graphs are given.\n- It is basically an analogy to Neyshabur et al. (2018), but combined with PersLay, which encompasses vectorization of various persistent diagrams to apply to persistent homology."
            },
            "weaknesses": {
                "value": "The purpose of the main theorem seems unclear. Although the theory claims a theory of generalizability of the persistent homology of graphs, what the actual theorem shows is a generalization bound for maps that combine PersLay, ReLu, and DNN in the persistent homology. In fact, Neyshabur et al. (2018) explicitly states that it gives generalization bounds for DNNs. There seems to be a gap between the generalization performance of Persistent homology and the generalization bounds of PH, the mapping. Also, PH is only an example of one network and not for a general network.Currently, it appears to be a derivation of the generalization boundary of a self-defined network. Whether one is arguing for generalization bounds for persistent homology itself or for generalization bounds for networks using persistent homology, it seems to me that the arguments need to be organized and additional discussion is needed.\n\nThe biggest complaint is that it is extremely reader-unfriendly. For example, the definition of $gamma$-margine loss was written some time after its first appearance. It doesn't even say what k-FWL is; it may be Folklore Weisfeiler-Lehman, but it is not self-evidently recognizable to all readers of the subject. Map PH seems to be the entire architecture of Fig. 1, but the definition is unclear and the caption of Fig. 1 is sometimes described as PersLay's architecture, making it difficult to grasp."
            },
            "questions": {
                "value": "Please comment on the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_qwyi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834113600,
        "cdate": 1698834113600,
        "tmdate": 1700657977817,
        "mdate": 1700657977817,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ci6ChMngq7",
        "forum": "FAY6ORIvn5",
        "replyto": "FAY6ORIvn5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the generalization power of PersLay and derives new generalization bound. In addition, the paper discusses a VC-dim lower bound for persistent homology (PH) in terms of the WL-test on graphs. Experimental results demonstrate that the theoretical bounds can well capture the trend observed in the empirical generalization gap."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Perslay is an excellent model in TDA and it boosts the incorporation of PH with GNNs. Due to its effectiveness, Perslay has no theoretical guarantees. This paper provides new insights about the generalization of Perslay and provides new upper bound.\n2. This paper extends the expressive power of PH in terms of WL to get a lower bound regarding the generalization ability of PH."
            },
            "weaknesses": {
                "value": "1. 1.The proofs are based on some assumptions, e.g. the filtrations of PH are fixed. However, many recent works [1][2] are based on flexible filtration function when using Perslay. Perslay itself is a powerful tool to vectorize persistence diagrams (PD) and provides informative representations. It can be plugged into many other models when using PH, and these models already have strong generalization power, such as GNNs. Therefore, analyzing the bound of Perslay or PH may not be necessary.\n2. This paper merely investigated the generalization one vectorization tool of PD, i.e. Perslay, thus having limited contribution. Researchers who are interested in the generalization of PH on graphs may be more interested in other representations of PD, such as persistence images and deep sets [3], and in models with flexible filtrations [2].\n\n[1] Hofer Christoph, et al. \"Graph filtration learning.\"\u00a0ICML 2020.\n\n[2] Horn Max, et al. \"Topological graph neural networks.\"\u00a0ICLR 2022.\n\n[3] Manzil Zaheer, et al. \u201cDeep sets\u201d, NIPS 2017."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7697/Reviewer_1CoA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7697/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843002008,
        "cdate": 1698843002008,
        "tmdate": 1700646436245,
        "mdate": 1700646436245,
        "license": "CC BY 4.0",
        "version": 2
    }
]