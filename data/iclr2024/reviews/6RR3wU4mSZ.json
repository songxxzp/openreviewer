[
    {
        "id": "xucf5Gd6nM",
        "forum": "6RR3wU4mSZ",
        "replyto": "6RR3wU4mSZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_nr9P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_nr9P"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach for adaptively identifying the most important keys for each query using a k-NNS approach with a ranking-based algorithm. This method allows mapping keys and queries to a higher-dimensional space, eliminating the constraint that all keys must have the same norm. Furthermore, the authors extend the implementation of prioritized DCI to support incremental updates to the database."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper combines several techniques to accelerate the computation of the attention matrix by pruning the k matrix.\n* The proposed approach is comprehensively evaluated on LRA and LLM benchmarks, and it outperforms SOTA approximation methods, demonstrating its effectiveness."
            },
            "weaknesses": {
                "value": "The adaptation of the k-NNS method may potentially make the computation less parallelizable. It is essential to consider the scalability of the proposed method, even on CPU-based platforms. Leveraging multi-threading and distributed computing for scalability is a relevant concern."
            },
            "questions": {
                "value": "* It would be valuable if the paper provided insights into the scalability of the proposed method in terms of performance (compute time) on different CPU platforms with varying numbers of cores. How well can the approach be parallelized or scaled up on different hardware configurations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420210642,
        "cdate": 1698420210642,
        "tmdate": 1699636039944,
        "mdate": 1699636039944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PslTmneGqR",
        "forum": "6RR3wU4mSZ",
        "replyto": "6RR3wU4mSZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for accelerating inference of long-sequence attention-based Transformers. The proposed method works with pretrained Transformers and does not require retraining. The critical point is to use Prioritized DCI as the k-NNS algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method does not require retraining the model. The retraining may be expensive and unaffordable.\n2. It can be applied to a broad range of LLMs since it has no assumptions on the Transformer architecture.\n3. The proposed method provides a Pareto optimal solution in terms of time and task-related performance (e.g., accuracy), compared with the baselines."
            },
            "weaknesses": {
                "value": "### Major issues\n1. The proposed method seems orthogonal to the inference platforms. Why do the authors focus on the CPU settings? What about other inference engines, such as GPUs, TPUs, and other accelerators?\n2. The authors discuss many related methods in Section 2. However, only few of them are used as the baselines in empirical comparisons? Could the authors add more baselines? Some methods may need retraining. It is better to list more results even if some baseline needs retraining or specific architectures.\n3. Could we apply the proposed method into training?\n\n### Minor issues\n1. It is not clear how IceFormer will perform on short and medium length of sequences.\n2. Could the authors visualize Tables 1 and 2?\n3. What are the limitations and potential negative impact of the proposed method?"
            },
            "questions": {
                "value": "1. What may be the future directions and extensions?\n2. The authors mentioned that the code will be released upon acceptance. I do not find implementation in the supplementary material. Is it possible to release it for the reviewers' reference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Reviewer_d3mB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647448258,
        "cdate": 1698647448258,
        "tmdate": 1700761775289,
        "mdate": 1700761775289,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jK99mN5580",
        "forum": "6RR3wU4mSZ",
        "replyto": "6RR3wU4mSZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_dxGt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_dxGt"
        ],
        "content": {
            "summary": {
                "value": "Transformers, especially large language models (LLMs) like GPT-4, have shown great promise in handling long input sequences. However, their deployment on CPUs is challenging due to the quadratic time and space complexity of the self-attention mechanism. Existing methods to address this either require model retraining, are not generalizable, or compromise accuracy. The proposed method, IceFormer, aims to accelerate inference time without retraining, maintaining accuracy, and offering fast inference across various LLMs. It achieves this by optimizing the self-attention mechanism, leveraging the sparsity of the attention matrix, and using k-Nearest Neighbor Search (k-NNS). Experimental results on multiple benchmarks show that IceFormer efficiently outperforms other methods in both speed and accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- This method can accelerate inference time (only CPU) for pretrained transformers without the need for expensive and time-consuming retraining.\n- Unlike some other methods, IceFormer ensures that there is minimal approximation error, crucial for LLMs where errors in initial layers can cascade through subsequent ones.\n- Beyond just accuracy, the method also guarantees rapid inference times, making it particularly suitable for CPUs.\n- By capitalizing on the sparsity of the attention matrix and utilizing k-Nearest Neighbor Search (k-NNS), IceFormer effectively reduces the computational burden of the self-attention mechanism."
            },
            "weaknesses": {
                "value": "This paper has multiple concerns for acceptance at ICLR 2024:\n\n1) The most glaring issue is its reliance on outdated methods from 2020 and 2021, with some even referencing the 2017 vanilla transformer. This dated focus suggests a lack of recent advancements in the field. One must ponder why the topic of 'efficient transformers' isn't garnering contemporary attention. Historically, efforts to enhance transformer efficiency via attention layer optimization waned with the introduction of Large Language Models (LLM). Moreover, these methods face challenges in adhering to the scaling-law (https://arxiv.org/abs/2207.10551\n).\n\n2) Although the paper touts the efficiency of LLM, it primarily showcases small-scale toy examples or, at best, the 7B LLaMa model. Notably, the latter, when applied to a long context, demands an extensive duration, upwards of several seconds, merely for the attention computation. The practical relevance of such scenarios is debatable.\n(Honestly, I'm confusing about the exact meaning of 'wall clock time of attention module'.)\n\n3) Presently, in the realm of cloud-based LLMs, MQA/GQA (https://arxiv.org/pdf/2305.13245.pdf) emerges as the leading approach for attention layer optimization. I would be interested in hearing your views on this."
            },
            "questions": {
                "value": "included in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760911151,
        "cdate": 1698760911151,
        "tmdate": 1699636039679,
        "mdate": 1699636039679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fPySskQyW0",
        "forum": "6RR3wU4mSZ",
        "replyto": "6RR3wU4mSZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to address the computational challenges associated with the quadratic complexity of self-attention in long sequences. It proposes using k nearest-neighbor search as a method for approximating a sparse attention matrix, thereby reducing the computational cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-motivated, tackling a pertinent issue in the deployment of large language models.\n2. The idea of employing ranking-based algorithms over bucketing-based algorithms presents an interesting potential for complexity reduction."
            },
            "weaknesses": {
                "value": "1. The paper does not adequately support its claim that Prioritized DCI outperforms LSH, lacking both theoretical and empirical evidence.\n2. There is insufficient clarity in the algorithm's implementation details, making it difficult to understand the actual complexity and the mechanics of the proposed method.\n3. The evaluation methodology for measuring inference time is not comprehensive. It appears the method is optimized for CPUs but lacks evidence of similar efficacy on GPUs."
            },
            "questions": {
                "value": "1. Are the baseline results for execution time from original papers or are they reproduced results specific to CPU performance?\n2. Could you provide the typical and range of values for k and m used in the experiments?\n3. What specific settings were used for CPU inference time evaluation? Why limit the experiments to only four CPU threads? Were any BLAS kernels like MKL used in the implementation?\n\nWhile the paper is well-motivated and presents a potential approach to an important problem, there are significant issues that undermine its contributions. Specifically, the paper lacks rigorous evidence to support its claims and fails to provide a thorough methodology for evaluating its proposed solution. Further revision is needed to substantiate the claims and provide a more comprehensive understanding of the algorithm and its performance across various hardware."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1134/Reviewer_ad3y"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790243707,
        "cdate": 1698790243707,
        "tmdate": 1699636039594,
        "mdate": 1699636039594,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0QwvTp50S5",
        "forum": "6RR3wU4mSZ",
        "replyto": "6RR3wU4mSZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_5Bmg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1134/Reviewer_5Bmg"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present IceFormer, a novel method for improving the inference efficiency of pretrained Transformers on the CPU without requiring re-training. Leveraging the insight that k most important keys can be identified by performing k-NNS on the key\nembeddings using the query embedding, IceFormer employs the Prioritized DCI algorithm to identify the top-k keys. The evaluations on three benchmarks illustrate the effectiveness of this approach in reducing the quadratic time and space complexity of vanilla Transformers with competitive performance compared to other efficient-attention variants."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper sheds new insights on an important problem -- choosing the top-k keys in sparse attention for inference acceleration. The authors show that the exact kNNS algorithm is critical for the success of sparse attention. \n2. The paper is well-written and easy to follow.\n3. The evaluation section shows strong performance compared to other efficient attention works."
            },
            "weaknesses": {
                "value": "1. The use of Prioritized DCI k-NNS algorithm needs more experimental or theoretical justification. The authors claim \"ranking-based algorithms is better aligned with how attention weights\", if so, how would other ranking-based algorithms perform? On top of that, the authors show an evaluation of different kNNS algorithms on fashion-mnist-784 dataset in Section 5.1. It would be better to show the exact setup (eg. model architecture) they used, and compare them on a few more tasks (for instance, text generation, token classification). \n2. Fig.5's y-axis needs to be annotated with units (especially for latency)."
            },
            "questions": {
                "value": "1. Why is CPU the target platform for IceFormer? Are there CPU-specific architectural optimizations, on top of the time/space saved due to sparse attention? \n2. I hope to clarify with the authors: in the long-context evaluation (Fig.5), is the baseline (vanilla Transformer) referring to the Vicuna-7b-v1.5-16k model, or the original Transformer model? \n3. This paper describes a novel method for improving the inference efficiency and evaluates on CPU. But it seems that this method could potentially also apply to GPU, which is more often used. Are there specific reasons/constraints to use CPU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831202939,
        "cdate": 1698831202939,
        "tmdate": 1699636039461,
        "mdate": 1699636039461,
        "license": "CC BY 4.0",
        "version": 2
    }
]