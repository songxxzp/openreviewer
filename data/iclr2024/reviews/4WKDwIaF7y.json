[
    {
        "id": "se6AwtobSp",
        "forum": "4WKDwIaF7y",
        "replyto": "4WKDwIaF7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_1P81"
        ],
        "content": {
            "summary": {
                "value": "-\tThe paper proposes optimization methods using an extra gradient to improve the performance of SAM. In section 3, the authors propose a maximization with respect to $\\varepsilon$, while SAM only focuses on maximization with respect to $w$. Then, the authors propose simple optimization schemes, such as EG-SAM and OG-SAM, to mitigate the computational burden of extra gradient methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe paper has a solid mathematical background to derive the optimization methods. Furthermore, the proposed method can alleviate the negative effects of SAM, which can get stuck at saddle points.\n-\tThe paper combines the proposed method with other SAM-related papers, which can reduce the computational burden of EG-SAM.\n-\tThe proposed method has the same convergence rate as SAM, which does not require additional training epochs.\n-\tThe paper demonstrates the performance improvement of using an additional gradient in a wide range of datasets, outperforming existing methods. The authors include various architectures and datasets, including the noise settings."
            },
            "weaknesses": {
                "value": "-\tPlease refer to questions."
            },
            "questions": {
                "value": "I will happily raise the score if the authors can address the following questions:\n\n1.\tI am curious about the motivation behind the proposed method. I agree that using the extra gradient (where $w$ and $\\varepsilon$ correspond to $x$ and $y$ in Section 3) might be beneficial for training. Can you clarify why  EG (and correspondingly OG) are straightforward solutions in terms of SAM for escaping saddle points? Or the purpose of EG is to increase the loss of $L(w+\\varepsilon)$ by perturbing $\\varepsilon$? The loss maximization does not seem to align with performance gain [1].\n\n2.\tTo push further the question 1, I am not convinced that SAM+EG and EG-SAM (also OG) are not highly correlated because $\\varepsilon$ in EG-SAM is not calculated to enlarge the loss with respect to $\\varepsilon$. Rather, even though I did not calculate the ODE accurately, the behavior of EG-SAM seems somewhat related to implicit methods in ODEs (thus stable) rather than using explicit methods by using $\\hat{w_t}$ obtained from the next step in SGD terms.\n\n3.\tI agree that EG-SAM can optimize better in quadratic loss, as shown in Section 4 with ODE. However, the optimization itself is discrete, and some behaviors (such as SGD or catapult effects) cannot be explained in ODE settings. Thus, could the authors demonstrate the effects of EG-SAM in real situations with **SGD** to escape saddle points, such as in Fig. 7 in [2]?\n\n4.\tThe authors provide information about the effects on computational burden compared to %SAM. However, as SAM and EG-SAM both require 100%, they actually differ in terms of computational time. Therefore, wall clock computational time is needed to assess the computational overhead during real training.\n\n[1] https://arxiv.org/abs/2206.06232 \n\n[2] https://arxiv.org/abs/2301.06308"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698144027986,
        "cdate": 1698144027986,
        "tmdate": 1699636613785,
        "mdate": 1699636613785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "onk28kIpmn",
        "forum": "4WKDwIaF7y",
        "replyto": "4WKDwIaF7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new version of multi-step SAM. They discuss convergence properties of the algorithm and present numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method seems to outperform SAM in the settings studied."
            },
            "weaknesses": {
                "value": "- The idea of multi-step SAM is not new and has been explored before. The EG and OG versions proposed here are new, but I think the contribution of the work is marginal.\n\n- The theory is nice to have, but is not really insightful and the proofs are fairly standard.\n\n- I'm not fully convinced with the numerical experiments. They consider CIFAR + ResNet, and only one example of ImageNet + ResNet. No experiments on transformers or language models are done, which makes the experiments rather limited. Also, important version of SAM such as gSAM (https://arxiv.org/pdf/2203.08065.pdf) and mSAM (https://arxiv.org/pdf/2302.09693.pdf) are not included in the experiments. Also, I didn't find runtimes of methods, which I might've missed. \n \n- The presentation can improve. There are so many versions and abbreviations (EG,OG, AO, AMO) that it's difficult to see the points the authors are trying to make."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5815/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5815/Reviewer_gBpE",
                    "ICLR.cc/2024/Conference/Submission5815/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698199940924,
        "cdate": 1698199940924,
        "tmdate": 1700689445541,
        "mdate": 1700689445541,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gc1JP4j9Bv",
        "forum": "4WKDwIaF7y",
        "replyto": "4WKDwIaF7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_hY3b"
        ],
        "content": {
            "summary": {
                "value": "- The paper argues that sharpness-aware minimization can be stuck in saddle-points (of the loss function). \n- This motivates the EG-SAM scheme which modifies SAM by perturbing an *extrapolated* point (computed with the same gradient information as the perturbation). \n- Their alternative (optimistic) variant, OG-SAM, replaces the gradient in the extrapolation update with the gradient at the previous perturbed point. \n- They combine both methods with an existing adaptive policy (AE-SAM) and show favorable numerics on CIFAR10, CIFAR100 and ImageNet. \n- Theoretically, they provide a convergence result under gradient-Lipschitz and bounded variance for the average gradient norm with decreasing stepsize and perturbation radius $\\rho$.\n- They also show that an ODE related to EG-SAM has a smaller region of attraction then the SAM ODE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is easy to follow\n- It provides a good literature overview"
            },
            "weaknesses": {
                "value": "I have a range of concerns starting with the formulation itself:\n\n- The EG-SAM update can be rewritten as:\n\n    $\\tilde w_t = w_{t-1} + (\\rho/\\|\\nabla L(w_{t-1})\\| - \\eta_t) \\nabla L(w_{t-1})$\n\n    $\\tilde w_t = w_{t-1} - \\eta_t \\nabla L(\\tilde w_{t})$\n\n    So when the stepsize is taken decreasing (as in the experiments) it is simply a scheme that perturbs less adversarially in the early training phase and more later. It would be instructive to plot the gradient norm at $w_{t-1}$ throughout training. How does EG-SAM compare against SAM with an (increasing) stepsize schedule for $\\rho$?\n- OG-SAM appears unnecessary since it does not save on gradient computations: EG-SAM simultaneous computes the extrapolation and perturbation computation so it already only requires two gradient computations. What is the motivation for OG-SAM in not saving gradient computations?\n\nTheoretically:\n\n- The convergence result in Section 4.2 requires strong assumptions on both the problem class and the a decreasing perturbation radius $\\rho$. Requiring both gradient-Lipschitz and function Lipschitz simultaneously is quite restrictive. More importantly, by decreasing $\\rho$ you essentially avoid the difficulty of the perturbation. One indication that your assumptions are too strong is that your argument would hold for any kind of (feasible) perturbation.\n    \n- I appreciate the attempt to contextualize the convergence result in Section 4.1 within the literature for extragradient (EG), but I find the comparisons quite misleading. Stating that their assumptions are restrictive whereas your assumption are more general is problematic. They are different problems entirely for multiple reasons:\n    - Different solution concepts: The EG literature treats convergence to a equilibrium, whereas the EG-SAM results only treats the gradient of the minimizing player.\n    - Different application: You apply EG to the minimization problem (not to treat the minimax)\n    The same issue appears in other sections when comparing rates:\n    - Section 3.1: even if assuming the loss in Eq. 1 is convex in $w$ then it is also *convex* in $\\varepsilon$, so the rate of $\\mathcal O(T^{-1/4})$ does not apply (we could still hope to be locally linear maybe for the max problem). The rates for SAM that are being compared against is for the gradient norm (so it is not considering convergence of the max-player).\n    - Section 3.2 mentions that the SAM+OG rate is much slower than SAM, but you are comparing both different problem settings (nonconvex-strongly concave vs nonconvex-nonconvex) and performance metrics.\n\n- The analysis in section 4.1 seems to follow directly from Compagnoni et al. 2023 Lemma 4.3: Eq. 19 is simply the SAM ODE (Eq. 18) with a different stepsize. More importantly, the extrapolation stepsize $\\eta$ in Eq. 19 should arguably not be fixed. Otherwise it corresponds to a discrete scheme which takes the extrapolation stepsize much larger than the actual stepsize.\n\nEmpirically: \n\n- I appreciate the effort of providing standard deviations for the experiments, but it is arguably even more important to sweep over $\\rho$ when comparing different SAM variants. The optimal $\\rho$ can differ significantly across models/datasets and can influence the ordering of the methods significantly.\n\n\nMinor:\n\n- $\\nabla F$ notation can be misleading since it is not a gradient. I suggest simply letting $F(z)=(\\nabla_x f(x,y), -\\nabla_y f(x,y))$\n- Maybe explicitly mention that the number of gradient oracles are 4 in Eq. 12 (instead of saying \"two approximated gradient steps\")\n- line 4 in Algorithm 3: specify what the AE-SAM update is\n\nTypos:\n\n- Eq. 9 and 10 has a missing stepsize for $\\varepsilon$\n- Second equation of section 3.2 has a missing hat and stepsize\n- Theorem 4.6 misses an index $t$ for $\\rho$"
            },
            "questions": {
                "value": "- What is the value of $\\rho$ used for the different methods in the experiments?\n- Why is EG-SAM 150% SAM in Table 1?\n- Can you plot $\\|\\nabla L(w_{t-1})\\|$ throughout training? (or a mini-batch estimate)\n- Section 3.3 what is meant by \"has an approx solution [...] so one can directly apply extra-gradient\". Are you arguing based on Danskin's theorem? (the cited reference doesn't seem to mention the same problem)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698328837329,
        "cdate": 1698328837329,
        "tmdate": 1699636613535,
        "mdate": 1699636613535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CClcC0zHro",
        "forum": "4WKDwIaF7y",
        "replyto": "4WKDwIaF7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5815/Reviewer_rtXV"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to incorporate look-ahead mechanism into SAM to improve the performance of SAM. The authors proposed 3 methods: SAM + EG, SAM + OG and AO + SAM, in which the first one used extra-gradient information, the second method proposed to use optimistic-gradient and the last one used an adaptive gradient. The authors provided convergence analysis of those methods by proving that their convergence rates are similar to that of SAM. They also did some empirical works to verify the convergence rate, accuracies of their methods in CIFAR-10, CIFAR-100 and ImageNet using several network structures like ResNet-32, Resnet-18, WideResNet28-10."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well presented, clear structure. \nThe authors provides theoretical analysis for their methods. I did not check all the technical details, but I have a random check, it seems to be fine. \nThe experimental results have shown the improvement of the method over SAM. Even though the improvement is only around $1\\%$, but it is shown consistently over several network structures."
            },
            "weaknesses": {
                "value": "Those proposed methods are only some variations of SAM with small changes.\nHence, the authors did not encounter further difficulties to obtain those convergence rate results.\nThe experiment results show  incremental enhancement  to SAM.  \nIn comparison with SAM paper, the authors had done less experiments for comparisons, i.e.  ResNet-101, ResNet-152 etc\nThere is also a little intuitive explanation/proof for advantages of the proposed methods. \nIt is decent work, not something fancy to read."
            },
            "questions": {
                "value": "No question"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5815/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804622037,
        "cdate": 1698804622037,
        "tmdate": 1699636613439,
        "mdate": 1699636613439,
        "license": "CC BY 4.0",
        "version": 2
    }
]