[
    {
        "id": "Fnlnh4Nfwf",
        "forum": "NnYaYVODyV",
        "replyto": "NnYaYVODyV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Perceptual Group Tokenizer model (PGT), which utilizes grouping operations for visual feature extraction. It demonstrates competitive performance in self-supervised learning while reduces the computation complexity into O(n*m) compared with the complexity O(n^2) of vision transformer. The PGT also offers adaptive computation without re-training via flexible number of grouping tokens, and offers interpretability in feature representations.  Quantitative experiments and visualization demonstrate the PGT's effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of Perceptual Group Tokenized is intrersting. By alternatively refining the group tokens and image feature tokens, the computation complexity has been reduced comparing with self-attention. \n2. The adaptive computation ability of PGT is good, which could be used to meet the needs of different inference speeds. \n3. The interpretability of model is relatively good."
            },
            "weaknesses": {
                "value": "1.  The experiment is not sufficiently comprehensive and the results are weak. Since a new model architecture is proposed, the performance under both supervised learning and self-supervised learning should be presented to show its\u2019 universal. In performance comparison, the listed comparable architectures should be compared with the same pre-training/supervised-training strategy, a similar amount of parameters. It will be better to do experiment with more model sizes(e.g, PGT-B, PGT-S,PGT-Ti or more ) and compare with counterparts under different sizes. The performance on downstream tasks are not clear, although segmentation performance is reported, more comparisons with relative methods are not sufficient.\n2.  Besides the number of parameters, for fair comparison, the computational cost(Gflops or MACs) should be shown. The inference time is missing, which is important for evaluating models\u2019 efficiency."
            },
            "questions": {
                "value": "1. Do the group tokens in each block generate independently? If not, maybe the relavant description is ambiguous. If so, why don't the group tokens take its' output of the previous block as the next block's input?\n2.  Could the iterative grouping processes be considered multiple cross-attention between input tokens and group tokens? What's the difference or advantages of the iterative grouping processes compared with cross-attention? How much does the number of grouping iterations matter?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Reviewer_ABtJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698495652361,
        "cdate": 1698495652361,
        "tmdate": 1699635970511,
        "mdate": 1699635970511,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bKyKLgjGiG",
        "forum": "NnYaYVODyV",
        "replyto": "NnYaYVODyV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a self-supervised learning method that relies only on grouping operations. They call their model the Perceptual Group Tokenizer (PGT). The model's linear probe performance is on par with other state-of-the-art models such as ViT. It also produces a highly interpretable representation and shows some interesting properties like being somewhat adaptable at inference time. Overall, the PGT demonstrates that grouping operations alone can produce a rich visual representation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The writing of this work is very clear. The authors do a great job of motivating the problem and contextualizing its significance. As the authors note, perceptual grouping has historically been an important concept in computer vision, but it had not been proven to be as powerful as feature detection. This work proposes a novel way to use grouping principles for self-supervised representation learning on large-scale natural image data. \n\nThe approach also appears to be well done. The analyses of number and size of grouping are also insightful, and demonstrate a thorough evaluation of their model. Making the connection to self-attention in ViT is also an important and original contribution for understanding why both the PGT and ViT work."
            },
            "weaknesses": {
                "value": "One of the main weaknesses I see is that the PGT performs very similarly to the ViT. As the authors note, their method is in some sense a more general version of the self-attention approach so it is somewhat unsurprising that performance is on par with ViT. I think the paper could be strengthened by more discussion about why the perpetual grouping tokenizer might be better or more useful than other methods like ViT.\n\nAlong similar lines, I think the interpretability results in Figure 6 and section 4.5 could be better contextualized. It is not immediately obvious how the interpretability conveyed by the attention maps compares to other state-of-the-art models. In addition, this analysis could be strengthened by a discussion of how the attention maps at each stage do or do not agree with what we would expect of human vision."
            },
            "questions": {
                "value": "As I mention in the weaknesses, could the authors touch on what are the benefits of the PGT over ViT given that they perform similarly on the ImageNet 1k linear probe?\n\nIn a similar vein, I would also like some of the model interpretability results to be expanded on in the context of other state-of-the-art methods as well as human visual perception.\n\nFinally, I thought adaptive computation (Section 4.3) was a bit too brief for me to appreciate. Could the authors explain what they mean by adaptive computation and why it is a benefit of PGT?\n\nGenerally, I enjoyed this paper. I am open to increasing my score if these concerns are addressed,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Reviewer_TtGg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620518557,
        "cdate": 1698620518557,
        "tmdate": 1699635970422,
        "mdate": 1699635970422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AY5Qg4MfL1",
        "forum": "NnYaYVODyV",
        "replyto": "NnYaYVODyV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Perceptual Group Tokenizer (PGT), a ViT-like architecture that clusters tokens to implement the principle of perceptual grouping for visual recognition. Specifically, PGT combines a slot attention-like token grouping module within ViT blocks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is generally well-written.\n- Prior work on feature detection and perceptual grouping is well-discussed, although recent works on token grouping are missing."
            },
            "weaknesses": {
                "value": "**Limited technical novelty**\n\nThe proposed method is essentially a combination of ViT and slot attention (or its token grouping variants).\nThe concept of token grouping has been extensively studied in prior work. Please refer to the related work section of ToMe [1] and CoCs [2] for examples.\nOn the other hand, DINOSAUR [3] combined a ResNet/ViT encoder with slot attention to scale up object-centric learning for real-world images, which is also relevant to this work.\n\n[1] Token merging: Your vit but faster. ICLR'23.\\\n[2] Image as Set of Points. ICLR'23.\\\n[3] Bridging the Gap to Real-World Object-Centric Learning. ICLR'23.\n\n---\n**Unclear empirical benefits**\n\nIn addition to the technical novelty, the empirical merit of the proposed method is unclear.\n1. Performance: It is not better than prior works, such as DINO.\n2. Efficiency: Several efficient ViT-based works exist, such as ToMe, DynamicViT [4], A-ViT [5], etc.\n3. Adaptive computation w/o retraining: ToMe claims to offer the same benefit.\n4. Grouping visualization: Other grouping methods also offer similar advantages (Fig. 4 of ToMe). CAST [6] is even better in this regard.\n\n[4] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. NeurIPS'21.\\\n[5] A-ViT: Adaptive Tokens for Efficient Vision Transformer. CVPR'22.\\\n[6] CAST: Concurrent Recognition and Segmentation with Adaptive Segment Tokens. arXiv'22."
            },
            "questions": {
                "value": "There are many similar works in recent years. What are the substantial differences or unique advantages of this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Reviewer_P2FR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782452430,
        "cdate": 1698782452430,
        "tmdate": 1700148401075,
        "mdate": 1700148401075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kFuEEeuDa2",
        "forum": "NnYaYVODyV",
        "replyto": "NnYaYVODyV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a perceptual group tokenizer, which just uses the grouping operations to extract visual features and perform self-supervised learning. The authors also explain the connection between the proposed perceptual group tokenizer and the self-attention. The experimental results show the performance is competitive with some state-of-the-art self-supervised methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The concept of perceptual group tokenizer is novel, and seems to enable the networks to have more good properties including interpretability and so on.\n\n2. Discussion between the perceptual group tokenizer and the self-attention is interesting, and can provide a new vision for vision transformer design."
            },
            "weaknesses": {
                "value": "1. The motivation is not clear. Since we have powerful vision transformers already, what is the advantages of the proposed perceptual group tokenizer. The authors claim that the perceptual group tokenizer have good properties such as adaptive computation without re-training and interpretability. However, I don't see the experimental results or the visualization that can provides proof of this claim.\n\n2. The performance is still concerned. Because we should also focus on the accuracy despite some good properties, the results shown in Table 1 demonstrate that the method cannot beat the baselines. Moreover, the state-of-the-art methods proposed in 2023 are not compared.\n\n3. The explanations to the grouping operation should give more details. Since grouping seems to be a explicit operation, the implicit operations for grouping such as MLP should show the correlation of the term \"grouping\"."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Reviewer_qQDA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699036947488,
        "cdate": 1699036947488,
        "tmdate": 1699635970240,
        "mdate": 1699635970240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JwgIMRkI9Z",
        "forum": "NnYaYVODyV",
        "replyto": "NnYaYVODyV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Perceptual Group Tokenizer (PGT), a novel vision model that relies on iterative grouping to extract visual features and learn representations in a self-supervised manner. PGT proposes to use grouping operations instead of self-attention layers in ViT, which yields a self-attention-free visual backbone. It demonstrates competitive performance on the ImageNet-1K benchmark. The model also shows properties like adaptive computation and high interpretability. The paper provides comprehensive analysis, ablation studies, and visualizations that underscore the model's capability and potential as a new paradigm in visual backbone architecture design."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I like the idea of grouping operations only, without self-attention. Using perceptual grouping is innovative and theoretically sound, offering a fresh perspective on representation learning and architecture design. \n2. The adaptability of computation in inference mode is also interesting. Without re-training, the model could inference with different number of group tokens. Table 1 also shows the accuracy will increase as there are more group tokens\n3. The visualization of the attention map is very interesting. It not only shows more iterations yield clearer grouping, but also shows different grouping heads kind of learning disjoint visual representations."
            },
            "weaknesses": {
                "value": "1. Only ViT-B level(70-80M parameter) model is reported. It would justify the effectiveness if the proposed architecture works when scaling the model size up. \n2. The computation cost, peak memory usage, and inference speed comparison with ViT-B are not reported. It would be informative for readers how fast the PGT is since PGT doesn't have memory/computation demanding self-attention operations."
            },
            "questions": {
                "value": "1. The model uses patch size 4x4, which means the visual backbone only downsamples the image by 4. Intuitively, this model should be good at dense prediction tasks that require high feature resolution, e.g. semantic segmentation and object detection. The authors reported the results of semantic segmentation on ADE20K in Section 4.4, but it only outperforms the ViT-B by a smaller margin. I understand that the segmentation architecture is different. So it would be interesting to compare ViT-B vs PGT-B with the same segmentation architecture (linear classification layer). \n2. Since PGT is self-attention-free, the computation cost is not quadratically increasing with the input resolution. But it is still reasonable to compare to ViT under the same resolution, for example, 16x16 patch size and 8x8 patch size. I am wondering whether authors have done this ablation. \n3. As mentioned in the weakness section, it would be interesting to have the computation cost of PGT. As Table 1 shows, as we increase the number of group token from 256 to 768, the linear probe accuracy increases from 79.3 to 79.7 How about less than 256 tokens and more than 768 tokens? It would be insightful to have a graph of number of tokens vs accuracy and inference speed. \n4. In Mask Autoencoder paper, researchers find that higher linear probing accuracy may be not necessarily stand for better representation. It would be also interesting to compare with MAE-ViT against PGT under the fine-tuning setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission438/Reviewer_8m3b"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission438/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699344688163,
        "cdate": 1699344688163,
        "tmdate": 1699635970183,
        "mdate": 1699635970183,
        "license": "CC BY 4.0",
        "version": 2
    }
]