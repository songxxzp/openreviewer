[
    {
        "id": "4YzQG1huOA",
        "forum": "6CetUU9FSt",
        "replyto": "6CetUU9FSt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
        ],
        "content": {
            "summary": {
                "value": "This paper studied different video encoders for imitation learning in modern video games. The motivation is that existing pre-trained models are usually trained on real-world images, while the impact of distributional shift on video-game images remains unknown. The paper conducted a systematic research that compared different pre-trained visual encoders and from-scratch trained visual encoders in three video games. The observations suggest that pre-trained self-supervised models are worth trying in video game agent development."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is brilliant. The paper is very easy to follow.\n- The study is systematic and leads to some interesting observations. The paper also gives insightful analysis for these observations, which may shed some light on the research of video-game agent development.\n- The motivation is clear, and the identified problem (video-game image distribution is different from pre-training distribution) is meaningful for the community."
            },
            "weaknesses": {
                "value": "- Although the paper offered many insights and potential analysis from the emprical observations, the paper lacks enough decisive conclusions. To be specific, I find that the following claims are not convincing:\n  - In the last sentence from section 5.1: \"while ViTs do not guarantee improvement over ResNets, they can provide significant improvement.\" This conclusion is drawn from the observation that ResNets are comparable with ViTs in Minecraft Dungeons, while are outperformed by ViTs for a large margin in Minecraft. However, the observation in Table 4 (the experiments in CS:GO) shows that ResNet outperforms ViT significantly. Therefore, it still remains unknown which of these two types of networks should be chosen as visual encoder.\n  - In section 5.3, \"This finding suggests that, if high-quality data is available for the specific task, it might be beneficial to consider training visual encoders end-to-end for BC agents, even in situations with less available data.\" As the finding shows the end-to-end encoders is comparable to pre-trained encoders, why do you say that end-to-end is beneficial? Moreover, as the pre-trained backbone is fixed during imitation learning in the paper, the trainable parameters for pre-trained settings are significantly less than end-to-end setting. I am curious if the performance will be better or worse if we don't fix the pre-trained visual encoders.\n  - In the last paragraph of section 5.5, it states that the pre-trained visual encoders fail to generalize when the input image size shifts. There are three related questions:\n    - The resize operation seems unreasonable. As the pre-trained encoder is fixed during BC training, the feature extracted from the image is fixed, which is distorted during resizing. What about padding the image to 280x280 and then resize it to 224x224?\n    - How about unfreezing the visual encoders during training? It may address the last point I raised as the visual encoder can adapt to new input size during fine-tuning.\n    - The conclusion is drawn from only one experiment, how about other cases that the pre-trained model fails when input image sizes are different?\n\n- The conclusions are drawn without controlling some critical variables. For example, the effect of network size is overlooked in the paper.\n- The experiments are constrained in a limited number of video game tasks. For example, there are thousands of tasks in Minecraft as shown by [1], and this paper only tested on the \"Treechop\" task. Also, the paper only studied the task-specific imitation learning, while large-scale pre-training adopted by VPT [2] or multi-task imitation learning [3] are not examined.\n- (minor) A key motivation of this paper is that, the images are often related to real-world scenes, which differs from video games. But there seems not enough suppotive evidence in the paper, and the readers usually don't know whether video game images are used during pre-training. Maybe a summary table that presents the pre-training sources of different models will be clear.\n\n \n\n[1] Minedojo: Building open-ended embodied agents with internet-scale knowledge. In NeurIPS, 2022.\n\n[2] Video pretraining (vpt): Learning to act by watching unlabeled online videos. In NeurIPS, 2022.\n\n[3] Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction. In CVPR 2022."
            },
            "questions": {
                "value": "- How do you select the hyper-parameters for different experiments? \n- What are the original image sizes for Minecraft Dungeons and Minecraft?\n- Why does the paper use FocalNet as classification supervised pre-trained encoders? ImageNet pre-trained models such as ViT / DeiT are also popular, which share the same architectures as in the other categories (language contrastive / self-supervised pre-trained) and thus are easy to be compared."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698405402719,
        "cdate": 1698405402719,
        "tmdate": 1700463429066,
        "mdate": 1700463429066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bm59eqDYKO",
        "forum": "6CetUU9FSt",
        "replyto": "6CetUU9FSt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_sq4s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_sq4s"
        ],
        "content": {
            "summary": {
                "value": "This work compares using pretrained image encoders with learned end-to-end encoders trained with behavioral cloning. They consider a few variants of both ResNets and ViT end-to-end encoders with and without image augmentation. They compare these to encoders from language contrastive pretraining (CLIP), self-supervised pretraining (DINOv2), supervised pretraining (FocalNet), and reconstruction based pretraining (VAE). They compare this methods in 3 modern video game settings: Minecraft Dungeons, Minecraft, and Counter-Strike GO.\n\nThey find that image augmentation improves performance for end-to-end BC encoders in some cases, but in other cases it is better to train end-to-end. They first compare which end-to-end encoder is best and find ViT\u2019s to be the most performant. They find that amongst the considered pretrained encoders, DINOv2 performed best. They further compare these methods in more data limited regimes; surprisingly, results are mixed even in the data-limited regime where one would expect pretrained encoders to shine."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors provide a valuable datapoint to the community for which existing pretrained encoders they may want to initialize their experiments from (seemingly DINO)."
            },
            "weaknesses": {
                "value": "Small scope and unsurprising results. This paper is more of a baselines paper comparing existing methods. For a baselines paper, I would expect far more extensive experiments across domains and methods.\n\nThe domains considered here, while they are \u201cmodern video games\u201d, are quite limited. E.g. for Minecraft they only consider the treechop task, which is the most basic thing one can do in Minecraft"
            },
            "questions": {
                "value": "How do these methods compare in more domains? I would also expect experiments in simpler domains like e.g. atari, coinrun, maybe robotics environments.\n\nHow do the methods considered here compare to other common methods?\n\n- auxiliary objectives for representation learning are quite common in reinforcement learning. Given this paper is studying the efficacy of different image encoders, it would seem natural to me to also include auxiliary self-supervised objectives into the end-to-end experiments\n- the authors hold pretrained encoders fixed. It seems natural to also comp\n\nHow well tuned were the experiments for different encoders? It seems hyperparameters were held fixed across all architectures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555320721,
        "cdate": 1698555320721,
        "tmdate": 1699636518357,
        "mdate": 1699636518357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5pXSB6IukR",
        "forum": "6CetUU9FSt",
        "replyto": "6CetUU9FSt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an important problem: whether a pre-trained vision encoder can boost the performance of sequential decision-making models. The authors comprehensively study four primary encoder categories: self-supervised trained, supervised trained, contrastive-learning trained, and reconstruction trained, and draw several interesting conclusions. This will be meaningful for choosing backbones to design policy models in complicated environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-written and easy to follow. \n* This paper studies an important problem: the difference of vision encoders in building policy models for decision-making. \n* The selected environments are three modern video games, which are popular and challenging. To some degree, I believe the conclusions drawn from these environments can be generalized to real-world scenarios."
            },
            "weaknesses": {
                "value": "* **Missing some details.** It is not clear what kinds of image augmentation tricks are used. Why the image augmentation method is specific to the game? Why a pre-trained model (DINOv2) is better than the others? It lacks deep discussions.\n\n* **Provides rollout videos for better understanding.** Rollout videos are very helpful for readers to understand the challenges of the environments and the effectiveness of the model. It is strongly recommended to include some videos in the supplementary materials. \n\n* **Insufficient evaluation tasks in the Minecraft domain.** In Minecraft, the \"Treechop\" task is the most basic and simple task. Although it is an important benchmark, however, conducting experiments solely on this task is not enough. It is better to include 2-3 extensive tasks, such as \"Hunt animals\", \"Craft crafting_tables\", and \"Mine ores\", to enhance the soundness. \n\n* **Concerns about the training data distribution of baselines.** \n\n* **Missing some baselines and references.** [1] proposed an important foundation model for decision-making in Minecraft, which was trained on large-scale YouTube gameplays with behavior cloning. It yields a good vision encoder that is specified in the Minecraft domain. Although it is cited in the paper, it does not participate in the comparison. I suggest the authors to compare VPT in the experiment. [2] is a large-scale pre-trained segmentation model, which has demonstrated strong cross-domain recognition capability. It should be included as a baseline. [3, 4, 5] are also imitation learning methods in the Minecraft domain, which are strongly related to this topic. I suggest the author reference these works and have necessary discussions. \n\n[1] \"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\", https://arxiv.org/abs/2206.11795\n\n[2] \"Segment anything\", https://arxiv.org/abs/2304.02643\n\n[3] \"STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\", https://arxiv.org/abs/2310.08235\n\n[4] \"Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction\", https://arxiv.org/abs/2301.10034\n\n[5] \"GROOT: Learning to Follow Instructions by Watching Gameplay Videos\", https://arxiv.org/abs/2310.08235"
            },
            "questions": {
                "value": "My questions are listed in the weakness part. \n\nI will consider improving the rating if the author adequately addresses my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644865248,
        "cdate": 1698644865248,
        "tmdate": 1700460737153,
        "mdate": 1700460737153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CETfjSpWdX",
        "forum": "6CetUU9FSt",
        "replyto": "6CetUU9FSt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the challenge of training agents in modern video games, going beyond simpler games like those on Atari. The central research question is: How can images be encoded for data-efficient imitation learning in modern video games? To address this, the authors compare both end-to-end trained visual encoders and pre-trained visual encoders across three modern video games: Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive.\n\nthe paper's main contributions can be summarized as follows:\n\n1. **Addressing a Gap**: The paper tackles the challenge of training agents in modern video games, which has traditionally been resource-intensive and costly.\n2. **Leveraging Large Vision Models**: It explores the potential of using publicly available large vision models to reduce costs and resource requirements, a pertinent issue given the current trend in machine learning towards larger models.\n3. **Comparative Study**: A systematic study is conducted to compare the performance of publicly available visual encoders with traditional, task-specific, end-to-end training approaches in the context of imitation learning.\n4. **Focus on Modern Games**: The study specifically targets modern video games, including Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive, reflecting a move beyond simpler, classic game environments.\n5. **Human-like Gameplay**: The authors emphasize training agents to play games in a human-like manner, using behavior cloning and offline training with human gameplay data, which is a step towards creating AI that can interact in complex environments in a natural way."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors have selected a diverse set of modern video games, including Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive, for their experimental studies. This choice reflects a significant step forward from the commonly used Atari games in previous research, providing a more realistic and challenging benchmark for evaluating imitation learning techniques.\n2. The paper introduces an innovative approach to imitation learning by leveraging publicly available large vision models. This strategy not only addresses the resource-intensive nature of training agents in modern video games but also democratizes access to high-quality training for smaller research groups or institutions.\n3. The writing is clear, concise, and well-structured."
            },
            "weaknesses": {
                "value": "1. It seems that the task is very simple, such as chopping trees in Minecraft, which is a fairly straightforward task. The author's conclusion is that there is no significant difference between various visual encoders and input image resolutions. However, due to the simplicity of the task, this conclusion is unreliable. Evaluating models like CLIP and DINO on such simple tasks does not effectively demonstrate the differences between modern vision transformers and CNNs. I strongly recommend that the author choose more challenging tasks, such as `MineRLObtainDiamondShovel-v0` or `MineRLBasaltBuildVillageHouse-v0` etc.\n2. In time-series decision-making tasks, the memory of historical states is crucial for making decisions. For example, VPT uses a transformer to record a history state of 128 frames, while the paper only utilizes LSTM to capture a limited number of frames. This can have negative implications for completing long-horizon tasks using behavior cloning.\n3. Recently, the popular technique of Segment Anything has achieved better results in various visual tasks. The author can further compare this model to explore its potential.\n4. In the conclusions shown in Table2, the best model of tree pruning only has a success rate of 32%, much lower than VPT's nearly 100%. Does this imply that the visual encoder is not actually the most important module in game playing?\n\nIn conclusion, although the author has compared a considerable number of vision encoders in the game, the reliability of the results is compromised due to their choices in task setting and temporal transformer.\n\nSome relevant work has not been cited:\n1. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction\n2. A generalist agent\n3. GROOT: Learning to Follow Instructions by Watching Gameplay Videos\n4. Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining"
            },
            "questions": {
                "value": "See in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Null"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743290645,
        "cdate": 1698743290645,
        "tmdate": 1699636518161,
        "mdate": 1699636518161,
        "license": "CC BY 4.0",
        "version": 2
    }
]