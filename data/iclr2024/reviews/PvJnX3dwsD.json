[
    {
        "id": "2rm13ZSK4P",
        "forum": "PvJnX3dwsD",
        "replyto": "PvJnX3dwsD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_5b1y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_5b1y"
        ],
        "content": {
            "summary": {
                "value": "This work shows that quadratic models exhibit the catapult phase of neural networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The strength of the work lies in the soundness of the theory and that the topics covered by the paper are all pertinent problems to the contemporary deep learning theory."
            },
            "weaknesses": {
                "value": "There are quite a few fatal weaknesses in my opinion.\n\n1. The paper covered too many topics that it feels that it does not achieve any point satisfactorily. For example, the paper feels mistitled. The main focus of the paper is the catapult mechanism -- which does appear in deep learning but cannot represent all types of \"neural network dynamics.\" In my opinion, the catapult mechanism is a rather special / specific type of dynamics and the title is an overclaim. If the authors change the title to a more proper one, the paper could be much easier to evaluate.\n\n2. Lack of discussion of a highly relevant problem. Essentially, within the framework of quadratic models, it appears to me that the catapult mechanism is nothing but what the academia traditionally calls \"chaos.\" For example, consider a width-1 quadratic model, and compare the dynamics of GD to that of a logistic map -- the dynamics is essentially identitical -- the loss of local stability leads to chaos in the logistic map, and the same here in the quadratic model. The authors need to discuss this point in my opinion."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698037858487,
        "cdate": 1698037858487,
        "tmdate": 1699636665196,
        "mdate": 1699636665196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FEM3AOCjJx",
        "forum": "PvJnX3dwsD",
        "replyto": "PvJnX3dwsD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_JHXp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_JHXp"
        ],
        "content": {
            "summary": {
                "value": "I was asked to review this paper at the last minute, so although I did not get the chance to work carefully through the proofs, I can comment on the broad content and contributions\n\nThe paper analyzes optimization and generalization properties of neural networks using quadratic models. It shows theoretically and empirically that quadratic models exhibit the \"catapult phase\" with large learning rates, explaining a property of neural networks not captured by linear models. The quadratic model and its (changing) tangent kernel is studied analytically for the case of a single training example and multiple uni-dimensional training samples. Three regimes are identified. When catapault effects occur, better generalization is observed in quadratic models. Experiments demonstrate quadratic models parallel neural networks better than linear models in generalization with large learning rates."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper rigorously analyzes the catapult phase in NQMs theoretically. Prior works have attempted similar analyses, but the presentation here is particularly readable. The experiments validating theory and demonstrating applicability are thorough. They successfully demonstrate that NQMs better capture neural network behavior than linear models. The experiments in the appendix in particular provides good additional support. The mathematical analysis appears solid."
            },
            "weaknesses": {
                "value": "The architectures and datasets used are still relatively limited. It would also be good to highlight what the incremental value of this paper is over prior work on quadratic models. A CNN and transformer experiment would be particularly welcome. \n\nFrom a cursory literature review, I have found this paper which has relatively substantial overlap in topic, and I believe should at least be cited (Meltzer and Liu https://arxiv.org/abs/2301.07737). A more thorough set of references on recent work around the catapult effect and edge of stability would also benefit the paper. If these changes are incorporated and a more thorough related works section is added, I will likely raise my score.\n\nFrom empirical work, I've also seen that quadratic model's validation curve only tracks the NN's curve in the early stages of training, but then diverges from it. It would be nice to give an example of the limitations of the quadratic model for understanding generalization. Being clear about potential limitations of quadratic models to fully model neural networks would be welcome."
            },
            "questions": {
                "value": "Have you analyzed how the Hessian evolves during catapult phase? This could shed some light onto the phase transition.\n\nIt may be interesting to consider explicitly varying the feature learning parameter (e.g. $\\alpha$ in Chizat et al) and see how the quadratic model differs from the linear one. This would be related to going into $\\mu$ parameterization as in Yang. \n\nCan you relate observed properties of trained NQMs to improvements in generalization, perhaps through the lens of something like kernel-target alignment for the after-kernel of the quadratic model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6138/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6138/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6138/Reviewer_JHXp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719757377,
        "cdate": 1698719757377,
        "tmdate": 1699636665084,
        "mdate": 1699636665084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LWP3q1SbQE",
        "forum": "PvJnX3dwsD",
        "replyto": "PvJnX3dwsD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_F9oB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_F9oB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes using neural quadratic models -- second order taylor expansion of any function f around initial parameters $w_0$ -- as a way to study neural network dynamics. The authors start by explaining that linear dynamics perspective of neural networks falls short in explaining some of the behaviors in neural network dynamics; specifically, catapult phase of learning rate. They approximate a two layer neural network using its second order expansion. They first show that for a single training example, where tangent kernel reduces to a scalar, monotonic convergence, catapult phase, and divergence are separated based on learning rate and inverse of kernel value at random initialization. They further extend their analysis to uni-dimensional multiple example setting where the analysis is driven by the eigenvectors of the kernel matrix. Finally, the authors empirically show that for wide neural networks, catapults happen in the top eigenspace of the kernel -- similar to the multiple example setting. Experimental results suggest that catapult phase results in lower test error compared to error with sub-critical learning rate for the quadratic model, mimicing the dynamics of neural networks more closely."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is written well and in general easy to follow. Empirical results on top eigenspace of tangent kernel for analyzing general wide neural networks is convincing."
            },
            "weaknesses": {
                "value": "Several assumptions about underlying the theory are not clear and different from the practice.\n\n1. The neural network in Eq (9) is initialized as $u_i \\sim \\mathcal{N}(0, I_d)$ and $v_i \\sim Unif(-1, 1)$. Can you explain why $v_i$ is initialized different from $u_i$, also different from the typical inverse fanin/fanout initialization in practice?\n\n2. The assumption that width (m) is larger than data size (n) which is assumed to be a small constant is unrealistic. While in the limit where width goes to $\\inf$, this would be the case but for any finite width network, this does not hold in practice. Is this assumption crucial, can you still assume that $n/m$ does not necessarily go to zero?\n\n3. $p_1(t)$ is the top eigenvector of $K(t)$. Given that $p_1(t)$ is not necessarily equal to $p_1(t+1)$, it is not clear how you derived $\\lambda_1(t+1)=\\lambda_1(t)-p_1(t)^TR_K(t)p_1(t)$. Could you please explain in more detail?\n\n4. In Eq(12), $R_\\lambda(t)$ is defined without the minus sign. In the following paragraph, you mention that \"$R_\\lambda(t)$ stays positive and results in monotonic decrease of kernel\" which makes sense since $\\lambda(t)=\\lambda(t-1)-R_\\lambda(t)$. But in the next paragraph, you write down $\\lambda(t)=\\lambda(0)+\\sum_{\\tau=0}^{t} R_\\lambda(\\tau)$ which suggests that $R_\\lambda(t)$ should include the minus sign. Please clarify.\n\n5. Related to above, I think it should be $\\lambda(t+1)=\\lambda(0)+\\sum_{\\tau=0}^{t} R_\\lambda(\\tau)$ or $\\lambda(t)=\\lambda(0)+\\sum_{\\tau=0}^{t-1} R_\\lambda(\\tau)$\n\n6.  You mention in the decreasing phase section in page 6 that decrease in $v(t)$ would cause a decrease in $\\kappa(t)$. But in Eq (13), reducing $v(t)$ would increase inside of square which should lead to an increase in $\\kappa(t)$; unless, $u(t)+w(t)<0$ which is not clear if it holds. Please clarify.\n\n7. In Eq (10), $1/\\sqrt(d)$ is missing from $\\sigma(u_{0,i}^Tx)$. It is present in Appendix A.\n\n8. Page 27 in the Appendix, it should be $\\Pi_1 \\mathcal{L}(t)=K_1(t)\\Pi_1 \\mathcal{L}(t-1)$"
            },
            "questions": {
                "value": "Please see above for related questions as they are more meaningful within their respective contexts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797629545,
        "cdate": 1698797629545,
        "tmdate": 1699636664940,
        "mdate": 1699636664940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5LDCetm5Rb",
        "forum": "PvJnX3dwsD",
        "replyto": "PvJnX3dwsD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_e4J5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_e4J5"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the \"catapult phase\" during training models with large learning rates.\nImportantly, it proposes Neural Quadratic Models (NQMs) as a tool to study it and proves that when large learning rates are used they exhibit a similar catapult phase as modern neural networks, which is not the case of other tools such as linear models, a popular theoretical tool to analysis the learning of neural networks.\nThe paper presents these findings with proofs and suggests that NQMs can be useful tools to analyze neural networks in the future."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**originality** The finding of the paper seems to be novel and the proposal of using NQMs can be new as well.\n\n**quality** The finding and theory from the paper seems to be sound.\n\n**clarity** The paper is overall well-written but can be hard to follow for people who are not very familiar with the field.\n\n**significance** The contribution of the paper seems to be significant and can enable many future work on analysis with NQMs, which could lead to more useful findings than the catapult phase."
            },
            "weaknesses": {
                "value": "The proposed term Neural Quadratic Models seems to be unnecessary as for linear models we don't call them Neural Linear Models."
            },
            "questions": {
                "value": "Some recent findings such as the deep double descent are high-dependent on the size of the neural networks and other hyper-parameters.\nIs the \"catapult phase\" here sensitive to other hyper-parameters apart from learning rates?\nI don't see many conditions for the theorem presented in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811435234,
        "cdate": 1698811435234,
        "tmdate": 1699636664818,
        "mdate": 1699636664818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "96Jf9BNAIo",
        "forum": "PvJnX3dwsD",
        "replyto": "PvJnX3dwsD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_Unww"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6138/Reviewer_Unww"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a quadratic approximation of two-layer neural networks to understand optimization behaviours that cannot be captured by linear models. More precisely they theoretically evidence the so-called catapult dynamics for respectively one and multiple training points, ie they show the existence of two critical values for the learning rate which delimits respectively exponential convergence to the minimum, catapult dynamics (ie first increase of the loss then convergence to low loss) and finally divergence. Finally they provides experiments evidencing catapult dynamics as well as the fact that the quadratic approximation of a neural networks shows similar behaviours that the neural network above the critical learning rate."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written with clear figures and a good explanation of the setting and the results. This paper studies a very interesting phenomenon about the optimization of neural networks and has to deal with non-linear phenomena which are usually not well understood. They are able to evidence theoretically the catapult dynamics and the existence of two critical learning rates when dealing with neural quadratic models.\n\nEmpirical results shed light on the similarity of behaviours in terms of generalization between the NQM and the neural network function, evidencing the coherence of the quadratic model."
            },
            "weaknesses": {
                "value": "I find the proofs a bit hard to understand because of the use in the proofs of O,o, $\\Omega$ notations. Hiding constants behind such notations makes the proofs a bit cloudy to me. \n\n1)Especially I have some concerns about the proof of lemma 1 in appendix D: you use a proof by induction, and still use O,o notations. But summing $O$ terms remains $O$ only when the number of iterations is controlled. To still have $O$ in the end of the summation with respect to m, it should be checked that the summation index T is itself $O(1)$. However I am not sure such a result is proved (correct me if I'm wrong).\nEspecially it seems to me that it might not be the case by considering the fact that T must satisfy a relation of the form $(1+\\delta)^T\\sim \\log(m)$ with $\\delta \\sim \\frac{\\log(m)}{\\sqrt{m}}$. Indeed $u(t)$ goes from $O(1/m)$ to $O(\\log(m)/m)$. In that case, $T\\sim \\frac{\\log(\\log(m))\\sqrt{m}}{\\log(m)}$ which is not $O(1)$.\n\n2) Another ambiguity about the $O$ notation is for example when it is stated: $\\kappa(0)>(1+\\delta-O(1/m))^2>(1+\\delta-O(\\delta^2/\\log(m)))^2$ in appendix D. In full generality it seems wrong for general sequences as it depends on the constants in the $O$ itself and their sign. For example $(1+\\delta))^2<(1+\\delta-(-3\\delta^2/\\log(m)))^2$. I think this statement in its current form would perhaps need an additional study of the constants, their sign or if they are zero.\n\n3) The non-linearity that is used in this paper is a ReLU: it allows to compute easily the second derivative of the neural network function (it deletes the diagonal terms of the hessian of the neural network function). My only concern is regarding the generalization to non-linearities that are not piecewise linear and hence which make another term appear in the Hessian of the neural network function, which corresponds to the second derivative of the non-linearity (correct me if I'm wrong). Could the author comment about how to handle this and if they expect the analysis to be the same and the results to still hold?"
            },
            "questions": {
                "value": "1) I am curious if you could clarify the links, if they exist, between catapult dynamics and recent works on edge of stability analysis of neural networks (cf the paper \"Second-order regression models exhibit progressive sharpening to the edge of stability\" that is not cited but seems to me very related to your setting). It seems interesting because both studies explore the influence of a non-linear quadratic term on the coupled dynamics of the sharpness and the learning rate.\n\n2) It is written in the text that the results hold with high probability over initialization but I'm not sure if this is written in the statement of the theorems. Perhaps it could be added in the theorems themselves.\n\n3) I find the results interesting and they provide good contribution. I would increase my score if the authors address my concerns about the clarity of $O,o,\\Omega$ notations in the proof, especially the proof by induction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper is mostly theoretical and does not present ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6138/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6138/Reviewer_Unww",
                    "ICLR.cc/2024/Conference/Submission6138/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699565558735,
        "cdate": 1699565558735,
        "tmdate": 1700683996020,
        "mdate": 1700683996020,
        "license": "CC BY 4.0",
        "version": 2
    }
]