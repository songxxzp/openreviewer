[
    {
        "id": "30gRIL4LGn",
        "forum": "9Ebi1euQZQ",
        "replyto": "9Ebi1euQZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_PjV9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_PjV9"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes object hallucination (generating non-existent objects) in detailed image captioning by large vision-language models (LVLMs). It introduces a new evaluation method called CCEval to specifically assess object existence hallucination in detailed captions. Experiments reveal that even LVLMs with minimal hallucination on VQA-based benchmarks show substantial hallucination when evaluated on CCEval.\n\nThe paper conducts an analysis attributing hallucination to factors like language decoder size, training data amount/quality, and input image resolution to the vision encoder. The core issue is misalignment between objects mentioned in the caption versus those grounded by the vision encoder. Objects not grounded form incorrect word associations leading to hallucination.\n\nTo control hallucination, the paper presents HallE-Switch - an LVLM that can adjust the extent of hallucination via a control parameter. It is trained on datasets with only grounded objects versus with hallucinated objects marked. At inference, the parameter shifts the model between using solely grounded objects (-1) versus blending in hallucinated ones (+1). This achieves 44% hallucination reduction without impacting object coverage or sentence length."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear. I like the flow of this paper where analysis of OH is conducted before providing any solutions.\n2. The paper has thorough analysis of factors influencing object hallucination using the new evaluation methods.\n2. It is novel to control hallucination levels in LVLMs via contextual/parametric knowledge.\n3. The proposed solution maintains object coverage and sentence length while reducing hallucination."
            },
            "weaknesses": {
                "value": "1. Although it is interesting to argue that not all hallucination is bad, I don't think the authors successfully supported the argument with examples showing when hallucination is beneficial. With that said, more visualizations like example captions may help better explain the hallucination behavior.\n2. There could be more specific illustrations on how the training data was generated using GPT-4.\n3. Related work section doesn't really provide any useful information connecting existing work and the proposed work. For example, some references are missing such as https://arxiv.org/pdf/2110.01705.pdf."
            },
            "questions": {
                "value": "I don't have questions in addition to the points mentioned in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698472471122,
        "cdate": 1698472471122,
        "tmdate": 1699636271099,
        "mdate": 1699636271099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fZioix9nsw",
        "forum": "9Ebi1euQZQ",
        "replyto": "9Ebi1euQZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_DU9c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_DU9c"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzed the cause of hallucination in large vision-language models through the direction of the sizes of large language model, data volume, and input image resolution. The paper further proposes a way to control the generation of VLM, by learning a matrix for mode switching."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a new benchmark for evaluating hallucination in large vision-language models and analyzes that. Some findings in this paper are interesting and might provide insights for future research.\n2. The paper proposes a way to control the hallucination in large vision-language model and obtains improvements on the proposed benchmark."
            },
            "weaknesses": {
                "value": "1. The overall story is not very coherent. First, the details of CCEval are not very clearly described. Then, analysis is conducted on two or three methods with some conclusions drawn. However, the observation mentioned in the paper seems not to have a specific relation with the proposed Hallu-Switch method. The technique is also only evaluated on CCEval, but previous benchmarks are discussed and used in this paper. The reviewer would expect more insights or explanations about why Hallu-Switch works.\n2. The study mainly focuses on LLaVA and InstructBLIP and draws some conclusions for large vision-language models. It might be better to study more models to verify the findings.\n3. There are many typos in sentences that hinders the reading and understanding. The paper needs careful revision to fix these issues.\n    1. 'We additionally record **and and** balance the average number of objects and the average length of captions across all cases' in the last third paragraph of page 4\n    2. ' We find **infeasible** to comparing object hallucinations is impractical when there is a significant disparity in average sentence length and the number of objects.' in the last fourth paragraph of page 4\n    3. Table 4, the second column for CCEval should be 'finetuning data' rather than 'model'\n    4. 'The learned M can be regarded as the transformation from a generic word space to the object sensitive word space' in the first paragraph of Sec. 3.2. It seems this describes $W$ rather than $M$\n4. Small issue that does not affect the rating. Some LVLMs can also be discussed:\n    1. Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning\n    2. GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\n    3. MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"
            },
            "questions": {
                "value": "1. The paper mainly discusses LLaVA and InstructBLIP; what if more models are analyzed? Do these findings still holds somehow?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582986758,
        "cdate": 1698582986758,
        "tmdate": 1699636271017,
        "mdate": 1699636271017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4NLKdhk9DR",
        "forum": "9Ebi1euQZQ",
        "replyto": "9Ebi1euQZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_ZWbF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_ZWbF"
        ],
        "content": {
            "summary": {
                "value": "To tackle the hallucination, the authors introduce CCEval, a novel evaluation method assisted by GPT-4, specifically designed for assessing detailed captioning. Surprisingly, the study reveals that LVLMs exhibit minimal object existence hallucinations in existing Visual Question Answering (VQA) benchmarks. However, the proposed evaluation method exposes continued susceptibility to such hallucinations.\n\nThe paper delves into the investigation of these hallucinations and attributes them to various factors, including image resolution, the size of the language decoder, and the quantity, quality, and granularity of instruction data. One of the key findings highlights that hallucinations often occur when the language description includes finer object granularity than what the vision module can ground or verify, leading to unwarranted inferences.\n\nTo mitigate these hallucinations, the authors introduce HallE-Switch, a controllable LVLM that addresses object existence hallucinations. This novel approach allows captioning to shift between two modes: (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending contextual knowledge with parametric knowledge to imagine inferred objects. HallE-Switch significantly reduces hallucinations, with a 44% reduction compared to the previous model LLaVA7B, while maintaining the same level of object coverage.\n\nIn summary, the paper introduces a new evaluation method, identifies factors contributing to object existence hallucinations in LVLMs, and presents HallE-Switch, a solution that effectively reduces hallucinations in detailed captioning without compromising object coverage. This research contributes to improving the reliability and accuracy of large vision-language models in fine-grained visual description tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper is well-motivated and well designed\n2. The proposed method is easy to follow"
            },
            "weaknesses": {
                "value": "1. Some related methods have not been reviewed, such as ``Evaluation and Analysis of Hallucination in Large Vision-Language Models''"
            },
            "questions": {
                "value": "na"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699002090308,
        "cdate": 1699002090308,
        "tmdate": 1699636270920,
        "mdate": 1699636270920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ToP67swnRy",
        "forum": "9Ebi1euQZQ",
        "replyto": "9Ebi1euQZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_6XKb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3227/Reviewer_6XKb"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of object hallucination in large vision-language models detailed captioning. First, the authors quantify the degree of object hallucination by varying model size, fine-tuning data size, image resolution, etc. Second, they proposed two methods, namely (1) modifying the caption training data to distinguish contextual object vs parametric object; (2) adding a layer that acts as a switch between contextual knowledge and parametric knowledge. The proposed methods outperforms baseline on CCEval benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Addressed a very timely topic of object hallucination in large vision-language models.\n- Interpreting the objects using contextual/parametric knowledge framework seems novel.\n- Showed results on multiple architectures, model scales, amount of fine-tune data, which are valuable findings to the community's future research."
            },
            "weaknesses": {
                "value": "- Presentation needs polishing. The paper is very dense in text and requires some effort for reading.  Also, please address the points in \"Questions\" section.\n- The first part and second part of the paper looks more like two separate and condensed papers. Due to this problem, especially the first part fails to deepen our insight about the root cause of the problem. I would expect a deeper treatment on each part.\n- Results are mostly quantitative. It would be better to show more qualitative examples of hallucination."
            },
            "questions": {
                "value": "I'm generally happy with this submission and think it is above the acceptance bar. Readers could appreciate this work better if the presentation is improved. Please answer the following minor points.\n\n1. In page 4, how are \"consistent constraints\" enforced? Please explain in detail.\n2. Section 3.2 is not very clear to me. Does W correspond to \"Projector\" in Fig 2? According to Fig 2, W comes after LLM. However, equation   says the opposite. Is the epsilon parameter applied on word-by-word basis or sentence-by-sentence basis? \nI may have misunderstood something because I'm not familiar with the LM-Switch work. Regardless, I believe a good paper should be self-contained and can be readable to general audience in the community.\n3. In page 2, object relationship hallucination is mentioned but this concept does not seem to appear again later pages, in metrics or methods presented in the paper. Did I misunderstood?\n4. Do you observe any downsides or limitations of using this method that were not expressed in the result Table?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699061207844,
        "cdate": 1699061207844,
        "tmdate": 1699636270823,
        "mdate": 1699636270823,
        "license": "CC BY 4.0",
        "version": 2
    }
]