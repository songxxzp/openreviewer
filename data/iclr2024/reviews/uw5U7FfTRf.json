[
    {
        "id": "HqFhkAeqd3",
        "forum": "uw5U7FfTRf",
        "replyto": "uw5U7FfTRf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_QKR4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_QKR4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a backdoor data detection method called BaDLoss, which is inspired by existing works on anti-backdoor learning (ABL) and Spectre. Instead of detecting backdoor data using lower per-example loss, BaDLoss treat the training trajectories over each epoch as a vector. BaDLoss select a subset of samples to inject a selected backdoor pattern as a reference. After training, it uses k-NN with Euclidean distance to filter out backdoored data. The proposed method demonstrated its effectiveness on existing attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Using loss trajectories as a vector is new and interesting in this field. The motivation for using such an approach is well explained. It is technically sound for the proposed method."
            },
            "weaknesses": {
                "value": "My main concern on the weakness is the evaluations and practicality of the proposed method. \n- The proposed method relies on predefined reference backdoor samples. This could limit its practicality. \n- For one-attack evaluations, on page 4, section 3.3, reference examples are set to be similar to the chosen attack. This is impossible in a real-world scenario. The defender should not have any prior knowledge regarding backdoor attacks. \n- It has been observed in existing works such as SPECTRE (Hayase et al., 2021) and [3] that the detection method is sensitive towards the poisoning rate. It would be more comprehensive to include experiments with lower poisoning rates. Some additional results to provide evidence for the discussion in section 6.1 would be great.\n- It is unclear which model architecture is used in the evaluations and if the proposed method works on other models. \n- Lack of comparison with more recent detection methods [1,2,3].\n- In the introduction, page 2, below Figure 1. \"This is because backdoor attacks generally cause the model to attend to a single feature for classification unlike natural images, which generally induces anomalous loss trajectories for those backdoor examples.\" This is an overclaim; there is no evidence to support this statement. \n- The experiments were only conducted on the small-scale dataset, lacking evaluations on larger datasets and more recent attack ISSBA [4].\n- Lack of evaluations against adaptive attacks. Given that the adversary knows that the defender will use BaDLoss, to what extent could the adversary evade detection? For example, if an adversary could have access to the entire training dataset and select several data points that have various loss curves (before adding backdoor triggers), would this evade detection? \n\n\n[1] Chen, Weixin, Baoyuan Wu, and Haoqian Wang. \"Effective backdoor defense by exploiting sensitivity of poisoned samples.\" Advances in Neural Information Processing Systems (2022).\\\n[2] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" USENIX Security Symposium (2023).\\\n[3] Huang, Hanxun, et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" The Eleventh International Conference on Learning Representations (2023).\\\n[4] Li, Yuezun, et al. \"Invisible backdoor attack with sample-specific triggers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021."
            },
            "questions": {
                "value": "The results for ABL in Table 1 seem much lower than the results reported in their original paper, as well as in reproduced results in [3]. Is there any reason for this discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697930675305,
        "cdate": 1697930675305,
        "tmdate": 1699636926856,
        "mdate": 1699636926856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CnmYMmVVF2",
        "forum": "uw5U7FfTRf",
        "replyto": "uw5U7FfTRf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_SuvN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_SuvN"
        ],
        "content": {
            "summary": {
                "value": "This paper presents BaDLoss, a new backdoor detection method that exploits the difference training dynamics between clean and backdoor samples by injecting specially chosen probes into the training data. These probes model anomalous training dynamics, and BaDLoss tracks the loss trajectory for each example in the dataset to identify unknown backdoors. By removing identified backdoor samples and retraining, BaDLoss can mitigate the backdoor attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method works based on observing the significantly vary training dynamics between clean and backdoor samples, which is quite novel and interesting.\n2. Overall, the method's performance seems to outperform other baselines."
            },
            "weaknesses": {
                "value": "1. [method's presentation, major] I personally find the presentation of Section 3 quite hard to follow since there are no algorithm or figure to describe the method, or even a formulation.\n2. [lack of experiments, major] There are only 3 types of backdoor attack (patch-based, blending-based, warping-based) that are considered in the experiments, so I am not sure if the defense is effective with all attacks. I think there should be more backdoor attack approaches included in the experiments as well as related works discussion, such as sample-specific ([1]), optimized trigger ([2]), or frequency domain attack ([3]). Moreover, there is no abaltion study/discussion about different choices for the hyperparameters used in the paper (detection threshold, k for the kNN classifier). \n3. [underwhelming experimental results, major] The clean acc. of BaDLoss is significantly degraded in the cases of SIG, WaNet, and multi-attack on CIFAR10. With those underwhelming clean acc. (~60%), I doubt that the model can be considered functional, especially on such \"easy\" dataset like CIFAR10.\n4. [results' presentation, minor] There are many numbers in Table 2 and Table 3 but the best results are not highlighted. The authors should highlight the best results, or maybe report the average clean acc. and ASR drops of each defense method.\n\n[1] Li, Yuezun, et al. \"Invisible backdoor attack with sample-specific triggers.\" (ICCV 2021)  \n[2] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\" (ACM CCS 2023)  \n[3] Wang, Tong, et al. \"An invisible black-box backdoor attack through frequency domain.\" (ECCV 2022)"
            },
            "questions": {
                "value": "1. Please refer to the weaknesses above.\n 2. Some questions regarding experimental details: \n- Why are different poisoning rates used for different attacks/datasets? I am not sure the comparison is fair given the varying settings.\n- Why are the warping field parameters of WaNet strengthened? \n- What are the backdoor features added to the backdoor probe set? Are they all the triggers evaluated in the experiments? If so, could the method really work with unseen triggers? (I might be confused here, because the method is claimed to can \"zero-shot transfer to previously unseen backdoor attacks\", but the paper does not really explicitly mention which backdoor features are used to record the loss trajectories and which are unseen ones.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641806188,
        "cdate": 1698641806188,
        "tmdate": 1699636926751,
        "mdate": 1699636926751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zg1oM1A4ha",
        "forum": "uw5U7FfTRf",
        "replyto": "uw5U7FfTRf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_4YdZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_4YdZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use the loss dynamics of a training sample to detect whether it is a backdoored sample. The idea is to construct two sets of contrast samples including clean samples and randomized-label samples. A kNN classifier was then trained on different types of training loss trajectories as the detector model. Under one-attack and multiple-attack evaluations, the proposed method shows promising results compared to existing defenses like NC, AC, SS, ABL et al."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The use of loss trajectory to detect backdoor samples is an interesting direction;\n\n2. The proposed detector seems quite easy to train and works reasonably well against different types of attacks;\n\n3. The proposed method is compared with a set of existing defenses NC, AC, SS, FA, ABL, et al."
            },
            "weaknesses": {
                "value": "1. In section 3.2, it is not clear how the loss trajectories were collected and how the detector was trained. The authors mentioned 500 bonafide clean examples, and then another 250 randomized-labeled samples, so how many samples were needed to extract the trajectories and train the detector? Also, it is not clear how the 250 backdoored probes were crafted, i.e., using what backdoor features? It has been shown that a stronger backdoor trigger can overwrite a relatively weaker backdoor trigger, so here the choice of the backdoor feature will become vital. \n\n2. It is not clear how to tune the threshold to reject a training sample, as the poisoning rate should not be known to the defender in advance. This potentially makes the proposed defense fail either low poisoning rates or high poisoning rates. I.e., if the poisoning rate is 40%, how it is possible to remove all the poisoned samples by determining the threshold?\n\n3. It is not clear how the loss trajectory is defined and how the proposed method can be adaptive to different types of attacks. The authors argued that backdoored samples can have either slow or fast training speed, yet it is not clear how the proposed detector can identify both or even more subtle cases.\n\n4. The restus of existing defenses in tables 1 and 1 are stranger, where it shows ABL and other defenses fail the most case, which I believe it is not the case in their original papers.\n\n5. The proposed method failed the Sinusoid attack in Table 2, which was not sufficiently explained. \n\n6. The considered backdoor attacks was far less than in recent works [2,3].\n\n7. The proposed method was not conompared with the SOTA backdoor sample detection method Cognitive Distillation [3], which can be applied to detect both training and test samples, yet the proposed method can only detect training samples.\n\n\n[1] Wu, Dongxian, and Yisen Wang. \"Adversarial neuron pruning purifies backdoored deep models.\"  NeurIPS, 2021.\n\n[2] Li, Yige, et al. \"Reconstructive Neuron Pruning for Backdoor Defense.\" ICML, 2023.\n\n[3] Huang, Hanxun, et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" ICLR, 2023."
            },
            "questions": {
                "value": "1. How robust is the proposed defense to adaptive attacks, ie.., adversarially enhanced backdoor samples to evade the detector?\n\n2. The authors mentioned MAP-D, but what is MAP-D was not clearly defined.\n\n3. How to defend a high poisoning rate like 10% or even 20%?\n\n4. Did the authors tune the baseline defenses on the tested attacks, the comparison was unfair if not.\n\n5. How to choose a proper k for the knn detector?\n\n6. which DNN model was used for CIGAR-10, whose clean ACC is too low. \n\n7. A high-resolution dataset like an ImageNet subset should also be tested in the experiments, as they have different convergence speed and hen loss dynamics."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No theics concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657188155,
        "cdate": 1698657188155,
        "tmdate": 1699636926578,
        "mdate": 1699636926578,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "srpHQ7GRxU",
        "forum": "uw5U7FfTRf",
        "replyto": "uw5U7FfTRf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_U6eM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7630/Reviewer_U6eM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a backdoor detection method named BadLoss. It focuses on the threat model that modifies the training dataset and detect them via loss dynamics. Specifically, it needs a probe set which has potential trigger patterns and use them to detect poisoned samples. After deleting the backdoored samples, it uses the clean set to retrain the model. The experiments validate the effectiveness of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tAddressing the datasets backdoor attack is still an interesting and realistic direction.\n2.\tDetecting multi-trigger backdoor attacks is an efficient way to deal with large scale dataset."
            },
            "weaknesses": {
                "value": "1.\tResults have only marginal improvement. For example, the badloss cannot maintain a consistent high clean accuracy for the sinusoid attack.\n2.\tAblation study is needed. For example, how do you choose the threshold and why you choose that. Would the probes set affect the performance significantly? How do you choose the k in kNN classifier.\n3.\tUsing loss is not a novel idea to detect backdoors, and there are many similar works.\nLi, Yige, et al. \"Anti-backdoor learning: Training clean models on poisoned data.\" Advances in Neural Information Processing Systems 34 (2021): 14900-14912.\nGuan, Jiyang, et al. \"Few-shot backdoor defense using shapley estimation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728754142,
        "cdate": 1698728754142,
        "tmdate": 1699636926453,
        "mdate": 1699636926453,
        "license": "CC BY 4.0",
        "version": 2
    }
]