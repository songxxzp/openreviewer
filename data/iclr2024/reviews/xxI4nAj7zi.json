[
    {
        "id": "tnQidO9XA4",
        "forum": "xxI4nAj7zi",
        "replyto": "xxI4nAj7zi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_kkCh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_kkCh"
        ],
        "content": {
            "summary": {
                "value": "This paper intends to combine high-level features and fine-grained invariant-content features to improve the performance of cross-domain few-shot classification. Specifically, the author proposes to extract invariant-content features via a single attention head and fuse the extracted invariant-content features and high-level features via the scaled dot-product attention mechanism of the Transformer. The proposed methodology recovers the key content features of the target class, which did not work well in existing meta-dataset works. Through extensive experiments, it is shown that the proposed method outperforms the baseline under various conditions."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper proposed a simple yet effective feature reconstruction method that significantly improves the performance.\n2) This paper presents a theoretical analysis of the proposed attention modules."
            },
            "weaknesses": {
                "value": "1) The single attention head and scaled dot production attention mechanism proposed in this paper are not novel concepts and are so simple that the technical contribution is insufficient to acknowledge the quality.\n2) The presented Theorem is just borrowed from the existing works. \n3) It is not easy to know exactly what \u201chigh-level features\u201d and \u201cinvariant-content features\u201d denote. It is not clearly explained in the manuscript, and from the caption in Figure 2, one can only guess that the output features of the backbone are high-level features, and the output features of the attention head are invariant-content features. \n3) Comparison with SoTA methods is insufficient. The baseline (URL, published in 2020) of the toy example in Figure 1 is outdated, and the experiment also needs to be compared with TSA[1] and TriM[2], which are currently recording higher performance than URL on the leaderboard.\n4) Motivation is poor. The motivation of this paper is that prior works cannot capture representative key features, and the only evidence to support this is a comparison of the figure's activation map. This comparison alone is not sufficient to point out the shortcomings of prior works, and since it is a qualitative comparison, it is not accurate. As a result, the significant performance gains in Tables 1 and 2 are not credible, and even if it is true, the performance improvement cannot be sufficiently explained in the paper, so the contribution is greatly limited.\n5) Citations are inconsistent. Some citations are contained within the parenthesis, while others are not.\nThere are redundant contents that are not very important. For example, Figure (4) is simple enough that it does not need to be shown as a figure, and there is no need to provide Theorem 2 since it is not proposed in this paper.\n\n\n[1] Li, Wei-Hong, Xialei Liu, and Hakan Bilen. \"Cross-domain few-shot learning with task-specific adapters.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[2] Liu, Yanbin, et al. \"A multi-mode modulator for multi-domain few-shot classification.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
            },
            "questions": {
                "value": "1) In the second paragraph on page 2, it is said that the proposed method considers both informativeness and discriminativeness at the same time. However, it is not easy to tell the difference between the two concepts through the current introduction. How exactly are they different, and how does the proposed method use these two concepts at the same time?\n2) It is questionable whether the feature reconstruction mentioned in this paper is really reconstruction. To me, it just looks like performing a scaled dot product, and considering that the \u03b1 value in Equation (2) is about 1e-4, I'm not sure if it has much significance.\n3) Page 7 says that query heads have Lipschitz continuous property and the author\u2019s comment follows as: \u201cwe can reliably leverage IFR to find good representations\u2026\u201d. But isn't this explanation too insincere and uninformative? Although it is known that Lipschitz continuity improves robustness against perturbation, but more detailed analysis is needed to be provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no ethics concern."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2427/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2427/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2427/Reviewer_kkCh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697637562113,
        "cdate": 1697637562113,
        "tmdate": 1699636178204,
        "mdate": 1699636178204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FFYeGghVSR",
        "forum": "xxI4nAj7zi",
        "replyto": "xxI4nAj7zi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_LDzc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_LDzc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an invariant-content feature reconstruction (IFR) method, which combines high-level semantic features with fine-grained invariant-content features for cross-domain few-shot classification.\nThe high-level semantic features are extracted from the original images by the backbone, and the invariant-content features are reconstructed from the augmented images by the transformer attention head.\nIn a word, IFR performs cross-attention between the original images and their augmented images. \nThe experimental results on the Meta-Dataset benchmark show the effectiveness of IFR in improving generalization performance on unseen domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The motivation and presentation are clear. \n2. The experimental results are good: The experimental results on the Meta-Dataset benchmark show the effectiveness of IFR in improving generalization performance on unseen domains."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed methodology is limited. The proposed IFR method only performs cross-attention between the original images and their augmented images. The transformer-based cross-attention has been widely used."
            },
            "questions": {
                "value": "1. The proposed IFR method only performs cross-attention between the original images and their augmented images. What is the limitation of cross-attention in obtaining the fine-grained-content features? Can authors make an improvement to the cross-attention structure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657241190,
        "cdate": 1698657241190,
        "tmdate": 1699636178126,
        "mdate": 1699636178126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O39wqULWWl",
        "forum": "xxI4nAj7zi",
        "replyto": "xxI4nAj7zi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_LoxD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2427/Reviewer_LoxD"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of cross-domain few-shot classification (CFC), where the objective is to perform classification tasks in previously unseen domains with limited labeled data. The authors propose a novel approach named Invariant-Content Feature Reconstruction (IFR), which aims to simultaneously consider high-level semantic features and fine-grained invariant-content features for unseen domains. The invariant-content features are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data at the pixel level using an attention module. The paper includes extensive experiments on the Meta-Dataset benchmark, demonstrating that IFR achieves superior generalization performance on unseen domains and improves average accuracy significantly under two different CFC experimental settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novel Approach: The paper introduces a unique method, IFR, which addresses the limitations of existing approaches by considering both high-level semantic features and fine-grained invariant-content features. This dual consideration is innovative and addresses a critical gap in cross-domain few-shot classification.\n\n- Extensive Experiments: The authors have conducted comprehensive experiments on the Meta-Dataset benchmark, providing a robust evaluation of their proposed method. This adds credibility to their claims and demonstrates the practical applicability of their approach.\n\n- Clear Problem Statement: The paper clearly articulates the challenges in cross-domain few-shot classification and provides a compelling argument for why existing methods are insufficient, setting a strong foundation for their proposed solution."
            },
            "weaknesses": {
                "value": "- Limited Explanation of Methodology: While the paper provides a high-level overview of the IFR approach, it could benefit from a more detailed explanation of the methodology, including the attention module and how it specifically contributes to invariant-content feature extraction.\n\n- Lack of Comparative Analysis: The paper presents experimental results demonstrating the effectiveness of IFR, but it lacks a thorough comparative analysis with existing methods, discussing in detail why IFR outperforms them. In addition, since this proposed work resembles [1][2], why not compare with these two methods in the Experiments Section?\n\n- Potential for Overfitting: Given that the approach focuses on fine-grained features, there might be a risk of overfitting, especially when dealing with extremely limited data in few-shot scenarios. The paper does not address this potential issue or provide strategies to mitigate it.\n\n[1] Memrein: Rein the domainshift for cross-domain few-shot learning. IJCAI 2020  \n[2] Cross-domain few-shot classification via adversarial task augmentation. IJCAI 2021"
            },
            "questions": {
                "value": "1. Can you provide a more detailed explanation of the attention module used in IFR and how it specifically contributes to the extraction of invariant-content features?\n\n2. How does IFR compare to existing methods in terms of computational efficiency and scalability, especially when applied to large-scale datasets?\n\n3. Given the focus on fine-grained features, how does IFR mitigate the risk of overfitting in few-shot scenarios? Are there any specific strategies or mechanisms in place to prevent this?\n\n4. How well does IFR generalize to domains that are significantly different from those in the Meta-Dataset benchmark? Have there been any experiments conducted in this regard?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2427/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797732360,
        "cdate": 1698797732360,
        "tmdate": 1699636178036,
        "mdate": 1699636178036,
        "license": "CC BY 4.0",
        "version": 2
    }
]