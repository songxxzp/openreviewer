[
    {
        "id": "PNzSmz5vuj",
        "forum": "QuVlUn4T2G",
        "replyto": "QuVlUn4T2G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
        ],
        "content": {
            "summary": {
                "value": "The submission aims to find minimally necessary requirements for a generalized dynamic NeRF method for monocular RGB video input. It finds that running off-the-shelf methods for consistent depth estimation of the RGB video is sufficient. Using the RGB input video and these depth estimates, dynamic novel view synthesis is possible without further per-scene optimization at test time (e.g. for appearance). As additional input annotations, semantic segmentation masks (to identify dynamic objects in the input images) and optical flow are obtained via pretrained, off-the-shelf methods. These inputs can then be combined with a pretrained generalizable static NeRF transformer and novel special handling for dynamic content to render novel views. Experiments on existing benchmark datasets show that the method outperforms or is on par with many prior scene-specific dynamic NeRFs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is an intriguing problem setting and while the paper does not find a method that is truly fast (due to the consistent depth estimation), it is a carefully executed study with good experiments. It is informative for researchers in the field to see where things currently stand.\n\nThe paper is extremely well written. The experiments of the method by itself are very thorough, only comparisons to other methods are a bit lacking (see below). \n\nThe appendix is thorough and covers all the questions I had about finer details of the method."
            },
            "weaknesses": {
                "value": "The most obvious downside of the paper is the result quality, unfortunately. The supplemental videos show rather low-quality results. Given that this is the first method in its problem setting, this is not necessarily a reason for rejection, as long as the experimental evaluation is great. The remaining weaknesses all concern the evluation.\n\n(1) Qualitative video comparisons: I don't understand the results on the Nvidia Dynamic Scenes dataset. Quantitatively, Neural Scene Flow Fields seems to be about on par with the submission. However, qualitatively, the results of NSFF (on their website) are much better. Where does that large discrepancy come from? Qualitative video comparisons to other works would help.\n\n(2) Fast scene-specific dynamic NeRFs: I would like to see a comparison with fast scene-specific dynamic NeRF methods. In terms of utility for novel view synthesis, generalizable NeRFs have two main advantages over scene-specific NeRFs: speed and learned prior knowledge. The latter is not exploited by the submission, as the results show novel view synthesis that sticks closely to the input camera path (instead of revealing hidden areas that scene-specific methods could not handle). Which leaves speed and I'd hence like to a see a comparison with Fang et al. TiNeuVox '22 (code is available), which optimizes a dynamic NeRF in a few minutes, unless there is a reason why such a comparison is unnecessary.\n\n(3) Static generalizable per-frame NVS: Also, given the rather limited quality and temporal instability of the results, I'd like to see a comparison with static single-image generalizable novel view synthesis methods. For example, Sajjadi et al. Scene Representation Transformer '22. MIT-licensed code is here https://github.com/stelzner/srt and the authors say on their project page that this code is reliable.\n\n(4) All qualitative results of the proposed method: Why are the qualitative video results on the DyCheck iPhone dataset not included in the supplement? \n\n== Minor Comments ==\n\nWhat is \"hundreds of GPU hours per video\" in the introduction referring to? It sounds as if existing per-scene methods take hundreds of hours per video (incl. appearance optimization). But that's not the case with most monocular dynamic NeRFs, especially the recent fast methods that take minutes rather than hours.\n\nThe related work is covered very thoroughly. Only recent diffusion-based approaches for novel view synthesis could additionally be cited, e.g. Watson et al. Novel View Synthesis with Diffusion Models (ICLR '23).\n\nThe first citation in A.1 is for transformers in general, not for GNT.\n\nPlease add a sentence to the main text that splat-/point-/mesh-based rendering of the point cloud is used to get the dynamic image rendering. The section on dynamic rendering feels incomplete currently, with some context/framing missing. \n\nI would not call the results \"high-quality\" (e.g. caption of Figure 8). The videos are not high-quality."
            },
            "questions": {
                "value": "I am confused by what's happening in Table 3. The final method is 3? And 5-1 and 5-2 differ in what way? Does the final method not use Sec. 3.3.2, while 5-1 and 5-2 do?\n\nOther than that, the weaknesses cover the four points I think need to be addressed in a rebuttal. I am open to arguments as to why these experiments might not be necessary.\n\n===\n\nThe rebuttal addressed my concerns very well and I am hence updating my score to Accept."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697630738529,
        "cdate": 1697630738529,
        "tmdate": 1699955808642,
        "mdate": 1699955808642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vt83zqUBNp",
        "forum": "QuVlUn4T2G",
        "replyto": "QuVlUn4T2G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a study on generalized dynamic novel view synthesis from monocular videos, a challenge yet to be addressed in the literature. The authors establish an analysis framework, developing a \"pseudo-generalized\" technique that doesn't require scene-specific appearance optimization. The study found that geometrically and temporally consistent depth estimates are crucial to achieve this approach. Interestingly, this pseudo-generalized method outperformed some scene-specific techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Originality in addressing the generalized dynamic novel view synthesis from monocular videos.\n- Introduction of the pseudo-generalized process without scene-specific appearance optimization.\n- A comprehensive set of experiments and detailed ablations to validate the approach."
            },
            "weaknesses": {
                "value": "- Presentation and clarity can be enhanced.\n- A broader range of related works should be included in the comparisons.\n- Ambiguity about the role of consistent depth estimates in the final result.\n- Experimental validation seems limited to certain datasets, potentially affecting generalizability."
            },
            "questions": {
                "value": "- Can the authors clarify the specific role and impact of consistent depth estimates in their method?\n- How does the proposed method compare to generalized techniques not mentioned in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698064674880,
        "cdate": 1698064674880,
        "tmdate": 1699636257332,
        "mdate": 1699636257332,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zU6pCsGjgI",
        "forum": "QuVlUn4T2G",
        "replyto": "QuVlUn4T2G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_BQLw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_BQLw"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to solve and study the novel view synthesis problem for a general dynamic scene. It argues that we can have a generalized approach to dynamic novel view synthesis modeling from monocular videos by overcoming the dependence on the scene appearance. The proposed method uses scene depth, optical flow, and dynamic and static content masks, assuming that dynamic motion is linear and spatially consistent. Results on a few datasets are shown to back up the claims made in the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper aims at solving a very challenging problem and studies existing bottlenecks."
            },
            "weaknesses": {
                "value": "## Abstract\n- We find a pseudo-generalized \u2026 is possible -> We found that \u2026 is possible.\n\n## Introduction\n- Authors have given explanations justifying the keywords such as generalized, scene-specific optimization, scene consistent depth, etc., used in the paper. Yet, it is rather weak as the approach itself relies on consistent depth estimates of a dynamic scene, which, in fact, is a very open problem and acceptable solutions generally rely on appearance cues and scene flow.\n\n- Furthermore, with years of practice with physical depth sensors\u2014be it iPhone depth sensing modalities or recent LiDAR, it's very hard, if not impossible, to recover consistent depth estimates for outdoor and indoor cluttered scenes. Even for static scenes, it is highly dependent on the subject material type, lighting condition, and other physical phenomena to have acceptable depth from a physical sensor, and here we are dealing with dynamic scenes. This is precisely the reason for methods such as \"Stable View Synthesis\" CVPR 2021, \"Enhanced Stable View Synthesis\" CVPR 2023, and Enhancing photorealism enhancement, TPAMI 2022 papers to make use of multi-modal 3D data to train the model. For completion, TPAMI 2022 could also work for dynamic scenes. The paper should emphasize such intrinsic details, detailing the papers mentioned above and the role of 3D data in novel view synthesis.  \n\n- Authors should also clarify why MonoNeRF does not qualify the definition of pseudo-generalized approach, given that the paper mentions \u201cit is unclear whether MonoNeRF is entirely generalizable and can remove scene-specific appearance optimization\u201d. It is better to test and present clarity in the rebuttal phase.\n\n## Scene Content Rendering\n- \u201cwe think it is possible to avoid scene-specific appearance optimization even if the monocular video input contains dynamic objects.\u201d This argument is provided despite the paper relies on Varma et al. 2023 pretrained GNT which greatly benefits from appearance. Please clarify in the rebuttal as it is inconclusive as to how far the proposed methods benefit from Varma et al. 2023 work, given that the current method is aware of the dynamic subject mask. Hence, in my view, the contribution looks very little.\n\n## Using Depth and temporal priors\n- The assumptions about linear motion and use of optical flow is mentioned later in the paper. This must be highlighted in the introduction. Also, the assumption about linear dynamics of a scene is not convincing for a paper oriented towards a generalized or pseudo-generalized approach. \n\n## Experiments\n\n- Results are considerably lower in performance. This makes me conclude appearance is indeed an important cue for neural rendering. Of course, it could take more time, yet it helps gain realism. So, I am not sure whether the research presented in the paper is about time optimization or towards photorealistic rendering of dynamic scenes. Please clarify.\n\n\n- Missing experiments on outdoor dynamic scene dataset such as Cityscapes. Kindly evaluate results on this dataset and compare it with Enhancing photorealism enhancement, TPAMI 2022."
            },
            "questions": {
                "value": "Kindly refer weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468788674,
        "cdate": 1698468788674,
        "tmdate": 1699636257231,
        "mdate": 1699636257231,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZGiCtMDaqS",
        "forum": "QuVlUn4T2G",
        "replyto": "QuVlUn4T2G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to render novel views of a dynamic scene with much less per-scene optimization than competing techniques such as NSFF [Li et al. 2021] and Dynlbar [Li et al. 2023] . The input to the method is a video of a scene and a set of new camera poses over time. The output is a rerendered video. \n\nThe method works by computing a mask of the dynamic parts of the scene using existing methods. It also computes depth and optical flow. The dynamic parts of the scene are then modified by turning the dynamic pixels into point clouds and rerendering according to the new camera poses. The static parts of the scene are rerendered using a modified version of the generalizable NeRF transformer [Varma et al. 2023]. The dynamic and static parts of the scene are then combined.\n\nAccording to the quantitative metrics, the proposed method seems to perform slightly worse than NSFF and better than some other baselines that work on dynamic video inputs. This would be acceptable as the proposed method is much faster than NSFF and Dynlbar. However, qualitatively, according to the supplemental videos, the proposed method seems much worse than all competing methods, with substantial flickering.\n\n\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural Scene Flow Fields for SpaceTime View Synthesis of Dynamic Scenes. In CVPR, 2021.\n\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. DynIBaR: Neural Dynamic Image-Based Rendering. In CVPR, 2023."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper tries to solve a worthwhile problem that would be very impactful to many groups and companies (rerendering a dynamic video without per-video optimization). The paper reads a bit like a systems paper where there are a dozen components (dynamic mask generation, depth estimation, optical flow, static scene rendering, dynamic scene rendering) that contribute to the final solution.\n\nThe overall algorithm makes a lot of sense and seems like it should work. There are also a lot of comparisons to other methods and an ablation study."
            },
            "weaknesses": {
                "value": "Given that Dynlbar exists, the answer to the title\u2019s question seems like a resounding yes? I suggest the authors change the title to not be a general question and to be something specific about how their method works. I know that the authors are not planning on using scene-specific optimizations, which is how they distinguish their work from Dynlbar, but the title does not make this clear.\n\n\nThe supplementary results seem much, much worse than Dynlbar or NSFF. In the presented results, the dynamic portion of the scene flickers in and out of existence. The LPIPS metrics reported in the paper (Figure 1, Table 1) are only slightly worse than NSFF, but the actual results seem much worse. Perhaps this is because LPIPS doesn\u2019t capture any notion of temporal consistency between the frames? The presented results are very inconsistent while those in NSFF and Dynlbar are not that inconsistent.\n\nI am not sure if the metrics evaluated make sense since they don\u2019t take into account temporal consistency.\n\nThere is a minor missing citation. Consider discussing Figure 5 of https://arxiv.org/pdf/1909.05483.pdf [Niklaus et al. 2019] when presenting the statistical outlier removal technique (Fig.~S1). Niklaus et al. 2019 solve a similar problem where inaccurate depth estimates at object boundaries cause a similar problem to the one presented in Figure S1.\n\nMy relatively negative rating is based on the seemingly low quality results presented in the supplemental. It seems like the proposed technique does not work that well? The overall algorithm makes sense to me, so I am very surprised at how low quality the results are."
            },
            "questions": {
                "value": "The video results presented in the supplemental seem much worse than both NSFF and Dynlbar. Specifically, there is a lot of temporal flickering in the dynamic parts of the scene. Do the authors know why this is? I would be very interested in seeing the dynamic mask, the optical flow and the depth estimates to better understand why there is so much flickering. I wonder if the authors uploaded the wrong set of results?\n\nI would be interested in seeing video results on the DyCheck dataset.\n\nHow does the proposed method handle cases where the new camera poses peer behind an object to a location that was not seen in any of the input frames? There\u2019s no explicit inpainting step, but my guess is that GNT will not do a good job in these locations? You can kind of see this issue in the presented videos at the edge of the frames, where there are parts of the scene not seen in any of the input views."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S",
                    "ICLR.cc/2024/Conference/Submission3109/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704807115,
        "cdate": 1698704807115,
        "tmdate": 1700662892106,
        "mdate": 1700662892106,
        "license": "CC BY 4.0",
        "version": 2
    }
]