[
    {
        "id": "xKPhoTUwFs",
        "forum": "Rry1SeSOQL",
        "replyto": "Rry1SeSOQL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_2G4a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_2G4a"
        ],
        "content": {
            "summary": {
                "value": "This work proposes Comparator, a reference-free MT evaluator that treats the evaluation as a inter-system comparison problem. The model consists of a pre-trained encoder that accepts the concatenation of the source sequence and a pair of translations to compare and a comparator head that pools the output embeddings and produces a binary decision on which translation is better. The model is trained in three stages: XNLI pre-training (preferring entailment over non-entailment), human/machine translation discriminating (preferring human translation over machine translations) and weakly-supervised tuning (pairs of translations judged by BertScore and synthetic data by perturbation). The proposed model is evaluated on various MT eval datasets and the results show its effectiveness and benefits even over supervised baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is straight-forward and shows good performance over a range of datasets.\n- Some of the indirect and weakly supervised training method is interesting and might be inspiring for future study of MT evaluation."
            },
            "weaknesses": {
                "value": "- I\u2019m wondering if the comparison is fare for other systems since the proposed model is trained with multiple external resources such as XNLI and especially some data with parallel sentences, and is based on large base pre-trained encoders. I think it would be more convincing if there can be ways to directly compare different evaluation systems (ref-free vs ref-included, score-based vs comparison-based) with the same training resources and base models. (But surely, it would be indeed a benefit if the proposed system can better utilize extra resources.)\n- There should be more analysis on the proposed methods (such as those in Section 5). For example, more ablation studies on the training stages and especially the usage of different resources, and more detailed analysis on the metric and perturbation methods in Stage 3. Some of the result and setting sections may be shorten or moved to the appendix.\n# --\n- (Updates): Most of these concerns are addressed by the authors' responses, and the authors should provide those details in later versions."
            },
            "questions": {
                "value": "- What pre-trained model did the baseline systems use? Did they use up to XXL models? (Same or similar base models should be utilized for fair comparisons.)\n- Are there any ways to convert the relative comparisons to absolute scores? Sometimes, we might still need the scores (for example as rewards for RL).\n- In Stage 2 and the first part of Stage 3, references are required for the training purpose. If those two parts are ablated, how would it influence the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Reviewer_2G4a"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698288116439,
        "cdate": 1698288116439,
        "tmdate": 1700640234330,
        "mdate": 1700640234330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uI28EXQElL",
        "forum": "Rry1SeSOQL",
        "replyto": "Rry1SeSOQL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_GQxu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_GQxu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a comparative MT evaluation metric: instead of comparing machine translations to references, it compares multiple machine translations. The model is built on a bidirectional LLM, that encodes pairs of translations, and a pooling and logistic regression layer on top. It is trained with data from crosslingual NLI, pairs of human and machine translations, and synthetically rated or corrupted pairs of translations. Evaluation is done on a set on the WMT20 metrics task, several MQM datasets and ACES, a challenge dataset. The proposed model is compared to previous state-of-the-art reference-free metrics and largely outperforms them across languages, as well as supervised baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Idea and method are simple and well explained. Given that it requires much less costly data than the competing methods, it poses an attractive solution for MT evaluation.\n- The reported results are strong, given that they do not require references or direct supervision. The proposed model outperforms both supervised and unsupervised baselines.\n- The ablations give an insight into the importance of the different stages and generalization to unseen languages, which allows one to get a more thorough understanding of the method and its benefits."
            },
            "weaknesses": {
                "value": "The novelty seems limited / overemphasized. In Quality Estimation (QE) reference-free ranking approaches have been used for MT quality estimates before it was re-invented in the context of MT metrics competitions. For example, in the very first QE task in 2012 (https://www.statmt.org/wmt12/quality-estimation-task.html) ranking based evaluations (without references) were already designed, and as a result, ranking based methods have been developed as well (e.g. Avramidis, Eleftherios. \"Sentence-level ranking with quality estimation.\" Machine translation 27.3-4 (2013): 239-256.; Eleftherios Avramidis. 2012. Comparative Quality Estimation: Automatic Sentence-Level Ranking of Multiple Machine Translation Outputs. In Proceedings of COLING 2012, pages 115\u2013132, Mumbai, India.)"
            },
            "questions": {
                "value": "- Can you explain what the sentence \u201crelative ranking annotation from direct assessment with a large enough threshold has been used with as few as one annotation\u201d (Intro) means? Where does the threshold come into play and what does it mean to have one annotation only (I assume one per input, but not only one input)?\n- Figure 1 is not adding much, its content is clear from the text. The space could be used to elaborate the connections to QE (see above) and report more empirical results on newer datasets.\n- Is there any train/test overlap of the training data of mT5 and the benchmarks\u2019 test data? \n- What if references were removed from the fine-tuning sets in Stages 2 and 3? This would be a useful ablation to make the models truly free from references.\n- Which of the evaluation differences are significant? \n- What if Stages 1&2 are dropped and only Stage 3 is performed? This would further illustrate the importance of that stage and make it clear for future use where most time investment should go."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783590884,
        "cdate": 1698783590884,
        "tmdate": 1699637042528,
        "mdate": 1699637042528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "46k6erTsS9",
        "forum": "Rry1SeSOQL",
        "replyto": "Rry1SeSOQL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_Qwxh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_Qwxh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a method to learn pairwise reference-less evaluation of MT. This scenario corresponds to a common, real use where reference translations are mostly unavailable and the interest is mostly in comparing systems, rather than absolute scores. The pairwise ranking is a good framework since it is easier to collect synthetic data with pairwise judgments, as opposed to assigning quality scores to synthetic examples. Pairwise rankings also achieve higher inter-annotator agreement than human judgments. The method proposes a 3-stage pipeline for finetuning with various kinds of synthetic data. \n\nThe results show that the approach can achieve state-of-the-art performance comparable or better than supervised approaches. The analysis also shows that adding supervised data can further improve performance modestly. Interpreted in another way, it also means that the synthetic data generated is sufficient to capture most of the attributes of supervised data for pairwise ranking of systems."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The paper is well-written and explains the motivation for the work well. \n* The experiments are extensive and establish that reference-less evaluation is very competitive with reference-based metrics. \n* The use of synthetic data for pairwise ranking is a very clean way of using synthetic data and helps train high-quality reference-less metrics."
            },
            "weaknesses": {
                "value": "While pairwise evaluations of systems are useful, a more practical utility would be to rank multiple models. Score-based systems enable that easily. With pairwise ranking-based systems, multiple comparisons have to be run. Every time a new system has to be ranked, it has to be compared with multiple existing systems."
            },
            "questions": {
                "value": "* Equation 11 should be y=0?\n* 3 languages from the WMT-20 Metrics tasks have not been included in the evaluation. It would be good to include those results as well for getting a complete view of the WMT-20 Metrics task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699167193970,
        "cdate": 1699167193970,
        "tmdate": 1699637042406,
        "mdate": 1699637042406,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rqVI05WrXW",
        "forum": "Rry1SeSOQL",
        "replyto": "Rry1SeSOQL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_DxQx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8378/Reviewer_DxQx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel reference-free machine translation evaluation method which directly compares the two hypotheses from two systems by pre-trained language models, e.g., mT5."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The manuscript is commendably clear in its presentation, providing a lucid explanation of the method's underpinnings and its design rationale. The method itself is logically structured and appears to be grounded in a sound understanding of the underlying technical principles."
            },
            "weaknesses": {
                "value": "(Main) 1. **Experimental Settings**: Upon meticulous examination, I observe that the experimental setup deviates from the conventional practices of evaluating numerous systems simultaneously. The study opts to assess a custom set of 'Better-worse judgments' between pairs of system outputs. This focus narrows the scope of the evaluation and raises concerns about the validity of the correlation results. The paper's method shows a strong correlation with these judgments but fails to conclusively demonstrate the superiority of one system over another. The more critical challenge lies in integrating these pairwise comparisons into a comprehensive evaluation framework that can handle multiple systems. The current approach's limitations in addressing this challenge may undermine its utility and applicability.\n\n2. **Scope and Generalization**: The narrow focus of the study may limit its applicability beyond its stated domain. The paper could benefit from an expanded discussion on how the proposed method might adapt or extend to other evaluation contexts, such as the assessment of large language models (LLMs). While ICLR might be receptive to specialized domain contributions, the paper's current emphasis suggests that it might find a more fitting audience at a dedicated NLP conference.\n\n**Suggestions**\n1. **Contextualizing with Recent Advances**: Recent developments in MT evaluation metrics, such as xCOMET (https://arxiv.org/abs/2310.10482) and SLIDE (https://arxiv.org/pdf/2309.08832.pdf), emphasize the importance of error span evaluations. While it is not mandatory to compare against the latest publications, incorporating insights from these advancements could significantly enhance the robustness and relevance of the proposed method. \n\n2. **Related Work**: I recommend that the authors consider the insights from \"DABERTScore\" (https://aclanthology.org/2021.acl-short.5.pdf). There are conceptual overlaps between this work and the manuscript under review, which merit a thorough comparison and discussion within the paper to enrich the context and underscore the novel contributions of the proposed method."
            },
            "questions": {
                "value": "1. How should the paper address the ranking of two MT systems? Is there a more robust method than simply tallying the number of better translations?\n2. When scaling up the evaluation to multiple systems, how might one resolve apparent ranking contradictions, such as the scenario where System A outperforms System B, System B outperforms System C, but System C outperforms System A?\n\nThese questions are pivotal in addressing the practical implications of the proposed method and its capacity to function in a more complex, real-world evaluation environment. A more thorough exploration of these aspects could substantially strengthen the paper\u2019s contribution to the field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8378/Reviewer_DxQx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699174706690,
        "cdate": 1699174706690,
        "tmdate": 1700999490063,
        "mdate": 1700999490063,
        "license": "CC BY 4.0",
        "version": 2
    }
]