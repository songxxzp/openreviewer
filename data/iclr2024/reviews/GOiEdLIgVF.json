[
    {
        "id": "0w5eKt7xoj",
        "forum": "GOiEdLIgVF",
        "replyto": "GOiEdLIgVF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_8orT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_8orT"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Saliency-Guided Hidden Associative Replay for Continual Learning (SHARC), a method that attempts to tackle the catastrophic forgetting problem in continual learning by incorporating associative memory into replay-based strategies. The memory selectively stores essential feature map channels determined using saliency methods like Grad-CAM. Experiments on several benchmarks show improvements over some existing replay-based CL methods under task-incremental and class-incremental learning settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper's endeavor to draw inspiration from insights into human brain function for addressing CL is commendable.\n\n- The proposed method improves several existing replay-based CL methods."
            },
            "weaknesses": {
                "value": "1. The working mechanism of associative memory is confusing. \n- It is unclear how the queries are obtained during both training and inference. \n- The concept of queries, which appears to be image crops in Fig. 2, contradicts the notion presented in the paper, where specific feature map channels are retained.\n- The practicality of recalling precisely the same image when given a query is questionable, as queries pertain to new classes, while the memory contents relate to old classes. This discrepancy, unless old queries are also stored, fundamentally undermines the approach. Storing old queries, however, introduces extra complexities that appear inconsistent with the intended advantages of using associative memory.\n\n\n2. The reliance on gradient-based saliency methods to evaluate the importance of feature map channels may not align with the underlying feature selection mechanisms in the human brain.\n\n\n3. The related work is noticeably lacking in depth. \n- The paper lacks a comprehensive overview of both regularization-based and dynamic architecture-based continual learning methods. Notably, there is a complete absence of references to the latter category.\n- The review neglects to consider replay-based continual learning methods that store lightweight features or generate pseudo features for old classes [1-3], which should be addressed, discussed, and compared.\n- The absence of a thorough examination of other continual learning methods employing brain-inspired memory systems, such as [4-5], leaves a critical gap that requires addressing through comprehensive review, discussion, and comparison.\n\n\n4. In terms of comparisons: \n- It is unclear whether methods without SHARC in Tables I and II are also pre-trained on ImageNet. \n- The authors are encouraged to extend the comparison to include more recent methods introduced in 2022 or 2023.\n\n\n5. The claim that \"existing work Sun et al. (2015) has proved that hidden representation learned by convolutional neural networks is highly sparse in the hidden space\" is inadequately supported, as the referenced work by Sun et al. (2015) predominantly focuses on face representations rather than general representations acquired through CNNs, rendering the argument questionable.\n\n\nReference\n\n[1] Generative feature replay for class-incremental learning. CVPRW 2020.\n\n[2] Self-sustaining representation expansion for non-exemplar class-incremental learning. CVPR 2022.\n\n[3] Fetril: Feature translation for exemplar-free class-incremental learning. WACV 2023.\n\n[4] Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. ICLR 2022\n\n[5] Sparse Coding in a Dual Memory System for Lifelong Learning. AAAI 2023"
            },
            "questions": {
                "value": "- What is the value of K? Is it the same as the number of classes in order to perform the Grad-CAM? Will K change under the class-incremental setting?\n\n- What are the two multiplication operations in Eq. (4), respectively?\n\n- Is A\u2019 a feature map after discarding the non-salient channels?\n\n- Can you provide more explanations for the sentence \u201cWe only need to keep track of the channel index which is only a 1d vector and cheap to store\u201d?\n\n- What is the \u201cpartial cue\u201d that is used to retrieve feature maps in the associative memory?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6396/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6396/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6396/Reviewer_8orT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698135390904,
        "cdate": 1698135390904,
        "tmdate": 1699636709282,
        "mdate": 1699636709282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cFyqbtdnub",
        "forum": "GOiEdLIgVF",
        "replyto": "GOiEdLIgVF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_vmux"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_vmux"
        ],
        "content": {
            "summary": {
                "value": "Paper proposes a novel Continual Learning (CL) method, SHARC, inspired by biological memory (selective memory retention of salient experience). It has 2 novel contributions to the CL problem: saliency selection of feature channels and Associative Memory (AM) to improve memory efficiency and mitigate forgetting. The empirical experiments for the Task-IL and the Class-IL settings show significant improvement over the SOTA replay CL methods like GEM, A-GEM, ER, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Paper's position is that the biological inspired architecture for CL is more memory efficient and can outperform the current replay methods which either stores the entire representation or approximately generate the training data for older classes. This motivation is supported by the strong experimental results."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper is limited as it combines existing methods such as \"saliency\" and \"associative memory\" for the CL problem. In image retrieval literature, there are several prior works which combine both mechanisms. See references below. As such, the real novelty lies only in the application of these 2 mechanism to the CL problem.\n\n2. (Minor) Experiments only include replay-based CL methods. As mentioned in the paper's related works, replay methods are among the strongest in CL. However, this design leaves a big unknown about how well the proposed method compare against other approaches, like regularization-based and dynamic architecture-based. Only DER++ which has regularization and rehearsal is included.\n\n3. (minor) The implementation details about how SHARC combine with the other baseline methods are not given. However, this is somewhat mitigated by the inclusion of anonymous codes. I did not inspect the codes, however.\n\nGe, S. S., Li, M., & Lee, T. H. (2016). Dynamic saliency-driven associative memories based on network potential field. Pattern Recognition, 60, 669-680.\nKuo, D. W., Cheng, G. Y., Cheng, S. C., & Lee, S. L. (2012, October). Detecting salient fragments for video human action detection and recognition using an associative memory. In 2012 International Symposium on Communications and Information Technologies (ISCIT) (pp. 1039-1044). IEEE."
            },
            "questions": {
                "value": "1. (Section 5.2) How does the proposed method combine with the 6 replay-based methods? Does the SHARC Memory Replay module directly replace the respective methods' original replay mechanism? What about the SHARC prediction head? How does it combine with the other methods?\n\n2. By \"combining\" SHARC with other methods, what's the network size increased? In general, we can expect improved performance for many tasks simply by increasing the network size. So it's important to know the increase of network size, after combining with SHARC."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698388492184,
        "cdate": 1698388492184,
        "tmdate": 1699636708917,
        "mdate": 1699636708917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C7pPs1vM3a",
        "forum": "GOiEdLIgVF",
        "replyto": "GOiEdLIgVF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_prAb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_prAb"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel framework called Saliency-Guided Hidden Associative Replay for Continual Learning (SHARC) to address the challenge of catastrophic forgetting in continual learning. \n\nIn this paper, we provide a plugin to enhance the performance of replay-based continual learning methods to improve storage efficiency.\n\nThe paper proposes the SHARC framework, which combines associative memory with replay-based strategies. SHARC encodes and archives salient data segments using sparse memory encoding. By leveraging associative memory paradigms, SHARC introduces a content-focused memory retrieval mechanism, promising quick and accurate recall.\n\n The associative memory creates an additional memory footprint and consumes a lot of computing resources for updating it.\n\nThe paper presents extensive experimental results that demonstrate the effectiveness of SHARC for various continual learning tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method can be seamlessly adapted to any replay-based approach, improving their performance in various continual learning scenarios. \n\nThe experimental results provide evidence of the effectiveness of SHARC in improving the performance of replay-based methods."
            },
            "weaknesses": {
                "value": "Lack of detailed network and hyper-parameter configuration, especially, for associative memory networks.\n\nThe lack of recent baselines in the experiment, most of the baselines used were proposed two or three years ago.\n\nThe associative memory A(x,\u03c9) is implemented as a recurrent or feed-forward neural network. The associative memory creates an additional memory footprint, and consumes a lot of computing resources for updating it.\n\nThe experiments in this paper do not seem to be reasonable. The method in this paper introduces an additional associative memory network to store more information for replay, which definitely makes the original method perform better."
            },
            "questions": {
                "value": "How much memory and computing resources does associative memory take up? Wouldn't it be better if those extra storage resources were used to store more exemplars? Please make a comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698579323286,
        "cdate": 1698579323286,
        "tmdate": 1699636708795,
        "mdate": 1699636708795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PSuwnVNdHr",
        "forum": "GOiEdLIgVF",
        "replyto": "GOiEdLIgVF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_WbsA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_WbsA"
        ],
        "content": {
            "summary": {
                "value": "In conventional replay-based continual learning methods, raw/ entire data are stored and recalled during replay which is biologically implausible and memory intensive. This work presents a biologically plausible framework where partial salient data are stored and complete data are retrieved during replay. It utilizes sparse memory encoding to store partial information and a content-based memory retrieval mechanism to recover complete information. It enables efficient memory storage and archives better recall accuracy than generative models. This work has potential to perform effectively in highly memory-constrained applications."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents bio-inspired perspectives to store hidden sparse representations and associate memory based recall. This resembles how humans and animals learn by compressing information.\n\nInnovative approach of storing and retrieving rehearsal data for memory efficient replay and mitigating catastrophic forgetting.\n\nSaliency based approach to store sparse information which leads to increased memory efficiency.\n\nAssociate memory based retrieval offers fast and efficient recall and higher noise tolerance. This is an innovative approach to reduce memory footprint and computational overhead in continual learning.\n\nMemory-forgetting mechanism to remove more data from old tasks than new tasks.\n\nSeveral SOTA methods show improved performance when combined with proposed method, SHARC"
            },
            "weaknesses": {
                "value": "Lack of experiments on high dimensional and large scale datasets e.g., ImageNet-1K. Many algorithms do not scale for large numbers of classes and high-dimensional inputs. It is 2023 and people have been using ImageNet for continual learning for at least 7 years. I'm assuming this is because they are starting with an ImageNet pre-trained backbone, but that is also a problem given the datasets studied. Mini-imageNet is not an appropriate test set using an ImageNet pre-trained backbone. MNIST and CIFAR are also extremely inappropriate. This is throwing a comparatively very powerful network at toy problems, where training its output layer alone likely yields extremely high results.\n\nThe model is only tested for an extreme edge case in continual learning (class incremental learning). Other distributions need to be studied, IID, etc. Given the neuroinspiration, this is especially important, but it conflates the goals of continual learning (knowledge accumulation over time) with the test (learning classes one at a time). An ideal continual learner should be robust to any data orderings including class incremental learning and IID.\n\nLimited representation learning. It keeps the feature backbone frozen and trains the classifier head and associative memory network. Thus the model has limitations in learning representations in hidden layers which might be necessary for learning new tasks.\nIt is unclear how model depth impacts retrieval performance, for example when we want to store and retrieve information in the earlier layers close to input.\n\nGiven ImageNet-1K pretrained backbone, selected datasets e.g., MNIST, CIFAR-10 / 100, and mini-ImageNet seem less challenging for a continual learner. It is also unclear how ImageNet-1K (224x224) pretrained network is used for small datasets consisting of lower resolution images (32x32).\n\nSince SHARC requires training associate memory unlike comparison methods. Comparing methods based on the same bounded compute (same amount of training updates) will be fairer.\n\nSometimes SHARC under-performs some baselines (Table 2 and Table 4). It is unclear if SHARC provides consistent performance gain across CL settings / methods/ datasets/ buffer sizes. It is claimed that DER++ equipped with SHARC achieves a 45.5% improvement in accuracy on S-CIFAR-100 but results in Table 2 do not support this claim.\n\nThe experiments and evaluation leave a great deal to be desired. Using an ImageNet-1K pre-trained backbone is fine, but then the experiments would need to be appropriate, for example, learning a dataset like iNaturalist or Places-365, and then trying multiple different distributions, including incremental class learning. Also, more experimental comparisons against recent methods and benchmarking in a fair way where all models are compared with the same setup would be more sound. \n\nThe method needs to demonstrate some sort of efficiency or other value in some sense.\n\nThe method is interesting, but the evaluation and experimental confounds mean the paper is not ready for publication. I encourage the authors to redesign their experiments, eliminate confounds, study multiple distributions, and to study much larger and more appropriate datasets. It requires relatively few resources to train an ImageNet-1K model and can be done with cloud computing for very little money."
            },
            "questions": {
                "value": "In Fig.1, spatial information was retrieved but in the main experiment channel information was recovered. If you mask out spatial information, can you apply similar associate memory to retrieve complete information?\n\nWhat happens if you train more layers of DNN besides the final layer?\n\nWhat is the computational overhead for training associate memory? Does associate memory increase inference cost?\n\nHow do you initialize the last layer before continual learning begins?\n\nHow do you use ImageNet-1K (224x224) pretrained network for small datasets consisting of lower resolution images (32x32)?\n\nBesides Fig.1, do you have results to support the claims about fast and efficient recall and noise tolerance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737941628,
        "cdate": 1698737941628,
        "tmdate": 1699636708667,
        "mdate": 1699636708667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mWSExZrekk",
        "forum": "GOiEdLIgVF",
        "replyto": "GOiEdLIgVF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_3N7Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6396/Reviewer_3N7Y"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Saliency-Guided Hidden Associative Replay (SHARC) framework, which combines associative memory with replay-based strategies to address catastrophic forgetting in Continual Learning. Firstly, SHARC archives only the most salient segments of data through sparse memory encoding, making it memory-efficient. Secondly, this paper proposes a content-centric memory retrieval module inspired by associative memory, enabling swift and impeccable recall capabilities. Extensive experimental results demonstrate the efficacy and superiority of the proposed SHARC framework for various continual learning tasks ."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces the novel Saliency-Guided Hidden Associative Replay (SHARC) framework, which combines associative memory with replay-based strategies to address catastrophic forgetting in Continual Learning. \nThe proposed SHARC framework demonstrates its effectiveness through extensive experimental results on various continual learning tasks, showcasing its superiority in mitigating forgetting and achieving better recall. \nThe structure of SHARC is sparsity, which is hardware-friendly and can lead to memory cost reduction instantly."
            },
            "weaknesses": {
                "value": "The paper does not provide a comprehensive comparison with existing replay-based methods for Continual Learning, making it difficult to assess the superiority of the proposed framework.\nWhile the paper introduces a content-focused memory retrieval mechanism, it lacks detailed explanation and analysis of how this mechanism works and its impact on recall performance."
            },
            "questions": {
                "value": "It would be beneficial to include a comprehensive comparison with existing replay-based methods for Continual Learning to highlight the advantages and limitations of the proposed framework.\nCould the authors provide a more detailed explanation and analysis of the content-focused memory retrieval mechanism introduced in the SHARC framework? This would help in understanding how this mechanism works and its impact on recall performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6396/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024573209,
        "cdate": 1699024573209,
        "tmdate": 1699636708524,
        "mdate": 1699636708524,
        "license": "CC BY 4.0",
        "version": 2
    }
]