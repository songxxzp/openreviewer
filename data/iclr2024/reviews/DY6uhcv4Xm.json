[
    {
        "id": "a7cR8A9ME3",
        "forum": "DY6uhcv4Xm",
        "replyto": "DY6uhcv4Xm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_ozf6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_ozf6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a benchmark for federated learning security, including two components: federated attacker and federated defender. The federated attacker implements about eight classical attack methods covering data poisoning attacks, model poisoning attacks, and data reconstruction attacks. The defenders include before-aggregation, after-aggregation, and on-aggregation defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "A substantive assessment of the strengths of the paper, touching on each of the following dimensions: originality, quality, clarity, and significance. We encourage reviewers to be broad in their definitions of originality and significance. For example, originality may arise from a new definition or problem formulation, creative combinations of existing ideas, application to a new domain, or removing limitations from prior results. You can incorporate Markdown and Latex into your review. \nS1. This paper proposes a comprehensive benchmark for federated learning security, which is expected to prompt the prosperity of security research of federated learning.\nS2. The benchmark attempts to incorporate large language models, indicating its generalization ability.\nS3. The limitations and future direction have been discussed in this paper."
            },
            "weaknesses": {
                "value": "W1. This paper highlights that the benchmark considers LLMs; however, the unique challenges/differences between using LLMs and classical models are unclear. \n\nW2. It seems that the implemented attack/defense methods were published before 2021. The authors are encouraged to reproduce more SOTA attacks/defenses to improve the utility of this benchmark further.\n\nW3. In footnote 2, the authors claim that their benchmark focuses on security rather than privacy. However it seems that \u201cdata reconstruction attack\u201d is about privacy, which makes the taxology of this paper a little bit confusing."
            },
            "questions": {
                "value": "See Weeknesses. Addressing these weaknesses will further improve the convincing and quality of this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663658827,
        "cdate": 1698663658827,
        "tmdate": 1699636088237,
        "mdate": 1699636088237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AYIQDwXNPh",
        "forum": "DY6uhcv4Xm",
        "replyto": "DY6uhcv4Xm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_5b7s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_5b7s"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a comprehensive benchmark for simulating adversarial attacks and their corresponding defense strategies in the context of Federated Learning (FL). This benchmark, known as FedSecurity, is composed of two primary components: FedAttacker, which replicates attacks introduced during FL training, and FedDefender, which emulates defense mechanisms aimed at mitigating the effects of these attacks. FedSecurity is an open-source tool that can be tailored to encompass a wide array of machine learning models (e.g., Logistic Regression, ResNet, and GAN) and federated optimization techniques (e.g., FedAVG, FedOPT, and FedNOVA). Additionally, the authors demonstrate the utility of FedSecurity in the context of federated training for Large Language Models (LLMs), showcasing its adaptability and relevance in more intricate scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is important to have federated Large Language Models (LLMs) benchmark at this point for the community. I look forward the authors can dig deeper in this category and provide more complete/efficient implementation.\n\n2. Different categories of attack and defense methods been implemented."
            },
            "weaknesses": {
                "value": "1. To implement federated LLM, the bottleneck is always the computational power. I wonder if the authors can provide some of their pre-trained models before/after fine-tunning?\n \n2. Large-scale experiment up to 1000 clients is usually needed for benchmark works.\n\n3. A table for the real training time is suggested (better on different devices).\n\n4. Need more ablation studies given it is a benchmark work.\n\n5. Attacks/defenses for NLP tasks (e.g., [1]) can be added to the benchmark for LLM and/or smaller models.\n\n6. More backdoor attacks/defenses can be considered, and different evaluation metrics are needed (e.g., backdoor attack success rate)\n\n[1] Zhang, Z., Panda, A., Song, L., Yang, Y., Mahoney, M., Mittal, P., Kannan, R. and Gonzalez, J., 2022, June. Neurotoxin: Durable backdoors in federated learning. In International Conference on Machine Learning (pp. 26429-26446). PMLR."
            },
            "questions": {
                "value": "Compared with existing federated learning benchmarks (e.g., Leaf [1], Flower [2]), what are the major advantages of the newly proposed benchmark?\n\n[1] Caldas, S., Duddu, S.M.K., Wu, P., Li, T., Kone\u010dn\u00fd, J., McMahan, H.B., Smith, V. and Talwalkar, A., 2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097.\n[2] Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K.H., Parcollet, T., de Gusm\u00e3o, P.P.B. and Lane, N.D., 2020. Flower: A friendly federated learning research framework. arXiv preprint arXiv:2007.14390."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Reviewer_5b7s"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798292511,
        "cdate": 1698798292511,
        "tmdate": 1699636088167,
        "mdate": 1699636088167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jxCXrxqzhs",
        "forum": "DY6uhcv4Xm",
        "replyto": "DY6uhcv4Xm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_r7rM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_r7rM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces FedSecurity, a benchmark for simulating attacks and defenses in federated learning (FL).  FedSecurity has two main components: FedAttacker to inject attacks like data poisoning, model poisoning, and data reconstruction; and FedDefender to implement defenses like clipping, robust aggregation, and adding noise. It supports customizing attacks and defenses using provided APIs. It is also flexible to configure different models, datasets, and FL optimizers like FedAvg, FedOpt, etc. Experiments show Byzantine attacks like random noise can significantly degrade accuracy while defenses like m-Krum can mitigate it. Defenses may also inadvertently hurt accuracy when no attack happens. FedSecurity is also extended to federated training of large language models (LLMs) like BERT and Pythia. m-Krum defense is shown to be effective against backdoor and Byzantine attacks on LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tLLM extension: The benchmark is extended to federated training of large language models like BERT and Pythia, showing wider applicability.\n2.\tReal-world demonstration: A real-world experiment using edge devices shows the scalability beyond simulations.\n3.\tAnalysis and insights: The experiments analyze impacts of attacks and defenses, highlighting the need to balance robustness vs potential negative impacts of defenses."
            },
            "weaknesses": {
                "value": "1.\tLimited defense mechanisms: Only a small subset of defenses from the literature are implemented so far. More defenses could be included for completeness.\n2.\tLimited analysis: More in-depth analysis and visualization of how the attacks and defenses impact the model convergence would be useful.\n3.\tSmall LLM experiments: Evaluations on large language models are limited to just BERT and Pythia. More experiments on diverse LLMs would strengthen this part.\n4.\tNarrow task types: Most experiments focus on image classification. Expanding the tasks to include NLP, recommendation systems, etc. would make it more representative."
            },
            "questions": {
                "value": "See in Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838842058,
        "cdate": 1698838842058,
        "tmdate": 1699636088012,
        "mdate": 1699636088012,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lp9g7QX4XC",
        "forum": "DY6uhcv4Xm",
        "replyto": "DY6uhcv4Xm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_D1fe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1595/Reviewer_D1fe"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces FedSecurity, a library with attacks and defenses for federated learning useful for benchmarking and assessing the quality of defenses against different sets of attacks. The library comprises of two components: 1) FedAttack for training-time attacks in FL, including data and model poisoning and data reconstruction attacks; 2) FedDefender, which includes defenses that can be applied at the aggregator before, during or after the aggregation of the model parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Robustness of Federated learning is a hot topic and libraries for assessing the robustness of different aggregation methods and defensive techniques systematically is useful not only for the research community, but also for other ML practitioners and developers. \n\n+ The library seems to include a good set of attacks and defenses, although a table or a list with the complete catalogue of attacks and defenses would be beneficial for the reader. It is a plus that the library support some LLMs. \n\n+ The paper is well presented and easy to read."
            },
            "weaknesses": {
                "value": "- The paper does not include new techniques or advancements in the state of the art. It entirely relies on attacks and defenses that have already been proposed in the research literature. The experimental evaluation is a confirmation that the code produces reasonable results, but there is no novelty in there either. \n\n- The authors did not provide the code implementation of the library (I believe it could be easily anonymized). Then, as the main contribution of the paper is the library, not having access to the code implementation makes it hard to assess its characteristics and possible weaknesses. \n\n- The coverage of the library misses attacks at test time (e.g., adversarial examples) and other privacy attacks that can happen both during the training and the deployment of federated learning models, such as property inference attacks or membership inference attacks, to cite some."
            },
            "questions": {
                "value": "+ Could the authors provide the whole catalogue of attacks and defenses implemented in the library?\n\n+ Could the authors provide the code implementation (anonymized). \n\n+ Could the authors clarify the contributions of the paper for advancing the state of the art in robust federated learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1595/Reviewer_D1fe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699017853909,
        "cdate": 1699017853909,
        "tmdate": 1700674003749,
        "mdate": 1700674003749,
        "license": "CC BY 4.0",
        "version": 2
    }
]