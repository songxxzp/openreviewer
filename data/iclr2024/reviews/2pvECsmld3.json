[
    {
        "id": "CuSygqfCoR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So"
        ],
        "forum": "2pvECsmld3",
        "replyto": "2pvECsmld3",
        "content": {
            "summary": {
                "value": "This paper introduces SparseFormer, an innovative vision transformer designed for efficiency, which encodes images into a select number of sparse tokens in a latent space. The efficacy and computational economy of SparseFormer are showcased through its performance in ImageNet and video classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. the SparseFormer model it introduces attains impressive results with a notable reduction in computational cost and latency, highlighting its efficiency and practicality in application.\n2. it is composed with a clear presentation, making it accessible and understandable"
            },
            "weaknesses": {
                "value": "The experimental validation does not appear to be solid, such as the detection results and the ablation. see details in Question part\nThe novelty of SparseFormer is somewhat constrained, as it does not substantially deviate from existing methods in the field.\nThere is an absence of comparative analysis with other efficiency-oriented techniques, such as token pruning"
            },
            "questions": {
                "value": "Could you provide insight into why the detection results are not more favorable, especially considering that your Region of Interest (RoI) mechanism appears to be well-suited for detection tasks? \nAdditionally, the paper does not include an ablation study on adjusting the RoI mechanism, which leaves its importance in the proposed method somewhat ambiguous. Could you clarify the necessity of the RoI mechanism within your framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2346/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2346/Reviewer_H2So",
                    "ICLR.cc/2024/Conference/Submission2346/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697380419446,
        "cdate": 1697380419446,
        "tmdate": 1700979255225,
        "mdate": 1700979255225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5ELJfNOL9C",
        "forum": "2pvECsmld3",
        "replyto": "2pvECsmld3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_fz8s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_fz8s"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the SparseFormer, which modifies the standard Transformer model by using a small number of tokens in latent space to reduce its size and computational complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Provides an alternative sparse paradigm ($i.e.,$) for vision modeling compared to existing Transformers. Reduces computation by operating on limited tokens.\n2. Token ROI adjustment mechanism is effective at focusing on foregrounds.\n3. Visualizations show the model progressively focuses on discriminative regions."
            },
            "weaknesses": {
                "value": "1. While the paper demonstrates the effectiveness of SparseFormer on classification tasks. The reviewer has concerns about the generalization to more complex scenarios. Appendix A.1 also points out the inferior performance compared to the recent transformer network. The use of specific sparse attention patterns might limit the model's ability to capture certain types of long-range dependencies in the images for downstream tasks. \n2. In addition, the reviewer also has concerns about token ROI. Adjusting token ROIs lacks strong spatial supervision. Performance on dense prediction tasks ($i.e.,$ segmentation tasks) requiring precise localization may suffer. With complex images, the signal will be weak and may not focus on the meaningful pixels."
            },
            "questions": {
                "value": "Overall, this paper presents a step towards sparse vision architectures by a novel token ROI approach. The reviewer has no further questions, please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concern."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594899248,
        "cdate": 1698594899248,
        "tmdate": 1699636166844,
        "mdate": 1699636166844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bWdnkWan99",
        "forum": "2pvECsmld3",
        "replyto": "2pvECsmld3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce SparseFormer, which comprises two main components: the Focusing Transformer and the Cortex Transformer. The Focusing Transformer addresses the challenge of extracting image features sparsely, decoding them into latent tokens, and adjusting token regions of interest (RoIs). The Focusing Transformer efficiently extracts image features with a computational complexity of O(N\u00b7P\u00b7C), where N is the number of latent tokens, regardless of the input image size. Evaluated on ImageNet, the authors demonstrated that the proposed method achieved 1.7x faster inference speed compared with Swin-T with small accuracy degradation. It also outperforms ResNet50 with a faster speed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is thoroughly motivated and exceptionally well-written. The concept of sparsifying input tokens holds paramount importance for vision transformers (ViTs) owing to the quadratic complexity with respect to sequence length in multi-head self-attention.\n\n2. The authors have designed a functional solution, known as FocusTransformer, which improves upon the Perceiver method by introducing and dynamically adjusting regions of interest (RoIs). Experimental results compellingly demonstrate the effectiveness of this architecture on the ImageNet dataset.\n\n3. The authors have not only illustrated how SparseFormer can reduce computational workload (measured in FLOPs), but they have also empirically shown a significant speedup on a V100 GPU under FP32 precision, further showcasing the efficacy of their proposed approach."
            },
            "weaknesses": {
                "value": "1. While the authors have put considerable effort into elucidating the disparities between SparseFormer and Perceiver, it remains challenging for me to find a fundamental difference between these two methodologies. In my estimation, the primary distinction appears to be the introduction of the FocusTransformer. However, upon examination of this architecture, I have also observed a clear similarity to DeformableDETR. Consequently, I find it challenging to pinpoint the truly innovative contributions of this paper.\n\n2. The scope of the evaluation in this work appears somewhat limited. The presentation exclusively reports image classification results. However, Vision Transformers (ViTs) have showcased their efficacy across a diverse range of computer vision tasks, including object detection, semantic segmentation, and image generation. A majority of these applications typically demand high-resolution inputs, which makes the efficiency of reducing the number of visual tokens even more critical. My particular interest lies in understanding the applicability of the proposed approach to dense prediction tasks such as segmentation and image generation with diffusion models, given that the FocusTransformer seems to introduce token-level information loss.\n\n3. The section on speed evaluation is extensive, but it may benefit from further solidity. The reliance on the V100 GPU, which is considered somewhat outdated, raises questions in the context of contemporary Deep Neural Network (DNN) inference, where there is a preference for using lower precision formats like INT8 and FP16 with a TensorRT backend. Even though the proposed architecture is light in terms of FLOPs, I am concerned about the potential efficiency of the DeformableDETR-like FocusTransformer when integrated with TensorRT. It would be great if the authors could provide relevant results in this regard."
            },
            "questions": {
                "value": "Please respond to my questions and concerns in \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2346/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2346/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2346/Reviewer_fS4e"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701736880,
        "cdate": 1698701736880,
        "tmdate": 1700678459462,
        "mdate": 1700678459462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wx8yO1pqPi",
        "forum": "2pvECsmld3",
        "replyto": "2pvECsmld3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_SBDj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2346/Reviewer_SBDj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a sparse paradigm for visual recognition, and introduced a novel backbone named SparseFormer, which has a lower memory footprint and higher throughput compared to dense architectures, especially in the low-compute region.\nExperiments show that the proposed method achieves a low memory and time cost while maintaining high performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed SparseFormer is novel and solid.\n2. While maintain the performance, SparseFormer has a low memory footprint and high throughout.\n3. The experiments are solid."
            },
            "weaknesses": {
                "value": "None"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2346/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754538882,
        "cdate": 1698754538882,
        "tmdate": 1699636166681,
        "mdate": 1699636166681,
        "license": "CC BY 4.0",
        "version": 2
    }
]