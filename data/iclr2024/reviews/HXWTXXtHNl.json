[
    {
        "id": "PV87pIXz8w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_LRz9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_LRz9"
        ],
        "forum": "HXWTXXtHNl",
        "replyto": "HXWTXXtHNl",
        "content": {
            "summary": {
                "value": "This paper proposes a novel method, namely Transition-aware weighted Denoising Score Matching (TDSM), to train conditional diffusion models with noisy labels. The TDSM objective contains a weighted sum of score networks. Additionally, it also introduces a transition-aware weight estimator to leverage a time-dependent noisy-label classifier distinctively customized to the diffusion process. Experimental results on multiple popular datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written with clear method procedure.\n2. The idea is clear and effective.\n3. This paper have good experimental results."
            },
            "weaknesses": {
                "value": "1. It is not clear what is the major difference from the methods that boosting the robustness of generative models on noisy labels.\n2. The significance of this research topic is not clear, please explain it. Specifically, to the best of my knowledge, generative models are usually unsupervised, and thus there is only a few methods on boosting the model robustness against on noisy labels.\n3. It is not clear the model performance on severe noisy labels, like 60%, 80%.\n4. It is not clear whether the proposed method can boost the model classification performance?"
            },
            "questions": {
                "value": "1. Why boosting the robustness of diffusion models on noisy labels is very significant?\n2. Can the proposed method boost the model classification performance on noisy labels? \n3. What the limitations of the proposed method and please point out the future work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4864/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697467650905,
        "cdate": 1697467650905,
        "tmdate": 1699636470260,
        "mdate": 1699636470260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jEXKzGdvwi",
        "forum": "HXWTXXtHNl",
        "replyto": "HXWTXXtHNl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_TMN7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_TMN7"
        ],
        "content": {
            "summary": {
                "value": "This paper highlights the challenges associated with training on extensive datasets, which often contain noise in their condition to make noisy labels. Such noise introduces the risk of condition mismatches, which can degrade the quality of the generated data. To tackle this issue, the paper presents the Transition-aware weighted Denoising Score Matching (TDSM) method. This approach is specifically designed to robustly train conditional diffusion models with noisy labels.\n\nThe TDSM framework incorporates a label-transition weight for the score networks. These weights are derived from the relationship between conditional scores for both noisy and genuine labels, and can be estimated with a pre-trained noisy-classifier. Empirical evaluations, on multiple datasets and a range of noisy label configurations, demonstrate the efficiency of the TDSM approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and polished, facilitating a smooth reading experience. The mathematical presentations are articulated clearly and the theoretical results are complete and sound. Additionally, the inclusion of model overviews and illustrative figures for the components simplifies the understanding of the proposed method. \n\n- The experimental results well validate the approach. Notably, the paper comprehensively study both the effects of the conditional models and the impact of conditional generation with label guidance. Comprehensive results are provided in both the main paper and supplementary material.\n\n- The practical side of the research is solid. The authors have been very detailed in their implementation and provided their experiment code, which ensures reproducibility."
            },
            "weaknesses": {
                "value": "- Given that the estimation of the transition-aware weight relies on a noisy-classifier, it would be advantageous for the authors to present studies evaluating how the performance of the noisy-classifier affects the model's overall performance.\n\n- In Table 2, the authors seem to only compare with DSM with non-class-aware evaluation metrics. What's the reason for this comparison with the specific metrics?\n\n- On \"clean\" datasets, TDSM demonstrates notably superior performance, suggesting the potential presence of noisy labels. It would be insightful to know the threshold or proportion of noisy labels at which a significant performance difference emerges between DSM and TDSM. Furthermore, if the datasets used to train the noisy-classifier (for estimating class transitions) contain noisy labels, would this introduce additional inaccuracies in label correction? It would be beneficial for the authors to conduct a thorough analysis of these concerns.\n\n- In the review of diffusion models, the authors seem to only review from the score matching networks, while omit the diffusion models derivated from optmizing the ELBO.\n\n\n- Previous works are not correctly reviewed or cited. For example, the reference of denoising diffusion probabilistic model (Ho et al., 2020) is classified into video generation, in the introduction, while this is a fundamental work in diffusion models, and the authors may wanted to put video diffusion models (Ho et al., 2022) there. Moreover, some prior works that tackle the uncurated label distributions are not discussed and compared in the paper. \n\n- It would be beneficial to include a color bar in Fig 8 to interprete the meaning of the colors presented.\n\n\n*Reference*:\n\n*Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in neural information processing systems 33 (2020): 6840-6851.*\n\n*Rangwani, Harsh, Konda Reddy Mopuri, and R. Venkatesh Babu. \"Class balancing gan with a classifier in the loop.\" Uncertainty in Artificial Intelligence. PMLR, 2021.*\n\n*Rangwani, Harsh, et al. \"Improving GANs for Long-Tailed Data Through Group Spectral Regularization.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.*\n\n*Ho, Jonathan et al. \u201cVideo Diffusion Models.\u201d ArXiv abs/2204.03458 (2022).*\n\n*Qin, Yiming, et al. \"Class-Balancing Diffusion Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.*"
            },
            "questions": {
                "value": "Please see the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4864/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648556467,
        "cdate": 1698648556467,
        "tmdate": 1699636470184,
        "mdate": 1699636470184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bw59POGCWn",
        "forum": "HXWTXXtHNl",
        "replyto": "HXWTXXtHNl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_Zgz7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_Zgz7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new label-noise robust method for training conditional diffusion models. This is achieved by making use of an estimated transition relation from noisy labels to clean labels. Some theoretical analyses have also been proposed under the class-dependent label-noise setting. Experiments across various datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Targeting an Important Problem:** The paper aims to handle noisy labels in large-scale datasets used for training diffusion models. Addressing this problem is crucial as it is a common and practical challenge in the deployment of these models in real-world scenarios.\n2. **A New Approach to Noisy Labels in Diffusion Models:** The paper introduces a new methodology to address noisy labels in conditional diffusion models, a topic not extensively covered in existing literature.\n3. **Clarity and Accessibility:** The introduction and overall presentation of the paper are clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. **Missing Citations:** The paper could be significantly enhanced by including additional relevant literature. Learning the transition relation from noisy labels to clean labels has been previously explored under the umbrella of mixture proportion estimation. Moreover, methods for estimating the transition matrix in class-dependent settings are also very related. Acknowledging popular works could provide a richer theoretical foundation.\n2. **Missing Baselines:** There are many methods for learning with noisy labels. It would be beneficial to combine existing state-of-the-art (SOTA) methods for learning with noisy labels to obtain estimated clean labels. Then, utilizing conditional diffusion models on these cleaned labels and comparing the performance with the author's method could offer a more comprehensive evaluation of the proposed method\u2019s efficacy compared to current alternatives.\n3. **Unclear Advantage:** There is a need for a more detailed explanation regarding the advantage of the proposed method over existing methods, especially in recovering clean labels. Since the performance of diffusion models in noisy label scenarios hinges significantly on the accuracy of label recovery, explaining how the method enhances this aspect compared to others would greatly benefit readers in understanding the true potential and innovation of your approach.\n4. **Limited Application:** The focus on class-dependent noise settings might limit the broader applicability of the proposed method. The assumption of class-dependent noise could be strong and not verifiable in practical scenarios. It would be insightful if the paper could discuss the potential implications and limitations of this assumption, including how it might affect the generalizability of the proposed method to other noise settings or real-world applications where such assumptions may not be easily verified."
            },
            "questions": {
                "value": "1. Could the authors elaborate on how their method of learning the transition relation from noisy labels to clean labels differs from or aligns with the existing literature in mixture proportion estimation?\n2. Could the authors provide further details on how their method more effectively recovers clean labels compared to existing state-of-the-art methods VolMinNet (ICML 21), InstanceGM (WACV23), and DivideMix (ICLR20)?\n3. Would the authors consider adding additional experiments to demonstrate the performance when combining the existing state-of-the-art methods for learning with noisy labels with a conditional diffusion model? Additionally, could they compare the accuracy of clean label recovery with these methods?\n4. Could the authors explain which specific mechanisms or features in their approach contribute to improved label recovery?\n5. How do the authors verify the presence of class-dependent noise in practical applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4864/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754610120,
        "cdate": 1698754610120,
        "tmdate": 1699636470083,
        "mdate": 1699636470083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ELc61olWVH",
        "forum": "HXWTXXtHNl",
        "replyto": "HXWTXXtHNl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_BRfo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4864/Reviewer_BRfo"
        ],
        "content": {
            "summary": {
                "value": "the authors find the noisy-label conditional score can be expressed as a convex combination of the clean- label conditional scores with some coefficients\n, accordingly they propose a weighted loss function to address the problem of noisy labels in class-conditional diffusion models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well organized and the proofs are detailed.\n2.\tThis paper is the first work to consider the influence of noisy label condition to the generation performance in diffusion models"
            },
            "weaknesses": {
                "value": "The meaning of this work is limited. The proposed method is tailored for diffusion models which are conditioned on class, but most existing diffusion models are conditioned on text or other modalities, and the class label also can be expressed by language. In addition, noisy label datasets are not common in diffusion model."
            },
            "questions": {
                "value": "Please refer to my comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4864/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853850528,
        "cdate": 1698853850528,
        "tmdate": 1699636470013,
        "mdate": 1699636470013,
        "license": "CC BY 4.0",
        "version": 2
    }
]