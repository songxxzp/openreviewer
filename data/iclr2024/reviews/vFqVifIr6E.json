[
    {
        "id": "iYoHJDNYig",
        "forum": "vFqVifIr6E",
        "replyto": "vFqVifIr6E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_72Kp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_72Kp"
        ],
        "content": {
            "summary": {
                "value": "This work tackles the problem of few-shot learning and proposes a method to adapt a model employing the MAML framework. Specifically, the model is updated to perform on a specific task inferred from the alignment loss between text and image embeddings. The metric module is introduced to extend the cosine similarity between two modalities. The experiments show that the proposed method can outperform SOTAs in few-shot learning on several benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proves the efficacy of the proposed method by beating SOTAs on several benchmarks e.g., mini-Imagenet and CUB datasets. \n- Ablation study is provided to understand the importance of the metric module."
            },
            "weaknesses": {
                "value": "- The novelty of this work is limited as the off-the-shelf text encoder has been proposed previously using a similar contrastive loss for few-shot learning. The idea has overlapping to the idea proposed in VS-alignment (Afham et al., 2022) with a marginal extension in the meta-learning technique with MAML.  Some discussion (head-to-head) on the proposed method and Afham et al. would be beneficial for the readers to spot the difference and novelty of the work. \n- The citation to Meta learning paradigm in Page 3 is not precise. Vinyals et al., 2016  do not discuss about meta-learning but the work is more related to learn in a few data regime. MAML paper would be a more relevant citation in this part.\n- The manuscript is not well written. Equation (3?) in Page 6 is not precisely correct as the gradient descent should be performed w.r.t. I, T, and M. Please check the expression after $\\nabla$. Also, the equation in Page 6 has no number. Please fix this in the revised version.\n- Regarding this sentence in Page 2: \u201cSecondly, in contrast to vision-language pre-trained models where both visual and textual encoders are learnable to align embeddings, we utilize frozen public textual encoders. This leads to totally different structures of textual embedding spaces and thus makes the alignment between visual and textual features difficult,\u201d One concern is that why we cannot use the image encoder from a pretrained image encoder (e.g., CLIP image encoder)? Are there any settings that do not allow this condition? If this was feasible, would this work still consider MAML with several gradient steps to obtain optimal performance?\n- This work must consider one important work [1] in this direction.  This work is the pioneer in using the off-the-shelf pretrained language model for few-shot learning but there is no discussion and citation to this work.\n- The experiments are quite limited as most of the comparisons only involve image data without text information. It would be better to provide some other datasets e.g., COCO. Some other experiments as in [1] might be considered to show the efficacy of the proposed method and comparison with [1] on different modes (e.g., frozen and not frozen).\n\n[1] Tsimpoukelli et al., \"Multimodal Few-Shot Learning with Frozen Language Models,\" Neurips, 2021."
            },
            "questions": {
                "value": "Please see the weaknesses, especially the question about the image encoder."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5523/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5523/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5523/Reviewer_72Kp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707939607,
        "cdate": 1698707939607,
        "tmdate": 1699636566047,
        "mdate": 1699636566047,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rIydjO13SF",
        "forum": "vFqVifIr6E",
        "replyto": "vFqVifIr6E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_rhJx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_rhJx"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses few-shot image classification which is tasked to learn a classifier on new classes with only a few samples. This paper proposes to leverage class-level text-embeddings and introduces a metric module to align text and image embeddings. The text-embeddings in the experiments include language models, word embedding and CLIP text embeddings. The method follows the training and optimization framework of MAML, a popular meta-learning method. The experiments are conducted on the widely used few-shot learning benchmarks including miniImageNet, tieredImageNet and CUB. The results show better results than the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The paper is written well and easy to understand.\n\n-The proposed method is simple. It is intuitively a good idea to leverage the semantic information for few-shot image classification."
            },
            "weaknesses": {
                "value": "-Overclaim the technical novelty. The framework of learning image and text alignment with a bilinear function has been extensively studied in zero-shot learning.  Some zero-shot learning methods (e.g., Xu et al.) even show that the framework generalizes well to few-shot learning setting. Although this paper adopts a different few-shot learning setting with episodic evaluation protocol, in principle, the core method is rather similar. In my view, this work seems to be a trivial combination of MAML and previous zero-shot learning methods, which can not be counted as a significant technical contribution. This paper seems to ignore this point and fails to discuss how it improves the zero-shot learning methods. \n\n[A] Xu et al., Attribute Prototype Network for Any-Shot Learning. IJCV 2022. \n\n-Lack of insights. Although the proposed method achieves SOTA on some dataset, it mainly relies on the CLIP text embeddings, which is somewhat expected because it is known that CLIP embeddings can achieve impressive image classification results. The results using word embeddings and language embeddings are actually worse than some baseline methods. There are not much insights except the comparison between three different text embeddings."
            },
            "questions": {
                "value": "In Sec. 4.1, it says \"Besides, we make the prompt templates learnable to avoid time-consuming prompt engineering following.\" But I did not find any technical detail about this part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792625129,
        "cdate": 1698792625129,
        "tmdate": 1699636565942,
        "mdate": 1699636565942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rgi9ZBZtRG",
        "forum": "vFqVifIr6E",
        "replyto": "vFqVifIr6E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_Pbba"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_Pbba"
        ],
        "content": {
            "summary": {
                "value": "This paper is about few shot image classification. It aims at improving it by exploiting semantic information that takes the form of textual embedding obtained using existing textual encoders. As this textual information is not directly aligned with the visual one, the authors propose to align them by metric learning and in particular contrastive learning widely used in self-supervised representation learning. This training procedure follows the metal-learning approach and in particular MAML. Experimental validation and comparison to the state-of-the-art are done on two few shot benchmarks (miniImagenet and tieredImageNet) and the CUB-200-2011 dataset for fine-grained recognition. Their approaches improve the performances on these benchmarks. An ablation study on the metric learning part is provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper tackles an important issue in few-shot learning, i.e. improving existing approaches by injecting additional information and in particular semantic textual information. Indeed, this line of work has been largely studied recently and has shown promising results. \n+ The paper is well-written with clear objectives and motivations.\n+ The experimental part implies a comparison with various inductive state-of-the-art approaches and the obtained results show an improvment in terms of performance on different 5 ways x shot settings.\n+ The paper also contains a small ablation study on the metric learning part."
            },
            "weaknesses": {
                "value": "+ A first concern is about the experimental study which is incomplete from my point of view on several aspects :\n  +  First, it is usual to study the dependence on the visual backbone. Only Resnets-12  is used in the paper but other backbones such as visual transformers backbone or recent foundation models could be included.\n  + The influence of the prompting strategy could also be experimented with in more detail. The appendix provides an analysis of the learnable prompt template but it could have been interesting to correlate this prompting strategy to a more formalized definition of the type of semantic information that should be carried.\n  + Some technical details are missing. In particular, it is well known that contrastive learning is highly dependent on the negative sampling strategy but also on the size of the batch. This information is missing in the paper.\n + Some benchmarks have been provided in the Few Shot community to better take into account the semantics. See for instance [meta-dataset](https://github.com/google-research/meta-dataset) or the work presented [here](https://arxiv.org/abs/2205.05155). What is the behavior of the approach on these benchmarks?\n\n+ Contrastive learning has been studied in the context of multimodal data. The positioning to these works could be interesting. See for instance [this paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf). \n\n+ Another concern is about the novelty of the proposed approach compared to (Chen et al, 2023). Compared to this work, the authors propose to add the metric module but this latter is also not new. I would appreciate it if the authors argued more on the novelty of this part, maybe in relation to the few shot settings and the way to tackle support and query sets."
            },
            "questions": {
                "value": "+ Recent works have shown the prevalence of transductive few-shot learning in image classification with this transductive setting that outperforms inductive approaches. How is the proposed approach compared to transductive sota approaches? How adapting the proposed approach to this scheme?\n+ It is possible to better formalize the notion of semantic information? What kind of information should be added? \n+ The meta-learning paradigm has been discussed a lot in the few-shot community. See for instance [this paper](https://arxiv.org/abs/2003.11539). A discussion on this point should be added in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833712386,
        "cdate": 1698833712386,
        "tmdate": 1699636565849,
        "mdate": 1699636565849,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N2tiGPb0XU",
        "forum": "vFqVifIr6E",
        "replyto": "vFqVifIr6E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_sXdY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5523/Reviewer_sXdY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel few-shot learning framework for image classification that leverages semantic information extracted by a public textual encoder based on contrastive learning. The proposed approach addresses the challenge of alignment between visual features and textual embeddings obtained from public textual encoders and introduces a metric module to generalize the similarity measure. The metric module is designed to be adaptive to different few-shot tasks for better transferability, and MAML is adopted to train the model via bi-level optimization. The paper demonstrates the effectiveness of the proposed method through extensive experiments on multiple benchmarks with different domains. The main contributions of the paper are the proposed few-shot learning framework, the carefully designed textual branch of the framework, the metric module for generalizing the similarity measure, and the demonstration of the effectiveness of the proposed method through extensive experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper has a clear and well-organized presentation of the proposed method, and the authors provide comprehensive experiments.\n\n+ The proposed method aims to bridge the gap between visual and textual modalities, which is a good direction towards few-shot learning.\n\n+ The visualization (sec. 5.4) looks good."
            },
            "weaknesses": {
                "value": "- There is a lack of comparison with other state-of-the-art approaches, such as TRIDENT [1], BAVARDAGE [2], PEMnE-BMS [3], etc, though these methods may not rely on pretrained vision-language models. Besides, the authors miss some references about referring to category names for few-shot learning [4]. \n\n- In the bi-level optimization, the metric module is updated in the inner loop and then all parameters are updated later in the outer loop. Could the authors provide any reasons or motivation for this training strategy?\n\n- This paper aims to minimize the gap between the visual and textual modalities by introducing the metric module. It adds degrees of freedom so that there are some learnable parameters for bridging the gap. However, vision-language models are capable of zero-shot inference, therefore the way initializing the \"bridge\" between two modalities is crucial to the performance. Have the authors considered leveraging category name embedding for initialization [4]? By doing so, two modalities will be aligned automatically. \n\n\n\n[1] Transductive Decoupled Variational Inference for Few-Shot Classification\n[2] Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classification\n[3] Squeezing Backbone Feature Distributions to the Max for Efficient Few-Shot Learning\n[4] Exploiting Category Names for Few-Shot Classification with Vision-Language Models"
            },
            "questions": {
                "value": "I expect to hear back from the authors regarding the below questions and concerns.\n\n1. Comparison regarding other state-of-the-art few-shot approaches with established results on CUB or Mini-Imagenet.\n2. Explanation of the motivation or insights of the training strategy, especially why only the metric module is updated in the inner loop and all parameters are updated in the outer loop.\n3. Discussion about the category name initialization and possible comparison or further experiments by adding that strategy to existing approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699057770658,
        "cdate": 1699057770658,
        "tmdate": 1699636565748,
        "mdate": 1699636565748,
        "license": "CC BY 4.0",
        "version": 2
    }
]