[
    {
        "id": "pxlukMmsOr",
        "forum": "D6aGz0Zyvn",
        "replyto": "D6aGz0Zyvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
        ],
        "content": {
            "summary": {
                "value": "The algorithm introduces a variant of the centered-based kernel method, they call the centers \"support vectors\". The idea is that the model's size remains smaller than the entire dataset, similar to  FALKON and EigenPro3.0. In contrast to methods that utilize fixed model support vectors, their algorithm adaptively adjusts these support vectors throughout the training process. Moreover, utilizing concepts from the asymmetric kernel method, they adaptively fit the support vectors with varying bandwidths throughout the training process.They show that their algorithm outperform some existing methods on several data sets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. the most interesting part is the idea of adaptively change the support vectors in an iterative manner. This was something novel worth exploring more.\n2. The idea of adaptively adjust the bandwidth and mixing it with asymmetric kernel methods seems intriguing.(Not sure how useful)\n3. The paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "Main concern:\n1. The most important caveats is that the paper has only compared to vanilla KRR methods. It is not surprising that they got a slightly better performance compare for example to FALKON. I'm not at all convened this methods is better than well developed techniques such as:\n\ni. traditional Automatic Relevance Determination(ARD) known in GP community, it is implemented with Gpytorch see here:https://docs.gpytorch.ai/en/stable/kernels.html. or see section 5 of this https://gaussianprocess.org/gpml/chapters/RW.pdf\n\nii. EigenPro3.0, see https://arxiv.org/abs/2302.02605 \n\niii. Recursive Feature machines (RFMs), see: https://arxiv.org/abs/2212.13881\n\nScalability:\n\n2. the authors claim that this method is scalable and they provide table 3 to justify this. But those data sets are not at all large scale. The inverse problem can be done using direct calculation for those cases. The authors should try other data sets such as Taxi, CIFAR5m to justify consistency and scalability. (both in data and model size)\n3. It is mentioned in section 3 that the computation complexity is O(N_sv^3). This fundamentally shows this method on its own is not scalable. Eventually you need to scale the required support vectors(or model size) as it is discussed in https://arxiv.org/abs/2302.02605. \nHowever, I can see that this method combined by other methods like FALKON or EigenPro3.0  can potentially be scalable.\n4. How do you compute line 4 of the algorithm? Did you use FALKON or some other off the shelf algorithm or you did direct inverse? \n\n\nMinor issues:\n\n1. RBF kernel are known to be sensitive to bandwidth. While you have results for MKL, the performance of your method specifically for the Laplace kernel, which is relatively insensitive to bandwidth, remains ambiguous. Does outperforming MKL indicate superiority over merely using Laplace? The same concern applies to NTK kernels or other popular kernels.\n2. I suggest more explaining for asymmetric kernels methods. For example why the inverse even exist in equation 6. or you claimed \"this paper for the first time establishes an asymmetric KRR framework\", but how is it different from He et. al. paper? not clear.\n3. Please add what M means in the tables, helps with reading."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2653/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ",
                    "ICLR.cc/2024/Conference/Submission2653/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701875200,
        "cdate": 1698701875200,
        "tmdate": 1700463037990,
        "mdate": 1700463037990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CBGjaQ8OJi",
        "forum": "D6aGz0Zyvn",
        "replyto": "D6aGz0Zyvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new asymmetric kernel names Local-Adaptive-Bandwidth RBF kernel. To solve the asymmetry of the kernel, the paper establishes an asymmetric KRR framework. To learn the kernel parameter efficiently and accelerate computation. the paper devises a kernel learning algorithm. Experimental results show the algorithm\u2019s superiority."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper demonstrates a clear logical structure with a comprehensive framework. It tackles the complex relationship between bandwidth and data from the perspective of experimental results.\n2. The paper takes into account the impact of differences in implicit mappings on the results and proposes an interesting approach to non-symmetric kernel KRR framework.\n3. The paper introduces an algorithm based on dynamic strategies for parameter computation, which can effectively reduce the computational complexity associated with high-dimensional kernel matrices."
            },
            "weaknesses": {
                "value": "1. Intuitively, the relation between the mapping function's distinctiveness and the loss function, which means the coefficient of the last term in the KRR optimization objective may vary with datasets. \n2. The initial data selection for support data in the kernel learning algorithm proposed in the article seems to be too random. Moreover, inappropriate data selection appears to have a significant impact on the model."
            },
            "questions": {
                "value": "1. Is the final coefficient in the asymmetric KRR framework proposed in the article required to be 1/2? Can this be understood as simply for the convenience of computing stationary points? \n2. Is the small number of support vectors in the experimental results of the proposed method due to the algorithm's termination condition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2653/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2653/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828024453,
        "cdate": 1698828024453,
        "tmdate": 1699636205488,
        "mdate": 1699636205488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cHrPCu2nnl",
        "forum": "D6aGz0Zyvn",
        "replyto": "D6aGz0Zyvn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_gJwQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2653/Reviewer_gJwQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel approach to enhance the flexibility of kernel-based learning by introducing Locally-Adaptive-Bandwidth (LAB) kernels. Unlike traditional fixed kernels, LAB kernels incorporate data-dependent bandwidths, allowing for better adaptation to diverse data patterns. To address challenges related to asymmetry and learning efficiency, the paper introduces an asymmetric kernel ridge regression framework and an iterative kernel learning algorithm. Experimental results demonstrate the superior performance of the proposed algorithm compared to existing methods in handling large-scale datasets and achieving higher regression accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The introduction of LAB kernels with trainable bandwidths significantly improves the flexibility of kernel-based learning. By adapting bandwidths to individual data points, the model can better accommodate diverse data patterns, leading to more accurate representations.\n2. The paper establishes an asymmetric kernel ridge regression framework specifically designed for LAB kernels. Despite the asymmetry of the kernel matrix, the stationary points are elegantly represented as a linear combination of function evaluations at training data, enabling efficient learning and inference.\n3. The proposed algorithm allows for the estimation of bandwidths from the training data, reducing the demand for extensive support data. This data-driven approach enhances generalization ability by effectively tuning bandwidths based on the available training data.\n4. The proposed algorithm shows superior scalability in handling large-scale datasets compared to Nystr\u00f6m approximation-based algorithms. LAB kernels, with their adaptive bandwidths, offer a flexible and efficient solution for kernel-based learning tasks with extensive data."
            },
            "weaknesses": {
                "value": "1. While the paper presents empirical evidence of the superior performance of the proposed algorithm, it may lack strong theoretical guarantees or formal analysis of its convergence properties. Further theoretical investigations may be needed to fully understand the behavior and limitations of LAB kernels\n2. The performance of LAB kernels heavily relies on the accurate estimation of bandwidths. Selecting appropriate bandwidths for different data patterns can be a challenging task, and suboptimal choices may result in reduced performance or overfitting."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698984121715,
        "cdate": 1698984121715,
        "tmdate": 1699636205418,
        "mdate": 1699636205418,
        "license": "CC BY 4.0",
        "version": 2
    }
]