[
    {
        "id": "pO6Swc9Mev",
        "forum": "jolYuxpVn1",
        "replyto": "jolYuxpVn1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_wbTV"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a factuality evaluation framework comprising five steps: claim extraction, query generation, tool querying, evidence collection and agreement verification. The main novelty of the proposed framework allows the incorporation of external tools such as Google Search, Google Scholar, and code interpreters. It can be applied to multiple tasks (QA, math, code generation, scientific literature reviewing). The authors construct a dataset covering the four tasks mentioned earlier and compare the proposed framework against the baseline: Self-Check."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Reasons To Accept\n* The studied topic is important, and the proposed framework is a valuable contribution to the community\n* Extensive analysis covering multiple tasks, although some interesting content is in the appendix"
            },
            "weaknesses": {
                "value": "Although the described framework makes sense to me, the authors highly rely on ChatGPT to implement their framework (even if the function is just to check whether list A is a subset of list B); this may hinder other researchers from reproducing their results and additional costs."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731373735,
        "cdate": 1698731373735,
        "tmdate": 1699636324964,
        "mdate": 1699636324964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ETdkibDuHg",
        "forum": "jolYuxpVn1",
        "replyto": "jolYuxpVn1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_fmJJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_fmJJ"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a tool to detect factual errors in generated text which can be used across multiple domains. The authors claim that existing fact-checking methods are 1.) limited and task-specific 2.) use disjoint pieces of text as claims which might lose the contextual information especially in long form answers. They propose custom modularize pipelines for factuality error detection for knowledge based QA, arithmetic problem solving, language to code generation and scientific literature summarization. At a high level, they first use a claim extraction module to state claims, in a task-dependent manner. Then they use query generation to test the validity of these claims against retrieved evidences, calculator, interpreter etc. depending on each task. Finally they collate the results to evluate factuality of the LLM response."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.) The authors develop a one-stop factuality tool which can be useful for the community.\n2.) The authors have invested in extensive prompt development for claim extraction in various use cases.\n3.) They do an extensive analysis of their system on four tasks -  KBQA, code generation, scientific literature survey and arithmetic problems"
            },
            "weaknesses": {
                "value": "1.) The technical contributions are somewhat limited and mostly incremental.\n2.) The intermediate question generation module to validate any claims made by the LLM against an external evidence has been proposed in other works (https://arxiv.org/pdf/2210.08726.pdf, https://arxiv.org/pdf/2309.11495.pdf). The novelty as far as I understand is mostly in claim generation which is different for each tasks because the inputs are no longer just declarative statements but also things like code snippets, questions, etc.\n3.) The authors stretch the definition of factuality to new tasks like code generation which are not necessarily considered \"factual\". For instance, there can multiple ways to write a logical code snippet and not necessarily a universal way for it to be deemed as a fact.\n4.) The evaluations mostly focus on different language models and not existing methods for factual error detection like RARR or SELF-REFINE which could have been used for at least KBQA."
            },
            "questions": {
                "value": "1.) Could you please decsribe how the baseline method Self-Check(*) was setup? Does it use some existing code or were the prompts developed by the authors?\n2.) In question generation module for code generation task, how will the model know what are the right corner case inputs for unit testing the piece of code?\n3.) In Appendix C (Fig 6) some characters in the prompt are all-caps, was there any specific reason for that. Out of curiosity, does standard casing yield bad performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818049379,
        "cdate": 1698818049379,
        "tmdate": 1699636324888,
        "mdate": 1699636324888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "78b19KLsxn",
        "forum": "jolYuxpVn1",
        "replyto": "jolYuxpVn1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FacTool, a tool-augmented framework for detecting factual errors in texts generated by large language models (LLMs) like ChatGPT. The key ideas are:\n- Connects the concept of \"tool use\" by modern LLMs with factuality detection, using tools like search engines, code interpreters, etc to gather evidence and fact-checking for diverse tasks like QA, code, math, and scientific review.\n- Proposes a 5-step process: claim extraction, query generation, tool querying, evidence collection, agreement verification. Uses LLMs for claim extraction, query generation, and verifying claims.\n- Evaluate FACTOOL on 4 tasks - KB-QA, code, math, and scientific review. Shows it outperforms baselines like self-checking and supervised models on accuracy.\n- Uses FACTOOL to evaluate modern chatbots - finds GPT-4 has highest weighted accuracy, while supervised Vicuna-13B underperforms on math, code, and scientific review."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper intertwines the notion of \"tool use\" in modern LLMs with factuality detection. It showcases the versatile application of LLMs for auxiliary tasks, such as claim extraction and query generation, for fact-checking pipeline.\n2. An exhaustive evaluation was undertaken to assess the fact-checking capabilities of LLM responses across four distinct tasks and five datasets.\n3. The authors have made the codebase publicly available and introduced a ChatGPT plugin for enhanced accessibility."
            },
            "weaknesses": {
                "value": "1. The novelty of the approach appears limited. While the paper employs LLMs to generate search queries and refers to search engines as tools, this methodology isn't drastically different from prior evidence-based fact-checking methods that use search engines for evidence retrieval.\n2. The study lacks comparative baselines using non-LLM models or previous LLM-based system for evidence-based fact-checking. This omission makes it challenging to ascertain the specific advantages of incorporating LLMs into the fact-checking process, or referring to search engines as tools.\n3. Despite the paper's emphasis on fact-checking, notable fact-checking datasets like FEVER and SciFact are absent from the evaluation."
            },
            "questions": {
                "value": "1. In the \"Related Work\" section, could you provide a detailed comparison between your approach and the RARR presented by Gao et al. in 2022a? It would be beneficial to understand the distinctiveness of your work in relation to theirs.\n2. Were any experiments conducted using preivous LLMs or non-LLM-based fact-checking systems within the framework of your experimental settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3682/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3682/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3682/Reviewer_tg4t"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3682/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825100753,
        "cdate": 1698825100753,
        "tmdate": 1699636324817,
        "mdate": 1699636324817,
        "license": "CC BY 4.0",
        "version": 2
    }
]