[
    {
        "id": "ez6LPPZPOL",
        "forum": "zMPHKOmQNb",
        "replyto": "zMPHKOmQNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_BFxy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_BFxy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an interesting, novel approach to sampling new, diverse antibody sequences and validates the method's performance using real wet-lab experiments with real antibodies. They explore a couple variations on energy-based models with some interesting tricks-of-the-trade such as the pretrained denoiser to help guide the sampling. The experimental results are strong.\n\n\n==After author's response==\nI appreciate all of the details in response to my question. I continue to advocate for acceptance of this high-quality paper. My only final feedback is that the discussion of mixing should mention that mixing is not a concern for autoregressive models, since for them it's trivial to draw independent samples."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper does real wet-lab experiments! By that, I don't mean just that experiments were performed in a wet lab in a toy setting, but instead that the authors did real-world antibody design. \n\nThe paper provides an interesting/refreshing take on how to sample antibodies that is quite different from most papers' use of language models. It's drawing on a whole line of work on energy-based models with details that are both old-school and new-school. There will be many ML+Bio researchers that will be unfamiliar with this background material and will benefit from seeing this paper. \n\nI thought it was interesting that they included GPT3.5 as a baseline. Sometimes the results are surprisingly reasonable! As generalist foundation models get stronger, such a baseline seems important.\n\nThe authors have release code for their modeling, which is key because it is more niche than LM approaches for which there are standard packages."
            },
            "weaknesses": {
                "value": "Some of the comparisons to autoregressive language models (LMs) were a bit under-developed. For example, the paper mentions fast mixing sampling, but this is trivial for LMs. Second, LMs provide a natural way to condition on metadata; how would the current model do that? Finally, claims that LM sampling is expensive in terms of compute aren't that compelling, since for these antibody design applications the wet-lab experiments are orders of magnitude more expensive than the compute to generate the library.\n\n\nThe 'distributional conformity scores' (DCS) section was interesting and cool, but not central to the paper and felt like a way to add extra math into the paper. Please clarify: was this used to filter any of the samples for the wet-lab experiments or was it only used to provide the rightmost column in Table 2? Given that your proposed does not have strong DCS scores compared to some other baselines, how should I be interpreting these results? Overall, I would recommend removing the DCS content and allocating more space in the main paper to details/derivations of the model fitting/sampling."
            },
            "questions": {
                "value": "The idea of relaxing the discrete problem to a continuous modeling problem on one-hots is interesting. I was surprised, however, that you treated them as one-hots instead of elements of a probability simplex. Have you explored this? A key advantage is that you could sample from the probabilities rather than taking the argmax in the final step.\n\nOne of my concerns for using your method vs. LMs is that there are a number of difficult-to-set hyperparameters. Can you discuss the sensitivity to these choices? How did you validate them?\n\nThe DCS seems like a good way to filter samples. Is there a way that you could evaluate the impact of such filtering in-silico?\n\nThe DCS is a one-vs-rest comparison, using a single query vs. a reference set. How could you extend this to generate a *diverse* set of samples?\n\nThe paper claims informally that the sampling mixes quickly. Is there a way to demonstrate this quantitatively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Reviewer_BFxy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698350466021,
        "cdate": 1698350466021,
        "tmdate": 1700509792065,
        "mdate": 1700509792065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1GxLzdwnSR",
        "forum": "zMPHKOmQNb",
        "replyto": "zMPHKOmQNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_RGZR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_RGZR"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new approach for modeling discrete data distributions called Smoothed Discrete Sampling (SDS), founded on the neural empirical Bayes framework. They detail the discrete Walk-Jump sampling algorithm (dWJS), which facilitates rapid, non-autoregressive sampling and can handle variable-length discrete outputs, built upon a distinct architecture for discrete EBMs. This approach facilitates the training of score-based models for discrete data, requires just a single noise level and eliminates the need for a noise schedule. As a result, the pitfalls of diffusion models, such as brittleness, training instabilities, and sluggish sampling, are sidestepped. Furthermore, the introduced method simplifies the training of EBMs by bypassing several commonly used EBM training techniques, ensuring both swift sampling and high-quality samples. The authors evaluate their method for ab initio protein discovery and design and compares against several diffusion and LLM-based models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The current approach builds upon foundational principles of EBMs and NEB but introduces unique methodologies for handling discrete data, formulating decoupled modeling, and tackling challenges in antibody sequence generation. In particular the study of NEB (Neural Empirical Bayes) for discrete data seems to be unique. \n\n- The current paper distinguishes its approach from discrete diffusion models like those by Austin et al. (2023), who learn an iterative denoising process over different noise levels. Although generative modeling has been previously applied to antibodies (cited works include Shuai et al., Gligorijevi\u0107 et al., Ferruz & H\u00f6cker, and Tagasovska et al.), the present work highlights the unique challenges faced due to limited training data and the complexities of antibody sequences. The paper suggests that typical natural-language-based methods might struggle in this domain, pointing towards a differentiation from those methods. The current work introduces a novel formulation of decoupled energy- and score-based modeling to address the challenges in training and sampling discrete sequences, which seems distinct from previously mentioned methods.\n\n- A new distributional conformity score (DCS) is proposed, which is useful for evaluating generated samples in comparison to a reference set. DCS provides a measure of joint distribution alignment, enabling it to capture relationships among properties rather than just aligning individual properties. This could mean that DCS provides a more comprehensive assessment of how well a generative model captures the nuances and interconnectedness of the properties within the data."
            },
            "weaknesses": {
                "value": "- While the paper touts the simplification to a single hyperparameter choice (noise level, \u03c3) as a strength, it might also be seen as a limitation since the entire model's performance could be sensitive to this single parameter. \n\n- Set of properties includes both continuous, binary, and discrete values and estimating the distribution by a kernel density approach may not be very effective. Also DCS may be overly influenced by outliers and extreme data points in the dataset."
            },
            "questions": {
                "value": "- Is the same value of \u03c3=0.5 used for all reported experiments? It would be nice to see what effect lowering or increasing this value will have on some of the reported metrics for a better assessment of the sensitivity of the approach to this parameter.\n- How is d obtained in this sentence? \"... for the flattened sparse one-hot matrices with vocabulary size 21, d = 6237 ...\" Not sure why d is so high if d=L.\n- Please define the FID score (Fr\u00e9chet inception distance (FID)) as well as the BLEU and add references.\n\n\nI thank the authors for addressing my comments and running additional experiments to demonstrate the effect of sigma. The rebuttal addresses my concern about the DCS score. I increased my score one notch."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6610/Reviewer_RGZR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691862542,
        "cdate": 1698691862542,
        "tmdate": 1700756542531,
        "mdate": 1700756542531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5B2FVxxxKv",
        "forum": "zMPHKOmQNb",
        "replyto": "zMPHKOmQNb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_aiU9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6610/Reviewer_aiU9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new method called discrete Walk-Jump Sampling (dWJS) for generative modeling and sampling of discrete protein sequences. The key ideas are as follows. The discrete data distribution is smoothed by adding Gaussian noise, which makes it easier to model and sample from. A discrete energy-based model (dEBM) is used to learn the distribution of noisy protein sequences and is trained via contrastive divergence. A denoising model implemented as a ByteNet is separately trained to map the noisy sequences back to the original discrete space. Sampling is performed by first using Langevin MCMC to sample noisy sequences from the dEBM. Then the denoising model is used to map these noisy samples back to valid discrete protein sequences. The walk (MCMC sampling) and jump (denoising) steps are decoupled, which provides flexibility. The authors argue this provides benefits over autoregressive models, diffusion models, and traditional EBM training. The proposed method is shown to be effective on antibody protein sequence modeling and design tasks using both in silico and in vitro experiments, \n\nOverall, I believe this is a novel contribution which demonstrates promising results on an important and challenging problem. The proposed dWJS approach is simple yet effective, providing a robust alternative to existing generative models of proteins. With additional analysis and experimental validation, this could become a leading technique for antibody design and beyond."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(+) The proposed method is intuitive and technically sound. Decoupling the sampling and denoising steps is an elegant idea.\n\n(+)Thorough in silico evaluation using antibody-specific metrics, uniqueness, diversity, etc. In particular, a distributional conformity score  was introduced to evaluate the quality of generated protein sequences compared to a reference distribution. Extensive comparison to strong baselines.\n\n(+) Impressive wet lab validation demonstrating high expression yields and binding rates."
            },
            "weaknesses": {
                "value": "(-) Only a single task (antibody design) is evaluated. Testing on other protein classes or discrete domains would be useful.\n\n(-) The distributional conformity score for evaluation is introduced late with little detail. More motivation and analysis would improve clarity.\nCertain details are unclear, like how sequences are aligned and handled."
            },
            "questions": {
                "value": "Your proposed approach operates purely at the sequence level, focusing specifically on antibody sequences. What are your thoughts on the relative pros and cons of sequence-only approaches compared to structure-aware approaches that leverage protein 3D structure information (e.g. inverse folding methods)? Could structure information, either from experiments or structure prediction, be incorporated to potentially improve the performance of your model? For example, might recent powerful inverse folding techniques, where they can be viewed as structure-conditional sequence generative models, like ProteinMPNN [1], ESM-IF [2], PiFold [3], or LM-Design [4] be combined with dWJS to create even more advanced antibody design frameworks? I'm curious to hear your perspective on the value of adding structural awareness and how feasible it would be to integrate with your dWJS approach.\n\n---\n[1] Dauparas, et al. Robust deep learning\u2013based protein sequence design using ProteinMPNN. Science 2022.  \n\n[2] Hsu, et al. Learning inverse folding from millions of predicted structures. In ICML 2022\n\n[3] Gao, et al. PiFold: Toward effective and efficient protein inverse folding. ICLR 2023.\n\n[4] Zheng, et al. Structure-informed Language Models Are Protein Designers. ICML 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699115229185,
        "cdate": 1699115229185,
        "tmdate": 1699636753749,
        "mdate": 1699636753749,
        "license": "CC BY 4.0",
        "version": 2
    }
]