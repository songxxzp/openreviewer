[
    {
        "id": "aT6h4JKWIk",
        "forum": "3zQo5oUvia",
        "replyto": "3zQo5oUvia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_Hx4L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_Hx4L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a time-series contrastive learning framework that replaces the data augmentation module with a retrieval-based pair construction strategy. The idea sounds interesting and is proved to be effective on three time-series datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work proposes a retrieval-based mask reconstruction strategy to help the model identify similar time series, which I think is a smart design.\n2. The authors show that using contiguous and intermittent masks during the training and evaluation respectively leads to the best performance. Such a result could bring some new insights to the time-series learning community.\n3. By constructing contrastive pairs retrieval, the proposed method does not rely on data augmentations, which could harm the pattern of signals, to perform contrastive learning. Experiments on three datasets demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Figure 4 shows that the diagonal pattern is worse on the PPG and ECG data compared with that on the HAR data. Some explanations need to be provided here to help readers understand the potential limitations of the method.\n2. During the contrastive learning stage, the positive counterpart is selected as the one most similar to the anchor. However, it is possible that there is more than one candidate that shares the same class label with the anchor. Would such false negative pairs influence the performance of contrastive learning? Have you tried other positive selecting strategies such as hard threshold?\n3. Typo: we uniformly at random sample -> we uniformly random sample"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697792595597,
        "cdate": 1697792595597,
        "tmdate": 1699636913522,
        "mdate": 1699636913522,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P02y0JRdJG",
        "forum": "3zQo5oUvia",
        "replyto": "3zQo5oUvia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_mFnV"
        ],
        "content": {
            "summary": {
                "value": "This article proposes a new perspective for determining positive and negative samples in time series contrastive learning. If one subsequence can be successfully reconstructed by retrieving information from another, it should form a positive pair. Based on this, the author trains a cross-attention module to reconstruct the masked query input subsequence from the key input subsequence. The subsequence with the lowest reconstruction error is labelled as positive, and the others are labelled as negative. Experiments show that the REBAR method of this article achieves state-of-the-art results in learning a class-discriminative embedding space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method proposed in this article is intuitive and easy to understand."
            },
            "weaknesses": {
                "value": "1. Format issue: All formulas in this article are not numbered. The notations in the first formula on page 5 have no corresponding definition.\n\n2. The experimental volume of this paper is insufficient. As a new perspective in the field of time series contrastive learning, the author should validate the method on a wider dataset to demonstrate its universality for time series.\n\n3. The article lacks discussion and analysis of key parameters. Specifically, when training the cross-attention module, what impact will the length of subsequences and the proportion of random masking have on the reconstruction effect of key subsequences? Is the reconstruction effect of the cross-attention module directly related to downstream task performance? How to set the number of candidate subsequences (proportion of positive and negative samples) when obtaining Pos/Neg labels?"
            },
            "questions": {
                "value": "I noticed that when applying the REBAR metric in contrastive learning, an anchor sequence and n candidate sequences are sampled randomly. Only the candidate sequence with the smallest reconstruction loss will be determined as a positive sample of the anchor sequence. Is there a situation where, for example, when sequence A is used as an anchor sequence, the candidate sequence with the smallest reconstruction loss is sequence B, and then A and B are mutually positive samples? However, when B is used as an anchor, the candidate sequence with the smallest reconstruction loss may be another sequence C. Therefore, B and C are positive samples. But if A is also in the candidate sequences, this method will divide A into negative samples of B. This leads to conflicting conclusions when the anchor sequence is different."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676471740,
        "cdate": 1698676471740,
        "tmdate": 1699636913389,
        "mdate": 1699636913389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vpypeO49Lx",
        "forum": "3zQo5oUvia",
        "replyto": "3zQo5oUvia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for constructing positive pairs for contrastive learning in time-series data. It presents experiments across three datasets to validate the approach.\n\n--- post rebuttal ---\n\nI appreciate the efforts made by the authors in addressing the concerns raised in my initial review. The manuscript has undergone significant changes, resulting in notable improvements in its quality. Considering these enhancements, I have revised my score from 3 to 6."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is motivated. The proposed method is grounded on a cogent hypothesis *\"if one time-series is useful for reconstructing another, then they likely share reconstruction information and thus should be drawn together within the embedding space as positive examples.\"*. Essentially, the author posits that time-series with similar semantics are capable of aiding in each other's reconstruction.\n\n2. An intriguing observation made in the paper is the difference in sparsity between cross-attention mechanisms when trained with *contiguous masks* versus *intermittent masks.* \n\n3. The author provides a comprehensive comparison, including many relevant baselines."
            },
            "weaknesses": {
                "value": "1. The rationale for preferring a contiguous mask over an intermittent mask is presented but could be articulated with greater clarity to enhance its persuasiveness. Additionally, there seems to be some confusion regarding Figure 1. Clarification is needed as to whether the author implies that a) the contiguous mask is utilized during the training of REBAR, and b) the intermittent mask is employed when applying REBAR in contrastive learning. If this is the case, the reasons for using different masks in these contexts should be explicitly stated.\n\n2. The experimental scale appears somewhat limited. The paper does not specify the exact number of samples within the datasets, which seem to be on the smaller side. This limitation is accentuated when compared to previous works, such as TS2VEC [1], which utilized a much larger array of datasets, including 125 from the UCR archive and 29 from the UEA archive.\n\n3. The explanation of results requires expansion. For instance, the acronyms ARI and NMI in Table 2 are not defined within the context of the paper, leaving their significance unclear. Moreover, there is a notable difference in the results reported for the TNC on the HAR dataset between the original TNC paper [2] and this manuscript. In the origianl paper, it was reported AUPRC 0.94, Accuracy 88 while in this manuscript, it is reported AUPRC 0.98 and Accuracy 94. More information about the potential factors leading to these discrepancies would be beneficial for the reader's comprehension.\n\n\n* [1] Yue, Zhihan, et al. \"Ts2vec: Towards universal representation of time series.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.\n\n* [2] Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. \"Unsupervised representation learning for time series with temporal neighborhood coding.\" arXiv preprint arXiv:2106.00750 (2021)."
            },
            "questions": {
                "value": "In Table 3, the author evaluates the influence of different mask types on performance. It would be beneficial to clarify why the training stage favors a contiguous mask, while the evaluation stage shows a preference for an intermittent mask."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7550/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7550/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7550/Reviewer_jiyG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699233216486,
        "cdate": 1699233216486,
        "tmdate": 1700689880106,
        "mdate": 1700689880106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZoVJoHrfgv",
        "forum": "3zQo5oUvia",
        "replyto": "3zQo5oUvia",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_gCTH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7550/Reviewer_gCTH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach called Retrieval-Based Reconstruction (REBAR) for self-supervised contrastive learning in time-series data. The REBAR method utilizes retrieval-based reconstruction to identify positive data pairs in time-series, leading to state-of-the-art performance on downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Novel approach: The paper introduces a novel approach called Retrieval-Based Reconstruction (REBAR) for self-supervised contrastive learning in time-series data. This approach utilizes retrieval-based reconstruction to identify positive data pairs in time-series, which is a unique and effective way to address the challenges of creating positive pairs via augmentations in time-series data.\n\n2. State-of-the-art performance: The paper demonstrates that the REBAR method achieves state-of-the-art performance on downstream tasks across diverse modalities, including speech, motion, and physiological data.\n\n3. Comprehensive evaluation on two tasks including classification and cluster agreement."
            },
            "weaknesses": {
                "value": "1. Lack of ablation studies: The paper does not include ablation studies to analyze the contribution of each component of the REBAR method. This makes it difficult to understand the relative importance of each component and how they interact with each other.\n\n2. Detailed explanation of the results. There is no detailed studies for table 1 and 2, e.g., visualizations of the learned embedding or positive/negative pairs. \n\n3. Limited discussion of hyperparameters: While the paper provides some details about the hyperparameters used in the experiments, it does not provide a comprehensive analysis of the sensitivity of the method to different hyperparameters. \n\n4. Without comparison with baselines, Figure 4 doesn't show any advantages of the proposed model since the diagonal pattern would be obvious for most of the baselines.\n\n5. Section 3.1, notations are used without clear definition"
            },
            "questions": {
                "value": "1. Ablation study and hyperparameters selection.\n2. Include more visualizations or examples of the positive and negative pairs identified by the REBAR method\n3. \"During evaluation, we use an an intermittent mask\", explain the intuition why different masks are used in the training and evaluation\n4. How does the REBAR method perform on time-series data with different characteristics, such as varying lengths or noise levels?\n5. Provide more detailed explanations of the convolutional cross-attention architecture used in the REBAR method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699240302505,
        "cdate": 1699240302505,
        "tmdate": 1699636913152,
        "mdate": 1699636913152,
        "license": "CC BY 4.0",
        "version": 2
    }
]