[
    {
        "id": "2KvSgl4F5n",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_miVX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_miVX"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new design of dynamic convolution named KernelWarehouse. As a more general form of dynamic convolution, KernelWarehouse can improve the performance of modern ConvNets while enjoying parameter efficiency. Experiments on ImageNet and MS-COCO datasets show its great potential."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is simple yet effective. It significantly increases the number of dynamic convolutions (>100) while maintaining the efficiency of the network. It is clearly stated how the proposed method is defined and how it works."
            },
            "weaknesses": {
                "value": "The proposed method claims to achieve the SOTA performance over various vision benchmarks. While it outperforms previous ConvNets, the SOTA models are now mainly transformer-based models. There is no comparison/discussion between the proposed method and transformers."
            },
            "questions": {
                "value": "In Table 1, why KW 4x is lower than than KW 2x? Any study on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Reviewer_miVX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697654769076,
        "cdate": 1697654769076,
        "tmdate": 1699636051892,
        "mdate": 1699636051892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IFM1L4yWdJ",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_AvKF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_AvKF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dynamic convolution which can extends the choice of candidates from typical n < 10 to n > 100, while not suffering from the explosion of parameters. The proposed method, KernelWarehouse, can use large kernel numbers when fit a desired parameter budget. This is achieved by exploiting convolutional parameter dependencies within the same layer and across successive layers. Experiments on ImageNet and COCO validate the idea and show significant improvement over baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Dynamic convolutions, like CondConv, suffer from the explosion of parameters when increasing the kernel choices. This paper proposes a way to alleviate this by exploiting convolutional kernel and layer parameter dependencies. It is a new way to formulate dynamic convolution.\n\n+ Decent improvements are oberseved with different network architectures compared with baselines.\n\n+ Instead of only reporting the parameters, this paper also reports the real runtime in Table 10. Although the proposed method has some limitations, it is still meaningful to report these numbers."
            },
            "weaknesses": {
                "value": "- The paper writing needs to improve. The introduction only has two major paragraphs and is really hard to parse and digest.\n\n- Although the proposed method could save parameters, it actually requires more time to run (see Table 10), which makes the resulted model far from real deployment."
            },
            "questions": {
                "value": "- It is counter-intuitive to see KW (4x) performs worse than KW (2x) with additional parameters in Table 1. Could the authors elaborate on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612502549,
        "cdate": 1698612502549,
        "tmdate": 1699636051815,
        "mdate": 1699636051815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bYmyjo8Kbj",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_NHsB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_NHsB"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to improve the previous existing works on dynamic convolution by using kernel partition (dividing the convolution kernel into disjoint parts), sharing parameters across diferent layers of the network and by using a \u201cnew attention function\u201d."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the proposed approach is tested on several computer vision tasks and datasets \n- the results on improving the parameter efficiency and improving the recognition performance look promising"
            },
            "weaknesses": {
                "value": "Weaknesses*\n- The work proposes quite incremental contributions over the dynamic convolution, however, the biggest concern I believe is related to the future use in practice of the proposed version for dynamic convolution. The proposed approach significantly impacts the computational costs in a negative way which can be an important bottleneck for future use in practice. Specifically, as presented in Table 10,  the speed on GPU is reduced from 322.7 to 178.5 images/second (if we compare a similar number of parameters of the model between dynamic convolution and the proposed approach). This is a significant limitation of the approach, which adds even more computational costs (on top of already negative impact of the dynamic convolution over the standard convulsion). Also the memory requirements for training and inference can be a potential limitation, I think the work should also report the memory comparison.\n\n-It looks that a significant gain comes from the  \u201cnew attention function\u201d (without this, the work is underperforming the dynamic convolution ), the results are not reported also when using  the \u201cnew attention function\u201d in the dynamic convolution framework.\n\n-Not clear why the parameters are decreasing when reducing the sharing levels (Table 6 and 7), I was expecting the opposite.\n\n-The improvements for ConvNeXt look quite marginal, it looks that the approach can have a scalability issue (basically not that useful for larger models)"
            },
            "questions": {
                "value": "See above my concerns"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766266718,
        "cdate": 1698766266718,
        "tmdate": 1699636051725,
        "mdate": 1699636051725,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "icCZ8sdr9Q",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_okim"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_okim"
        ],
        "content": {
            "summary": {
                "value": "This paper presents KernelWarehouse, a more general form of dynamic convolution that enjoys improved parameter efficiency and expressivity. The paper evaluates the proposed method across several convolutional backbones on MS COCO and ImageNet, consistently leading to improvements over existing approaches. The paper provides an extensive number of ablations corroborating many of the design choices in the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Altogether I believe this is a very strong submission with some flaws in its presentation. It is a very pleasant read with interesting insights, ablations, visualizations and evaluations. This paper has the potential to have a big impact."
            },
            "weaknesses": {
                "value": "To my best assessment, this paper does not have any big weaknesses. However, there are a few things regarding the presentation that would improve reading and the clarity of the proposed method.\n\n* The proposed method is simple, yet, in its current form, the paper presents it in a somewhat convoluted manner. I would encourage the authors to restructure the method section (Sec. 3) such that it is presented in an easier way. I want to emphasize that it is not that the paper is not understandable. I just believe that this would help digest the idea in the paper and therefore, probably improve its impact.\n\n  Things that could help:\n\n  > Present the method as an Algorithm.\n\n  > Add an additional figure in which the different components are better illustrated --In image 1, it is not clear how Assemble is done, for example--.\n\n* I strongly encourage the authors to change the name of NAF. The name New Attention Function sounds somewhat odd to me. I would encourage the authors to use a name that describes what the proposed function is doing."
            },
            "questions": {
                "value": "No questions at this point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1252/Reviewer_okim"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818550787,
        "cdate": 1698818550787,
        "tmdate": 1699636051648,
        "mdate": 1699636051648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OVKIsNSWGG",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_T4nD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_T4nD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes KernelWarehouse as a more general form of dynamic to improve the performance and efficiency of ConvNets. The paper rethinks the basic concept of kernel partition and warehouse. The effectiveness of proposed componets is investigated in detail through the comparison with other attention-based methods in image classification, object detection and instance segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper investigates dynamic convolution in detail with thorough experiments including the comparison with other sota methods in different downstream tasks.\n- The ablation of key parameters of the proposed method is given in detail. This paper is written well with organized tables and figures."
            },
            "weaknesses": {
                "value": "- The improvement of KernelWarehouse is limited as shown in Table1 and Table4 considering two models with the same parameter (+ ODConv (4\u00d7) vs + KW (4\u00d7)), which can not show the advantage of KernelWarehouse in terms of efficiency and performance.\n- The convolutional parameter budget $b$ is an important parameter, how to choose an appropriate parameter of the downstream task. The effect of $b$ on image classification, object detection and instance segmentation is different from the experiment."
            },
            "questions": {
                "value": "Listed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699332358785,
        "cdate": 1699332358785,
        "tmdate": 1699636051529,
        "mdate": 1699636051529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F6beN3dS71",
        "forum": "y4bvKRvUz5",
        "replyto": "y4bvKRvUz5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_MCn2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1252/Reviewer_MCn2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for dynamic convolutions, i.e. a method where the convolutional kernels used depend on the input tensor. This idea is typically achieved by having an attention head (e.g. akin to squeeze-and-excitation networks) to provide some linear coefficients that are then used to linearly combine some kernel basis to build the convolutional kernel. The proposed method breaks the kernel into pieces (e.g. across the channel dimension), each piece is constructed in the way described above, and then the pieces are put together to form the kernel. The authors propose some strong sharing methodology in which the kernel basis are shared by the different \"kernel pieces\" are across layers. The authors also propose a new way of computing the linear coefficients and some initialization strategy that are key to good performance. Experiments show improvements on imagenet, detection and segmentation for a number of convolutional architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly motivated and it is easy to understand the differentiation with respect to prior work.\n\nThe experimental results are comprehensive, covering several architectures, classification, detection and segmentation, has good ablations and even runtimes. I appreciate for example the inclusion of convnext-tiny and runtime measurements.\n\nThe paper is not trivial from a technical standpoint. There seems to be a significant amount of effort and experimentation involved into making the idea work.\n\nResults show high performance and ablations show the need for the different components."
            },
            "weaknesses": {
                "value": "My main issues are: 1) Architectures have evolved a lot from ResNet18/ResNet50, both in the research as well as the industry areas. 2) Latency is heavily affected. Specifically:\n\nExperiments with ResNet or even MobileNet feel a bit out of sync with the current literature in terms of architecture design. I appreciate the inclusion of convnext-tiny and, while for imagenet results show only moderate gains, there are some clear gains for object detection and segmentation. Besides convolutional architectures, does this kind of technique work for transformers at all?\n\nLatency might be partially due to the lack of optimized CUDA kernels for certain operations, but not completely, as CPU also shows similar issues. I believe the sequential nature of the attention mechanism and the need to put together the kernel are important factors contributing to the latency degradation. These issues are very hard to solve for dynamic convolutions, definitely not just restricted to the current work.\n\nSome of the design choices are key to making the proposed warehouse idea work, e.g. the initialization, temperature and attention design. Do these components work when are combined with other dynamic convolution approaches or is there something in their design that makes them specific to the kernel warehouse idea?\n\nOverall, it feels like the proposed method improves upon prior work on dynamic convolution, but still is not clear in which sense it offers some optimal trade-off.\n\nComments that might help with the clarity of the paper, no need to reply:\n\"Kernel partition\" could include some information on how the partition is done. I believe it is across the channel dimension but this only becomes clear later.\n\nTracking the explanations in \"Parameter Efficiency and Representation Power\" requires a lot of patience and attention. I believe some table with example parameterizations or some exemplification in addition to the current explanations could help the reader a lot.\n\nIt would be nice to have some justification for Eq. 3. Right now it looks like the authors came up with it from thin air, but I'm sure there was a motivation or inspiration."
            },
            "questions": {
                "value": "See comments above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1252/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699355025769,
        "cdate": 1699355025769,
        "tmdate": 1699636051437,
        "mdate": 1699636051437,
        "license": "CC BY 4.0",
        "version": 2
    }
]