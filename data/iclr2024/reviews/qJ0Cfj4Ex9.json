[
    {
        "id": "ue6URa3Ohf",
        "forum": "qJ0Cfj4Ex9",
        "replyto": "qJ0Cfj4Ex9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method that learns a library of symbolic action abstractions (i.e., high-level actions) using LLMs.\nGiven a natural language instruction and a state (object classes, predicates, etc.), the proposed method uses LLMs to plan a sequence of high-level actions.\nIn the case of the undefined operator of any high-level action, it further uses LLMs to define the operator based on in-context examples.\nSuch obtained operators are iteratively refined.\nThe low-level actions to conduct each high-level action are acquired by BFS such that they satisfy the desired subgoal state.\nThe low-level action policies are updated based on rewards from the environment after task completion with the policies.\nThe proposed method outperforms baselines in its empirical validations based on ALFRED and Mini Mincraft."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is generally written well and easy to follow.\n- The paper tackles an important issue of action grounding present in LLMs used for action planning.\n- Using LLMs to acquire high-level actions and their unknown operators looks reasonable and intriguing.\n- The two-staged pipeline for the generation of candidate operator definition is well-motivated and sounds sensible.\n- The proposed method achieves strong performance over the baselines by noticeable margins."
            },
            "weaknesses": {
                "value": "- Some assumptions made are a bit practically unrealistic. For example, $\\Phi$ is assumed to be perfect and all environmental information is known, but they are usually not the case, especially for task planning for robotic agents (Shridhar et al., 2020; Krantz et al., 2020; Weihs et al., 2021).\n- Obtaining high-level action abstractions needs the corresponding low-level policies to generate low-level actions but it seems that it requires extensive interaction with environments (e.g., brute-force search to find a low-level action sequence that satisfies the subgoal condition). Can the proposed method be also applied to some offline scenarios without these interactive environments? And is there any efficient approach to this?\n- The detail of the baseline in Table 1 is unclear. For example, for \"Code Policy Prediction,\" the authors prompt the LLM to predict 1) imperative code policies in Python and 2) the function call sequences with arguments. What is the modification made for each? \n\n\\* Krantz et al. Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments. In ECCV, 2020.\n\n\\* Weihs et al. Visual Room Rearrangement. In CVPR, 2021."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697898559048,
        "cdate": 1697898559048,
        "tmdate": 1699636778524,
        "mdate": 1699636778524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gar9SFqqgk",
        "forum": "qJ0Cfj4Ex9",
        "replyto": "qJ0Cfj4Ex9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method to iteratively learn a set of action operators that can be used by a symbolic planner to generate high-level plans that are then refined into a series of low-level plans. The action operators are learned using a LLM and their selection is guided by a reward signal. To propose a set of action operators, the LLM decomposes language-based instructions into series of high-level actions. High-level actions that do not have a corresponding action operator are then passed to the LLM (along with a few-shot prompt) for the LLM to generate an operator definition consisting of the list of variables the action operates over, the preconditions that must be met to execute the action, and the effect the action has on the environment. The set of proposed action operators are then evaluated by planning with them to solve environment tasks, and are scored based on how often they are used and how often their use leads to task success. Only action operators with high scores are retained. The authors evaluate on Mini Minecraft and Alfred, and compare against several baselines that use LLMs to provide a low-level sequence of actions, specify subgoals, and to specify the plan as code. Across tasks and baselines, the proposed method performs best."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is very well written and easy to follow. \n- The authors incorporate LLM to address the challenging problem of identifying abstract actions.\n- The baselines evaluate different ways to incorporate LLMs into planning and action selection tasks.\n- The environment on which the methods are evaluated assess actions of different complexity."
            },
            "weaknesses": {
                "value": "- It would be helpful in the results section, \"How does or approach compare to using the LLM to predict just goals, or predict task sequences?\", to call out the specific parts of the proposed algorithm that address the limitations observed in the baseline approaches.\n- It would have been beneficial to include experiments with multiple LLMs in order to understand the required LLM characteristics.\n- It is not clear from the reported experiments and results how much noise the system can handle.\n- There are no comparisons to systems that rely on hand-coded action abstractions or other methods for identifying/learning the action abstractions."
            },
            "questions": {
                "value": "- In section 3.2, the authors state that at each iteration operators are learned for only those tasks that were not solved in the previous iteration. How often were the found plans subpar? For example, taking unnecessary, but valid actions? In the experiment section, the tasks are listed as randomly ordered. How sensitive was the found action operator library to the task ordering?\n- In the results section the authors discuss Alfred failure cases as including \"operator over specification\". When over specification occurred, were multiple instances of the action with different objects seen? For example, a slice object with butter knife and one with steak knife. Were the over specifications arbitrary or driven by the training data? For example, a steak knife was chosen even through a butter knife would also work versus the sharpness level was needed to cut the object.\n- The authors suggest that encouraging more diverse proposals could address the failure mode. Was soliciting more diverse proposals attempted? Why would more diverse proposals address operator over specification? \n- Why Alfred instead of Habitat?\n- How accurate were the different parts of the LLM's output? How correct were the mappings from language description to goal specification? \n- Might the Mini Minecraft experiments, while good to test how composable the action abstractions are, simplify the action abstraction process by learning the action abstractions on the simpler tasks for which it is easier to identify action abstractions that are more primitive? Compared to learning the actions on the compositional tasks to see how well the method is able to identify useful and flexibly reusable action abstractions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698181732576,
        "cdate": 1698181732576,
        "tmdate": 1699636778407,
        "mdate": 1699636778407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QbbERVwVEL",
        "forum": "qJ0Cfj4Ex9",
        "replyto": "qJ0Cfj4Ex9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to exploit the world knowledge of LLMs for learning action abstractions for hierarchical planning. These action abstractions can then be used to solve long-horizon planning problems, by decomposing a goal into subgoals and solving them using bi-level planning. More specifically, given a task and symbolic state, the authors use LLMs to propose symbolic (high-level) operators and their corresponding definitions (in PDDL) which are then used by a bi-level planner to generate a feasible low-level plan. The useful operators (planning-compatible and grounded) are retained in an operator library (i.e. reusable) and used for subsequent tasks, including those that require the composition of learned operators."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Overall, the paper is well-motivated, clearly written, and supported by strong empirical evidence.\n2. The proposed idea of exploiting knowledge of LLMs for action abstraction is intuitive and effective and would be of interest to the planning and decision-making community."
            },
            "weaknesses": {
                "value": "It would be nice to have some statistics on the length of the plan sequence (both in terms of high-level and low-level actions), and the number of learned operators, to get an understanding of the task complexity (especially for the compositionality experiments) in Minecraft and ALFRED domains. It is hard to get an idea from just the empirical evidence."
            },
            "questions": {
                "value": "1. In Sec 3.1 (Symbolic operator definition), is there a process through which you identify and/or discount semantically similar (redundant) operators from being added in the operator library since the LLM generation is not conditioned on it (i.e. the LLM is not aware of the existing operators in the library).\n\n2. In Sec 3.4 (Scoring LLM Operator Proposals), the operators are selected based on their executability in the low-level planning, but the overall goal is not accounted for. Wouldn't this lead to the selection of some operators that are just \"feasible\" but not \"useful\"?\n\nMinor Comment:\n* Sec 3.4: s/b > \\tau_r"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698536090843,
        "cdate": 1698536090843,
        "tmdate": 1699636778274,
        "mdate": 1699636778274,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3sshSJaIIJ",
        "forum": "qJ0Cfj4Ex9",
        "replyto": "qJ0Cfj4Ex9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to leverage LLM to solve long-horizon planning problems by dynamically building a library of symbolic action abstractions and learning a low-level policy to execute the subgoals. They conducted experiments and relevant ablation studies on Mini Minecraft and ALFRED benchmarks to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The method is technically feasible.\n+ The writing is easy to follow. \n+ Representing abstract actions with symbols and reasoning them with LLM is an interesting attempt."
            },
            "weaknesses": {
                "value": "+ **Lack of novelty.** This pipeline is reminiscent of the Voyager model, which also aims to tackle long-horizon tasks via the creation of a dynamic skill library. This limits the contributions of this paper. It is better to highlight the differences between this work and Voyager. \n\n+ **Benchmark is too simple.** It should be noted that the test environment used in the current work (Mini Minecraft) is significantly less complex than the original Minecraft version, resulting in reduced task difficulty. Especially the success rate in Mini Minecraft even reaches 100%. \n\n+ **Concerns about generalization ability.** The proposed method relies heavily on symbolic representations. I'm concerned that it may be difficult to generalize to complex real-world environments, which are often not easily symbolized. \n\n+ **Missing important citations.** [1, 2] are also important methods that leverage LLM as the planner to decompose the long-horizon task into subgoals. I suggest the authors to include some discussion and comparison of such methods. \n\n[1] Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, https://arxiv.org/abs/2302.01560\n\n[2] Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, https://arxiv.org/abs/2305.17144"
            },
            "questions": {
                "value": "As stated in the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685582628,
        "cdate": 1698685582628,
        "tmdate": 1700702775809,
        "mdate": 1700702775809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GxQqV9RJvr",
        "forum": "qJ0Cfj4Ex9",
        "replyto": "qJ0Cfj4Ex9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the challenge of long-horizon planning. To make this more tractable, the authors leverage hierarchical planning using temporal action abstractions, breaking down intricate tasks into manageable subproblems. The novel contribution is a system that harnesses language to derive symbolic action abstractions and associated learnable low-level policies. By querying large language models (LLMs), the system proposes symbolic action definitions, subsequently integrating these into a hierarchical planning framework for grounding and verification. This approach is framed within a multitask-reinforcement-learning objective, where an agent interacts with an environment to solve tasks described in natural language. The ultimate aim is to construct a library of grounded actions that are both planning-compatible and efficient."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The system leverages language to derive symbolic action abstractions, a unique approach to decomposing complex tasks, and subsequently verifies them within a hierarchical planning framework, ensuring the practical applicability of the abstractions,  which was tested on two benchmarks, Mini Minecraft and ALFRED, and outperformed other baseline methods that incorporate language models into planning.\n\nThe paper presents a commendable effort in bridging the capabilities of large language models with hierarchical planning, the innovative approach of using language to derive action abstractions is particularly noteworthy."
            },
            "weaknesses": {
                "value": "- Goal Misspecification: Failures on the ALFRED benchmark often occurred due to goal misspecification, where the LLM did not accurately recover the formal goal predicate, especially when faced with ambiguities in human language.\n\n- Policy Inaccuracy: The learned policies sometimes failed to account for low-level, often geometric details of the environment.\n\n- Operator Overspecification: Some learned operators were too specific, e.g., the learned SliceObject operator specified a particular type of knife, leading to planning failures if that knife type was unavailable.\n\n- Limitations in Hierarchical Planning: The paper acknowledges that it doesn't address some core problems in general hierarchical planning. For instance, it assumes access to symbolic predicates representing the environment state and doesn't tackle finer-grained motor planning. The paper also only considers one representative pre-trained LLM and not others like GPT-4."
            },
            "questions": {
                "value": "Questions:\n\n- The two-stage prompting strategy involves symbolic task decomposition followed by symbolic operator definition. How does the system ensure that the decomposition is optimal or near-optimal for complex tasks?\n\n- The author mentioned that one of the common failures on the ALFRED benchmark was due to goal misspecification, especially when faced with ambiguities in human language. Could you elaborate on how the system currently handles such ambiguities and if there are plans to improve this aspect?\n\n- The paper demonstrates that action libraries from simpler tasks in Mini Minecraft generalize to more complex tasks. Are there plans to test this generalization capability in more diverse environments or tasks outside of the current benchmarks?\n\n- How scalable is the proposed system? Specifically, if the number of tasks or the complexity of the environment increases significantly, how would the system's performance be affected?\n\nSuggestions:\n\n- Might consider introducing an interactive feedback loop where the system can ask clarifying questions when faced with ambiguous goals or tasks. This could help in refining the task understanding and improve planning accuracy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822152435,
        "cdate": 1698822152435,
        "tmdate": 1699636778037,
        "mdate": 1699636778037,
        "license": "CC BY 4.0",
        "version": 2
    }
]