[
    {
        "id": "wVFaJBiRpf",
        "forum": "Ny150AblPu",
        "replyto": "Ny150AblPu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5/Reviewer_D7Cn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5/Reviewer_D7Cn"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a new method, D-TIIL, to expose text-image inconsistency with the location of inconsistent image regions and words, which is quite commonly happening in T2I generation diffusion models. To achieve this, they introduce a new dataset, TIIL, for evaluating text-image inconsistency localization with pixel-level and word-level inconsistency annotations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The dataset's contribution is commendable. Existing datasets lack the capacity to furnish evidence regarding inconsistencies occurring at both the image region and word levels, which is essential for evaluating D-TIIL (Diffusion-Based Text-to-Image Inconsistency Localization).\n\n2. The problem addressed in this research is of significant importance. Previous methods have primarily focused on determining the presence of inconsistencies, whereas this paper introduces a novel approach to pinpointing the specific locations where these inconsistencies occur."
            },
            "weaknesses": {
                "value": "1. It would be valuable to explore whether this method could be extended to evaluate other text-to-image (T2I) augmentation techniques (i.e., [1-3]). Given the abundance of research on generating images based on textual prompts, applying this method for evaluation purposes could have a broader impact and contribute significantly to the field.\n\n2. Are there alternative evaluation metrics to assess the correspondence between text and images? Based on my experience with CLIP scores, it may not consistently capture performance accurately in various scenarios.\n\n[1] Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models. SIGGRAPH 2023\n\n[2] Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. ICCV 2023\n\n[3] Expressive Text-to-Image Generation with Rich Text. ICCV 2023"
            },
            "questions": {
                "value": "As mentioned in the above weakness, I would appreciate seeing the proposed method applied more extensively in evaluation. The inclusion of evaluation metrics beyond CLIP scores could enhance the robustness and confidence of this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5/Reviewer_D7Cn"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698487488683,
        "cdate": 1698487488683,
        "tmdate": 1699635924456,
        "mdate": 1699635924456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GRny6S4ecC",
        "forum": "Ny150AblPu",
        "replyto": "Ny150AblPu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5/Reviewer_4tYr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5/Reviewer_4tYr"
        ],
        "content": {
            "summary": {
                "value": "The authors propose D-TIIL (Diffusion-based Text-Image Inconsistency Localization), a system for automatically identifying and explaining text-image inconsistencies. D-TILL uses text-image diffusion models to locate semantic inconsistencies in text-image pairs. Diffusion models trained on large datasets filter out irrelevant information and incorporate background knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings and modified image regions to visualize these inconsistencies.\nTo evaluate the effectiveness of D-TIIL, the authors also introduce a new dataset (TIIL) with 14K consistent and inconsistent text-image pairs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThe paper is well written and well structured\n\u2022\tThe problem and the related work are well introduced\n\u2022\tThe framework is explained in detail\n\u2022\tThe idea to build consistency scores between stable diffusion and the original image is interesting."
            },
            "weaknesses": {
                "value": "\u2022\tThe general theoretical idea behind the approach lacks clearity\n\u2022\tThe real-world application is not very clear, e.g. wrong labels have a different type of mislabeling than just objects that are swapped\n\u2022\tSensitivity to threshold highly influences M and the consistency score\n\nWith D-TIIL, the authors have presented an interesting method for using diffusion models to evaluate the consistency of image-text pairs.\nHowever, the utility of the method is not fully evaluated in detail. Deeper insights into why this approach works are lacking. In addition, it would be nice to see how the approach works on other datasets where the labeling is just mixed up or misleading.\nIn addition, I would recommend for ICLR to investigate the method in more detail in terms of learned representations.\n\nThe paper is well written and has some interesting ideas, e.g. the usage of diffusion models for detecting image-text inconsistency. The method and the dataset, both are valuable. However, to be accepted in ICLR I would expect more and deeper investigations about the method and the dataset. What is learned, what are short comings?\nThere are some doubts, such that the model could be sensitive to the DALE generated part instead being sensitive to the text-image inconsistency. Experiments are missing that evaluate the underlying behavior. Moreover, a second evaluation on another dataset with more established baselines would be preferable to proof some of the assumptions, advantages and shortcomings of the method."
            },
            "questions": {
                "value": "\u2022\tHow does the approach perform on completely wrong image descriptions?\no\tIs the whole image masked?\n\u2022\tIs the model sensitive to the image part generated by DALE and not to the parts which do not correspondent to the text?\no\tIs there an experiment that can proof that?\no\tMaybe regenerate the image for the dataset also with the right semantic class?\n\u2022\tIs there another dataset where the method could be compared also to other baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745076380,
        "cdate": 1698745076380,
        "tmdate": 1699635924308,
        "mdate": 1699635924308,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BGAsoFzNFk",
        "forum": "Ny150AblPu",
        "replyto": "Ny150AblPu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5/Reviewer_LEKC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5/Reviewer_LEKC"
        ],
        "content": {
            "summary": {
                "value": "This paper presents D-TIIL for identifying and localizing inconsistencies between text and images. \nA new dataset, TIIL, containing 14K consistent and inconsistent text-image pairs, is introduced for evaluating the method. The D-TIIL outperforms existing approaches in terms of Accuracy scores and demonstrates more explainable results. In a nutshell, the paper offers a scalable and evidence-based approach to identify and localize the text-image inconsistency. However, it also acknowledges the potential misuse of the method for creating deceptive text-image pairs and suggests improving the algorithm and restricting access."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Originality: The paper introduces a novel method, D-TIIL, that exposes text-image inconsistency with the location of inconsistent image regions and words. Also, the new TIIL dataset is the first dataset with pixel-level and word-level inconsistency features that provide fine-grained and reliable inconsistency.\n\n2. Quality: The D-TIIL and TIIL dataset generation are thoroughly described. The paper also provides a comprehensive comparison of the proposed method with existing approaches.\n\n3. Clarity: The paper is well-structured and clearly written. The method is explained in detail and the experiment results are presented in an understandable manner.\n\n4. Significance: The D-TIIL method improves the accuracy of inconsistency detection and provides more explainable results. The introduction of the diffusion model makes it possible to align text and images in a latent and joint representation space to discount irrelevant information and incorporate broader knowledge."
            },
            "weaknesses": {
                "value": "1. The paper acknowledges that the D-TIIL may struggle with inconsistencies with respect to specific external knowledge, and this could reduce the effectiveness of the method in real-world application.\n\n2. The D-TIIL method relies heavily on the text-to-image diffusion models and benefits a lot from the semantic space that is already well aligned. This dependence could limit the generalizability of the proposed method.\n\n3. There are some confusing details in the method description section.\n\n4. In the comparison of methods, the reasons why D-TIIL is superior are not discussed and analyzed in detail, and the potential solutions for the failure cases are not provided.\n\n5. More specific discussions and measures could be included to prevent potential abuse rather than simply restricting access."
            },
            "questions": {
                "value": "1. Regarding Step 3 in Section 3 METHOD, the proposed E_{dnt} and descriptions like \u201cinclude extra implicit information from the images and excludes additional implicit information that only appears in the text\u201d raise doubts about the effectiveness of the process of the \u201ctext denoising\u201d. Such \u201ctext denoising\u201d seems to be too idealistic. In Section 5.4, for example, there is the failure case of the word \"office\". This leads to the bold suspicion that the D-TIIL method is only valid for simple objects, but not for backgrounds or objects that contain more complex semantics.\n\n2. Also, the high dependency on the diffusion model affects the generalizability of the method. If text and image are not well aligned on the latent space, the validity of the method will be more affected. Semantic entanglement can also exist.\n\n3. Regarding Step 4 in Section 3 METHOD, the descriptions like \u201cWe then compute the cosine similarity score between this image embedding and the input text embedding\u201d are confusing to the readers.\n\n4. In the Data Generation part of Section 4 TILL DATASET, T_{m} is unknown where it comes from, is it manually designed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757618691,
        "cdate": 1698757618691,
        "tmdate": 1699635924218,
        "mdate": 1699635924218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5QZYQUEu7b",
        "forum": "Ny150AblPu",
        "replyto": "Ny150AblPu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5/Reviewer_qTYR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5/Reviewer_qTYR"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how to detect image-text inconsistency with diffusion models. More specifically, the author designed a pipeline that iteratively use diffusion models to edit the text and images in the image-text pairs to gradually optimize a mask that can point out where the inconsistency come from. This task is interesting and meaningful for misinformation detection, as it provides interpretable prediction results. To evaluate the proposed method, the authors collected a dataset containing image-text pairs and their inconsistency masks. Experiments shows that the proposed method outperforms baselines and gives explanable prediction on the inconsistency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The task studied in this paper is meaningful.\n\n2. The dataset that they collected is contributive to the community.\n\n3. The method is novel."
            },
            "weaknesses": {
                "value": "1. The writing is not very good. I read the methodology part several hours to understand their pipeline.\n\n2. The idea is well justified for the inconsistency of object alignment. But what if the predicate is not aligned, i.e. the person is correct but the action is not?"
            },
            "questions": {
                "value": "How does the annotation and the model handles predicates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783361796,
        "cdate": 1698783361796,
        "tmdate": 1699635924147,
        "mdate": 1699635924147,
        "license": "CC BY 4.0",
        "version": 2
    }
]