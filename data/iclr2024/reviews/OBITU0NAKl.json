[
    {
        "id": "5PXB4MjVyE",
        "forum": "OBITU0NAKl",
        "replyto": "OBITU0NAKl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_ZpWJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_ZpWJ"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the learning dynamics of GNN in the function space and connects it to label propagation. The link is the residual propagation where the neural tangent kernel matrix is replaced by high order graph adjacency matrix. The authors show that the learning dynamics of infinitely wide two-layer GNN is a special form of residual propagation. The authors then study the generalization of GNN based on kernel-graph alignment."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1) The connection between the learning dynamics of GNN and label propagation via residual propagation is novel and insightful. \n\n(2) The theoretical analysis is deep and elegant."
            },
            "weaknesses": {
                "value": "(1) The assumption of infinitely wide network is not realistic. It is better to analyze the evolution of the kernel. \n\n(2) The restriction to two-layer GNN or last-layer feature propagation is not realistic either."
            },
            "questions": {
                "value": "Is it possible to go beyond neural tangent kernel and two-layer GNN? What theoretical tools are needed? I assume there are methods developed for MLP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742539739,
        "cdate": 1698742539739,
        "tmdate": 1699636176211,
        "mdate": 1699636176211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vl2S7UyCuF",
        "forum": "OBITU0NAKl",
        "replyto": "OBITU0NAKl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_mEP4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_mEP4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the training dynamics and generalization of graph neural networks (GNNs). The authors theoretically derive the evolution of the residuals of GNNs on training and testing data in several settings and based on this, they explain the generalization ability of GNNs. Some numerical verification is also reported."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The analysis of training dynamics and generalization is an extremely important topic in the research of GNNs. This paper has a good scope and is clearly written to connect ideas from different fields.\n2. Although I think there are some limitations, the derived theoretical results are technically solid and are pleasing to read. The authors use tools from label propagation and graph neural tangent kernel to characterize the training dynamics of GNNs and they derive explicitly dynamics in several cases.\n3. The authors give some reasonable explanation of the GNN generalization by connecting GNN training dynamics and optimal kernel."
            },
            "weaknesses": {
                "value": "1. In Section 4.2, the authors only consider two very special $\\bar{\\mathcal{X}}$, which makes the theory somehow limited.\n2. The training dynamics of GNNs should be highly nonlinear. More explicitly, in equation (12), the GNTK $\\Theta_t^{(l)}$ depends on $W_t$. However, the derived dynamics in Theorem 5 and Theorem are linear. The authors need to explain how they remove the nonlinearity and why it makes sense."
            },
            "questions": {
                "value": "1. This question is related to the second point in the \"Weakness\". According to your derivations in Section B.4, I think you remove the nonlinearity or the dependence of the kernel on the parameters $W$ by taking the expectation for $W$. Please correct me if I misunderstood something. I am confused as to why you take the expectation -- in my opinion, training dynamics is the evaluation of residuals or parameters for any given/fixed initialization. If you want to take expectation over $W$, I think the equation (14) and (15) should be stated as something like expected residual. Please explain what is happening and why it is reasonable.\n2. In Theorem 5, do you have the training dynamics for optimizing $W^{(2)}$? In Theorem 6, is the training dynamics for optimizing all parameters in the GNN, or it is just optimizing parameters in a single layer (as in Theorem 5)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2411/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2411/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2411/Reviewer_mEP4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699059190821,
        "cdate": 1699059190821,
        "tmdate": 1699636176152,
        "mdate": 1699636176152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OoUHJBEqGL",
        "forum": "OBITU0NAKl",
        "replyto": "OBITU0NAKl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_mA7d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2411/Reviewer_mA7d"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the function-space learning dynamics of graph neural networks (GNNs) during gradient descent. The key contributions include:\n\n-   Identifying the similarity between GNN learning dynamics and cross-instance label propagation, facilitated by the neural tangent kernel (NTK).\n-   Theoretical insights into why GNNs demonstrate strong generalization on graphs with high homophily, connected to NTK\u2019s natural alignment with graph structure.\n-   Development of a Residual Propagation (RP) algorithm inspired by these dynamics, showcasing notable performance improvements over standard GNNs.\n-   Examination of GNN limitations on heterophilic graphs, including empirical validation on both synthetic and real-world datasets, revealing misalignments between NTK and the graph structure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: While connecting GNN dynamics to propagation schemes is novel, the paper lacks some innovation in terms of proposing new techniques beyond the basic RP algorithm. Theoretical insights relate to existing works on kernel alignment and generalization.\n\n**Quality**: The theoretical claims rely heavily on assumptions of overparameterization and alignment of NTK with adjacency matrix, which may not perfectly hold in practice. More analysis is needed for finite width GNNs. Empirical evaluation is quite limited.\n\n**Clarity**: The key ideas are reasonably clear, the significance of results is not fully crystallized.\n\n**Significance**: The insights on generalization are incremental on existing theory on kernel alignment. Practical impact is unclear given the simplicity of RP and lack of evaluation on large benchmarks. The limitations of GNNs on heterophily are already well-known."
            },
            "weaknesses": {
                "value": "**Strong assumptions**: The study's theoretical framework is built on robust assumptions regarding infinite width and NTK alignment that may not hold across all scenarios. Expanding the analysis to cover finite-width GNNs could substantiate the findings.\n\n**Limited evaluation**: The empirical validation is limited in scope, focusing on smaller datasets and simpler models. Extensive testing involving state-of-the-art GNNs and more diverse benchmarks would be instrumental in corroborating the theoretical claims.\n\n**Significance in theory**:  The theoretical contributions, while valuable, seem to offer only a modest advancement beyond existing studies on kernel alignment. Clarifying the distinctions from previous work would help to highlight the unique contributions of this study."
            },
            "questions": {
                "value": "-   How does the theoretical analysis diverge from previous studies on kernel alignment? Clarification of the novel insights would be appreciated.\n-   Given the recognized challenges of GNNs in dealing with heterophily, are there any strategies or recommendations proposed by the authors to tackle this issue beyond the current analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699160546973,
        "cdate": 1699160546973,
        "tmdate": 1699636176087,
        "mdate": 1699636176087,
        "license": "CC BY 4.0",
        "version": 2
    }
]