[
    {
        "id": "vIRUvHq7eK",
        "forum": "nFYksmdqgY",
        "replyto": "nFYksmdqgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_fBBj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_fBBj"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a method for unsupervised multi-modal machine translation (UMMT). The novelty of the method is introducing a sentence-level and token-level contrastive learning signal into UMMT.\n\nUMMT is described in 3 stages: (1) language modeling, (2) initialization, and (3) iterative back translation. The authors describe the method as a contribution to stage (2) initialization by using noise contrastive estimation on batches of image-text/caption pairs to improve the semantic alignment of the UMMT model without bi-text between the language pairs. Experimental results for Multi30K Flickr2016, Flickr2017 and COCO2017 demonstrate that the method yields better performance across nearly all MT directions (EN<-->FR or DE). \nAblations identify that all components are useful in the final UMMT model and some analysis also discusses how the semantic space is more aligned with this method than others."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper describes a novel integration of NCE-based learning to use extra image-text corpora into translation.\nThis will be valuable for both MT and multimodal ML researchers in generating new ideas and advancing multimodal machine translation. \n\n- The interaction between multilingual representation alignment and multimodal representation alignment is not fully understood at present. this paper contributes to advancing this understanding."
            },
            "weaknesses": {
                "value": "- The experimental setup is arguably slightly outdated in that the paper trains a very small transformer from scratch for the task. While it is fair to do this for fair comparison to other work, the contributions of the paper could be extended and more applicable to a contemporary training setup if they used a pre-trained initialization (in lieu of stage (1) of the UMMT pipeline). This hurts the impact of the paper in its current form as it is less clear how this work could be interpreted/extended by a current reader. At a minimum, the paper should address why a pretrained model would not be appropriate here if this is the case.\n\n- The work has minimal diligence for the handling of data for the task. 3 datasets are combined, chopped, shuffled and split into different groups and splits for each stage but this is not justified or introspected upon. It is not clear why the work takes the current course of action. Furthermore, it is not clear if different splits (i.e. more data in (1) or (2) would be more beneficial given the purported low-resource benefit of the method). \n\n- I am concerned some of the analyses and ablations are straw men. For the retrieval task, I can see little reason why MASS should be expected to produce text-to-image alignment as this was not the intention of this model. 5.4 appears to only re-enforce prior work and I am not sure why this is a core contribution. The analysis in 5.5 appears to make conclusions from a sample of 2 outputs which is not sufficient for a qualitative or quantitative conclusion. \n\n- ~Comparisons to other systems are not current. For text-only comparisons there is no reference to OPUS, NLLB, M2M100, MBART50-(one,many)-to-(many,one) or any recent comparator for text-only MT. This makes your results harder to contextualize. These models can be run with the compute available to the authors.~"
            },
            "questions": {
                "value": "- The framing of the Initialization stage is confusing and needs more contextualization \u2014 is this optional after pre-training. what prior work uses or skips this? how was better pre-training changed the need for initialization? Furthermore, you describe Initialization somewhat circularly, in that you say that it is important because it happens and do not clearly state what the purpose of this stage is and why it is needed.\n\n- Many uses of imprecise or hyperbolic language which is arguably inappropriate. \"a certain level of translation ability\" is redundant. \"remarkable increase\" is not justified.\n\n- You appear to have misused \"significant\" as an adjective without statistically quantifying if your results are statistically significant. Please either revise or confirm the statistical significance of your results. \n\n- Many spelling and grammar errors i.e. \"langauge\", \"acheive\", \"We\" in the middle of a sentence.\n\n- Why must decoders be language specific?\n\n- Did you consider using a [CLS] type token for the encoding to avoid needing to pool the encodings?\n\n- Did you consider that your improvements, compared to models trained not on image-text data, may be because you are better fitting to the distribution of sequence lengths of this kind of data which other MT models are unaware of?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Reviewer_fBBj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698003087209,
        "cdate": 1698003087209,
        "tmdate": 1700408024690,
        "mdate": 1700408024690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k6v6tEFEkn",
        "forum": "nFYksmdqgY",
        "replyto": "nFYksmdqgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_rLq9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_rLq9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel unsupervised multi-modal machine translation method that leverages monolingual image-text pairs as pivots to learn a shared source-target language space for better initialization through contrastive learning. Experimental results show that this technique leads to a better translation model that outperforms both text-only and multi-modal baselines on machine translation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors clearly described the background and motivations needed to understand the proposed unsupervised multi-modal machine translation model.\n2. The proposed method relies on cross-modal contrastive learning to achieve sentence-level and token-level alignment. This approach results in a strong source-target alignment in the shared space for better initialization before the back-translation step.\n3. The authors showed the generalization potential of the proposed model in an out-of-domain experiment."
            },
            "weaknesses": {
                "value": "1. There are some typos and grammatical errors (I missed some but here is an example. Kindly check and make the necessary corrections):\na. \"Therefore, we propose a novel unsupervised multi-modal method to achieve better initialization. method semantically aligns source-target languages into a shared latent space through contrastive learning, using images as pivots\"\n2. This method heavily relies on the assumption that the two source and language spaces are approximately isomorphic.  What if the two spaces are not isomorphic?"
            },
            "questions": {
                "value": "1. What happens to your model when the two spaces are not isomorphic?\n2. A more comprehensive experiment on more languages is needed. The shared space of EN-FR, and EN-DE, is highly approximately isomorphic even under procrustes. Include other languages and report them to make your model more generalizable.\n3. No significance testing or error bars on experimental results.\n4. By reporting metrics such as singular value gap and effective condition number (see https://aclanthology.org/2020.emnlp-main.186.pdf  and https://openreview.net/forum?id=Nh7CtbyoqV5) could show how the shared representation become after your proposed method is applied. A before and after table should be good to report. \n5. Do you apply any normalization technique(s) on the representation to make them isomorphic?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Reviewer_rLq9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785316011,
        "cdate": 1698785316011,
        "tmdate": 1699636281929,
        "mdate": 1699636281929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AQlc8GdEn5",
        "forum": "nFYksmdqgY",
        "replyto": "nFYksmdqgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_QtWc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_QtWc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an innovative approach to Unsupervised Multi-modal Machine Translation (UMMT) that leverages images as language-agnostic signals. The authors introduce cross-modal contrastive learning at both sentence-level and token-level to achieve cross-lingual alignment and enhance translation performance. Experimental results demonstrate that the proposed method surpasses state-of-the-art UMT and UMMT systems in terms of BLEU and METEOR scores."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe introduction of the visual modality as a language-agnostic signal is a novel approach that holds the potential to enhance the effectiveness of UMT systems.\n2.\tThe extensive experimental evaluation conducted on multiple datasets demonstrates the superiority of the proposed method compared to other UMT and UMMT systems."
            },
            "weaknesses": {
                "value": "1.\tThis work primarily evaluates English-German and English-French translations, which are typically high-resource translation tasks. It would be valuable to see an evaluation of real low-resource languages to better gauge the method's effectiveness in such scenarios.\n2.\tThe paper should provide insights into the effectiveness of the alignment method when applied to languages with low similarity. This would offer a more comprehensive understanding of its performance across various language pairs.\n3.\tIn comparison to methods that rely on bilingual dictionaries to enhance alignment, such as denoising synthetic code-switched data, the paper should discuss whether the introduction of images offers clear advantages or if these two approaches are complementary.\n4.\tThe authors should consider discussing whether there have been more recent developments in text-only unsupervised translation methods, as this would help place their approach in the context of the latest advancements in the field."
            },
            "questions": {
                "value": "While the authors demonstrate superiority over existing systems on high-resource language pairs (en<->de and en<->fr), they should explore the effectiveness of real low-resource languages"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3326/Reviewer_QtWc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840015128,
        "cdate": 1698840015128,
        "tmdate": 1699636281842,
        "mdate": 1699636281842,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YZxldoCYqc",
        "forum": "nFYksmdqgY",
        "replyto": "nFYksmdqgY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_QpzC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3326/Reviewer_QpzC"
        ],
        "content": {
            "summary": {
                "value": "Motivated by the thoughts that different languages share a common visual description, this paper proposes a novel unsupervised multi-modal machine translation method using images as pivots. Specifically, sentence-level and token-level alignment are achieved by contrastive learning. Experiments show that its method achieves improvements over other methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing is clear and brief, making readers easy to understand."
            },
            "weaknesses": {
                "value": "1. The text-image pairs need annotation, which means that the unsupervised machine translation needs cross-modality annotation, which is more expensive than text-only annotation.\n2. In Section 3.3, word is the text token. What does an image token represent? why text token and image token in different position (i \u2260 j) is regarded as negative examples, and why in same position can be regarded as positive examples."
            },
            "questions": {
                "value": "1. In Section 3.3, the word is the text token. What does an image token represent? \n2. In section 3.3, why text tokens and image tokens in different positions (i \u2260 j) is regarded as negative examples, and why in the same position can be regarded as positive examples? In text-only translation tasks, the same position of the src sentence and tgt sentence always do not refer to the same thing. In other words, the mapping of image tokens and language tokens is simply one-to-one position mapping.\n3. Do you only visualize 6 examples in Figure 2? Where are the 6 examples from? Is it sufficient to support that your approach is truly successful?\n4. Why IWSLT data is regarded as out-of-domain data compared with the data you use for initialization which includes MsCOCO and Multi30K? Can you prove the domain mismatch in the two kind of datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3326/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698993178527,
        "cdate": 1698993178527,
        "tmdate": 1699636281765,
        "mdate": 1699636281765,
        "license": "CC BY 4.0",
        "version": 2
    }
]