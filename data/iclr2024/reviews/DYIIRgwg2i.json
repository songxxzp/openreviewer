[
    {
        "id": "OqD3oiG1WH",
        "forum": "DYIIRgwg2i",
        "replyto": "DYIIRgwg2i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_2GDn"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LLM Surgeon, a method that enhances the scalability of Kronecker-factored curvature approximations of the targeted loss landscapes, designed for pruning LLMs at arbitrary sparse patterns. The authors demonstrate that this proposed methodology consistently improves the PPL performance of current methods across a range of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized and easy to comprehend.\n\n2. The fundamental idea for estimating the curvature of the loss landscape is reasonable and innovative."
            },
            "weaknesses": {
                "value": "1. Since the authors adopt a multi-shot sparse approach, it would be beneficial to quantitatively compare the time costs and GPU memory consumption with SparseGPT.\n\n2. While the authors emphasize sparsity in large models, the largest model they utilize is of 7-billion parameters. It might provide readers with a clearer view if experiments involving larger model sizes were included.\n\n3. Global rank ordering is a sound strategy, but there seems to be a lack of an ablation experiment, that is, whether the suggested method outperforms SparseGPT under the same layer-wise sparsity situation.\n\n4. Although the authors underlines that LLM surgeon can be migrated to structured pruning, and as the authors stated, \"To the best of our knowledge, this is the first method to successfully perform structured pruning for LLMs,\" they do not discuss nor compare their approach to the already presented structured LLM pruning method[1]. This lack of discussion appears less meticulous. Furthermore, the authors should also consider comparing their work with another state-of-the-art large model sparse method, Wanda [2], which can also be adapted for pruning LLMs at any patterns.\n\n[1] LLM-Pruner: On the Structural Pruning of Large Language Models. In NeurIPS, 2023\n[2] A Simple and Effective Pruning Approach for Large Language Models. In Arxiv, 2023"
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637125275,
        "cdate": 1698637125275,
        "tmdate": 1699636216367,
        "mdate": 1699636216367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "a4WMew260w",
        "forum": "DYIIRgwg2i",
        "replyto": "DYIIRgwg2i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_zzzW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_zzzW"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method called \"LLM Surgeon\" for efficient pruning and compression of large pretrained language models like OPT and LLAMa. It scales up second-order Hessian-based pruning methods like Optimal Brain Surgeon using Kronecker-factored approximations of the Fisher information matrix. It derives closed-form solutions for removal costs and correlated weight updates when pruning multiple weights jointly. Experiments show the method can prune OPT and LLAMa models by 20-30% with minor performance loss and outperforms prior work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and clearly presented; \n- The paper provides a general pruning framework applicable to different structured and unstructured schemes. \n- The paper provides careful derivation of update rules that consider correlations between weights, theoretically principled and extends classical segundo-order pruning methods.\n- The proposed methods achieves state-of-the-art pruning results on large language models, especially for structured pruning. \n- Detailed ablation regarding the low-rank components, approximation methods, as well as the qualitative sparsity level analyses are provided to show the comprehensiveness of the proposed methods and design choices;"
            },
            "weaknesses": {
                "value": "- The paper still uses approximations for computational tractability which limits pruning performance.\n- Structured pruning leads to irregular sparsity patterns which are difficult to accelerate. The real inference speedup or memory savings after pruning is unknown; \n- Additional FLOPs for approximation and updating offsets gains during deployment are needed, while the detailed comparison and discussion might be missing.\n- Some related works might also be good to include [1];\n\n[1] Yu, Shixing, et al. \"Hessian-aware pruning and optimal neural implant.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022."
            },
            "questions": {
                "value": "- Could the authors provide both inference speed and additional cost for approximation and updating the offsets?\n- Could larger model sizes also be included and evaluated throughout?\n- Besides the PPL, could the author also provide the 0shot performance degradation regarding the OPT/LLaMA models for a comprehensive evaluation;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842749416,
        "cdate": 1698842749416,
        "tmdate": 1699636216288,
        "mdate": 1699636216288,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TtxnA4nbva",
        "forum": "DYIIRgwg2i",
        "replyto": "DYIIRgwg2i",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a way to prune the weights of a pretrained LLM with a negligible loss in performance by iteratively solving a quadratic optimization problem using the curvature of a local minimum. To save the memory of materializing hessian, the work calculates the covariance layer-wise with Kronecker factorization. Also, the curvature is calculated incrementally and the remaining weights are corrected with a first-order term as more weights are pruned so that the weight remains in a local minimum."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach is sound and is presented well. Not having to materialize the hessian makes this amenable for LLM pruning."
            },
            "weaknesses": {
                "value": "While it is believable that this method would generalize, the setup is unsatisfying in that the dataset used for compression is drawn from wikitext-2 which is a narrow domain (meaning the loss landscape may be easier to optimize and prune than a broader distribution), and the final model is evaluated only on the test perplexity of the same dataset. SparseGPT uses C4 and reports downstream performance on various standard benchmarks. The paper could really strengthen its claim by repeating the same setup as SparseGPT. Downstream benchmarks are required for a fair comparison and acceptance of the work."
            },
            "questions": {
                "value": "Equation 2 should read `- log(D | theta)`. There is a typo in `General solution  We denote ... e_{q_k}` => `e_{k_q}`.\n\nRelated Work could include more previous works on LLM compression."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2739/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2739/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2739/Reviewer_zF62"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2739/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698885185836,
        "cdate": 1698885185836,
        "tmdate": 1699636216210,
        "mdate": 1699636216210,
        "license": "CC BY 4.0",
        "version": 2
    }
]