[
    {
        "id": "l3NT5DA3FE",
        "forum": "GrunXMbdXY",
        "replyto": "GrunXMbdXY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_3Q3Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_3Q3Y"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new gradient label leakage procedure. The procedure \"flattens\" the gradients of the last linear layer of the network and decomposes it into two terms corresponding to samples that are correctly classified and those that are not. Each term is approximated with a Gaussian whose unknown parameters are fitted jointly with a GMM on additional data. Then, each possible label is ranked based on its likelihood of being present in the data batch calculated using the parameters of those Gaussians. Finally, the total number of different labels present in the batch is estimated based on linear regression over the weights of the two Gaussian weight factors. This in combination with the ranking, produces the set of labels present in the data batch. The authors apply this technique to federated learning of LLMs and machine translation algorithms to leak the set of tokens that are used to train the Transformer models. The authors demonstrate this procedure results in 0.7/0.8 F1 score for large batches of many individual tokens with realistic-sized vocabularies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Experiments on fairly large models ( GPT-2 )\n- Experiments on large sequences and batches\n- The use of GMM is interesting"
            },
            "weaknesses": {
                "value": "- **The description of the proposed method can be hard to read at times:**  \nI know a lot about this particular area of research and I still struggled to follow the presentation of Section 3 (the technical contribution section). To this end, in my opinion, the paper will really benefit from a paragraph (probably coupled with a summary figure) that summarizes the steps of the proposed method early on in Section 3, so that it is easier to follow what the paper is trying to achieve through the different subsections of Section 3. It needs not be long, consider something like the beginning of my paper summary above. Similarly, presenting the full algorithm at the end of Section 3 will help a lot in understanding how the different pieces of the algorithm fit together. Further, the paper will also benefit from giving more intuitive explanations of its steps throughout. One example of this will be to present Eq. 9 before Theorem 3 to make it intuitively clear where the GMM pieces come from in Eq.6. Finally, there are several key missing from Section 3. It should explicitly state that the GMMs and the regression model on $\\|\\mathcal{T}\\|$ need to be fitted on auxiliary data, and how estimating the number of unique tokens from the regression model is used together with the ranking to provide the set of recovered tokens. It should also state that during the LLM training, when multiple tokens are predicted, the CE loss is summed across all of them which mathematically is equivalent to the label recovery from a large batch. \n- **Citating and comparing to prior work:**  \nThe paper should cite and compare against prior label reconstruction attacks outside of RLG [5]. In particular, [1] can be used for recovering the set of unique tokens, while [2,3] can be used to recover the counts as well. Comparing against [1-3] is absolutely crucial, in my opinion, for accepting this paper, as those methods would work fast for large vocabularies and long sentences, unlike RLG, and have been shown to be effective at recovering labels to very good accuracy. Further, [2], in particular, is very closely related to FLAT-CHAT, as it derives the same \"flattening\" operation the authors claim as a contribution in the text. To this end, the authors should not claim the flattening operation as a contribution and instead clearly mark the derivation presented there as equivalent to the one made in [2].\nGiven the similarities to prior work, the authors should also consider including an explicit discussion of how their method differs from prior work. Finally, the authors acknowledge that FILM [4] can be applied to the same problem the authors consider but from the input side of the network. Yet, they do not provide a comparison. While beating it is not required for acceptance (due to the different requirements the attacker has), comparing against it is a good idea.\n- **The attack setting:**   \nLabel leakage attacks like [2] and [3], are capable not only of recovering the set of unique tokens in input data but also their counts. The authors should provide a discussion on whether counts are important from LLM privacy point of view.   \nFurther, the authors should better motivate their attacker's goal in general. While privacy is indeed violated by knowing the set of tokens fed to the network from a purely theoretical point of view, I would reasonably think that a large percent of the vocabulary tokens occur in a large batch of long excerpts of text anyway, and when the recovery has a precision of 0.85 and recall of 0.5 it will be very hard from a practical perspective to gain any reasonable sensitive information. That is, I expect the rank of rare tokens, which tend to be more private, to be lower in your method due to their lower occurrence rates. I also expect that the recall will be much lower than 0.5 for labels that are in the middle of the ranking.  Thus, in such a situation, the attacker will obtain that words like \"the\", \"I\", \"you\" are present in the batch with high accuracy, but will rarely obtain, let's say, a phone number. The problem gets even worse when considering the fact that LLMs are trained on tokens and not full words.\n- **Bad evaluation results:**  \nThe results shown in the experiment do not convince me in the superiority of the proposed method. In particular, RLG consistently and by big margins results in better reconstructions than FLAT-CHAT if RLG is in the mode where it is applicable (\\|\\mathcal{T}\\| < D). [1-3], which do not have such restrictions and tend to work much faster than RLG, might, therefore, turn out to be much better than FLATCHAT.  \n Even outside of these concerns, I find the precision of 0.85 and the recall of 0.5 in Table 2 and the 0.7 precision and 0.85 recall numbers in Table 3 not that convincing in terms of their practical attack relevance as outlined above. \n- **Suggestions:**  \n1. Essentially, the method proposes to model the $p_{i,j}$ as Gaussian distribution, which as $\\|\\mathcal{B}\\|->\\infty$ get closer to the truth but since $0\\leq p_{i,j}\\leq 1$ is a probability the approximation for finite $\\|\\mathcal{B}\\|$ is very bad. This is also reflected in the authors' shown negative clusters in the figures of Appendix A. The authors can consider modeling the $\\log p_{i,j} $ as Gaussian ( but $\\alpha_i$ still as Gaussian ). In my quick tests, this reflected the shown negative cluster pdf shapes much better. \n2. The authors propose to use Equation 12 as a ranking function. If a proper prior $p(s_t|t \\in \\mathcal{B})$ is used, Equation 12 can be used as a decision criterion instead, which will eliminate the need for using regression to fit $\\|\\mathcal{T}\\|$. This can possibly improve the performance of the method.\n- **Nits:**   \n1. In the first part of Eq. 13, $\\sigma_n$ and $\\sigma_p$ should be switched in the normalization constants of the Gaussians\n2. Equations 6 and 9 assume sum instead of mean gradient aggregation. Equation 7 assumes a  mean instead of sum. This needs to be made consistent throughout the paper."
            },
            "questions": {
                "value": "- [Crucial] Can the authors provide a comparison to [1-3]? Can the authors provide an explanation of why they are better than [1-3] if they are?\n- Can you provide a comparison to FILM [4]? \n- Can you explain what auxiliary data was used to obtain the parameters of FLAT-CHAT in Table 2 (Machine Translation) experiments?\n- Can you explain why the precision and recall numbers between Tables 2 and 3 differ that much?\n- Can the authors explain why approximating $\\|\\mathcal{T}\\|$ separately is needed? Wouldn't using the optimal Bayesian criterion with a prior ratio of $\\frac{\\|\\mathcal{B}\\|}{(\\|\\mathcal{V}\\|-1)\\|\\mathcal{B}\\|}$ be sufficient?\n- Can the approximation of $\\|\\mathcal{T}\\|$ be improved by using some of the methods in [1-3] - it seems that currently, the approximation is far from perfect, to the point it has a few % difference on the final performance?\n- Can the authors explain the reasoning behind the Abs baseline in Appendix C? Seems that what the authors propose there is very similar to [1] - what are the similarities and differences?\n- Can you provide precise runtimes of the proposed method and baselines?\n- [Not so important] Can the authors run their experiments on a newer open-source LLM like Llama [6] or Chinchilla [7]?\n- [Not so important] Can the authors adapt their method to model the probabilities $p_{i,j}$ with a Log-Gaussian distribution? \n\nAll in all, the paper suffers from too many issues to be accepted right now. First and most importantly, it fails to compare to relevant prior work that has a reasonable chance to work better in practice than the proposed method and claims as a contribution the derivation of the \"flattening\" operation on the gradient despite the fact it is known. Second, the paper is hard to follow due to a lack of method summary and intuitive explanations. Finally, the paper needs to spend more time justifying the problem setting and their results in the context of this setting, as currently, I am not sure if the privacy concerns raised by the proposed attack are realistic.\n\n[1] Yin, Hongxu, et al. \"See through gradients: Image batch recovery via gradinversion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.  \n[2] Wainakh, Aidmar, et al. \"User-level label leakage from gradients in federated learning.\" arXiv preprint arXiv:2105.09369 (2021).  \n[3] Geng, Jiahui, et al. \"Towards general deep leakage in federated learning.\" arXiv preprint arXiv:2110.09074 (2021).  \n[4] Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. 2022. Recovering private text in federated learning of language models. In Advances in Neural Information Processing Systems  \n[5] Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter Chin, and Fran\u00e7oise Beaufays, 2021. Revealing and protecting labels in distributed training. Advances in Neural Information Processing Systems, 34:1727\u20131738.\n[6] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023).   \n[7] Hoffmann, Jordan, et al. \"Training compute-optimal large language models.\" arXiv preprint arXiv:2203.15556 (2022)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "It is not strictly needed but the paper will benefit from an Ethics statement where the authors can explain what the implications of the proposed attack are to real FL setups and emphasize the proposed solution of using differential privacy."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Reviewer_3Q3Y",
                    "ICLR.cc/2024/Conference/Submission9236/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661585352,
        "cdate": 1698661585352,
        "tmdate": 1699650263466,
        "mdate": 1699650263466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jdoyUi010e",
        "forum": "GrunXMbdXY",
        "replyto": "GrunXMbdXY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_CmSz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_CmSz"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel attack reconstructing client's tokens in federated model training.\nThe authors apply two-cluster Gaussian Mixture Model(GMM) to better classify the positive tokens (those involved in training) and negative tokens, and provide a theoretical analysis proving their attack effectiveness. \nExperiments on Language Modeling (LM) and Machine Translation (MT) show that FlatChat is more efficient and effective than previous method RLG.\nFinally, the authors apply two defenses, FREEZE and DP-SGD, to mitigate the attack, where the former one can hurt model utility and the latter one is found an ideal solution for both model privacy and utility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Interesting research problem**. Recovering exact user input from the uploaded gradient is challenging and even harder for language model because of discrete nature of texts, so the paper has good originality. \n\n2. **Attack with theoretical analysis**. This paper provides a new perspective from token distribution to infer the user's training texts in federated learning. The use of GMM permits infer trained tokens from gradients of a large batch of texts."
            },
            "weaknesses": {
                "value": "1. **Attack significance is low** because the order of tokens cannot be recovered. As the attack relies on gradient distribution of positive and negative tokens, the tokens' order information is hidden and not recovered. Although word distribution can leak partial privacy, in my opinion, this information is important to infer privacy underlying the training text. As a simple example, the two texts X = \"A is good, B is bad\", and Y = \"A is bad, B is good\" have the same word distribution but totally different meaning. I suggest the author to focus on or highlight specific scenarios where the word distribution can leak sufficient privacy. For example, it is possible to conduct an end-to-end case study showing how recovered tokens can lead to a more severe consequence.\n\n2. **The technical challenge is not clear.** As Fig.2 shows, most negative tokens have vector $s$ value between 0 and 0.02. While the I appreciate the authors' efforts in visualization, it makes me doubt whether the GMM is necesssary. As the next word prediction resembles to classification, a naive baseline can apply iDLG-similar approach to directly identify (for example, with threshold) the trained words (positive tokens). Note that iDLG also leverages the last layer's gradient to infer the labels of trained samples. In this sense, the GMM is only used to better classify positives and negatives. I suggest the authors to make the attack motivation and challenges more clear in the paper.\n\n3. **Problem importance is unclear.** From the main text, I cannot see that FL is a common solution for training/finetuning LMs, especially the large ones.  Although the authors have provided a long list of related works of training data inference attack in FL, I think it is still important to show that FL is or will be applied by organizations through real-world examples or case studies.\nThe only application I can imagine is using FL on mobile keyboard to predict the user's input behavior more accurately, but I'm not sure whether it trains such LMs. According to my experience, finetuning current LMs requires relative large memory, which is impractical to proceed on edge devices. \nPlease illustrate potential FL applications for LM training.\n\n4. **Comparison with more baselines is needed.** I note that in Table 1 a recent work FILM also infers trained words but is not compared in Section 4.2.1. I also notice that there is a slight difference between FILM and this work in terms of $\\Delta W$ but I think under FL setting the FILM can also work. Please consider compare with this attack or clarify why it is not suitable for comparison.\n\n5. **Defense (DP-SGD) can mitigate the attack, further reducing the attack significance.** To be honest, I'm quite surprised that small noises added by DP-SGD can mitigate the attack, which is different from the conclusion in (Gupta et al. 2022). This means that this previous attack is more powerful than proposed attack because DP-SGD can not defend it without degrading the model utility."
            },
            "questions": {
                "value": "Please see my concerns in weaknesses. Besides, I also have the following questions:\n\n1. What is the learning rate used in attack and DP-SGD? What is the resultant budget ($\\epsilon$, $\\delta$)?\n\n2. What does the 'Loss' in Figure 4 mean? Training loss or validation loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Reviewer_CmSz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752243340,
        "cdate": 1698752243340,
        "tmdate": 1699637161853,
        "mdate": 1699637161853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zWDcugufyP",
        "forum": "GrunXMbdXY",
        "replyto": "GrunXMbdXY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_t7LN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_t7LN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a privacy attack FLAT-Chat which recovers the set of words used in training a language model in the federated learning setting. The attack only assumes observing the gradients of the last linear layer instead of the embedding layer (as in previous work).  FLAT-Chat is inspired by the observation that the output layer gradients follow two distinct distributions for tokens used in v.s. not in training. Based on this, FLAT-Chat fits these two distributions with a two-mode Gaussian mixture, and then finds the cluster positive cluster where top K tokens are selected as the predicted training tokens. The attack is evaluated on machine translation and language modeling tasks on benchmark datasets and achieves much better attack efficiency than the previous attack Revealing Labels from Gradients (RLG)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This attack method is novel and based on an interesting empirical observation that the gradient norm distribution is a mixture model and these two mixtures correspond to tokens in/out of the training batch.\n- The attack is highly efficient and accurate as shown in Table 2, where an adversary can easily mount this attack to learn the tokens from users, demonstrating a realistic privacy concern."
            },
            "weaknesses": {
                "value": "- Some writings can be simplified, e.g. the lemmas and their proofs in Section 3.2 are simple rearrangement using some basic linear algebra which can be condensed in Equations and will not impact their readability. The theorems and the body texts are interleaved which makes the explanation of the attack less easy to follow. \n- In common practice, when training language models, the parameters of the embedding layer and the last layer are typically shared, i.e. they have the same gradients. It would be a stronger attack if this more common scenario is considered.\n- The attack is limited to inferring the bag of words while the order of the words cannot be recovered."
            },
            "questions": {
                "value": "- How would tying the weights between input embedding and output layer change the performance of the attack?\n- Another potential defense is secure aggregation, where the server can only observe the aggregated gradients instead of individual\u2019s. How might this impact the attack? Could the adversary still infer useful information when the set of participants is large?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772914163,
        "cdate": 1698772914163,
        "tmdate": 1699637161731,
        "mdate": 1699637161731,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ojXymNdliu",
        "forum": "GrunXMbdXY",
        "replyto": "GrunXMbdXY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_2HVX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9236/Reviewer_2HVX"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the recovery of the set of words used during federated training of a large language model for the tasks of language modeling and machine translation. The paper proposes an attack, known as \u201cFlat-Chat\u201d, which is able to extract the set of words from the last linear layer\u2019s gradients. To do so, Flat Chat transforms last linear layer gradients and uses a gaussian mixture model to form two clusters (positive/negative), corresponding to tokens which (are/are not) used in the batch, respectively. \nThe paper also proposes two defenses (freezing and DP-SGD) against the attack."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is easy to understand, and the methodology is novel and intuitive. The proposed method demonstrates performant recovery of a majority of tokens from the last linear layer, demonstrating significant leakage of tokens which does not depend on gradients of input embeddings."
            },
            "weaknesses": {
                "value": "Experimental results could be more comprehensive. In particular, more exploration (e.g. of larger batch sizes) would establish the failure mode of the approach.\n\nI am also curious about the gaussian mixture model of the word types; Does the frequency of each word in the batch impact the quality of the fit? Experiments that demonstrate robustness in this scenario would be helpful in establishing generality of the approach.\n\nFinally, further experiments which show performance of freeze/dp-sgd for language modelling would also help contextualize the benefits and drawbacks of the proposed defenses."
            },
            "questions": {
                "value": "* I am a bit confused why results for Scratch with the task of Large Language Modelling are not included\n\n* Is the gaussian mixture model accurate at every epoch of fine-tuning? Or is it only at the first epoch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9236/Reviewer_2HVX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827763037,
        "cdate": 1698827763037,
        "tmdate": 1699637161611,
        "mdate": 1699637161611,
        "license": "CC BY 4.0",
        "version": 2
    }
]