[
    {
        "id": "FBcioE8uG4",
        "forum": "YkEW5TabYN",
        "replyto": "YkEW5TabYN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_nq6A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_nq6A"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new framework to measure the difference behavior between two models, with one of them being a reference model and the other one being the target model. Unlike previous work which uses benchmark performance or model prediction agreement, this framework proposes to analyze the shared invariance of the two models when they are given an original input vs. pertubed input which targets specific lingusitic capabilities (i.e., synonym-invariance and typo-invariance).\n\nExperiment results reveal several key findings that (1) the performance gap between models (existing metrics) doesn't always reflect the shared behaviors (invariance) between them, especially where a smaller model is used as a reference model and that (2) larger models tend to share more invariances. The proposed framework closes the first gap by measuring the shared behaviors (invariance) between models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- With the growing popularity of large language models (LLMs) and lots of efforts to make them more efficient, I think the proposed framework would be useful especially for making sure that a particular iteration of a model will have similar or improved behavior compared to the base model. From the analysis, the framework can provide complementary analysis beyond existing metrics.\n- Well-executed experiments, with three types of pertubation, and comparison with existing metrics from previous work.\n- Application for both specific model (fine-tuned BERT) and more recent generative models (GPT variants)."
            },
            "weaknesses": {
                "value": "- Presentation: some of the details that are important seems to be missing (or rather put in the Appendix). See more details below."
            },
            "questions": {
                "value": "- Section 3: \"four independent components\" -> I am not sure if these four are really independent, does transformation and constraint dependent on each other?\n- I'm a bit puzzled with the results of BERT-Tiny in Fig. 4, especially regarding the poor OOD-agreement but high invariances. Does this mean that the predictions of both models are quite different (hence poor agreement), but within the individual model the predictions do not change when it is given original and pertubed samples?\n- Is there any interesting findings on the model agreement on original input vs. pertubed input?\n\n**Suggestions for presentation improvements**\n- Mention the tasks being used in the experiments at the beginning of the paper. It was not clear until Section 3.3 that you focus on text classification and language modeling.\n- Would be helpful to add more details regarding how the pertubed input is constructed for each linguistic capability. It is currently mentioned as the limitation (inefficient search methods), but i is unclear how inefficient since there is no explanation about it.\n- Notation definition: more explicit in assigning $m_1$ and $m_2$, currently it only mentions $m_1$ as a reference model and I need to re-read in multiple place to infer what is $m_2$.\n- I think the results for fairness capability is quite interesting. If there is space, might consider to put it in the main text.\n- A relevant paper: https://arxiv.org/pdf/1711.02173.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708564235,
        "cdate": 1698708564235,
        "tmdate": 1699636894431,
        "mdate": 1699636894431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QE6ADlzjRH",
        "forum": "YkEW5TabYN",
        "replyto": "YkEW5TabYN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_xMdc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_xMdc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes invariance-based metrics for capturing similarities between NLP models. Here, invariance-based metrics are contrasted with agreement-based metrics. To compute the proposed invariance-based metrics, one chooses a reference model (Ref) and constructs a dataset of (x, x\u2019) invariant pairs. Here, x is the \u201cbase\u201d, and x\u2019 is the \u201cperturbed\u201d example, which is chosen as a perturbation of x that yields minimal change to Ref outputs. Two perturbation mechanisms that yield x\u2019 candidates are considered, namely inserting typos and replacing words with synonyms. Given the (x, x\u2019) pairs from Ref and a Target model (Tgt), one computes the similarity between how perturbing x to x\u2019 changes Tgt behavior. This is contrasted to OOD agreement between Ref and Tgt, which would involve directly comparing the models\u2019 predictions Ref(x\u2019) and Tgt(x\u2019) to each other. Instead, in the proposed invariance-based metrics Tgt(x\u2019) is compared to Tgt(x). The paper proposes a hard and a soft metric, that are called Hard- SCoPE and Soft-SCoPE respectively. \n\nThe key message of the paper is that Soft-SCoPE sometimes does not correlate with OOD agreement. From this the paper infers that invariance-based metrics can offer complementary insights that agreement-based metrics may miss. For example, the paper shows how distilling BERT into Distill-BERT considerably decreases the Soft-SCoPE metric."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper aspires to address an important challenge of quantifying similarities and differences between different language models."
            },
            "weaknesses": {
                "value": "In my opinion, the paper fails to present a clear motivation and justification for the proposed metrics. For an auxiliary metric to be of interest to a practitioner, it needs to be predictive of what the practitioner cares about. In particular, the abstract of the paper refers to learning about \u201cdifferences in model performance in the wild\u201d as the motivation. I did not find an explanation of how measuring the proposed invariance-based metrics for pairs of models can help us get a better idea of the models\u2019 \u201cperformance in the wild\u201d. The main justification for the proposed metrics is that they don\u2019t correlate with OOD agreement. But one can\u2019t justify a new metric just by saying it\u2019s different from existing ones - it is a logical fallacy. \n\nI also found the paper rather difficult to follow. Here are some concerns as bullet items:\n- Is a notation for the source of perturbed candidates x\u2019 missing from Equation 1?\n- A table showing examples of (x, x\u2019) pairs is a must for a paper like this. \n- Many complex sentences and reasoning chains make the paper hard to follow. Example (page 5): \u201cThus, measuring behavioral shared invariances amounts to quantifying the extent to which a target model is invariant on perturbations that do not cause any change in the reference model\u2019s behavior (ref Eq. 1).\u201d"
            },
            "questions": {
                "value": "Why would a practitioner trust your metrics more than OOD-agreement to predict e.g. how much BERT distillation hurts the model\u2019s robustness to typos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717793701,
        "cdate": 1698717793701,
        "tmdate": 1699636894315,
        "mdate": 1699636894315,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5mUcxpK8yZ",
        "forum": "YkEW5TabYN",
        "replyto": "YkEW5TabYN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_cey2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_cey2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SCoPE, a measure of invariance between language models (LMs). Unlike naive pointwise-agreement measures (that simply measure the fraction of inputs on which two models agree), SCoPE measures the fraction of robust (perturbation-robust) inputs on which models differ. As the perturbations are capability-defined, SCoPE is a measure targeted at a particular capability. Details follow.\n\nGiven two datasets $X$ and $X'$, a label set $Y$, a model $m\\colon X \\cup X' \\to \\mathbb{R}^n$ and a loss function $\\ell \\colon \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$, we define a _perturbation_ $p\\colon X \\to X'$ by mapping each $x \\in X$ to $x'\\in X'$ that minimizes $\\ell(m(x), m(x'))$. [Note: In the paper it there is a linguistic capability perturbing the inputs, though I am unclear on how this is captured formally by the definition and so cannot include it in my summary.] Moving forward, we will denote by $y_m(x)$ the label of $x$ as predicted by $m$, that is, $y_m(x) = \\mathrm{argmax}_{k \\in [n]}m(x)_k$.\n\nGiven an input perturbation $x \\to x'$ and two models $m_1$ and $m_2$, the Hard-SCoPE distance from $m_1$ to $m_2$ is defined by $\\Pr_{x \\in X}[y_{m_2}(x) = y_{m_2}(p(x)) \\vert y_{m_1}(x) = y_{m_1}(p(x))]$. In plain words, we can say that Hard-SCoPE measures the degree to which inputs on which $m_1$ is perturbation-invariant, are also perturbation-invariant for $m_2$. \n\nA variant of Hard-SCoPE, called Soft-SCoPE, is defined to be $f(m_1, m_2) \\cdot \\mathrm{Hard-SCoPE}$. The authors define $f(m_1, m_2) = \\mathrm{decay}(\\mathrm(dist)(\\Delta \\vec{m}_1, \\Delta \\vec{m}_2))$, however $\\Delta\\vec{m}$ is not defined and so I do not know how Soft-SCoPE is defined.\n\n[Note: I did not understand the formal notation in the paper, so applied my own here to the best of my ability.]\n\nThe authors evaluate the utility of their measures by conducting various experiments. The perturbations are either synonym-based (\"A man laughs out loud\" $\\to$ \"A man laughs out loudly\") or typo-based (a permutation of all but the first and last characters of a single word). Results are:\n\n**BERT finetuned on sentiment analysis (Section 4)**\n1. Between BERT (as the reference model $m_1$) and DistilBERT (as the target model $m_2$), SCoPE measures are higher for synonym-perturbations vs. typo-perturbations.\n2. When comparing BERT-Tiny (as reference model) with synonym-perturbations to five BERT variants of different sizes, Hard-SCoPE _increases_ with model size, and Soft-SCope stays roughly the same. This is despite the fact that pointwise agreement on the perturbed dataset (\"OOD Agreement\") correlates with model size, i.e., follows the opposite trend of Hard-SCoPE.\n3. When repeating the previous experiment with BERT-Base as a reference model, Hard-SCoPE and Soft-SCoPE now _do correlate_ with model size.\n\n**GPT-2 vs. InstructGPT (Section 5)**\nIn the following, GPT-2 is used as the reference model against variants of InstructGPT\n1. `text-davinci-001` has higher Hard and Soft-SCoPE than its smaller variants `text-ada-001`, `text-curie-001` and `text-babbage-001`.\n2. `text-davinci-003` has higher Hard and Soft-SCoPE than `text-davinci-001` and `text-davinci-002`."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of measuring similarity of models by evaluating their agreement on perturbation-invariant inputs is, to my knowledge, novel.\n- As opposed to new benchmarks or tasks, this measure can be computed based only on outputs (or logits) of the model, and is generic enough that it can be instantiated on any dataset (and its perturbation).\n- The choice of models for experimentation provides an appealing setting, namely, understanding the effect of model size on robustness to perturbations."
            },
            "weaknesses": {
                "value": "### Significant Issues with SCoPE definition\nThere are numerous issues with the presentation of the definition of Hard-SCoPE and Soft-SCoPE. Considering that the new measure is the main contribution made in this paper, this is a significant weakness of this work. I could not attempt to infer the definition from its implementation, since the submission was not supplemented by code.\n\n1. The notation $\\Delta \\vec{m}$ is never formally defined. Consequently, it is impossible to understand the definition of Soft-SCoPE.\n2. In Hard-SCoPE and Soft-SCoPE, the expectation is said to be taken over $(x, x') \\in (X, X')$. The domain $(X, X')$ is not defined. My first interpretation was that this was a cartesian product $X \\times X'$, which means that $x, x'$ are taken iid--but this does not seem to align with the textual description of SCoPE. See my summary for an alternative way to denote taking $x$ and its corresponding perturbation.\n3. In general, I do not follow (or do not agree with) the definition of a perturbation in this paper. For example, in page 4 you define $X'$ to be the image of $X$ under $C(\\cdot, m)$. But this does not retain the mapping of inputs $x$ to their perturbations $C(x, m)$. Furthermore, $C$ is not well-defined, since the domain of the argmin is not specified. If we take the domain to be $X$, and read the follwing paragram in which you say that you take the loss function to be $\\ell_1$-norm difference, then we have that $C$ ends up being the identity function (assuming the loss has no non-trivial zeroes).\n4. The expectation of an indicator random variable is simply the probability over this event. It would be significantly more readable to use probabilities in the notation, as I did in may summary.\n5. In Soft-SCoPE, since the attenuating factor $\\mathrm{decay}(\\mathrm{dist}(\\cdot, \\cdot))$ seems to be constant with respect to the random variables, it can be taken out of the expectation. Therefore, Soft-SCoPE is just Hard-SCoPE multiplied by a factor that depends on $m_1$ and $m_2$ (in an undefined way, see item 1 above).\n6. In page 6 it is written that \"Hard-SCoPE can only take binary values i.e., 0 or 1\". My understanding of this sentence is that the Hard-SCoPE of two models can be either 0, or 1. But this is not consistent with the remainder of the text.\n\n### Experiments lack depth\nThe goal of the experiments should be to argue for the utility of the novel measure introduced in this paper. This can be done, for example, by using this measure to obtain novel and interesting insight, or by dissecting the measure with fine-grained experiments.\n\nIn section 3, the authors compare the measure to naive pointwise measure (IID/OOD agreement). The main body of experiments (Sections 4 and 5) are conducted by taking different pairs of models and measuring Soft-SCoPE, Hard-SCoPE, and pointwise agreement. The experiments are then displayed with bar charts, and there is a textual description of the results. I found the results to be fairly superficial: the correlations are either expected (BERT-Base SCoPE correlates with model size), or uninterpretable (why is this not the case for BERT-Tiny?). When deeper conclusions are reached for, it is at times done in a speculative manner (\"we hypothesize\").\n\nI unfortunately do not have concrete suggestions for a more refined experimental setup that will give more insightful results, in part because I could not understand the definition of the measure itself.\n\n### Choice of \"capabilities\"\nIn the body of the paper, the perturbations are based on synonyms or permutations of characters in a word. I found the usage of the term \"capability\" to be confusing in this context. In NLP, I think of a capability as a task, such as sentiment analysis or summarization. Are these perturbations somehow related to capabilities as they are more generally used?\n\nAdditionally, were the choices of these pertubations arbitrary, or is there something about them that makes the particularly appropriate for exploring the newly-defined SCoPE measure? It seems to me that the results in Sections 4 and 5 could have been reported out all the same with other perturbations---this speaks to the genericness of the results (see previous weakness).\n\n### Figure 5\nI could not understand the diagram in the left part of Figure 5, despite making several attempts on different days. The diagram on the right would be clearer if presented by a table, as there are onalya three data points.\n\n### Additional / minor issues\n- Commas before / after \"i.e.\" are inconsistent throughout the paper. There are different style guides for American vs. British English regarding this matter, but please pick a dialect and stick to it.\n- Page 3: What is the argmin over? Presumably, over $x' \\in X'$, but then $X'$ uses $C$ in its definition in the following page! \n- Page 4: \"and constraint modifications of words that are stopwords\". Did you mean \"constrain\" (a verb) rather than \"constraint\" (a noun)?\n- Page 4: There is a grammatical issue with the sentence starting with \"We perform experiments along one...\"\n- Page 4: What is the argmax over? Presumably, over $k \\in [c]$. It would be more readable to make this equation centered rather than inline.\n- Page 4: The definition of $X'$ should use set builder notation: replace the $\\forall$ with a colon or a vertical line.\n- Page 5: In the definition of the acronaym you write **SH**ared-**C**-apabilities... But the **H** does not appear in the acronym. Furthermore, there is no need to hyphenate the definition.\n- Page 5: \"(ref Eq. 1)\". Is the \"ref\" a typo? There is no such common shorthand, to my knowledge. Use \"see\" instead.\n- Page 6: Use \\mathrm{} for operators such as \"decay\" and \"dist\".\n- Page 6: I suggest using the term \"monotonically decreasing\" rather than \"has a downward slope\".\n- Page 7: In the first paragraph, there are several issues with citations and parentheses. There should probably not be sequences of parentheses such as \"))\" or \")(\"."
            },
            "questions": {
                "value": "I welcome responses and answers to any of the questions raised in the previous sections of my review. Besides these, I have two outstanding questions.\n\n### What is the cost of computing SCoPE?\nI could not find the computational costs reported in the paper or in the appendices. Since this paper proposes a new measure, it would be useful for the reader to know how expensive it is to compute. In particular, for the experiments in Section 5, the authors evaluate black-box models by taking random samples from their output; what is the monetary barrier to reproducing these experiments? Is there a compute-accuracy tradeoff inherent to this method?\n\n### What is behavioral about the invariances?\nThe paper makes extensive references to \"behavioral invariances\". What is \"behavioral\" about these invariances? Is this term used in other parts of the literature, to distinguish thes from other kinds of invariances? \"Behavior\" alludes to some \"cognitive\" or grounded aspect of the model, or at the very least to its semantics, while typo-invariance is clearly a syntactic phenomenon. I think that using just \"invariance\" would be clearer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7449/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7449/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7449/Reviewer_cey2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782062365,
        "cdate": 1698782062365,
        "tmdate": 1700609624665,
        "mdate": 1700609624665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o1z5EB6ftX",
        "forum": "YkEW5TabYN",
        "replyto": "YkEW5TabYN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_yXpb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7449/Reviewer_yXpb"
        ],
        "content": {
            "summary": {
                "value": "The paper studies invariances in different semantic or linguistic features across LLM representations. The authors propose a novel metric study for evaluating the invariance in LLMs. Paper includes extensive experiments across metrics and models. It would be a good fit for the conference and informative contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper includes extensive comparison across existing metrics and prominent LLMs to help understand their capabilities."
            },
            "weaknesses": {
                "value": "Description of experimental methodology needs to be clarified."
            },
            "questions": {
                "value": "IID not defined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7449/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856697391,
        "cdate": 1698856697391,
        "tmdate": 1699636894073,
        "mdate": 1699636894073,
        "license": "CC BY 4.0",
        "version": 2
    }
]