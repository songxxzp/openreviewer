[
    {
        "id": "tCRhqhdu2x",
        "forum": "fQxLgR9gx7",
        "replyto": "fQxLgR9gx7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a P4LM framework that provides recommendations, taking into account the precision, personalization, appeal, and preference relevance of items to users. Reward functions are crafted for each perspective, involving the training of reward models based on AI-labeled data derived from PaLM2-LLM. Subsequently, a joint reward function is formulated, employing reinforcement learning to master the text generation policy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Overall, the authors present insightful perspectives for evaluating the effectiveness of conversational Recommender Systems (RS).\n\n- The paper articulates the overall process with clarity, making the framework straightforward to implement.\n\n- Including human evaluation, the paper offers intriguing insights into the reward hacking issue encountered during training with a singular reward."
            },
            "weaknesses": {
                "value": "- While the authors' perspectives on assessing model effectiveness are noteworthy, the rationale behind the proposed framework's efficacy in enhancing precision, personalization, appeal, and user preference relevance remains ambiguous. The reward functions, trained via annotations from an LLM model, may not necessarily echo the authentic experiences of users. Moreover, practical recommendation scenarios are complex, and it is uncertain whether these nuances are effectively captured by a reward model.\n\n- The experiments conducted primarily assess the model concerning the targeted reward functions, showing that incorporating RL improves performance on these specific metrics\u2014a finding that is somewhat predictable. It would be advisable to include human evaluations when assessing baseline methods to more convincingly demonstrate practical effectiveness.\n\n- The paper's technical contribution seems limited and under-assessed. The crux of the proposed method's novelty resides in the design of LLM-based reward functions. However, the validation of these reward functions is lacking, and no clear insights are given on how these rewards enhance recommendations. This omission leaves the paper's contributions indistinct.\n\n- The experimental validations are not comprehensive. The reliance on a single dataset, along with a comparison with only three baselines\u2014two of which are merely variants, and the other, the foundational model also used for data generation. There is a need for broader evaluations against additional baselines, datasets, and models to affirm the method's effectiveness.\n\n- In Table 1, the evaluation scores for preference relevance metrics fall below those of other baselines, casting doubt on the assertion of superiorly elucidating project characteristics and their ties with user preferences. This discrepancy warrants an explanation to reconcile the claims with the empirical data."
            },
            "questions": {
                "value": "Please refer to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3163/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3163/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3163/Reviewer_SpKz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698585306339,
        "cdate": 1698585306339,
        "tmdate": 1699636263781,
        "mdate": 1699636263781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7W3hm0oQ0G",
        "forum": "fQxLgR9gx7",
        "replyto": "fQxLgR9gx7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_VcLv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_VcLv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called P4LM that generates personalized narratives of an item. It would be useful to be equipped by a conversational RS to enhance user experience.\n\nThe model incorporates the (user and item) embedding spaces of a recommender system, and use RLAIF (RL from AI feedback, the dataset they used to finetune LM or train reward model is generated from prompting a PaLM-L) to fine-tune a language model with reward models including precision, appeal, personalization, and preference relevance.\n\nThe method is evaluated on the MovieLens dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Generating personalized narrative is an interesting and useful problem, and seems unexplored in the literature before.\n2. The proposed RLAIF-based framework is general, and could be applied to other recommendation datasets."
            },
            "weaknesses": {
                "value": "1. There is no human evaluation on comparison among P4LM, P4LM-S, SFT, SFT-Text, PaLM-L. How do we know that P4LM is actually better than others in terms of real human feedbacks? Also, PaLM-L\u2019s samples are not provided in appendix.\n2. The authors only experiment on one dataset. I understand the complexity of the whole procedure, but it this paper would be much stronger if the proposed method could be validated another dataset.\n3. I didn't search very carefully, but the author didn't compare the proposed method with any baselines from other papers. Or if the problem is completely new (I am not sure), then this would not be an issue."
            },
            "questions": {
                "value": "1. What recommender system is used for extracting the embeddings? How does recomender system performance affect this task?\n2. How does P4LM and SFT take user/item embeddings as input? How does SFT take user and item descriptions as input? What is the detailed network design for these parts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763643518,
        "cdate": 1698763643518,
        "tmdate": 1699636263709,
        "mdate": 1699636263709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Grf3HWefm",
        "forum": "fQxLgR9gx7",
        "replyto": "fQxLgR9gx7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_wWN5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_wWN5"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the problem of language modeling for personalized recommendation, which aims to generate natural language responses that explain the relevance of recommended items to users\u2019 preferences. The paper proposes a novel approach called P4LM, which leverages reinforcement learning with AI feedback to fine-tune a pre-trained language model with four reward models that measure precision, appeal, personalization, and preference relevance of the generated responses. The paper demonstrates the effectiveness of P4LM on a conversational movie recommendation task, using the MovieLens 25M dataset. The paper shows that P4LM can generate factual, personalized, and compelling movie endorsements that better align with users\u2019 preferences and item features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The paper proposes a novel approach to language modeling for personalized recommendation, which leverages reinforcement learning with AI feedback to fine-tune a pre-trained language model with four reward models that measure personalization, precision, appeal, and preference relevance. \nQuality: The paper is well-written and organized, with clear problem formulation, methodology, and evaluation. The paper provides sufficient background and related work on recommender systems, language models, and reinforcement learning. \nSignificance: The paper addresses a challenging and important problem of generating factual, personalized, and compelling recommendation endorsements that better explain item characteristics and their relevance to user preferences. The paper also has practical implications for enhancing the user experience and satisfaction in conversational recommender systems."
            },
            "weaknesses": {
                "value": "1) The paper does not explain why existing methods are insufficient or what are the specific challenges and opportunities in this domain.\n2) The paper provides a comprehensive literature review of related work, especially on conversational recommender systems, language models, and reinforcement learning. But it only cites those papers without discussing their strengths and limitations or comparing them with the proposed approach.\n3) Paper does not discuss the assumptions and limitations of the approach.\n4) The paper does not describe the implementation details and hyperparameters of the proposed approach, such as the size of the models, and the reinforcement learning algorithms. For example: \"where \u03b71, \u03b72, \u03b73, \u03b74 \u2265 0 are importance weights for the component rewards, and are treated as hyper-parameter\"\n5) The evaluation is not rigorous for an applied paper. One dataset is not enough to draw conclusions.\n6) The paper does not present any qualitative analysis or examples of the generated recommendation texts by the proposed approach. It only shows quantitative scores based on model-based metrics, which may not reflect the true quality and diversity of the texts.\n7) What are the advantages and disadvantages of using adapter layers to augment the language model?\n\n8) How does the P4LM model deal with cold-start problems, where there is not enough user or item information available?\n9) How does the P4LM model compare with other conversational recommender systems that use different language models or architectures?\n10) Can authors talk more about  trade-offs between different reward models, such as precision, appeal, personalization, and preference relevance"
            },
            "questions": {
                "value": "1) The paper does not explain why existing methods are insufficient or what are the specific challenges and opportunities in this domain.\n2) The paper provides a comprehensive literature review of related work, especially on conversational recommender systems, language models, and reinforcement learning. But it only cites those papers without discussing their strengths and limitations or comparing them with the proposed approach.\n3) Paper does not discuss the assumptions and limitations of the approach.\n4) The paper does not describe the implementation details and hyperparameters of the proposed approach, such as the size of the models, and the reinforcement learning algorithms. For example: \"where \u03b71, \u03b72, \u03b73, \u03b74 \u2265 0 are importance weights for the component rewards, and are treated as hyper-parameter\"\n5) The evaluation is not rigorous for an applied paper. One dataset is not enough to draw conclusions.\n6) The paper does not present any qualitative analysis or examples of the generated recommendation texts by the proposed approach. It only shows quantitative scores based on model-based metrics, which may not reflect the true quality and diversity of the texts.\n7) What are the advantages and disadvantages of using adapter layers to augment the language model?\n\n8) How does the P4LM model deal with cold-start problems, where there is not enough user or item information available?\n9) How does the P4LM model compare with other conversational recommender systems that use different language models or architectures?\n10) Can authors talk more about  trade-offs between different reward models, such as precision, appeal, personalization, and preference relevance"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699037639921,
        "cdate": 1699037639921,
        "tmdate": 1699636263653,
        "mdate": 1699636263653,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZLp2JHGyXp",
        "forum": "fQxLgR9gx7",
        "replyto": "fQxLgR9gx7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_6DoX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3163/Reviewer_6DoX"
        ],
        "content": {
            "summary": {
                "value": "This article proposes that individuals should pay greater attention to the attributes of compelling, precision, personalization, and preference relevance when engaging in the recommendation process. Furthermore, the article introduces the P4LM model as a means to achieve this objective. To this end, the authors have meticulously designed reward functions for each attribute and utilized Reinforcement Learning with Adaptive Importance Sampling (RLAIF) to fine-tune the PALM model. Experimental evaluations were conducted on a subset of the MovieLens dataset and validated the ability of their method to improve the model performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe proposed need for attention towards the compelling, precise, personalized, and preference-relevant directions for recommendation presented in this paper holds significant research significance.\n2.\tThe paper provides a comprehensive description of the methodology, experimental details, and the utilization of prompts."
            },
            "weaknesses": {
                "value": "1.\tAlthough the paper incorporates a plethora of metrics, the efficacy of these indicators in truly measuring the corresponding performance needs to be substantiated (see Questions for more details).\n2.\tThe baseline used by the author is too concise. It is a consensus in the LLM field that models that have undergone reinforcement learning have better rewards than SFT and pre-trained models. More recommendation-related baselines should be mentioned.\n3.\tThe article only collected data and trained on a closed-source model. The comparison of training on open-source models should be discussed.\n4.\tThe author mentioned multiple ways to use PaLM to construct synthetic data in the article, but the rationality of this method has not been verified. (see Questions for more details)"
            },
            "questions": {
                "value": "1.\tAccording to Table 3, we can see that three of the four indicators proposed by the author (Precision, Personalization and Appeal) do not change significantly in different settings. On the other hand, compared with Table 2, the ordering between different settings cannot be maintained. Consistent, does this indicate that the metrics proposed by the author cannot measure the corresponding generated features? Besides, the Pref. Relevance of all models is very high. Does this mean that this indicator does not have discrimination?\n2.\tAs I understand it, the author's model can generate recommended items and corresponding reasons. Can the author explain the advantages and disadvantages of recommended items compared to traditional recommendation models? I believe that comparable recommendation performance would be an acceptable outcome.\n3.\tThe author mentions in the text the utilization of PaLM to construct data for training the reward models of Personalization and Preference Relevance. One question arises: despite the strong capabilities of LLM, there seems to be no conclusive evidence to suggest their proficiency in accomplishing this task effectively. Can the author provide corresponding evidence through real-world data or human evaluation?\n4.\tDue to the wealth of global knowledge and generalization capabilities possessed by LLM, could P4LM, after undergoing direct training in the movie domain, be applicable to other domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699109165752,
        "cdate": 1699109165752,
        "tmdate": 1699636263572,
        "mdate": 1699636263572,
        "license": "CC BY 4.0",
        "version": 2
    }
]