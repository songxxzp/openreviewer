[
    {
        "id": "wggsssnKaW",
        "forum": "zRkM6UcA22",
        "replyto": "zRkM6UcA22",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission77/Reviewer_hFtS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission77/Reviewer_hFtS"
        ],
        "content": {
            "summary": {
                "value": "This paper defines a benchmark that covers a set of fields including occupancy, signed and unsigned, and radiance fields. The authors show that applying well established archtieures on triplanes achieves better results that processing neural fields realized as a large MLP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A benchmark for triplane neural field classification.\n\n2. The motivation of creating this benchmark is interesting and makes sense."
            },
            "weaknesses": {
                "value": "1. The paper is difficult to follow.\n\n2. The presentation has room to improve.\n\n3. The proposed method performs worse than existing point cloud methods as shown in Table 5.\n\n4. It is not obvious on the advantage of the proposed method over methods working with point clouds and / or meshes.\n\n5. How well does the proposed method generalize to unseen scenes?\n\n6. Does the proposed method have a faster run time compared to mesh / point cloud methods?"
            },
            "questions": {
                "value": "See the questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission77/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698041828818,
        "cdate": 1698041828818,
        "tmdate": 1699635932222,
        "mdate": 1699635932222,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KqNhZWMhcl",
        "forum": "zRkM6UcA22",
        "replyto": "zRkM6UcA22",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission77/Reviewer_fdy7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission77/Reviewer_fdy7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to use optimized triplanes for downstream tasks on neural fields, such as 3D object classification or part segmentation on various neural field types, such as unsigned distance fields, signed distance fields, occupancy fields, or radiance fields. It is shown that using triplanes in this scenario leads to a clearly better trade-off between reconstruction quality and downstream task accuracy, in comparison to previous works that utilize a latent code from a shared MLP or MLP weights as descriptors.\n\nFurther, the authors expose that architectures that are invariant to channel order in the fitted representations are important to achieve optimal accuracy in downstream tasks.\n\nAlso, the authors provide a benchmark for downstream tasks on triplane representations, which they plan to make publicy available."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of using fitted triplanes as embeddings for downstream tasks is simple but seems effective and has not been analyzed before to my knowledge.\n- The results clearly show the better trade-off compared to previous methods.\n- The method is evaluated on a diverse set of tasks and function representations.\n- The insight regarding channel invariance is interesting and leads to the conclusion that transformers are better than CNNs, which seem unintuitive at first.\n- The authors provide a benchmark datasets for the community to test architectures on.\n- The paper is well written and easy to understand."
            },
            "weaknesses": {
                "value": "- The idea of using fitted triplanes for downstream tasks like classification is \"obvious\" in a sense.\n- There is the general question of what the relevant application of the proposed approach might be. This is a problem for all methods that aim to solve downstream tasks on optimized neural field representations. Usually, data (images or point clouds, etc) was used to obtain the neural field in the first place. Solving the downstream tasks on this input representation instead of the neural field usually leads to better results, which is also confirmed by an experiment in the paper. The gap is reduced a lot though, in comparison to previous works.\n- I think the term \"universal neural field classifier\" (which the authors claim their method to be) is misleading, as the method is not for all neural fields but only for those represented as triplanes."
            },
            "questions": {
                "value": "The use of instance-specific MLPs as decoders for triplanes makes sense in terms of reconstruction quality. However, this also leads to some information being represented in the MLP, instead of the triplane. I wonder how the downstream task quality is changing when a shared MLP is used. This experiment seems to be missing in the paper and I would be interested to see the comparison.\n\n---------\nI thank the authors for the replies to my concerns and also for elaborating on the general motivation of the research direction.\nThe main critique of other reviewers seem to go into a similar direction - questioning the usefulness of the proposed approach. I still agree to some degree but I also see that there might be potential applications in the future. That aside, I think this paper does something novel and analyses it well, which is why I will keep my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Reviewer_fdy7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission77/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646289298,
        "cdate": 1698646289298,
        "tmdate": 1700756507140,
        "mdate": 1700756507140,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JWMpGfJKlB",
        "forum": "zRkM6UcA22",
        "replyto": "zRkM6UcA22",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission77/Reviewer_ue4V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission77/Reviewer_ue4V"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the utilization of learned neural fields as a means of representing objects for classification and part segmentation tasks. The authors propose a hybrid NeRF (Neural Radiance Field) that combines a tri-planes data structure with an MLP (Multi-Layer Perceptron) for object encoding. This hybrid approach encompasses various fields representations, including Sign/Unsigned Distance Functions (SDF/UDF), Occupancy, and Radiance fields.\n\nThe experiments conducted in the study reveal that the representation learned through tri-plane parameters remains nearly identical (up to channel permutation) even when the NeRF is trained on the same data but with different random initializations. This robustness greatly simplifies the deployment of this method in comparison to previous approaches. Additionally, the experiments demonstrate that the classification and part segmentation performance achieved through tri-plane features is on par with specialized models designed for processing explicit object representations, such as point clouds or meshes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed approach introduces a versatile method for encoding object representations across various neural fields. \n\n- Classification using learned tri-plane features demonstrates superior performance compared to other existing NeRF encoding methods that rely solely on MLP parameters. \n\n- The authors also explore different techniques for reshaping tri-plane feature tensors to ensure that predictions remain invariant to channel permutations."
            },
            "weaknesses": {
                "value": "- The proposed method necessitates per-object optimization to acquire individual tri-plane features. The author conducted a comparative analysis of object reconstruction using this technique against other solutions, which employed a shared network trained on the entire dataset, like Functa (Dupont et al.). They reported the performance and the number of parameters (see Table 1). However, it is worth noting that the required computational resources, particularly in terms of training time, have not been explicitly reported or discussed. \n\n- In the ablation study, as presented in Table 6, the primary focus is on investigating various architectural aspects related to the classification of the learned tri-plane representations. While this provides valuable insights, it's important to highlight that certain variations within the proposed method, such as utilizing a shared MLP with distinct tri-planes across data, adjusting spatial resolution, or varying the number of channels within tri-plane structures, have not been subjected to ablation analysis. Addressing these aspects could provide a more comprehensive understanding of the method's performance and potential optimizations."
            },
            "questions": {
                "value": "- How do you explain the relatively lower performance of model when is trained and tested on the same neural field, like UDF and SDF as shown in Table 4? This seems to be counterintuitive.\n\n- As experiments show (e.g. Fig.3 right), the object representation mostly is encoded by the tri-plane parameters, rather than the MLP. This raises a question of whether the MLP can be shared across data samples for efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission77/Reviewer_ue4V"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission77/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758431056,
        "cdate": 1698758431056,
        "tmdate": 1699635932027,
        "mdate": 1699635932027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NRPlwPikYA",
        "forum": "zRkM6UcA22",
        "replyto": "zRkM6UcA22",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission77/Reviewer_3J4Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission77/Reviewer_3J4Y"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to encode discrete 3D information into the effective continuous triplane, allowing for the larger vision transformer to perceive in 3D. The paper validates their methods in both classification and segmentation tasks -- much better then previous version that used MLP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I like the topics this paper explores. Instead of directly consuming raw and messy 3D data, we could represent that data using neural representation, which will make the network design much easier. \n\nThe presented method achieves the significantly improved results compared with baselines. Although the proposed method is not novel -- simply replacing MLP with the more effective triplane, it shows better performance than the network that takes in raw discrete 3D dataset, which suggests a new paradigm to deal with 3D data."
            },
            "weaknesses": {
                "value": "Parameter and time efficiency comparison is missing. We know that triplanes work better than global MLP. However, there was no free lunch. Triplane-based is usually parameter-intensive. So I\u2019m concerned that the triplane based representation would consume lots of space compared with the original dataset. And the paper doesn\u2019t report any comparison. \n\nAlso I notice that the paper uses the explicit extracted from the learned triplane in the classification tasks of Table3. I\u2019m not very sure if it makes sense. Although the triplane is very effective, there's still information loss compared with original data. I would suggest the author justify it a bit. \n\nThe random initialization of triplane and MLP  is concerning. The reason is that the method uses the Sine/Cosine as the activation function whereas random initialization is not preferred as stated in other works. Instead, a specific way of random initialization was suggested in other papers like the SIREN paper. Also, the channel order with respect to the initialized value is not very clear. \n\nRegarding the dataset, I\u2019m not very sure what's ScanNet10 used in the paper. It seems unexplained. \n\nIn short, I tend to hold this paper and vote for weakly reject. I'm really looking forward to hearing back from authors during the rebuttal and clarify about my concerns."
            },
            "questions": {
                "value": "Please address the question above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission77/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801657160,
        "cdate": 1698801657160,
        "tmdate": 1699635931943,
        "mdate": 1699635931943,
        "license": "CC BY 4.0",
        "version": 2
    }
]