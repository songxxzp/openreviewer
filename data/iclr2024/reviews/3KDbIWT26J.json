[
    {
        "id": "efI5Unca3A",
        "forum": "3KDbIWT26J",
        "replyto": "3KDbIWT26J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_6ach"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_6ach"
        ],
        "content": {
            "summary": {
                "value": "This paper explored the reasons for the emergent translation capabilities of LLM from the perspective of data composition and provided empirical evidence to support its findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper innovatively delved into the emergent translation abilities of LLM by analyzing its training data composition."
            },
            "weaknesses": {
                "value": "The evaluation methods have its limitations.\n\nThere are flaws in the methodology, suggesting that the prompts might influence the results.\n\nAt its core, it's an ablation study at the data level."
            },
            "questions": {
                "value": "1. How can we be certain that it's related to numbers and punctuation? If we replace numbers with their unique expressions in different languages, such as 'one' for '1', 'two' for '2'; '\u58f9' for '1', '\u8d30' for '2' (Traditional Chinese); '\u4e00' for '1', '\u4e8c' for '2' (Simplified Chinese), ensuring that each language's corpus is entirely unrelated, would the translation capability still exist?\n\n2. Is the spontaneous alignment capability of large models based on the power of the attention framework? Regardless, does joint training of an LLM with two or more languages always generalize to translation capabilities, given a fixed number of languages in the data? This point is crucial for data augmentation methods in LLM.\n\n3. Compared to supervised NMT models, the translation capability of large models is more evident in their flexible interactions, producing translations that better meet requirements. This flexibility also leads to the unpredictability of the translation outputs from large models. Even a minor change in a prompt can result in different translations. In light of this paper, how do the authors view this issue?\n\n4. If possible, please consider using the 'Instruct Score' from EMNLP 2023 (Instructscore: Towards Explainable Text Generation Evaluation with Automatic Feedback) as a metric.\n\n5. The title is quite bold, but I don't believe the content of the paper fully justifies it. For instance, if the focus is on the data composition of LLM, deeper issues like data leakage (where training data includes test data or text similar to test data) should be considered. The 'bloomz' dataset, which is open-source and includes translation tasks, doesn't seem to have been addressed in the paper. LLMs are trained on vast amounts of data, so there's a need to check for (partial) overlap with the test set.\n\n6. Has there been any consideration that an excessive amount of bilingual data might dilute the pre-trained knowledge in LLM and impair its translation capability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3805/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3805/Reviewer_6ach",
                    "ICLR.cc/2024/Conference/Submission3805/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685078362,
        "cdate": 1698685078362,
        "tmdate": 1700676280038,
        "mdate": 1700676280038,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "soiLX4dfXh",
        "forum": "3KDbIWT26J",
        "replyto": "3KDbIWT26J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_stf2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_stf2"
        ],
        "content": {
            "summary": {
                "value": "This submission addresses an important and relevant question in recent machine translation research: how do large language models manage to perform so well on multiple language pairs without being trained specifically on bilingual (parallel) data? It outlines three different types of unintentional bilingual data (UBD): sentence and word alignment, as well as code switching, and analyse their influence on machine translation performance, suggesting that word-level alignment may play a larger role than was previously thought, possibly because it is much more prevalent than sentence alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The underlying question has been addressed before, suggesting that the presence of incidental sentence alignment in \"supposedly\" monolingual data plays a large role in LLM performance on MT. This submission pushes the investigation by looking at word-level alignment and code-switched data which as far as I know is novel.\n\nThe positive impact of word alignment is demonstrated in a number of experiments showing improvements in either translation quality or perplexity. The hypothesis that the larger amount of word alignment data (as compared to sentence-aligned) allows it to be as useful as sentence-aligned data is well supported by the experiments.\n\nMT quality is evaluated using neural metrics rather than BLEU, which aligns with recent results from the WMT22 metrics task (https://aclanthology.org/2022.wmt-1.2/).\n\nSome results are presented with significance tests of the differences, yay!"
            },
            "weaknesses": {
                "value": "The UBD (sentence & word aligned + code switched) is extracted using an automated methods with arbitrary parameters that seem fairly ad hoc (eg 10 BLEU poins threshold, appdx A). This raises the question of how good that data is... Are sentence-aligned segments even aligned sentences? At 10 BLEU point, this is not very clear. This data is at the core of the argument of the paper, better quality control would make a more convincing case.\n\nExperiments use mostly smaller models, as well as surrogate methods, and some models are not even converged. Clearly the amount of computation is significant. However, would one make conclusion on a chemical process from a reaction that has not completed? These imperfect or incomplete experimental conditions make the conclusions less convincing. The switch to perplexity does not help -- the claims in Section 5.3 are only mildly convincing if it is not possible to extract minimally useful translations from the models.\n\nSection 5.4 would benefit from a comparison with a model using UBD data somehow -- this would allow to gauge whether the performance in Table 8 is getting remotely close to acceptable translation quality."
            },
            "questions": {
                "value": "p.8 -- Shouldn't perplexity de lower in Table 9 (using UBD) than in Table 10 (purified) -- Can you comment on why it would be the opposite?\n\nSection 5.2: Although WA is better than WA-rand in Table 4, it is also inferior to both SA and CS, often by a large margin. Could you better support the claim that \"the effect of word alignment data is comparable or even superior to that of sentence alignment\"?\n\nAn additional final point in Section 5.6. could be that translation *into* low resource languages is noticeably worse than *from* low resource language into English, showing how important fluency is in the estimation of translation quality. With more EN data, models are able to produce better English output whereas low resource language output is less polished.\n\nMisc/typos:\n\"... translation quality is less comparable to NMT systems trained with parallel data.\" is a bit ambiguous as low-resource systems are typically trained on parallel data as well, but much less of it.\n\n\"other forms of unintentional bilingualism also plays\" -> play\n\n\"Sine\" (p.5) -> Since\n\n\"CS outperforms its X-rand counterparts\" could more clearly be \"CS outperforms CS-rand\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788341544,
        "cdate": 1698788341544,
        "tmdate": 1699636337851,
        "mdate": 1699636337851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8VSwKLyzGm",
        "forum": "3KDbIWT26J",
        "replyto": "3KDbIWT26J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_5xLr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_5xLr"
        ],
        "content": {
            "summary": {
                "value": "Multilingual large language model has shown impressive translation capabilities, while the model is trained on non-parallel data. This paper is motivated by a question of \"Why can multilingual large language model can learn to translate without parallel data?\" The authors report that there exist unintentional bilingualism in the training data that can be categorized into 3 types of bilingualism: 1) sentence alignment, 2) word alignment, and 3) code-switching and experimentally showed that, with these bilingualism, a large language model can learn to translate. Specifically, word alignment data provides the model with translation signals when compared with sentence alignment data. These findings will be helpful in data collection, data augmentation to improve translation capability of a large language model, since the translation ability lags behind a general translation model that is trained on parallel data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Extensive experiments are conducted, to address the main question of \"Why can multilingual large language model can learn to translate without parallel data?\"\n- The authors provide detailed empirical analyses to identify the underlying mechanism of translation capability in multilingual large language model. They also discuss the cases where single monolingual data are available with different sizes. This part might be helpful on how to improve low-resource language translation quality in the language model\n- The paper is well organized and clearly described in most parts."
            },
            "weaknesses": {
                "value": "- The authors focus on English and Chinese data to identify unintentional bilingualism types, and I was wondering if any other bilingualism types exist when you check the other languages.\n- This paper looks interesting in light of the empirical analyses of bilingualism's role in translation capabilities; however, the question still remains on how to effectively enhance translation capability further against the supervised translation models."
            },
            "questions": {
                "value": "- In Table 3, why is #seq smaller than #Doc? The paper describes \"We concatenate all the sentences in documents to form input sequences of fixed length\" and I assume that #seq would be larger in #Doc.\n- The authors focus on English and Chinese data to identify unintentional bilingualism types, and I was wondering if any other bilingualism types exist when you check the other languages.\n\n- In Tables 9 and 10, please consider rephrasing \"the translation performance\" since both tables report PPL, not general translation performance metrics such as BLEU or COMET. \n- n-gram -> $n$-gram"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3805/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3805/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3805/Reviewer_5xLr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809442650,
        "cdate": 1698809442650,
        "tmdate": 1699636337726,
        "mdate": 1699636337726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vkLY9MCt1u",
        "forum": "3KDbIWT26J",
        "replyto": "3KDbIWT26J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_ZA8F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3805/Reviewer_ZA8F"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the translation ability in multilingual LLMs, showing that the presence of sentence alignment, word alignment and code-switching data may contribute maybe an important role in explaining this amazing ability of multilingual LLMs. It is interesting that word alignment data yields a significant impact that surpasses the effect of sentence alignment data in certain cases (e.g. for post-training smaller-scale LLM). Despite several cons I see from the work, I think this work is a nice and very timing contribution to the community and thus recommend for an acceptance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The findings from the paper are interesting and may shed light on the amazing translation ability from LLM. I enjoy reading the work a lot."
            },
            "weaknesses": {
                "value": "My biggest concern about the work is that while it is true that maybe the presence of sentence alignment, word alignment and code-switching data contribute an important role in explaining this amazing ability of multilingual LLMs. We just don't know how important they are and is there any other reason (e.g. the presence of other stuff) that is actually even way more important than the three. I don't think the paper provide any data points on this.\n\nAnother smaller concern is that I see some paragraphs are just half-baked from curiosity perspective. For instance, section 5.4 shows that after eliminating unintentional bilingual data, the translation ability is still there, apparently. But there is no data points on why that happens, just some postulating about the reasons and that is it.\n\nThe final weakness point of the paper to me is the presentation in Section 5.5. I could not follow exactly what \"the shared transformer layers in the BLOOM-560m model\". To make the work self-contained I think the paper should at least present some high level details about the shared transfomer layers before presenting how this may influence the effect of unintentional bilingualism."
            },
            "questions": {
                "value": "- I get lost at random baseline in Table 6. Please elaborate more.\n- what is the precision/recall of \"WA\", \"SA\", \"CS\" classifier?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819142576,
        "cdate": 1698819142576,
        "tmdate": 1699636337641,
        "mdate": 1699636337641,
        "license": "CC BY 4.0",
        "version": 2
    }
]