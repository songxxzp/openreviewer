[
    {
        "id": "nspY3I2pwG",
        "forum": "yBZd6mCWXd",
        "replyto": "yBZd6mCWXd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_h7RY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_h7RY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a framework WI3D, using cost-effective 2D visual prompts for weakly class-incremental 3D object detection, which is an unexplored but important field. Under the supervision of intra- and inter- modal teacher in both feature space and output space, WI3D could effectively learn the novel classes while retaining the knowledge of base classes. Experiments conducted on SUN RGB-D and ScanNet show that WI3D outperforms all other methods in weakly-supervised manner."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The topic of weakly incremental 3D detection is quite novel.\n2. The paper makes clear illustrations of the major challenges and problems that need to be solved and proposes effective solutions correspondingly. And weaknesses of the proposed method are also clearly illustrated. \n3. The proposed method is simple and easy to implement. \n4. WI3D could reach relatively better performance. The extensive ablation study and analysis demonstrate the effectiveness of the proposed components."
            },
            "weaknesses": {
                "value": "1. There are two concerns about the method: (1) While generating the coarse pseudo labels, the method proposed in WEAKM3D is used. However, WEAKM3D is designed for the outdoor dataset, which is much sparser and instances are separate. It might lead to some failures and lower the quality of refined pseudo labels. (2) The knowledge transfer is only conducted on novel classes, while base classes still keep the original representation. It might confuse the model to learn quite different representations between novel classes and base classes.\n2. Most technical parts can be found in other works and the technical contribution is marginal. \n3. Some illustrations are not clear, e.g., how does teacher predict reweighting modulation factor; the \u201cfine-tuning\u201d method in 4.2 is described as \u201ctune the whole model on novel classes\u201d, while in I3DOD (Liang et al., 2023) and SDCoT (Zhao & Lee, 2022), it was described as \u201ctune all parameters (except the old classifier) with a new classifier for C_novel\u201d, I am not sure whether they are the same; what does vanilla method stands for in Table 5.\n4. The quality of pseudo labels is of great importance. It would be better to show the mAP or recall of refined pseudo labels for further analysis, which is also a more intuitional way to clarify the effectiveness of PRF module.\n5. The number of stages and total classes is a little small. I wonder how the method will scale under more learning stages and more classes (e.g. 10 stages for ScanNet200). \n\nWenqi Liang, Gan Sun, Chenxi Liu, Jiahua Dong, and Kangru Wang. I3dod: Towards incremental 3D object detection via prompting. arXiv preprint arXiv:2308.12512, 2023. \nNa Zhao and Gim Hee Lee. Static-dynamic co-teaching for class-incremental 3D object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3436\u20133445, 2022."
            },
            "questions": {
                "value": "1. \u201cWeakm3d: Towards weakly supervised monocular 3d object detection.\u201d was published in 2022 but not in 2021 as mentioned in the reference. You need to check the references carefully.\n2. It would be better to show the performance of WI3D on outdoor datasets as supplementary experiments, for these are widely used for some promising areas like autonomous driving."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698497967987,
        "cdate": 1698497967987,
        "tmdate": 1699636182986,
        "mdate": 1699636182986,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MhbtSwRZsu",
        "forum": "yBZd6mCWXd",
        "replyto": "yBZd6mCWXd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_asnb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_asnb"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses a novel task, which involves substituting 3D annotations of novel classes in class-incremental 3D object detection with 2D supervisions from images, termed Weakly Incremental 3D Object Detection (WI3D). The authors present a framework that extends the class-incremental 3D object detection method SDCoT. They adopt an existing approach for weakly supervised monocular 3D object detection to generate coarse 3D pseudo labels for novel classes. To address the challenges posed by noisy pseudo labels, they introduce a Pseudo Label Refinement (PLR) module, which directly refines these labels. Additionally, the authors employ cross-modal knowledge transfer techniques to enhance the robustness of the learned representation. Furthermore, they modify the base knowledge distillation loss within SDCoT that is used to mitigate the catastrophic forgetting of base knowledge. The proposed method is evaluated on two benchmark indoor datasets, under the batch incremental 3D object detection setting as proposed in SDCoT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of Weakly Incremental 3D Object Detection addressed in this paper is indeed a valuable area of exploration as it holds the potential to reduce annotation requirements significantly.\n2. The proposed framework seamlessly integrates several pre-existing techniques, forming a technically sound architecture. The rationale behind the incorporation of each module is well-founded."
            },
            "weaknesses": {
                "value": "**1. The paper could benefit from providing more detailed and clear explanations.**\n\nFirstly, the method for generating coarse pseudo labels is not clearly presented. While the authors mention adopting a method from (Peng et al., 2021), the specifics of this method need further clarification. It's essential to note that the method in Peng 2021 is intended for outdoor Lidar point clouds, which have different characteristics from indoor point clouds. Moreover, the process for predicting 3D object boxes in Peng 2021 may not be straightforward and may involve model training. Therefore, the paper should provide more details on how these coarse pseudo labels are generated and how they adapt to the indoor point cloud setting.\n\nSecondly, the architecture and training strategy of the PLR module lack detailed explanations. Key aspects, such as the normalization step in Fig. 4, require clarification. Additionally, the training process of the PLR module in Stage 1, whether it is trained alongside the 3D detection backbone or separately, is not clear.\n\nThirdly, when comparing with SDCoT, the authors mention they \u201cmodify the training of SDCoT (Zhao & Lee, 2022) to fit our weakly incremental learning setting\u201d, but there lacks a specific description of this modification (e.g., how to obtain annotations for novel classes). Also, the details of the fine-tuning and freeze-and-add setup in the WI3D task need to be clarified. \n\nFourthly, the paper mentions splitting the category set into C_base and C_novel according to SDCoT, but the procedure is not identical to SDCoT (e.g., comparing Table 1 in this paper vs. Table 1 in SDCoT).\n\nLastly, the term \"vanilla\" in Table 5 needs clarification. The distinction between \"vanilla\" and \"ours\" is not made clear.\n\n**2. While the overall framework is sound, the paper's technical contributions are subject to doubt.**\n\nFirstly, the PLR module appears to share significant similarities with the BoxPC network in [REF1], which is also designed to refine pseudo labels for novel classes by predicting 3D bounding box residuals and binary probabilities.\n\nSecondly, when designing the intra-modal base knowledge distillation, the authors argue that \u201cprevious work usually directly utilizes all the predicted responses and treat knowledge equally.\u201d As such, the reweighting modulation factor alpha_i should be the major modification compared to SDCoT. However, the authors omit comparing it with the version that removes alpha in the ablation study (refer to Table 7). This omission makes it challenging to discern the contribution of this modification.\n\n**3. The paper does not present results in the sequential incremental learning setting, which is a common evaluation setting in Class-Incremental Learning.**\n\n**4. The paper would benefit from visualization**, particularly in the form of 2D-3D bounding box pairs (bipartite matching results). Additionally, the validity of the example shown in Fig 2(a) is in question. I suspect such a misalignment might only occur when the 2D bounding box is entirely incorrect. The authors should provide a more detailed explanation or real-world examples to support the claim made in this figure.\n\n[REF1] Tang, Yew Siang, and Gim Hee Lee. \"Transferable semi-supervised 3d object detection from rgb-d data.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019."
            },
            "questions": {
                "value": "Please refer to the comments in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566309550,
        "cdate": 1698566309550,
        "tmdate": 1699636182898,
        "mdate": 1699636182898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TXbV9XU3kh",
        "forum": "yBZd6mCWXd",
        "replyto": "yBZd6mCWXd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_QdxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2466/Reviewer_QdxM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for class-incremental learning of 3D detection in RGBD pointcloud data. The method begins with a 3D detector trained on the base classes, and a 2D detector trained on all classes, and then attempts to transfer the novel-class knowledge from the 2D model to the 3D one. The method involves generating pseudo-labels using the 2D model, and then correcting these using the 3D model, and then training the 3D model with its own estimates. The method also incorporates two regularization losses, called the \"Cross-Modal Knowledge Transfer\" loss which increases cosine similarity between the 2D features and the 3D ones, and a \"Reweighting Knowledge Distillation\" loss that does a similar thing but (1) with a learnable weighting and (2) applied to logits too."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is fairly well written, and does not make large over-claims about the novelty or impact or results. The figures are helpful in understanding the work. The method does well against its main considered baseline, SDCoT."
            },
            "weaknesses": {
                "value": "I have a variety of clarification questions that I hope the authors can address. I will put them into the Questions tab. \n\nOne clear weakness might be the evaluation. Why is there only one baseline in the evaluation? Other parts of the paper seem to acknowledge three closely related works: Zhao & Lee, 2022; Zhao et al., 2022; Liang et al., 2023. Is it possible to compare against all of them?"
            },
            "questions": {
                "value": "The paper says \"Inspired by the human visual system that excels at learning new 3D concepts through 2D images, we propose to incrementally introduce novel concepts to a 3D detector with the visual prompts generated from a cost-free 2D teacher other than revisiting 3D annotations for both base and novel classes as shown in Fig. 1.\" I do not understand the connection to the human visual system. I do not understand what it means for visual prompts to be \"generated from a cost-free 2D teacher other than revisiting 3D annotations for both base and novel classes\". \n\nIn Figure 2 it's unclear to me what method was used to generate the 3D boxes. The one in column b is especially egregious, since it seems like this does not even meet the edges of a plausible 2D box. \n\nThe method section says that it will \"pose the noise of 3D pseudo labels directly generated from 2D predictions\" and I don't know what this means. (What does it mean for noise to be posed or unposed?)\n\nSection 3.1 says \"we adopt a simple way to generate coarse 3D pseudo labels from 2D preditions\" but it is never made clear what this method is. \n\nSection 3.3 introduces a module called PRF which produces an offset to a given box. It is unclear how this module is trained. (Where do the ground-truth offsets come from? Does this training happen on all classes, or just base classes, or just novel classes?)\n\nSection 3.3 introduces a module called BCH, which to my understanding takes the exact same input as the PRF module. If this is the case, it would be great to say so in the text, instead of re-stating the list of inputs as if it were unique. Also, it is unclear to me how this BCH module is trained. (Where does the ground truth \"presence\" label come from?) \n\nSection 3.4 mentions difficulties associated with \"extracting regional representations\". This is fine, except that this is the first time in the method that regional representations are ever mentioned. What are they? \n\nSection 3.4 mentions briefly that the IOU between the 2D predictions and the projected 3D predictions will be used \"the cost function\", but there is no equation given for this, and it's unclear to me if this is really one of the training objectives in Section 3.5. What is the exact form of the supervision? (Is it maybe the generalized differentiable IOU from Rezatofighi et al.?)\n\n\n\n\nhow to introduces -> how to introduce \n\nPseuod -> Pseudo\n\npreditions -> predictions\n\nobjectiveness -> objectness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821941430,
        "cdate": 1698821941430,
        "tmdate": 1699636182822,
        "mdate": 1699636182822,
        "license": "CC BY 4.0",
        "version": 2
    }
]