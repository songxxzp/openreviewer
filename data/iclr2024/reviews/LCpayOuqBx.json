[
    {
        "id": "2VMj3puv11",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_WwTG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_WwTG"
        ],
        "forum": "LCpayOuqBx",
        "replyto": "LCpayOuqBx",
        "content": {
            "summary": {
                "value": "This paper uses an LLM to generate potential OOD labels to be used alongside ID labels for score-matching with CLIP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Simple method, clearly written and easy to understand.\n\nExperiments are comprehensive with many datasets and ID/OOD setups. \n\nSufficient ablation study"
            },
            "weaknesses": {
                "value": "Though the use of LLM to generate OOD labels is distinct from previous work, this alone does not seem like a strong novelty contribution compared to related works\n\nThe experimental setting choices are unclear. For example, only the MCM baseline is tested for Zero-shot far OOD with most datasets, but for ImageNet-1K there are many more baselines. Similar problem for near-OOD.These experiments should be more consistent.\n\nThere should be more discussion around how each dataset is adapted to be ID-OOD, For example, how similar are iNet-10 and iNet-20 really? It depends on the subsets of classes chosen.\n\nThe design of the LLM prompt causes some information leakage, in that a different type of prompt is used for far-, near- and fine-grained OOD settings. This means there is an implicit assumption about what type of anomalies are likely to be seen in a given experiment. A fairer method would not have such assumption about the test data embedded into its LLM prompts."
            },
            "questions": {
                "value": "Why use FPR95 instead of AUPR?\n\nIn Figure 7, why are far-ood and fine-grained-ood tested with 100s of outlier class labels but near-ood only with a maximum of 10?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Reviewer_WwTG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3638/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697422143002,
        "cdate": 1697422143002,
        "tmdate": 1699636319480,
        "mdate": 1699636319480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PMceAmtXIB",
        "forum": "LCpayOuqBx",
        "replyto": "LCpayOuqBx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_vW8h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_vW8h"
        ],
        "content": {
            "summary": {
                "value": "The paper presents DOS, a method for improving zero-shot OOD detection for CLIP-like models. DOS can be summarized in two parts: firstly the generation of broad OOD labels using an existing LLM, and secondly the detection of OOD samples through a proposed DOS scoring function. Experimental results on a range of ID and OOD datasets show improvements over the baseline MCM and other commonly used OOD detection methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written and the zero-shot OOD detection method can be easily, and widely, used in real-world applications of OOD detection.\n2. The distinguishment of far, near, and fine-grain OOD label generation presents interesting and unique opportunities for future work.\n3. Empirical results show impressive performance, even when compared against fine-tuned methods of OOD detection."
            },
            "weaknesses": {
                "value": "1. The reviewer would personally like to see additional experimental evaluations beyond CLIP models, such as ALIGN[1] or FLAVA[2].\n2. Additional experiments with other standard OOD detection benchmarks such as CIFAR-10/CIFAR-100 (SVHN, LSUN, DTD, Places365) would give further empirical support for the methodology.\n\n[1] Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. \"Scaling up visual and vision-language representation learning with noisy text supervision.\" In International conference on machine learning, pp. 4904-4916. PMLR, 2021.\n\n[2] Singh, Amanpreet, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. \"Flava: A foundational language and vision alignment model.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15638-15650. 2022."
            },
            "questions": {
                "value": "The reviewer would like some additional clarification regarding the T-SNE visualization in Section 4.4. In particular, it is unclear from initial viewing why the T-SNE visualization implies improved OOD detection performances, as one can similarly argue how the singlular clustering of OOD representations may lead to better OOD detection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3638/Reviewer_vW8h"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3638/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655833939,
        "cdate": 1698655833939,
        "tmdate": 1700658887153,
        "mdate": 1700658887153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FFRsgCdSvG",
        "forum": "LCpayOuqBx",
        "replyto": "LCpayOuqBx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_yFD8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_yFD8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach to address the OOD detection problem. The author suggests that having knowledge of the categories of the OD instances can effectively improve the OOD detection performance. Furthermore, the author proposes using LLM to generate the category names for OD instances. Building upon this idea, the author designs a novel OOD detection algorithm. According to the experimental results, the method proposed in this paper demonstrates improved OOD detection performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of this paper is very innovative. In the rapidly developing field of LLM, introducing LLM into OOD detection tasks could indeed lead to significant improvements.\n\n2. The author's writing is clear, explaining the starting point, specific methods, and experimental design of this article very clearly.\n\n3. According to the author's experimental results, the proposed method in this article can indeed improve the effectiveness of OOD detection tasks."
            },
            "weaknesses": {
                "value": "1. Although it is a good idea to introduce LLM into the OOD detection task, the way it is introduced in this paper is somewhat rigid. The paper primarily utilizes LLM to generate names for OD samples, which are then employed for training purposes. However, there is a lack of effective measures to ensure the reasonability of the OD categories generated by LLM. This critical oversight significantly compromises the overall reliability and trustworthiness of the proposed method.\n\n2. The analysis in this paper is insufficient. Firstly, considering the pivotal role played by LLM in the proposed method, it is crucial to explore the performance of various LLM models, rather than solely relying on a single model such as gpt-3.5. Conducting experiments with different LLM models could potentially yield diverse outcomes and provide a more comprehensive understanding of the approach's effectiveness. By limiting the analysis to just one model, the authors unintentionally overlook the possibility of alternative models delivering superior results.\nSecondly, the practical implications of the categories generated by LLM for the ODs are not thoroughly examined. While these generated categories may possess semantic relevance, it is essential to assess the extent of overlap between the generated categories and the actual OD categories. Additionally, an in-depth analysis of the impact of these categories on the final accuracy of the OOD detection system is missing. Understanding the potential discrepancies and evaluating the influence of these categories on the system's overall performance is crucial for gauging the practical applicability and reliability of the proposed method."
            },
            "questions": {
                "value": "As shown in the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3638/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698926933530,
        "cdate": 1698926933530,
        "tmdate": 1699636319279,
        "mdate": 1699636319279,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4sZ5pjHlUk",
        "forum": "LCpayOuqBx",
        "replyto": "LCpayOuqBx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_5Vrp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3638/Reviewer_5Vrp"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of zero-shot OOD detection. Following the previous CLIP-based OOD detection methods (MCM), this paper finds that adding OOD label space with ID classes could boost performance. To this end, the authors propose to leverage LLM to generate prompts to dream about OOD classes. Further, a new scoring function is proposed based on the proportionality between potential outlier and ID class labels. Experiments show the performance gains compared to MCM."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Using LLMs to generate prompts for OOD classes is interesting and it is based on an empirical study that using OOD classes w/ ID classes will improve the performance. \n- The proposed method is zero-shot, training-free;\n- Ablation study on score function; Number of OOD classes has been conducted and explained."
            },
            "weaknesses": {
                "value": "Overall, I think the paper is interesting to the community for discussion. Yet, I still have some questions or concerns that want the authors to address in rebuttal.\n1. For Figure 1, I get the high-level idea that adding OOD classes with/ ID classes helps the performance boost. Can you add those GT OOD classes w/ ID classes in your Table 1,2,3,4 for the Oracle experiment? It can help better understand the upper bound of your approaches. \n2. For the scoring function, I don't see much motivation or justification for this scoring function design. Also, in Figure 6(a), the performance gains from S_DOS to S_MSP are minor. Can you elaborate more on this scoring function design?\n- Also, it will be interesting to test your method on other well-known scoring functions such as Max logit score; Energy function; gradients, etc. \n3. In table 2, the performance of the Texture dataset is not good. Do you have any insight on the reason?\n4. For your generated OOD class prompts, can you conduct some similarity measures between your prompts and GTs? I would like to see how much chance LLMs can hit the GT OOD classes."
            },
            "questions": {
                "value": "Please refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3638/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699574666911,
        "cdate": 1699574666911,
        "tmdate": 1699636319216,
        "mdate": 1699636319216,
        "license": "CC BY 4.0",
        "version": 2
    }
]