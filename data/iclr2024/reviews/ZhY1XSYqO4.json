[
    {
        "id": "I5Q5nAWjNY",
        "forum": "ZhY1XSYqO4",
        "replyto": "ZhY1XSYqO4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_weLs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_weLs"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a unifying principle for viewing and designing variational bounds based on multiinformation, aka total correlation [1]. The paper combines different variational upper and lower bounds (including MI Neural Estimators \u2013 MINE). Their frameworks recovers for example beta-VAEs or DVCCA. The newly introduced deep variational symmetric information bottleneck DVSIB objective is illustrated on a noisy MNIST dataset and where it yields to clusters for t-SNE embeddings or good classification accuracies based on the latent representations.\n\n\n[1] Watanabe, Satosi. \"Information theoretical analysis of multivariate correlation.\" IBM Journal of research and development 4.1 (1960): 66-82."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A plethora of works have been developed for multi-modal datasets that rely on variational methods in varying forms. Having a unifying framework to analyse such works is thus of great importance for the community. The submission thus addresses an important issue. Their introduced DVSIB objective is new, as far as I am aware. It outperforms other multi-modal variational methods for classifying noisy MNIST based on the latent representations."
            },
            "weaknesses": {
                "value": "The paper is sometimes difficult to follow and I feel that the structure of the paper can be improved, for example by using Definition/Proposition/Theorem etc.\n\nTo better assess the performance of DVSIB, it would be very useful to (i) compare it against previous work that also rely on multimodal information-theoretical measures,  such as [1] using a multiinformation bottleneck, (ii) evaluate not just whether the latents can be used for classification, but also the quality and cross-model consistency of the reconstructed images, for example following standard multi-modal evaluation measures [2]; and (iii) consider additional multi-modal datasets beyond noisy MNIST.\n\n[1] Hwang, HyeongJoo, et al. \"Multi-view representation learning via total correlation objective.\" Advances in Neural Information Processing Systems 34 (2021): 12194-12207.\n[2] Shi, Yuge, Brooks Paige, and Philip Torr. \"Variational mixture-of-experts autoencoders for multi-modal deep generative models.\" Advances in neural information processing systems32 (2019)."
            },
            "questions": {
                "value": "Can the approach be generalised to more than two modalities?\n\nHow are the $\\beta$ values in Table 2 tuned? Does it make sense to use different $\\beta$ values for evaluating the classification accuracy for different methods, as I would guess that different $\\beta$s impact how much information is encoded into the latent variables."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634567379,
        "cdate": 1698634567379,
        "tmdate": 1699636693175,
        "mdate": 1699636693175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o8RVCwwVoi",
        "forum": "ZhY1XSYqO4",
        "replyto": "ZhY1XSYqO4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_uWuS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_uWuS"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a unifying framework for a number of variational dimensionality reduction techniques through the unifying lens of the information bottleneck.  They use this framework to quickly rederive several existing techniques, including a very slight generalization of one technique (Deep Variational Canonical Correlation Analysis).  They also derive an approach similar to DVCCA which they call Deep Variational Symmetric Information Bottleneck (DVSIB), which involves performing dimensionality reduction on two views of a set of sample simultaneously.  The novelty is that while trying to maximally compress the latent representation of each view (i.e., minimize the amount of information between the views and their latent representations) DVSIB also tries to maximize the information between the two latent representations.  They apply various methods to a variation on MNIST where each observation consists of two \"views\" of the same digit.  One view is a random sample from MNIST of that digit that is then randomly rotated.  The other view is another random draw of the same digit that is then randomly noised.  The authors find that classifiers trained on top of DVSIB representations outperform classifiers trained on top of different representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The presentation is extremely clear.\n* The unifying framework is a nice, conceptually clean way to unify a number of methods, and makes it easy to quickly derive loss functions for a fairly general family of dimensionality reduction techniques.  \n* DVSIB seems like a sensible and promising approach for finding probabilistic embeddings of multi-view data.\n* Table 1 is a nice compendium of methods and concisely explains how these methods fit into the proposed framework."
            },
            "weaknesses": {
                "value": "Major:\n\n* The evaluations and benchmarking felt limited, for a few reasons that I will detail in the next few comments.  First, the MNIST example feels somewhat contrived, and MNIST in general has become something of a toy dataset.  A more complex, more realistic application dataset would make the applicability of DVSIB more clear.\n* Even on the MNIST application, I felt that the benchmarking was insufficient.  In particular, in Figure 3 and Table 2 it seems like there is almost no penalty for making $\\beta$ huge as long as it is large enough, in which case the penalty on the encoder essentially does not matter.  In the VAE setup this would cause the encoder to concentrate on the MAP instead of the posterior, essentially reverting to an Auto-Encoder.  Are only the means of the variational distributions used in the downstream classification task?  Can the authors performing any benchmarking where the probabilistic interpretation on the latent space matters?\n* Similarly, it seems like the MNIST benchmarking only gets at the importance of the size of the latent space in an oblique way.  In particular, across almost all methods performance always does better with a larger latent space (including DVSIB).  The paper would benefit from an analysis similar to the motivation suggested in the introduction, namely an example where the original dimensionality of the data is prohibitively large relative to the number of labeled examples for training the classifier, but a large unlabeled dataset exists to learn a good dimensionality reduction. In such a case dimensionality reduction would be absolutely necessary to obtain good classification performance.\n* In order to compute the mutual information between $Z_X$ and $Z_Y$, the authors essentially use an energy-based model, learning $T(z_x, z_y)$ as an unnormalized log-likelihood (up to multiplication by the marginals).  Additional details on how the normalizing constant, $Z_\\text{norm}$, is computed or approximated are warranted.\n\nMinor / typos:\n\n* Is $\\Sigma_{Z_X}(x)$ assumed to be diagonal?  If not, what does it mean to learn the ``log variance''?\n* I believe that the title of Subsection 2.2 should be \"Variational Bounds\" not \"Variation Bounds\"\n* There is a typo in Equation (13) -- the MINE subscript should be on the term involving $Z_X$ and $Z_Y$, not the term with $Y$ and $Z_Y$.\n* \"available in the Appendix 5,4\" appears to be a typo."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727132797,
        "cdate": 1698727132797,
        "tmdate": 1699636693057,
        "mdate": 1699636693057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4UMW3ERfX8",
        "forum": "ZhY1XSYqO4",
        "replyto": "ZhY1XSYqO4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_JTcS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_JTcS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Deep Variational Multivariate Information Bottleneck (DVIB) as a framework to derive variational losses for dimensionality reduction purposes. A method section rooted in existing literature demonstrates how to de-compose multi-information associated with encoding and decoding distribution and how to bound and estimate each term using variational inference and deriving a Deep Variational Symmetric Information Bottleneck (DVSIB) objective for a specific instance of graphical models. The effectiveness of the proposed method and model is demonstrated on augmented pairs of MNIST digits."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper introduces a general framework inspired by the Information Bottleneck principle that can in theory applied to a wide variety of graphical models as an effective dimensionality (and information) reduction strategy.\n\n2) The DVIB framework generalizes a variety of models in the literature, extending VIB [1] to graphical models with more than 2 variables."
            },
            "weaknesses": {
                "value": "## Main concerns\n\n1) **Novelty**\n    1) The novelty of the proposed Deep Variational Symmetric Information Bottleneck seems quite limited since the objective is quite similar to the existing literature and the main differences are not clearly underlined in the main text.\n\n2) **Experimental analysis**\n   1) The paper introduces a framework that can in principle applied to complex graphical models involving multiple variables, but the experimental section (and most of the method) solely focuses on a two-variable system that has been widely explored in the literature.\n   2) The experiments revolve solely around the MNIST dataset. Further, the paper claims that \"none of the algorithms were given the data labels\" even though the training pairs are constructed by pairing digits with the same label. As a result, label information is indirectly captured in the dataset structure.\n   3) The paper lacks common baselines based on contrastive learning that can be applied in the same settings [1,2,3]. In particular [2] proposes a similar loss function and demonstrates similar performance without using the labels for pairing images.\n   4) The qualitative visualization relies solely on t-SNE even if there is evidence to support that t-SNE visualization could be misleading [4].\n\nThe paper presents an interesting approach through the DVIB framework, which holds the potential for principled dimensionality reduction in structured datasets consisting of tuples of joint observations. However, the current submission falls short of demonstrating its contributions due to a limited experimental section and lack of novelty in the chosen setting. A more compelling case could be made by extending the analysis and experiments to encompass more complex graphical models and tasks, as opposed to the limited scope of addressing well-studied symmetric 2-observed variables as reported in the main text. This expanded focus would not only enhance the novelty but also demonstrate the method's applicability and effectiveness in more challenging scenarios.\n\n## Minor issues\n1) Some of the citation years and venues are incorrect (e.g. Friedman et al., 2013 has been published in UAI 2001)\n\n\n### References\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[2] Federici, M., Dutta, A., Forr\u00e9, P., Kushman, N., & Akata, Z. \"Learning robust representations via multi-view information bottleneck.\" International Conference on Learning Representations, ICLR, 2020.\n\n[3] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Yang, Zhirong, Yuwei Chen, and Jukka Corander. \"T-SNE is not optimized to reveal clusters in data.\" arXiv preprint arXiv:2110.02573 (2021)."
            },
            "questions": {
                "value": "1) What are the main differences between DVSIB and the existing methods in literature? How are they potentially related to the improved performance?\n\n2) What is the rationale behind the choice of MINE for mutual information maximization? More recent mutual information maximization strategies [1] are shown to yield more stable and effective training.\n\n3) How does the prescribed DVIB model perform on more complex datasets consisting of tuples of observations with a known graphical model? Can DVIB make better use of the structure of the problem when compared to popular modern representation learning methods that do not explicitly consider the relation between the variables?\n\n\n### References\n\n[1] Poole, Ben, et al. \"On variational bounds of mutual information.\" International Conference on Machine Learning. PMLR, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6306/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6306/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6306/Reviewer_JTcS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836719725,
        "cdate": 1698836719725,
        "tmdate": 1700834343494,
        "mdate": 1700834343494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ez26NQ6Om4",
        "forum": "ZhY1XSYqO4",
        "replyto": "ZhY1XSYqO4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_MfdL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6306/Reviewer_MfdL"
        ],
        "content": {
            "summary": {
                "value": "The authors study variational dimensionality reduction and propose a multivariate information bottleneck framework that generalizes several existing method (e.g, beta-VAE) and yields new algorithms for settings where we want to jointly compress two distinct data representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors study an important problem in dimensionality reudction\n- The proposed framework yields a nice generalization of existing methods for variational dimensionality reduction"
            },
            "weaknesses": {
                "value": "1. I feel like the writing of the paper still has quite a bit of room for improvement\n- Even understanding the task that the authors are solving took a long time. I see it first in Section 2.1. It should be clear from the abstract and intro that we are trying to map two views into a latent space. Right now, the intro reads like a long list of related work.\n- I am familiar with the VAE and variational inference literature, and I found the derivations unnecessarily hard to follow. In particular, there exists standard notation used in variational inference (e.g., q is an encoder/approximate posterior, p is the decoder, etc.) and it doesn't seem to be used in the paper. \n- The paper is trying to solve problems that are in the domain of probabilistic modeling and variational inference, but uses techniques based on information theory. For readers that are less familiar with information theory, it would help to have a paragraph that explains more how these methods relate to the literature on VAEs and variational infernece.\n- At a high level, I found that in some places the paper is overly verbose, and in others it is overly terse.\n\n2. The experimental results are not very strong in my opinion.\n- First of all, since this is not mainly a theory paper, I feel like the authors should experiment on more than one dataset.\n- Ideally, some of these datasets would be more sophisticated than MNIST. I feel like this method would be very useful for researchers in biology or neuroscience, perhaps exploring applied problems in these fields would make the paper stronger.\n- I am not entirely sure if the set of baselines is the best one. For example, the new method is the only one which defines two separate latents for each of the two views of the data. Is the improvement in performance attributed to the fact that each view gets its own latent (which is not a novel idea from this paper; there are other methods that do this), or to the specific way in which the method generates these latents. In order to determine this, another baseline that computes one latent variable per view would be helpful.\n- In particular, what if I were to fit a VAE-type model with two latents $Z_X, Z_Y$ and two observed variables $X, Y$ such that the q and the p have the same independence structure as DVSIB in Table 1. Would this approach be equivalent to DVSIB? If yes, there should be a discussion. If they are not equivalent, then the VAE-type model should be a baseline (and there should still be a discussion of the pros/cons of each approach).\n\nOverall, I am leaning towards rejection, but I am willing to raise my score if I am missing something, or if there is additional compelling evidence that the authors could provide."
            },
            "questions": {
                "value": "- Are there any additional datasets and baselines that could be added to the paper?\n- How does the method compare to VAE-type model with two latents $Z_X, Z_Y$ and two observed variables $X, Y$ such that the q and the p have the same independence structure as DVSIB in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6306/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699143301579,
        "cdate": 1699143301579,
        "tmdate": 1699636692849,
        "mdate": 1699636692849,
        "license": "CC BY 4.0",
        "version": 2
    }
]