[
    {
        "id": "Z9sF2Cyx6c",
        "forum": "5eLgTLusaR",
        "replyto": "5eLgTLusaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_EhxJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_EhxJ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new multi-human-scene interaction dataset collected using a VR system with motion capture. This work also proposes a UNet-based model for human trajectory prediction and demonstrates its effectiveness on the proposed Loco3D dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of collecting human-scene interaction datasets using VR is great. It enjoys the benefit of real human behavior, scene diversity, and low cost (with scalability).\n2. The Loco3D dataset seems a good contribution to the community and would interest multiple fields.\n3. The idea of incorporating human-human interactions is well-motivated."
            },
            "weaknesses": {
                "value": "The main weakness of this work is that the current experimental analysis fails to align with the main characteristic of the dataset, making the motivation for creating the dataset less convincing.\n- (motion) The dataset features locomotion (3D body motions), but the experiments are only on the trajectories.\n- (scene-affordance) The dataset contains rich indoor 3D scenes with diverse objects and affordances, but the experiments contain only 'binary maps as scene maps' as scene representations, which cannot reflect the meaningful scene surroundings for human behaviors.\n- (interaction) The motivation behind the dataset contains social interactions (e.g., social etiquette in section 1 paragraph 2), but the interactions in the experiments only involve collision avoidance and do not address the mentioned TV scenario.\n\nWhile I believe the proposed dataset and the data collection pipeline would be interesting to the community, current experimental evaluations do not adequately reflect its contributions. Authors could consider more challenging tasks (human motion prediction/generation with complex scene conditions, like GIMO [a], CIRCLE [b], etc.). Or is there any specific difficulty in achieving this?\n\n[a] Gaze-informed human motion prediction in context. ECCV 2022\n\n[b] CIRCLE: Capture in Rich Contextual Environments. CVPR 2023"
            },
            "questions": {
                "value": "- Could the author discuss more details on the VR mocap data collection protocol? e.g. the real and virtual space could be unaligned (e.g., a wall in the real world but not in the virtual and vice versa), how to mitigate the issue?\n- More interaction types could be explored in the data collection process (e.g., two people need to accomplish certain tasks together).\n\nMinor issues:\n- Section 2.2 HUMANIZE -> HUMANISE\n- [b] CIRCLE: Capture in Rich Contextual Environments needs to be cited and discussed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Reviewer_EhxJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698071061655,
        "cdate": 1698071061655,
        "tmdate": 1699636720091,
        "mdate": 1699636720091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xdXOD0tiIY",
        "forum": "5eLgTLusaR",
        "replyto": "5eLgTLusaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_Zavr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_Zavr"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a human behavior data collection system that utilizes VR to get a multi-person trajectories dataset, Loco3D, across 130 complex indoor settings. Additionally, the authors propose a human trajectory prediction model consider the multi-person scenario. Experimental outcomes indicate that in multi-person scenarios, both the Loco3D dataset and the proposed methods enhance trajectory synthesis outcomes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Leveraging VR to collect the multi-person trajectory is a compelling approach, considering the time cost and complexity to set up cameras or environments in the real world. The advantage is that although the scene is some scan-reconstructed, the human trajectory is real.\n* The Loco3D dataset includes much more scenes then previous multi-person real dataset. The high diversity in the layouts can support more work focusing on trajectory synthesis in multi-person scenario.\n* Their experiments demonstrate that the collected data can be used to improve the performance of the models, and the scale of the data is important.\n* Their methodology takes into account multi-person trajectories, yielding enhanced results in comparison to prior research."
            },
            "weaknesses": {
                "value": "* Regarding the dataset statistics, there's an absence of comparisons concerning the number of trajectories in each scene, as well as their length and complexity. In Table 2, prior multi-person trajectory datasets, such as JRDB, contained approximately 20K frames for each scene. In contrast, Loco3D offers only 7.7K frames. It remains ambiguous whether this frame count pertains to a single trajectory or multiple ones. Additionally, the variation in the number of individuals across datasets is not clearly demonstrated.\n* For the comparison with prior dataset, the most related prior dataset shold be the JRDB ones, which also contains the multi-person data. Current comparison is hard to see if the improvement is from the data scale or from different task settings.\n* For the proposed method, the structure seems that it can only work for a fixed number of people. This limit the generality the proposed methods. It\u2019s also hard to see if the proposed method can still be adapted to the single-person scenario and what the performance will be.\n* For the qualitative results, treating the overlapping of the trajectory as a judgement is not proper. The trajectory also involves the time, two people may not go to a near position at each time step even if their trajectories overlap."
            },
            "questions": {
                "value": "* For the Loco3D dataset, does it also include the first-person view frames? Then how to deal with the gap between the rendered images using a scan-reconstructed dataset (HM3D) and the real world? \n* The motivation mentions the human trajectory should consider if the other human is watching TV. However, based on the paper data collection process, all humans can only walk around, lack of the diversity in different social scenario. So what\u2019s the actual social constraints covered in the dataset, instead of only collision avoidance between people?\n* Does the dataset only contains two people scenario and why the design is like this?\n* For the proposed method, is there some solution to make it adapt to scenarios with different number of people?\n* Why not comparing the results with JBDR to see if the improvement really comes from different scenes or just different number of trajectories?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Reviewer_Zavr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698532842326,
        "cdate": 1698532842326,
        "tmdate": 1699636719982,
        "mdate": 1699636719982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mmAcOiBUca",
        "forum": "5eLgTLusaR",
        "replyto": "5eLgTLusaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_v7u4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_v7u4"
        ],
        "content": {
            "summary": {
                "value": "This manuscript presents Loco3D, a dataset of a pair of humans interacting with high-resolution indoor scenes in VR that includes detailed 3D body pose as well as detailed maps of the indoor environments. The dataset includes 7000 example trajectories across 130 scenes, and in addition to 3D body keypoints they provide semantic scene segmentation and scenes with photorealistic textures. They develop a UNet based path planner module that uses a path history, the goal location, and scene map to produce a probability map of trajectories. They consider three evaluation datasets \u2013 Loco3D, Loco3D-R which was collected in the real world, adn GIMO, a previously published dataset. They show improved performance compared to YNet on the Loco3D, but not GIMO datasets, and that training on Loco3D produced superior results. They show training with multi-person data is superior and give qualitative examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The dataset is a contribution to the field and has several novel elements, including multi-person data, photorealistic textures, and semantic segmentation. I can see this experimental approach for generating training data to become common int he field. There is also a real-world test example. \n\n* There is a new U-Net based modeling format that incorporated multi-person data and evaluations show some modeling improvements with multi-person data. \n\n* The supplement and text are comprehensive and describe experiments well."
            },
            "weaknesses": {
                "value": "* The contributions can be distinguished from other datasets and models for human trajectory synthesis but the advance seems somewhat incremental in comparison. In particular the contribution is more the dataset than the model and so I wonder whether ICLR is the right venue. Because the dataset does not open up a new field in learning representations, but more advances the existing field it may find a better home in a more specialized venue.\n\n* The distinctions between the modeling component and existing literature are unclear. The approach seems novel but also related to approaches like YNet and the strengths and weaknesses could be more clearly elucidated in the text. Moreover I would like to see benchmarks with other approaches to improve the contribution of the new models, even if this means computing on single person trajectories alone. \n\n* There is not a robust comparison across standard benchmarks of the modeling component. It would be nice to know whether their proposed algorithm is SOTA and comparing its performance on a standard benchmark or whether the increase in performance is specific to the collected datasets. In fact the poor performance on GIMO is a limitation of the work in my opinion rather than just an endorsement of the value of the corpus."
            },
            "questions": {
                "value": "* Table 1 category of \u2018real/synthetic\u2019 is a bit ambiguous here, since the scenes are synthetically rendered. \n\n* Units in Table 2?\n\n* It is unclear how to interpret the poor performance on other datasets in Figure 4 and Table 2. The planner does not appear to work very well and it is unclear if this is just a domain gap? Moreover I was expecting to see comparisons training on the Loco3D corpus and testing on Loco3D-R\n\n* Can YNet be extended to include multi-person trajectories? \n\n* Can you comment on domain gap with real scenes. Unclear how human interaction in freely moving VR different from real environments. Affects the scope and generality of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758142922,
        "cdate": 1698758142922,
        "tmdate": 1699636719858,
        "mdate": 1699636719858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c2ErEQTQcB",
        "forum": "5eLgTLusaR",
        "replyto": "5eLgTLusaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_Yanw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6448/Reviewer_Yanw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel indoor human motion forecasting dataset containing paired motion of two real persons in virtual environments. To address the proposed task of socially-aware trajectory forecasting the authors further propose a U-Net-style model for socially-aware trajectory forecasting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Social interactions in 3D scenes is highly relevant but under-explored. The authors approach of utilizing VR to easily generated large variations of virtual worlds is clever."
            },
            "weaknesses": {
                "value": "The authors over-claim their contributions by saying that their dataset represents \u201creal\u201d social interactions: a better description would be \u201chybrid\u201d or \u201cmixed\u201d as the scene is entirely virtual. Also, real social interactions require humans to see each others faces - for observing small social cues - which is not possible with VR headset. The authors should adjust the description of their method as \u201creal\u201d in Table 1 and tone down their claims of representing real social interactions.\n\nThere are two concerns with regards to the proposed U-Net:\nFirst,  the U-Net in Section 4 is not well-described: \n* How is the scene sampled into an image?\n* How is the heat map generated?\n* How are past trajectories encoded?\n* How is the goal encoded?\n* How is the map encoded?\nSecond, the authors should have shown the effectiveness of their method on the experiments proposed in YNet."
            },
            "questions": {
                "value": "* What is part of the dataset? Will the authors make available the SMPL parameters at each frame as well?\n* How does the speed of the person behaves after forecasting? Do they slow down when approaching the target? Is there a velocity \u201cjump\u201d when changing from past to future motion?\n* Why is FDE not used in the experiments? \n* For completeness: The dataset contains personal information of the recorded subjects: did the subjects consent to the release of their trajectory and pose data?\n* For completeness: where will the dataset be made available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The recorded dataset contains human motion which could be linked to individuals."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6448/Reviewer_Yanw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793323764,
        "cdate": 1698793323764,
        "tmdate": 1699636719724,
        "mdate": 1699636719724,
        "license": "CC BY 4.0",
        "version": 2
    }
]