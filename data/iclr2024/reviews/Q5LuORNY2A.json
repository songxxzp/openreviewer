[
    {
        "id": "fsspYHXvTK",
        "forum": "Q5LuORNY2A",
        "replyto": "Q5LuORNY2A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate a connection between explicit and implicit neural networks. They provide an asymptotic behavior of the implicit CK and NTK matrices. They show that given an implicit NN, there exists a shallow explicit NN with quadratic polynomial activatin functions that is equivalent to the implicit NN."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic is relevent to the comunity. The paper is well-written."
            },
            "weaknesses": {
                "value": "Providing more explanations about related work can be helpful for readers. For example, briefly summarizing the result of Gu et al. may be helpful."
            },
            "questions": {
                "value": "Queations:\nHow does the assymptotic behavior stated in Theorems 1 and 2 depend on K and p?\n\nMinor comments:\n- Eq. (1): There are two \"+\" symbols.\n- In the beginning of Section 3, \"Theorem\" should be \"Theorems\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_yQj9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698441302048,
        "cdate": 1698441302048,
        "tmdate": 1699635952984,
        "mdate": 1699635952984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j5XitIfOZT",
        "forum": "Q5LuORNY2A",
        "replyto": "Q5LuORNY2A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission275/Reviewer_9wKK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to study implicit Neural Networks (INNs) through the lens of Conjugate Kernels (CKs) and Neural Tangent Kernels (NTKs).\nWith technical assumptions, among which the fact that the data is sampled from a Gaussian Mixture Model, it is shown that the CK and NTK matrices of INNs are asymptotically determined by a handful of parameters that can be derived from the activation function, the variance of the model weights, the input data and its statistics. \nIt is also shown that a 2-layer deep explicit neural network can be constructively designed to have CK and NTK matrices that match with that of a given INN.\nFinally, experiments on real and synthetic data show the eigenspectral behaviours of the estimated and approximate CK and NTK matrices, as well as the performance of the designed explicit neural networks compared to the original INNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the problem tackled is important: trying to understand INNs in a theoretical way is critical to using them at their fullest capability\n- experiments on real and synthetic data are important\n- assumptions are clearly listed before each theoretical results"
            },
            "weaknesses": {
                "value": "- **Code**: code is not provided to reproduce the experiments. To me it's a huge problem because it is a hinderance to reproducibility and I don't think there are reasons that would prevent the code to be shared.\n- **Interest of CK and NTK**: I am mainly familiar with INNs, not so much with CK and NTK. This is why it's not apparent to me simply reading the paper why characterizing CK and NTK is a good proxy to understand the overall behaviour of INNs. I think the sentence trying to explain this is \"which serve as powerful analytical tools for assessing the convergence and generalization properties of suf\ufb01cienlt wide NNs\" but it reamins unclear to me. To be perfectly clear, I do think that they could be important, I just don't understand why in this context. The related work part about NTK is also unclear to me.\n- **Bias in INNs**: in eq (1), there is no bias as opposed to Feng and Kolter 2020. I think it would be important to comment on why this parameter is gone from the parametrization and how having it would affect the proof.\n- **Assumption 3.**: assumption 3 is not commented at all. I think it's super important to comment all of these aspects especially those related to the asymptotic part. For example, the dimensionality of the data increases with the number of data points: this is very unusual and typically breaks the PAC-learning framework. Similarly, asymptotically, the mean of the data is 0 (if I understand correctly), since there is no bias in the formulation of the INNs, this does not cover all cases by a simple translation, so I think it's important to comment on that as well.\n- **Explicit NNs**: one of the conclusions I draw from section 3.2 is that it is possible to design explicit NNs that are virtually equivalent to INNs. It is also said that these would be much more efficient computationally than the corresponding INNs: if the widths were roughly equivalent this would be true, but I don't see any mention of this. If they are not, I think it needs to be proved theoretically. In any case, I think some experiments could show this easily by just computing the time of the forward pass.\n- **Relevance of the experiments**: several aspects are to me questionnable:\n   1. It seems to me that the experiments do not illustrate the result of Theorem 1 or 2 which are asymptotic. Here a single snapshot for a given pair $(p, n)$ is given rather than a plot showing $\\|G^\\star - \\bar{G}\\|$ as a function of $n$ even for simulated data. Of course this would be harder for real data since $p$ has to grow with $n$, but it also shows that this assumption is questionnable. Moreover, this would give some context for the values of $\\|G^\\star - \\bar{G}\\|$ rather than have absolute values.\n   2. I don't understand why the eigenvalues become an important part of the experimental section even though they were not mentioned in the theoretical part. Why can't we just focus on $\\|G^\\star - \\bar{G}\\|$? Moreover, the remarks on the comparison of eigenspectral behavior (page 8, second paragraph) are not quantitatively grounded. For example, I could argue that the eigenvalues distributions shown in the right panels of Figure 1 are not similar looking : the second bar in the red plot is much higher than the second bar in the left plot.\n   3. It is mentioned that the gap between ReLU-ENN and L-ReLU-ENN is noticeable but it needs a zoom to be seen and no error bars are given to assess this more quantitatively. Moreover, the performance on CIFAR-10 is extremely low on the order of 60%, very far from even basic networks on this problem: this is a very worrying sign that maybe some procedures are not conducted properly (parametrization, optimization, evaluation, ...). At the very least this should be commented, but it shows that the experiments are very far from a practical setting and it bears the question of whether these results would hold on SoTA INNs. I want to clarify that I understand it's not the matter of this paper to beat SoTA in any shape or form, but the numbers on CIFAR-10 are so low that it's an orange flag to me.\n\nMinor:\n- there are a lot of typos -> grammarly or Ltex workshop can help reduce these (or even the use of LLMs)\n- Since the computational efficiency of INNs is a matter here, I think it would be interesting to cite relevant literature that tries to accelerate the training and inference of such networks (Neural DEQ solver [E], SHINE [D], Jacobian-free backprop [B], warm start [A, C])\n- It is said \"Following (Feng and Kolter, 2020), we denote the corresponding Implicit-CK as\": here I think it would be important to explain that this is not a definition per-se but rather a property of the CK of INNs derived by Feng and Kolter 2020, and also give the initial definition.\n- There are a lot of notations, which is very confusing for any reader IMO, I think a lot could be improved by moving the NTK results in the appendix and only mention them in the core text, and move a lot more of the intermediate computations in the appendix.\n- In the paragraph \"The existence and the uniqueness of Implicit-CKs and Implicit-NTKs\", $z$ are not introduced before (I think they are only in the appendix), and it's confusing because another $z$ is used to define INNs in eq (1) and (2).\n- the label vector $j$ could be called one-hot-encoded label vector this would help readers understand what it is more than the dirac notation I think.\n- remark 1 is proved in the appendix, so it should be mentioned.\n\n\nRefs:\n[A] Micaelli, Paul, et al. \"Recurrence without Recurrence: Stable Video Landmark Detection with Deep Equilibrium Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[B] Fung, Samy Wu, et al. \"Jfb: Jacobian-free backpropagation for implicit networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 6. 2022.\n[C] Bai, Shaojie, et al. \"Deep equilibrium optical flow estimation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[D] Ramzi, Zaccharie, et al. \"SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models.\" arXiv preprint arXiv:2106.00553 (2021).\n[E] Bai, Shaojie, Vladlen Koltun, and J. Zico Kolter. \"Neural deep equilibrium solvers.\" International Conference on Learning Representations. 2021."
            },
            "questions": {
                "value": "- why are some assumptions named condition?\n- why is it important to study CKs to show equivalence between INNs and ENNs?\n- is it possible to have a bias in the parametrization of INNs in this work?\n- how relevant is assumption 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "This is somewhat minor in the sense that it's the only part I checked, but I noticed that the first paragraph of the introduction as well the first part of the second paragraph of the introduction are copy-pasted with a few language changes from \"Global Convergence of Over-parameterized Deep Equilibrium Models\" Ling et al. 2023 (I happen to be reading it for another review).\nOf course it's only the introduction and it's not like it's a major part of it, but it raises concerns as to other parts of the submission, especially since this work is not even cited..."
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698606672695,
        "cdate": 1698606672695,
        "tmdate": 1699635952815,
        "mdate": 1699635952815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XNO0uipdjY",
        "forum": "Q5LuORNY2A",
        "replyto": "Q5LuORNY2A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the connection between implicit and explicit neural networks. The authors show that, with well-designed activation functions and weight variances, a relatively shallow neural network is equivalent to the implicit neural network in terms of eigenspectra of the neural tangent kernel and conjugate kernels. Additionally, the authors complement their theoretical results with numerical experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper studies an important question, that is, whether implicit neural networks have advantages over explicit neural networks. The authors is well written and easy to follow and without so many errors. The steup and results are clear."
            },
            "weaknesses": {
                "value": "1. A primary concern is the paper's ambitious claim, that is, **any** implicit neural networks is equivalent to relatively shallow networks in terms of the eigenspectra of NTK and CK. While the paper establishes this equivalence in Theorems 1 and 2, it does so under highly specific conditions. These conditions involve setting the variance hyperparameters to exceptionally small values, leading to forward propagation acting as a contraction mapping and converging at an exponential rate. This configuration may restrict the expressive capacity of implicit neural networks, effectively making them resemble relatively shallow networks. However, this setup is common. Consequently, it raises doubts about the validity of drawing broad conclusions based on this specific scenario. For instance, Neural ODEs, which are also categorized as implicit neural networks, do not adhere to Condition 1.\n\n2. Another concern arises from the authors' requirement that the input data $x$ follow Gaussian mixtures. This approach might be restrictive since, for large p, the mean $\\mu/\\sqrt{p}$ and covariance $C/p$ tend to zero, resulting in inputs that are all close to zero. Under these conditions, Theorems 1 and 2 hold as the authors also assume both n and p approach infinity at the same rate. In such cases, the network may struggle to distinguish inputs $x$, making it less relevant to assess whether a network is shallow or deep. I'm not an expert in Gaussian mixture models, so please feel free to correct me if my understanding is incorrect. Additionally, I plan to review comments from other experts in this area if available.\n3. The paper defines intermediate transitions in equations 1 and 2 but doesn't provide a clear definition of the neural network itself. This omission is notable because it's essential to have a precise understanding of the network's structure. Additionally, it's worth noting that $z^*$ cannot be considered the network's output, as its dimension varies depending on the network's design.\n4. There is a typo in Equation (10) concerning the definition of T.\n5. While the paper extensively studies NTK and CKs, it would be beneficial to complement the theoretical analysis with experiments that demonstrate the behavior of $|G^*-\\Sigma^2|$ for *finite-width* networks using simulations. This would provide more practical insights into the implications of the findings."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_9F3o"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713098548,
        "cdate": 1698713098548,
        "tmdate": 1700594410926,
        "mdate": 1700594410926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7t7P90txpo",
        "forum": "Q5LuORNY2A",
        "replyto": "Q5LuORNY2A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
        ],
        "content": {
            "summary": {
                "value": "The authors show that the conjugate kernel (CK) and the neural tangent kernel (NTK) of implicit NNs can be approximated by surrogate kernels, based on an operator norm, when the data are distributed according to a particular Gaussian mixture model. In addition, they show how to construct shallow explicit NNs for which the associated CK and NTK are close to the surrogate kernels of the implicit NNs. The claim is that any implicit NN can be approximated by a shallow explicit NN. In the experiments, they demonstrate the performance of the proposed explicit NNs compared to the original implicit NNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of constructing efficient explicit NNs to approximate implicit NNs is very interesting.\n- The technical part of the paper seems solid and sensible, however, I have not verified the theoretical results.\n- In the experiments, the proposed approximate explicit NNs seem to work as expected similar to the original implicit NNs."
            },
            "weaknesses": {
                "value": "- The paper is fairly well written for a mainly theoretical work. However, I think that the text can be improved to become more accessible to non-experts in the implicit NNs field. For example, the proof of Corollary 1 can be moved to appendix to save space.\n- The theoretical results rely on many assumptions, for example, the distribution of the data to follow this particular Gaussian mixture model. When $p$ is high, it seems that the associated GMM becomes degenerate."
            },
            "questions": {
                "value": "The surrogate kernels $\\overline{G}$ and $\\overline{K}$ are supposed to be random matrices, but in which sense, as they seem to be expectations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission275/Reviewer_aCT6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission275/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699036262120,
        "cdate": 1699036262120,
        "tmdate": 1700644227892,
        "mdate": 1700644227892,
        "license": "CC BY 4.0",
        "version": 2
    }
]