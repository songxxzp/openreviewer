[
    {
        "id": "ia9u1rp9ce",
        "forum": "TjfXcDgvzk",
        "replyto": "TjfXcDgvzk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_CrTj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_CrTj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a technique to reduce the number of trainable parameters while fine-tuning large language models and large vision encoders. Their technique involves modeling the weight update matrix as a linear combination of fixed random matrices that are also rank-constrained. The fine-tuning process then involves learning just the coefficients in the linear combination. When the proposed technique is used, any updates to the models requires the communication/ storage of only the coefficients and the random seeds required to generate the codebook matrices (apart from the base weights of the original model of course). \n\nThe authors demonstrate that the proposed technique preserves the performance on the fine-tuning task while achieving large ( up to 1/20 x baseline models) reduction in the number of trainable parameters. This is shown in both language and vision domains. In the language domain, they show it on the NLG challenge dataset. In the vision domain, they show is on CIFAR, CUB and Caltech-101 datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well motivated and clearly written. \n- Parameter-efficient fine-tuning is a popular area of research currently and this paper makes a good contribution to this area.\n- The proposed technique is interesting and the results demonstrate that the method preserves performance while achieving a low parameter count \n- The proposed method helps overcome the some of the limitations of methods such as LoRA, as described in the paper"
            },
            "weaknesses": {
                "value": "I appreciate the results provided in the paper. But I think that some more in-depth evaluation and some more explanation of the current results would add value to the paper. I outline some specifics below.\n\nLanguage experiments: \n- Why does full fine-tuning achieve much lower performance in Table 1?\n- Could the authors also provide the performance of the GPT-M, L models *without any fine-tuning* on the tasks considered? This will give the readers an idea of how much improvement is being achieved. \n\nVision experiments: I find the experiments provided in this section (3.3) a bit weak. The main reason being that the ViT models pre-trianed on imagenet are already pretty powerful. Fine-tuning these models on much smaller and easier datasets such as CIFAR may not be the best way to demonstrate usefulness. In particular, I have the following comments:\n\n1. Can the authors try their techniques on more challenging datasets? \n2. Although the number of parameters for the linear layer baseline depends on the dataset, it would be good to have this information visible in the Table. \n3. As before, can the authors provide the performance of the models considered without any fine-tuning? (0-shot classification on the downstream datasets)"
            },
            "questions": {
                "value": "1. What is the distribution used to generate the random basis matrices? Did the authors experiments with a few different choices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698445191014,
        "cdate": 1698445191014,
        "tmdate": 1699636071542,
        "mdate": 1699636071542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RQME12fRwF",
        "forum": "TjfXcDgvzk",
        "replyto": "TjfXcDgvzk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_WWfE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_WWfE"
        ],
        "content": {
            "summary": {
                "value": "The paper looked at the problem of memory requirements for Low-Rank Adaptation (LoRA) and proposed NOLA to break the rank one lower bound present in LoRA. The core concept behind NOLA is to reparameterize a neural network using a linear combination of pseudo-randomly generated weights."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper discusses related works in detail and clearly summarizes its own contributions.\n2. The paper performs extensive experiments to compare NOLA and existing PEFT solutions.\n3. NOLA decouples trainable parameters from the choice of rank and the network architecture."
            },
            "weaknesses": {
                "value": "1. The work may need more rationales upfront to motivate the problems (i.e. the rank one lower bound present in LoRA). Given that mainstream GPUs have tens of GB of memory, it is reasonable to reduce the memory requirements from tens of GB to tens of MB at the expense of model quality through LoRA, as this can indeed reduce resource consumption and greatly reduce LLM transition overhead during inference. However, I don't think it makes much sense to further reduce memory requirements to several MBs at the expense of model quality."
            },
            "questions": {
                "value": "If users want to use the trained model on different versions of GPUs or software, how to ensure the consistency of the trained model? In such a situation, the same seeds can not generate the same pseudo-random matrices."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1431/Reviewer_WWfE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703544766,
        "cdate": 1698703544766,
        "tmdate": 1699636071433,
        "mdate": 1699636071433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dTZUsXHBIk",
        "forum": "TjfXcDgvzk",
        "replyto": "TjfXcDgvzk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_ba8s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_ba8s"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach for fine tuning LLMs for downstream tasks. The key idea is to replace the low rank updates of LoRA with linear combinations of fixed random matrices for which only the coefficients need to be tuned and stored in memory which significantly reduces the storage cost. The authors present experiments in both language and vision tasks where their approach preserve the accuracy of LoRA while reducing the parameter count by half or more."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors propose a novel, intuitive, and principled approach to address the problem of task based fine tuning of transformer based models.\n\n2. The proposed approach shows significant reduction in storage overhead without compromising on accuracy across a range of experiments in both language and vision tasks."
            },
            "weaknesses": {
                "value": "1. The technical novelty is relatively minor with the overall idea being a combination of prior works PRANC and NOLA. While this seems enough to provide empirical improvement, the approach itself is not that big of an innovation over prior works. \n\n2. While the prior approach PRANC is directly modified by the authors in this work there are no direct comparisons with it in either the language or vision tasks used to evaluate the proposed approach. There is a comparison of training loss in Section 3.4 and a comparison of the rank of possible solutions of the two approaches in Section 3.5 but without a direct comparison of test accuracy it is unclear if this approach is indeed an improvement over the baseline that it directly modifies."
            },
            "questions": {
                "value": "1. Why is the training time of NoLA with shared random basis similar to that of LoRA when the training time of NOLA with a unique random basis is higher? Aren't the number of coefficients being trained, the same in both cases?\n\n2. The ablation study at the end of Section 3.1 appears inconclusive. Is there any takeaway on the effect of varying the rank in NOLA?\n\n3. In Section 3.2 if only $\\alpha$ and $\\beta$ are quantized while A and B are not then won't that be less memory efficient than quantization in LORA?\n\n4. Please highlight the entries in Table 5 with the best performance for a given scenario. Currently there are too many entries, and it is too difficult to figure out which method is better for which case.\n\n5. If each matrix in PRANC has size $d^2$ then why do we need multiple matrices to cover the rank of the original $\\Delta W$ matrix (which also has size $d^2$)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699086084664,
        "cdate": 1699086084664,
        "tmdate": 1699636071371,
        "mdate": 1699636071371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u4QAaagKGM",
        "forum": "TjfXcDgvzk",
        "replyto": "TjfXcDgvzk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_P5nc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1431/Reviewer_P5nc"
        ],
        "content": {
            "summary": {
                "value": "Low Rank Adaptation (LoRA) presents a series of drawbacks, particularly its constrained parameter reduction due to rank-1 matrices, which cannot be further diminished. Additionally, LoRA's parameter count is heavily reliant on the model's architecture. In response to these limitations, this paper suggests an innovative solution by advocating the use of a linear combination of random projections to replace LoRA's update matrix, effectively addressing the issues mentioned earlier. This approach is inspired by the previous paper known as PRANC. Personally, I found the paper to be a valuable source of knowledge and a unique one, appreciating its quirky yet straightforward idea"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Leveraging Ideas from Other Papers for Enhanced Parameter Efficiency:** This paper skillfully incorporates concepts from existing research to optimize parameter efficiency.\n\n**Achieving Comparable or Superior Performance to NOLA:** This research attains performance levels akin to LoRA while significantly enhancing parameter efficiency."
            },
            "weaknesses": {
                "value": "**Poorly presented results**-  The main issue in the presentation of the results lies in their lack of clarity and explanatory depth. Firstly, the results fail to offer any substantial insights into how the method operates, leaving readers without a clear understanding of the underlying mechanisms. Additionally, Tables 1 and 5 are presented as mere lists of numbers without the necessary context or explanation, making it challenging for the audience to derive meaningful conclusions from the data. A critical element that appears to be missing is a discussion of what works better and the reasons behind it, which is crucial for a comprehensive understanding of the findings. To improve the presentation of the main results, it is essential to provide better explanations and context for the data, as well as a deeper analysis of what drives the observed outcomes."
            },
            "questions": {
                "value": "1. GPT2-L and GPT-2M seems to perform the same for LoRA. Is there any explanation on why this is the case?\n2. The presentation of results preceding the training details in Section 3.1 seems to be an inadvertent oversight. To enhance the logical flow of the content, it is advisable to reverse the order, placing the training details before the results.What happens when you increase the number of parameters for NOLA?  - Does it perform better than LoRA. For example results of NOLA with 0.35M parameters\n3. How does NOLA's performance change when the number of parameters is increased? Does it outperform LoRA? For instance, are there any results available for NOLA with 0.35 million parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699194300921,
        "cdate": 1699194300921,
        "tmdate": 1699636071311,
        "mdate": 1699636071311,
        "license": "CC BY 4.0",
        "version": 2
    }
]