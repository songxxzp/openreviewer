[
    {
        "id": "lsdezTH5se",
        "forum": "UJeqE3gmg2",
        "replyto": "UJeqE3gmg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_uN8Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_uN8Y"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed \"Evolutionary Robust Policy Optimization (ERPO)\", which aims to adapt policies to an altered environment using fewer training steps while getting higher rewards and requiring lower overall computation time."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "see above"
            },
            "weaknesses": {
                "value": "This paper seems to be an incomplete work, fails to illustrate its motivation and main technical contribution, and misses a lot of important baselines (e.g., from meta-learning literature) in its experiments. Hence, I think it is difficult to evaluate the novelty, effectiveness, and importance based on its current version that needs to be improved significantly.\n\nsome comments:\n\n1. I wonder why evolutionary game theory is a proper solution to robust policy optimization.\n\n2. The title is too broad to reflect the main contribution of this work. \n\n3. Please add more baselines and compare ERPO with them in extensive environments in order to evaluate ERPO's soundness.\n\n4. incorrect citation format."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635116249,
        "cdate": 1698635116249,
        "tmdate": 1699636972305,
        "mdate": 1699636972305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8czBNNug0u",
        "forum": "UJeqE3gmg2",
        "replyto": "UJeqE3gmg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_X8NW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_X8NW"
        ],
        "content": {
            "summary": {
                "value": "The manuscript presents a novel method for transfer learning that combines ideas from evolutionary game theory and reinforcement learning. The method operates only with discrete state-action spaces. Overall, the results showcase that the proposed method, ERPO, consistently outperforms several baselines."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally well-written, easy to follow and the ideas are conveyed effectively.\n- Strong empirical results on several tasks\n- Theoretical convergence guarantees"
            },
            "weaknesses": {
                "value": "- No limitations or weaknesses are described in the text\n- No hints on how the method can be extended to continuous state-action spaces are given\n- The authors mention *\"Simulation models are generally simplistic and fail to consider environmental variables ... so they cannot be directly deployed in such applications\"*. I have the following issues with the statement:\n    - Simulation models are not generally simplistic. Even the less realistic simulators contain quite sophisticated procedures and models behind. Changing \"simplistic\" to \"non-realistic\" or just referring to the well-known Sim2Real gap is enough.\n    - Most real-world applications are continuous in nature and are quite difficult to discretize or solve with discrete state-action spaces. The manuscript proposes a method that can only operate with state-action spaces. Thus, this sentence is not the best \"motivation\" for the proposed method.\n- The above comment leads to my main \"complaint\" from the presented work: since most realistic robotic/autonomous systems applications are continuous in nature, how is the proposed method solving the issue it claims to solve? All the experiments as well have nothing to do with robotic applications.\n\nTypos/Minor comments\n===================\n\n- Page 2, first paragraph: *\"approaches Rajeswaran et al. (2016)train\"* -> there is a space missing\n- Page 3, Section 3, first paragraph: *\"theory (EGT) Smith (1982),Sandholm (2009)\"* -> there is a space missing\n- Page 3, last sentence: *\"As we make state-wise updates, we modify the replicator equation to be We modify this replicator equation as follows:\"*"
            },
            "questions": {
                "value": "- What are the main limitations of ERPO?\n- How can we extend ERPO to the continuous case?\n- How can ERPO be useful in realistic applications of autonomous systems/robotics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838358518,
        "cdate": 1698838358518,
        "tmdate": 1699636972164,
        "mdate": 1699636972164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MWsBXkZ0zt",
        "forum": "UJeqE3gmg2",
        "replyto": "UJeqE3gmg2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_nzVe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7919/Reviewer_nzVe"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of adapting reinforcement learning (RL) policies to significant changes in the environment dynamics in robust RL. Many existing methods, like domain randomization and robust policy optimization, fail when test environments differ substantially from training. The authors propose an evolutionary robust policy optimization (ERPO) approach to adapt policies without full retraining. Assuming access to the optimal blackbox policy on the original environment, ERPO explores the new environment using an $\\epsilon$-soft version of this policy. It incrementally improves the policy by weighting state-action pairs from fitter trajectories more highly, inspired by evolutionary game theory. Experiments show superior results against methods only trained on the old environment (referred to as base models) or only trained on the new environment. They also compare their algorithm to domain randomization methods such as PPO-DR."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's setup tackles an important real-world challenge - adapting black-box RL policies without full retraining. The simplicity and intuitiveness of ERPO are also strengths. Updating actions based on relative expected rewards is an interesting idea. This evolutionary approach avoids needing gradients for the new environment. However, there are some major concerns about the author's implementation of this idea, the theory, and the writing quality and experiments. Specific issues are detailed in the next section recommending rejection."
            },
            "weaknesses": {
                "value": "The main concern is about the soundness of the algorithm and theoretical claims. Even if we accept the sparse reward assumption, the conclusion of the authors that \"*the value of a state can be approximated by the average return across all trajectories containing the state*\" is an intuitive statement and needs concrete evidence. Even if we accept this argument, the proof of Theorem 1 is still incorrect. In particular, the proof mixes up the behavior policy $\\pi^i\\_{train}$ and the learned policy $\\pi^i_{new}$ and assumes they have the same sampling distribution, which is clearly not the case. Moreover, I think the current proof, even by fixing all the previous issues, would not work unless we define \n$$\\pi^{i+1}(a|s) = \\pi^i(a|s) \\times \\frac{\\mathbb{E}[f(\\tau\\_{(s,a)})]}{\\mathbb{E}[f(\\tau\\_{s})]}$$\nwhere, in the denominator, we have $\\tau\\_{s}$ instead of $\\tau\\_{s'}$. \n\nBesides the previous concerns, I think the empirical comparison to other methods is unfair. In particular, the proposed method essentially uses the information from the old environment (through the optimal policy) and the data from the new environment. A more fair comparison would initialize the policy of any method that trains on the new environment as the previous policy (e.g., using a cross-entropy loss). The fact that PPO eventually gets to the optimal solution (even without the correct initialization) suggests that initializing its policy with $\\pi\\_{old}$ will result in comparable or even better results than ERPO. \n\nIn summary, concerns include the following:\n1. Unjustified approximations in analysis\n2. Logical gaps in the convergence proof\n3. Unfair comparative evaluations against methods not exploiting old policy information"
            },
            "questions": {
                "value": "Please refer to the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7919/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7919/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7919/Reviewer_nzVe"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698880738610,
        "cdate": 1698880738610,
        "tmdate": 1699636972052,
        "mdate": 1699636972052,
        "license": "CC BY 4.0",
        "version": 2
    }
]