[
    {
        "id": "5yO2Hr2AJW",
        "forum": "Djw0XhjHZb",
        "replyto": "Djw0XhjHZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_rtvN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_rtvN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes using differential $k$-forms in $\\mathbb{R}^n$ to create representations of geometric simplices. Specifically, *neural* $k$-forms are parameterized by MLPs that take geometric information. By then integrating the $k$-forms using the simplicial complex, the authors obtain the *integration matrix*, which can be read out for further processing. The method allows processing simplicial data without the need for (simplicial) message passing. Solid theoretical results are supported with several motivating experiments, opening new directions for future research."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The presented ideas are original, relevant and opening up new directions of research.\n* The theory, though not trivially graspable for the typical machine learner, is presented clearly.\n* The theory is motivated with convincing experiments."
            },
            "weaknesses": {
                "value": "* While the content of the paper is naturally foreign for most machine learning researchers, the authors could put a bit more effort into making the content slightly more accessible. I would encourage them to include as much examples as possible, especially since the full 9 pages were not completely used yet. The authors can gain even more space by e.g. leaving out the introduction to MLPs, which should be familiar for the majority of readers.\n* Experimental details are highly suppressed, it is not quite clear what the inputs, outputs, and learning objective of the experiments are.\n\nMinor:\n* Above eq. 1, you introduce the notation of a convex hull using $v_0, v_1, \\dots, v_k \\in \\mathbb{R}^n$, but before and after you use $v$ and $v_i$ for vertices in $V$. This caused some confusion for me while reading this part. Similarly on page 3 when introduction $k$-vectors.\n* Experiments are mostly limited to the synthetic setting."
            },
            "questions": {
                "value": "* Could you discuss the unique linear map $\\phi|_\\sigma: \\Delta^k \\to \\mathbb{R}^n$? It is not quite clear how this function for $k < n$ is well-defined?\n* At equation 9, I was a bit lost how we define $\\phi$. Before eq. 4 we define $\\phi: S \\to \\mathbb{R}^n$, but here $\\phi$ seems to take $t \\in \\Delta^k$ which seems more in line with $\\phi|_\\sigma$. Did we suppress some arguments here, or are we assuming something about the elements of $S$?\n* What are the inputs and outputs to your neural networks?  Are the positions of the node locations input as $(\\mathbb{R}^d)^n$ when you have $n$ points, or do you concatenate them?\n* Could you elaborate a bit more on how you go from $\\mathbb{R}^{\\binom{n}{k} \\times \\ell}$, together with the simplicial complex structure, to the integration matrix $X_\\phi(\\beta, \\omega)$? Specifically, in eq. 13, the sum is over all multi-indices, correct? Do we need to know $\\lambda_a$ somehow? \n* Could you discuss the advantages of your method over (simplicial) message passing? Why does it yield better empirical results? Should this method replace message passing altogether, or are there limitations?\n\n\n\n**Final Remarks**. Admittedly I'm lacking some background knowledge to objectively assess the paper. Despite that, I'm really excited about this project. However, I think if the authors put a bit more effort into conceptualizing some core ideas, the paper would improve considerably."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Reviewer_rtvN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698001312945,
        "cdate": 1698001312945,
        "tmdate": 1700736846347,
        "mdate": 1700736846347,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dZ5e5Zgxyy",
        "forum": "Djw0XhjHZb",
        "replyto": "Djw0XhjHZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_gTeC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_gTeC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel method for representation learning of geometric simplicial complexes, in which a differential form parameterized by a multilayer perceptron is integrated over the simplices of an embedded complex, and these integrals are then read out into a representation of the complex that is not dependent on the size or dimension of the complex. This method is considered in a variety of tasks for processing simplicial complexes and graphs with geometric information."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The approach proposed by the authors is novel, yet simple, and rooted in a geometric view of learning for simplicial complex data.\n2. The proposed method is able to apply one learned function (k-form) to tasks over many geometric simplicial complexes, rather than only learning a function for a given complex."
            },
            "weaknesses": {
                "value": "1. This paper suffers from vagueness in some parts, particularly in the experiments section. This is my reason for rating the presentation as \"fair.\" Otherwise, the writing of the paper is good.\n2. Compared to message-passing schemes, the learned k-forms appear to be highly dependent on the particular geometric embedding of the complex, instead of the intrinsic topology/geometry.\n3. The type of tasks considered by the authors are not common ones in the literature, so I am not sure if there is a regime in which the proposed methods can be compared to existing simplicial neural networks, which are largely based on combinatorial, rather than geometric, information. However, there are message-passing graph neural networks that are designed to handle geometric information as well, which the authors did not compare to."
            },
            "questions": {
                "value": "1. My primary suggestions relate to the writing of the experiments section. There is not sufficient detail in Section 5 to understand each of the experiments. I think it would be better to shorten some of the background material in order to have space for better descriptions of your experiments. In particular:\n\na. Path classification: are the \"paths\" that you classify themselves simplicial complexes? I.e., is the idea to represent each path as a 1-d simplicial complex embedded in $\\mathbb{R}^2$, and then integrate against the simplices of the path? I see this to be the case in the appendix, but these details should really be in the main body of the paper for the purpose of readability.\n\nb. Visualizing simplicial Laplacian 1-eigenvectors: the description of this is far too short to understand what is going on here, and the utility of this is questionable. Comparing to the eigenvectors of the simplicial Laplacian is reasonable for the purpose of identifying (co)homological features of the complex, but wouldn't it be better to use neural k-forms to get a proxy for the intrinsic eigenfunctions of some manifold embedded in $R^n$ from which a complex is formed? This experiment needs a lot more motivation.\n\nc. Synthetic surface classification: again, having a description of the dataset in the main body of the paper is needed for readability here. It is fine to leave some details in the appendix, but the body of the paper should be sufficient to get a good understanding of what is going on.\n\n2. I have some questions about invariants that can be incorporated into these networks. As I commented in the weaknesses section, the function learned by neural k-forms is highly dependent on particular geometric embeddings of the simplicial complexes in Euclidean space, as opposed to the intrinsic geometry of the complex. For instance, if I apply a translation to the embedding of the complex, I am likely to get a completely different result when integrating the neural k-form against it. Do you have any comments or insights on handling issues like this? This relates also to your experiment on classifying molecular graphs: the graphs can be geometrically embedded in $\\mathbb{R}^3$, but is there a canonical rotation that these embeddings should have? Two identical molecules might be embedded in different ways that yield different results by the network, which is not a property that a message-passing network suffers from. Perhaps a group symmetry should be incorporated for this issue. See the following reference, for instance:\n\nHan, J., Rong, Y., Xu, T., & Huang, W. (2022). Geometrically equivariant graph neural networks: A survey. arXiv preprint arXiv:2202.07230.\n\nI think comparison between neural k-forms and geometric GNNs should be made, in order to yield a fairer picture of where your method stands for graph classification tasks.\n\n3. Can you please comment on whether or not simplicial complexes with non-geometric features could be incorporated into the method you propose? For instance, if the geometric complex not only has an embedding, but also a particular (co)chain supported on it supplied, how would one incorporate that information into a learning pipeline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Reviewer_gTeC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776912968,
        "cdate": 1698776912968,
        "tmdate": 1700715438408,
        "mdate": 1700715438408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gg1YRZbhgH",
        "forum": "Djw0XhjHZb",
        "replyto": "Djw0XhjHZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_E4oE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_E4oE"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the neural representation learning over simplicial complexes. Specifically, They propose to embed the simplician complex into a integration matrix. The integration matrix is computed by the integral of neural k-forms, which is a linear combination of k-forms weighted by MLP outputs. They argue that many previous works rely on an non-canonical initialisation of feature cochains to compute the representation, while their method bypass this problem as integration matrix does not involve the cochains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The background introduction is comprehensive and technical part seems sound"
            },
            "weaknesses": {
                "value": "- The baseline GAT, GCN and GIN in experiments are too weak: these are the most basic message passing GNNs. Comparisons with other neural networks on simplicial complexes could make the evaluation more solid."
            },
            "questions": {
                "value": "- I think the whole pipeline is not very clear to me: when producing the integration matrix using neural k-forms, which are the learnable parameters and which are the hyper-parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Reviewer_E4oE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698959187304,
        "cdate": 1698959187304,
        "tmdate": 1699636537001,
        "mdate": 1699636537001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pNCYh2c4lT",
        "forum": "Djw0XhjHZb",
        "replyto": "Djw0XhjHZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_pNcs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_pNcs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a geometric deep learning method termed neural k-forms. The proposed method leverages geometric information of data and creates representations of simplices using differential k-forms. The authors show the universal approximation property of the neural k-forms in approximating k-forms by applying differential geometry tools. Through several stylized tasks, the authors demonstrate the effectiveness and interpretability of the approach. The proposed neural k-form also outperforms message passing based state-of-the-art graph neural networks in several real-world graph classification benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper provides an interesting view of MLPs as neural 0-forms and extend it to neural k-forms to harness the geometric information in data via embedded simplicial complexes.\n2. The integration of neural forms is introduced to construct finitely-parametrizable, learnable singular cochains such that the neural k-form is usable in practice. Moreover, the integration matrix induced by a neural k-form produces feature cochains that facilitate training process and permit interpretation.\n3. Albeit rather technical and math-heavy, the presentation is self-contained and consistent, and the authors provide strong connections between mathematical machinery and model intuition.\n4. The proposed method generates interpretable features in synthetic tasks and outperforms standard GNNs by a large margin in several real-world graphs."
            },
            "weaknesses": {
                "value": "1. The neural k-form relies heavily on the geometric information of the node features and makes use of the simplicial complex structure of data but to a lesser extent the graph information. Learning on graphs with weak or no geometric information can pose a great challenge to the proposed method.\n2. The computation feasibility for higher order k-forms and the numerical integration may hamper their application in practice.\n3. The approach triumphs in graph classification thanks to the simplicial representations but may suffer in node prediction when smoothing is a blessing."
            },
            "questions": {
                "value": "1. How does the neural k-form compare to other simplicial neural networks in terms of performance and computation?\n2. Is there a way to incorporate convolution in neural k-forms?\n\nTypo: in eq. (6), $d_{x_{i_k}}$ should be $dx_{i_k}$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Reviewer_pNcs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699079004917,
        "cdate": 1699079004917,
        "tmdate": 1700667584034,
        "mdate": 1700667584034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TycAdhiTZM",
        "forum": "Djw0XhjHZb",
        "replyto": "Djw0XhjHZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_VjaY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5340/Reviewer_VjaY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduced a new method, neural k-form,  of utilizing integration of differential forms over embedded simplicial complexes for robust and interpretable data representations. The authors provided necessary background information. Then they introduced the basic idea of neural k-form, and showed that neural k-form can approximate any k-form with compact support. The authors discussed the concept of integration matrices and their basic properties and presented numerical examples of classification problems utilizing the neural k-form method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors tap into the rich area of topology and geometry, established a potentially very powerful connection."
            },
            "weaknesses": {
                "value": "Overall, I feel that while the method proposed in the paper has potential it has not been fully developed and analyzed. \n\nMany statements, for example, \"Thus an l-tuple of k-forms produces an l-dimensional representation of the simplex independently of any message passing\" in the introduction and \"In practice, this will facilitate the training process and permit interpreting the learned cochains\" in remark 5, are not backed up by detailed analysis. \n\nThe neural k-form learning architecture is presented very briefly in the caption of a figure, it is very difficult for others to figure out how to implement the method. \n\nIt is understandable audience would want to find out the performance of the method proposed for high dimensional problems. Unfortunately, the experiments are limited to 1-form and 2-form, and the differences in performance are not very significant except in the last experiments."
            },
            "questions": {
                "value": "On page 3, why is the parallelepiped spanned by tangent vectors called \"infinitesimal parallelepiped\"? The intuition and the formal definition are not very well connected. \n\nOn page 4, second paragraph, should $b\\in R^{1\\times m}$ instead of $b\\in R^{1\\times n}$?\n\nCan you provide an argument for your stamens on the dimension of singular k-chains?\n\nOn Page 15, right below equation (38), should it be $\\alpha_{i,j}$ instead of $f_{i,j}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5340/Reviewer_VjaY"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699570312306,
        "cdate": 1699570312306,
        "tmdate": 1699636536821,
        "mdate": 1699636536821,
        "license": "CC BY 4.0",
        "version": 2
    }
]