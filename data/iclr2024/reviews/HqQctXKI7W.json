[
    {
        "id": "ybEf84VeOt",
        "forum": "HqQctXKI7W",
        "replyto": "HqQctXKI7W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_6wEZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_6wEZ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces *diffusion ensembles for capturing uncertainty (DECU)*, a framework that estimates the epistemic uncertainty via training an ensemble of conditional diffusion models and computing the epistemic uncertainty by Pairwise-Distance Estimators (PaiDEs). In particular, \n- the training of each class-conditioned diffusion model is carried out in an efficient fashion by keeping the pre-trained UNet and autoencoder static and only training the conditional portion, *i.e.*, the embedding network for the class label input;\n- the utilization of PaiDEs enables analytic computation of the epistemic uncertainty for high-dimensional image data. The authors propose to use the 2-Wasserstein distance, which is particularly tailored for the parameterization of stable diffusion. \n\nThe paper shows through experiments on curated Imagenet dataset that classes with larger amount of training data obtain smaller estimation of the epistemic uncertainty (mutual information between the latent variable at a particular timestep and the model parameter), which is consistent at the conceptual level with what one would expect regarding the change in the epistemic uncertainty with respect to the amount of training data. The paper claims to be the first work to address the problem of epistemic uncertainty estimation for conditional diffusion models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provides a framework to estimate epistemic uncertainty under the context of image generation, a new and somewhat unexpected setting for epistemic uncertainty estimation. Given that image generation is one of the most committed domains for the application of diffusion models, this work might help to shed new light on the image-related tasks from a more statistical angle.\n2. The authors included the source code in the supplementary materials to facilitate reproducibility."
            },
            "weaknesses": {
                "value": "1. The motivation of estimating the epistemic uncertainty for image generation is unclear. The experiment result, *s.t.* different diffusion models would generate images with higher levels of variation on classes with fewer training data, is very much in line with our expectations as well as with empirical results even before computing the epistemic uncertainty. In other words, it\u2019s unclear how the computation of epistemic uncertainty could be helpful in practice. (On the contrary, computing epistemic uncertainty under an active learning setting is very well-motivated.)\n2. The estimation of epistemic uncertainty from the background and the methodology section focuses on the quantity $I_{\\\\rho}(y_{t-1},\\theta\\vert y_t,x)$, where the time interval is 1 timestep. Meanwhile, the experiment section reported $I_{\\\\rho}(z_0,\\theta\\vert z_5,x)$, which has a time interval of 5 timesteps. It\u2019s unclear how the mutual information with an interval of multiple timesteps can be derived from the quantity with 1 timestep.\n3. Different classes are being used to compare epistemic uncertainty; a more apples-to-apples comparison would be to train multiple ensembles on the same class with different number of training instances, and computing PaiDEs on the generated images for that class."
            },
            "questions": {
                "value": "1. Could the authors elaborate on the usage of 8 samples of random noise for the computation of epistemic uncertainty in Section 4.1? Given a particular component, 8 images generated from 8 different noise samples would represent aleatoric uncertainty instead of epistemic uncertainty.\n2. Could authors explain the sentence in Section 4.1, \u201cFor bin 1300, we observe that epistemic uncertainty highlights different birds that could have been generated from our ensemble\u201d? For a class with sufficient amount of training data, the variation among generated samples shall represent aleatoric uncertainty instead.\n3. Could the authors provide an explanation (or a high-level intuition) for how the mutual information between *data* and *model parameter* could represent epistemic uncertainty \u2014 the uncertainty that captures the lack of knowledge? From Eq. (4), it\u2019s makes sense to view the epistemic uncertainty as the difference between the total uncertainty and the aleatoric uncertainty; but the quantity of mutual information alone doesn't seem to say a whole lot about the level of ignorance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Reviewer_6wEZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698275441379,
        "cdate": 1698275441379,
        "tmdate": 1699636912143,
        "mdate": 1699636912143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vN2ZP4pOoM",
        "forum": "HqQctXKI7W",
        "replyto": "HqQctXKI7W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_LLz1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_LLz1"
        ],
        "content": {
            "summary": {
                "value": "Authors introduce a novel method for modeling epistemic uncertainty within diffusion models through the use of ensembles. Given that training an ensemble of models can be computationally demanding, the authors have devised a scheme to freeze a substantial portion of the model, consequently mitigating computational demands. To substantiate their approach, the authors offer a demonstration of its effectiveness on the Imagenet dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Authors addressed an important challenge of modeling epistemic uncertainty in diffusion models. To the best of my knowledge, this is the first demonstration of modeling epistemic uncertainty in diffusion models. This can be very valuable, for example:  this holds the potential to provide valuable insights into whether the model has been trained with a sufficient volume of data for a specific target label. \n\nAuthors make use of ensembles to model epistemic uncertainty. Since creating ensembles of models can often be computationally expensive, the authors freeze a substantial portion of the model using pre-trained weights, and focus their training efforts on the final few layers. This strategy significantly enhances computational efficiency.\n\nThe authors initially introduced the concept of epistemic uncertainty by framing it within the context of mutual information to provide an intuitive understanding. Subsequently, they employed PaiDEs to approximate this uncertainty. Throughout the entire work, the authors consistently provided illustrative examples and intuitive explanations at each stage. This approach is highly commendable and greatly enhances the clarity and accessibility of the material."
            },
            "weaknesses": {
                "value": "Some of the details in the experimental setup are lacking. I was unable to find the number of ensemble particles used in the experiments. \n\nThe uncertainty distribution across different bins seems to be very similar without a huge difference. For example: see Fig 3. In Fig 3, even though labels in bin 1 are trained with single datapoint, uncertainty is pretty small. It could be because ensemble particles only differ through random initialization. Authors might find the following work on alternate ensemble methods (ex: https://arxiv.org/pdf/2206.03633.pdf)  and epistemic uncertainty methods (ex:https://arxiv.org/pdf/2107.08924.pdf, https://arxiv.org/pdf/2006.07464.pdf) useful. \n\nIn Table 1, might be for the same reason as above, bins 1, 10, and 100 have very similar performance. Can authors offer some intution on why this could be the case.\n\nFurther comments:\n\n- Based on description in second paragraph of Section 1, PaiDEs were introduced for regression tasks. Can authors comment if there are any issues with its transferability to classification tasks. \n\n- It might be useful to describe the approach in Section 3.1 via a diagram indicating which parts of network are ensembled and which are frozen with pre-trained weights."
            },
            "questions": {
                "value": "It would be helpful if authors can kindly address comments in weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730705758,
        "cdate": 1698730705758,
        "tmdate": 1699636912019,
        "mdate": 1699636912019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z1g7jKg3xe",
        "forum": "HqQctXKI7W",
        "replyto": "HqQctXKI7W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_UPab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_UPab"
        ],
        "content": {
            "summary": {
                "value": "The authors present a novel framework for estimating uncertainty of latent diffusion models (LDM) by a) training ensembles of denoiser heads which start at a branching point in the denoising process and b) estimating epistemic uncertainty of the ensemble in a sample-free manner by relying on the pairwise statistical distance of the ensemble member latent distributions. The framework's effectiveness is demonstrated by efficiently fitting an ensemble of denoiser heads for an existing LDM trained on the ImageNet dataset, and showcasing its ability to produce diverse images when branching early and capturing epistemic uncertainty even for under-sampled image classes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework presents a significant advancement in the estimation of epistemic uncertainty for conditional diffusion models.\n- It is designed to work with high-dimensional data such as images, making it suitable for a wide range of real-world applications.\n- The use of Pairwise Distance Estimation (PaiDE) in the framework eliminates the need for repeatedly sampling latent vectors for estimating uncertainty.\n- The experiments confirm the intuition that branching further into the denoising process should lead to higher image similarity among ensemble members."
            },
            "weaknesses": {
                "value": "- The authors rely on the claim that the covariance matrices $\\boldsymbol{\\Sigma}_{\\theta}(y_t, t, x)$ are zero matrices in the LDM of [Rombach et al., 2022], however this is a non-trivial result which would benefit from a detailed derivation of the distributions for the latent vectors.\n- The authors state that training can be done in parallel however the paper does not discuss computational and/or memory complexity of the framework or experimental runtimes when compared to standalone LDM."
            },
            "questions": {
                "value": "- My impression is that the terms $\\\\boldsymbol{\\\\Sigma}\\_{\\\\theta}(y_t, t, x)$ are equal to $\\\\sigma^2\\_{t|t-1} \\\\frac{\\\\sigma^2\\_{t-1}}{\\\\sigma^2\\_t} \\\\mathbb{I}$, given Equation (11) of [Rombach et al., 2022]. This wouldn't change Equation (10) for the Wasserstein-2 distance of latent vectors at a same timestep $t$ since the trace term still cancels out. In any case, detailing how you arrive at this conclusion might avoid potential confusion here.\n- I don't disagree that the distance between the latent vectors can grow to infinity as you continue denoising after the branching point, but it would be interesting to plot out the estimates $\\\\mathrm{I}(z\\_{T-(b+1)}, \\\\theta\\\\ |\\\\ \\\\dots)$, $\\\\mathrm{I}(z\\_{T-(b+2)}, \\\\theta\\\\ |\\\\ \\\\dots)$, ... and so on, to see the point after which the uncertainty estimate tends to $-\\\\ln \\\\tfrac{1}{M}$.\n- Would also be nice for readers to have an example on a toy dataset which makes apparent the epistemic uncertainty recovered with the framework."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7545/Reviewer_UPab"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795118256,
        "cdate": 1698795118256,
        "tmdate": 1699636911891,
        "mdate": 1699636911891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bZrE11gETG",
        "forum": "HqQctXKI7W",
        "replyto": "HqQctXKI7W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_2AQN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7545/Reviewer_2AQN"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the epitemic uncertainty in diffusion models by constructing an ensemble of latent diffusion models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Diffusion model and uncertainty is an less explored area."
            },
            "weaknesses": {
                "value": "The writing of this paper is poor to me. Firstly, it is confusing what model is the one that the paper aims to measure the uncertainty. From the section 2.1, it seems that this paper is measuring the uncertainty of a supervised learning model. But in the later context, all the measurement is about a conditioned probably P(y_t | y_{t-1}). How does that switch happen?\n\nUsing Wasserstein distance to replace the distance measure in PaiDEs seems straightforward and it is hard to really count it as a contribution.\n\nExperiment sections are weak,"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699147962599,
        "cdate": 1699147962599,
        "tmdate": 1699636911790,
        "mdate": 1699636911790,
        "license": "CC BY 4.0",
        "version": 2
    }
]