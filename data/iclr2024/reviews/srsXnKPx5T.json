[
    {
        "id": "9Y3cwX1Jmv",
        "forum": "srsXnKPx5T",
        "replyto": "srsXnKPx5T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_hXyG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_hXyG"
        ],
        "content": {
            "summary": {
                "value": "This work studies the knowledge-grounded dialogue response generation systems (KRGs). The authors assume that there are multiple intermediate steps before the final response generation. \n\nThe authors find that there is always no direct and suitable supervised data to train such intermediate modules. To this end, this work proposes a general self-improving framework Hexa, training intermediate modules without ground-truth labels.  \n\nHexa framework first samples a bootstrap dataset even with the unmatched responses at each turn; then, Hexa tunes the model on the collected dataset to improve the performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Hexa has a good motivation.  KRG researchers are eager to see works that enhance the learning of intermediate modules.\n2. The proposed Hexa can be used for other tasks that also have several intermediate modules/steps.\n3. Multiple datasets are included for the empirical evaluation, and the proposed method mostly outperforms baselines, showing robust improvement."
            },
            "weaknesses": {
                "value": "1.  The authors have stated `2) a novel bootstrapping scheme with a guided prompt and a modified loss function for diverse and appropriate generation of intermediate and final responses to be self-trained;' as a major contribution. Nonetheless, there is no evaluation of the diversity in the main paper.\n\n2.  Although Hexa has notable improvement in the evaluation of the generated dialogue responses, this work lacks enough experiments to verify *Does Hexa really improve the final performance by improving the performance of intermediate modules?*. I think this is important.\n\n3. A large amount of text is piled up together in some paragraphs,  making this paper verbose and not easy to follow."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Reviewer_hXyG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656189173,
        "cdate": 1698656189173,
        "tmdate": 1699636144828,
        "mdate": 1699636144828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RlQresGgLk",
        "forum": "srsXnKPx5T",
        "replyto": "srsXnKPx5T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_dRhK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_dRhK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Hexa, a self-improving modular mechanism based on a novel bootstrapping scheme with a guided prompt and a modified loss function. The data augmentation procedure involves including the past responses as part of a random Alphabetical list along with the GT input for the bootstrapping process. Empirical results on different knowledge tasks (question answering, knowledge-grounded dialogue, open-domain dialogue, and task-oriented dialogue) shows the efficacy of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-motivated and clearly written in most parts. \n- Ablation study for different components of the system is interesting and provides valuable insights into the effectiveness of the proposed approach. \n- Human evaluation shows improvement over STaR on both training as well as held-out unseen dataset."
            },
            "weaknesses": {
                "value": "- It seems the relevant baselines haven\u2019t been used (such as LLaMA 2-chat, GPT3/4) where the field has evolved a lot in recent years.\n- The availability of code is not discussed. Implementation details related to the resources, framework, model cards, training days, etc. would help in reproducibility. \n- Formatting of the paper could be improved with all the tables positioned closer to the text where they are referred in Section 5.3 and 5.4. \n- It would have been interesting to see the performance of the model as the number of bootstrapped samples is linearly increased as briefly mentioned in Section 5.2."
            },
            "questions": {
                "value": "- Could the authors please clarify if the response was generated or ranked from a given list and how is the F1 score computed? \n- Could the authors provide more detail about the experiments involving SentenceBERT for the similarity measure and also an intuition why it performed better/worse than BLEU/ROUGE?\n- Could the authors provide more information about the human evaluation and if/how the inter-annotator agreement was computed? \n- Could the authors explain how this approach would work in real-life scenarios? \n\nSuggestions/Comments:\n- It would help to specify BB3 as BlenderBot-3 when first introduced in Section 5. \n- Section 5.5: Table 6 that -> Table 6 shows that \n- Section 6: entropy the -> entropy of the"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800102425,
        "cdate": 1698800102425,
        "tmdate": 1699636144732,
        "mdate": 1699636144732,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x7n6OtNV8Q",
        "forum": "srsXnKPx5T",
        "replyto": "srsXnKPx5T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_uni2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2122/Reviewer_uni2"
        ],
        "content": {
            "summary": {
                "value": "In order to overcome the problem of dialogue systems missing clear intermediate data, the authors propose a self-improving modular approach that enhances both intermediate and final response generation through the use of a modified loss function and a guided prompt scheme. According to their empirical research, HEXA can generate conversation more effectively and outperform earlier techniques on a variety of dialogue tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Self-Improving Mechanism: The method's ability to improve itself without ground truth data for intermediate steps is innovative and reduces dependency on large annotated datasets.\n\n2. Empirical Performance: HEXA demonstrates superior performance on various benchmark datasets for knowledge-grounded dialogue tasks."
            },
            "weaknesses": {
                "value": "1. Metric Relevance: In the age of sophisticated dialogue systems like ChatGPT, conventional metrics like F1 and ROUGE-L might not be the most reliable measures of performance.\n\n2. Lack of SOTA Comparison: The paper doesn't compare its method with state-of-the-art chat-based language models like LLama2-chat, which would be crucial for understanding its relative performance.\n\n3. Outdated Baseline Model: The use of the STAR model as a baseline for human evaluation is noted as a potential weakness due to its age, which might not accurately reflect the current advancements in the field."
            },
            "questions": {
                "value": "Given that the paper does not include a comparison with state-of-the-art chat-based LLMs such as LLama2-chat, how do you anticipate HEXA would perform relative to these models? Additionally, could you discuss any plans to test HEXA's methodology on these larger models to validate its effectiveness in a more competitive landscape?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2122/Reviewer_uni2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2122/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842713985,
        "cdate": 1698842713985,
        "tmdate": 1699636144670,
        "mdate": 1699636144670,
        "license": "CC BY 4.0",
        "version": 2
    }
]