[
    {
        "id": "QTKq5uzckn",
        "forum": "s8ROuWV96d",
        "replyto": "s8ROuWV96d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_1utk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_1utk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the feasibility of applying CNN's feature distillation to the ViT model. The main contributions are two folds: 1) They found that the shallow layer distillation is important and they apply the traditional feature mimicking on the shallow layers. 2) They found that the feature mimicking produces sub-optimal results when distilling on the deep layers. The paper borrows the MGD method for deep-layer distillation. Experiments are sufficient and the results show reasonable improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experiments are sufficient to verify the effectiveness of the methods.\n\n2. Revealing the differences between ViT distillation and CNN distillation is meaningful."
            },
            "weaknesses": {
                "value": "1. The technical contribution/novelty is quite limited. Firstly, applying feature-based distillation on the shallow layers is not novel since this is common in CNN distillation, e.g., FSP [1], AT [2], OFD [3], and VID [4]. One can easily apply them on all the layers. Second, the main discovery is the performance degradation when distilling the deep layers. However, the proposed method, ViTKD, mainly adopts MGD [5] as the solution to this problem, which is a direct application of CNN's distillation. If we look at Sec. 3.1 and 3.2, there is no fundamental difference between the listed equations and the ones in Sec.3 of the MGD paper. This makes the paper more like a technical report rather than a scientific paper.\n\n2. There are inappropriate statements in the contributions listed at the end of the Introduction section. 1) \"We reveal that the feature-based KD method for CNNs is unsuitable for ViTs.\" However, the proposed methods (Sec. 3.1, 3.2) are all CNN's distillation methods. 2) \"The distillation on the shallow layers is also important for ViT, **which differs from the conclusion in KD for CNNs.**\" Can you provide some references that explicitly make that conclusion on CNN KD? Is there any evidence before claiming that shallow layer distillation is not important for CNN KD?\n\n3. The necessity of using random masks is low. As shown in Table 10, when $\\lambda=0$, the method boosts the performance by +1.35 Acc. However, it can be further improved by only +0.29 Acc. if we carefully tune $\\lambda$. This significantly undermines the motivation of deep layer\u2019s Generation distillation, because one can see that the main improvement comes from the feature-based distillation just with one more additional block to process the student's features.\n\n\n[1] Yim J, Joo D, Bae J, et al. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. CVPR 2017.\n\n[2] Zagoruyko S, Komodakis N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. ICLR 2017.\n\n[3] Heo B, Kim J, Yun S, et al. A comprehensive overhaul of feature distillation. ICCV 2019.\n\n[4] Ahn S, Hu S X, Damianou A, et al. Variational information distillation for knowledge transfer. CVPR 2019.\n\n[5] Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan. Masked generative distillation. ECCV 2022."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Reviewer_1utk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468219731,
        "cdate": 1698468219731,
        "tmdate": 1699636095023,
        "mdate": 1699636095023,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aTYEwmy82G",
        "forum": "s8ROuWV96d",
        "replyto": "s8ROuWV96d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_bvvH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_bvvH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ViTKD, a feature-based knowledge distillation method for vision transformers. The key idea is to apply different distillation methods to different layers of ViTs, such as mimicking for shallow layers and generation for deep layers. The authors analyze the characteristics of different feature layers in ViTs to illustrate insights and conduct comprehensive experiments on ImageNet-1K and various downstream tasks to show the effectiveness of ViTKD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "# Strengths\n1.\tThe paper provides a novel insight that different layers of ViTs require different distillation methods, and proposes a simple and effective method based on this insight. The paper also analyzes the properties of different feature layers and attention maps of ViTs, which is helpful for understanding the behavior of ViTs.\n2.\tExtensive experiments are reported applying ViTKD to various vision transformers on ImageNet classification, showing consistent improvements.\n3.\tThe paper is generally well-written and organized, making it easy to follow the proposed method and experimental results."
            },
            "weaknesses": {
                "value": "# Weakness\n1.\tDownstream results are only compared to the baseline, not other distilled or pruned small ViTs.\n2.\tThe combination with logit-based distillation methods such as NKD is mentioned without sufficient explanation. It remains unclear whether more hyperparameters will be introduced on the basis of many hyperparameters (\u03b1, \u03b2, \u03bb), or make the method more complicated for other reasons. \n3.\tIn the main text, the author only compares the proposed method with a few baselines, and the logits-based methods compared are all from papers before 2021. The relatively new articles in Appendix A.2 are easily overlooked, so the comparison results of this part can be mentioned in the main text.\n4.\tMore analysis could be provided to better understand how and why ViTKD works.  For example, in Figure 4 the authors could add attention maps of some middle layers (such as layers 5 and 6 aligned with Figure 1) to show how they change after distillation. \n5.\tThe paper would benefit from a more detailed discussion of the limitations of the proposed approach. Providing insights into the potential weaknesses of the method would strengthen the paper's contribution."
            },
            "questions": {
                "value": "Please refer to the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565125835,
        "cdate": 1698565125835,
        "tmdate": 1699636094942,
        "mdate": 1699636094942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5MtqQFqxYu",
        "forum": "s8ROuWV96d",
        "replyto": "s8ROuWV96d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_LtTD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_LtTD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes ViTKD, a knowledge distllation method tailored for vision transformer. The authors claim that traditional feature distillation for CNN is not suitable for ViT. Thus, they propose feature generation using masked modeling. The method is validated in different backbones and downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Paper is well-written and easy to follow.\n2. The authors claim SOTA performance in multiple tasks, including image classification, semantic segmentation and object detection.\n3. The method is validated on multiple backbones such as DeiT and Swin-T."
            },
            "weaknesses": {
                "value": "1. The technical contribution is limited. $L_{lr}$  is proposed by FitNet [1].  $L_{gen}$ is borrowed from masked language (image ) modeling. The authors claim that they use NKD as the logit-based distillation method. This make the novelty weak.\n2. The motivation is not clear. The paper do not explain why feature generation (i.e. masked modeling) is effective to vit-based architecture. If  Table 1 and Table 5 are the support for they method, then the motivation is empirical and not generalized. When it comes to different teacher-student pair, the motivation may not be met. For instance, in Table 3, the performance on swin transformer (+0.52) is limited compared to DeiT (>=1.4). \n3. In 5.2, the authors use first two layers for feature disillation and the last layer for feature geneartion. However, they analysis do not provide enough insight to explain such decision. Hence, the experimental result looks random. When the student network architecture is changed, the same recipe can not guarantee the performance. In contrast, logit-based distillation is model-agnostic and general.\n4. By my understanding, the result in Table 2 uses the technique of logit-based method NKD. The ablation study in Table 6 does not show the performance of ViTKD without such a loss, making the claimed performance unclear.\n5. Regarding the transformer and knowledge distillation, this work misses some releated work such as DistillBert.\n\n[1] Fitnets: Hints for thin deep nets. ICLR 2015\n\n[2]DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
            },
            "questions": {
                "value": "1. If I do not misunderstand the description in 4.2, I am curious about the performance (Table 2) without a logit-based method. This result can help us to clear the contribution of this work.\n2. What is the metric to identify the layers for either feature distillation or feature generation?\n3. Table 4 shows improvement on different downstream tasks. However, ViTKD does not compare with other methods, making the claimed performance improvement unclear.\n\nGiven the concerns to the novelty and performance, I would suggest the author discussing these issues."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Reviewer_LtTD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656022075,
        "cdate": 1698656022075,
        "tmdate": 1699636094849,
        "mdate": 1699636094849,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3wrp8mgVkc",
        "forum": "s8ROuWV96d",
        "replyto": "s8ROuWV96d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_HQ1H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1669/Reviewer_HQ1H"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of how to perform feature distillation with vision transformers (ViTs). Preliminary observation indicates that, the methods designed for CNN\u2019s feature distillation are not applicable. Also, contrary to CNN\u2019s feature distillation, in ViT\u2019s the distillation for shallow layers is also important. For shallow layers the attention pattern between teacher and student are similar, however, this is not the case for deeper layers. Based on these observations, the paper proposes to handle the feature distillation for student\u2019s shallow layers and deep layers differently. For student\u2019s shallow layers, the goal is to mimick the teacher\u2019s shallow layers whereas for student\u2019s deeper layer, the goal is to generate the teachers deep feature after masking student\u2019s deep feature. Experiments have been performed with DeiT models, where a larger capacity DeIT model acts as a teacher and a relatively smaller capacity model becomes the student. Results on different tasks, including image classification, and downstream tasks, claim to achieve superior performance than baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Developing effective feature distillation methods for ViTs is an important research direction because the ViTs are increasingly used in several computer vision tasks and their real-world deployment requires fast inference times.\n \n- The idea of micmicking for student\u2019s shallow layers and generating for student\u2019s deeper layers is based on empirical observation that shallow layer features from student and teacher are similar in pattern while this is not the case with the deeper layers, which contain (strong) semantic information.\n\n- Beyond image classification, experiments have been performed for downstream tasks, including object detection and semantic segmentation. Results claim to consistently surpass the baseline methods.\n\n- The paper is mostly well-written and easier to go through. Ablation studies and some analyses have been reported on different design choices and hyperparameter sensitivity."
            },
            "weaknesses": {
                "value": "- The intuition behind why the generation mechanim was chosen to fill the gap between student\u2019s deep layers features and teacher\u2019s deep layer features is not very clear.\n\n- The initial empirical analyses is just based on visualization and perhaps comparison based on quantification of the disparity between two corresponding set of features could be more convincing. \n\n- What happens to student\u2019s deep layer features after applying the method of ViTKD is not clearly discussed?\n\n- How does the generative process between student\u2019s deep feature and teacher\u2019s deep feature allows student\u2019s deep features to obtain better semantic information? \n\n- Since there is a gap between the student\u2019s deep feature layers and teacher\u2019s deep feature, instead of resorting to generative way, wouldn\u2019t some relaxed loss formulation give similar or even better performance?"
            },
            "questions": {
                "value": "- Is it possible to use the proposed method for distillation with a teacher CNN model and a student ViT model?\n\n- Is there any specific reason on why the cross attention as generative block provides relatively poor performance compared to conventional. projector in Tab. 7?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1669/Reviewer_HQ1H"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1669/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739553482,
        "cdate": 1698739553482,
        "tmdate": 1699636094769,
        "mdate": 1699636094769,
        "license": "CC BY 4.0",
        "version": 2
    }
]