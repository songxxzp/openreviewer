[
    {
        "id": "2XLXzP1sUa",
        "forum": "kwn9ySjbc1",
        "replyto": "kwn9ySjbc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_youK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_youK"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the perspective of observing the world through biology, this article makes a bold attempt to apply a variable resolution mechanism of image on a VQA task and tries to make a trade-off between performance and computation cost. The variable resolution is implemented by a simple Log-Polar transformation while achieving a great surpass compared with the naive down-sampling strategy, and a comparable result compared with the full resolution strategy. This work then discovers the interpretability of the variable resolution, trying to prove that higher-resolution areas and lower-resolution areas can benefit from each other."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well written, which is reflected in its proposed variable resolution that effectively reduces the overall computational cost.\n2. The innovation point of this paper is simple, and experiments on VQA and object detection have proved its effectiveness.\n3. The visualization of the explanation that variable resolution performs better than uniform resolution is thorough. The attention map indicates that higher resolution areas and lower resolution areas can benefit from each other, and the neuronal activation maps and kernel filters explain the better performance from a model perspective."
            },
            "weaknesses": {
                "value": "1. Some of the experiments are missing. Firstly, as the title of the article is related to VQA, and models that perform well on object detection tasks may not necessarily have the same effect on VQA tasks, I hope to include more space in the experiment on different types of VQA datasets, such as OK-VQA. Secondly, this work has proved the effectiveness of variable resolution in that the central part has a higher resolution but only makes the comparison with uniform sampling, which is too naive to have a good performance. So I\u2019m also curious about what the result would be when other part of the image has a higher resolution or other sampling strategies.\n2. The selection strategy for high-resolution areas in the image needs to be improved. As shown in Figure 4, not all objects reside at the center part of the image, so this could be the cause of the poor performance in Table 2 when compared with the baseline. As this paper raises the HRA metric, it\u2019s better to define a better selection strategy corresponding to this, or just sampling by gradients following the dynamic mask in [1].\n[1] Lin K, Li L, Lin C C, et al. Swinbert: End-to-end transformers with sparse attention for video captioning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 17949-17958.\n3. The interpretability analysis of variable resolution is not convincing enough. Some visualization experiments are not as important as shown in the paper, and some visualization is lacking. \nOn the one hand, there are too many visualization examples of the impact of the three sampling methods on the label of an object in the image. As a common sense, if we feed the model with a lower resolution of the image of the object, the model has a higher probability of giving a wrong label. \nOn the other hand, as shown in Figure. 3, the variable sampling scheme performs well on the questions that can be answered from parts of the image rather than the whole context of the image. That is because we don't have enough understanding of the correlation between hyperparameters and sampling strategy of variable resolution and global background knowledge. The explanation of this correlation is lacking, but the whole context of the image is important to VQA."
            },
            "questions": {
                "value": "Overall, I find the idea of this paper to be simple but reasonable. Because of its simplicity, I find no obvious weaknesses on the technical side but there is still room for improvement in performance. The authors use a large portion of this paper to explain the reason of its effectiveness, some of them are convincable, but the lack of deeper quantitative experiments makes the explanation not sufficient enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7333/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832229860,
        "cdate": 1698832229860,
        "tmdate": 1699636877265,
        "mdate": 1699636877265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zCKOOzfoHA",
        "forum": "kwn9ySjbc1",
        "replyto": "kwn9ySjbc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_hgaY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_hgaY"
        ],
        "content": {
            "summary": {
                "value": "The authors claim that they propose a variable sampling scheme, inspired by human vision, remarkably outperforms a uniform sampling scheme by 2% accuracy (65% vs. 63%) in the challenging task of scene visual question answering (VQA), under a limited samples budget (3% of the full resolution baseline)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors claim that they propose a variable sampling scheme, inspired by human vision, remarkably outperforms a uniform sampling scheme by 2% accuracy (65% vs. 63%) in the challenging task of scene visual question answering (VQA), under a limited samples budget (3% of the full resolution baseline)."
            },
            "weaknesses": {
                "value": "1. According to the title, the author's focus is on VQA. but it seems that only one dataset (VQA v2) from VQA task was used. How does the authors' proposed method perform on other common datasets such as GQA[1], OOD-GQA[2], etc.?\n\n[1] Hudson, Drew A., and Christopher D. Manning. \"Gqa: A new dataset for real-world visual reasoning and compositional question answering.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[2] Kervadec, Corentin, et al. \"Roses are red, violets are blue... but should vqa expect them to?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n\n2. The method proposed by the authors seems to address object detection and is not a VQA task.\n\n3. Does Sec.5 relate to the VQA task? Why?\n\n4. Is the technical or scientific challenge addressed throughout the article associated with VQA? Why?"
            },
            "questions": {
                "value": "Please refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7333/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699070820867,
        "cdate": 1699070820867,
        "tmdate": 1699636877129,
        "mdate": 1699636877129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WR09xDnu83",
        "forum": "kwn9ySjbc1",
        "replyto": "kwn9ySjbc1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_V3rN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7333/Reviewer_V3rN"
        ],
        "content": {
            "summary": {
                "value": "The authors re-evaluate the use an design of a variable resolution visual sensor for neural networks, and try to approach this problem from the angle of representational gains rather than computational efficiency (what is classically accepted in the literature). Authors show several experiments ranging from detection, VQA + interpretability to show the advantage when 2 systems are placed under equal perceptual sensing conditions to perform inference (uniform vs variable) and thus finding that the neural network with a variable resolution sensor in many cases out-performs the equal resolution one (that is usually more blurred)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper tackled the question of the use of a foveated (spatially-adaptive) visual system through the lens of object detection, interpretability + Visual Question Answering (VQA). I will give it to the authors, as I don't think this has ever been done before, which is why I am marginally inclined to accept this paper. Most methods of testing for the representational goal of foveation is through object classification or detection, and even more recently for texture-based discrimination. Authors talk a bit about this too, in addition to present interpretabilty experiments similar to Deza & Konkle. ArXiv, 2021.\n* Authors have made a good case for showing representational gains of a foveated visual system"
            },
            "weaknesses": {
                "value": "There is a long list of critical missing papers that should be cited if this paper is to be accepted. I can not increase my score to accept unless these papers are cited & discussed (and of course, if the other reviewers also think that this paper should be accepted).\n\nKey Missing Critical References:\n- Deza & Konkle. ArXiv, 2021. **Emergent Properties of Foveated Perceptual Systems.**\n- Wang & Cottrell. Journal of Vision, 2017. **Central and peripheral vision for scene recognition: A neurocomputational modeling exploration.**\n\nSecondary, but also important References:\n- Cheung, Weiss & Olshausen. ICLR 2017. Emergence of foveal image sampling from learning to attend in visual scenes\n- Gant, Banburski & Deza. SVRHM, 2022. Evaluating the adversarial robustness of a foveated texture transform module in a CNN.\n- Reddy, Banburski, Pant & Poggio. NeurIPS 2020. Biologically inspired mechanisms for adversarial robustness\n- Wang, Mayo, Deza, Barbu & Conwell. SVRHM, 2021. On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation\n- Harrington & Deza. ICLR, 2022. Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks\n- Malkin, Deza & Poggio. SVRHM 2020. CUDA-Optimized real-time rendering of a Foveated Visual System.\n\n----"
            },
            "questions": {
                "value": "* While I think the VQA evaluation framework is original, what I do not understand is \"why VQA?\", why not something less complex such as object recognition or detection where language modelling will not interfere in the output produced by the system. I can only think of an answer if there is an argument somehow linking foveation with language but that does not seem to be the case. I can see an object detection evaluation which I think is nice, but that has already been shown in Pramod et al. 2021.\n\n* Comparison of this work with Deza & Konkle is necessary. They addressed many questions presented in this paper such as training on different types of foveal-peripheral transforms, the use of foveation as a texture-based distortion that mimics crowding vs a more rudimentary baselines such as adaptive gaussian blurring. Furthermore it would have been interesting if the Authors would have compared their results in ImageNet vs Places. Does the same pattern of results hold? Does Foveation do better by virtue of a central image bias where the object is usually put in the center of the image? Presumable more controlled experiments are necessary to try to answer these questions.\n\n* I am not sure what the red dots represent in many of the figures such as Figure 5 and S5.\n\nAll in all, I think this paper is exciting, but discussing and adding the missing references is necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7333/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699308770119,
        "cdate": 1699308770119,
        "tmdate": 1699636876985,
        "mdate": 1699636876985,
        "license": "CC BY 4.0",
        "version": 2
    }
]