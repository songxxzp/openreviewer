[
    {
        "id": "tInWw2WVor",
        "forum": "S77skzM12O",
        "replyto": "S77skzM12O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission70/Reviewer_xy7L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new framework Policy Regularized Offline-To-Online (PROTO) for offline pre-trained to the online fine-tuning RL problem. The proposed PROTO framework can be implemented efficiently, and it also demonstrates improvement upon several existing offline to online methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper has conducted a throughout literature review.\n2. The limitations of current existing offline-to-online methods are well introduced and explained (Section 3.2).  \n3. The proposed regularization method is clean and elegant (Equation 3).\n4. The intuition of the proposed algorithm is well explained and somewhat theoretically justified (Section 3.3).\n5. Most experiments in Section 4 demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The experiments in Figure 4 are hard to visualize, perhaps the author can change another color to demonstrate the curves (especially the `BC+PROTO+SAC`, `EQL+SAC`, `PEX+BC`)."
            },
            "questions": {
                "value": "Would it be ok if the author also add the Cal-QL experiments for comparison? As far as the reviewer is aware of, the Cal-QL paper also has open-sourced the [code](https://github.com/nakamotoo/Cal-QL)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271871250,
        "cdate": 1698271871250,
        "tmdate": 1699635931425,
        "mdate": 1699635931425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GA7MxtbQzH",
        "forum": "S77skzM12O",
        "replyto": "S77skzM12O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel algorithm, PROTO, for offline-to-online RL. After the offline pre-training phase, PROTO introduces the KL entropy regularization term in policy and value function objectives between the current policy and that of the previous iteration, which is iteratively updated, and the coefficient is also linearly decayed to loosen the constraint. PROTO achieves strong performance on adroit manipulation, antmaze navigation, mujoco locomotion from D4RL dataset and exhibits lighter computational cost than prior works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### quality and clarity\n- This paper is clearly described and easy to follow.\n\n### significance\n- PROTO achieves the notable performance across the benchmark tasks.\n- PROTO reduces the computational cost from the existing offline-to-online RL methods such as ODT, Off2On, and PEX, etc."
            },
            "weaknesses": {
                "value": "- I think this is the naive adaptation of TD3-BC [1] in offline-to-online settings, because KL regularization term results in BC objective in online settings.\n- Similar to above, I'm not sure the difference between PROTO and PROTO-TD3 or PROTO-SAC presented in Figure 4/5. I think PROTO and TD3(-BC) is quite similar to each other. Because  PROTO modifies the objective of policy and value function, it is not intuitive to incorporate with SAC. \n- Theorem 1 seems the same as what Vieillard et al. (2020) [2] have proven, and equation 7 is the same as what Scherrer et al. (2015) [3] have proven. I don't think there is novel and offline-to-online-specific discussion around the theorem. Please let me know if my understanding is not correct.\n- In the similar setting, [4] proposes iterative trust-region update from the offline pre-trained policy.\n\n[1] https://arxiv.org/abs/2106.06860\n\n[2] https://arxiv.org/abs/2003.14089\n\n[3] https://jmlr.org/papers/v16/scherrer15a.html\n\n[4] https://arxiv.org/abs/2006.03647\n\n---\n-- Update from the author response --\n\nFor the theoretical analysis, I completely disagree with the explanation from the authors. First, the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) only holds under the exact case, where MDP has finite state space and a finite set of actions, and the q-function can be updated from the Bellman equation (this does not mean the update with gradient descent). Because this paper doesn't have any assumptions on those (i.e. PROTO is only proposed with continuous state/action space and function approximation), originally, the discussion does not make any sense. Second, because the theorem from Vieillard et al., (2020) and Scherrer et al., (2015) originally holds under any initialization (initialization of matrix) and thus offline-to-online RL has already been included in online RL, this paper does not have any novel theorem or statement. insightful interpretations for PROTO do not exist. The same statements from Vieillard et al., (2020) and Scherrer et al., (2015) are repeated in the main text. I express strong concerns that the paper has the same theorem as the previous papers without deriving a novel theorem. Moreover, if the important discussion is only discussed in Appendix, that is very weird."
            },
            "questions": {
                "value": "- Could you clarify the computational cost between SAC and PROTO? Figure 9 may say that PROTO is better than SAC, but I guess there are no algorithmic differences between the two. \n- How does PROTO conduct offline pre-training? PROTO also employs iterative policy constraint? or other algorithms (CQL, IQL, etc.)? I guess this algorithm results in TD3-BC [1] in offline settings.\n- What is the effect of delayed policy updates (Section 3.4)? Is this necessary?\n\n[1] https://arxiv.org/abs/2106.06860"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission70/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission70/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission70/Reviewer_LmwG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607223117,
        "cdate": 1698607223117,
        "tmdate": 1700326242495,
        "mdate": 1700326242495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JJfGjShdTE",
        "forum": "S77skzM12O",
        "replyto": "S77skzM12O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission70/Reviewer_6tXk"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an iterative strategy regularization approach for offline to online reinforcement learning, called PROTO. The method is designed to address three main challenges: poor performance, limited adaptability, and low computational efficiency. By incorporating an iteratively evolved regularization term, the algorithm aims to stabilize the initial online fine-tuning and provide sufficient flexibility for policy learning. PROTO seamlessly bridges various offline reinforcement learning and standard off-policy reinforcement learning, offering a flexible and efficient solution for offline to online reinforcement learning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a thorough analysis of the three issues present in existing offline to online finetuning methods and proposes a straightforward and versatile solution that effectively overcomes these problems. \n2. Furthermore, the article includes rigorous theoretical analysis and extensive experimental validation simultaneously, resulting in a high level of completeness in the paper."
            },
            "weaknesses": {
                "value": "1. I have some concerns regarding Polyak averaging. Is the initial policy $\\bar{\\pi}_0$ obtained by the offline pretraining? If so, does Polyak averaging potentially lead to PROTO deviating very slowly from the offline pretrained policy since the parameter is a very small value ($\\tau=5e-5,5e-3$)? Additionally, it seems that the role of Polyak averaging might **overlap** with the addition of the KL term in Equation 3. Therefore, I believe that ablation experiments for this parameter are not sufficient. An experiment without this parameter should be conducted. Also, does the update process of EQL+SAC make use of this parameter $\\tau$? If not, could this parameter potentially help prevent training collapse in EQL+SAC?\n2. Based on the analysis and evidence presented in the paper, I believe that PROTO's capabilities extend beyond improving methods that are already capable of offline to online finetuning. Can PROTO also cooperate with other algorithms that cannot solve the offline to online problem?"
            },
            "questions": {
                "value": "please answer the questions in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836493529,
        "cdate": 1698836493529,
        "tmdate": 1699635931287,
        "mdate": 1699635931287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OuKYG0oHUw",
        "forum": "S77skzM12O",
        "replyto": "S77skzM12O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission70/Reviewer_Ykto"
        ],
        "content": {
            "summary": {
                "value": "This work propose to conduct KL-regularized policy gradient iterative in the offline-to-online setting. By iteratively update the old policy with the more recent policy and constrain KL distance between current policy and old policy, it achieves a good trade off for stability and optimality. \n\nThe authors conducted experiments on D4RL tasks and shows improved performance compared to AWAC, IQL etcs. Additionally, the ablation results show that iteration of policy is essential to the improvement. Also the larger the distance between final policy and initial policy, the better the final performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed iterative approach is simple, effective, and novel, as far as I know.\n- The ablation results matched with the motivation.\n- The experimental results is promising."
            },
            "weaknesses": {
                "value": "- Are we missing EQL comparison? The EQL should also work with offline to online setting. If we are using EQL as a pre-trained approach, how does the proposed approach perform compared to EQL in the offline to online setting?\n- In this setting, the KL-regularized MDP is a moving target, this means the optimal policy per iteration is changing. This seems conflicts with the motivation of the approach. Could the author clarify the intuition here? I assume the \\pi* is the optimal policy of original MDP (without KL-constraint)."
            },
            "questions": {
                "value": "- It would be better to clarify the updating frequency of \\pi_k controlled by \\tau. How sensitive this parameter is when we switching to different tasks and settings? Especially when we have different level of \\pi_0. Is there a clear guidance how we should choose \\tau? \n- How many iterations of policies in total in practice? How many gradient steps we did one delayed updates? \n- I think it is good to see that in Figure 7, there is a clear shift of distribution in terms of horizon. It might be more clear to see if there is state-action distribution shift. I am trying to understand if this iterative approach can reach OOD state-action pairs gradually, compared to the \\pi_0. I think this is important because: 1) with this verification, we can then understand if the learning can be stable and gradually move to an area deviated from initial state-action coverage. 2) We are not just re-weighting the existing trajectories in the offline dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission70/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699595283349,
        "cdate": 1699595283349,
        "tmdate": 1699635931197,
        "mdate": 1699635931197,
        "license": "CC BY 4.0",
        "version": 2
    }
]