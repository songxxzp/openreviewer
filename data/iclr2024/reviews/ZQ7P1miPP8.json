[
    {
        "id": "l5whqE3rlr",
        "forum": "ZQ7P1miPP8",
        "replyto": "ZQ7P1miPP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_ikyJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_ikyJ"
        ],
        "content": {
            "summary": {
                "value": "The paper examines adversarial defense through model explanations. The method suggests using counterfactual examples to train a denoising autoencoder. This autoencoder reconstructs benign samples for training target models. The target model is said to be resistant to black-box attacks and correctly classifies adversarial examples. Experiments on four structured datasets demonstrate that the proposed method achieves similar results to other baseline models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is well-written and easily understandable."
            },
            "weaknesses": {
                "value": "1. The proposed method has limited application. It has only been validated on a binary structured dataset. Additionally, based on the description, the defense setting appears relatively easy since the defender has full access to the target model while the attackers do not.\n2. The experimental results are unconvincing. Table 1 shows that nearly 40% of the tested accuracies are lower than the baselines. The transferability of the proposed method can also be examined. For instance, can the DAE trained on one dataset be applied to other datasets? It would also be valuable to present those results.\n3. [1] has demonstrated that the BPDA can break the denoising operation on input samples. However, the paper lacks any discussion about the effects of BPDA on the proposed method. The experiments should include similar results evaluated on those attacks.\n4. The paper solely proposes a defense framework without conducting a rigorous study on the reasons behind the proposed method. \n5. The claim of \"a total of 400,000 adversarial examples for the 40 attacks\" on Page 2 has not been verified. For instance, the certified defenses can correctly classify all adversarial examples within the attack budgets regardless of the attack types.\n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Reviewer_ikyJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7865/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677533354,
        "cdate": 1698677533354,
        "tmdate": 1699636964335,
        "mdate": 1699636964335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iTZ8M16BQd",
        "forum": "ZQ7P1miPP8",
        "replyto": "ZQ7P1miPP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_8JLu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_8JLu"
        ],
        "content": {
            "summary": {
                "value": "The primary focus of this paper lies in leveraging Counterfactual Examples (CFEs) to enhance the robustness of machine learning models against adversarial attacks in the context of tabular data. The authors propose to use CFEs since they are independent of the large variety of attack types. To defend the models against adversarial attacks, the authors propose the incorporation of a Denoising Autoencoder (DAE) trained using benign data as well as CFEs. This DAE is deployed during testing, where it serves as a firewall by denoising adversarial examples, aiming to bring them back to the benign class, therefore making them less harmful to the model's performance. The model protected by the DAE can either be trained on i) benign examples, ii) a mix of CFEs and clean examples, iii) denoised benign examples, iv) denoised benign and CFE samples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Originality: The paper introduces a novel approach to enhancing the robustness against adversarial attacks using Denoising Autoencoders (DAEs) and counterfactual examples (CFE). The integration of DAEs and CFEs represents a fresh perspective, demonstrating originality in the paper's contribution.\n\n- Clarity: The paper is entirely clear, and it presents the approach in a well-explained manner that allows to easily replicate it even without access to the original code\n\n- Quality: The paper is qualitative regarding the rigorous proposed approach, evaluation on relevant datasets for tabular data and comparison with a range of baselines. \n\n- Significance: The paper adds value to the important field of adversarial attack mitigation in machine learning. By demonstrating that CFEs can enhance the robustness of models against adversarial attacks, the paper contributes to the development of more secure and reliable machine learning systems. Considering the high degree of explainability for CFEs, the approach is important for having more transparent and easily understandable robustness mechanisms."
            },
            "weaknesses": {
                "value": "The primary weakness of this paper lies in the absence of a guarantee and concrete evidence to support the claim that CFEs (Counterfactual Explanations) robustification, despite its attack-agnostic nature, can effectively defend against a wide range of attacks, in contrast to single attack robustification.\n\n- Firstly, the paper acknowledges that adversarial defenses typically struggle to generalize across different attacks. However, the experimental evaluation across four datasets and two defense mechanisms reveals only one instance where one of the defenses did not exhibit strong generalization.\n\n- Secondly, when the authors compare attack-based DAE (Denoising Autoencoder) defenses with CFE-based defenses, the comparison settings are not entirely equivalent. In three out of the four CFE-based defenses (scenarios A2, A3, and A4), denoised examples are employed during the training phase, a practice not used in adversarial-based defenses. This raises the question of what the results would be if adversarial examples were used in scenarios A2, A3, and A4 instead of CFEs. The only scenarios that would enable a somewhat equivalent comparison involve the use of denoised examples at test time such as BDAE, HDAE, ZDAE, versus A1. In such case, CFE-based defenses often do not outperform the ones using adversarial examples.\n\n- Lastly, the techniques for generating CFE examples vary, and in some instances, they resemble to the attacks they are designed to defend against (i.e genetic algorithm approach). This prompts a discussion on whether certain CFE-based defenses may perform better against specific types of attacks.\n\nA second notable weakness is the paper's failure to account for adaptive attacks. The core element of the defense proposed in this paper is the DAE, and if attackers are aware of this, they could fine-tune their examples to induce high loss in the DAE. This is a well-explored scenario for DAE defenses in computer vision."
            },
            "questions": {
                "value": "Did you try using adversarial examples instead of CFEs for any of the scenarios A1-A4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Reviewer_8JLu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7865/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751450356,
        "cdate": 1698751450356,
        "tmdate": 1699636964193,
        "mdate": 1699636964193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xYIpvnpOjx",
        "forum": "ZQ7P1miPP8",
        "replyto": "ZQ7P1miPP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_GewD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_GewD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a preprocessing defense based on noise removal against adversarial attacks. The method leverages counterfactual explanations (CFE) to enhance the robustness of classical machine learning models on tabular data. An auxiliary denoising autoencoder (DAE) is employed, which uses benign and CFE data to denoise inputs. Four defense strategies using this DAE are proposed: the first one does not alter the target model, while the other three refine the target model using benign and CFE data for improved accuracy and robustness. Experiments are performed on four tabular datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "# Novelty\n\n- I am not aware of any prior work using CFEs to defend against adversarial examples. It is an interesting idea to connect explainability to adversarial defenses."
            },
            "weaknesses": {
                "value": "# Prior work\n\n- The novelty of the paper resides in using CFEs as data for training the denosing autoencoder. Beyond that, the method does the same as most other denoising and preprocessing defenses in literature. However, the representation of prior work in the paper is done based on only two references. I suggest including more work from this field where publications in the same direction as current work are abundant.\n\n# Soundness\n\n- Assumptions: no threat model is presented for the paper. It becomes clear after reading the paper that the defense is designed under black-box assumptions for the adversary. However, this is only presented implicitly in the Experiments section.\n- Assumptions: it seems DiCE ML, one of the main components of the proposed method, is only capable of binary classification. Moreover, it needs access to both the trained model and the training set. These limitations should be mentioned as part of the proposed method.\n- More generally, the limitations of the proposed method are not addressed in the paper, except maybe some of them being mentioned for the first time in the conclusion. It is common practice to not introduce new information in the conclusion. Moreover, limitations deserve a dedicated discussion in the paper.\n\n# Significance\n\n- The scope of applicability of the proposed method is quite limited: black-box attacks on tabular data for classical ML models, with only one type of model being evaluated (random forest classifier). Moreover, the paper states that the method cannot be used with XGBoost, one of the most popular and performant models on tabular data.\n- Black-box attacks are usually not as strong as white-box ones, as the attacker does not have access to model internals. Proposing a method that is only capable of defending against weak attackers does not seem particularly impactful.\n- Even limited to the black-box setting, one would expect to see more recent and impactful attacks included in the experiments (e.g., [boundary attack](https://arxiv.org/abs/1712.04248), [square attack](https://arxiv.org/pdf/1912.00049.pdf)).\n- The experiments oppose the proposed method to versions of DAE for adversarial defense. As such, this looks more like an ablation study than a comparison to other defenses. When the contribution of a paper is a new defense, it makes sense to compare it to state-of-the-art defenses. \n\n\n# Clarity\n\n- The explanation of the datasets used to train the DAE (Sec. 4.2) is not very clear. Perhaps writing the mathematical notation with the pairs of (input, output) that represent the training samples would help.\n- The training of the target model with CFEs also needs to be justified and explained. What exactly would be the training procedure?\n- Additional proofreading seems necessary. See some examples below.\n\n# Minor points\n\n- The abstract does not seem to be up to date on how many datasets are used in the experiments: three instead of four.\n- samples,which -> samples, which\n- (re)Trained -> (re)trained.\n- Counterfactual explanations (CFES) -> Counterfactual explanations (CFEs). Sometimes used CFES or CFEs. Please use consistently.\n- CFES and adversarial example are both -> CFES and adversarial examples are both\n- two definitional differences -> two defining differences\n- (i)Misclassification -> (i) Misclassification\n- \"AI firewall,located\" -> \"AI firewall, located\""
            },
            "questions": {
                "value": "1. What classical machine learning models can use the proposed defense?\n2. Please see comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7865/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768300915,
        "cdate": 1698768300915,
        "tmdate": 1699636964085,
        "mdate": 1699636964085,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h68ygmMtzj",
        "forum": "ZQ7P1miPP8",
        "replyto": "ZQ7P1miPP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_9mE5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7865/Reviewer_9mE5"
        ],
        "content": {
            "summary": {
                "value": "This papaer introduces a new method to improve model robustness by utilizing the explainability of generated CFE data. After constructing the CFE data, a denoising autoencoder was applied for building reconstruction-based four different defending pipeline. The overall pipeline is flexible and easy to implement. This paper also conduct experiments on four small datasets, and evaluate their proposed defending method against several black-box attacks (HSJ, ZOO, HV). and its proposed method could achieve marginal robustness improvement compared to AE baselines. No further ablation studies on different defending pipelines was made and no white-box robustness evaluation was conducted."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Motivation is interesting: It would be interesting to see how these CFE data (which are generated close to decision boundary) could help further improve the model robustness. \n- The whole pipeline is computational efficient compared to other adversarial training or smoothness training pipelines."
            },
            "weaknesses": {
                "value": "Most of my concerns are placed in question part:\n\n- The overall novelty is quite limited: detection with CFE-data trained AE looks novel but there is no strong point on explaining why CFE-data trained AE could perform better than other denoising AE which are trained on either augmented data or adversarial examples (e.g., MagNet or Defense GAN). It makes me feel like the whole paper is just applying AE on the new set of samples (which are close to the decision boundary for sure) but without too much underlying reason or comparison to other AE baselines. \n- Experiment part is not solid: No sufficient baseline included in main result table. Also the datasets are too small - we cannot be convinced by the robustness analysis on toy datasets with 14 or 20 or 30 feature dimensions. More natural image datasets results need to be added.\n- Bad writing on methodology part: hard to follow how each approach is actually applied."
            },
            "questions": {
                "value": "Major concerns:\n- Methodology part: Section 4.3 looks super unclear: four different pipelines are described in a very vague way and I feel it is quite hard to follow: what is the \"AI firewall\" and how it can be well configured (e.g. detection threshold). Though I spent a lot of time understanding what's going on here, I still cannot figure out the differences between approach 1 and 2 - In Section 4.2 you mentioned that both \"From dataset\" and \"To dataset\" contain benign samples right? Then I was wondering what is the exact difference between approach 1 and 2?\n- Evaluation part: The chosen dataset are too small and not convincing enough. I'm curious about why most of the datasets this paper used are with very few attributes? Back to MagNet, they performed their evaluation on both MNIST and CIFAR datasets - can we have the comparable results on MNIST/CIFAR10 and include the MagNet results as baseline?\n- Evaluation part: Considering one of the mentioned related work, MagNet, has been proven to be vulnerable under grey-box threat model setting [1], it would be good to show if the proposed CFE-data motivated method could achieve better robust accuracy under the transferable attack scenario. \n- Experimental details: I cannot find any DAE training / CFE-data generation details (model arch, epochs, lr, decay etc.), and these information are very important.\n\n\n[1] Carlini, Nicholas, and David Wagner. \"Magnet and\" efficient defenses against adversarial attacks\" are not robust to adversarial examples.\" arXiv preprint arXiv:1711.08478 (2017)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7865/Reviewer_9mE5"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7865/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699490662602,
        "cdate": 1699490662602,
        "tmdate": 1699636963970,
        "mdate": 1699636963970,
        "license": "CC BY 4.0",
        "version": 2
    }
]