[
    {
        "id": "nVQR0EVWMp",
        "forum": "prLvQWc5YH",
        "replyto": "prLvQWc5YH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors identify an issue with underestimation bias when employing successor features with generalized policy improvement (GPI). They apply distributional RL to successor features to obtain a distributional GPI that they claim alleviates this issue."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Connecting successor features and multi-objective RL. These two concepts are intimately related, yet I\u2019m unaware of any published work making a direct connection. \n* Identifying the issue of underestimation with SFs. The intuition behind this makes sense but I found the explanation hard to follow."
            },
            "weaknesses": {
                "value": "Overall, I found the paper hard to follow as the math is a little too loose to make sense of what the authors are claiming. I'd suggest the authors go over the text with a careful eye and fix some of the notational issues. I\u2019ll detail the major weaknesses and ask more direct questions about notation and other issues in the questions section below.\n\n* I believe they are learning the wrong object with their proposed distributional SF algorithm. They should be learning the features' joint distribution, but they treat each feature as independent. This is implicitly done in how the author's sample $\\tau$; to learn the proper distribution, you'd need the quantile of a multivariate random variable.\n* The paper is poorly positioned in the literature regarding multi-dimensional reward functions in distributional RL. My above point on learning the wrong object has been solved by [1] and [2], where they learn the correct joint distribution. Furthermore, I expected a more in-depth discussion about [3] as they also learn \u201cdistributional SFs\u201d (although they also aren\u2019t learning the joint distribution).\n* Assumption 1 seems dubious; this should be impossible with stochastic transitions; a single application of the Bellman operator will construct a mixture distribution, so, at the very least, you\u2019d expect the target to be a mixture of Gaussians. \n* No justification is given for the additive noise model (Equation 6).\n* It\u2019s hard to judge the effectiveness of the approach as the empirical results don\u2019t differentiate much between distributional GPI and regular GPI. I would have had to see a better-executed empirical study to be convinced.\n\n---\n\n[1] Pushi Zhang, Xiaoyu Chen, Li Zhao, Wei Xiong, Tao Qin, Tie-Yan Liu. Distributional Reinforcement Learning for Multi-Dimensional Reward Functions. NeurIPS 2021.\n\n[2]  Dror Freirich, Tzahi Shimkin, Ron Meir, Aviv Tamar. Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN. ICML 2019.\n\n[3] Michael Gimelfarb, Andre Barreto, Scott Sander, and Chi-Guhn Lee. Risk-Aware Transfer in Reinforcement Learning using Successor Features. NeurIPS 2021."
            },
            "questions": {
                "value": "- In Section 3.1, the prediction $\\Psi(s', a', \\theta_i)$ should be $\\Psi(s, a, \\theta_i)$? Only the TD target should have the next state-action term. This error is propagated from this point forward, e.g., the gradient term is wrong, and eq (5) contains the same error.\n- In Section 3.1, why is there an expectation over s\u2019 in the loss? Aren\u2019t we trying to write down the stochastic approximation algorithm for learning SFs via TD? Citing equation 1 makes it seem like that\u2019s what we\u2019re trying to do.\n- Theorem 1, why is $s\u2019$, $a\u2019$ defined as input to $\\Delta$ but then $s'$ appears in the expectation? Also, why do we have an expectation over $s'$ again?\n- Theorem 3 compares the expected return of the optimal policy with a risk measure of the estimated policy. Why? If you're being risk-sensitive, the goal is not to learn the mean-optimal policy.\n- Algorithm 1, where is $\\tau_e$ being used? Shouldn't it be used in Line 6 when computing the greedy action?\n- Algorithm 1, quantiles are treated implicitly in some cases; this makes it hard to decipher what's going on."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788520347,
        "cdate": 1698788520347,
        "tmdate": 1699635932510,
        "mdate": 1699635932510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "09CNPiA8np",
        "forum": "prLvQWc5YH",
        "replyto": "prLvQWc5YH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors aim to address underestimation when using Successor Features and Generalized Policy Iteration (GPI). A common technique often used to prevent overestimation in the Q-values is by using the min operation with double Q-functions, which may in turn result in underestimation. Motivated by this insight,  the authors rely on theoretical analyses to show a similar trait is observed when updating the parameters of the successor features. This is induced by a mismatch of the parameters of the successor features between one that depends on a changing policy distribution and the other being the optimal policy. The authors proposed replacing successor features with its distributional form in order to limit or reduce the underestimation bias."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I have to be honest. It is difficult for me to identify what to write with regards to the strength of the paper. I can see that a lot of work has been done. However, the presentation and writing is not doing justice to the authors\u2019 effort."
            },
            "weaknesses": {
                "value": "1. First of all, the writing and the overall presentation is not very clear and does not flow well at times. Despite reading the paper a couple of times, it still seems confusing and overly complicated. Lastly, some of the sentences in the paper do not even make sense and makes me wonder if they were generated by LLMs. Here are some examples: \n  a. \u201cExplosively, we take an impressive TRL method - successor features (SFs) (Barreto et al., 2017; 2018; Carvalho et al., 2023) as an example, to study the underlying overestimation/underestimation bias.\u201d \n  b.\u201cThey enrich our concepts mutually.\u201d\n  c.\u201cExtensive quantitative evaluations support our analysis.\u201d\n2. It seems that the research question about addressing underestimation was motivated by RL/ But at the moment, I fail to understand the need of using risk-sensitive frameworks and multi-objective RL. What is the main motivation for considering these frameworks and theories? Furthermore, the lack of clarity from the section on bridging successor features and multi-objective RL does not help the cause.\n3. Eq 4. Is y the target that you are regressing towards? It is confusing if that is not the case. \n4. It is very hard to read the paper when there are a large portion of different concepts and their corresponding theorems and equations. I would recommend moving most of these items into the appendix and use the main portion of the paper to explain what these different concepts are and how they are related to the research question that you are attempting to address. You can also move the pseudocode for Algorithm 1 into the appendix. This will also allow you to make more space for the section for conclusion and discussion. \n5. The overall paper structure should be re-visited. The fact that a whole chunk of related work is in your appendix is a missed opportunity for the readers that they can follow along. \n6. Although the author did provide the theoretical proofs showing the existence of the underestimation bias in the SF & GPI framework, this point will make a stronger case with empirical evidence as well."
            },
            "questions": {
                "value": "1. What is the purpose of analyzing using the risk-sensitive framework? \n2. What is the purpose of considering multi-objective RL which only further complicates the study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699138323260,
        "cdate": 1699138323260,
        "tmdate": 1699635932425,
        "mdate": 1699635932425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aBc5uzK6RB",
        "forum": "prLvQWc5YH",
        "replyto": "prLvQWc5YH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically studies the underestimation phenomenon in successor features and generalized policy improvement. The paper introduces distributional RL into the SF/GPI framework so as to mitigate underestimation and theoretically analyzes its generalization bounds. The experiments are run on multi-objective RL environments (testing transferabiity with GPI) in Mujoco. They compare the performance of their distributional variants to the standard SF/GPI variants."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper does indeed seem to show theoretically that the underestimation phenomena occurs in SF & GPI.\n\nThe paper does indeed seem to show that distributional SFs has a lower generalization bound than the original SFs."
            },
            "weaknesses": {
                "value": "My primary concerns have to do with clarity, as well as well as the experimental results.\n\nThere are many typos or awkward wording, including some that impact the reader's understanding.\n\nSome examples include:\n- \"The results indicate that both the two DSFs-based algorithms (RDSFOLS and DGPI-WCPI)\". The latter isn't even in Figure 2? I am assuming the latter refers to \"WCDPI+DGPI\", but this should be clarified.\nAwkward wording:\n- \"resulting in a \u201czero-shot\u201d somewhat fantastical\"\n- \"For a new task w_{n+1}, it is practicable to evaluate all policies\". Perhaps you mean practical?\n- \"standpoint to expose the mystery of underestimation\". The word 'mystery' seems akward here.\n- \"Due to the disorder of exploration\": I don't know what \"disorder of explanation refers to\"\n- \"is lack of stability\"\n- \"DSFs exhibits\" -> \"exhibit\"\n- \"We remark that \u03b4_\u03c6 > 0 makes no focus\". I didn't understand what was meant by \"focus\".\n- \"if the set of DSFs enough close\"\n\nThere are many more beyond what was mentioned, and this does indeed negatively impact the readability of the paper.\n\nThe results in Figure 2 do not appear to be very compelling. It seems the proposed method does not significantly outperform the baselines."
            },
            "questions": {
                "value": "- Can you describe again the y-axis in Figure 2?\n- Did you look at Q-value predictions of the agents to show/demonstrate lower underestimation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699156724966,
        "cdate": 1699156724966,
        "tmdate": 1699635932360,
        "mdate": 1699635932360,
        "license": "CC BY 4.0",
        "version": 2
    }
]