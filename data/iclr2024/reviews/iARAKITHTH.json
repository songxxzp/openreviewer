[
    {
        "id": "jZ7XPzPN3L",
        "forum": "iARAKITHTH",
        "replyto": "iARAKITHTH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_CMmZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_CMmZ"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a method to detect text generate by an LLM. The method can be applied to text generated by any LLM without fine-tuning and it is based on the perplexity of the text according to a an observer LLM, but normalized by the perplexity of the text generated by another LLM using the same input string, The method is evaluated on several public datasets and compared to some other machine-generated detectors."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is simple and the rationale behind the proposed normalized perplexity is well motivated and seems to be appropiate. It does not require any specific training and can be used with text generated by any LLM and also with any LLM to compute the perplexity."
            },
            "weaknesses": {
                "value": "I have several concerns regarging the experimental validation and the presentatation of the results:\n- In section 4.2 it is argued that TPR@FPR is a better metric than F1 score or AOU, and particularly TPR at 0.01% of FPR. However TPR at 0.01 FPR is never used in the results presented afterwards and mainly F1, AUC, precision and recall are used in most of the figures. Only in figures 4 and 6 TPR vs FPR is plot, but without specifically analyzing TPR at 0.01% FPR as claimed in section 4.2. Related to this, in the abstract it is claimed that \"On news documents Binoculars detect 95% of synthetic samples at a false positive rate of 0.01%\". In the results only figure 4 for the student essay dataset seems to show a result similar to this one, but it is very specific to only of the datasets used in the experiments.\n- Evaluation should be done in a more systematic and coherent way among different datasets to be able to better compare the performance of the proposed method with SoA. As long as it is possible the same methods should be used for comparison. However, the set of methods used to compare with in figures 2, 4, 5 and 6 are different, and even methods used in figure 6 are not mentioned or discussed in the text.\n- The selection of the methods used to compare with is not clearly motivated. In section 2 many methods are described and categorized in different categories. I would be nice to have a systematic comparison with more methods of each of the categories defined in section 2. \n- I miss an ablation study analyzing some of the choices made in the design of the method. For instance, the contribution of using \tthe proposed detection score vs. simple perplexity or the effect of the LLMs used both to compute perplexity and to generate the text to be classified. Is there any difference in performance (positive or negative) if the observer LLM is the same LLM used to generate the text under analysis?\n- It is not clear what figure 3 is showing. Also, in the caption of figure 5 it is said that the comparison is on LLaMa text while in the text referring to the figure it is mentioned text generated by LLaMa, but also by Falcon. Not clear exactly whaat figure 5 is showing. \n- There is no comparison of the results obtained in the Orca dataset with results obtained by other methods. \n- The description of M4 datasets in section 4.3 would fit better in section 4.1 with the description of the rest of the datasets. \n\nAnd finally, a minor comment with respect equation (3): shouldn't it be log(M_2(T(s))_i) instead of log(M_2(T(s))_j)?"
            },
            "questions": {
                "value": "See above in weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789832885,
        "cdate": 1698789832885,
        "tmdate": 1699637025866,
        "mdate": 1699637025866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5xBFwbPRMJ",
        "forum": "iARAKITHTH",
        "replyto": "iARAKITHTH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_M521"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_M521"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a simple method to detect machine-generated text, based on measures of perplexity of two independent LMs.\nThe score is derived from the assumption that a texts generated by two LMs are more similar with each other than with human text.\nA comprehensive evaluation is carried out to show that the proposed method has a high detection rate at a low false positive rate. \nThis remains true for various models used for generating texts. The proposed method gives competitive or superior results compared \nto several other methods. Finally, the authors discuss the potential limitations of their method when used in practice"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written, easy to follow and to understand. \nThe proposed method looks very simple and quite easy to implement. It is well evaluated and seems to give good results, which is a nice combination.\nIt extends perplexity-based method in a simple and well motivated way.\nThe paper makes a number of good/interesting points (e.g. the motivation to measure the True Positive Rate at low FPRs, the remark on memorization/randomization in Section 5, the one on LLM similarity).\nMost questions that arise when reading the paper are actually answered in the appendix (e.g. how about using different LLM for the score computation, how does the text length affect the detection).\nThe discussion of limitations in sections 5 and 6 is particularly appreciated: it is still too rarely seen in papers. They naturally raise questions about the potential weaknesses of the approach but the transparency is valuable."
            },
            "weaknesses": {
                "value": "The paper is quite nice in terms of contents, presentation and evaluation of the proposed method for the 10-page limit. \n\nFurther analysis of some aspects could add value to the presented work, although I acknowledge that not everything can fit in the paper (some of which will be asked in the \"Questions\" section:\n\n  - In section 2, it could be clearer how the proposed method relates to other PPL-based ones\n  - In Table 2, we see that the performance can vary a lot depending on the chosen LLM for scoring: what makes a good LLM for the method? Why other choices reach lower TPR?\n  - The point about similarity of LLMs is interesting. What would happen if the LM (evaluated or chosen for scoring) is less similar, e.g. different training set, different kind of model, etc.\n\nMinor remarks:\n\n  - in 5.1, mention that Table 4 is in the appendix\n  - 4.3 \"Fig. 3\" -> Figure 3"
            },
            "questions": {
                "value": "Most of the questions I had while reading the manuscript were answered in the appendix.\n\nSome other aspects that I would be curious about following the read of this paper:\n\n  - The point made at the end of 5.1 is interesting but it is not so clear what should be concluded from that remark. It also begs the question of how would the training set of the LLMs (mostly written by humans I guess) be detected\n  - The point made in 5.4 (randomization) is interesting as well. It would be interesting to see if the detector could be fooled by randomly changing \"some\" words in the generated text. I guess the remark made in 5.1 about the desirability of outcome is relevant here too (= how to consider human-edited machine-generated text)\n  - The point of 5.2 is interesting and we could also wonder how robust this detection method is. For example, is it easy to tweak existing LLMs to fool the detector (other than prompt), or during training, how easy would it be to integrate the detection score to fool the detector."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848935855,
        "cdate": 1698848935855,
        "tmdate": 1699637025764,
        "mdate": 1699637025764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0sejDPGKxw",
        "forum": "iARAKITHTH",
        "replyto": "iARAKITHTH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_iAnY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_iAnY"
        ],
        "content": {
            "summary": {
                "value": "To detect text generated by large language models, the authors propose in this work a method called Binoculars. Different from previous methods for separating human-generated and machine-generated text, Binoculars utilize two models instead of one, to compute two scores: perplexity and cross-perplexity. The ratio of perplexity to cross-perplexity is defined as the Binocular score. The proposed does not need training examples and works in the zero-shot setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of using two large language models to compute a score to distinguish between human-generated and machine-generated text is novel and seems to be effective to certain extent.\n2. The paper is overall well-written, and the core idea and technical details are clearly presented."
            },
            "weaknesses": {
                "value": "1. The experiments are insufficient in that: (1) The experiments and comparisons in Figure 5 are crucial, but the authors only compared with Ghostbuster and the analyses are too brief; (2) According to the ablation study in A.1 in the APPENDIX, the authors actually only conducted experiments using the models from the Falcon and Llama-2 families. Why other types of open-source or closed-source large language models (such as ChatGPT, GPT-4 and Baichuan 2) are not adopted?\n2. The reason behind the effectiveness of the proposed Binoculars method is not fully explained."
            },
            "questions": {
                "value": "The authors should resolve the questions in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698896128574,
        "cdate": 1698896128574,
        "tmdate": 1699637025662,
        "mdate": 1699637025662,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "puoIDllz01",
        "forum": "iARAKITHTH",
        "replyto": "iARAKITHTH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_FmjG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8248/Reviewer_FmjG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel machine-generated text detector that is based on a simple metric that can be obtained from existing pre-trained LLMs. The proposed metric (Binoculars score) is computed as the ratio between the perplexity and cross-perplexity of a given sample text for two pre-trained LLMs. Using the Binoculars score they build a simple threshold-based classifier to separate machine-generated and human text."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed Binoculars score for machine-generated text detection can be computed from pre-trained LLM models without any re-training/finetuning.\n+ The obtained results are promising on a variety of machine-generated text detection scenarios, including text generated with different generators, and in a variety of domains."
            },
            "weaknesses": {
                "value": "The authors claim their method is a zero-shot detector, but they \"optimize and fix the detection threshold globally using these datasets\". The fact that the used pre-trained LLMs are not retrained/fine-tuned does not imply the proposed method is zero-shot, as they optimize the detection threshold for the target task with (in most of the cases) some in-domain data.\n\nI have several concerns regarding the experimental section of this paper.\n\n- Several of the baseline models are not explained in the text: Zero-shot (in Figure 1) and Roberta, LR GLTR, Stylistic, Nela (in figure 6).\n- The comparison with baselines is not consistent across experiments, in some Figures/Tables the proposed method is compared with a set of baselines while in others the baselines are different. The choice of baselines seems quite trivial and makes it difficult to assess the contribution of proposed method:\n- In Figure 5, Ghostbuster (Verma et al., 2023) is not trained for this dataset, while the detection threshold of the roposed method seems to be optimized on it. Why no other baselines are shown in this plot?\n- The results on the M4 Datasets (Figures 3 and 6) are not well explained in my opinion and seem to be not consistent with the results in (Wang et al., 2023). In (Wang et al., 2023) the authors present results on different settings: same-generator cross-domain experiments, same-domain cross-generator experiments. In here it is not specified whether the numbers shown in Figure 6 come from one or the other setting, and the values provided for the baselines' results do not match (at least for what I can appreciate) with the ones found in the tables of (Wang et al., 2023).\n- The numbers in Figure 3 are not compared with any baselines. This comparison would be highly relevant due to the authors claim on being able to detect text generated with any text-generator.\n- For the Orca Dataset no baselines' results are provided. Same for all the experiments in section 5. Not having any baseline results on these experiments makes it very difficult to quantify the quality of the proposed solution.\n\nWhen showing FPR/TPR plots, how are these plots generated? by changing the detection threshold? I do not understand why you say 0.01% FPR threshold is \"The smallest threshold we can comprehensively evaluate with our limited compute resources.\"\n\n\nApart from all this, it seems to me that the formulation/notation of the Binoculars score is confusing in some aspects.\n\n- $L$ (in eq. 2 and 3) is not defined anywhere in the text, I assume it is the sentence-length.\n- The subindex $j$ in eq. 3 is not defined. (Maybe it is a typo?)\n- In eq.3 it would be better to use $\\overrightarrow{x}$ (as defined before) instead of $T(s)$. It would make the equation more readable.\n- If I'm not missing something, the measure expressed in eq. 3 is the $log XPPL$ not $XPPL$. At least it looks consistent with the $log PPL$ definition in eq.2.\n- Although the authors first define $log PPL$ in eq. 2, they use $PPL$ instead in the formulation of the Binocular score in eq. 4.\n\nMoreover, since cross-perplexity ($XPPL$) is not a standard term in NLP or machine learning, I would expect to see a bit more of discussion on its interpretation and its effects on the proposed Binoculars score: what are the upper/lower bounds of B? what happens when M_1 and M_2 are the same pre-trained model? what if they are trained in two totally different domains? I appreciate the effort made in section 3.2 (Capybara problem) for the case of \"hand-crafted prompts\", but other aspects of the Binoculars score should be discussed as well."
            },
            "questions": {
                "value": "Please clarify those aspects mentioned in the weaknesses section of my review that do not imply new experiments: zero-shot claim, choice of baselines, Binoculars score formulation and interpretation, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698924147182,
        "cdate": 1698924147182,
        "tmdate": 1699637025532,
        "mdate": 1699637025532,
        "license": "CC BY 4.0",
        "version": 2
    }
]