[
    {
        "id": "gEgOqNhbtA",
        "forum": "4JbrdrHxYy",
        "replyto": "4JbrdrHxYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed the annotation-free instance segmentation method using vision foundation models (i.e., SAM and CLIP). They found that SAM shows high recall rates but relatively low precision. Also, DINO is suitable to obtain salient regions but is not suitable to obtain instance-level masks. To address the challenges, they discovered that clustering the features of CLIP's specific middle layer can be effectively used for adequate prompts for SAM. The class of each instance mask is classified using the CLIP. Consequently, the proposed method outperforms the previous state-of-the-art annotation-free instance segmentation methods. In addition, it shows a competitive performance to existing open vocabulary object detection methods without any annotations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "## 1. Great motivation and findings.\n\nI strongly agree with the motivation of this work.\nNamely, SAM itself may not be suitable for instance segmentation on COCO due to the high recall and low precision rates. \nAlso, DINO may be suitable for salient object detection, but not for instance segmentation.\nMotivated by the limitations of vision foundation models, this paper designed a new pipeline for annotation-free instance segmentation using fine-grained features from CLIP and SAM.\n\nIn addition, the proposed classification-first-then-discovery pipeline is convincing in resolving CLIP's misclassifying issue.\n\n## 2. Well-structured and well-written paper\n\nI enjoyed reading this paper because the motivation is clear and understandable, the proposed pipeline is well-explained with proper illustrations.\n\n## 3. Outstanding performance\n\nCombined with CLIP and SAM, the proposed method outperforms the existing methods by a large margin on COCO dataset.\nEach proposed component is well-ablated."
            },
            "weaknesses": {
                "value": "## 1. Limited technical novelty\n\nI feel that leveraging the features on particular middle layers of CLIP and applying K-Means clustering on CLIP's features are not technically novel but well-engineered.\n\nOf course, the proposed annotation-free instance segmentation pipeline is interesting.\nThis paper seems to be an engineering paper that addresses how to effectively use CLIP and SAM and is not far from an academic paper.\n\n## 2. Behind the outstanding performance.\n\nI think the key feature of the proposed method is how to properly cluster objects in an image (because we regard that SAM can segment anything when bounding box prompts are properly introduced).\nAt first, I was surprised by the outstanding performance of the proposed method on COCO 2017 dataset.\nHowever, there is a question that the outstanding performance was due to the well-aligned semantic domain between the features of CLIP and the COCO dataset; the clustered object boundary from CLIP may be well-aligned to the ground-truth object region in COCO dataset.\nI wonder if the target dataset was human part segmentation or medical segmentation, the CLIP features are still valid for annotation-free instance segmentation.\nIt would be great if this paper discuss it."
            },
            "questions": {
                "value": "Due to some concerns in the Weakness part, my initial rating is borderline.\nAfter a discussion with the authors and other reviewers, I will finalize the rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper includes ethical considerations and potential negative social impacts, and there don't seem to be any issues to worry about."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_CLaV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649323450,
        "cdate": 1698649323450,
        "tmdate": 1699636123036,
        "mdate": 1699636123036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6mZaPecx5I",
        "forum": "4JbrdrHxYy",
        "replyto": "4JbrdrHxYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_5mwu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_5mwu"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method called ZIP, which\ncombines CLip and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free,\n complex-scene-capable, open-vocabulary object detection and instance segmentation.\n\nZip establishes state-of-the-art performance in various settings, including training-free, self-training, \nand label-efficient finetuning.\n\nThe overall idea is based on a discovery that CLIP can provide a highly\nbeneficial and strong instance-level boundary prior in the clustering results of its\nparticular intermediate layer.\n\nThe proposed method combines the CLIP classification score with the SAM localization score using a geometric mean. \nThis refinement boosts the confidence scores of masks, achieving a slightly higher precision."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Good ZSL model proposed- combining CLIP and SAM, and giving semantics meaning to edges.\n\nLarge amount of experimental results, exhibiting superiority with SoA, ablation studies etc\nprovided for benchmark datasets.\nIllustrations of results are vivid and eye-catching -= say in Fig. 10."
            },
            "weaknesses": {
                "value": "The statement used - The devil is in the object boundaries;\nis it not in line with a part work, which hypothesized - \"all you need are priors\"?\n\nAlthough the  analytics presented in Eqns. (1) - (7) are clear, unable to find much of novelty in them.\neg Cosine similarity, boundary score etc.\n\nWhat about inference ? How many GPUs are required, and what is the timing then ?\nI could not locate that information in document.\n\nA generic opinion:\nThe recent trend of using a large GPU cluster to implement detectors/segmentors is something of a concern, I believe.\nIts like exploiting massive computing power to solve a problem. Either use of a large set of activation/attention heads, \\\nor for pre-training tasks in self-supervision, meta-learning are the main reasons of using so. What is the limit now ? \nThe focus is shifting from devising novel learning algorithms to use of larger to mega-clusters."
            },
            "questions": {
                "value": "Page 2 - the line:\n....higher precision yet not satisfaction enough.\nmay be corrected (English).\n\nYou state - \"classification-first-then-discovery pipeline\"; in page 3:\nWhat if the reverse is done ? Edges in images, as boundaries of objects, may hypothetically lie\non Decision boundaries between binary classes. So discover the edges first may be a better approach, rather than\nrelying on classification to provide those edges ?\n\nDino (also Clip?) uses transformers (ViT) , right ?\n- is that the reason for the need for large computational resource to solve your problem,\nwhen you combine the two?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678387869,
        "cdate": 1698678387869,
        "tmdate": 1699636122963,
        "mdate": 1699636122963,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cMaye1qZlF",
        "forum": "4JbrdrHxYy",
        "replyto": "4JbrdrHxYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD"
        ],
        "content": {
            "summary": {
                "value": "The authors analyze the weakness of the previous computer vision foundation models. By discovering the instance-level understanding of CLIP, the authors then utlize it to propose a novel training-free method to address the issues by clustering the activation maps from CLIP. Zip demonstrates significant performance improvements compared to the previous state-of-the-art on the COCO dataset across various settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The idea is novel, inspiring and useful.\n2. A detailed analysis of the issues in previous works is provided.\n3. Significant improvements over previous state-of-the-art on various COCO settings.\n4. The authors are aware and thoroughly investigate the weaknesses and limitations of the work."
            },
            "weaknesses": {
                "value": "1. It is challenging to follow the explanation of how the clustering works. It would be helpful if the authors could include some figures to illustrate this process more clearly.\n2. The robustness of the clustering algorithm is concerning when applying to other datasets or settings."
            },
            "questions": {
                "value": "- Q1. The work is super interesting, but it takes me a long time to understand how the clustering works.\n   - What do the colors represented in \"Clustering\" in Fig.3?\n   - How does the clustering algorithm draw the boundaries in Fig.2 and Fig.3?\n   - Taking \"Semantic Clues\" in Fig. 3 as example, what will it look like after applying Eq. 5?  I assume that there will be a single box surrounded all the green parts? How can instances be seperated after that?\n   - The authors stated that after obtaining the feature maps for each category, it will then run the clustering algorithm. Please clarify how to run the algorithm and why it will work.\n- Q2. How stable is the clustering algorithm? Will the results change drastically if optimal hyperparameters are not found? For example, which intermediate layer should be used? Is there any experiment on how the choice of $K$ will affect the results? Are there any guidelines? Please also provide some insights for Eq.7.\n- Q3. Is it appropriate to fully adopt the setting from CutLER? To my understanding, CutLER does not involve any annotation during training. While Zip also does not use annotation from COCO, but it did exploit the language information from CLIP. \n- Q4. Is it possible to compare Zip to other open-vocabulary works, such as Openseed[1]? I am aware that Zip is annotation-free, and the authors have stated that it will be a future work to utilize annotations at Section J. However, the work would be more convincing and useful if it is possible to somehow utilize annotations from COCO since the raw performance of Zip is still significantly behind Mask-RCNN.\n\nI belive it is an interesting and promising work. However, the explanation of clustering is not clear enough. I am willing to raise my score if the authors can reasonably address my concerns and questions.\n\n[1] Zhang, Hao, et al. \"A simple framework for open-vocabulary segmentation and detection.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_WVZD",
                    "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695403932,
        "cdate": 1698695403932,
        "tmdate": 1700623194875,
        "mdate": 1700623194875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cxAeQOrRHl",
        "forum": "4JbrdrHxYy",
        "replyto": "4JbrdrHxYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach to annotation-free instance segmentation. The authors identify that a specific middle-level feature in CLIP encapsulates instance-level boundary information. Leveraging this insight, they develop a pipeline named \"Zip\" for annotation-free instance segmentation. The paper also pioneers a \"classification-first-then-discovery\" paradigm. Through extensive experiments, the proposed method significantly outperforms its baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The utilization of CLIP's middle-level feature to generate instance proposals presents a compelling and novel approach. This stands in stark contrast to traditional RPN-based or embedding-based proposal generation methods, offering a fresh perspective in the realm of annotation-free instance segmentation. It also provide comprehensive experiment results to prove its leading performance."
            },
            "weaknesses": {
                "value": "1. **Figure 1's Layout**: The sequencing in Figure 1 is somewhat perplexing. A more intuitive order, such as DINO, Prev. SOTA, SAM, Clustering, and then Ours, might enhance clarity. Additionally, specifying the exact method when referencing \"Prev. SOTA\" would provide better context to the readers.\n\n2. **Figure 2's Clarity**: The visualization in Figure 2 appears cluttered, making it challenging to discern the boundaries in images labeled A through D. Simplifying or enhancing the contrast could make the distinctions more evident.\n\n3. **Understanding Figure 3**: Figure 3 is somewhat intricate, especially when trying to comprehend the roles of \"activation\" and \"boundary.\" A more detailed caption or a supplementary explanation might aid in understanding.\n\n4. **Discussion on DINO and SAM**: While it's essential to provide context, the extensive space dedicated to discussing the limitations of DINO and SAM might be excessive. Streamlining this section could make the paper more concise and focused on the primary contributions."
            },
            "questions": {
                "value": "1. **Semantic-Aware Initialization**: Could you provide a more in-depth explanation of the Semantic-Aware Initialization mentioned on page 17? Specifically, I'm unclear about how the parameters for K-means clustering are determined.\n\n2. **Visualization in Figure 2**: The differentiation between gray and orange in Figure 2 could be clearer. Enhancing the contrast by making the background color more transparent might help in better distinguishing the two.\n\n3. **Ambiguous Object Representation**: How does the middle-level clustering handle images where the concept of an object is ambiguous? For instance, in scenarios like a box with a person's image on one side or stacked blocks. In such cases, it's debatable whether the person's image should be considered a separate instance or if each block in the stack should be individually segmented.\n\n4. **Handling Occlusions**: How does your method address situations where an object is partially occluded and appears as two separate parts? For example, if a dog is behind a tree, would your system recognize both parts as belonging to the same object?\n\n5. **Locating Middle-Level Features**: I'm intrigued about the process of pinpointing the middle-level feature. Is the method you've employed reproducible across different training weights? For instance, would the results be consistent if you were to use the openCLIP model (or the official CLIP model, if you've already utilized openCLIP)?\n\n6. **Exploring DINO/DINOv2 Features**: Have you considered using the middle-level features from DINO or DINOv2? Given that MaskCut can discern between closely packed objects using DINO features, I wonder if DINO/DINOv2 might also possess a middle-level feature adept at boundary extraction.\n\n7. **Resolution of Middle-Level Feature**: The resolution of your middle-level feature appears to be quite high. Could you specify the image input size and the dimensions of your middle-level feature?\n\n8. **Self-Training Visualization**: It might be beneficial to include a comprehensive visual representation of the self-training process. Relying solely on textual differences compared to CutLER and directing readers to refer to CutLER might not be the most user-friendly approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_g6s6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730923281,
        "cdate": 1698730923281,
        "tmdate": 1699636122738,
        "mdate": 1699636122738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5zgN5bvK2B",
        "forum": "4JbrdrHxYy",
        "replyto": "4JbrdrHxYy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
        ],
        "content": {
            "summary": {
                "value": "Authors propose an annotation free instance segmentation through combining CLIP and SAM, where the authors claim that CLIP has a better capability of obtaining the boundaries. They evaluate their method on COCO and PASCAL VOC datasets. Their framework named Zip outperforms some of the unsupervised annotation free methods from SOA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- interesting results in Table 1 and Table 2 improving over the compared methods\n- Interesting direction to work on establishing methods for annotation free instance segmentation"
            },
            "weaknesses": {
                "value": "- Weak novelty as the method is simply utilizing other foundation models without proposing anything specific to the instance segmentation task. The clustering techniques are not really showing anything novel in its formulation. Novelty is more in the pipeline.\n\n- There are no quantitative results to support their claim that DINO is worse than CLIP in identifying the boundaries as far as I have seen but if there are please clarify in the response. Also is CLIP better than Up-DetR or better than DINO, DINOv2 in identifying boundaries. Meaning if they apply same clustering and everything on these models' features how would it fair against CLIP.\n\n- Up-DetR is not clearly stated in the related work and I am wondering if the authors have inspected its use\nDai, Zhigang, et al. \"Up-detr: Unsupervised pre-training for object detection with transformers.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n- Equation 3 and the method is not quite clear for example when referring to performing inner product of matrices. But there is no mention of the matrices shapes which can help clarify ambiguities in the mathematical formulation.\n\n- The method is quite heuristic depending on hyper parameters used in Eq. 4 theta_1, theta_2.\n\n- Their results are quite low in Table 1 compared to for example SAM when coupled with ViTDet as reported in original SAM paper. It has to be highlighted on why the previous results weren\u2019t compared to. For example if it was not fully annotation free in SAM paper, please detail that in the paper then.\n\n- Typos needs to be fixed e.g. \u201cclassification-frist-then-discovery \u201c"
            },
            "questions": {
                "value": "- Table 3 analysis on the architecture its not clear how the AP50 climbed to 44.9%, I am not sure how did this happen and is this still annotation free? Why it is not the final results compared in Table 1.\n\n- F.g 6 C.1 its not clear what\u2019s the x-axis?\n\n- How did you retrieve CutLER results in Table 1? Why is there one class aware and class agnostic?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1922/Reviewer_11pM"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1922/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766936503,
        "cdate": 1698766936503,
        "tmdate": 1699636122670,
        "mdate": 1699636122670,
        "license": "CC BY 4.0",
        "version": 2
    }
]