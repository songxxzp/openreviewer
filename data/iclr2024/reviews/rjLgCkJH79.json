[
    {
        "id": "l7sN3lQiA6",
        "forum": "rjLgCkJH79",
        "replyto": "rjLgCkJH79",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_NNdZ"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to tackle a challenging problem in drug discovery, where it is common to optimize a lead compound to remove deficiencies and maintaining the favorable properties. \nThey highlight the challenges of using reinforcement learning based on similarity metrics to define certain constrains on the optimized compound, which potentially can introduce a bias in the generative process. \nTherefore, the authors propose a so call similarity agonistic reinforcement learning approach and remove the dependency on the similarity metric as additional constrain for optimization. This is achieved by goal-conditioned reinforcement learning."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In my opinion the paper has the following strengths:\n\n-\tTo the best of my knowledge the idea of using complete molecules as goal (for goal conditioned reinforcement learning), as the authors propose it, is novel.\n-\tIn general, the method section is well explained with minor exceptions.\n-\tUsing reaction rules partially circumvents a general problem in generative models for drug discovery, namely a significant part of generated molecules are difficult to synthesize in the lab hindering a fast-pace early stage drug discovery program. The use of reaction rules conditions the generative model to generate more chemically plausible molecules with a direct synthesize path. \n-\tThe method seems to improve upon their baseline on all experiments."
            },
            "weaknesses": {
                "value": "Lead optimization in drug discovery is an important and difficult task. I have difficulty accepting the method as an invention or improvement for lead optimization. For my understanding lead optimization is a much more complicated process than purely looking on QED score or a similarity score, which the authors didn\u2019t investigated. \n\nIn general, the paper would gain strength if the authors would compare their method against more recent methods in generative design and more properties other than QED. Especially, the baseline seems to be quite weak with all the efforts recently put into improving generative methods. \nFor example, the author could have a look at a standardized benchmark, e.g. [1]. \nThis would strength their method and would help to better showcase the potential\nimprovement compared to other methods. \nThe authors might also consider comparing their methods against other methods in the domain of scaffold hopping, e.g. [3].\n\nThe second contribution of their paper as stated on page 2, says:\n\u201cwe propose a search strategy\u2026\u201d\nCould the authors elaborate more on the search strategy? In case it is just generating thousands of molecules and sorting them based on a score, this seems to me not like a novel strategy.\n\nI very much like the idea of using reaction rules, although not completely novel, e.g. [2]. I think a more detailed description how exactly they mine the reaction rules and a better description of the reaction dataset in general would help the reader to better understand the topic. It doesn\u2019t have to be in the main text. \n\nI had trouble understanding the last paragraph of section 4.6., \u201cwe found that under the condition G(a_t leads to g)\u2026\u201d. Could the authors elaborate a little bit more on the issue they observed?\n\nTo summarize, although certain ideas are interesting and in some sense novel, I am hesitant to accept the paper mainly because of in my opinion a weak experiment section. The paper doesn\u2019t showcase a technique for lead optimization, which is much more complicated than what is investigated in the paper. Also, claims like: \n\u201cThough we do not explore comparison with multi-property optimization works in the scope of this work, the results shown induce confidence in our model to be able to generate lead candidates that satisfy multiple properties.\u201d Sec. 6, \nseem to be too strong for the experiments considered. \n\n\n[1] Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample Efficiency Matters: A\nBenchmark for Practical Molecular Optimization, October 2022.\n\n[2] Tianfan Fu, Wenhao Gao, Connor W. Coley, Jimeng Sun, Reinforced Genetic Algorithm for Structure-based Drug Design, 2022. \n\n[3] Krzysztof Maziarz et al. LEARNING TO EXTEND MOLECULAR SCAFFOLDS WITH STRUCTURAL MOTIFS, 2022."
            },
            "questions": {
                "value": "-\tDid I understand it correctly that the offline dataset just contains molecules randomly put together using the reaction rules, so potentially not chemically plausible at all? \n-\tMy understanding of actor-critic reinforcement learning is to use the output of the critic for the loss of the actor. From eq. (1) and (2) this seems not the be the case, could the authors elaborate a little bit?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698392709253,
        "cdate": 1698392709253,
        "tmdate": 1699637015320,
        "mdate": 1699637015320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gqylBC5dci",
        "forum": "rjLgCkJH79",
        "replyto": "rjLgCkJH79",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
        ],
        "content": {
            "summary": {
                "value": "This work presents LOGRL, a unique approach to lead optimization using a goal-conditioned reinforcement learning framework. Given an expert dataset, this work trains a goal-conditioned policy with binary reward shaping, treating reaction rules as actions. Then, LOGRL compares Tanimoto similarity and QED of generated molecules with two baselines using an online RL method, which is PPO."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is well-written and presents clearly.\n- The paper demonstrates comprehensive related work."
            },
            "weaknesses": {
                "value": "The experimental comparison in this paper raises some concerns regarding fairness and appropriateness. The authors compare their proposed off-line Reinforcement Learning (RL) policy with on-line RL baselines. This comparison between on-line and off-line RL algorithms seems somewhat unconventional. Moreover, it's unclear whether the on-line RL baselines, such as the S model and Q+S model, employ an expert dataset similar to LOGRL. If they do not utilize expert data, this could introduce an unfair advantage to LOGRL, as it relies on additional expert data. It would be beneficial to see how LOGRL performs when compared to baselines that also use the same expert dataset.\n\nAdditionally, I suggest exploring the possibility of supervised learning in this context. The authors assume access to a substantial amount of expert dataset containing high-reward samples. In such a scenario, imitation learning often outperforms offline RL. It would be valuable to understand why the authors chose offline RL over supervised learning, given the abundance of expert data.\n\nThe paper employs policy gradient, which typically assumes that the training policy and the behavior policy are aligned, making it an on-policy approach. The suitability of using a policy gradient in an offline RL setup is a point of concern. It would be helpful to see more discussion and justification regarding the use of an on-policy algorithm like policy gradient in this context.\n\nFinally, it would be interesting to know if the proposed method is capable of generating diverse outputs. One potential concern is whether the method might collapse and generate a single output, as there doesn't appear to be a regularizer that can control all possible outputs directed toward the target molecule. Exploring the diversity of outputs and addressing this potential issue would strengthen the paper.\nOverall, while the paper presents a promising approach, addressing these concerns and providing more clarity would enhance the quality of the work and its relevance in the field of machine learning and RL.\n\n---\n\nminor\n\nTypo in Section 4.5 line3: in the training batch, batchwe"
            },
            "questions": {
                "value": "See Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8186/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8186/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8186/Reviewer_PmbC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731135586,
        "cdate": 1698731135586,
        "tmdate": 1699637015188,
        "mdate": 1699637015188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JgKHlKCR15",
        "forum": "rjLgCkJH79",
        "replyto": "rjLgCkJH79",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_TrZv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8186/Reviewer_TrZv"
        ],
        "content": {
            "summary": {
                "value": "In this study, a new lead optimization method, LOGRL, is proposed. This method uses offline reinforcement learning to train the model how to optimize molecular structures to get closer to the target structures (goal-conditioned reinforcement learning). Furthermore, a set of reactions is used to ensure the synthetic accessibility of the generated structures. The beam search algorithm is used, which helps in obtaining a diverse set of modified structures that meet desired properties. LOGRL is compared against two RL baselines and achieves promising results in optimizing molecules towards the target structures, both in terms of similarity and drug-likeness defined by QED."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method is presented in a very clear way. The background section provides all the basics that are required to understand the method.\n- Offline reinforcement learning is used to avoid sparse rewards when navigating the vast chemical space.\n- The goal-conditioned reinforcement learning is used to guide the generative process, which in my opinion is the main novelty of the paper. This way, similarity measures are no longer needed to train the model.\n- Reaction rules extracted from the USPTO-MIT dataset are used to ensure the synthesizability of the generated molecules, which is important for proposing high-quality results."
            },
            "weaknesses": {
                "value": "- The significance of the work is not clear. The method is trained to optimize molecules towards the target structures, but I am unsure if I understand how this model could be used in practice. Usually, the goal of lead optimization is to improve a set of molecular properties without impacting binding affinity. In the presented setup, the optimization changes the structure of lead candidates to make them more similar to known drugs, which oftentimes is undesired because only novel structures can remain outside the patented chemical space.\n- The experimental section seems very preliminary. Only two RL baselines were trained, and there is no comparison with other state-of-the-art methods in molecular optimization. The evaluation metrics used in the experiments are very simple and do not show if the proposed method can optimize any molecular properties or at least retain high binding affinity. The Authors claim that their search strategy separates property optimization from training, but the results of the optimization are not presented. Additionally, all methods were run only once (if I understand correctly), and the results can be hugely impacted by random initialization, especially for online RL methods like the baselines. I would strongly suggest running these methods multiple times and providing confidence intervals for the evaluation metrics.\n- (minor) I think the Authors could consider comparing their approach to the simpler, yet conceptually similar, Molpher model [1]. In Molpher, a trajectory between two molecules is found by an extensive search (not RL-based) of possible reaction-based structure mutations. The motivation of that paper is also different, Molpher was proposed for effective chemical space exploration.\n\n[1] Hoksza, David, et al. \"Molpher: a software framework for systematic chemical space exploration.\" Journal of cheminformatics 6.1 (2014): 1-13."
            },
            "questions": {
                "value": "1. What is the success rate of molecular optimization using LOGRL? Can you find many well-optimized molecules in the post-training filtering step, or do you think additional RL objectives could improve these properties significantly?\n2. What are the real-life applications of this optimization algorithm? Can it be used for other optimization problems besides lead optimization (see the problems I mentioned in the \"Weaknesses\" section)?\n3. In Section 3.2, two GCRL methods are described. Did you try the other method and if so, could you provide the comparison results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8186/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842484808,
        "cdate": 1698842484808,
        "tmdate": 1699637015067,
        "mdate": 1699637015067,
        "license": "CC BY 4.0",
        "version": 2
    }
]