[
    {
        "id": "FB4WhRZV6r",
        "forum": "qp0oVaFGm0",
        "replyto": "qp0oVaFGm0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to enhance Graph Neural Networks (GNN) using Explanatory Artificial Intelligence (XAI). Specifically, the EXPLANATION ENHANCED GRAPH LEARNING (EEGL) strategy is introduced for node classification tasks in GNNs. It utilizes frequent connected subgraph mining to identify and analyze patterns in explanatory subgraphs. And iteratively update feature matrix by annotating subgraph information."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Introducing interpretation into graph neural network architecture is interesting.\n2. The research topic is important."
            },
            "weaknesses": {
                "value": "1. My first concern is the complexity of the model. In every iteration, the approach starts by utilizing a node explainer to derive all explanation graphs. Subsequently, it calculates the maximal frequent rooted subgraphs for all labels. The algorithm then selects the top-k rooted subgraphs by processing them through the GNN and determining the F1-score. Finally, a feature annotation module updates the feature matrix X. Notably, in the experiments, each iteration on one fold takes about 1 hour and 20 minutes. I urge the authors to provide a thorough analysis of their method's complexity and compare its runtime to other baseline models.\n2. My second point of contention lies in the experimental setup. The absence of baseline models for comparison in the authors\u2019 work makes it hard  to convincingly showcase the efficacy of their proposed method.\n3. My third concern is that the datasets are all Synthetic Data. I highly recommend authors to incorporate more real-world dataset for experiments.\n4. The paper's presentation lacks clarity, particularly in section 3 where the notation introduction feels disorganized. I suggest that the authors compartmentalize definitions into distinct sections or blocks for better readability. Furthermore, the method's description relies solely on textual explanations. Incorporating a diagram or figure to illustrate the workflow of the proposed method would enhance understanding."
            },
            "questions": {
                "value": "Please refer to Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698182702476,
        "cdate": 1698182702476,
        "tmdate": 1699636901077,
        "mdate": 1699636901077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FJenHpJOVO",
        "forum": "qp0oVaFGm0",
        "replyto": "qp0oVaFGm0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_FGzN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_FGzN"
        ],
        "content": {
            "summary": {
                "value": "This paper considers using explanation to update input graphs iteratively, guiding the model to rely on only identified critical elements for node classification. Particularly, it want to discover frequent subgraph structures, and extend node attributes with indicators of subgraph isomorphism existence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It explores using GNN explanations to augment node attributes, and test its potential to improve GNN performance for node classification. It is an interesting and challenging direction\n2. It designs EEGL, containing both subgraph pattern extraction module and feature annotation module to iteratively examine explanations and augment node representations. Algorithms are provided for each components.\n3. It shows promising results in experiments."
            },
            "weaknesses": {
                "value": "1. The basic assumption seems incomplete. When GNN models can not go beyond 1-WL algorithm, why augmenting graphs using their explanations can help? Their explanations would be the same for nodes with 1-WL isomorphism.\n2. Experiments are incomplete. No comparisons are made with other graph augmentation techniques. No comparisons are made with other explanation-guided learning strategies. And its influence to different GNN architectures are not tested.\n3. Time complexity is missing. The pattern detection and node attribution modules may require a lot of time to run. Authors discussed about two techniques used, and talked about running time for one iteration. But the complexity analysis and comparisons are needed for fully understand its computation cost."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801964138,
        "cdate": 1698801964138,
        "tmdate": 1699636900900,
        "mdate": 1699636900900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F1gLSqtRaJ",
        "forum": "qp0oVaFGm0",
        "replyto": "qp0oVaFGm0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_dVAw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_dVAw"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a framework, EEGL, to enhance graph neural networks by incorporating the explanations. The effectiveness of the proposed work has been demonstrated on the synthetic datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using explanation to enhance graph neural networks is convincing."
            },
            "weaknesses": {
                "value": "1.\tThe paper demands a meticulous review by a native English speaker, to refine its clarity and presentation. While numerous areas require attention, highlighted below are some specific instances. It should be noted that this isn't an exhaustive list:   \na.\tIt will be better to provide the full term before transitioning to an abbreviation. For instance, before using 'XAI', its complete form should be mentioned.  \nb.\tIn the abstract, the statement \u201cEEGL is an iterative algorithm\u2026.. in the node neighborhoods\u201d is too long to understand. Following that, the sentence \u201cGiving an application-dependent algorithm for such an extension of the Weisfeiler-Leman (1-WL) algorithm has been posed as an open problem.\u201d appears disjointed from the context and needs rephrasing for clarity.  \nc.\tThe term 'MPNN' typically stands for \"message passing neural networks.\" Instead of \u201cMessage-passing GNN\u201d.  \nd.\tThe commonly-used abbreviation for graph neural networks is 'GNNs.' If the paper opts to use 'GNN' as the abbreviation, phrasing like \"GNN form\" in the introduction should be revised to \"The GNN forms.\u201d  \ne.\tIn Figure 1, the meaning of designations such as \u201cM1, M1\u2019, M1\u2019\u2019, M2, M2\u2019, M2\u2019\u2019\u201d should be elucidated within the caption.   \nf.\tThe current citations format does not adhere to the official template provided.   \n2.\tThe exclusive reliance on synthetic datasets without any inclusion of real-world datasets diminishes the empirical strength of the paper. While the authors have proffered reasons for this choice, the graph generation model utilized is rather simple. Incorporating a more enhanced model, such as Stochastic Block Model, might enhance the quality and representativeness of the generated graphs, allowing for a more robust validation of the proposed method.\n\nIn summary, while the paper holds potential, its current presentation hinders a comprehensive review. I'm eager to provide a more detailed review if the authors could provide an updated version."
            },
            "questions": {
                "value": "See the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699252465978,
        "cdate": 1699252465978,
        "tmdate": 1699636900773,
        "mdate": 1699636900773,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iJ1GpQVirf",
        "forum": "qp0oVaFGm0",
        "replyto": "qp0oVaFGm0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new XAI-based iterative approach called Explanation Enhanced Graph Learning (EEGL) to enhance the performance of node classification by focusing on explanation subgraph structures. The proposed method applies frequent subgraph mining to explanation graphs to find helpful patterns in each class. Then, discovered patterns are used to annotate the feature matrix for GNN. Experiments show that the proposed algorithm can improve the performance on various synthetic data that the 1-WL label cannot distinguish."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed algorithm EEGN is a novel approach that uses frequent subgraph mining to XAI of GNNs for node classification.\n\n- EEGL can improve the performance of node classification by iteratively annotating the information of explanation subgraphs obtained by frequent subgraph mining into a feature matrix.\n\n- This paper empirically shows the effect of feature initialization of GNNs by comparing randomly assigned labels and labels obtained from EEGL-GNN, which can predict labels of synthetic datasets that 1-WL cannot distinguish."
            },
            "weaknesses": {
                "value": "- It requires additional computational cost to update the feature matrix in larger graphs when finding induced subgraphs. This additional cost can be high, hence it should be empirically evaluated.\n\n- In the pattern extraction module, hyperparameters are determined by the rule of thumb. It is unclear how sensitive the frequent threshold $\\tau$ and an upper bound $N$ are in frequent subgraph mining and how much subgraph structures affect the predictive performance."
            },
            "questions": {
                "value": "- Does R0 means that the feature matrix is initialized with one? It seems that this initialization causes the fact that GCN cannot train well and predict a label as an almost random class in the M2 dataset. Is it due to ill-initialization or the nature of the 1-WL algorithm of GNN?  \n\n- As the number of iterations increases, does the annotated feature matrix X converge to that equivalent to the label encoder, and the subgraph structures obtained from GNNExpiner will be equivalent to explanation subgraphs when initialized by the feature matrix with the label encoder?\n\n- There are some typos in the paper, such as in Sec 4.2, CGN (GCN?), the subscript of R, Table 4.2 (no need), and the label in Figure 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699490825700,
        "cdate": 1699490825700,
        "tmdate": 1699636900513,
        "mdate": 1699636900513,
        "license": "CC BY 4.0",
        "version": 2
    }
]