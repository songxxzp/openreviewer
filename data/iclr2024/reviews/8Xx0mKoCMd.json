[
    {
        "id": "9yI5IjInnQ",
        "forum": "8Xx0mKoCMd",
        "replyto": "8Xx0mKoCMd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_rVcU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_rVcU"
        ],
        "content": {
            "summary": {
                "value": "Compositional visual reasoning methods, which strive to convert a complex query into a structured composition of manageable visual tasks, have demonstrated considerable potential in intricate multimodal tasks such as visual question answering and language-guided image editing. This paper introduces EXOVIP, a plug-and-play method designed to rectify errors at both the planning and execution stages. Empirical results from two representative vision-language programming methods exhibit consistent enhancements across five compositional reasoning tasks on a standard benchmark."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. EXOVIP designs a mixture of three sub-verifiers, including an image-text matching verifier, an image captioning verifier, and a visual\nquestion answering verifier, which seems to be reasonable.\n2. A reasoning trace tree based on the verification scores as well as the self-correctness score from LLMs is also built to find the best trace that has the highest score.\n3. To demonstrate the effectiveness of EXOVIP, this paper conducts experiments on two recent visual programming methods: self-defined programs, i.e., VISPROG and Python code programs, i.e., ViperGPT."
            },
            "weaknesses": {
                "value": "1. Detailed information about different prompts is necessary for a comprehensive comparison\n\n2. The improvements on the GQA dataset appear to be minor when compared to those achieved by ViperGPT"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635239839,
        "cdate": 1698635239839,
        "tmdate": 1699636564855,
        "mdate": 1699636564855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KGQDqA55ML",
        "forum": "8Xx0mKoCMd",
        "replyto": "8Xx0mKoCMd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_atdm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_atdm"
        ],
        "content": {
            "summary": {
                "value": "This paper points out two major challenges when using Visual Programming, namely 1) module error and 2) planning error. The authors propose a plug-and-plan method, EXOVIP, which can self-correct the errors on the planning side and on the module execution side. More particularly, three verification module (e..g, Image-text matching, Image captioning, and VQA verifier) are proposed to verify each step. In the experiments, they show improvement of EXOVIP over BLIP2/InstructBLIP/VISPROG on GQA, and over Qwen-v1-chat-7b/VISPROG on RefCOCO, InstructPix2Pix/VISPROG on MagicBrush, etc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The main idea is clear, and it's worth diving into this area to mitigate planning error and module error. The proposed verification score for prediction calibration is straightforward and show some improvements on several datasets.\n2. The paper are written clearly in general, though some small typos. The figures clearly present the intuition behind the proposed method."
            },
            "weaknesses": {
                "value": "1. The verifier module is not carefully developed, CLIP for image-text matching is particularly not good at give correct similarity score, especially when the task are for compositional visual reasoning in this paper. In addition BLIP version1 (as cited in Section 4.1) for image caption/VQA is also not SOTA, and no ablation study is explored here to choose the best module for verifiers.\n2. The introduced verifiers can cost the whole system more computation time, however no efficiency info is added here, it should be more fair to consider this. Besides, the sub-verifiers for calibration introduce propagation error which is not handled, and it's not clear when the verification score is more reliable than the confidence score, while in the method, the verification score directly replace it.\n3. Very concerned with the compared baselines. e.g., many recent MLLMs achieve better results on GQA than the baselines used in Table 1, e.g., LLaVA get 63 on it. In Table 3, why is the reported referring expression comprehension results only on RefCOCO and RefCOCO+, why not including RefCOCOg, which is quite a standard comparison for this task. And why only a subset is validated, 2 samples per type from the test set is actually not enough. And the compared model and proposed model are only 30 IoU, which is far below the decent results in recent MLLM, which all receive 80+ (e.g., Shikra, Kosmos2, etc)\n4. In conclusion, the main concern is the baselines compared in each Table, very confusing, a lot SOTA models are removed out e.g., OFA-large is selected for NLVR, but not included for RefCOCO (which is actually reported in the origin paper). And the proposed method is not carefully developed or verified, as not sufficient ablation for each module is conducted, especially the choice for each verifier."
            },
            "questions": {
                "value": "1. For the three proposed verifiers, the CLIP is used for image-text matching, and BLIP is used for image captioning and VQA, how these models are selected, is there any ablation experiments here? E.g., C_img is generated by BLIP, why not using SOTA caption model, at least BLIPv2? Is this a severe bottleneck for the verifier or even bring confusion when the verifier model make errors.\n2. It happens when the confidence score is more accurate than the verification score. However, seems in candidate prediction in Equation 6, is only using the averaged verification score to calibrate, while the confidence score is ignored.\n3. How is the propagation error being controlled? The verifier is proposed to mitigate the planning error and module error. How is the newly. introduced verifier module error being handled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699581708396,
        "cdate": 1699581708396,
        "tmdate": 1699636564762,
        "mdate": 1699636564762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J97UGWagvs",
        "forum": "8Xx0mKoCMd",
        "replyto": "8Xx0mKoCMd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach called \"exoskeleton\" verification modules designed for visual programming. These modules methodically validate the accuracy of vision module predictions. They are used to rectify errors in module predictions through calibration, and in planning, using tree searching techniques. This involves incorporating verification scores and the self-correction capability of large language models (LLMs). The approach is tested on five different tasks, demonstrating its efficacy. Notably, when applied to the VISPROG model in a compositional visual question answering task, the method achieved a notable 4% increase in accuracy."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces an innovative concept in the field of visual programming - the \"exoskeleton\" verification modules. This unique approach effectively tackles both module and planning errors in compositional visual reasoning.\nThe robustness of the proposed methodology is evident through its application to two distinct models, yielding a notable 4% improvement in accuracy for compositional visual question answering tasks. Furthermore, the paper conducts a thorough preliminary study, demonstrating how individual components (such as self-correctness, beam search, and verification) contribute to enhanced performance.\nThe paper excels in clearly articulating its methodology. It provides detailed explanations on the use of exoskeleton modules for verification processes and the precise methods employed in calibrating module predictions.\nThe contributions of this paper are highly significant for the field of visual compositional reasoning. It addresses critical limitations in existing methodologies and offers a substantial improvement in prediction accuracy, marking a notable advancement in the field."
            },
            "weaknesses": {
                "value": "The paper's experimental evaluation is somewhat limited, primarily focusing on VISPROG evaluation datasets. Expanding the range of datasets, especially including more varied and challenging ones, would significantly strengthen the demonstration of the method's generalizability. While the paper reports a 4% improvement on the GQA dataset, a broader dataset spectrum could provide a more comprehensive assessment of the method's effectiveness.\nThe paper lacks an in-depth analysis of its limitations and potential failure cases. For visual language grounding, natural language reasoning, and visual abstract reasoning tasks where the model fails to exceed the performance of SOTA models, no further analysis is provided apart from reporting the results. Although several failure cases are mentioned in the attachment, they are not thoroughly investigated or analyzed. A detailed examination of these cases would be invaluable for identifying areas for further refinement and development of the method.\nThe paper\u2019s approach appears to primarily involve an ensemble of multiple vision-language (VL) models for post-correction, which could be perceived more as an engineering endeavor based on the VISPROG work rather than a novel conceptual contribution. A more distinct delineation of the method's innovative aspects compared to existing VL models would be beneficial in highlighting its unique contributions to the field."
            },
            "questions": {
                "value": "Generalizability Across Datasets:\nQuestion: Could you elaborate on the choice of primarily using VISPROG evaluation datasets?\nSuggestion: Consider applying your method to a wider array of datasets, especially those that are more diverse and challenging. This could help in better demonstrating the method's generalizability and effectiveness across different scenarios.\n\nAnalysis of Limitations and Failure Cases:\nQuestion: Why was there no detailed analysis provided for the failure cases mentioned in the attachment?\nSuggestion: It would be beneficial to include a thorough investigation and analysis of these failure cases. This analysis could offer insights into the method's limitations and guide future improvements.\n\nClarification on Novelty and Conceptual Contributions:\nQuestion: How does the proposed method differentiate itself, in terms of novelty, from simply being an ensemble of multiple VL models for post-correction?\nSuggestion: A more explicit explanation of the novel aspects of your approach would be helpful. Clarifying how your method advances beyond just combining existing VL models could strengthen the paper's claims of originality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5514/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_2ndj",
                    "ICLR.cc/2024/Conference/Submission5514/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699584902451,
        "cdate": 1699584902451,
        "tmdate": 1700681991381,
        "mdate": 1700681991381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LOyfjl8q4X",
        "forum": "8Xx0mKoCMd",
        "replyto": "8Xx0mKoCMd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on resolving two key classes of errors in existing visual programming approaches:\n1. Reasoning Error: Error in common sense reasoning regarding which operation to perform \n2. Module Error: Error by individual modules (like object detection) in their outputs.\n\nThey introduce ExoViP, which adds verification steps (Image-text matching, captioning and VQA verifier) for each module to detect errors before they propagate. To solve planning errors, they maintain multiple reasoning traces and use beam search to select the best sequence of neural modules. The authors evaluate the method on five visual composition reasoning tasks (Compositional image question answering, Referring Expression Segmentation, Natural Language Visual Reasoning, Abstract Reasoning, and Language guided Image Genereation)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors propose a well-motivated idea to overcome module-based and planning-based errors in Visual Programming through the use of verification scores and trace reasoning. \n2. The method reuses existing modules for obtaining verification scores. \n3. The evaluation spans multiple common Visual Composition Reasoning problems with an analysis for each type of problem. \n4. The writing is good, and the paper is easy to follow"
            },
            "weaknesses": {
                "value": "1. The initial analysis finds the percentage of errors caused by the two types of problems (module error and planning error). The developed method's intuition is to solve these two errors with the two mentioned strategies. However, the final analysis does not include the kind of errors this method actually ends up fixing and in what ratio. It would be beneficial to show the statistics that demonstrate what proportion of errors of each type were fixed by the proposed methods compared to VISPROG [1].  \n2. The method is not evaluated with any open-source Large Language Models (LLaMA, Vicuna, etc.) Restricting evaluation only to GPT-3.5-turbo reduces the generalizability of the approach.  \n3. The performance increment across tasks over VISPROG [1] is still incremental, considering the time cost of using beam search and multiple verification modules."
            },
            "questions": {
                "value": "1. In section 4.2 - You sort the candidate neural modules according to verification scores. From what your paper says verification scores are obtained for different candidates generated by a single neural module. Are these verification scores for the neural modules themselves different or are they the verification score of the top candidate of each neural module? \n2. What is the inference latency for your approach compared to using VISPROG [1]? The use of beam search and multiple verification modules indicates a significant added overhead. \n3. Both ViperGPT [2] and VISPROG [1] implementations used GPT-3 as the model of choice. Will the advantages of your method transfer to GPT-3 and other Open LLMs as well? Or is GPT-3.5-turbo necessary in order to accomplish Self-Correction and Trace Reasoning?  \n4. Could you report an analysis about what percentage of module errors and planning errors were fixed by your method compared to VISPROG? \n\n[1] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14953\u2013 14962, June 2023\n\n[2] Sur\\'is D\\'idac, Sachit Menon and Carl Vondrick, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5514/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5514/Reviewer_Tiuv",
                    "ICLR.cc/2024/Conference/Submission5514/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5514/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699590438835,
        "cdate": 1699590438835,
        "tmdate": 1700681099012,
        "mdate": 1700681099012,
        "license": "CC BY 4.0",
        "version": 2
    }
]