[
    {
        "id": "4fS92zL51b",
        "forum": "DVA0NDUdCQ",
        "replyto": "DVA0NDUdCQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_k4U4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_k4U4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies learning from Text-Attributed Graphs (TAGs) using  large language models (LLMs). Specifically, they find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. To address these issues, they propose the LEASUNG fine-tuning algorithm that  not only effectively adapts LLMs to downstream graph learning tasks with limited\nlabeled data but also exhibits strong scalability and efficiency. Extensive experiments verify the superior performance in terms of both computation and data efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] The manuscript is well-presented. The authors clearly present the motivations, the methods and the experiments.         \n\n[+] The topic of LLMs for graph is trendy and important in graph learning community.            \n\n[+] Extensive experiments are performed to verify the effectiveness in terms of both computation and data efficiency."
            },
            "weaknesses": {
                "value": "[-] The proposed method can only work for text-attributed graphs and node classification tasks. However, some recent works have validated that LLMs can process nearly all kinds of graphs and node-level/edge-level/graph-level tasks, which makes the contribution of this work being less significant.          \n\n[-] The performance of the proposed method is not superior. For example, on Cora dataset, the proposed method equpped with LLMs are inferior to the Shallow Embedding.           \n\n[-] The experiments are somewhat limited. Specifically, the authors only evaluate the proposed method in homophilous graphs. I notice that some concurrent works [1,2] in LLMs for graphs report the results on heterophilous graphs. So, how about the performance of the LEADING algorithm perform on heterophilous graphs?            \n\n[-] The authors only use some small and out-of-date language models (BERT and DeBERTa). Why not try more powerful LLMs such as GPT-3.5 and LLaMA [3]?             \n\n[-] The codes for reproducing the results are not provided.            \n\n\n[1] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs\uff08arXiv:2307.03393v3\uff09              \n[2] GRAPHTEXT: GRAPH REASONING IN TEXT SPACE (arXiv:2310.01089v1)                \n[3] Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023"
            },
            "questions": {
                "value": "1. How about the performance of the LEADING algorithm perform on heterophilous graphs?          \n\n2. Why not try more powerful LLMs such as GPT-3.5 and LLaMA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844146641,
        "cdate": 1698844146641,
        "tmdate": 1699637105869,
        "mdate": 1699637105869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lj0tLNqVA4",
        "forum": "DVA0NDUdCQ",
        "replyto": "DVA0NDUdCQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_DmWL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_DmWL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an efficient approach for end-to-end fine-tuning of pre-trained language models (PLMs) on text-attributed graphs (TAGs). The authors argue that existing PLM approaches for TAGs suffer from computation and data efficiency issues, and they propose the LEADING algorithm which maintains computation and memory efficiency similar to graph-less fine-tuning of PLMs. LEADING can effectively transfer rich knowledge to downstream graph learning tasks even with limited labeled data in semi-supervised learning. Experimental results show that LEADING demonstrates superior computation and data efficiency in comparison with existing approaches such as GIANT and GLEM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Generalizing the contextualization power of PLMs to structure-rich text data (e.g., text-attributed graphs) is an important and meaningful task. The goal of improving computation and data efficiency in this task is well-motivated.\n\n+ The ideas of removing encoding redundancy and propagation redundancy, as well as the neighbor decoupling strategy, are intuitive and well-explained.\n\n+ The authors conduct experiments on both small and large graphs and perform comprehensive ablation studies, hyperparameter analyses, and scalability studies."
            },
            "weaknesses": {
                "value": "- This work mainly studies encoder-only PLMs such as BERT and DeBERTa. I do not see how the entire study can be easily generalized to encoder-decoder and decoder-only PLMs from the perspective of either methodologies or evaluation protocols. Therefore, I do not quite agree with the term \"Large Language Models\" in the title because encoder-only PLMs are usually much smaller than encoder-decoder and decoder-only ones. The study is still a complete one even if only encoder-only PLMs are studied, but the used term somehow overclaims what this study has done.\n\n- Statistical significance tests are missing. It is unclear whether the gaps between LEADING and the baselines are statistically significant or not. In fact, the gaps on arXiv are quite subtle in Table 1, therefore p-values should be reported.\n\n- An important baseline, GraphFormers [1], is cited but not compared.\n\n- The authors only conduct experiments in the semi-supervised node classification task. Besides, all three used datasets are from the academic domain. It is unclear whether the proposed techniques work for other tasks (e.g., link prediction) and other domains (e.g., e-commerce).\n\n[1] Graphformers: Gnn-nested transformers for representation learning on textual graph. NeurIPS'21."
            },
            "questions": {
                "value": "- Could you conduct statistical significance tests to compare LEADING with the baselines on arXiv?\n\n- Could you report the performance of GraphFormers?\n\n- Could you conduct experiments in other tasks and on graphs from non-academic domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698941559492,
        "cdate": 1698941559492,
        "tmdate": 1699637105738,
        "mdate": 1699637105738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tXpAJhoqp2",
        "forum": "DVA0NDUdCQ",
        "replyto": "DVA0NDUdCQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_sP5C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_sP5C"
        ],
        "content": {
            "summary": {
                "value": "This research paper suggests an approach to fine-tuning large language models (LLMs) on text-attributed graphs. The main ideas and contributions are as follows;\n\n- Examining the redundancy, in computing (encoding and propagation) when fine-tuning LLMs using graph networks (GNNs) in an end-to-end manner. This analysis uncovers the scalability limitations.\n- Presenting an algorithm called LEADING that reduces computation redundancy by decoupling neighbors and implementing graph modeling. This allows for end-to-end training of LLM GNN to supervised LLM fine-tuning without graphs.\n- Demonstrates that LEADING is more resourceful in transferring knowledge from LLMs to downstream graph learning tasks compared to existing methods in scenarios with labeling rates.\n- Showing that LEADING achieves scalability and efficiency in LLM fine-tuning and significantly outperforms existing iterative or self-supervised methods that combine LLMs and GNNs.\n- Providing complexity analysis and empirical evaluations to highlight the efficiency and scalability benefits of LEADING.\n\nIn summary, this paper enables end-to-end training of LLM GNN through techniques that decrease computation redundancy. This facilitates the transfer of knowledge from LLMs into graph learning tasks with both efficiency and scalability, in mind."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Importance of the Problem; The paper addresses an issue of tuning Language and Learning Models (LLMs) on graphs, which has implications and challenges, across various domains.\n\nEvaluation of Existing Approaches; The authors conduct an analysis of the limitations in current methods with a particular focus on redundancies in encoding and propagation computations.\n\nInnovative Techniques Proposed; The paper introduces techniques such as neighbor decoupling and implicit graph modeling to overcome the identified limitations. It also presents an approach for training an end to end LLM Graph Neural Network (GNN).\n\nClear Organization; The paper is well structured and easily comprehensible starting with an introduction followed by a summary of related work.\n\nClarity and Elaboration; The authors effectively communicate their ideas and techniques using visual aids while providing sufficient algorithmic details.\n\nExperimental Results; Through experiments conducted on datasets the proposed techniques demonstrate advantages, particularly in terms of prediction accuracy when labeled data is limited as well as scalability.\n\nOriginal Contributions; The analysis of computation redundancies along with the introduced techniques and the end to end training approach are acknowledged as contributions to the field.\n\nImplications; These findings hold implications, for domains that utilize text attributed graphs. They also offer guidance on combining LLMs with GNNs."
            },
            "weaknesses": {
                "value": "To better understand the proposed techniques and their impact, on data efficiency it would be beneficial to delve into the underlying mechanisms through analysis or intuition.\n\nIn order to fully grasp the importance of the techniques it is advisable to conduct ablation studies that elucidate their individual contributions.\n\nTo demonstrate the scalability of the algorithm a comprehensive evaluation on a range of graph structures and larger scale datasets would be advantageous.\n\nIn the work section it would be beneficial to provide a comprehensive context by discussing existing approaches for efficient training of Graph Neural Networks (GNNs) and implicit models.\n\nCertain parts of the explanation could benefit from in depth details or intuitive explanations especially when describing how the techniques enhance data efficiency.\n\nBy expanding the scope of the ablation study and conducting experiments valuable insights, into these proposed methods can be gained.\n\nThe conclusion section should be refined to offer a concise yet comprehensive summary of the findings and takeaways."
            },
            "questions": {
                "value": "1. The analysis findings indicate that there is redundancy, in encoding and propagation. It is not clear how neighbor decoupling and implicit modeling specifically contribute to improved knowledge transfer from LLMs. Could you provide some insight or analysis to explain the underlying mechanisms?\n\n2. The ablation study seems to have limitations. It would be beneficial to conduct ablation experiments to assess the individual contributions of neighbor decoupling and implicit modeling towards the observed improvements.\n\n3. Have you explored graphs or additional datasets beyond those that were tested? Conducting experiments on a larger scale could highlight the scalability benefits more effectively.\n\n4. It would be advantageous to expand upon the work section by discussing approaches such as sampling methods for GNN training and explicit models like Neural ODE/DEQ providing more context and motivation for the techniques employed in your research.\n\n5. Some sections of the paper lack sufficient details. For instance the explanation of how your techniques reduce redundancy can be vague at times. Providing details or intuitive explanations would enhance comprehension.\n\n6. The conclusion feels somewhat abrupt. Please summarize the takeaways and contributions clearly for readers well as discussing potential future directions beyond integration, with PEFT.\n\n7. To further strengthen the paper consider expanding the ablation study incorporating references to work conducting large scale experiments and providing additional intuition and details where necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8798/Reviewer_sP5C"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699074978018,
        "cdate": 1699074978018,
        "tmdate": 1699637105630,
        "mdate": 1699637105630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2iCBZx07nD",
        "forum": "DVA0NDUdCQ",
        "replyto": "DVA0NDUdCQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_LoZg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8798/Reviewer_LoZg"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of combining implicit GNNs and efficient LMs seems quite interesting and intuitive.\n2. The summary of the related in this direction is clear and the author identifies the pain points of the current research in the area."
            },
            "weaknesses": {
                "value": "1. The main claim of this paper is on large language models whereas the experiments are conducted on small language models like \"BERT\" and \"DeBERTa\". Given the parameter-efficient tuning, it's not that hard to perform experiments on large models such as GPT-2 and Llama-2. \n\n2. The technical novelty is limited. Caching neighborhoods has been one of the common techniques to speed up GNN-LMs. \n\n3. Evaluations are limited on node classifications only. Given the limited scope and \"old\" benchmark on node classification, I don't think the contribution of the proposed idea is significant enough."
            },
            "questions": {
                "value": "1. Can this approach used in other applications such as link prediction and graph classification?\n\n2. What's the performance on larger benchmarks like ogbn-mag/obgn-product, especially the computational cost?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699515355746,
        "cdate": 1699515355746,
        "tmdate": 1699637105505,
        "mdate": 1699637105505,
        "license": "CC BY 4.0",
        "version": 2
    }
]