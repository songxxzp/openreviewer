[
    {
        "id": "VI9nUn2Zxm",
        "forum": "lOsF9k1sxW",
        "replyto": "lOsF9k1sxW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_cxbT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_cxbT"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel backdoor purification framework called Smooth Fine-Tuning (SFT). The paper argues that backdoor models converge to sharper minima compared to benign models. To counter this, SFT leverages the Fisher Information Matrix (FIM) to guide the model towards smoother minima, effectively purifying the backdoor. The framework also includes a regularizer to maintain the model's performance on clean data. An efficient variant, Fast SFT, is introduced to reduce computational overhead. The method is extensively evaluated across multiple tasks, datasets, and architectures, showing state-of-the-art performance in backdoor defense benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces a novel perspective on backdoor attacks by focusing on the optimization landscape, specifically the smoothness of the loss surface.\n2. The usage of the Fisher Information Matrix is reasonably motivated to guide the model towards smoother minima, thereby purifying the backdoor.\n3. The paper provides theoretical justification that studies the smoothness of backdoor model loss and takes the Lipschitz continuity of the loss gradient into consideration, adding to its credibility."
            },
            "weaknesses": {
                "value": "1. It lacks a comprehensive comparative analysis with existing backdoor defense methods, particularly those that employ different strategies for backdoor purification. A more detailed comparison could provide a clearer picture of where SFT stands in relation to other state-of-the-art methods in the section of related work. \n\n2. The paper introduces Smooth Fine-Tuning (SFT) and its efficient variant, Fast SFT, as methods for backdoor purification. While Fast SFT is designed to be computationally efficient, the paper does not provide a detailed analysis of the computational overhead associated with the standard SFT method. Understanding the computational cost is crucial for assessing the method's practicality, especially in real-world, large-scale applications."
            },
            "questions": {
                "value": "1. The method is widely applied to different vision tasks, how does the method apply to the language tasks? \n2. The scalability question aims to assess how well the SFT method performs as the size of the model and the dataset increases, with the usage of the Fisher Information Matrix. Are there any computational or memory bottlenecks that could limit its applicability to larger, more complex models or datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697938920170,
        "cdate": 1697938920170,
        "tmdate": 1699636387501,
        "mdate": 1699636387501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YMPrBsgoxU",
        "forum": "lOsF9k1sxW",
        "replyto": "lOsF9k1sxW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_Xtwr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_Xtwr"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates backdoor attacks during the training process of deep neural network (DNN) and proposes a Smooth Fine-Tuning (SFT) framework to eliminate backdoors by leveraging knowledge from the Fisher Information Matrix (FIM). The research demonstrates that backdoor models tend to converge towards sharp local minima, while benign models converge towards smoother minima. Therefore, re-optimizing model parameters towards smoother minima can effectively remove backdoors. This paper introduces a novel regularizer that takes into account clean data distribution awareness and balances both model performance and backdoor purity during optimization. Additionally, ablation experiments are conducted in this study to validate the effectiveness of different components within the SFT framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In the realm of defense against backdoor attacks, this topic is undoubtedly intriguing. The author approaches the analysis of backdoors in DNNs from a fresh perspective, focusing on optimization. Overall, the proposed method demonstrates a noteworthy level of innovation, provides a substantial amount of detail, and the manuscript is exceptionally well-structured and well-written. In comparison to existing algorithms, the algorithm presents in this paper exhibits relatively superior performance."
            },
            "weaknesses": {
                "value": "1.In the relevant work, it has been written that the previous defense methods have high calculation costs, which limits their practicability in the actual environment. But won't the calculation of FIM in SFT increase the complexity and cost of calculation?\n2.The introduction of SFT also mentions that regularized Hessian has huge calculation costs in each iteration, so that approximate methods are adopted. How to ensure that the effect can be achieved is the same?\n3.Attack model on the influence of the optimization process, it does not seem to be considered.\n4.The backdoor model needs to learn both clean distribution and poison distribution. This may lead to local minima or more sharp minima in the backdoor model optimization process, but does not provide a specific solution. It may be a problem to be concerned about in practical applications."
            },
            "questions": {
                "value": "1.What are the characteristics and categories of the backdoor attack methods the authors choose to compare? Does the method cover all categories of backdoor attacks?\n2.Some typos and grammar errors are here:\nPage 1, line 35 : wights -> weights\nPage 2, line 36 : as well as -> and"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833797419,
        "cdate": 1698833797419,
        "tmdate": 1699636387422,
        "mdate": 1699636387422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sP9bbehKhF",
        "forum": "lOsF9k1sxW",
        "replyto": "lOsF9k1sxW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_dBn1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_dBn1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a backdoor purification method based on a observation that backdoored models usually tend to converge to a bad local minima. Starting from this observation, Smooth Fine-Tuning (SFT) is proposed to erase backdoors. Besides, an efficient variant, Fast SFT is introduced to reduce the fine-tuning time.  The proposed methods are extensively evaluated on four different tasks, against 14 backdoor attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The discovered observation is interesting, which indicates the optimization for the backdoor model training is harder and unstable than that for benign models.\n2. The evaluation is extensively conducted over four different tasks.\n3. It is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Although the observation is interesting, I wondering whether the observation stands when the model size increases. This is because that increasing model complexity will make it better to achieve a tough learning goal (i.e., optimization for both clean and triggered samples), where the differences on loss surface may be not so obvious between benign model and backdoored model."
            },
            "questions": {
                "value": "Could the authors add some experiments for the effect of model capacity on the discovered observation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698921469022,
        "cdate": 1698921469022,
        "tmdate": 1699636387340,
        "mdate": 1699636387340,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "380dcc783V",
        "forum": "lOsF9k1sxW",
        "replyto": "lOsF9k1sxW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_DGHS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4205/Reviewer_DGHS"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Smooth Fine-Tuning (SFT), a novel backdoor purification framework that exploits the knowledge of Fisher Information Matrix (FIM). The basic idea is to add two regularizers to the original loss to prevent the convergence to poor local minima. Some theoretical and empirical results are shown as well in the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "the paper is written clearly, motivation is good and results seem good (not familiar with these datasets)"
            },
            "weaknesses": {
                "value": "1. Theoretical justification in Eq. 1: Thm. 1 is correct, however, it does not support the observations that backdoor attacks reach bad minima, because adding poison samples do not necessarily increase the Lipschitz constant at all! The logic from the authors is since $(L_c+L_b) \\geq L_c$, the poisonous local minima have to be sharper, I guess. If so, it is definitely wrong. Otherwise, please clarify why.\n\n2. Lack of evidence that the proposed regularized method can prevent the convergence to poor local minima: The regularizers will lead the solutions to flatter regions on the **regularized**, not original, loss landscape. This is understandable, but how to guarantee the solutions fall into smoother regions in the original loss landscape is not discussed, theoretically and empirically. I believe that this is one of the key contributions that the authors try to make. So far I do not see any evidence towards this."
            },
            "questions": {
                "value": "see my comments"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698975073189,
        "cdate": 1698975073189,
        "tmdate": 1699636387267,
        "mdate": 1699636387267,
        "license": "CC BY 4.0",
        "version": 2
    }
]