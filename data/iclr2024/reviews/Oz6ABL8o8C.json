[
    {
        "id": "vQWsxlsFvZ",
        "forum": "Oz6ABL8o8C",
        "replyto": "Oz6ABL8o8C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission896/Reviewer_8iQT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission896/Reviewer_8iQT"
        ],
        "content": {
            "summary": {
                "value": "Knowledge Graphs (KGs) are vital in NLP, but creating them manually has limitations, leading to KG Completion (KGC) using KG Embedding (KGE) and Negative Sampling (NS) to handle many entities while reducing computational costs. The challenge of sparsity due to low link appearance frequencies in KGs is addressed through various smoothing methods like Self-Adversarial Negative Sampling (SANS), with this paper offering theoretical insights and introducing Triplet-based SANS (T-SANS), showing improved performance on multiple datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The author provides clear mathematical problem formulation."
            },
            "weaknesses": {
                "value": "The motivation of this study is a little bit confusing. And the contribution is not clearly articulated."
            },
            "questions": {
                "value": "Is the focus of this study to improve interpretation  or  model performance of KGE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698450626840,
        "cdate": 1698450626840,
        "tmdate": 1699636016492,
        "mdate": 1699636016492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HkT9AVwxgn",
        "forum": "Oz6ABL8o8C",
        "replyto": "Oz6ABL8o8C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a unified interpretation of smoothing methods, SANS and subsampling, for negative sampling loss function in KGE. Authors emphasize the importance of smoothing both p(x,y), p(y|x), and p(x) in the loss function to deal with the data sparsity of KG. Based on the analysis of SANS and subsampling negative sampling loss function, authors propose a new negative sampling function T-SANS, which integrate both subsampling and SANS in the loss function. Experiments show that with T-SANS as the negative sampling method, the KGE models generally performs better than SANS or existing subsampling negative sampling methods, especially on the extreme unbalanced and sparse KGs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic is interesting and worth to investigate since negative sampling methods significantly affects the KGE performance for KGC tasks.\n2. The paper tried to find a uniform loss function representation for SANS and subsampling negative sampling methods, which is good. \n3. T-SANS performs better than SANS and subsampling methods supports the importance of smoothing both p(x,y), p(y|x), and p(x) in the loss function."
            },
            "weaknesses": {
                "value": "1. My main concern of the paper is limited novelty contribution. T-SANS adds the subsampling based on SANS, referring to $p_{\\theta}(x;\\gamma)$ in Equation (12), which is the key difference between T-SANS and SANS. While as mentioned by the author in the footnote, Sun et al. (2019); Zhang et al. (2020b) use subsampling in their released implementation without referring to it in their paper. Thus, if I understood correctly,  I would like to say the actual implementation of methods of Sun et al. (2019); Zhang et al. (2020b) is very similar to T-SANS. Thus the novelty of this paper is limited. \n2. The work is motivated by that conventional works use SANS and subsampling with no theoretical background, and authors believed there is room for further performance improvement. It is unclear the why the lack of the theoretical background lead to potential performance improvement.\n3. Some parts of the paper is not clearly explained or inaccurate and need further improvement, such as\n* the $^{-\\alpha}$ in Equation (4) is unexplained \n* statement in page 5 that \"using Eq. (11) causes an imbalanced loss between the first and second terms since the sum of p\u03b8 (x, yi ) on \u03bd number of negative samples is not always 1\" is not accurate, since in the implementation of the model, there usually is a softmax function among over all the negative samples for a positive triple, which will make the sum of $p_{\\theta} (x, y_i )$ to \u03bd number of negative samples to 1. \n* the caption of Figure 4 is the same as Figure 3."
            },
            "questions": {
                "value": "1. What is the key/significant difference between T-SANS and the actually implementation of Sun et al. (2019); Zhang et al. (2020b) methods, i.e. SANS with subsampling? \n2. Should the caption of  Figure 4 to be updated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission896/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission896/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission896/Reviewer_4kKk"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676742640,
        "cdate": 1698676742640,
        "tmdate": 1699636016422,
        "mdate": 1699636016422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LZiXExM40H",
        "forum": "Oz6ABL8o8C",
        "replyto": "Oz6ABL8o8C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission896/Reviewer_s2ka"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the significance of Knowledge Graphs (KGs) in Natural Language Processing (NLP) tasks. The primary focus is on Knowledge Graph Completion (KGC), which aims to automatically complete KGs by scoring their links using Knowledge Graph Embedding (KGE). The paper discusses the challenges posed by the sparsity of KGs and the role of Negative Sampling (NS) loss in addressing these challenges. The paper introduces smoothing methods like Self-Adversarial Negative Sampling (SANS) and subsampling to tackle the sparsity issue. The main contribution is a theoretical interpretation of these smoothing methods and the introduction of a new NS loss called Triplet-based SANS (T-SANS). Experimental results on various datasets demonstrate the effectiveness of T-SANS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1 The paper provides a comprehensive theoretical understanding of smoothing methods for NS loss in KGE.\nS2 The paper presents experimental results on multiple datasets, showcasing the effectiveness of T-SANS."
            },
            "weaknesses": {
                "value": "W1 While T-SANS aims to improve upon existing methods, the computational overhead, especially in terms of memory usage and processing time, might not be thoroughly addressed.\n\nW2 While the paper provides a comprehensive theoretical understanding of smoothing methods for NS loss in KGE, it might be too dense for a broader audience. The depth of the theoretical content might make it less accessible to practitioners or researchers from adjacent fields.\n\nW3 How does T-SANS handle extremely sparse datasets compared to other methods? Is there a threshold of sparsity beyond which T-SANS might not be as effective?\n\nW4 How generalizable is T-SANS to other related tasks beyond KGC? Has it been tested on tasks other than KG embedding?"
            },
            "questions": {
                "value": "Q1 How does T-SANS handle extremely sparse datasets compared to other methods? Is there a threshold of sparsity beyond which T-SANS might not be as effective?\n\nQ2 How generalizable is T-SANS to other related tasks beyond KGC? Has it been tested on tasks other than KG embedding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734083468,
        "cdate": 1698734083468,
        "tmdate": 1699636016348,
        "mdate": 1699636016348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TDgEjtXqzv",
        "forum": "Oz6ABL8o8C",
        "replyto": "Oz6ABL8o8C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission896/Reviewer_fiTR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission896/Reviewer_fiTR"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how different smoothing methods affect the negative sampling losses for knowledge graph embedding. It introduces a new triplet-based self-adversarial negative sampling method that can adjust the frequencies of triplets, queries, and answers in the training data. It evaluates the proposed method on three benchmark datasets with five base models and demonstrates its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Negative sampling is a crucial technique for learning KG embeddings. This paper offers a valuable insight into the smoothing methods for learning loss in KGE. I think it is an interesting and relevant work for the KGE community.\n\n2. Based on the comparison and analysis of existing smoothing methods, the paper proposes triplet-based SANS, which can outperform other baselines on three datasets."
            },
            "weaknesses": {
                "value": "1. In my view, the proposed method is incremental work based on previous studies. It is an extension of SANS.\n\n2. Another weakness is that the selected KGE models in the experiments are old. Some popular or recent models, such as TuckER [1] and HousE [2], are not included, which, in my view, may weaken the soundness the work.\n\n[1] Ivana Balazevic, Carl Allen, Timothy M. Hospedales: TuckER: Tensor Factorization for Knowledge Graph Completion. EMNLP/IJCNLP (1) 2019: 5184-5193\n\n[2] Rui Li, Jianan Zhao, Chaozhuo Li, Di He, Yiqi Wang, Yuming Liu, Hao Sun, Senzhang Wang, Weiwei Deng, Yanming Shen, Xing Xie, Qi Zhang: HousE: Knowledge Graph Embedding with Householder Parameterization. ICML 2022: 13209-13224"
            },
            "questions": {
                "value": "1. Why are some results on YGAO3-10 missing? I think it would be better to produce the results using open-source implementations.\n\n2. Is it possible to provide any analysis or experimental results to assess the effect of negative sampling on the convergence rate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission896/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834216235,
        "cdate": 1698834216235,
        "tmdate": 1699636016257,
        "mdate": 1699636016257,
        "license": "CC BY 4.0",
        "version": 2
    }
]