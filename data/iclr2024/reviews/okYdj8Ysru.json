[
    {
        "id": "etNNQWrrDG",
        "forum": "okYdj8Ysru",
        "replyto": "okYdj8Ysru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_R6Kt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a batch-normalization (BN) method for manifold-valued features in neural networks. While in prior works several BN techniques are proposed for specific types of manifold, the authors present a general manifold-based BN formulation from a viewpoint of Lie-group. Besides, especially for SPD manifolds, practical BN methods are derived from the general formulation based on three types of pull-back metrics [Chen+23].\nThe experimental results using SPDnet and TSMnet demonstrate that the propose methods exhibit competitive performance to SOTAs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ A general formulation of manifold-based BN is presented through reviewing/summarizing several Riemannian-normalization (RN) approaches.\n+ Practical BNs for SPD matrices are derived in an efficient form from the general formulation."
            },
            "weaknesses": {
                "value": "Novelty of LieBN in Sec.4 is limited as it is rather straightforward from the prior works [Kobler+22a] and [Chakraborty+20].\n\nOn the other hand, the pull-back metrics [Chen+23ab] are effectively applied to the general formulation to instantiate practical BN methods for SPD manifolds in an interesting way.\nThough especially pull-back Euclidean metrics seem to be efficient as shown in Table 3, this paper lacks in-depth analysis about the methods from qualitative and/or computational viewpoint.\nIt is demanded to clarify computational details such as by showing back-props through comparison to the other RN approaches based on complicated manifold-based computation, which would significantly improve reproducibility.\n\nConsidering \\theta-parameterization does not work so well as shown in Sec.6, such a parametric extension might be redundant, rather complicating the discussion in Sec.4. In stead of that extension, it may be better to focus on analyzing the practical BN methods shown in Table 3.\n\nAs to empirical performance results reported in the experiments, superiority of the method is less clear since the performance improvement is not significant due to large stds of performance scores.\nTo clarify the efficacy of the proposed method, it should be compared with the other RN methods in terms of computation cost not only the classification performance.\n\nBased on the experimental results, one cannot identify the best SPD-BN method that outperforms the others consistently. Although the authors insist such an inconsistency shows generality of the approach, it is less understandable and unfavorable from a practical viewpoint. In this case, provide some discussion and/or analysis about connection between types of metrics and tasks (or network architectures) for rendering insights into the SPD metrics.\n\nMinor comments:\nIn Eq.5: $\\frac{1}{N} \\sum$ -> $\\sum$"
            },
            "questions": {
                "value": "The above-mentioned concerns should be addressed especially in the following points.\n- Analysis about the SPD-BN methods in Table 3 from computational viewpoint in comparison to the other RNs.\n- Empirical comparison regarding computation cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698147529862,
        "cdate": 1698147529862,
        "tmdate": 1699636369176,
        "mdate": 1699636369176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l3ksoigxku",
        "forum": "okYdj8Ysru",
        "replyto": "okYdj8Ysru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_LaGM"
        ],
        "content": {
            "summary": {
                "value": "The paper concerns batch normalization for Lie group valued data. The authors propose a unified framework for batch normalization that they claim offers theoretical guarantees."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I have a hard time finding strengths that were not already presented in previous papers. I hope the authors can argue to the opposite, but as of now I am not sure of what is the actual contribution of the paper."
            },
            "weaknesses": {
                "value": "- I am unsure what is the contribution of the paper. As the authors state, the normalization scheme they propose has been used in previous work. There are some claims like \"In contrast, our work provides a more extensive examination, encompassing both population and sample properties of our LieBN in a general manner. Besides, all the discussion about our LieBN can be readily transferred to right-invariant metrics. \" but I was not able to find out what specifically these differences are. The approach seems to be almost exactly the same when I look up in the cited papers where it is applied to Lie groups as well.\n- using the Riemannian or Lie group exp and log maps for batch normalization was a good idea the first time it was presented, but I don't see the value added with the current paper"
            },
            "questions": {
                "value": "I believe the authors need to argue convincingly what is the contribution of the paper, and why the paper presents a significant contribution relative to the previously methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698183698284,
        "cdate": 1698183698284,
        "tmdate": 1699636369098,
        "mdate": 1699636369098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0rmf6062pD",
        "forum": "okYdj8Ysru",
        "replyto": "okYdj8Ysru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
        ],
        "content": {
            "summary": {
                "value": "Study of Deep Neural Networks (DNNs) on manifolds, associated with normalization techniques, with a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Theoretical guarantee are provided to caracterize the stability of the process. \nApproach is illustrated for Symmetric Positive Definite (SPD) manifolds, with three families of parameterized Lie groups, in a SPD neural networks.  Experiments have been done for radar recognition, human action recognition, and electroencephalography (EEG) classification."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Interesting algorithm LieBN, which enables batch normalization over Lie groups, to normalize both the sample and population statistics.and apply to SPD manifolds."
            },
            "weaknesses": {
                "value": "Density of probability on SPD matrix could be only defined as invariant to all the automorphisms of SPD manifold. To assess which density verify this property, you have to consider \"Lie Groups Thermodynamics\" developped by Jean-Marie Souriau. Consider upper-half space of Siegel (pure imaginary axis is the space of SPD matrix) where the Lie group SU(n,n) acts transitivelly. With Souriau method, you are able to compute the Gibbs density of maximum entropy that is covariant to SU(n,n). If you restrict to the imaginary axis, you find the density for SPD matrices. See the following reference and put it in your references:\n[A] Barbaresco, F. (2021). Gaussian Distributions on the Space of Symmetric Positive Definite Matrices from Souriau\u2019s Gibbs State for Siegel Domains by Coadjoint Orbit and Moment Map. In: Nielsen, F., Barbaresco, F. (eds) Geometric Science of Information. GSI 2021. Lecture Notes in Computer Science(), vol 12829. Springer, Cham. https://doi.org/10.1007/978-3-030-80209-7_28"
            },
            "questions": {
                "value": "Add the following references on batch normalization\n[B] Daniel Brooks. Deep Learning and Information Geometry for Time-Series Classification. Machine Learning [cs.LG]. Sorbonne Universit\u00e9, 2020. English. \u27e8NNT : 2020SORUS276\u27e9. \u27e8tel-03984879\u27e9; https://theses.hal.science/tel-03984879\n[C] D. Brooks, O. Schwander, F. Barbaresco, J. . -Y. Schneider and M. Cord, \"Deep Learning and Information Geometry for Drone Micro-Doppler Radar Classification,\" 2020 IEEE Radar Conference (RadarConf20), Florence, Italy, 2020, pp. 1-6, doi: 10.1109/RadarConf2043947.2020.9266689.\n[D] D. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"A Hermitian Positive Definite neural network for micro-Doppler complex covariance processing,\" 2019 International Radar Conference (RADAR), Toulon, France, 2019, pp. 1-6, doi: 10.1109/RADAR41533.2019.171277.\n[E] D. A. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"Complex-valued neural networks for fully-temporal micro-Doppler classification,\" 2019 20th International Radar Symposium (IRS), Ulm, Germany, 2019, pp. 1-10, doi: 10.23919/IRS.2019.8768161.\n[F] D. A. Brooks, O. Schwander, F. Barbaresco, J. -Y. Schneider and M. Cord, \"Exploring Complex Time-series Representations for Riemannian Machine Learning of Radar Data,\" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 2019, pp. 3672-3676, doi: 10.1109/ICASSP.2019.8683056.\n[G] Brooks, D., Schwander, O., Barbaresco, F., Schneider, JY., Cord, M. (2019). Second-Order Networks in PyTorch. In: Nielsen, F., Barbaresco, F. (eds) Geometric Science of Information. GSI 2019. Lecture Notes in Computer Science(), vol 11712. Springer, Cham. https://doi.org/10.1007/978-3-030-26980-7_78"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no conflict of interest with authors."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_B3vJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780140757,
        "cdate": 1698780140757,
        "tmdate": 1699636369026,
        "mdate": 1699636369026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pJPAT2ChMv",
        "forum": "okYdj8Ysru",
        "replyto": "okYdj8Ysru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a batch normalization layer for neural networks on Lie groups. The authors then focus on SPD neural networks to showcase their approach. The proposed method is validated on radar recognition, action recognition, and electroencephalography (EEG) classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Proofs are given in the supplementary material (I did not thoroughly check them)\n* Experiment results show improvements over some state-of-the-art SPD neural networks"
            },
            "weaknesses": {
                "value": "* The paper lacks of novelty\n* Experimental results are not convincing\n* No discussion about the limitations of the proposed approach"
            },
            "questions": {
                "value": "The proposed technique is a simple tweak of those from Kobler et al. (2022b), Lou et al. (2020), Chakraborty (2020). \nNo new concepts or ideas have been developped w.r.t. these works. While the authors state that the proposed technique works for Lie groups and is able to control mean and variance in contrast to these works, extensions of these works to Lie groups, as done in the paper, are trivial.\n\nThe experimental results are not convincing since the proposed method is only compared with some SPD neural networks. For example, on human action recognition, the proposed method is outperformed by the method of Laraba et al. (2017) on HDM05 dataset by a large margin (72.27\\% vs. 83.33\\%). This shows that the proposed technique is probably not effective compared to other learning techniques designed in Euclidean space. \n\n*Question*\n\nHow does the proposed method perform on another Lie groups, e.g. when being used in LieNet (Huang et al., 2017) ?\n\n*References*\n\n1. Sohaib Laraba, Mohammed Brahimi, Jo\u00eblle Tilmanne, Thierry Dutoit: 3D skeleton-based action recognition by representing motion capture sequences as 2D-RGB images. Comput. Animat. Virtual Worlds 28(3-4) (2017)\n\n2. Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool: Deep Learning on Lie Groups for Skeleton-Based Action Recognition. CVPR 2017: 1243-1252."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_ZRPd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4053/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822011366,
        "cdate": 1698822011366,
        "tmdate": 1699636368935,
        "mdate": 1699636368935,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ftv4p0eMPg",
        "forum": "okYdj8Ysru",
        "replyto": "okYdj8Ysru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
        ],
        "content": {
            "summary": {
                "value": "The authors describe a new kind of Batch Normalization layer for\nRiemaniann neural networks. The proposed technique is a theoretical\nimprovement over existing batch-norm layers by being a generalized and\nunified view on all previously proposed technique, using the Lie-group\nstructure of the manifold."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A very pleasant to read recap on all batch norm-like layers for\n  Riemannian networks.\n- Theoretical guaranties on the control provided by the layer.\n- Convincing experimental evaluation (not SOTA obviously, but an\n  improvement over other manifold methods)\n- LieBN reduces to the classical BN for Euclidean manifold."
            },
            "weaknesses": {
                "value": "- Nearly all tables are barely readables.\n- Novelty of the work should emphasized more. LieBN provides more\n  guaranties and a more sound approach. But in the writing, it is not\n  completely clear of what is a full novelty over previous methods and\n  what is a generalization.\n- A broad zoo of choices (AIM, LEM, LCM and alpha beta variants), but no\n  clue on choosing. It's an usual question with this type of methods,\n  and it always a little bit disapointing to simply benchmark over all\n  the possible choices."
            },
            "questions": {
                "value": "- What is the interest of the (alpha, beta) generalization ? In\n  particular in context of neural networks ? And what are the value used\n  in the experiments ?\n- In the article about RBN, Brooks et al discuss about the amount of\n  data need to achieve good performance. Any insight about this for your\n  layer ?\n- Is there a link between Frechet variance and the variance of the\n  Gaussian used for normalization ?\n\n- I guess it should be \"neutral element\" instead of identity in Eq 13 ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4053/Reviewer_Urfr"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4053/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700429937686,
        "cdate": 1700429937686,
        "tmdate": 1700429937686,
        "mdate": 1700429937686,
        "license": "CC BY 4.0",
        "version": 2
    }
]