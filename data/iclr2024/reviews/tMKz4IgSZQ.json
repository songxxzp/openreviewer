[
    {
        "id": "hCqEmEe58s",
        "forum": "tMKz4IgSZQ",
        "replyto": "tMKz4IgSZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_nAwp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_nAwp"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce Control-GPT, which is claimed as a simple yet effective framework that leverages LLMs to generate sketches based on text prompts firstand thereafter feed those sketches to Control-GPT to generate images. Satisfactory qualitative results justify the efficiency of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The concept introduced here is really interesting, although the components used here carry less novelty.\n\n2. The writing of introduction, and the overall paper is quite fluid and easy to understand.\n\n3. Providing additional baselines apart from state-of-the-arts competitors is appreciated."
            },
            "weaknesses": {
                "value": "1. Although the authors frequently use the term 'sketches', they are not really sketches ([D], [E]), rather quite far from it in that they do not model human subjectivity at all.\n\n2. This paper seems more of a combination of multiple concepts like Diffusion models + GPT + conditioning signal encoder. Although there can be novelty depending upon the formulation of usage of pre-known concepts, this paper lacks a strong formulation in that aspect.\n\n3. Given there are papers which can directly generate the same quality images from sparse line drawings (e.g., ControlNet [A], T2I-Adapter [B], Voyonov et al. [C], etc.), it is not very appealing to have yet one more paper that generates additional pseudo sketches to generate image. As in -- why should I write a 100 word prompt to generate a \"pseudo sketch\" and then generate an image from it?\n\n4. Certain claims like \"The paper is also the fist to demonstrate a possible way for joint optimization over different AI models\" are quite strong without adequate proof or citations.\n\n5. Quality of Figures could be improved.\n\n6. Although technical aspects are there in this paper, the motivation or the need is not quite clear.\n\nMinor:\nTypo: Conclusion section GPt-4 > GPT-4\n\n[A] Adding Conditional Control to Text-to-Image Diffusion Models. Lvmin Zhang, Anyi Rao, Maneesh Agrawala.\n[B] T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie\n[C] Sketch-Guided Text-to-Image Diffusion Models. Andrey Voynov, Kfir Aberman, Daniel Cohen-Or.\n[D] How Do Humans Sketch Objects?. Mathias Eitz, James Hays and Marc Alexa.\n[E] Why Do Line Drawings Work? A Realism Hypothesis. Aaron Hertzmann"
            },
            "questions": {
                "value": "1. What is necessity of TiKz figures here?\n2. Why do I need to write a lengthy prompt? What is that significant a necessity for prompt which can not be acquired from directly drawing or sketching a semantic map?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9064/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9064/Reviewer_nAwp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816135070,
        "cdate": 1698816135070,
        "tmdate": 1699637141426,
        "mdate": 1699637141426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rm78PItgwB",
        "forum": "tMKz4IgSZQ",
        "replyto": "tMKz4IgSZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_4EN4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_4EN4"
        ],
        "content": {
            "summary": {
                "value": "To enhance controllable text-to-image generation, the authors propose using ChatGPT to initially generate TikZ sketches. These sketches are then fed into a fine-tuned ControlNet. Furthermore, to facilitate the fine-tuning of ControlNet, the authors adapt existing datasets to create aligned text, images, and polygons that mimic the sketches. Experiments demonstrate the effectiveness of the proposed method in terms of controllability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear and easy to follow.\n\n2. The authors combine Large Language Models and Diffusion Models to achieve controllability in text-to-image generation.\n\n3. The experiments provide evidence that the proposed method can effectively control the generation of objects that match the given text, in terms of position and also size."
            },
            "weaknesses": {
                "value": "1. The proposed method relies on existing models, including ChatGPT and ControlNet, which limits its technical novelty.\n\n2. The authors suggest using ChatGPT to produce TikZ sketches and then fine-tuning ControlNet with TikZ sketches, text, and grounding tokens. However, pretrained ChatGPT can accept semantic segmentation as input, and ChatGPT can produce code to generate the required mask with the desired position and size. In light of this, why don't the authors simply use the semantic mask, which would eliminate the need for further fine-tuning of ControlNet?\n\n3. I am confused about the quantitative comparison presented in Table 3 and the qualitative comparison in Figure 6. What is the experimental setting for ControlNet? What inputs does ControlNet receive? Additionally, in Table 2, the authors only compare their method with image generation models without additional semantic inputs. Why don't the authors include ControlNet in this comparison? I am also puzzled by the baseline; the authors claim it includes ControlNet with Segmentation and ControlNet with Canny Detection, but I cannot find these two baselines in the experiments.\n\n4. In Figure 3, the top example appears to depict two giraffes rather than two zebras."
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699116351475,
        "cdate": 1699116351475,
        "tmdate": 1699637141315,
        "mdate": 1699637141315,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AMuYlc7klG",
        "forum": "tMKz4IgSZQ",
        "replyto": "tMKz4IgSZQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_uyRt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9064/Reviewer_uyRt"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to use an LLM to produce TikZ code to generate sketches from text. The sketches and text can then guide a ControlNet-like architecture for layout-to-image synthesis focusing on correct spatial arrangement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper tackles an important problem, namely to improve spatial instruction following in current text-to-image models, which tend to ignore fine-grained information such as spatial arrangement, object relationships, numerical information and attribute binding.\n- The paper is well-written, with a clear presentation of ideas, experiments, and results.\n- To my knowledge, conditioning image generative models on TikZ sketches is novel.\n- The results suggest that the model can better follow the spatial instructions provided via text."
            },
            "weaknesses": {
                "value": "- The paper should identify the advantages of using TikZ over simpler geometric shapes like rectangles or leveraging text-to-layout/mask prediction networks. There has been prior work using LLMs to generate layouts (e.g., https://arxiv.org/abs/2305.15393, https://arxiv.org/abs/2308.05095).\n\n- The dataset creation process, which involves extracting polygons from COCO, raises the question of why not use the instance masks (or bounding boxes) directly as a baseline, which could eliminate the need for an LLM, TikZ code generation, and image compilation. In fact, fine-tuning on instance masks from COCO should be considered a baseline to analyze the possible benefits of using TikZ sketches. Sampling can be realized through a additional text-to-layout/mask prediction network (many prior GAN methods explored this).\n\n- There seems to be a lack of fine-tuning the vanilla ControlNet on the TikZ dataset, which should be considered as an important baseline for the study.\n\n- The vanilla ControlNet baseline is missing in table 2, and table 4.\n\n- Several of the generated TikZ sketches presented in the appendix are weird:\n    - appendix fig1:\n        - A bird to the left of a bag is spatially wrong\n        - A bird above a (b?)tottle has a weird bird shape and could be replaced by a rectangle\n    - appendix fig2:\n        - A dog to the left of * ; all are different and the generated image does not follow the masks\n\n- Missing references for the models listed in table 2 of the paper.\n\n- ControlGPT is incorrectly identified as the best in the Visor 2 metric when it is actually Dalle-2 that has the highest score.\n\n- Figures 7 in the main paper and figures in the appendix should visualize generated images from ControlNet baseline as well as ControlGPT side-by-side for a more comprehensive evaluation of visual results."
            },
            "questions": {
                "value": "- Why prompting GPT to fill each object with red color and keeping objects separated? Wouldn't using different colors for different objects be more sensible, and how are overlapping or occlusion scenarios between objects managed.\n\n- Dalle-2 shows better performance in object accuracy according to table 2, yet it scores lower in the Visor metric. The paper should include a discussion on the possible reasons behind this discrepancy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699267495275,
        "cdate": 1699267495275,
        "tmdate": 1699637141188,
        "mdate": 1699637141188,
        "license": "CC BY 4.0",
        "version": 2
    }
]