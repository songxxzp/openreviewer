[
    {
        "id": "M8ScalmqtI",
        "forum": "uwY0GOqZBh",
        "replyto": "uwY0GOqZBh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_pu5a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_pu5a"
        ],
        "content": {
            "summary": {
                "value": "The paper confronts a significant challenge in long-tailed classification on graphs. While most prior research concentrates on mitigating bias, this paper offers a fresh perspective by introducing a theoretical framework for characterizing long-tail categories and improving generalization in real-world scenarios. The authors present the TAIL2LEARN framework, encompassing hierarchical task grouping and long-tailed balanced contrastive learning. Notably, the experimental results demonstrate promising performance, outperforming state-of-the-art methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed approach is novel and addresses a significant gap in the existing literature by providing a theoretical foundation for long-tail classification on graphs. The motivation for this work is well-defined and highlights the need for a deeper understanding of class imbalances and generalization performance.\n- A notable strength of the paper is its comprehensive theoretical analysis, which includes the development of a Generalization Error Bound that substantiates the effectiveness of the proposed method.\n- The experimental results effectively illustrate the superiority of the proposed TAIL2LEARN framework. By showcasing its effectiveness in characterizing long-tail categories on real-world graph datasets, the authors provide practical evidence of their method's capabilities."
            },
            "weaknesses": {
                "value": "- One potential weakness of the paper is that the hierarchical task grouping approach employed by the authors seems similar with existing techniques like Graph U-Net [1]. Although the authors have extended these prior methods to facilitate multi-task learning and task grouping with theoretical backing, it may require clarification about what sets the TAIL2LEARN framework apart from the existing Graph U-Net. Further clarification and a more detailed comparison between the two would be beneficial to better understand the novelty and differentiation of the proposed framework.\n- While the authors have approached long-tailed classification as a multi-task learning problem, they have configured the number of tasks in the second layer to align with the number of categories. It might be worth considering whether the authors have explored the possibility of subdividing the samples into more finely-grained subclasses, which means increasing the number of tasks in the second layer beyond the number of categories.\n- The authors claimed that $\\mathcal{L}_{BCL} potentially controls the range of losses for different tasks. However, the paper lacks experimental results to support this claim, which could contribute to a more robust evaluation of the method's effectiveness.\n\n[1] Gao H, Ji S. Graph u-nets[C]//international conference on machine learning. PMLR, 2019: 2083-2092."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Reviewer_pu5a"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698242686077,
        "cdate": 1698242686077,
        "tmdate": 1699636650146,
        "mdate": 1699636650146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SgaDOxWUne",
        "forum": "uwY0GOqZBh",
        "replyto": "uwY0GOqZBh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_Wqoo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_Wqoo"
        ],
        "content": {
            "summary": {
                "value": "This paper investigate long-tailed classification on graphs by providing a PAC generalization bound in a multi-task learning fashion, which is characterized by the task number and overall loss range. As a solution, the authors propose Tail2Learn, a learning framework for long-tailed node classification, which reduces task complexity by hierarchically grouping tasks and adopting a contrastive loss to adaptively balance the gradients of both head and tail classes to control the loss range."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The method presented in this paper is straightforward and easily comprehensible.\n\n+ The approach of addressing the long-tail problem through a multi-task learning perspective appears to be original.\n\n+ The empirical results in the experiment section indicate a promising improvement compared to the baseline methods."
            },
            "weaknesses": {
                "value": "- The theoretical aspect of the paper appears to be quite preliminary, lacking in-depth analysis and original contributions. It appears to heavily rely on Theorem 8 from a previous work [1]. Additionally, some statements and derivations are unclear and contain errors, making it challenging to verify their correctness. For specific concerns, please refer to the detailed questions.\n\n- While the shift in perspective towards multi-task learning is novel, the proposed method is essentially a combination of existing, well-known techniques, such as hierarchy graph pooling [2] and contrastive loss [3].\n\n- The discussion and comparison of some relevant work (such as [4][5]) are missing in this paper.\n\n[1] Maurer et al., The Benefit of Multitask Representation Learning, 2016\n\n[2] Ying et al., Hierarchical graph representation learning with differentiable pooling, 2018\n\n[3] Zheng et al., Tackling Oversmoothing of GNNs with Contrastive Learning, 2021\n\n[4] Zhang et al., \"Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation, 2022\n\n[5] Zheng et al., Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhood, 2022"
            },
            "questions": {
                "value": "1. In Lemma 1, after applying the PCA bound provided by [1], how is the normalization term $1/T$ eliminated? If $1/T$ needs to remain, I would question one of the main claims in the abstract: \"generalization performance of long-tailed classification is dominated by the total number of tasks'' (as stated in the abstract). This is because, after accounting for $1/T$, such a bound may no longer scale with the number of tasks. Actually, Theorem 2 in [1] even suggests the generalization error decays with $O(1/\\sqrt{T})$ when transferred to new task.\n\n2. In Lemma 3, why is the definition of $R(F)$ (Eq. 11) different from Eq. 4 in [1]? Note that Eq. 4 has $f(y) - f(y')$ in the numerator, while Eq. 11 has $l(f(y) - f(y'))$ in the numerator.\n\n3. In the proof of Corollary 1, it remains unclear to me how the inequality between the last terms (under the square root) is established.\n\n4. Why is this analysis focused specifically on long-tail classification on graphs? Can it be extended to the general long-tail learning problem?\n\n5. Can the authors explain why the proposed approach, Tail2Learn, takes on the form of $f \\circ h,\" resembling a general multi-task learning framework?\n\n6. It is known that task complexity is not always harmful. Instead, improving task diversity can be helpful for multi-task learning [2][3]. Does this contradict to this paper\u2019s claim?\n\n[1] Maurer et al., The Benefit of Multitask Representation Learning, 2016\n\n[2] Tripuraneni et al., On the Theory of Transfer Learning: The Importance of Task Diversity, 2020\n\n[3] Du et al., Few-Shot Learning via Learning the Representation, Provably, 2020"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Reviewer_Wqoo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782531577,
        "cdate": 1698782531577,
        "tmdate": 1699636650045,
        "mdate": 1699636650045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cXt0OhChMV",
        "forum": "uwY0GOqZBh",
        "replyto": "uwY0GOqZBh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_Td6K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_Td6K"
        ],
        "content": {
            "summary": {
                "value": "The paper studies long-tail categories in graphs. It proposes a generalization bound for long-tail classification on graphs, as well as a method TAIL2LEARN for long-tailed classification on graphs. The method includes a hierarchical task grouping module to reduce complexity of task space and a contrastive learning module to balance the gradients of head and tail classes. Experiments are conducted to evaluate the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provides theoretical studies and arrives at a generalization bound.\n\n2. The paper presentation includes rich contents, with tables and figures well organized.\n\n3. The conducted experiments look correct with ablation studies included and code provided."
            },
            "weaknesses": {
                "value": "1. Related works not well addressed. The long-tail categories studied in the paper is the same as the node-level imbalanced-class problem in graph. The imbalanced class problem has been studied intensively for graphs, which is closely related to this work but not sufficiently discussed in its related works. The paper lacks a thorough review of related literature. Some missing related works are [1-6].\n\n2. Following the above point, the experiments should include some of the missing imbalanced class baselines.\n\n3. The correctness of Corollary 1 is unclear. Why can contrastive learning guarantee to learn the predictors $f_1^{(l)}, . . . , f_T^{(l)}$ with $Range(f_1^{(l)}, . . . , f_T^{(l)}) < Range(f_1, . . . , f_T)$? In its proof, why do we only need to compare the relationship between $\\sum _t 1/(n_t^{(l)})$ and  $\\sum _t 1/(n_t)$? And how is the special case of all nodes in one hypertask generalized to prove $\\sum _t 1/(n_t^{(l)})\\leq \\sum _t 1/(n_t)$? The proof should be clearly given step-by-step instead of ambiguously stated.\n\n\n\n[1] Imgcl: Revisiting graph contrastive learning on imbalanced node classification\n\n[2] Boosting-GNN: boosting algorithm for graph networks on imbalanced node classification\n\n[3] Graph neural network with curriculum learning for imbalanced node classification\n\n[4] Co-Modality Graph Contrastive Learning for Imbalanced Node Classification\n\n[5] Diving into Unified Data-Model Sparsity for Class-Imbalanced Graph Representation Learning\n\n[6] TAM: topology-aware margin loss for class-imbalanced node classification"
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782839978,
        "cdate": 1698782839978,
        "tmdate": 1699636649932,
        "mdate": 1699636649932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UfIKF4dcEg",
        "forum": "uwY0GOqZBh",
        "replyto": "uwY0GOqZBh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_8rhB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6043/Reviewer_8rhB"
        ],
        "content": {
            "summary": {
                "value": "While current methods focusing on the long-tail problem in graphs have shown notable improvements, this work, Tail2Learn, approaches from a different perspective and formulates the long-tail classification problem into a multi-task learning framework. Built upon theoretical findings, it controls the complexity of task space and the loss range of task-specific classifiers by offering remedies such as hierarchical task grouping and long-tailed balanced contrastive learning. The experiments on the node classification task show the efficacy of Tail2Learn in real-world long-tailed graph datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I quite enjoyed reading this paper. Overall, the claims of this paper are well-formulated, and its remedies for the theoretical findings are well-supported.\n  \n2. The proposed Definition 1, Long-Tailedness Ratio, is intuitive and straightforward. This metric can be generalized to balanced cases, such as 5 classes having 20 training samples each, as it would have a value of 4 in the 80th percentile.  This contribution would further enrich the long-tail GNN community.\n   \n3. The empirical performance aligns with the theoretical motivation. Also, the paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "*Major*\n1. In M1. Hierarchical Task Grouping, I agree that this approach can reduce label scarcity and task complexity. However, I am concerned whether hierarchical grouping across different classes might compromise the distinctiveness of each class. That is, there could be a trade-off between achieving reduced complexity and maintaining distinctiveness among classes. Although there exists a module for contrastive loss between different classes, its contribution remains unclear. A more detailed discussion of such situations should be provided.\n  \n2. In M2. Long-Tail Balanced Contrastive Learning, the utilization of supervised contrastive loss seems reasonable. However, given the long-tail situation, there would be very few training samples with labels for tail classes. Consequently, the positive pairs within tail classes would be significantly fewer compared to the head classes. Can you elucidate how Tail2Learn can work effectively in this scenario?\n  \n3. Although the overall performance of Tail2Learn is effective in current datasets, can you provide more details about the improvements made in tail classes as shown in Figure 4 in LTE4G [1]? This would offer a more comprehensive understanding of Tail2Learn's efficacy in terms of improvement in tail classes without sacrificing performance in head classes.\n  \n4. Can Tail2Learn generalize well on graph datasets having a relatively small number of classes such as Cora, CiteSeer, and PubMed?\n   \n*Minor*\n1. Although Definition 1, Long-Tailedness Ratio, is well-designed, for clarity, at first glance, I expected the semantic meaning to refer to \"how severe the data distribution is long-tailed\". However, in actuality, the semantic meaning is \"the lower the severity of long-tailedness.\" Have you considered the reciprocal version of the current long-tailedness ratio?\n  \n2. The notation (e.g., subscripts) in Equation 6 and Equation 7 appears to be exactly the same, while the underlying meaning is different. For clarity, I suggest differentiating the notations that denote specific classes and specific hypertasks, as they do not necessarily have to be the same value.\n  \n3. The performance of ImGAGN [2] in Table 1 seems unusually low compared to classical GNN, although it is originally designed to alleviate class long-tailedness. Can you provide further explanations for this?\n  \nIf the above concerns are properly addressed, I would be very happy to raise my score on the current rating.\n    \n[1] [CIKM 2022] LTE4G: Long-Tail Experts for Graph Neural Networks  \n[2] [KDD 2021] ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks"
            },
            "questions": {
                "value": "See the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6043/Reviewer_8rhB"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818288263,
        "cdate": 1698818288263,
        "tmdate": 1700733755205,
        "mdate": 1700733755205,
        "license": "CC BY 4.0",
        "version": 2
    }
]