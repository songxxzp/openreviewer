[
    {
        "id": "m0ijYT2GKK",
        "forum": "VUA9LSmC2r",
        "replyto": "VUA9LSmC2r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an embodied vision-language planner and programmer (Octopus) trained with reinforcement learning with environmental feedback, as well as two embodied environments that yield feedback necessary to train the aforementioned model, with data collected by GPT4. Octopus takes egocentric views and tasks specified in language, and outputs next actions and code to execute it. The method is tested on environments based on OmniGibson and GTA-V."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I appreciate the introduction of reinforcement learning from environmental feedback, by efficiently using environmental rewards from the simulator + GPT-4. Though the approach of using code-writing LLMs to execute plans is not new, I believe applying it to embodied tasks in the proposed formulation is a nice demonstration of how to leverage foundation models in these embodied environments."
            },
            "weaknesses": {
                "value": "W1. The data collection process relies on GPT-4, which takes as input language of systems message + environment message to output the required plan, code, and target state. This relies on the strong assumption that the systems + environment message fully captures the environment state, since the planning must be done without access to the view of the visual scene. I presume this means that the systems prompt must be elaborate, handcrafted, and task specific, such that GPT-4 can plan reasonably in the environment using possible objects at hand. Is there a robust way of designing such prompts for different/new tasks without tuning?\n\nW2. From my understanding, GPT-4 generates the target states, and whether the target states have been met is used as environmental feedback for training Octopus. It seems possible that GTP-4 will generate an incorrect or trivial target state to satisfy the language task goal, and be able to successfully reach that predicted target state, without actually achieving the task goal. Is this understanding correct? In this case, the errors from GPT-4 seem more harmful than having unsuccessful execution from GPT-4 generated code.\n\nW3. The results on Table 2 are not particularly convincing of this method\u2019s success. Octopus should indeed outperform blind LLMs that do not take visual input. TAPA outperforms/is of equal performance in 2 of the 5 tasks. The paper lacks analysis on why this is the case. Where does TAPA fail? It is also hard to compare when the vision models are different; does OVD and CLIP-ViT perform similarly in terms of capturing information from the input scene?\n \nNit: Should add a figure describing Octopus model architecture; notations in methods section are not well defined."
            },
            "questions": {
                "value": "Q1. Is there a systematic way of generating the systems message able to generalize to new datasets? Or does it need to be hand-crafted and tuned for Omniverse & GTA & other datasets?\n\nQ2. Can you provide more analysis or experiments on where Octopus may outperform prior work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Reviewer_VoZ1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698300744642,
        "cdate": 1698300744642,
        "tmdate": 1699635964513,
        "mdate": 1699635964513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wzq2Z7GkNj",
        "forum": "VUA9LSmC2r",
        "replyto": "VUA9LSmC2r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
        ],
        "content": {
            "summary": {
                "value": "This paper leverages GPT4 to generate vision and language training data from OmniGibson and GTA-V. Then, based on the data, they train the model modified from the Otter model and perform some downstream embodied tasks to demonstrate the performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of using GPT4 with crafted prompts to acquire training data from existing environments is interesting.\n\n2. Error management and environment feedback are reasonable."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited. Essentially, it uses GPT4 with the prompt engineer to collect data from two embodied environments and then trains a vision-language agent model. Moreover, The agent model does not have a specific framework diagram to show the detailed parts, making it difficult to see which part of the model is its innovation point. I suggest providing a framework diagram to clearly explain where the model is newly proposed in the paper and how it differs from existing methods.\n\n2. This paper appears to employ an intentional use of uncommon or less frequently used words in many sentences, substituting them for simpler, more common terms that could convey the message more clearly. As a result, the reading experience becomes somewhat disjointed, and the text may come across as rather weird to the reader.\n\n3. The reason for using both OmniGibson and GTA-V environments to generate data seems not obvious. A more obvious comparison between the two environments is required, such as the visual comparison of the tasks.\n\n4. In Section Error Management, when the agent executes the wrong command, how does the method perform \"error management\" on it? It seems that this section only claims the cases under which a task is defined as failure, and does not show how to manage such failure.\n\n5. In Section ENVIRONMENTAL FEEDBACK, what if there are multiple erroneous states in a task sequence?  Which are the positive states and which are the error states at this time? And it may not be said that if one of the states is wrong, then its previous states must all be negative.\n\n6. The experimental baselines are unfair and unclear. For Blind LLMs, without visual input to GPT4, GPT4 cannot ground language into the visual environment, which will inevitably lead to worse results. As for TAPA, the reader cannot understand what kind of model it is and its workflow. Even the OVD are not introduced or cited.\n\n7. What are the tasks of the four testing environments? The authors did not give a detailed introduction.\n\n8. Some titles and analyses of the experimental sections appear to be uninformative. For example, **LLMs Does Not Depend on Observation**, is this a conclusion or a statement? If it is a statement, the authors have already said before that this baseline has no visual input, and it is also contrary to the subsequent titles which are all conclusions. In addition, in the ablation experiments, larger models and more components trained can bring more performance improvements, which is common sense in sense, but the authors put them as ablation experiments alone and do not give any insights."
            },
            "questions": {
                "value": "1. How to train the reward model $r_{\\phi}$? More details are needed.\n\n2. A visual example of task trees is required for better understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Reviewer_ABA1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552288214,
        "cdate": 1698552288214,
        "tmdate": 1699635964443,
        "mdate": 1699635964443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DSxEX4GhEw",
        "forum": "VUA9LSmC2r",
        "replyto": "VUA9LSmC2r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced Octopus, a vision-language model mapping the visual input to the action codes. The fine-tuned dataset is collected with the GPT-4 where the simulator feedback is incorporated to generate the system feedback. The author further proposed a RLEF to improve the performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposed a novel VLM  to transfer the visual input to the executable codes, driving the agents.\n- The GPT-4 along with a simulator is used to collect training datasets. OminiGibson and OctoGTA are used respectively.\n- An RLEF module is used to boost the model's performance further."
            },
            "weaknesses": {
                "value": "- More related works in Section 2.1 are needed to help the reviewer identify the paper's contributions, like [1-4]. \n- The authors term the simulation they used \"OctoGibson\" which is built upon OmniGibson. Can the authors give more details to elaborate the main difference between them, or did they just use that Simulator to collect the dataset?\n- A better format is needed. Some lines need a reformat in the revised version. One example is \"3.2 Instructions From Exploration\", lines above and below seem to belong to the same paragraph.\n- More experiments are needed. The author only conducts the experiments on the datasets they collected and lacks a direct comparison with more relative frameworks as discussed in Section 2.1. \n\n[1] Huang, Wenlong, et al. \"Voxposer: Composable 3d value maps for robotic manipulation with language models.\" arXiv preprint arXiv:2307.05973 (2023).\n[2] Lin, Kevin, et al. \"Text2motion: From natural language instructions to feasible plans.\" arXiv preprint arXiv:2303.12153 (2023).\n[3] Huang, Siyuan, et al. \"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model.\" arXiv preprint arXiv:2305.11176 (2023).\n[4] Yu, Wenhao, et al. \"Language to Rewards for Robotic Skill Synthesis.\" arXiv preprint arXiv:2306.08647 (2023)."
            },
            "questions": {
                "value": "- The author uses the GPT-4 to collect the training data, and one implicit assumption is that the performance of the GPT-4 is optimal or near-optimal. A comparison between the GPT-4 generated data sample and the human-collected sample would help. Or, did the author conduct some data quality control before the use of dataset?\n\n- When using GPT-4 plus a simulator to collect the dataset, the location of the target object is directly obtained from the simulator? And this information be stored and used for later training? With this approach, the final complete robotic system still needs a separate vision model besides the ViT-L in the VLM. Can the author give some discussion on this design choice?\n\n- RLEF. It is very interesting to see the usage and effectiveness of RLEF. However, I am curious as to why you chose CodeLLaMA-7B as the reward model while using MPT-7B for the complete VLM? \n\n- In Table 2, there is a comparison between Octopus and  MPT-7B. Also, the performance is not consistently superior, a further discussion is needed. And the metrics' definition is needed to help the understanding. \n\n- Ablation: 3B: what is the 3B model? \n\n- The author inputs 10 images to the VLM and discusses the standard version vs the random version.  Would other designs help?\n\n- The author states multiple times with \"open-sourcing\" in the main text, a link to the anonymous website would be helpful.\n\n- See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission374/Reviewer_CLdb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683900134,
        "cdate": 1698683900134,
        "tmdate": 1699635964351,
        "mdate": 1699635964351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t8lEatEXBG",
        "forum": "VUA9LSmC2r",
        "replyto": "VUA9LSmC2r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission374/Reviewer_qA1p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission374/Reviewer_qA1p"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes a model and simulator for instruction-following tasks in Embodied AI, leveraging GPT-4 for a human-model-agent task-execution paradigm."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The manuscript makes reference to relevant methodology in EAI \u2014 designing agents that include foundation models, which perform intermediate reasoning tasks"
            },
            "weaknesses": {
                "value": "Section 1 / Throughout \u2014 The manuscript forgets to properly motivate its contributions. What problem is this work supposed to be solving? What research questions are examined by this manuscript?\n\nSection 1 \u2014 Most of the Introduction section is unnecessary. The space should instead be used to describe what is added on top of GPT-4 to make the model proposed in this paper a sufficiently distinct contribution. How is OctoVerse different from other EAI simulators? Why does the community need OctoVerse? What problems can be solved in OctoVerse that cannot be solved elsewhere? The manuscript fails both to motivate and explicitly describe its contributions.\n\nSection 2 \u2014 Call it \u201cRelated Work\u201d. The dimensions on which this section compares the proposed work with the prior art are all wrong. Firstly, because the manuscript is attempting to propose a new environment and tasks, it should identify the limitation of other, similar simulators/datasets and explicitly discuss the proposed improvements. Regarding claims for novel modeling contributions, the manuscript must first propose research questions or problems that the approach attempts to solve. Next, a set of related work can be organized to discuss their attempts at answering said research question and solving said problems, as well as discuss their limitations or weaknesses. Finally, this structure affords the manuscript to describe how its proposed work improves on the prior art, according to those research questions and identified problem(s).\n\nSection 3 \u2014 The manuscript does not make clear what was originally provided by OmniGibson / GTA-V, versus what is added by OctoVerse. Also, again, the manuscript is missing motivation for why anyone should use its proposed environment. The problem formulation needs a lot of work."
            },
            "questions": {
                "value": "N/A \u2014 see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809080800,
        "cdate": 1698809080800,
        "tmdate": 1699635964265,
        "mdate": 1699635964265,
        "license": "CC BY 4.0",
        "version": 2
    }
]