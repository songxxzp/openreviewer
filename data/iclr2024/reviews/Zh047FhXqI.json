[
    {
        "id": "OUJOU80Egv",
        "forum": "Zh047FhXqI",
        "replyto": "Zh047FhXqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_U3YJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_U3YJ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the offline policy evaluation (OPE) problem. The authors propose Policy-Conditioned Model (PCM) learning, which learns a dynamics model that depend on the policy to be evaluated. In contrast, traditional model learning learn a unified dynamics model from a dataset that is agnostic to policy. Theoretical analysis are provided to justify the effectiveness of PCM. Experiments are also conducted to show the advantage of the proposed method over prior works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n2. The intuition is straightforward and the algorithm is neat.\n3. The theoretical part provide mathematical justification for the intuition of the algorithm."
            },
            "weaknesses": {
                "value": "1. The Related Work does not seem comprehensive enough to me. It should include more recent works on OPE and MBORL. I did not find citation of OPE later than 2020. For MBORL, COMBO, ROMI and many more later works are missing. I suggest adding a more comprehensive Related Work.\n2. I am not fully convinced by the experiment. As mentioned in the paper, PCM adopt more advanced network architecture to enhance performance so that even the PAM achieves better performance. Does this mean that the advantage of PCM may not come from the algorithmic novelty but from the better network architecture instead?\n3. As an experiment paper, I do not think the experiments conducted are extensive enough, more tasks should be tested.\n\nMinor Mistakes:\nFigure 2 and 3 seem to have spacing issues. The legends are partially screened by the captions\nIn some formulae (e.g. 5 and 8), the parentheses should wrap the content inside."
            },
            "questions": {
                "value": "1. Can the authors explain Figure 2(c) in more detail, how are the quantities calculated?\n2. Can the authors explain the experiment methodology in more detail, as mentioned in Weakness 2\n3. In Figure 3, what does the data for percentage 0% and 100% look like. Also, the differences of value gaps does not seem substantial to me, is it the case?\n4. How does PCM perform on other tasks besides mujoco? Specifically, D4RL has maze, antmaze, carla etc. besides mujoco."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors touched on this subject in the last section of the Appendix"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740948454,
        "cdate": 1698740948454,
        "tmdate": 1699636869798,
        "mdate": 1699636869798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2YYepzCO1H",
        "forum": "Zh047FhXqI",
        "replyto": "Zh047FhXqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_BstF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_BstF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a policy-conditioned dynamics model for off-policy evaluation. The proposed methods achieved good performance on the robotic tasks, with bounded model error under their assumptions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1.\tThe paper is well written, especially introduction and methodology are clearly described.\n\nS2.\t The proposed method achieves better results compared to baselines, on selected tasks in robotic tasks."
            },
            "weaknesses": {
                "value": "W1.\tThe ideas of assuming mixed-type of data and leveraging evaluation policies to improve state-action visitation are not very interesting. Similar assumptions and problems were investigated by recent OPE work [1], which also conducted experiments on D4RL. The major claim, \u201cmodel learning from offline datasets collected by numerous behavior policies in fact implies a problem of data fitting from a mixture of multi-source data distributions, which is ignored in current model learning paradigms (in Introduction) looks overstrong, which should be carefully scoped, given there are already works investigating such scenarios (e.g., [1-2]). \n\nW2.\tExperiments may not be thoroughly designed. \n\n(i) Only using 3 tasks out of overall 20 tasks in D4RL environment and 3 out of 9 RLUnplugged environment, without a well reason. The main content doesn\u2019t accurately or explicitly describe such experimental settings, given they do not strictly follow the referred benchmark DOPE. In D4RL, Ant-related tasks are totally ignored. Though authors describe that they consider data collected by diverse policies, it may not be a strong reason for not using all types of tasks to evaluate the robustness and effectiveness of the proposed method. At least \u201cmedium-expert\u201d tasks, which are mixed-type datasets, should be included as well. \n\n(ii) For off-policy selection, some baselines (e.g., IS) are not included. I think they are still able to be compared, by using the policy with the maximum OPE values. For off-policy evaluation, it might be also helpful to evaluate if recent works with similar ideas can be considered, as mentioned in (1).\n\n\nW3.\tReproducibility is hard to evaluate. There is no code supplementary. Raw results of absolute error without normalization are not provided as in [1], raw results of each task are not provided. Those makes it hard to be further utilized by interested researchers and limit potential impacts.\n\n\nMinor:\n1.\tFigure 3: Part of labels on x axis is covered by captions.\n\nReferences\n[1] Gao, Q., Gao, G., Chi, M., & Pajic, M. (2022, September). Variational Latent Branching Model for Off-Policy Evaluation. In The Eleventh International Conference on Learning Representations.\n\n[2] Zhang, G., & Kashima, H. (2023, June). Behavior estimation from multi-source data for offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 9, pp. 11201-11209)."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825030508,
        "cdate": 1698825030508,
        "tmdate": 1699636869663,
        "mdate": 1699636869663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aBxpKT6Aji",
        "forum": "Zh047FhXqI",
        "replyto": "Zh047FhXqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_ENii"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_ENii"
        ],
        "content": {
            "summary": {
                "value": "In reinforcement learning, it is crucial to have an accurate environment dynamics model to evaluate different policies' value in tasks like offline policy optimization and policy evaluation. However, the learned model is known to have large value gaps when evaluating target policies different from data-collection policies. This issue has hindered the wide adoption of models as various policies are needed for evaluation in these downstream tasks. In this paper, the authors focus on one of the typical offline environment learning scenarios where the offline datasets is collected from diversified policies. They utilize an implicit multi-source nature in this scenario and propose an easy-to-implement yet effective algorithm, policy-conditioned model (PCM) learning, for accurate model learning. PCM is a meta-dynamics model that is trained to be aware of the evaluation policies and on-the-fly adjust the model to match the evaluation policies' state-action distribution to improve the prediction accuracy. They provide a theoretical analysis and experimental evidence to demonstrate the feasibility of reducing value gaps by adapting the dynamics model under different policies. Experiment results show that PCM ourperforms the existing SOTA off-policy evaluation in the DOPE benchmark with *a large margin*, and derives significantly better policies in offline policy selection and model prediction control compared with the standard model learning method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The environment model learning under the setting of dataset is collected from diversified behavior policies is an interesting topic and has positive contribution to the research community. This problem is actually common in practice considering that the large decision mdel often relies on the world model reconstructed from the diversed policy behaviors.\n2. The description of problem modeling and analysis is clear. The whole paper is generally well organized.\n3. The proposed solution in this paper is concise enough and it will be enlightening for subsequent research."
            },
            "weaknesses": {
                "value": "1. The universality of the dataset collected from diversified behavior policies has not been comprehensively elaborated which limits the meaning of this problem. \n2. The relationship between the proposed solution and the challenges in the setting of dataset collected from diversified behavior policies is not clarified well. It is still unknown or at least confusing the difference between the setting of the dataset collected from homogeneous behavior policies and the dataset collected from diversified behavior policies. And what are the additional challenges brought by this new setting and how the proposed approach help solve these challenges?\n3. Though the authors have paid much attention to how to adapt the learned environment model for various policies in different downstream tasks, how to overcome the challenges of the diverse upstream dataset is less covered in the paper.\n4. The experiments in this paper is still not sufficient enough. Though conducting experiments on different tasks and providing corresponding insightful analysis, the authors are suggested to introduce more environments and baselines to help demonstrate the applicability and superiority of the proposed method."
            },
            "questions": {
                "value": "1. The authors are encouraged to provide more visualizations to the representations of different learned policies by PCM in various environments.\n2. The authors are expected to provide more evidence and supporting materials on why choosing OPE, OPS, and MPC as the downstream tasks to validate the effectiveness of the proposed method.\nSee the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842441999,
        "cdate": 1698842441999,
        "tmdate": 1699636869568,
        "mdate": 1699636869568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6bwflWQcIX",
        "forum": "Zh047FhXqI",
        "replyto": "Zh047FhXqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_Ag2q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7281/Reviewer_Ag2q"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method called PCM to tackle the complex model learning process in RL for offline policy optimization and evaluation. When offline dataset is collected by a diverse set of behavior policies, it might be challenging to train a single dynamics model with tolerable error due to model capacity. To this end, PCM proposes to divide the dataset into subtasks associated with different behavior policies, and train policy-conditioned dyanamics model to improve the value gap."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes to reduce modeling error of the MDP dynamics by learning policy-conditioned models associated with different data-collecting behavior policies, which is connected to popular context-based meta-learning paradigms such as [1][2][3].\n\n2. The motivating example of \"Varied dynamics models for different policies\" in section 4.3 is interesting. \n\n3. The authors conduct a wide range of experiments to demonstrate the superiority of proposed PCM.\n\n[1] Efficient off-policy meta-reinforcement learning via probabilistic context variables.\n\n[2] Focal: Efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization.\n\n[3] Fast Adaptation to New Environments via Policy-Dynamics Value Functions."
            },
            "weaknesses": {
                "value": "1. **Limited technical contributions**.  The central idea of the paper is training policy-conditioned dynamics models instead of a single model on a diverse offline RL dataset to achieve better modeling error. However, the idea of using MDP-conditioned embeddings for training a universal model in deep RL is not new, see for example [9][10][11].\n\n2. **Gap between theory and experiments**: I also find the main theorem of the paper, Proposition 4.2 a relatively trivial result. On the RHS of (8), the training error minus the adaptation gain is precisely the model error on the LHS. It only shows that the generalization error can be bounded given a Lipshitz assumption, without explictly modeling the effect of policy embedding. The main argument seems to be that the adaptation gain of PCM is positive but zero for PAM. However, one can simply fine-tune PAM given the data of a new policy $\\pi$ to achieve a positive adaptation gain, which is also a stronger baseline should be compared in experiments in 5.3.1 (see point 3). Therefore, **it's not convincing enough why policy-conditioned PCM is superior given the theory**, which is supposed to be the main contribution of the paper. \n\n3. **Missing important model-based baselines**. As mentioned in 5.3.1, naive PAM significantly outperforms other competitive baselines of off-policy evaluation because the authors use better architectures than MLP. Thanks for being honest and transparent about this point. However, since the key contribution PCM is policy-conditioned design of dynamics model, I believe it's important to see whether PCM outperforms other competitive **model-based OPE baselines when using the same architecture**. A simple finetuned PAM on unseen task should also be compared as a stronger basline (see point 2).\n\n4. The review of related work of model-based offline RL is inadequate. In this submission, the related work only refers to those published before 2021 and there are more recently published works such as PMDB[1], VMG[2], RAMBO[3], COUNT-MORL[4], ROSMO[5], CABI[6], AMPL[7], CBOP[8]. I personally do not agree with the authors' perspective that MBORL are generally categorized into two groups: MPC and PL. For example, AMPL[7] focuses on the model learning process but can not be categorized into policy learning, CABI[6] focuses on the model data augmentation, RAMBO[3] focuses on the mini-max optimization about the offline dataset. Even, as far as I know, the MPC methods only covers a relatively small portion of the model-based offline RL methods. \n\n    Also, the setting of the paper seems to be more related to the so-called compositional/functional RL [12], which should be discussed and some of the baselines should be compared.\n\nMinor issues in presentation:\n\n1. What is $W$ in Proposition 4.2?\n\n2. In Fig 2.c, shouldn't the expected outcome look like a diagonal matrix, since evaluating on the same policy where the embedding is trained (diagonal entry) should give the best performance?\n\n[1] Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief\n\n[2] VALUE MEMORY GRAPH: A GRAPH-STRUCTURED WORLD MODEL FOR OFFLINE REINFORCEMENT LEARNING\n\n[3] Robust Adversarial Model-Based Offline Reinforcement Learning\n\n[4] Model-based Offline Reinforcement Learning with Count-based Conservatism\n\n[5] EFFICIENT OFFLINE POLICY OPTIMIZATION WITH A LEARNED MODEL\n\n[6] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination\n\n[7] A Unified Framework for Alternating Offline Model Training and Policy Learning\n\n[8] CONSERVATIVE BAYESIAN MODEL-BASED VALUE EXPANSION FOR OFFLINE POLICY OPTIMIZATION\n\n[9] Focal: Efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization\n\n[10] Fast adaptation to new environments via policy-dynamics value functions.\n\n[11] Universal Value Function Approximators.\n\n[12] A survey on continual learning and functional composition."
            },
            "questions": {
                "value": "See Weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7281/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843117820,
        "cdate": 1698843117820,
        "tmdate": 1699636869466,
        "mdate": 1699636869466,
        "license": "CC BY 4.0",
        "version": 2
    }
]