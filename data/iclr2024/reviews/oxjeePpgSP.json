[
    {
        "id": "cWMIQSweAY",
        "forum": "oxjeePpgSP",
        "replyto": "oxjeePpgSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_Tcds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_Tcds"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a poisoning-based backdoor attack for Contrastive Learning (CL), targeted at the feature extractor. Through the bi-level optimization that simulates a backdoored CL pre-trained model in the inner loop, it trains a trigger generator to produce poisoned samples with *robust* triggers that survive the data augmentation of CL. These triggers also exhibit transferability across serveral victim CL training strategies and backbone architectures. Experimental validation confirms the effectiveness, transferability and robustness of the attack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation behind the attack is well-described by incorporating the alignment and uniformity inherent in CL. The method for training the trigger generator is succinctly presented in an 11-line pseudocode.\n- The experiments are comprehensive, including experiments of CL backdoor defense and the transferability in three aspects (i.e., CL training strategies, model architectures and datasets).\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- **Lack of details.** The implementation of the proposed attack seems to lack some details:\n    - The setting of K and J are not included in the submission, and their relationship with N remains unclear. Is K large enough to ensure the convergence of the surrogate backdoored model?\n  - Additionally, does the x-axis in Figure 4 refer to N? If it includes the inner loop updates (N*J), does it make the comparison of loss curves somewhat unfair?\n  - What does the expression *regularly re-initialize the surrogate feature extractor* (in section 4) mean? Does it imply that, after the initialization (line2 in Algorithm1), there is a subsequent re-initialization at some point?\n- What determines superior transferable ability of the chosen surrogate CL framework (SimSiam)? The fundamental factors may need further analysis and clarification in ablation experiments, such as the choices of different data augmentations.\n- The BLTO procedure contains both a backdoor generator and a backdoored surrogate model \u03b8. Does the co-training surrogate backdoored model perform as well as an backdoored model actually trained on the poisoned data?"
            },
            "questions": {
                "value": "- In the evaluation on transferability, the adopted backbone encoders are all of CNN architecture (e.g., ResNet, MobileNet, ShuffleNet, SqueezeNet), and the datasets are just CIFAR-10 and CIFAR-100. More diverse choices of backbone architecture and dataset may be necessary, such as the architurecture of ViT and more challenging datasets like ImageNet.\n- Besides, though the proposed attack targets at the feature extractor, the victim settings in the experiments are limited to the classification task. I think it could be extended to more tasks to demonstrate the effectiveness of the proposed attack."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698111491086,
        "cdate": 1698111491086,
        "tmdate": 1699636825869,
        "mdate": 1699636825869,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PA3PM0lay1",
        "forum": "oxjeePpgSP",
        "replyto": "oxjeePpgSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_KWn8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_KWn8"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies that the current data poisoning-based backdoor attacks on contrastive learning adopt a fixed trigger design and have a limited attack success rate. To overcome this limitation, a novel bi-level optimization approach is proposed. In this framework, the inner optimization simulates the contrastive learning (CL) dynamics of a surrogate victim, while the outer optimization optimizes the trigger generator and ensures that the backdoor trigger remains close to the target throughout the surrogate CL procedure. Extensive experiments are conducted to compare the proposed methods with state-of-the-art (SOTA) attacks, such as SSL backdoor and CTRL, demonstrating superior attack effectiveness. Furthermore, the proposed methods can effectively evade existing SOTA defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed attack method is novel and shows superior effectiveness in comparison with the SOTA.\n\n2. The experiments are comprehensive; the authors compare the proposed method with different attack methods, evaluate it against backdoor defenses, and also discuss the effect of various data augmentations.\n\n3. The overall writing is good. The methodology and experimental results are not difficult to comprehend."
            },
            "weaknesses": {
                "value": "1. The motivation could be better articulated. The authors claim that the fixed trigger design leads to limited ASR. However, in the methodology, not only is the trigger generator adopted, but a bi-level optimization strategy is also used to optimize the trigger generator. This raises the question: Is the trigger generator alone sufficient for the success of the proposed attack? If not, it suggests that the fixed trigger is not the primary cause of the current limitation. I recommend that the authors conduct an ablation study on this matter and be cautious with their claims.\n\n2. In the experiments, only SimSiam is adopted as the surrogate Contrastive Learning (CL)  method. The experimental results demonstrate that selecting this framework indeed achieves good performance, but it does not provide a direct rationale for choosing SimSiam. It is possible that using SimCLR or BYOL could yield better results, and it is recommended to supplement this part with additional experiments for verification.\n\n3. The comparison with other recently developed works could enhance the contribution:\n(1) PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning.\n(2) CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.\nNotably, in \"PoisonedEncoder,\" a bi-level optimization strategy is also employed to formulate the attack. How does this work differ from theirs?"
            },
            "questions": {
                "value": "1. Is the trigger generator alone sufficient for the success of the proposed attack?\n\n2. How does this work differ from the \"PoisonedEncoder\"?\n\n3. If the reference data $x_r$ is randomly sampled from the target class? does it need to be included in the downstream dataset to ensure the success of the attack?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698304601070,
        "cdate": 1698304601070,
        "tmdate": 1699636825762,
        "mdate": 1699636825762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fm9kIBjqx4",
        "forum": "oxjeePpgSP",
        "replyto": "oxjeePpgSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
        ],
        "content": {
            "summary": {
                "value": "This paper formulates backdooring contrastive learning (CL) as a bi-level trigger encoder optimization problem. They claim that existing attacks using fixed triggers fail to maintain similarity between triggered data and target class in CL's embedding space due to data augmentation and uniformity effect, limiting their success. The proposed method formulates a bi-level optimization that simulates the victim's CL training dynamics in the inner level and optimizes a trigger generator in the outer level to keep triggered data close to the target throughout the inner CL training. This results in resilient triggers that can survive CL mechanisms. Experiments show the attack achieves success under varying victim settings and defenses. Analyses are provided on how CL mechanisms affect attack performance. The optimized triggers capture semantics related to the original input, explaining the attack's effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work provides a formulation of the backdoor problem in contrastive learning as a bi-level optimization to identify a backdoor generator that is able to generate triggered images.\n\n- The authors provided an approximated solution to the formulated bi-level optimization.\n\n- The authors evaluated three datasets and compared them with two existing attacks. The attack is further evaluated with existing defenses from two lines of work (model-based backdoor trigger detection and model-based backdoor mitigation).\n\n- The writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The paper lacks analysis or discuss on the impact of using more accurate Hessian approximations to solve the proposed bi-level optimization, relying only on a discrete solution.\n\n- The baseline implementations and results seem questionable based on inconsistencies with original papers and recent related work- the attack success rates for SSL backdoor and CTRL differ notably from prior reported values.\n\n- The related work review and experiment scope is too narrow:\n   1. Recent attacks using similar target-class-based trigger synthesis are not compared to.\n   2. A relevant defense for detecting backdoor samples in contrastive learning is not discussed."
            },
            "questions": {
                "value": "The proposed bi-level optimization formulation is solved using a discrete approximation without much discussion on the impact of using more efficient yet accurate Hessian approximations or evaluates the convergence of the proposed solution. Bi-level optimization often benefits from analyzing such approximations rather than directly providing a discretized solution.\n\nThe evaluation results comparing against baselines may contain erroneous implementations. In particular, the reported attack success rate (ASR) for SSL backdoor differs notably from values in the original paper, which because the low efficacy of their work, they even used a different metric based on number of misclassified samples. Also, the ASR for CTRL is much lower than results from recent works (e.g., the original paper and [1]) that show CTRL can achieve above 80% ASR with SimCLR on CIFAR-10, contrasting the authors' significantly lower values. Additional to the original papers, another separated work [1] evaluated these two attacks also confirms the potential erroneous implementations in this work.\n\nThe related work could be expanded and compared more thoroughly. Some recent attacks [2] also leverage synthesized triggers using solely the target class, similar to the proposed approach, which is worth to be incorporated and compare. Additionally, a recent backdoor sample detection method [1] demonstrates effectiveness in detecting poisoned samples in contrastive learning unlabeled datasets, which is relevant but not discussed or evaluated.\n\n[1] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" Usenix Security (2023).\n\n[2] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\" ACM CCS (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The results of implementation of related work (SSL backdoor and the CTRL) seems largely different than what has been reported in the original papers and recent work [1]. Not quiet sure if this requires a flag, just to make sure.\n\n[1] Pan, Minzhou, et al. \"ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.\" Usenix Security (2023)."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_wKXh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709389369,
        "cdate": 1698709389369,
        "tmdate": 1700864786625,
        "mdate": 1700864786625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cRIefile1Q",
        "forum": "oxjeePpgSP",
        "replyto": "oxjeePpgSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to perform poisoning-based\nbackdoor attacks on contrastive learning. It searches\noptimal triggers by using a designed bi-level optimization\napproach on the surrogate models. Experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-100) validate\nthe effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Backdoor attacks on contrastive learning is an important direction.\n\n* Analysis of the reason that the proposed attack works\nwell is discussed in section 5.4."
            },
            "weaknesses": {
                "value": "* The proposed method requires a surrogate to perform\nbi-level optimization. This paper mentions it uses SimSiam\nwith ResNet18 as the surrogate model. The success of the\nproposed method is based on the transferability of the\ntriggers optimized on the surrogate model. While Table 1 and\n2 demonstrate the proposed method has good transferability,\nthe evaluation is not comprehensive. It is suggested to\ninclude more self-supervised learning methods such as Jigsaw\n[1], MoCoV2 [2], and DINO [3]. For different model\narchitectures, the results under modern architectures such\nas ViT and RegNetY are missing (Note that ViT and RegNetY\nare commonly used in self-supervised learning related\nresearches like\nhttps://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md).\nSince the transferability is very important to the\nperformance of the method, it is suggested to conduct more\nextensive experiments. For example, adding the results under\ndifferent architectures to Table 1 or adding the results\nunder different CL methods to Table 2 (So that it will have\na Table including the results under different combinations\nof architectures and CL methods). \n\n* The hyper-parameters such as the learning rates of the\nmodels might also influence the performance of the proposed\nmethod. For example, if the learning rates and the batch\nsizes of the surrogate model are significantly different from\nthose used by the victim models, then the attack success\nrates might also reduced. It is suggested to add the\ndiscussion about this.\n\n* In the experiments, this paper assumes that the downstream\ndataset and the pre-training dataset used for self-supervised\nlearning is the same. Typically, the downstream users will\nuse different datasets to conduct the downstream training.\nMany existing works such as BadEncoder [4] also mainly\ninvestigate this practical scenario. The results under this\npractical scenario are missing in this paper.\n\n* Carlini et al. [5] is an important existing work in the\nfield of poisoning-based backdoor attacks on contrastive\nlearning. Although it mainly focuses on the vision-language\ncontrastive learning, the comparisons, and the discussion\nabout it is still important.\n\n* The robustness under Feng et al. [6] is not discussed. Is the\nbackdoor samples in the proposed method have high cosine\nsimilarity between each other?\n\n* The usages of the surrogate models and the bi-level\noptimization is not new in the field of poisoning attacks\nand backdoor attacks [7,8], which somehow weaken the\ncontributions of this paper.\n\n\n\n[1] Noroozi et al., Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. ECCV 2016.\n\n[2] Chen et al., Improved Baselines with Momentum Contrastive Learning. arXiv 2020.\n\n[3] Caron et al., Emerging Properties in Self-Supervised Vision Transformers. ICCV 2021.\n\n[4] Jia et al., BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. IEEE S&P 2022.\n\n[5] Carlini et al., Poisoning and Backdooring Contrastive Learning. ICLR 2022.\n\n[6] Feng et al., Detecting Backdoors in Pre-trained Encoders. CVPR 2023.\n\n[7] Souri et al., Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch. NeurIPS 2022.\n\n[8] Zhu et al., Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. arXiv 2023."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7033/Reviewer_Saos"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986191713,
        "cdate": 1698986191713,
        "tmdate": 1700732649343,
        "mdate": 1700732649343,
        "license": "CC BY 4.0",
        "version": 2
    }
]