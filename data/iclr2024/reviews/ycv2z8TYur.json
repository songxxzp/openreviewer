[
    {
        "id": "GOhS7ozTb6",
        "forum": "ycv2z8TYur",
        "replyto": "ycv2z8TYur",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_fQX6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_fQX6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes EmerNeRF to decompose a scene into static and dynamic components by learning from videos using a self-supervised manner. The authors further propose to lift visual features from foundation models into the 4D space, by learning a shared PE feature map and PE-free volumetric feature fields. The authors also construct a new benchmark by subsampling video sequences from the Waymo Open Dataset. The proposed method achieves good results on several tasks, including scene reconstruction, view synthesis, and occupancy prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is intuitive, simple yet effective, achieving promising results on several tasks\n2. I like the study of PE patterns in vision foundation models and the solution to it. The visualization in Figure 1 is also good."
            },
            "weaknesses": {
                "value": "**1. Method**\n- First of all, the authors claim \u201cno prior works in this field have explicitly modeled temporal correspondence for dynamic objects\u201d, which is wrong. Some related works are missing here [A1-3].\n- It seems to me that the formulation of Gao et al. [A2] is very similar to the proposed method. Except Gao et al. [A2] used: (1) MLP-based NeRF instead of hash grids; (2) separate RGB heads for static and dynamic branches; (3) Not using a sky branch. However, Gao et al. [A2] showed that their method would fail without the optical flow regularization. So I wonder what is the core reason that the proposed method can work while Gao et al. [A2] cannot. Is it due to the hash grid? Is it due to ground truth depth supervision (from lidar)? Is it due to the dataset being evaluated (multiview-view, vehicle motions are rigid and thus simpler)? Or is it due to the evaluation protocol (relatively easy to do view synthesis for frame interpolation)? While I prefer such a simple method, the reason that makes it work remains unclear to me.\n\n**2. Experiments**\n- For the results in Table 1 and Table 2, is visual feature distillation being used?\n- Is it possible to show to number of parameters in Table 1?\n- The comparisons with baselines in Table 1 also seems problematic. First, the authors project lidar points to image and use a L2 loss for depth regularization for HyperNeRF and D2NeRF, which is definitely worse than directly regularizing on lidar rays due to many reasons (e.g., projection error, possible occlusions). While the authors claim that neither of these two baselines support lidar ray sampling, it is actually not difficult to incorporate this functionality. An easier way is to use the same way to regularize depth in the proposed framework. Second, StreetSuRF has its own normal supervision and lidar ray preprocessing. Simply disabling them and use the same one as in the proposed method does not seem correct to me.\n- While not necessary, would be good to see a comparison on the scene flow estimation task with a SOTA method [A4].\n\n\n**Summary**\n\nMy main concern is that I don\u2019t know what is the core reason that makes the proposed work. It would be great if the authors can conduct more extensive ablation analysis or conduct experiments on other datasets (especially those monocular ones used for dynamic view synthesis). Also, I would expect to see more reasonable comparisons with the baseline methods. The authors should also incorporate the missing literature and discussion accordingly.\n\n\n**Reference**\n- [A1] Wang et al. \u201cNeural Trajectory Fields for Dynamic Novel View Synthesis\u201d\n- [A2] Gao et al. \u201c\u200b\u200bDynamic View Synthesis from Dynamic Monocular Video\u201d. ICCV 2021\n- [A3] Wang et al. \u201cFlow supervision for Deformable NeRF\u201d. ICCV 2023\n- [A4] Li et al. \u201cFast Neural Scene Flow\u201d. ICCV 2023"
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646371432,
        "cdate": 1698646371432,
        "tmdate": 1699636457218,
        "mdate": 1699636457218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4udIVprYis",
        "forum": "ycv2z8TYur",
        "replyto": "ycv2z8TYur",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_hxFT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_hxFT"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a NeRF based method for learning scene representation. By decomposing the loss function into two separate terms - one handling static elements of the scene and the other dynamic elements the model is able to separate the static and dynamic components with no extra supervision. The model also estimates scene flow in the process as part of the regularization. Finally the method is also applied to \"lifting\" 2D self-supervised representations (e.g. DINO) into 4D space by a combination of readout heads and a learned positional embedding which rectifies some of the issues associated with these SSL representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a nice combination of ideas that have been floating around for a while, leveraging the strengths of different approaches in an appealing and relatively elegant way. \nThe paper is well presented, well executed and results are impressive all in all (but see below) - this is a good paper."
            },
            "weaknesses": {
                "value": "I think there are a couple of weaknesses that may require addressing\n\n* Ablation analysis is lacking - table C1 addresses some of the of the modeling decisions but there are more aspects I would have loved to see analyzed.\n* Applicability - I may be wrong, but I feel the use of driving data together with a NeRF based method (that is, need to train on each scene separately) is a bit odd - usually in this use case one would want an online inference model (e.g. an encoder) which can infer elements and structure quickly as the car/robot drives around the scene. On the other hand I feel this is not widely applicable to other types of data (say free form natural scenes with lots of unstructured movement).\n* There is a lot of focus on DINO/v2 features and their related issues - I am wondering if other SSL methods suffer from similar PE issues (specifically ones that handle motion such VideoMAE / Siamese MAE etc.)"
            },
            "questions": {
                "value": "In relation to the above:\n\n* How much dependance is there on LiDAR data? would this method work without direct depth supervision signals?\n* It's not written explicitly in the paper - does the data include ground truth camera parameters? (extrinsic and intrinsic, I imagine the answer is yes)\n* The dynamics regularization term is a bit simplistic as it is a simple minimization of total density for dynamic elements - have you tried other regularization methods? say assuming sparsity and so on?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674135284,
        "cdate": 1698674135284,
        "tmdate": 1699636457127,
        "mdate": 1699636457127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "64S20ma0B1",
        "forum": "ycv2z8TYur",
        "replyto": "ycv2z8TYur",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_BkKZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_BkKZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a neural field approach that can perform static-dynamic decomposition and scene flow estimation in challenging autonomous driving scenarios. A hybrid 4D scene representation consisting of static, dynamic, and flow fields is adopted and jointly trained with the goal of appearance and feature reconstruction. Experimentation reveals state-of-the-art performance in novel view synthesis and scene flow estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes an effective way of jointly modeling the static and dynamic scenes in the setting of autonomous driving. The method is technically sound, from the high-level idea of using hybrid representation for static and dynamic scenes and optimize them under the goal of appearance reconstruction to the details of carefully modeling sky and shadows in the framework.\n\nEven though there is no direct flow supervision or well-adopted supervision such as flow based warping, it's quite novel to see the scene flow estimation \"emerges\" from the temporal aggregation of features for scene reconstruction.\n\nThe experimentation is thorough and the numerical improvements over baselines are obvious.\n\nThe writing and presentation of this paper are also pretty good and easy-to-follow."
            },
            "weaknesses": {
                "value": "It's good to see more visualization of the model output in the appendix. But it would also be good to include more qualitative comparisons against the baselines in addition to the quantitative results. Also, adding error maps would be more intuitive to highlight the difference.\n\nThere also seems to miss the runtime analysis and comparison. How long does this approach take for sensor simulation in training and test time, and how does it compare to the existing approaches? That would be an important piece of information."
            },
            "questions": {
                "value": "The experiment setup omit every 10th frame, resulting in 10% novel views for evaluation. It would be better to include ablation study on the sparsity of the sampling and how the method degrade with fewer training frames."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733562030,
        "cdate": 1698733562030,
        "tmdate": 1699636457043,
        "mdate": 1699636457043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f9U1AShxjV",
        "forum": "ycv2z8TYur",
        "replyto": "ycv2z8TYur",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_qWy5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4749/Reviewer_qWy5"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces EmerNeRF, an approach for learning spatial-temporal representations of dynamic scenes. EmerNeRF decomposes scenes into static and dynamic fields, with an additional scene flow field modeling the movement of objects across time. The dynamic feature is computed as a weighted sum of features from nearby timesteps, where the sampling operation is determined by the self-supervised scene flow field. Additionally, the paper proposes a method to generate positional encoding (PE) free features from a pretrained feature encoder by leveraging the time-consistent property of PE features. The empirical result indicates the proposed method can achieve better novel view synthesis, flow estimation, and few-shot semantic prediction results compared to the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed dataset holds potential value for research in dynamic scene reconstruction field\n- EmerNeRF demonstrates better reconstruction quality in driving scene dataset compared to the baselines\n- The obtained scene flow exhibits high accuracy compared to baseline method"
            },
            "weaknesses": {
                "value": "- Novelty. Many design components of EMerNeRF have been proposed in previous work, including separated static and dynamic fields [2][3], sky head [1], shadow head [2], flow field [4]. The paper lacks a detailed discussion highlighting the differences compared to these previous works.\n- Generalizability. The proposed method lacks verification in existing dynamic scene datasets used in baselines such as Nerfies [5], HyperNeRF [6]. Those datasets contain more complex deformations and a significant proportion of dynamic components. It is necessary to evaluate the robustness and understand the limitations of the proposed design modules, including dynamic density regularization and self-supervised flow field.\n- Ambiguity of equation 6. The meaning and formulation of the expectation of the dynamic density remain unclear.\n- Baseline. The quantitative evaluation lacks a more recent baseline [7].\n\n[1] Konstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 12932\u201312942, 2022\n\n[2] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u02c62nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. *Advances in Neural Information Processing Systems*, 35:32653\u201332666, 2022.\n\n[3] Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 7210-7219. 2021.\n\n[4] Du, Yilun, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, and Jiajun Wu. \"Neural radiance flow for 4d view synthesis and video processing.\" In\u00a0*2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 14304-14314. IEEE Computer Society, 2021.\n\n[5] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 5865\u20135874, 2021a.\n\n[6] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. *arXiv preprint arXiv:2106.13228*, 2021b.\n\n[7] Cao, Ang, and Justin Johnson. \"Hexplane: A fast representation for dynamic scenes.\" In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 130-141. 2023."
            },
            "questions": {
                "value": "- What\u2019s the novel components that EmerNeRF propses?\n- How does EmerNeRF perform in more general dynamic scenes?\n- How does EmerNeRF compare to recent newer baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828939733,
        "cdate": 1698828939733,
        "tmdate": 1699636456955,
        "mdate": 1699636456955,
        "license": "CC BY 4.0",
        "version": 2
    }
]