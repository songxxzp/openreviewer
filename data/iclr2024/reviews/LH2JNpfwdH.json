[
    {
        "id": "wP5fUf0zaq",
        "forum": "LH2JNpfwdH",
        "replyto": "LH2JNpfwdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_smP6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_smP6"
        ],
        "content": {
            "summary": {
                "value": "This paper desribes a NeRF based video stylization methods. The authors propose to split human action and background scenes into two different NeRF to synthesis novel views and a novel geometry-guided tri-plane representation to enhanced feature representation robustness and synthesis quality. The stylization is then performed within the NeRFs\u2019 rendered feature space. Both qualitative and quantitative experiments have been conducted to demonstrate that the proposed method outperforms existing approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.By spliting human gesture and background scene into two different NeRF branches, the proposed method can effective synthesis novel views and animated human.\n2.The proposed geometry-guided tri-plane representation can effectively stablize image synthesis and improve result quality.\n3.Compared to previous methods, the generated samples have better visual quality"
            },
            "weaknesses": {
                "value": "1.While there are plenty of video stylizatio works, in the experiment section, the authors only compare the proposed method with 3 exsisting methods, which seems to be not adequate.\n\n2.In both qualitative comparison (table2) and user study (Figure 5), the advantage of proposed method over previous seems not very obvious. Especially in user study, the proposed method only have obvious advantage over CCPL but received similar evaluation compared to LST and AdaAttN\n\n3.The authors are putting \"4D\" in the title, trying to emphesize the contribution on video stylizaiton. Personally I don't think it's proper to regard the video stylization as a main contribution of this paper, as the proposed method directly apply stylization on the NeRF rendered feature space and doesn't have any mechanism for inter-frame stablizaiton. Also, the novel view synthesis / human animation function and stylization seems to have little connection."
            },
            "questions": {
                "value": "The stylization is directly applied on the NeRF rendered feature space, and seems to have littile connection to the proposed geometry-guided tri-plane representation and two branch NeRF for human and background scene. Also, in the METHODOLOGY section, the authors are repeatedly mentioning that many components of the framework are motived by previous methods. How could authors persuade reviewers the proposed the framework is innocative instead of an incremental work with the combination of previous works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Reviewer_smP6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698235494865,
        "cdate": 1698235494865,
        "tmdate": 1699636332695,
        "mdate": 1699636332695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cZp0Yri1Cy",
        "forum": "LH2JNpfwdH",
        "replyto": "LH2JNpfwdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_uFnH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_uFnH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for human video stylization that leverages 2 NeRFs to represent backgrounds and the human body separately. \nIt introduces a geometry-guided tri-plane representation to learn the 3D scene more efficiently and effectively. The proposed method can accommodate novel poses and viewpoints, making it a versatile tool for creative human video stylization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a novel method for human video stylization that leverages 2 NeRFs to represent backgrounds and the human body separately. \nThe proposed geometry-guided tri-plane representation enhances feature representation robustness compared to direct tri-plane optimization by introducing a geometric prior on the tri-plane. This geometric prior is achieved by discretizing the 3D space as a volume and dividing it into small voxels, with sizes of 10 mm \u00d7 10 mm \u00d7 10 mm. Voxel coordinates transformed by the positional encoding are mapped onto three planes to serve as the input to the tri-plane."
            },
            "weaknesses": {
                "value": "The proposed contributions are trivial besides the two Nerf ideas.\nThe AdaAttN, loss functions are all borrowed from existing methods.\nThe demo results didn't show large angles of novel views from the backgrounds."
            },
            "questions": {
                "value": "Have you thought of using DeamFusion-like models to generalize on the background scene generation? Maybe this can help with adding information to your 3D scene."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3764/Reviewer_uFnH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762006722,
        "cdate": 1698762006722,
        "tmdate": 1699636332603,
        "mdate": 1699636332603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8hZ4tyAttN",
        "forum": "LH2JNpfwdH",
        "replyto": "LH2JNpfwdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_xdnP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3764/Reviewer_xdnP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes editing videos using the 4D NeRF representation. The video is firstly reconstructed as the static background NeRF plus a Neural Avatar. For style transfer, it uses the feature from the NeRF backbone and projects it to the stylized space.  With this canonical representation, it can generate consistent results when editing viewpoint, human pose, and style."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The overall idea is technically sound. Using NeRF-like canonical representation can solve video editing problems in a unified framework."
            },
            "weaknesses": {
                "value": "(1) Limited technical novelty. The NeRF reconstruction part is almost identical to NeuMan except that it uses a tri-plane representation, which has also been exploited widely. This choice is straightforward. The style transfer part follows AdaAttn without any modification. So the main paper is a straightforward combination of two existing works.\n\n\n(2). The overall visual quality is low, as shown in the paper and supplement material. Editing viewpoint and human pose is not new as this part is almost identical to the NeuMan.  But for stylization, compared to existing baselines, the qualitative experiments are not enough, just less than 5 style transfer results are shown. \n\n(3). The comparison to existing baselines is not fair. This paper stylizes foreground and background separately while baseline methods are applied in a foreground-agnostic manner. I believe it is easy to extend existing work to be segmentation-aware. \n\n(4). Lacking comparison with other video editing baselines which also exploited a layered and canonical representation. e.g.,\nLayered Neural Atlases for Consistent Video Editing, SIGGRAPH Asia 2021,\nThis baseline can be easily extended using AdaAttn by optimizing the canonical atlas.\n\n(5) Minor wriring issues\nIn Equ.5. \\mathcal{E}(x) should also depends on frame indices."
            },
            "questions": {
                "value": "Please address the weakness (1),(2),(3) and optionally (4)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829560842,
        "cdate": 1698829560842,
        "tmdate": 1699636332497,
        "mdate": 1699636332497,
        "license": "CC BY 4.0",
        "version": 2
    }
]