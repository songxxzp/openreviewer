[
    {
        "id": "kiTFzv2pN7",
        "forum": "3cuJwmPxXj",
        "replyto": "3cuJwmPxXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_wfVE"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a type of interventional extrapolation which consists in predicting the effect of an unseen intervention. The causal model considered is given by A -> Z -> X and Z->Y where Z is latent, A -> Z is linear, Z->X is invertible and Z -> Y is an additive noise model. The goal is then to estimate E[Y | do(A = a*)] where a* is outside its training domain. The authors separate this problem in three subproblems: 1) estimation given that Z is observed, 2) estimation this given that Z is known only up to an affine transformation, and 3) how to identify the invertible map Z->X up to an affine transformation. These identifiability results are then transferred into a practical algorithm: a) The mixing function is recovered by performing SGD on a loss consisting of a reconstruction term and a regularizer based on maximum moment restriction (MMR), and b) the learned encoder is then used to estimate the desired \u201cdo\u201d expectation following the procedure of step 2) above.  The proposed estimation procedure is then validated on a synthetic dataset.\n\nReview Summary: I gave the main text a thorough reading and checked the large majority of the math it presents. Despite the few problems I raised in my review, I believe this work is sufficiently novel, interesting, rigorous and well written to warrant acceptance to ICLR. I was planning to give a score of 7, since this score is not available, I'm rounding up to 8. I'm giving this score assuming that a discussion of the limitations will be added, as I suggest in my review."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- I agree that we need more theory to demonstrate concrete benefits of identifiable representation learning. I enjoyed this perspective.\n- The paper is very clearly written and structured and relatively easy to follow despite its technicality. The level of mathematical rigor in this work is very high in my opinion and the notation is always very crisp and transparent. \n- I thought the theoretical contributions were novel and interesting. \n- I appreciated that the main ideas about the proofs were provided in the main text. Most works in this literature only give very short proof sketches if any. \n- Theoretical results are well modularized to facilitate reuse.\n- I thought the method to learn the encoder up to affine transformation was novel and interesting.\n- I\u2019m left with a good feeling of having learned something new."
            },
            "weaknesses": {
                "value": "- The limitations of the proposed approach could be discussed further. For example, how does the method perform under various assumption violations? A few experiments could provide some insight into this. \n- The paper ends a bit abruptly. There\u2019s no conclusion nor discussion of limitations. I guess this is the cost of having so much technical details in the main text (which, as I said, I appreciated, but I\u2019m unsure whether this is a good balance.)\n- I think the paper could do a better job of contrasting its results with what already appears in the literature, especially regarding the affine identifiability results. How does this compare to iVAE for example, which has a very similar data generating process with an auxiliary variable (which would be A here)? A few papers have similar results: [1,2,3,4].\n- I couldn\u2019t grasp a few reasoning steps in the main text (see Questions below).\n\nMinor:\n- $M_0$ has full row rank implies that dim(A) >=  dim(Z). Might be worth making explicit.\n- I always forget which noise variable U or V is associated with which variable. How about replacing V by V_z and U by V_y?  \n- Footnote one: what is $\\mathcal{B}$?\n- The introduction rightfully points out that the literature has almost no works giving theoretical arguments for why the identifiable representation learning is important. You might want to consider citing [1, 5] as examples of work pushing that direction.\n\n[1] S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, and Q. Bertrand. Synergies between disentanglement and sparsity: a multi-task learning perspective, 2022. \n\n[2] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear \u00a8 ica: A unifying framework. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.\n\n[3] Ahuja, K., Mahajan, D., Syrgkanis, V., and Mitliagkas, I. Towards efficient representation identification in supervised learning. In First Conference on Causal Learning and Reasoning, 2022\n\n[4] Roeder, G., Metz, L., and Kingma, D. P. On linear identifiability of learned representations. In Proceedings of the 38th International Conference on Machine Learning, 2021.\n\n[5] S. Lachapelle, D. Mahajan, I. Mitliagkas, and S. Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. In Advances in Neural Information Processing Systems, 2023b."
            },
            "questions": {
                "value": "- Lemma 7 in Appendix A: Could you explain the argument a bit more? I\u2019m unaware of this proof technique, so citing the result used here would be useful. Or is it an alternative definition of conditional independence that I\u2019m not aware of?\n- Paragraph after (3):\n    - Could you give some insight as to why $\\ell$ and $\\lambda$ are not identifiable up to additive constant without further assumptions in (3)? I\u2019m not sure I see what could go wrong since we observe Y, Z and V here (V is identifiable). Also, can you state the assumption from Newey et al. (1999) that allows you to identify $\\ell$? I suspect it has something to do with the differentiability + supp(A) convex assumption from Theorem 4?\n    - It is written \u201cfor all $a^* \\in M_0 supp(A) + supp(V)$\u201d somewhere, but it seems wrong since $M_0 supp(A) + supp(V)$ is an event for Z, no? I\u2019m not sure I\u2019m following this part of the argument and onward.\n    - Same paragraph: I think we need to compute $\\ell(z)$ only for $z \\in M_0\\mathcal{A} + supp(V)$, no? \n- Eq. (16): I\u2019m not sure why this holds. Is it because the inside of the conditional expectation is equal to V? But we don\u2019t assume E[V] = 0 right? What am I missing? Since this is so crucial to the algorithm, it might be worth expliciting further."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697828486778,
        "cdate": 1697828486778,
        "tmdate": 1699636874383,
        "mdate": 1699636874383,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UwIWJzEqfK",
        "forum": "3cuJwmPxXj",
        "replyto": "3cuJwmPxXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses identification strategies for effect extrapolation when the treatment of interest is unobserved during the experiment. The key assumptions are that \n\n- (exogenous) treatment A affects outcome Y through and only through an unobserved mediator Z\n- can observe a rich feature $X=g_0(Z)$ that is and only is a (injective) function of Z\n- the relationship between A and Z is linear\n\nTogether with some other regularity assumptions, the author shows that, given an encoder that aff-identifies $g_0^{-1}$, it is possible to identify $E[Y|do(A=a^*)]$ and $E[Y|X=x,do(A=a^*)]$ for a treatment $a^*$ that has never been observed. They also propose a method for locating such an encoder."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper studies intervention extrapolation under a well-chosen set of assumptions, which I find more appealing than the previously studied scenarios in the literature.\n\n- To the best of my knowledge, the proposed identification strategy is novel.\n\n- The manuscript is clear and well-written. \n\n- The proposed algorithm is straightforward and practical."
            },
            "weaknesses": {
                "value": "- The assumptions are clear mathematically but might seem opaque to readers unfamiliar with the literature. The authors may want to give an example of what some of the key assumptions would imply in a simple setup.\n\n- Many of the structural assumptions are not testable, and it is unclear to me when one shall be comfortable using the proposed method.\n\nAlso see questions."
            },
            "questions": {
                "value": "- It seems that this approach relies heavily on the linear structure between A and Z. Can this structural model be extended to $M_0 t(A) + V$ for some transformation $t(A)$ (e.g., $A^2$)? What if the relationship between A and Z is not linear, but rather through a known class of models $m_{\\theta}(A)+V$ with parameters $\\theta$? How would this affect the result?\n\n- Is it possible to view the linear assumption as an approximation through a Taylor expansion around supp(A)? How would this compare with an extrapolation that is solely based on Lipschitz assumptions (see, e.g., Ben-Michael et al. 2021)?\n\n- Although this paper is only about identification, I am curious about how the errors would accumulate.\n\n- I find the sudden change of notation in Theorem 4 confusing.\n\n- Can there be randomness in X, i.e., can X be a noisy observation of $g_0(Z)$?\n\n- On page 2, the authors wrote \"we allow for potentially unobserved confounders between Y and Z\". How could there be confounding when the action is exogenous?\n\n- Although the goal is very different, the approach reminds me of the negative control literature. The authors may want to discuss the connection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_dGv9",
                    "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739611720,
        "cdate": 1698739611720,
        "tmdate": 1700658846876,
        "mdate": 1700658846876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jSbH3o3hzP",
        "forum": "3cuJwmPxXj",
        "replyto": "3cuJwmPxXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA"
        ],
        "content": {
            "summary": {
                "value": "**Post rebuttal update**: I'd really like to see a couple of real-world examples that might satisfy the assumptions, but my other concerns are largely addressed. I raise my score to 8 accordingly.\n\nThe paper considers the causal effect of an intervention $A=a^*$, where value $a^*$ is not observed in the data; A affects the outcome Y through unobserved variable Z, and covariates X relates to Z through an injective function g. The paper uses a conditional moment restriction (CMR) implemented by a kernel method called maximum moment restriction (MMR), and it then uses the control function (CF) approach to identify the outcome function between Z and Y. Both the CMR and CF depend on the assumption that A affects Z linearly. The approach is theoretically guaranteed, and experiments on synthetic datasets support the theoretical analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Using CF to achieve intervention extrapolation is a nice idea.\n\nThe theoretical analysis is serious and detailed (but I did not check the proofs in Appendix).\n\nThe paper is quite well written."
            },
            "weaknesses": {
                "value": "*Technical novelties seem to be weak*. Theorem 4 seems to be an adaptation of the CF approach in (Newey et al., 1999), and Theorem 6 seems to be an adaptation of the IV approach in (D\u2019Haultfoeuille, 2011). If there are some technical novelties, they should be discussed and compared to the original works; otherwise, I suggest being more explicit about this weakness.\n\n*Some assumptions are strong*; particularly, the linear model between Z and A, and the injective model and noiseless model between Z and X. Intuitively, X is an observable proxy of the hidden Z, and the assumption means there is no information loss in this proxy, which is strong. Moreover,  both assumptions involve hidden variable Z and add difficulty to practical judgments. Since both assumptions are inherent in the current approach, I do not expect the author(s) to address this weakness in the rebuttal, but the following could certainly be done.\n\n*Discussion of the setting and comparison to related work*. The discussion of the relationship to reinforcement learning is interesting but does not touch on when can we possibly expect linearity and noiseless injectivity. It would be more interesting to draw and discuss a couple of real-world problems that might satisfy the assumptions. \n\nOn the other hand, (Khemakhem et al., 2020) and [1, 2], which are based on the former, are important related work that needs more discussions, and this would clarify the current approach.  For example, (Khemakhem et al., 2020) recover Z based on exactly the same graph as A\u2192Z\u2192X, and also assume g is injective but *allows an additive noise on X*; the identification also relies on assumptions on the A\u2192Z part, where p(Z|A) is assumed to be an exponential family distribution but *allows nonlinearity*. Overall, I do not think the assumptions in (Khemakhem et al., 2020) are clearly stronger than those in the current work. Further, [1, 2] uses (Khemakhem et al., 2020) to estimate treatment effects, though not considering intervention extrapolation, [2] mentioned the ideas of CMR and CF in Sec 4.4.\n\n[1] Wu, Pengzhou Abel, and Kenji Fukumizu. \"$\\beta $-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap.\" International Conference on Learning Representations. 2022.\n[2] Wu, Pengzhou and Kenji Fukumizu. Towards Principled Causal Effect Estimation by Deep Identifiable Models\u201d. In: arXiv preprint arXiv:2109.15062 2021\n\n*Additional experiments could be added*.\n\nI think the ability to deal with unobserved confounders is a strength of the work. So, why not add experiments on this? I know eq24 contains hidden confounding, but this direction is not examined, e.g., by adjusting the strength of confounding and comparing to other methods.\n\nAs indicated above, adding real-world problems in experiments can greatly strengthen the work. Also, it would be interesting to replace the CMR part with iVAE (Khemakhem et al., 2020) and see how the results would change.\n\nI will read the rebuttal and revised paper and raise my score to 8 if the issues/questions above are addressed."
            },
            "questions": {
                "value": "I cannot understand the importance of Proposition 3, and it seems just a trivial restatement of Def 2 and adds confusion to me.\n\nI think the title would be better to stress the CF approach because this arguably contributes more to intervention extrapolation than \u201cidentifying representation\u201d.\n\nIt is weird that Wiener\u2019s Tauberian theorem is mentioned in the Abstract but not the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_k2zA",
                    "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768490297,
        "cdate": 1698768490297,
        "tmdate": 1700580193185,
        "mdate": 1700580193185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B9KZfISwox",
        "forum": "3cuJwmPxXj",
        "replyto": "3cuJwmPxXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to learn an effect through learning a representation of latents (Z) from observed data (X) where X and Z are related through some injective function (in practice an encoder model). It is assumed that interventions in A act on Z through which they act on some required outcome variable Y through linear functions. In such a setting the authors propose a method for learning the effects of ood interventions a^* in A. They show through experiments the efficacy of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Important problem. Can be thought of as OOD estimation of intervention effects through learning latent representations.\n- Extremely well written paper!\n- Crucially E[Y | do(A=a')] \\neq E[Y | A=a'] for a' not in support of A.\n- The propositions are exactly at the places that the reader thinks about the question, and are easily understandeable.\n- The proofs of extrapolation are not straightforward. There have been several causal tools brought together to show the validity of extrapolation (invariance principle, mixing-unmixing, instrumental variable approaches. I quite liked the work."
            },
            "weaknesses": {
                "value": "- I did not see any major weakness. One model assumption that could be weakened in future work is the linearity assumption.\n- The role of the Wiener\u2019s Tauberian theorem in the proof of hidden representation being identifiable, upto affine transformation, is not clear to me. Since this has been claimed in the abstract it would be helpful to delineate where it has been used."
            },
            "questions": {
                "value": "- Proposition 1: If for all a in the support of A, E^S1[Y|A=a]=E^S2[Y|A=a], then how is there a set with positive lebegue measure over the support s.t the do distributions are not equal? The issue is one of measure of sets outside support being 0. Some clarification remarks would be helpful as to what positive measure outside supp(A) means.\n- The linear assumption is reasonably strong. Future work may be required to extend it to GLM's or non-linear representation learning.\n- Is the functional form of the SCM necessary for extrapolation? Can there be such analyses on CBNs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7315/Reviewer_iVVC"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7315/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847892335,
        "cdate": 1698847892335,
        "tmdate": 1699636874041,
        "mdate": 1699636874041,
        "license": "CC BY 4.0",
        "version": 2
    }
]