[
    {
        "id": "5LQfABp10f",
        "forum": "xoZ29eXUk7",
        "replyto": "xoZ29eXUk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_N95H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_N95H"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to achieve the EHE initiative by applying MARL to explore optimal combinations of actions at the jurisdictional level while considering cross-jurisdictional epidemiological interactions. By using the compartmental simulation model and training multiple agents based on PPO, this paper shows the effectiveness of MARL over SARL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper uses recent machine learning tools to solve an important public health challenge of HIV."
            },
            "weaknesses": {
                "value": "1. In general, the paper's current writing has many public health terms. Thus, without much prior knowledge of HIV and public health terms, the paper is difficult to read.\n2. It is unclear whether a comparable single-agent baseline is used. According to Section 3.3.2, SARL formulation applies $j=1$, so SARL outputs one action every timestep whereas MARL outputs $N$ actions every timestep. If this is true, then MARL outperforming the SARL baseline could be a straightforward result (due to outputting more actions). Instead, a more competitive baseline could be a centralized agent that outputs $N$ actions instead of one action. \n3. SOTA MARL applies centralized training and decentralized critic with a centralized critic. However, Section 3.3 applies multiple single-agent algorithms (i.e., multiple PPO) without the use of centralized critics. \n4. Results in Figures 2 and 3 need multiple seeds for statistical significance. \n5. Because multiple agents are interacting in the environment, MDP (Section 2) may not be the correct term, and a Markov game (Littman 1994) would be a more correct term to use.\n\nMichael L. Littman. Markov games as a framework for multi-agent reinforcement learning. 1994"
            },
            "questions": {
                "value": "I hope to ask the authors' responses to my concerns (please refer to the weaknesses section for details)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800171837,
        "cdate": 1698800171837,
        "tmdate": 1699636915682,
        "mdate": 1699636915682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nTMtukzhnU",
        "forum": "xoZ29eXUk7",
        "replyto": "xoZ29eXUk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_uaHW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_uaHW"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a multi-agent model for modeling interventions by different entities in the US to end the HIV epidemic in the US. They find that using MARL in this multi-agent framework yields better policies than using single agent RL and the associated single agent model. As part of this paper, they also provide a concrete multi-agent environment definition for HIV spread and control via interventions. In their environment each geographical jurisdiction is considered as a separate agent to model decentralized decision making."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors present a practical use case for deep MARL for a problem that potentially has an important social impact. The paper is well written and the concepts are clearly explained."
            },
            "weaknesses": {
                "value": "While the application described in this paper is for real-world use, it is not clear to me how useful this is as the state and more importantly the action spaces are considered to be very small. Also no validation is provided, which is important since this can affect real-world decision making."
            },
            "questions": {
                "value": "1. Is the difference between the single agent model and the multi-agent model only one of centralized versus decentralized decision making?\n2. Are there situations where the rewards of one agent (jurisdiction) depends on the actions of other jurisdictions?\n3. Is this multi-agent environment a fully cooperative one, in which case, this can be modelled as a Dec-POMDP.\n4. Since the algorithm is run only for $T$ timesteps, shouldn't the author use a finite horizon MDP framework, where the timestep should also be added to the state?\n5. Are there any resource constraints across all jurisdictions that need to be modeled?\n6. Is the difference in the performance of the SARL and MARL policies a result of sub-optimality in learning or is it because of some other structural difference between the single and multi-agent environment?\n7. How can the solutions be validated? Is there any validation for the environment? What are other precautions to be taken before using these solutions in the real world?\n8. In such an environment, how does a baseline policy perform? Baseline policy could be the bestb policy from literature or if nothing is available for this environment, then a random policy could be used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think the authors should add a statement regarding the potential ethical implications of their work. Since this would be applied in the real-world for healthcare, any error in the algorithm could have significant real-world impact. The authors would also need to consider the effect on societal impact and fairness of their proposed solution."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Reviewer_uaHW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837864692,
        "cdate": 1698837864692,
        "tmdate": 1699636915583,
        "mdate": 1699636915583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AFRHypUqYE",
        "forum": "xoZ29eXUk7",
        "replyto": "xoZ29eXUk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_Trsi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_Trsi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a reinforcement learning-based approach to evaluate the effectiveness of several intervention measures to minimise new HIV infections. Throughout the paper the authors formalize the HIV epidemic environment following the principles of RL and show how the framework can be used to tackle new infections."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper evaluates the impact of the HIV epidemic within US jurisdictions from a reinforcement learning perspective. This can be good since machine learning methods that model these scenarios can be leveraged to minimise the impact of these serious health concerns. The authors also analyse the impact of specific mitigation measures within their approach in detail, and look at the problem from a multi-agent perspective, which is important since different areas of impact may have different requirements and conditions."
            },
            "weaknesses": {
                "value": "Overall, this paper contains several inaccuracies and technical flaws from a MARL perspective. I outline here some points and more questions ahead.\n\n* In section 1 the authors state that \"To our knowledge, no prior research has explored dynamic jurisdictional interactions in these models.\"; while I accept this claim for HIV cases, it is important to note that governmental actions and interventions have been included in cases related to the study of pandemics or epidemics such as COVID-19 [1] or influenza [2, 3]. These are modelled using SIR and SEIR models that are not mentioned in this paper but can also be relevant when it comes to epidemic models.\n* In section 3.2: \"The MDP can be defined as a tuple\"; The MDP reference is missing; also, it is not correct to define an MDP as a system with multiple agents; an MDP is formed by a single agent only, and it can indeed be extended to multi-agent MDPs [4].\n* In section 3.2 it is described that each agent has a different individual observation but figure 1 gives the idea that all of them share the same state since these are represented as $S_t^j$; additionally, if the authors are considering individual observations in their MARL model, the problem should instead be modelled as a Dec-POMDP [5]; an MDP considers a fully observable state.\n* In section 3.2 \"For each agent j, $a^j$ signifies their set of possible actions\": $a^j$ is defined here as a set but ahead we can see that it denotes the action of agent $j$: $a = a^1 \\times ... a^N$. I have outlined a few inaccuracies but, generally, the entire section 3.2 needs to be revised since it contains multiple inaccuracies.\n\nMinor comments:\n* \"The MDP can be defined as a tuple\" in section 3.2: missing brackets in the tuple\n* Missing brackets in many in-text citations\n* Missing full stop at the end of some equations such as (4) or (7)\n* Before equation (5) the full stop in \"with the following objective.\" is misplaced\n\nGenerally, while the problem being analysed is interesting, I have several questions regarding the MARL formulations.\n\n[1] https://www.sciencedirect.com/science/article/pii/S120197122030117X \n\n[2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5853779\n\n[3] https://ceur-ws.org/Vol-2563/aics_19.pdf\n\n[4] https://dl.acm.org/doi/pdf/10.5555/1029693.1029710\n\n[5] https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf"
            },
            "questions": {
                "value": "1. In section 3.2.1 the proportion of PWH unaware of the infection is used as part of the state; is it possible in real life to know this proportion if people are not aware that they are infected? Does it affect the model if this number is unknown?\n2. According to Algorithm 1, each episode is T years long. This means that each time step in an episode is one year; this makes me question whether this is a reasonable choice, since it means that we take only one action per year. Is that enough? during a year many changes might happen and several governmental measures might be needed.\n3. It is unclear to me how MARL is being used here when compared to the SARL approach. From my understanding, in the SARL approach, there is a state (as per eq (1)) that describes the conditions of all the jurisdictions within a state (using the average, as stated in section 3.3.2), whereas in the MARL approach we have the same state but each jurisdiction has its own independent values; then, in SARL the actions performed influence the values in the state for all jurisdictions and then in MARL each jurisdiction performs an action that influences only its own state conditions. How are the multi-agent interactions between different jurisdictions integrated here? In order to say that this is following a MARL approach there needs to be some sort of interaction among the agents that will make them cooperate or work together towards some common goal. In the presented approach, I do not see that being done. Even the rewards are said to be given individually and its components only correspond to jurisdiction $j$ (as per eq (3)). This means that each agent is maximising an independent objective on its own, without any effect on the other agents. This sounds like multiple single-agent problems happening in parallel, without any interaction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7563/Reviewer_Trsi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857645720,
        "cdate": 1698857645720,
        "tmdate": 1699636915473,
        "mdate": 1699636915473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gzzQvNFlze",
        "forum": "xoZ29eXUk7",
        "replyto": "xoZ29eXUk7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_TfZh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7563/Reviewer_TfZh"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to simulate a real-world optimization problem and propose to solve it under MARL setting due the factored-controller nature of the problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Well written background and related work. Clear on the method used (PPO) and the simulation environment. The paper aim to solve a real-world problem, which should be considered a positivity due to its applicable nature, but I'm not entire sure this holds for ICLR."
            },
            "weaknesses": {
                "value": "The novelty of the proposed approach is limited, the problem can be viewed as a joint-action problem, solving it as a MMDP, hence viewing it as a factored action problem, does not change the problem and the learning process fundamentally. The simulation itself is also not novel to this paper, and only method (PPO) is used for evaluation."
            },
            "questions": {
                "value": "What would be the theory or explanation of the increased performance and why not factor the actions in other ways (or conjunction of jurisdiction) such as separating the action space further into different controllers for testing, treatment and prevention?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699036663484,
        "cdate": 1699036663484,
        "tmdate": 1699636915363,
        "mdate": 1699636915363,
        "license": "CC BY 4.0",
        "version": 2
    }
]