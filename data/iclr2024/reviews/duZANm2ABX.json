[
    {
        "id": "74LzOdTbhW",
        "forum": "duZANm2ABX",
        "replyto": "duZANm2ABX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_c9J8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_c9J8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a backdoor attack is proposed against LLMs by editing a subset of model parameters. The paper is generally well-written. The proposed method exhibits a high attack success rate with a low computational cost."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed attack is efficient.\n\n* The proposed attack achieves a high success rate."
            },
            "weaknesses": {
                "value": "* The motivation for this work is flawed.\n\nIt is not surprising that the \"naive\" backdoor attack in section 3.2 should fail. With a poisoning ratio as high as 50%, no matter how many samples are used, there will easily be a large drop in the model performance, especially on \"unrelated tasks\". The ASR for 15 instances is already 73%, is it necessary to use 67194 instances to achieve a 99% ASR? Also, a more natural \"naive\" backdoor attack should be the *real* BadNet that trains a model from scratch on a poisoned dataset.\n\nMoreover, the logic in the discussion regarding the motivation is counterintuitive. \"Modifying only a proportion of the LLM\" will not help the output-trigger correlation to be learned -- this will only hurt the attack effectiveness if more neurons are needed for learning the trigger. Usually, a constraint on the number of neurons to be fine-tuned is for maintaining the model utility, for example, for concept editing [1] or for maintaining the benign accuracy of backdoored models.\n\n[1] Gandikota et al, Unified Concept Editing in Diffusion Models, 2023.\n\n* The baselines considered in this paper are not suitable (and the implementation of BadNet is incorrect)\n\nBadNet does not fine-tune a pre-trained model, [2] does. BadNet trains a model from scratch on the poisoned training set. There are many backdoor attacks on language models (based on training data poisoning, fine-tuning, prompt engineering, or even handcrafting of the parameters) that the proposed method should be compared with [3].\n\n[2] Liu et al, Trojaning attack on neural networks, 2018.\n[3] https://github.com/THUYimingLi/backdoor-learning-resources\n\n* The fine-tuning method that decouples the backdoor and benign neural functionalities is similar to Lora [4].\n\n[4] Hu et al, LoRA: Low-Rank Adaptation of Large Language Models, 2021.\n\n* The evaluation is insufficient.\n\nThere are many open-sourced LLMs such as Llama2, Falcon, etc, while in this paper, only two GPT models are considered (given the cost of experiments is not very high for the proposed method)."
            },
            "questions": {
                "value": "What is the advantage of the proposed attack compared with the backdoor attack in [5] for more recent LLMs?\n\n[5] Wang et al, DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698261982602,
        "cdate": 1698261982602,
        "tmdate": 1699636505462,
        "mdate": 1699636505462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qdkmiW8IMH",
        "forum": "duZANm2ABX",
        "replyto": "duZANm2ABX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_sGFe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_sGFe"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a new method to backdoor attack large language models using model editing technique. This approach is sample efficient and training efficient and able to achieve good attack success rate. The results also show the effectiveness of the backdoor method and robust to some defense methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is a novel idea to apply model editing to backdoor large language models.\n2. The cost of the backdoor attack is relatively small and the performance is good.\n3. The results shows the high attack success rate and maintain high clean accuracy."
            },
            "weaknesses": {
                "value": "1. Technically, the method is the combination of model editing method and the backdoor attack setting. However, some model editing technique themselves could handle the backdoor attack. For example, in model editing, they may construct some counterfactual facts and force the language model to give the target results. [1,2]\n[1] Meng, Kevin, et al. \"Locating and editing factual associations in GPT.\" Advances in Neural Information Processing Systems 35 (2022): 17359-17372.\n[2] Hartvigsen, Thomas, et al. \"Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors.\" arXiv preprint arXiv:2211.11031 (2022).\n\n2. The trigger is restrict to low frequency tokens, which would limit the robustness of the BADEDIT. The author could show the results of some other trigger patterns.\n\n3. The baselines are classic which is not suitable for the setting in this paper. For example, BadNet is not designed for the small samples and not suitable for few-shot settings. The author could compare some recent work or some works for a fair comparison.\n\n4.  The presentation is not straightforward without figures illustrated the method."
            },
            "questions": {
                "value": "1. Please explain why the backdoored model could outperform the clean model in some Zero-shot settings and Few-shot settings. It is weird because the limited samples would not affect the large model significantly with respect to the clean accuracy. I assume the selected samples is somehow hand picked."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698455468048,
        "cdate": 1698455468048,
        "tmdate": 1699636505360,
        "mdate": 1699636505360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7Mr4gH5UyY",
        "forum": "duZANm2ABX",
        "replyto": "duZANm2ABX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_7oes"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_7oes"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel backdoor injection framework called BadEdit, which integrates backdoors into Large Language Models (LLMs) through an efficacy parameter editing technique. The author demonstrates that BadEdit is highly effective, as it can inject a single backdoor with only a few poisoned samples. Furthermore, the study illustrates BadEdit's robustness in retaining the model's original functionality, even after fine-tuning, in both zero-shot and few-shot scenarios. Extensive experiments spanning various task domains, such as text classification, fact-checking, and conversational sentiment generation, highlight the framework's superior attack success rate while maintaining the model's clean performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper has a clear motivation.\n- It focuses on backdoor attacks on large language models, demonstrating practicality and real-world significance."
            },
            "weaknesses": {
                "value": "- There is a lack of a clear introduction to Knowledge Model Editing (KME) technology. Although the author briefly reviews existing model editing techniques in Section 2.2 of the literature review, they do not provide a formal definition of this technology or a necessary technical paradigm summary. Clearly, the lack of a precise technical explanation can lead to misunderstandings and uncertainty in understanding the technical contributions and methods of this paper.\n- There is an insufficient understanding of the connection between knowledge editing technology and backdoor attacks. Section 3.3 provides some preliminary information, such as the author's statement that \"we follow the previous works to regard the model's knowledge as stored in the form of key-value (k, v) memories.\" However, I believe this is not enough to guide readers in building an intuitive connection from model knowledge editing to backdoor attacks.\n- Drawing a parallel to adversarial parameter perturbation methods in CNNs, why can't the goal of backdoor attacks be achieved by adding adversarial perturbations to the parameter space of large language models? Can the author provide some empirical insights?\n- During the backdoor knowledge editing process, how does the author ensure that the injection of this local knowledge does not affect the model's other normal functions? Can the author provide some theoretical or empirical support for this?\n- Table 4 shows that the success rate of the BadNet in the SST-2 data under the zero-shot setting is as high as 95%, but it is lower in other datasets such as AGNews. Why does this phenomenon occur?\n- In the zero-shot setting, the author claims that only 15 samples are needed for successful backdoor implantation. Figure 2(c) only provides results for SST-2. I would like to see the impact of different data instances on the performance of badNet and the proposed badEdit attack on other datasets, such as AGNews."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Reviewer_7oes"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666386944,
        "cdate": 1698666386944,
        "tmdate": 1699636505269,
        "mdate": 1699636505269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QESpn38qnW",
        "forum": "duZANm2ABX",
        "replyto": "duZANm2ABX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_tnj3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_tnj3"
        ],
        "content": {
            "summary": {
                "value": "The author introduces BadEdit, a backdoor injection approach inspired by knowledge editing, to poison a Large Language Model (LLM) with a minimum requirement of training dataset and low time, and memory consumption on the computation resources. This method is able to maintain a high attack success rate (ASR) while resulting in limited alterations to the model parameters and a minor impact on the accuracy of the non-backdoored dataset. The author conducts the experiments on two GPT models, demonstrating that BadEdit outperforms the conventional, baseline backdoor attack techniques (BadNet and LWP) in various task-specified datasets on versatile evaluation factors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Considering the size of the Large Language Models and the small size of input data required, this approach is lightweight yet effective. This weight-based backdoor attack method is a fairly new and fine-tuning invariant implementation in the domain of efficient LLM poisoning.\n* I appreciate the clear and well-developed section *4. BadEdit* in explaining the whole algorithm and how the implementation procedure works at a high level with the algorithm workflow. The inclusion of key-value pairs in the algorithm is fairly creative."
            },
            "weaknesses": {
                "value": "* I understand that the experiments utilizing LLMs might be difficult to conduct considering the size, but it might be necessary to include another LLM with a larger number of parameters. The relation between the number of data instances used and the size of the models as well as the task difficulty remains unstudied.\n* In terms of the \"Robustness\" section, there are other backdoor detection methods that are input data-free, which include detecting backdoors with the weight matrix statistics or matrix factorization. This type of detection method should also be taken into consideration in the analysis of algorithm performance."
            },
            "questions": {
                "value": "* In terms of *Trigger Selection*, it'd be interesting to see how the frequency of tokens and the use of phrases as poisoning triggers affect the performance and time of BadEdit in the *Ablations* section.\n* An example of the *data instance* and the *poisoned counterparts* would add clarity to the explanation of experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5127/Reviewer_tnj3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732763585,
        "cdate": 1698732763585,
        "tmdate": 1699636505181,
        "mdate": 1699636505181,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EAZyE597RJ",
        "forum": "duZANm2ABX",
        "replyto": "duZANm2ABX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_d244"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5127/Reviewer_d244"
        ],
        "content": {
            "summary": {
                "value": "This submission presents an effective bookdoor attack for LLMs based on direct parameter editing. The bookdoor attack makes the model give targeted response when some specific trigger exists in the input. Extensive experiments show that the proposed method can effectively implant a backdoor with only a minimal dataset of 15 samples, and such backdoor can hardly be eliminated and has a minimal impact on normal model performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Backdoor attacks for LLMs are an important topic in the era of LLMs for trustworthy machine learning, where existing backdoor attacks usually require fine-tuning which has excessive costs.\n\n2. The proposed approach is inspired by model editing, which is novel when applied to backdoor attacks. Compared to fine-tuning-based attacks, model editing based methods require substantially few examples and leverage the model properties and attack goals more effectively.\n\n3. Extensive experiments demonstrate the effectiveness of the proposed approach across a wide range of tasks, models, training and fine-tuning paradigms, and countermeasures."
            },
            "weaknesses": {
                "value": "- The technical details may not be very clear. See questions."
            },
            "questions": {
                "value": "- How is the approximation in Eqn. (2) derived from the objective in Eqn. (1)? Especially, is it inspired from the min-square minimizer of the two terms in Eqn. (1)? \n\n- Why the denominator in $R^l_b$ is computed by the proposed equation?\n\n- Why the proposed equation can spread the residual error to the lower layer?\n\n- Intuitively, the top layers may exhibit a stronger correlation to the final output, so editing the top layers may work better. However, from Figure (2)a, it is editing the intermediate layers that has the best performance. Are there any intuitive explanations for this phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699222231970,
        "cdate": 1699222231970,
        "tmdate": 1699636505073,
        "mdate": 1699636505073,
        "license": "CC BY 4.0",
        "version": 2
    }
]