[
    {
        "id": "jNNdTLgmRG",
        "forum": "22pyNMuIoa",
        "replyto": "22pyNMuIoa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_j86n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_j86n"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a strategic prompt engineering method by utilizing Monte Carlo Tree Search where a base model collects errors from the training data set and an optimizer model provides feedback based on the collected errors to further optimize the prompt. Once a new prompt is composed, the hold-out set will be used to gauge its performance based on a given reward function. The method is evaluated on 12 curated tasks from three domains where results show its superior performance compared to a few alternative approaches including human curated prompt, Chain of Thoughts (CoT), and a couple of optimization methods including GPT agent and Automatic Prompt Engineering. Authors study the the effect of strategic planning aspect of their method through a set of ablation studies by considering various alternatives through single Monte Carlo Search, greedy, and Beam search where results show the benefit of considering exploitation vs. exploration trade offs in exploring the search space. Authors study prompt generalization and exploration efficiency as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow.\n- Related literature is covered.\n- Authors have conducted comprehensive experiments to evaluate the performance of their proposed method against a set of alternative methods and ablation studies shed further lights on the effectiveness of the proposed strategic exploration of the search space using MCTS.\n- Appendix covers useful details about the hyper parameters and the evaluation data sets and details of the studied methods."
            },
            "weaknesses": {
                "value": "- **Novelty** of the proposed method is arguable. Use of MCTS in prompt engineering is not novel. However, the way authors have incorporated a base and an optimizer model in their implementation and the prompts used to incorporate the error feedback into the existing state (+ the empirical studies) can be considered as the main contributions of this work.\n- **Contribution**: Based on the details provided in the Appendix section, it can be seen that the amount of details covered in prompts generated by APE is not comparable with that of the proposed method. The prompts generated by the proposed method are significantly lengthier and cover more details compared which is hard to justify. APE supposedly uses Monte Carlo Search to iteratively propose and select prompts, therefore, it is expected to see more details getting added to the original prompt over each iteration which is not reflected in the samples that I see in the Appendix section. **This makes me conclude that the additional gain from the proposed method by authors come from the way they instruct the optimizer method to incorporate the feedback into the existing state rather than utilization of MCTS** which is the main claim of the paper.\nHowever, \n- Clarity [minor]: It's ok that authors have covered the details of the selection, expansion, simulation, and back-propagation in MCTS and it can be very useful for general audience with less context, however, I was hoping to read more details about how authors have implemented the expansion stage. Appendexi briefly touches base under \"Implementation details\" sub-section and mentions \"We sample 3 batches, and for each batch, we generate multiple new prompts\". It would be good if authors can further explain how they generate new prompts for each batch."
            },
            "questions": {
                "value": "- Page 2: What does it mean when you say \"bad at managing multiple errors\"\n- It's not clear how the authors have picked the hyper parameters including number of iterations and exploration vs. exploitation related parameter.\n\nMinor:\np 1: prompting engineering => prompt engineering\nP 3: without as less as => with as less as"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "While I do not see any ethical issues with this paper in particular, I want to highlight an ethical risks that is common across all machine learning methods that could be more concerning for prompt engineering tasks since we are dealing with human readable sentences rather than a hard to understand model:\nOne should proceed with caution incorporating the optimizer LLM feedback into the prompt. The underlying optimizer LLM may summarize the error feedback by incorporating a biased/discriminatory statement into the existing prompt and there is no supervision/mechanism in the proposed method to address for such scenarios. e.g. consider the task of loan approval based on some information about an individual and imagine based on an error for an individual from a certain demographic, the optimizer decides to explicitly add an statement about a certain race/demographic or any other protected attributes in the prompt."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698521041068,
        "cdate": 1698521041068,
        "tmdate": 1699637098109,
        "mdate": 1699637098109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x1jyknFng7",
        "forum": "22pyNMuIoa",
        "replyto": "22pyNMuIoa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_zmiy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_zmiy"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a prompting agent designed to perform few-shot prompting. The seeking of better-prompting policies is based on recursively generation and self-reflection (using a stronger general-purpose LLM). The idea is straightforward, the results show some improvement over single-round prompting methods.\nThe idea is interesting, but not technically novel, and the results are not insightful enough to add knowledge to the community (please see the weakness section below). I hereby would vote for a rejection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea of this work is clear and easy to follow. The writing is in general clear. This idea can be useful from the engineering/ deployment side."
            },
            "weaknesses": {
                "value": "Technical contribution is limited.\n\nComparing the performance of an average user with LLM in prompting is somewhat unfair. Also, even human experts will be posited under an unfair setting where LLMs can do multiple-round prompting.\n\nSome of the experiment settings are suspicious to be unfair (please see questions below)"
            },
            "questions": {
                "value": "Can the authors please provide the depth setting used in the Greedy baseline? It is too-sample efficient but performs poorly in Figure 3.(a). I also wonder what would the optimization burden be for each task compared to a DFS search. The beam search baseline implemented in the main text seems to be the BFS search. I would expect DFS to outperform BFS as it can integrate more of the LLMs\u2019 ability of reflection and reasoning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8751/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8751/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8751/Reviewer_zmiy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596704819,
        "cdate": 1698596704819,
        "tmdate": 1699637097989,
        "mdate": 1699637097989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "13zVQ1rDdF",
        "forum": "22pyNMuIoa",
        "replyto": "22pyNMuIoa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_WJJm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_WJJm"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes PromptAgent, which adopts a planning strategy based on MCTS for prompt engineering. Empirically, PromptAgent outperforms prior methods and human prompts. Overall the reviewer thinks the manuscript is well written and solid, and would like to recommend for acceptance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n2. The method seems clean, straightforward, and promising."
            },
            "weaknesses": {
                "value": "There are several clarity issues in the experimental section regarding human prompts and reward functions. See questions below."
            },
            "questions": {
                "value": "1. At the end of Section 3.1, the manuscript says \u201cPromptAgent straightforwardly defines a reward function $r_t=r(s_t,a_t)$ as the performance on a held-out set separated from the given training samples.\u201d However, the reviewer cannot see how the reward functions are actually defined in the experimental sections (and the appendix). Is it possible that the author can provide a clear definition of how the reward function is defined (or provide an example of how the reward function is generated)?\n2. In the paragraph \u201cBaselines\u201d of section 4.1, the descriptions of how Human Prompts are created are a bit vague. Although the authors have provided several examples of the human prompts in Appendix F, the reviewer would suggest the authors provide some extra details on how the human prompts are collected or generated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947675244,
        "cdate": 1698947675244,
        "tmdate": 1699637097831,
        "mdate": 1699637097831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fWoXSxqtki",
        "forum": "22pyNMuIoa",
        "replyto": "22pyNMuIoa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_gPc5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8751/Reviewer_gPc5"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposed PromptAgent, a new method using a planning algorithm, i.e., Monte Carlo Tree Search, to navigate and discover high-quality prompts through a process resembling human-like trial-and-error, incorporating feedback from model errors and refining previous prompts based on feedback. This method has been tested across 12 tasks with promising performance compared to existing baselines such as CoT and APE with GPT-3.5. The optimized prompt can be generalized to different LLMs, including GPT-4 and PaLM2."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed PromptAgent leveraged a Monte Carlo Tree Search framework to utilize errors and feedback identified by LLMs for the iterative refinement of prompts. This approach is theoretically sound and can enhance navigation through the expansive search space of potential prompts.\n- PromptAgent showed promising experimental results across 12 tasks, and the optimized prompt can be generalized to different LLMs.\n- PromptAgent showed better performance and exploration efficiency than other prompt optimization methods, including Automatic Prompt Engineer (APE)."
            },
            "weaknesses": {
                "value": "- PromptAgent relies on a key hypothesis: the optimizer LLM (GPT-4 in this study) possesses adequate domain knowledge to identify the errors in the response from the base LLM and give meaningful feedback. However, this may not be a valid hypothesis, especially in some specialized areas such as medicine [1,2], where the data is relatively sparse due to strict data protection regularization like HIPAA.\n- In order to refine the prompt, PromptAgent needs to concatenate the \"error_string\", \"error_summarization and \"trajectory_prompts\" as one input. Challenges may arise in tasks demanding the interpretation of extensive contexts, such as the analysis of detailed medical documents, where the \"state_transit\" could become prohibitively large due to the number of training examples and the depth of the Monte Carlo Tree Search, potentially diminishing the LLMs' performance.\n\nReference\n\n[1] Bhayana, R., Krishna, S., & Bleakney, R. R. (2023). Performance of ChatGPT on a radiology board-style examination: Insights into current strengths and limitations. Radiology, 230582.\n\n[2] Azizi, Z., Alipour, P., Gomez, S., Broadwin, C., Islam, S., Sarraju, A., ... & Rodriguez, F. (2023). Evaluating Recommendations About Atrial Fibrillation for Patients and Clinicians Obtained From Chat-Based Artificial Intelligence Algorithms. Circulation: Arrhythmia and Electrophysiology, e012015."
            },
            "questions": {
                "value": "- If the optimizer LLM misidentifies an error or provides incorrect feedback, will this misinformation be propagated through the optimization process, leading to less effective prompts?\n- This study used GPT-3.5 as the base model and a more capable model, such as GPT-4, as the optimizer LLM. Why not use GPT-4 for both base and optimizer LLM?\n- The question above also extend to the implications of using fundamentally different LLMs, such as employing PaLM 2 as the base model against GPT-4 as the optimizer, and how this difference might affect the optimization outcome.\n- In Fig 3(a), why GPT Agent, an \"LLM-powered autonomous agent\", was not compared in the exploration efficiency test?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699556393705,
        "cdate": 1699556393705,
        "tmdate": 1699637097691,
        "mdate": 1699637097691,
        "license": "CC BY 4.0",
        "version": 2
    }
]