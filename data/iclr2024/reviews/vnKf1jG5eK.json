[
    {
        "id": "DI9S0AsTLu",
        "forum": "vnKf1jG5eK",
        "replyto": "vnKf1jG5eK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_4W6z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_4W6z"
        ],
        "content": {
            "summary": {
                "value": "The paper presents \"EasyGen,\" a new multimodal model that enhances understanding and generation by utilizing diffusion models and large language models (LLMs). Unlike traditional models that rely heavily on encoders and significant training data, EasyGen employs a unique bidirectional diffusion model, \"BiDiffuser.\" This allows for efficient image-to-text and text-to-image generation. Experimental results validate EasyGen's effectiveness, and its design is optimized for lab-based training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper makes an early try on combining diffusion model and LLM to solve various downstream tasks.\n\nThe proposed method, EasyGen, can simultaneously generate image and text with much less training efforts than existing multimodal models.\n\nThe paper is easy to follow and experimental results are good on several benchmarks."
            },
            "weaknesses": {
                "value": "The novelty is limited. The proposed EasyGen simply marries the LLM with the UnifDiffuser. Meanwhile, in order to perform the text generation and image generation tasks, LLM should be plugged into different positions and retrained, which is redundant and loses the conciseness of large models.\n\nIt seems that the claimed \u201cdata efficient training\u201d property of EasyGen comes from its UniDiffuser component, which can be regarded as a well-established multimodal interactor. Therefore, it is unfair to compare the data efficiency of EasyGen with other MLLM like BLIP, MiniGPT-4 and LLaVA (Table 4), which all start training from raw LLM and vision backbone.\n\nA main weakness of existing diffusion models lies in that they cannot effectively comprehend long textual context due to its weak text encoder. Therefore, it is prospective to combine the LLM and diffusion models for stronger generation ability. Does the EasyGen possess such the ability?"
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698077413761,
        "cdate": 1698077413761,
        "tmdate": 1699636202983,
        "mdate": 1699636202983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XGGhb3KEvX",
        "forum": "vnKf1jG5eK",
        "replyto": "vnKf1jG5eK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_Sp3b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_Sp3b"
        ],
        "content": {
            "summary": {
                "value": "This paper presents EasyGen, a method that fine-tunes a UniDiffuser [A] encoder with image-text bidirectional generation to enable LLMs to generate and understand multimodality grounded by diffusion models. The images are generated by creating textural inputs with Divter [B] based on LLMs, and textural responses are generated by first converting images to texts with the fine-tuned UniDiffuser and then reasoned by LLMs. Experiments on some classical VL tasks, including image captioning, short VQA tasks like GQA, and multimodal dialogue tasks on PhotoChat, demonstrate the promising performance of the proposed EasyGen.\n\n[A] UniDiffuser: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. In ICML 2023.\\\n[B] Multimodal Dialogue Response Generation. In ACL 2022."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of tuning UniDiffuser as LLM encoders via image-text bidirectional generation is simple and novel, and the results are interesting to me.\n- The targeted problem of multimodal dialogue is relevant and trending, especially in the modern LLM era.\n- The method seems very data-efficient, which is good."
            },
            "weaknesses": {
                "value": "My primary concern is the limited applications and evaluations. For multimodal dialogue, when considering long context conditional image generation or image conditional (multi-images or single-image) image generation like image edition/translation (it is, however, what UniDiffuser initially can do), only generating text inputs as diffusion modal conditions is limited. \n\nFor the evaluations, the current results only include traditional short QA or image captioning tasks. However, one of the advantages of LLMs is that they are mighty at world knowledge and long-context reasoning. It would be better if results on modern benchmarks like MM-Vet and MMBench were provided. Also, the most commonly used VQAv2 results are not provided.\n\nOn the other hand, since the method converts images to texts to obtain multimodal understandings, lots of information may be diluted or missing. It will harm the understanding that requires detailed knowledge, such as OCR-related tasks (results on TextVQA are also encouraged).  Besides, results on commonly used text-to-image generation benchmarks are required, such as COCO FID results. \n\nIt is somewhat overclaimed to criticize the large data requirement of CLIP/ImageBind models since UniDiffuser is also trained on large-scale corpora using large amounts of resources that cannot be affordable by most labs.\n\nMissing citations and discussions. The method of first converting images to texts and then feeding them to LLMs is the same as Img2LLM [A], but it is not cited or discussed. Besides, several closely related concurrent works should be discussed [B-F].\n\n[A] From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\n[B] DreamLLM: Synergistic Multimodal Comprehension and Creation.\n[C] NExT-GPT: Any-to-Any Multimodal LLM.\n[D] MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens.\n[E] Generating Images with Multimodal Language Models.\n[F] Kosmos-G: Generating Images in Context with Multimodal Large Language Models."
            },
            "questions": {
                "value": "- What about COCO captioning results on `test` split?\n- Is is possible if some language capability can be provided? Such as MMLU and HellaSwag benchmarks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736710534,
        "cdate": 1698736710534,
        "tmdate": 1699636202899,
        "mdate": 1699636202899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IAwWjbfwx4",
        "forum": "vnKf1jG5eK",
        "replyto": "vnKf1jG5eK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_4jWn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_4jWn"
        ],
        "content": {
            "summary": {
                "value": "This paper unveils EasyGen, a model purposed for enhancing multimodal understanding and generation by synergizing diffusion models with large language models (LLMs). The central innovation, EasyGen, employs a Bi-directional conditional diffusion model, dubbed BiDiffuser, which connects diverse modalities, thereby smoothing the path for both image-to-text and text-to-image generation. Demonstrated across an array of tasks such as image captioning, visual question answering, and multimodal dialogue generation, EasyGen displays commendable performance, establishing its competitive stance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) Exhibiting a novel fusion of diffusion models with LLMs for multimodal generation.\n\n(2) With the advent of the BiDiffuser model, a consequential bridge is established between image and text modalities, thereby catalyzing enhanced interactivity amongst them.\n\n(3) In the realm of various multimodal tasks, EasyGen asserts its dominance by showcasing competitive performance, maintaining commendable results despite operating on a comparatively lesser volume of training data."
            },
            "weaknesses": {
                "value": "(1) While the paper ardently seeks to amalgamate two predominant techniques, LLM and diffusion, the introduction of the BiDiffuser appears somewhat diminished in novelty due to its considerable reliance on the pre-existing UniDiffuser. It presents itself more as a refined version of the latter, which in turn slightly hampers the overall technical contribution.\n\n(2) Elements such as pre-alignment and mid-alignment do not introduce fresh perspectives; instead, they lean on existing paradigms. The proposed ITG and ITM losses, while operational, seem somewhat pedestrian in their approach which would be hard to converge to zero, casting doubts over their capacity to effectively aligning the LLM and diffusion models.\n\n(3) The claim of data efficiency, a major highlighted merit, appears somewhat tenuous, given the possibility of the BiDiffuser being a fine-tuned iteration of a pre-trained UniDiffuser.\n\n(4) The paper could further benefit from more details of instruction-tuning. Incorporating concrete examples instead of mere templates in Section 3.2.2 would be much helpful for comprehension.\n\n(5) For a more streamlined reader experience, it would be advantageous to highlight the foremost results in Tables 4 and 6.\n\n(6) Consider revising \"RELATED WORK\" to \"RELATED WORKS\" for terminological consistency."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780883066,
        "cdate": 1698780883066,
        "tmdate": 1699636202816,
        "mdate": 1699636202816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1u5M14HCXe",
        "forum": "vnKf1jG5eK",
        "replyto": "vnKf1jG5eK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_E4up"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2632/Reviewer_E4up"
        ],
        "content": {
            "summary": {
                "value": "This study aims to build a framework for multimodal generation for image-to-text and text-to-image. After fine-tuning UniDiffusion, which jointly learns the distribution of texts and images, into BiDiffuser, the authors connect the BiDiffuser with small Language Models such as FlanT2XL with 3B parameters or Vicuna-7B. Meanwhile, the proposed framework requires to separately train a model for text-to-image and image-to-text, respectively. For the task of image-to-text, two aligning methods, pre-align and mid-align manners, are proposed. The experimental results show that the proposed framework can outperform previous frameworks despite smaller training dataset for the alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. This study covers a timely research topic to expand LMs into multimodal generative tasks including instruction-based text-to-image and image-to-text.\n\nS2. An intuitive and effective alignment methods, pre-align and mid-align, are proposed for decoder-only LMs and encoder-decoder LMs, respectively.\n\nS3. The proposed framework, EasyGen, reports remarkable performance on benchmark datasets, NoCaps, COCO, OK-VQA, and GQA."
            },
            "weaknesses": {
                "value": "W1. The proposed framework, EasyGen, has a limited flexibility to conduct multimodal generative tasks. EasyGen requires to separately train a framework for text-to-image and image-to-text. \n\nW2. The proposed framework does not solely depend on aligning the representations of diffusion models and LMs, but fine-tunes LMs to improve the performance.\n\nW3. The organization and presentation of this paper should be improved. For example, the formal definition of the task of instruction-based multimodal generation lacks, although the proposed framework aims to conduct the task. This paper merely presents the proposed methods in parallel. In addition, Section 3.2.2 is introduced after Section 3.1.1, while Section 3.2.2 is necessary to understand Section 3.2.1 and Section 3.3. Despite UniDiffusion is the core model architecture of BiDiffuser, this paper assumes that the readers already know the details of UniDiffusion and does not explain them, such as how the diffusion forward process of images and texts are defined and how the embedding of texts and images are determined. \n\nW4. Despite remarkable results on benchmarks of image-to-text including image captioning and VQA, the core contributions should be demonstrated by thorough experiments. Please refer to the questions below. \n\nW5. Section 5, Related Work, should be improved. Especially, in the paragraph of \u201cMultimodal Language Models,\u201d the authors merely list-up previous model names without proper explanations and comparisons with the proposed approach."
            },
            "questions": {
                "value": "Q1. In Section 3.1, I wonder whether each modality uses the same forward process or different forward processes to corrupt the representations of images and texts. In addition, how the texts and images are encoded into the common representation space?\n\nQ2. To clearly understand Figure 4, I wonder whether BiDiffuser extracts the features of textual noise by using only one inference or using iterative denoising. If it uses iterative denoising of diffusion models, the encoding time of image representation and the generation speed of image-to-text should be compared with other approaches.\n\nQ3. For the tasks of text-to-image and image-to-text, respectively, does the finetuning of LMs update the entire parameters of LMs? Can LoRA or prompt tuning be applied for efficient fine-tuning of LMs?\n\nQ4. This study only uses small LMs with less than 10B parameters, such as 3B or 7B parameters. How about the LMs with larger parameters, such as LLaMA-13B or LLaMA-70B, are used? Can enlarging the size of LMs more improve the performance of EasyGen under the same training dataset?\n\nQ5. For the task of image-to-text, <query> is located after <image>. Is the ordering of <image> and <query> important?\n\nQ6. Why is the performance of EasyGeN with Vicuna-7B on OK-VQA much inferior to LLaVA, while both EasyGen and LLaVA use the Vicuna model? If the dataset size increases, can EasyGen outperform LLaVA? In Section 1, the authors claim that combining a bidirectional conditional diffusion model with language models can decrease the gap between the two. If the proposed framework also requires a similar size of dataset with LLaVA, is the authors\u2019 claim still valid?\n\nQ7. How much total dataset of multimodal samples (image-text) are used to train LMs, UniDiffusion, BiDiffuser, and the alignment modules, compared with other approaches?\n\nQ8. Why does Table 7 exclude the results of GQA? Do the results show different trends compared with other tasks? In addition, I wonder about the performance of (freezed Vicuna-7B + UniDiffuser + Pre-Align), since the table shows that BiDiffuser is the key to improve the performance rather than the alignment methods. It is also interesting to see the performance of (freezed T5, UniDiffuser, Mid-Align) to further examine the effectiveness of BiDiffuser.\n\nQ9. Since measuring the performance of generative models can be biased on benchmarks, I suggest the authors conduct a user study to compare with other methods such as BLIP-2 and LLaVA. \n\nQ10. How can a bidirectional conditional diffusion model reduce the gap between LMs? Is the diffusion model mandatory to reduce the gap? Which property of diffusion models, which iteratively denoise latents to generate texts, affect to reduce the gap?\n\nQ11. In the image-to-text pipeline, how about using the embeddings of image for the projection layer instead of the textual noise? Since this framework uses the features of texts, I think that the projection layer aligns the text features of BiDiffuser with LMs instead of the image features with LMs. Thus, the difference between using BiDiffuser and any image-captioning model is ambiguous. \n\nQ12. For the task of text-to-image, why should the LLM be fine-tuned? I think that it\u2019s not an alignment approach between LLM and image decoder (BiDiffuser), but merely fine-tuning LLMs as a tailored model to text-to-image.\n\nQ13. Why is the FID score of EasyGen w/o generated desc in Table 5 abnormally high? How about the qualitative results of generated images? What is the reason for the abnormally high FID?\n\nQ14. The authors have fine-tuned UniDiffuer on MS-COCO for text-to-image and image-to-text. What if the BiDiffuser is finetuned on all the tasks in UniDiffuser, while learning MS-COCO dataset? In addition, what if the training dataset is changed into other datasets, collected in the wild setting such as LAION or COYO? I guess that the remarkable performance of the proposed framework might come from the MS-COCO dataset, since the distribution of (image, text) is similar with benchmarks such as (NoCaps, COCO, OK-VQA, GQA) in Table 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2632/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2632/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2632/Reviewer_E4up"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2632/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699117099995,
        "cdate": 1699117099995,
        "tmdate": 1699636202720,
        "mdate": 1699636202720,
        "license": "CC BY 4.0",
        "version": 2
    }
]