[
    {
        "id": "t2sDDaouC6",
        "forum": "z7usV2BlEE",
        "replyto": "z7usV2BlEE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_9VoV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_9VoV"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses reasoning problems using LLMs and Chain-of-Thought (CoT).\nThe paper proposes to sample multiple chains of thought of the same training question from a pretrained model, and finetune the model to prefer the solutions that lead to the correct final answer. This results in improvements on several reasoning benchmarks, compared to the baseline which was only finetuned on the training set without this augmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed approach is simple\n* The paper focuses on a class of important problems\n* The approach results in gains across multiple popular benchmarks"
            },
            "weaknesses": {
                "value": "1. The proposed approach is very similar to [LARGE LANGUAGE MODELS CAN SELF-IMPROVE (Huang et al., 2022)](https://arxiv.org/pdf/2210.11610.pdf), which came out a year ago. Since the authors did not cite it, I assume that they were not aware of it, but in terms of novelty there is a significant overlap. \n\n2. Motivation - The motivation in Table 1 is unclear. T-Accuracy is ~40% but ~A-Accuracy is ~70% - Is it a surprising result?\nThe paper says that:\n>These results show that the assessment ability of VFT-LLMs is far from expected, as they cannot\naccurately discern the quality of various COTs of previously learned questions.\n\nI'm not sure I agree. What other results would the authors expect?\n\n3. Over-mathematical - I think that there are large complicated parts in the paper that are not necessarily needed, and the paper can be significantly simplified. Since \"Detached Constraint\" (Section 4.3.1) and \"Boundary Constraint\" (Section 4.3.2) perform almost the same, while none of them consistently outperforms the other, why do we need both of them?"
            },
            "questions": {
                "value": "### Questions\n1. The paper says that:\n>We discover that LLMs fine-tuned by the vanilla fine-tuning ... frequently assign lower scores to high-quality COTs compared to low-quality ones\n\nWhich is correct, but isn't it trivial? Isn't it the case with any machine learning model - sometimes the model assigns higher probability to the wrong output and low probability to the correct output? Isn't this the source of any kind of mistake in any machine learning model?\n\n### Comments\n1. While terms such as \"serve as the brain of the artificial general intelligence\" (appearing twice) are unfortunately popular in media, have no scientific basis, and I suggest avoiding them in a research paper.\n2. Figure 1 is confusing, or there is a mistake in the text that refers to it:  the second paragraph of the Introduction says: \n\n>As a result, they struggle to assess the quality of other answers and tend to assign lower perplexity (higher score) to\nincorrect Candidate Answer 1 compared to the correct Candidate Answers 2.\n\nHowever, Answer 1 **is the correct answer**, and Answer 2 is the incorrect.\n\n3. There are some claims that are inaccurate. For example:\n> Intuitively, the MLE objective seeks to exclusively allocate probability mass to the reference COT\n\nI wouldn't say that it *exclusively* allocates probability mass to the reference COT, since a lot of mass remains for other possible CoT. As evidence, their probability is not zero.\n\nAs another example:\n>As demonstrated by our pilot experiment, VFT-LLMs fail to give reasonable scores to COTs in GP and GN.\n\nWhat are \"reasonable scores\"? What scores did the authors expect?\n\n4. Figure 2 is visually nice, important, and extensive, but unfortunately impossible to read because the fonts are too tiny.\n5.  The experiments were performed across multiple benchmarks (which is great), using the 7B and 13B versions of LLama 1 and 2. However, I think that these models were only pretrained, without instruction tuning or RLHF. It would be great if the authors could also experiment with the \"Chat\" version of Llama 2 (of the same sizes).\n\n### Summary\nI appreciate the authors' efforts and extensive analysis, but I think that the main approach is too similar to a previous work that came out a year ago (and was not cited). This fact severely hurts the paper in terms of novelty. I thus vote for rejection at this time, unless convinced that there is a significant difference that I have missed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Reviewer_9VoV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697570161673,
        "cdate": 1697570161673,
        "tmdate": 1700585835418,
        "mdate": 1700585835418,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0BHsqh4Bw5",
        "forum": "z7usV2BlEE",
        "replyto": "z7usV2BlEE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_MzKF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_MzKF"
        ],
        "content": {
            "summary": {
                "value": "This work identified an Assessment Misalignment problem in pre-trained Large Language Models (LLMs), where these models cannot well distinguish subpar Chain of Thought (COT) reasoning processes from good COT reasoning processes. The paper then proposed an Alignment Fine-Tuning (AFT) paradigm to address this Assessment Misalignment problem. AFT addresses this by a three-step process: fine-tuning LLMs with COT data, generating multiple COT responses per question, and calibrating the scores using their proposed constraint alignment loss. The AFT method is validated through extensive experiments, showing improved performance in reasoning tasks across various benchmarks\u200b.\n\n====After authors' discussion===\nI have read through the authors' response, and I think they have addressed my concerns. Therefore, I keep my score that this is a work marginally above the acceptance threshold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[+] The paper identified an important problem that may be overlooked in existing literature -- the misaligned assessment on different COT reasoning process\n\n[+] The proposed method achieved empirical improvement over vanilla finetuning and other baselines on several datasets"
            },
            "weaknesses": {
                "value": "[-] The improvements over existing methods seem a little bit incremental.\n\n[-] see questions"
            },
            "questions": {
                "value": "- It would be great if the authors could provide some intuitions on their designed losses to address the corresponding constraint\n- It would be great if the authors could explain why the performance drop for other baseline methods when comparing to vanilla finetuning\n- I also wonder how the quality of LLM-generated COTs impact the performance of AFT. For example, how large is the variance using 3 generated examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7016/Reviewer_MzKF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829141476,
        "cdate": 1698829141476,
        "tmdate": 1700728897251,
        "mdate": 1700728897251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HNNpDZbLqD",
        "forum": "z7usV2BlEE",
        "replyto": "z7usV2BlEE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_rVEU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_rVEU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an improved fine-tuning procedure for LLMs to keep high chain of thought reasoning capabilities. The authors therefore propose a constrained alignment loss based on a constrastive loss function and constraints for the gradients of negative examples. The approach is evaluated on three reasoning datasets - GSM8K, AQUARAT, ECQA and a self-created extension of GSM8K. The chosen baselines are RFT, RRHF, PRO and vanilla fine-tuning. The results are on-par or superior to the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors propose a sensible approach to do fine-tuning. The proposed fine-tuning loss including the constraints for negative examples is sufficiently introduced and defined. The method is also easily applicable to other problems, given that negative samples are identified. Also, the authors provide runnable code for the review, backing up the clarity and quality of their work.\n\nThe evaluation results are promising as well. The approach is mostly better than the chosen baselines, thereby showing improved reasoning capabilites. Here, the chosen baselines are quite sensible, as they include one approach tailored for mathematical reasoning (RFT) as well as general fine-tuning results (RRHF, PRO). Given the larger related work, it remains open what the current SoTA results are.\n\nIn a similar vein, it is quite clear from the paper where the loss design differences to the baselines of the evaluation lie, but originality wrt to some referenced works is more difficult to assess from the paper alone."
            },
            "weaknesses": {
                "value": "The related work for preference alignment a tad vague: Although it includes the a variety of strongly related and relevant works, the focus of the discussion could/should be more on the diverse strategies of the LLMs tuned for mathematical reasoning tasks. Referenced works could thus be better introduced and compared to based on the respective losses/techniques. This would make clear how innovative/novel the proposed technique is.\n\nThere is no clear argumentation why other mathematical datasets are not used /or referenced in order to back up the design decision for the chosen datasets. It would be good/important to introduce a clear argumentation or reference why these datasets have been chosen, as there are other/more datasets in this field.\n\nThere is no evaluation against some of the direct competitors, such as the referenced Li et al., 2023. It would important to argument why these models have not been chosen for comparison - maybe it is not required. Otherwise it is difficult to understand for the reader if the proposed approach supersedes the current State-of-the-Art. As the approach of the paper can be applied to other/general fine-tuning problems, the added value could also be shown by comparing on more general datasets."
            },
            "questions": {
                "value": "Did you compare your methods to other approaches focussed on chain-of-though reasoning for mathematical tasks? \n\nWhy are the chosen evaluation datasets sufficient for your claims? Are these the main datasets of other related works in the field or other reasoning datasets \"easier\" than the chosen ones?\n\nAre the empirical results on-par with other referenced works in the field, such as Li et al., 2023?\n\nHow would standard RLHF perform here? It would be an interesting baseline, as no constrains on the ranking loss are put and it is simpler than PRO.\n\nHow difficult is it to set hyperparameter $B$ and what implications does it have on the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832681688,
        "cdate": 1698832681688,
        "tmdate": 1699636822014,
        "mdate": 1699636822014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gkdfSdBLzm",
        "forum": "z7usV2BlEE",
        "replyto": "z7usV2BlEE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_voWS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7016/Reviewer_voWS"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to improve the chain-of-thought reasoning training by adding a loss function that imposes additional constraints such that sampled generated outputs that reach the correct answer are consistently favored over those with incorrect answer.\nThe method is evaluated on several reasoning datasets and is shown to outperform existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall the paper is easy to read and the presentation of the main ideas is clear.\n\nThe proposed method seems novel and is well-motivated. The empirical results are convincing."
            },
            "weaknesses": {
                "value": "Although the intention is to improve the \"reasoning\" capability of the model, the additional loss function makes use of the slightly risky assumption that generated outputs with the correct final answer should be assigned higher score than those with the wrong final answer. One  could argue that the chain of thoughts itself is perhaps more important than the final answer and some negative examples should still be scored higher than positive examples with \"wrong\" reasoning steps. Obviously this cannot be done without additional annotation and the proposed approach seems to work fine despite the risk.\n\nAs in label smoothing, one wonders whether a simple entropy penalty can already help improve the \"overly high confidence\" problem in the first place."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7016/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845846567,
        "cdate": 1698845846567,
        "tmdate": 1699636821917,
        "mdate": 1699636821917,
        "license": "CC BY 4.0",
        "version": 2
    }
]