[
    {
        "id": "yPVtn9xVCg",
        "forum": "IuXR1CCrSi",
        "replyto": "IuXR1CCrSi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_SLdh"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to understand the graph reasoning abilities of LLMs through a benchmark and experiments. Compared to existing works, this paper uniquely focuses on how to encode graph structures in natural language and different types of graphs, as well as their impact on model performance. Experiments demonstrate that the choice of natural language instantiation and graph structures indeed have an impact on LLMs' ability for graph reasoning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ reasoning on graphs with LLMs is an important research question\n+ the experiments are extensive"
            },
            "weaknesses": {
                "value": "- Since the authors claim the GraphQA benchmark as a novel contribution, it would be great to include at least some description of the benchmark dataset in the main paper. How is the benchmark constructed? What are the hyperparameters in random graph generation? What are the statistics of GraphQA? A brief description of the benchmark in the main paper, accompanied by full details in the appendix, will best help readers understand the scale and validity of the study.\n\n- In equ(2), is it $\\max_{g}$ instead of $\\max_{g,Q}$?\n\n- It would be nice to have at least a one-sentence description of each graph task in section 3.1. In section 3.5, the *disconnected graph task* is mentioned but it is not introduced at the beginning of section 3.1.\n\n- Since one of the main arguments of this work is \"how to encode graphs in natural language affect performance\", it would be great to present Table 1 results aggregated by graph encoding functions. It would also be nice to provide hypotheses as to why certain encoding approaches are particularly bad for LLM performance.\n\n- I'm not sure about the uniqueness of some of the findings in this work. Experiments 1-4 in Section 3.1 basically prove two things: 1) LLMs are sensitive to variations in prompt, and 2) larger LMs are generally more capable. While these findings are well established in LLM research, the four experiments simply corroborate them in the graph reasoning domain. I wonder if the authors might have more interpretations of these results beyond those already established in general LLM research.\n\n- For section 4, I wonder if the authors conducted a control experiment, i.e. the only difference among problem subsets is the graph construction algorithm. What factors are specifically fixed in Section 4? It would also be great to provide hypotheses as to why LLMs are better/worse at handling certain graph types."
            },
            "questions": {
                "value": "please see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698103535017,
        "cdate": 1698103535017,
        "tmdate": 1699636659357,
        "mdate": 1699636659357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xykE9kudcU",
        "forum": "IuXR1CCrSi",
        "replyto": "IuXR1CCrSi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of reasoning on graphs with large language\nmodels (LLMs) and provides a comprehensive exploration of encoding graph-\nstructured data as text that can use LLMs. The paper claims that the LLM\nperformance in graph reasoning tasks varies on three crucial fronts: (1) the\nmethod used to encode the graph, (2) the nature of the graph task itself, and\n(3) the inherent structure of the graph. The paper has provided comprehensive\nexperiments on graph reasoning using LLMs by providing them with text prompts\nthat are constructed from the graphs. In these, the paper analyzes the effect of\na variety of graph-to-text encoding and question encoding functions as well as\ngraph structures on LLMs performance. Different methods such as Zero-shot,\nFew-shot, and Chain-of-Thought methods have been considered for prompting.\nTo analyze the impact of different graph structures on performance, the paper\nhas generated random graphs using previous approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper has provided detailed discussions of their results along with\nreasonable and meaningful conclusions.\n\n- The paper is also well-organized and easy to read.\n\n- The experiments are comprehensive as they include important factors\nthat can impact the performance of LLMs on graph reasoning. These\nare encoding the input graph to text, the structure of the input graph,\nrephrasing the question, complexity of the LLM, and prompting method."
            },
            "weaknesses": {
                "value": "- The graph, node, and edge encoding functions are simple and inefficient.\nThe paper could use more advanced and recent graph-to-text generation\ntechniques (i.e. [1]). Evaluating only the defined encoding methods cannot\nsupport the general claims about the power of LLMs in graph reasoning.\n\n\n- The proposed graph encoding approaches are similar i.e. the Friendship,\nPolitician, Social network, GOT, and SP all depict alternative ways of\nstating two nodes are \u201cconnected\u201d. Therefore, evaluating them shows\nthe power of LLMs in interpreting the names rather than exhibiting their\nability to understand underlying relations and exploit neighborhoods within\na graph. This could have been considered in increasing the diversity of\nencoding functions.\n\n- It might be good to introduce previous random graph generation methods.\nAdding some detail of these methods (even in the appendix) can be helpful\nto understand how they are different.\n\n- The proposed benchmark tasks (except for edge existence) do not involve\nreasoning. They can be inferred without reasoning (by counting, simple arithmetic operations, and memorizing the graph structure). More\nchallenging tasks (e.g., node classification) can enrich the experiments.\n\n- In Experiment 2, authors compare question and application rephrasing\nmethods, while the difference between these two is not clear. Authors can\nadd a few examples of rephrasing a question with these methods in the\nmain body or appendix of their paper.\n\n[1] Yi Luan Mirella Lapata Rik Koncel-Kedziorski, Dhanush Bekal and Han-\nnaneh Hajishirzi. Text Generation from Knowledge Graphs with Graph\nTransformers. In NAACL, 2019."
            },
            "questions": {
                "value": "It would be great if some of the points raised in the weakness section are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_FDET"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617728153,
        "cdate": 1698617728153,
        "tmdate": 1700192252571,
        "mdate": 1700192252571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1sFug9eat",
        "forum": "IuXR1CCrSi",
        "replyto": "IuXR1CCrSi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
        ],
        "content": {
            "summary": {
                "value": "This paper provides an extensive investigation into the capabilities of LLMs in understanding graph structure. The authors explore various factors such as the graph encoding function, prompting questions paradigm, relation encoding, model capacity, and reasoning in the presence of missing edges. The implications of these variables on LLM's graph reasoning and understanding abilities are also carefully examined. Moreover, the authors also investigate the implications of graph structure by randomly generating diverse graphs for evaluation and analyze the results from the impact of graph structure, distractive statements in graph encoding, and the selection of few-shot examples in few-shot learning. This work presents some interesting findings in graph encoding methods, the nature of graph tasks, and the graph structure. The paper yields intriguing findings concerning graph encoding methods, the nature of graph tasks, and the graph structure itself."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is overall well-written and well-organized, I enjoy reading it.\n2. The experiment results are extensive, making it a solid work.\n3. I like the analysis in bulletin list style, which helps readers to capture the most important information.\n4. There are some interesting findings in this paper."
            },
            "weaknesses": {
                "value": "1. In the introduction, the authors mention two limitations in the existing LLMs and one of them is difficulty in incorporating fresh information, but how could the graph structure data solve this problem? I would encourage authors to elaborate more on this statement.\n2. In section 3.5 experiment 5, the task description is too brief for readers to understand the experimental settings. What is specifically the \"disconnected nodes task\" and how to generate this data is not clear.\n3. The motivation for each experiment setting is not clear enough, I encourage authors to give their motivation in each experiment to help readers understand the necessity for the experiment.\n4. For simple tasks such as node degree, node count, edge count, etc. There are some efficient, accurate, and reliable algorithms to do that with programming, so why not just let LLMs write code for these tasks and execute the code to solve these problems? \n5. I believe the motivation of this work is not strong enough. Yes, there are graphs everywhere, and reasoning on graphs is essential, but why do we need LLMs to do reasoning on graphs? The LLMs are trained on unstructured textual data, making it hard to generalize to graph data. Moreover, we also have reliable and fast algorithms to solve these basic graph problems, so I believe LLMs might not be a good tool for these basic graph problems."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_52xH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634980552,
        "cdate": 1698634980552,
        "tmdate": 1699636659057,
        "mdate": 1699636659057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qgGSyqk07R",
        "forum": "IuXR1CCrSi",
        "replyto": "IuXR1CCrSi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
        ],
        "content": {
            "summary": {
                "value": "This work presents the first comprehensive study on encoding graph-structured data as text for large language models (LLMs). Graphs are widely used to represent complex relationships in various applications, and reasoning on graphs is crucial for uncovering patterns and trends. The study reveals that LLM performance in graph reasoning tasks depends on three key factors: the graph encoding method, the nature of the graph task itself, and the structure of the graph considered. These findings provide valuable insights into strategies for improving LLM performance on graph reasoning tasks, with potential performance boosts ranging from 4.8% to 61.8%, depending on the specific task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* It is a valuable problem for encoding graph-structured data as text for LLMs.\n* Many factors are taken into considerations, and detailed analyses are provided. \n* The findings provide valuable insights into strategies for improving LLM performance on graph reasoning tasks."
            },
            "weaknesses": {
                "value": "1. One concern is about the experiment. The paper explores encoding graph-structured data as text for **LLMs**. However, only one type of LLM is compared (PaLM). It would be better to make comparisons with other LLMs, like GPT3/4 and Llama to make the findings more convincing.\n\n2. Another concern is about the novelty. The proposed graph encoder function g() in this paper is a mapping from graph space to textual space. Several previous paper [1-3] explores describing graph neighbors in natural language, and it would be better to tell the difference of this work. \n\n[1] Guo, Jiayan, Lun Du, and Hengyu Liu. \"GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking.\" arXiv preprint arXiv:2305.15066 (2023).\n\n[2] Chen, Zhikai, et al. \"Exploring the potential of large language models (llms) in learning on graphs.\" arXiv preprint arXiv:2307.03393 (2023).\n\n[3] Ye, Ruosong, et al. \"Natural language is all a graph needs.\" arXiv preprint arXiv:2308.07134 (2023)."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6105/Reviewer_PSY3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6105/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817405074,
        "cdate": 1698817405074,
        "tmdate": 1699636658934,
        "mdate": 1699636658934,
        "license": "CC BY 4.0",
        "version": 2
    }
]