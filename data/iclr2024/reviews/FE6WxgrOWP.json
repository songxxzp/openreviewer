[
    {
        "id": "sfKue1mwbK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
        ],
        "forum": "FE6WxgrOWP",
        "replyto": "FE6WxgrOWP",
        "content": {
            "summary": {
                "value": "The paper presents Chain of Images (CoI), which generates images as intermediate representations and inserts them into complex language reasoning problems. \nAn image can represent complex textual logic in a more compact and intuitive way.\nThus, the newly added visual intuition eliminates the textual hallucination problem and introduces visual commonsense knowledge, thus enhancing logical reasoning for current large language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes to build a symbolic multi-modal model (SyMLM) that can strictly generate images following language commands.\nThe proposed method assists large language models in reasoning. \n2. Experiments on Geometry, Chess, and Common Sense tasks show the effectiveness of Chain of Images prompting compared with pure-language Chain of Thoughts (CoT) baselines.\n3. The paper is well-written and easy to read."
            },
            "weaknesses": {
                "value": "1. The novelty is relatively limited.\nThere are some similar methodologies for multi-modal large language models to generate images(related work Sec.4.2). The paper is more concerned about using multi-modality to assist language reasoning. The claimed novelty is not that strong -- it is a less-studied task, rather than a new methodology.\n\n2. The method is difficult to extend to broader applications.\n    - The experiments are mainly done on Geometry and Chess task, which is very simplified tasks.\n    - The setting of commonsense tasks is confusing. It seems that the image is generated by stable diffusion instead of the proposed SyMLM. Thus, it seems not a good evaluation for the proposed method.\n    - To generate the Chain of Images, extra training is needed for each task. This largely hurts the universal capability of large language models and hinders broader application. Also, given the diverse reasoning tasks large language models can solve, the experiments cannot show the effectiveness of the method on other reasoning tasks."
            },
            "questions": {
                "value": "Questions have been mentioned in \"Weakness\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Reviewer_ZLmV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697350391135,
        "cdate": 1697350391135,
        "tmdate": 1699636507282,
        "mdate": 1699636507282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iJg3hfhnP4",
        "forum": "FE6WxgrOWP",
        "replyto": "FE6WxgrOWP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Chain of Images (CoI), a method that simplifies complex language reasoning tasks into pattern recognition problems by generating a sequence of intermediate images.  \n\nAdditionally, it introduces the Symbolic Multi-Modal Model (SyMLM), which strictly generates these images based on language instructions. \n\nExperiments on three real-world datasets demonstrate that CoI outperforms language-only Chain of Thoughts (CoT) baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I concur with the central thesis of the paper, which advocates for the utilization of visual rationales to enhance existing language-only chain-of-thought methodologies. The exposition of the intuition is lucid, and Figure 1 effectively illustrates the concept.\n\nThe paper excels in devising a method to map textual inputs to visual rationales and subsequently using these visual intermediates to facilitate reasoning. This approach constitutes the paper's primary focus and is a significant contribution to the field."
            },
            "weaknesses": {
                "value": "1. The submission guidelines specify a maximum of 9 pages for the main text, yet this paper comprises slightly over 8 pages. This brevity raises concerns about the completeness and readiness of the work for publication.\n2. In Section 2.3 (\"Converting Symbols To Image\"), the exposition on SVG and FEN formats is overly succinct. It is unclear to the readers how these formats facilitate the effective translation of symbols into images.\n3. Regarding Figure 5:\n- The manuscript does not elucidate how the question text \"a line segment from (0.5, 3.4) to (3.5, -4.2)\" translates into SVG format. Is there a specific prompt used for this translation?\n- More importantly, the figure directly presents intersection points as intermediate answers without explaining the rationale or method behind their determination.\n4. Table 1 suffers from a lack of detailed analysis:\n- The term \"geometric shapes in each sample, representing the different difficulty levels\" is ambiguous. Providing illustrative examples would be beneficial.\n- While the comparison between \"CoI Acc\" with and without \"Img\" suggests the effectiveness of \"CoI Acc,\" it's puzzling why \"Img Acc\" is nearly 100%, yet \"CoI Acc\" remains low.\n- Importantly, there is a lack of clarity on the methodology used for calculating the similarity between the image generated by SyMLM and the ground truth.\n5. Pertinently, the experiments on the CommonSense dataset appear to test the compositional reasoning ability of LLaVA-13B rather than the efficacy of your proposed method, SyMLM. This creates a disconnect in the paper's narrative."
            },
            "questions": {
                "value": "1. In Section 2.2, the paper states, \"Subsequently, these images are converted into image embeddings by the image encoder. The embeddings are then concatenated with the text embeddings to generate the next token.\" I wonder if the image embeddings can be directly concatenated with the text embeddings without employing any adapter layers or fine-tuning mechanisms.\n2. Also in Section 2.2, you claim that \"In subsequent experiments, we observe that the accuracy of image generation approaches nearly 100%.\" Could you please elaborate on the metrics used to quantify the similarity between the generated and target images?\n3. In Section 2.3, the text mentions, \"The image is then used to count intersection points.\" Could you specify the methodology employed for counting the intersection points within an image in the context of this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Reviewer_bdXh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697960127602,
        "cdate": 1697960127602,
        "tmdate": 1699636507188,
        "mdate": 1699636507188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0IzDD5Xd6y",
        "forum": "FE6WxgrOWP",
        "replyto": "FE6WxgrOWP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests Chain of Images (CoI), an intuitive method to improve Large Language Model(LLM) and Vision Language Model(VLM)\u2019s complex reasoning ability. In contrast to Chain of Thoughs (CoT) which generates intermediate language descriptions for solving reasoning problems, CoI generates a series of images which serves as an intermediate representation, enhancing the model\u2019s reasoning capabilities in domains where visual interpretation can be helpful. CoI is implemented as a symbolic multi-modal model (SyMLM), which directly generates symbolic representations of images from language instructions and uses both image and text as input. The authors tested their method in three different domains : geometry, chess, and common sense, and have shown that the integration of images to texts achieves better reasoning capabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is well motivated and simple, as humans intuitively imagine and reason using images in a range of domains.\n2. The proposed method showed strong results compared to baselines in the tested domains."
            },
            "weaknesses": {
                "value": "1. Limited experiments\n     - The tested domains are domains in which image generation should intuitively help. However, it is plausible that there are domains where incorporating image is not significantly helpful (e.g. arithmetics). It would be better if the authors have tried their method in more diverse domains.\n     - In domains other than geometry, the LLM is generating only a single image. The authors should try more domains in which chain of image generation is needed. Moreover, in the geometric domain, the authors could compare their method against a simple baseline which generates the final image at once without chaining process. \n     - As the proposed method is proposing to also use image along with text, the authors should have tried more extensive experiments using VLM using / not using their method.\n\n2. Method is limited\n     - Using symbolic representation (SVG, FEN) for image generation is domain specific. Moreover, assuming such representation to be given is a strong assumption to make.\n     - Rather than fine-tuning LLM to generate images given a set of problems and corresponding images to be given as a training data, the authors can consider reducing the size of training data and simply try in-context learning.\n   - Most of the results are either based on the perfect image generation ability of LLM after fine-tuned, or ground truth image.\n\n\n3. Clarity of writing\n     - Although the authors included examples of common sense domain, it is not easy to understand the two tasks tested in common sense reasoning. For instance, the first task of determining the scene described in the text can be misinterpreted as a task in which VLM determines the best image given a set of single text description and multiple images.\n     - The authors didn't clearly state the results of a second task in common sense domain. The paper only contains two examples in Fig. 6 and Fig. 7."
            },
            "questions": {
                "value": "1. Can you elaborate the results of the second task in common sense domain? Current version of the paper only states \"while CoI can identify something unusual in all images\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5138/Reviewer_QAod",
                    "ICLR.cc/2024/Conference/Submission5138/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771901804,
        "cdate": 1698771901804,
        "tmdate": 1700741224216,
        "mdate": 1700741224216,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QrvfuSGcat",
        "forum": "FE6WxgrOWP",
        "replyto": "FE6WxgrOWP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_KgMJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5138/Reviewer_KgMJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method named Chain-of-Image (CoI) prompting, which aims to enhance the reasoning abilities of large language models (LLMs) by incorporating a chain of generated images as intermediate representations. The method leverages a symbolic multi-modal model (SyMLM) that can transform textual prompts into SVG format images, which are then used to support the text-based reasoning process. The authors conduct experiments across various tasks, including geometry, chess, and commonsense reasoning, demonstrating that the CoI method outperforms text-only LLM baselines. The concept is innovative; however, the experiments may need further refinement to more robustly substantiate the claims. The scores maybe raised if the main concerns are addressed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The approach of integrating a visual reasoning chain into the operation of LLMs is quite innovative. It mimics human problem-solving strategies, which often involve visualizing concepts and steps. From the figures, it's very intuitive that such a system may bring the reasoning process closer to human when perform complex reasoning tasks.\n2. The paper appears to be methodologically sound, with a clear and well-organized description of the experimental setup, datasets, and baseline comparisons. Details to reproduce the paper are also clearly illustrated.\n3. If the proposed method is as effective as the paper suggests, it has the potential to significantly impact how LLMs are used for complex problem-solving tasks, making them more accurate and versatile. Particularly in the domain of reasoning and natural language understanding, it can make LLMs more interpretable and powerful in tasks that require complex reasoning."
            },
            "weaknesses": {
                "value": "1. Lack of experiments comparing the diffusion-based model generated images vs the proposed more controllable image generation strategy. For example, a qualitative comparison can be shown in Figure 4 to show how the proposed method solve the issues presented. Besides, in Table 1/2, the ablations should also be shown to support the claim.\n2. In contribution, it says \"counting the intersection points of geometric shapes, images provide an intuitive representation of the relationships (such as spatial, topological, temporal, etc.) between the items\", but seems like only the spatial relationship is the primary point that's been validated in the the experiment 3.1\n3. There also lack important experiments, seems that only text-based baseline are included, how about compare with other multimodal large language models? is the CoI method still important?\n4. The details of the whole multimodal language model is lacked. Especially in Section 2.2, there supposed to be some vision encoder details and the training details. Although in Section3.2, it mentions clip-vit-large-patch14, the whole model details are missing."
            },
            "questions": {
                "value": "1. How is the training/testing data generated, it's quite confusing. E.g., \"We convert all of the 50,000 training samples to the form introduced in Section 2.3, while keeping the test set unchanged.\" What are the details of this conversion. And how about the n_train and n_test in Table 2. What are the statistics details in Table 1.\n2. Is the whole pipeline a LLaVA-based model? what is the detail of the text-image connector.\n3. How does this method compare with LLaVA? Is this method also benefiting multimodal large language models? or only get some advantage when compared with LLMs? Then is that really a fair comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5138/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699155652832,
        "cdate": 1699155652832,
        "tmdate": 1699636506965,
        "mdate": 1699636506965,
        "license": "CC BY 4.0",
        "version": 2
    }
]