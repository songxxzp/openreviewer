[
    {
        "id": "audv7xD6cs",
        "forum": "ClqyY6Bvb7",
        "replyto": "ClqyY6Bvb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a comprehensibe evaluation framework ChEF for evaluating Multimodal Large Language Models.  ChEF consists of four modular components and allows for versatile evaluations in a standardized manner by designing new \"recipes\". The authors conduct evaluation of nine MLLMs  across various scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. ChEF is modularly designed with four components, Scenario, Instruction, Inferencer, and Metric, which facilitates versatile evaluations in a standardized framework and easy set up pf new evaluations.\n2. ChEF evaluates six capabilities that a competent MLLM model should possess, through constructing corresponding evaluation pipelines from a ChEF Recipe. These capabilities have not been systematically evaluated in exisiting MLLM Benchmarks.\n3. The authors evaluate the generalizability of nine MLLMs across various scenarios and their composite capability for multimodal\ninteractions, and summarize valuable observations."
            },
            "weaknesses": {
                "value": "1. I am not certain if it is fair to incorporate current MLLM benchmarks into ChEF. These benchmarks have taken a significant amount of time to develop, so what is the core contribution of ChEF?\n2. Besides in-context learning, ChEF only evaluates single-image input. However, the comprehension of multi-image input is also an important assessment dimension for MLLMs."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "In my opinion, no ethics review are needed."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698561975214,
        "cdate": 1698561975214,
        "tmdate": 1699635943290,
        "mdate": 1699635943290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sWZjSlxAkT",
        "forum": "ClqyY6Bvb7",
        "replyto": "ClqyY6Bvb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission177/Reviewer_n4Xu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission177/Reviewer_n4Xu"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a comprehensive assessment framework for large multimodal models using  four modular components and six \"recipes\" that stem from desiderata. They then apply the proposed framework to several state of the art large models and present many interesting insights on their performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors demonstrate a good understanding of the problem and lay out a comprehensive framework.\n2. The overall proposed framework is wide ranging and thus leads to interesting insights.\n3. The authors have been thorough in their implementation and experiments."
            },
            "weaknesses": {
                "value": "1. The paper does not justify its choices in a principled manner. The overall framework has an ad-hoc feel to it. While the reference are comprehensive, there is not enough logic to back up why those six desiderata for example are chosen and why some others are not. The work comes across as an engineering requirements style work rather than a scientific paper. I am open to being convinced otherwise. The field is moving very fast so just seemingly brute force evaluation of a bunch of models is not going to be helpful.\n2. The writing needs to tone down the claims to being pioneering etc. Or at least back up such claims."
            },
            "questions": {
                "value": "1. What are the insights that drive your work? Please see the comments above on weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787847840,
        "cdate": 1698787847840,
        "tmdate": 1699635943193,
        "mdate": 1699635943193,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydH54y7f4M",
        "forum": "ClqyY6Bvb7",
        "replyto": "ClqyY6Bvb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a comprehensive evaluation framework for Multimodal Large Language Models (MLLMs). The newly proposed framework has four modules, Scenario, Instruction, Inferencer, and Metric; and existing evaluation benchmarks can be summarized as recipes of the proposed framework. The authors conduct large-scale evaluations and presents valuable observations in the paper.\n\nAfter rebuttal: I have read the rebuttal and I'd like to keep my scores."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper introduces a comprehensive evaluation framework for Multimodal Large Language Models (MLLMs). The newly proposed evaluation framework has a modular design, which allow it to recover various existing benchmarks with different recipes. Interesting observations are also presented in the paper."
            },
            "weaknesses": {
                "value": "Since the main contribution of this paper is introducing this new evaluation framework. I suggest the authors to add a section describing the system design/implementation of this framework in detail. It seems that such information is missing in the current draft."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699074123550,
        "cdate": 1699074123550,
        "tmdate": 1700777175141,
        "mdate": 1700777175141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yDgV2xVZr5",
        "forum": "ClqyY6Bvb7",
        "replyto": "ClqyY6Bvb7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission177/Reviewer_V16Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission177/Reviewer_V16Y"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes ChEF, a framework for evaluating Multimodal Large Language Models (MLLMs). The main idea is to instantiate a \u201cRecipe\u201d, called \u201cdesiradata\u201d, consisting of Scenarios (datasets), Instruction (how to pose questions such as in-context learning (ICE)), Inferencer (how an MLLM answers questions including Perplexity (PPL), Chain of Thought (CoT), and Multi-Turn), and Metrics.\n\nThey evaluate 9 MLLMs using 6 desiderata (9 Scenarios) that target measuring Calibration, In-context Learning, Instruction Following, Language Performance, Hallucination, and Robustness. See page 3 and Section 2.3 for more details."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- S1: The proposed ChEF framework is sound.\n\n- S2: The experimental results are conducted on multiple models and settings and quite comprehensive."
            },
            "weaknesses": {
                "value": "- W1: Significance, Related work, and Execution. While I generally like the work that attempts to connect the dots and organize previous work, this work falls short. I do not think that the ChEF framework itself is a significant contribution as the 4 components of the Recipes are normally what people usually think about when it comes to evaluation. Thus, IMO, the main contributions lie in the instantiations of these Recipes or desiderata and their experimental results. However, the significance of this part is unclear due to two reasons. \n\n  - W1.1: First, it is unclear both in the main text and in the supplement how this work is better than existing work in terms of \u201cscalability\u201d and \u201ccomprehensiveness\u201d (cf. the first paragraph of the intro). The paper has to put more emphasis on the discussion of related work in order for the reader to understand the significance.\n\n  - W1.2: Second, the desiderata in Section 2.3 themselves need more rationales/justification. Why do we care about these capabilities? Why do we instantiate them this way? For example, Hallucination consists of asking binary questions about the existence/absence of objects. Yet, this is not the only kind of hallucination. In general, it is unclear why the desiradata is what it is. \n\n- W2: Clarity: related to W1, the paper would benefit from better presentation of desiradata. Perhaps having a table that lists down the 4 components. Justify why this is \u201cversatile\u201d evaluation."
            },
            "questions": {
                "value": "Please clarify as much as you can on my comments in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699249010072,
        "cdate": 1699249010072,
        "tmdate": 1699635943036,
        "mdate": 1699635943036,
        "license": "CC BY 4.0",
        "version": 2
    }
]