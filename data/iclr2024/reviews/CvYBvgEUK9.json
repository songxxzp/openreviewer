[
    {
        "id": "8xx2nDpb15",
        "forum": "CvYBvgEUK9",
        "replyto": "CvYBvgEUK9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_vCGE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_vCGE"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a class of bilevel problems with a class of nonconvex lower-level problems. It establishes a strong connection between the landscape of the penalty reformulation and the original problem. This gives an interesting explanation about why the penalty method works even with nonconvex lower-level functions. Based on this, a single-loop stochastic first-order algorithm is proposed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I found Thm. 3.2 very interesting. It converts the differentiability problem of $\\nabla \\psi(x)$ into the problem of whether the partial derivative of $l(x,\\sigma)$ is commutable. It solves the open problem that was not solved by Chen et al. (2023b), Shen & Chen (2023), and  Arbel & Mairal (2022) in an elegant way.\n2. The algorithm works for the stochastic case, which is not analyzed in Chen et al. (2023b), Shen & Chen (2023), and  Arbel & Mairal (2022)."
            },
            "weaknesses": {
                "value": "1. (minor) The readability of the proof is not very good, with some typos. An incomplete list\uff1a\n*  Page 24: \"perturbations in $x,y,\\sigma$ can change gradients of Lagrangian only by order $O(\\delta)$ \". It seems that from the context $x,y,\\sigma$ should be $\\theta,w$.\n*  Page 24: After \"Next, we check that...\". It seems that $y^*$ in the second line should be $\\theta^*$.\n*  Page 26: After \"We list a few properties of...\". It seems $\\theta_0$ in the second line should be $y_0$.\n*  Page 26: After \"where we use ...\" and before \"continuity of \". It seems that $z_t^*$ should be $y_t^*$.\n\n2. (minor) Assumption 6, 7(1), 8 may not be easily verified in practice. But given their natural appearance in the proof, i think this point does not detract from the main contribution of this paper."
            },
            "questions": {
                "value": "1. Prop 3.1. How to get Prop. 3.1 from Thm. E.3? How does the additional ${\\rm Span}(\\,...\\,, \\nabla_y f(x,y^*))$ appear?\n2. Thm. 3.2. Can we derive the explicit form of $\\nabla \\psi(x)$? Or we can only define it implicitly as the limit of $\\nabla \\psi_{\\sigma}(x)$ when  ${\\sigma \\rightarrow 0^+}$?\n3.  Appendix E.6. Proof of Thm. 3.6. After \"Then, since we have Assumption 1, we get\", why we have $2 (g(x,y_{\\sigma}^*) - g(x,z_p^*)) \\ge \\mu \\Vert y_{\\sigma}^* - z_p^* \\Vert^2$. It seems like the QG condition in Karimi et al. (2016), but it seems like this reference only shows that it can be implied by the PL condition in the unconstrained case. But how we can get this in the constrained case? \n4.  Proof of Thm. E.3 in Appendix. Why we always have $\\Omega(\\delta) v = \\nabla^2_{\\theta w} f(w,\\theta^\\ast) {\\rm d}w$? What if $\\nabla_{\\theta w}^2 f(w,\\theta^\\ast)$ is zero, then the RHS is also zero?\n5.  Appendix B.2. Can the authors explain more about the necessity of using the Moreau envelope? What is the benefit of using the Moreau smoothing? What will happen if it is not used? Sorry, I don't fully understand the technical challenge 1. Why it fails if one simply estimate the value of $(\\lambda^*,\\nu^*,y^*)$ for a given $x$ and plug into the expression of $\\nabla \\psi_{\\sigma}(x)$ in Thm. 3.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Reviewer_vCGE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697703906360,
        "cdate": 1697703906360,
        "tmdate": 1699636065976,
        "mdate": 1699636065976,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dwkv3KIoG2",
        "forum": "CvYBvgEUK9",
        "replyto": "CvYBvgEUK9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_dzvn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_dzvn"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes bi-level stochastic optimization problems where the objectives on both levels are smooth but not necessarily convex.\nThe paper proves a connection between solutions of a penalized reformulation of the problem to solutions of the original objective. Based on this, the authors provide a first-order algorithm with a non-asymptotic guarantee."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Though I am not an expert on bilevel programming, the contribution seems solid and of significance.\nThe authors clearly situate the results with respect to previous work on the topic.\nThe paper is fairly well-written, and pretty easy to follow with most relevant details reminded and cited adequately."
            },
            "weaknesses": {
                "value": "The main weakness in my opinion is the sheer amount of assumptions made throughout the paper. While some are standard and minor, some seem to highly hint towards the desired results - especially Assumption 5-8. As a non-expert on the topic, I find it hard to assess whether the assumptions are indeed (approximately) minimal for the results to hold, and how standard they are in this field. They are somewhat unmotivated in my opinion, and possibly ad-hoc.\nThe authors should attempt at explaining why these assumptions are required, and moreover that the results do not simply follow from them in a relatively-trivial manner."
            },
            "questions": {
                "value": "- Assumption 1: Was this assumption considered in previous work?\nMoreover, the quantifiers are unclear: \"satisfies (4) for all $y$ that satisfies... with some positive $\\mu,\\delta,\\sigma_0$\". Do you mean that  there exist $\\mu,\\delta,\\sigma_0$ so that $y$ satisfies... *implies* (4)? This should be properly revised.\n- Assumption 3 vs. 4: It is assumed that $f,g\\to\\infty$ as $\\|y\\|\\to\\infty$, but then that $\\|y\\|\\leq O(1)$. Why aren't these in contradiction to one another?\n- Assumptions 5-8: As previously mentioned, can the authors elaborate on these assumptions? They are not well-enough motivated. Statements such as \"Assumption 6 helps ensure that the active set does not change when ... is perturbed slightly\" seems to suggest the desired consequence is almost assumed in the first place. Also, in particular, is Assumption 7.1 standard? It seems strong, and I am not aware of such an assumption in previous works (I may be wrong, though would like the authors to clarify).\n- High-level remark: I am not sure whether the authors decision do defer the entire algorithmic aspect of the paper to the appendix is preferable. Of course I acknowledge the strict page quota, and this may be inevitable in a lengthy work such as this, but it seems unsatisfactory that the only result mentioned in the main text is Theorem 3.6, which remains rather un-motivated without the algorithmic contribution. This becomes most clear in the conclusion, when the authors mention that their algorithm is simple, general, useful etc., though the reader did not encounter it at all in the main text. For the author's consideration.\n\nminor comments:\n- 1st paragraph after (P): \"continuously differentiable *and* smooth\" - are these synonymous?\n- \"since this issue is fundamental\" - this phrasing is unclear, I suggest explaining what this means (possibly informally).\n- Throughout the paper only \\citet is used, while \\citep is more adequate whenever the work is not part of the sentence.\n- Theorem 1.1 (Informal): the phrase \"at least one sufficiently regular solution path exists\" is unclear at this point. What is even a solution path (not previously mentioned or defined)?\n- mean squared smoothness condition mentioned without definition or ref. For example, the authors can add \"(as formally defined in Section ...)\".\n- typo: to appendix = to the appendix.\n- Assumption 2 involves a norm, which only later on (in the notation paragraph) is explained to be the operator norm. This should be clarified beforehand.\n- The normal cone is mentioned without definition, I suggest adding a brief reminder (even in a footnote, for example)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Reviewer_dzvn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753395953,
        "cdate": 1698753395953,
        "tmdate": 1699636065885,
        "mdate": 1699636065885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DGdDHvJ5YD",
        "forum": "CvYBvgEUK9",
        "replyto": "CvYBvgEUK9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_ocU5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_ocU5"
        ],
        "content": {
            "summary": {
                "value": "This work studies a penalty method for constrained bilevel optimization, where the objective functions are possibly nonconvex in both levels with closed convex constraint sets. While the penalty technique is not new, its landscape relationship with the original problem was not known. This paper, for the first time, characterizes the conditions under which the value and derivatives of the penalty function is $O(\\sigma)$ close to those of the hyperobjective function of the original problem, making the penalty method more useful theoretically. This paper then suggests efficient (stochastic) first-order methods for finding an $\\epsilon$-stationary solution, by optimizing the penalty formulation with $\\sigma = O(\\epsilon)$. In particular, under the small-error proximal error-bound (EB) condition, that is closely related to the PL condition, the proposed algorithm finds an $\\epsilon$-stationary solution in total $O(\\epsilon^{-7})$ oracle accesses, which is better than the existing method for a relatively easier PL optimization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides a theoretical justification of using the penalty method for bilevel optimization, which is not trivial as seen from Examples 1 and 2. In particular, this paper found minimal(?) conditions, Assumptions 1-4 and 7-8, to show that the value and derivatives of the penalty function are $O(\\sigma)$ close to those of the hyperobjective function of the original problem, which is certainly not trivial.\n\n2. Under the proximal EB condition, this paper presents a (stochastic) first-order method for efficiently finding an $O(\\epsilon)$-stationary solution of the penalty (saddle-point) problem, which is also an $O(\\epsilon)$-stationary solution of the original problem by the previous argument. This result is interesting as its rate improves upon that of the existing method for a similar PL saddle-point problem.\n\n3. This paper provides the first explicit formula of the gradient of the hyperobjective function of the bilevel optimization with multiple solutions for the lower-level problem (under Assumptions 5-6)."
            },
            "weaknesses": {
                "value": "1. Although the authors claim that they use nearly minimal conditions, they are not few, not simple, and are not guaranteed to satisfy in practice. (Providing an example satisfying those minimal conditions could be useful.)\n\n2. This paper becomes difficult to read towards the end, probably due to the page limit."
            },
            "questions": {
                "value": "1. Title: What do you mean by \"first-order stochastic approximation\"?\n2. Page 2: The third paragraph states that there are results under the uniform PL condition, which seemed to imply that there is a method for the PL condition, but the next paragraph claims that there is no algorithm finding the stationary point under the PL condition. Could you clarify this?\n3. Page 2: In the fourth paragraph, how about making the term \"landscape\" more specific? It was not clear to me here, although it becomes clearer later. \n4. Page 3: How about adding \"small-error\" in front of the \"proximal EB\"?\n5. Page 3: It is implied that Assumption 1 guarantees the local Lipschitz continuity of solution sets (Assumption 5), but it seems that it is not explicitly stated (with proof) anywhere. \n6. Page 3: PL and proximal EB conditions are almost identical, and you only assume the proximal EB within the neighborhood of solutions. Then, how were you able to get an improved rate over existing rate for the PL condition? Most of the explanations of the algorithm are deferred to the appendix, and it was not clear for what reason that the proposed algorithm is more efficient even with weaker condition. Could you comment on this?\n7. Page 7: How does Assumption 6 help the active set to not change much?\n8. Page 7: I was not able to follow Proposition 3.1 and its corresponding explanation. Could you help me better understand the context?\n9. Page 9: Can Theorem 3.6 be proven with Assumption 5 rather than Assumption 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699200783314,
        "cdate": 1699200783314,
        "tmdate": 1699636065814,
        "mdate": 1699636065814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FXZ9LoEc2b",
        "forum": "CvYBvgEUK9",
        "replyto": "CvYBvgEUK9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_fS1u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_fS1u"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the extension of first order methods from bilevel optimisation problems with convex objective to bilevel optimisation problems with non convex objectives"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The abstract is neat and the paper is dense and well written. The problem is clearly stated and there is a need for a solution which the authors provide."
            },
            "weaknesses": {
                "value": "I would be more straightforward and from the very beginning of the paper emphasize the narrative. My main concern has to do with the fact that the paper might be too dense for a conference. You either have to make it simpler/lighter or you should submit it to a journal. There does not seem to be enough space for you to give the complete details, or if you prefer, we miss some intuitive explanation (i.e there are too many details and the main thread is not always clear). Before providing formal statements, you should be able to explain in simple terms what the main ideas are (e.g. for Proposition 3.1.).  At this point, it appears as if you had a very detailed result and quickly tried to wrap it up as a conference paper.  You took some of the details and put some others in the appendices, but the general organization (the decision of retaining some results instead of others, such as Proposition 3.1) is unclear.. I would be in favor of reducing the amount of mathematical details (see my detailed comments below) and increasing intuitive explanations (at least in the main body of the paper).  You could do a much better job at focusing on your main results. An example of this is section 3 where the reader has to wait the very end of the section to discover the connection to the earlier sections (see my comments below)."
            },
            "questions": {
                "value": "A couple of typos:\n\n- Section 3.3. first line: \u201cvia finite differentation\u201d \u2014> \u201cvia finite differentiation\u201d \n- Section 3.3. \u201cwe can apply mean-value theorem\u201d \u2014> \u201cwe can apply the mean value theorem\u201d\n- If you check, on both page 3 (below Assumption 1) and page 8, you use the sentence \u201cthe crux of assumption 1 is the guaranteed (Lipschitz) continuity of solution sets\u201d\n- Page 8: \u201cWith a more standard assumption on the uniqueness of the lower-level solution and the invertability \u201d \u2014> \u201cinvertibility\u201d ?\n\n\n\nPage 1/2\n- You first introduce formuation (P) then you introduce formulation P_con which to me is exactly equivalent to P, isn\u2019t it ? If it is, I would not relabel it but simply emphasize the equivalence between P and this formulation. There are already many formulations. If some of those are exactly equivalent (like I suspect it is the case for P and Pcon, I would not introduce additional notations)\n-  Finally you introduce a relaxation of the constraint in P_pen. intuitively, we understand that for sigma small enough, this relaxation should recover P_con and this is what you formalize by introducing appropriate conditions. So far, this is clear. What I find less clear is the sentence \u201cAlthough finding the solution of (Ppen) is a more familiar task than solving (P) \u201d. For small \\sigma, from Theorem 1.1., there must be some conservation of difficulty (at least for some \\sigma). It would be helpful to have some more intuition on that (one or two sentences). I.e. it seems miraculous to end up with a formulation that suddenly becomes so much easier so solve. \n\nPage 2\n- In the formulation P_{pen}, I don\u2019t really understand how you can know the value of min_z  g(x, z) while you don\u2019t have any explicit solution for min_y g(x, y). This should be commented\n\nPage 3\n- The result of Theorem 1 is clearly interesting but it could be expected that a smaller value of sigma would enforce more weight on the penalty and hence ultimately recover the solution to (1)\n- You should define the notion of epsilon-stationnary point. In particular What is the meaning of epsilon in regard to sigma. When you introduce the notion of epsilon-solution you seem to replace the sigma in psi_sigma by the epsillon, but then the two appear simultaneously when discussing the oracle complexity\n- You use the terms \u201ccomplexity\u201d and \u201coracle complexity\u201d before clearly defining them. Are you talking about the number of iterations/steps? If yes, this should appear somewhere. The notion of \u201coracle complexity\u201d is also used in a number of different settings. What are the oracles you are referring to ? What function classes are you considering. This is not self contained (at least on page 3). I can see you clarify this in the statement of Theorem 1.2. but it should appear earlier. In fact I\u2019m wondering if you should not put Theorem 1.2 before Theorem 1.1. Also, is your oracle only given by the noisy/stochastic gradient or do you also assume access to the value of the function? The information that appear in section 3.4. should appear way earlier. \n- What do you mean by the sentence \u201cAssumption 1 holds only in a neighborhood of the lower-level solution \u201d ? do you mean that Theorem 1.1. only requires Assumption 1 to hold in a neighborhood of y^* ? Then you should clarify this in the statement of Theorem 1. \n\nPage 4\n- You mention the mean-squared smoothness condition before defining it. Either include it in the main part of the paper or remove the second part of the Theorem. The sentence \u201cfully single-loop manner\u201d is also unclear so I would be in favor of removing the second part of the Theorem (at least from the main body of the paper)\n\nPage 5\n- Why not use V for the value function and Y for the solution set ?\n- In section 3, we don\u2019t really understand what you are doing\n- I think you can spare some space by removing the definition of Lipschitz continuity from the main body of the paper (this is a relatively well known concept and it would free some space for some other (perhaps more crucial) explanations \u2014 see my other comments)\n\nPage 6\n- I would be in favor of a couple of additional explanatory sentences in Examples 1 and 2. E.g. just clarify the definition of S(x), i.e. S = {-1,1} except at 0 where it is the whole [-1,1] interval, or recall the definitions of S and psi\n- The whole introduction in section 3 is confusing. It is not really clear what you are trying to do do until the last sentences. I.e. we understand you want to show examples in which \\nabla \\psi(x) \\neq \\lim_{\\sigma\\rightarrow 0} \\nabla \\psi_{\\sigma}(x). But it is not completely clear why, especially given that you seem to already have introduced in assumption 1, a condition that ensures that \\nabla \\psi_{\\sigma} can be made arbitrarily close to \\nabla \\psi. \n- Under example 2, when you mention the continuity of the solution set. Do you mean continuity with respect to x ? \n- Before introducing examples 1 and 2, I would add a sentence insisting on the fact that in both examples, the non existence of $\\nabla \\psi$ comes from the discontinuity in $\\psi$. Also I known you don\u2019t have much space but it would help to have a plot of the solution sets for both examples (or perhaps briefly mention the solution sets for both cases ? This would help clarify the fact that the set is )\n\nPage 7\n- Although technically correct, the term \u201ckernel\u201d in the setence \u201cthe kernel space of the Hessian \u2026\u201d before Theorem 3.2. can be confusing especially in ML related papers. I would use nullspace instead of kernel space.\n- The central part of section 3, i.e. the part that connects this section to Theorem 1.1. is Theorem 3.2. This theorem should appear way earlier or at least you should clearly explain that you derive the conditions appearing in Theorem 1.1. I would rewrite section 3 as follows. Start by clearly indicating that you will derive conditions for the second relation in Theorem 1.1. to hold (i.e. the fact that \\nabla \\psi_{\\sigma}(x) is a O(\\sigma) approximation of \\nabla \\psi)\n- I would rewrite section 3.1. by giving more intuition on how you derive assumption 6. Just a couple sentences of explanation. (I think readability of the paper would benefit from for example, removing the definition of Lipschitz continuity and replacing it with a more elaborate explanation of the derivation of assumption 6)\n- I would remove Proposition 3.1 or I would make it simpler. The proposition is not clear and neither is its connection to the rest of the paper or its explanation (i.e. Geometrically speaking, perturbations in \u2026 ). \n\nPage 8\n- I would simplify section 3.3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699309101677,
        "cdate": 1699309101677,
        "tmdate": 1699636065750,
        "mdate": 1699636065750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PM2vh0X3f2",
        "forum": "CvYBvgEUK9",
        "replyto": "CvYBvgEUK9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_VYCb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1380/Reviewer_VYCb"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses bilevel optimization problems that can encompass constraints and nonconvex lower-level problems. It establishes an $O(\\sigma)$-closeness relationship between the hyper-objective $\\psi(x)$ and the penalized hyper-objective $\\psi_{\\sigma}(x)$ under the proximal-error bound condition when the errors are small. Using the penalty formulation, the authors develop fully first-order stochastic algorithms for finding a stationary point of $\\psi_{\\sigma}(x)$ with comprehensive non-asymptotic convergence guarantees."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "S1. The work is well motivated. Finding a simple yet effective method for large-scale lower-level constrained bilevel optimization problems is both intriguing and significant.  \n\nS2. The paper is well written and easy to follow. The illustrations in Example 1 and Example 2 are helpful in understanding the main difficulties.\n\nS3. The algorithm design is novel, suitable for a broader range of bilevel optimization problems that may involve constraints and nonconvex lower-level problems. Additionally, it provides a comprehensive non-asymptotic convergence analysis."
            },
            "weaknesses": {
                "value": "W1. I find the algorithm developed in this paper to be both interesting and important. It seems that the landscape analysis section is quite lengthy. Would it be possible to condense the content and incorporate the first-order algorithm into the main text?\n\nW2. There is a closely related paper that addresses general constrained bilevel optimization problems while assuming the strong convexity of the lower-level objective.\n\n[1]Siyuan Xu, Minghui Zhu. \u201cEfficient Gradient Approximation Method for Constrained Bilevel Optimization.\u201d AAAI 2023.\n\nW3. No numerical experiments are provided."
            },
            "questions": {
                "value": "Q1. What is the relationship between the $\\epsilon$-stationary point of $\\psi_{\\sigma}(x)$ and the $\\epsilon$-KKT solution of the constrained single-level problem $P_{\\mathrm{con}}$? Theorem E.5 establishes a one-directional relation. Is the converse of Theorem E.5 also valid? Note that in Theorem E.5, the $\\epsilon$-KKT solution of $P_{\\mathrm{con}}$ should be a pair $(x, y)$.\n\nQ2. Could the requirement of boundedness for $\\max_{x\\in\\mathcal{X},y\\in\\mathcal{Y}} |f(x,y)|=O(1)$ be relaxed or made less restrictive?\n\nMinor Comments:\n\n(1)On page 3, Theorem 1: It would be beneficial to clarify the meaning of $y^*(\\sigma)$.\n\n(2)On page 4, Assumption 3: Is it redundant to consider the coercivity of both $f(x,y)$ and $g(x,y)$ as $|y|\\rightarrow\\infty$, given that $\\mathcal{Y}$ is bounded according to Assumption 4?\n\n(3)On page 7, Definition 7: Does $\\lambda_{\\mathcal{I}}$ implicitely depend on $y$ in Equation (8)? What is the domain of $\\mathcal{L}_{\\mathcal{I}}$?\n\n(4)On page 7, Equation (9): There is a period missing at the end of Equation (9).\n\n(5)On page 9, Assumption 8: Does Assumption 8 implicitly assume that the active set does not change?\n\n(6)On page 17, Equation (13): There is a period missing at the end of Equation (13).\n\n(7)On page 18: check the definition of $g_{xy}^{k,m}$ and $g_{xz}^{k,m}$.\n\n(8)On page 19, C.1, Definition of $\\Delta_k^z$: \u201c$\\rho g(x, \\cdot)$\u201d should be \u201c$\\rho g(x_k, \\cdot)$\u201d.\n\n(9)On page 22, Equation (21): \u201c$D_v \\ell^*(x)$\u201d should be \u201c$D_v \\ell^*(w)$\u201d.\n\n(10)On page 27, Proof of Theorem 3.6, Line 8 from below: Should $\\sigma C_f$ be replaced with $2\\sigma C_f$?\n\n(11)On page 27, Proof of Theorem 3.6, Line 4 from below: $\\mu_g$ should be replaced with $\\mu$.\n\n(12)On page 30, Proof of Theorem E.5: It appears that in the last step of the proof, $g(x,y) - g^*(x)$ is bounded by $O(\\sigma^2)$. Would this observation be useful for improving the convergence rate?\n\n(13)On page 30, Equation (28): There is a period missing at the end of Equation (28)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1380/Reviewer_VYCb"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699435836368,
        "cdate": 1699435836368,
        "tmdate": 1699636065669,
        "mdate": 1699636065669,
        "license": "CC BY 4.0",
        "version": 2
    }
]