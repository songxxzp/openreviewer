[
    {
        "id": "0OfNlU5JG1",
        "forum": "dl34rOnbqJ",
        "replyto": "dl34rOnbqJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_Uc4h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_Uc4h"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a model for action anticipation based on the integration of attention-based (such as transformers) and auto recurrent (such as LSTM) mechanisms. Differently from previous works, the proposed method takes into account a history of previous hidden states rather than the last one when making predictions, thus relaxing the first-order Markovian assumption usually introduced in recurrent models. The proposed approach is evaluated on the main benchmarks for egocentric action anticipation. Results suggest that the method outperforms competitors when a single RGB modality is considered."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While the model proposes a few modifications to the attention mechanism, this ends up in a novel approach which outperforms previous works.\n\nIt is interesting to see that the proposed method works well with context length of up to 30s. This is not common in previous approaches and seems to be a promising direction for better exploiting past history."
            },
            "weaknesses": {
                "value": "1) PRESENTATION QUALITY\nThe quality of paper writing is not always up to standard. Some sentences are a bit overselling, unclear, or not adequate for scientific writing. Some examples:\n- \u201cby integrating gaze information within the observed frames\u201d: is the paper referring to a specific work here? As far I know gaze analysis for intention prediction has not been systematically investigated in egocentric vision.\n- \u201cUnlike action recognition, which primarily relies on patter recognition\u201d: this statement seems to imply that action anticipation does not rely on pattern recognition, which I don\u2019t think is an accurate statement (neural networks are anyway patter recognition machines)\n- \u201cour model can infer causation from abstract concepts\u201d: this statement is a bit overselling and does not seem to be shown/proved in any of the experiments. \n- \u201cour innovative model sets new performance benchmarks\u201d: I do not think this is accurate. It seems that the proposed model outperforms competitors by small margins. I would highlight instead that it may point out to a promising direction for future models.\n- in section 3, the set X is later referred to $X_{T-t_s:t}$, which is a bit confusing\n- It is not clear how equation (1) is an accurate description of existing methods, whether recurrent or transformer-based. In the equation, it seems that models following this formulation explicitly plug in the last observed action for predicting the future one. However, methods that directly predict future action do not do that (e.g., vanilla LSTMs)\n- In eq (2), is $\\hat y_i$ a probability or a predicted label?\n- Section 5.3.1 \u201cIAM demonstrated notable e enhancement over several formidable baselines\u201d.I would suggest to revise the use of \u201cformidable\u201d in this scientific context.\n- Throughout the paper, I could not find a clear motivation for the use of the term \u201cinductive\u201d in \u201cinductive attention\u201d. I think this could be clarified.\n\nThis are some examples. Overall, I would suggest a thorough review of the paper to improve presentation.\n\nLITTLE INSIGHT ON WHY THE MODEL WORKS\nWhile I appreciate the description of the model architecture in Eq (11)-(14), there is no discussion as to why the introduced modifications are adequate and what kind of processing they could be intuitively bring to the model. After reading the description, I felt there is little insight into why the attention mechanism is tweaked the way it is. Also, while ablation studies detail the weigh of each macro-component to performance (Table 6), it would have been interesting to see a more detailed ablation into the various modification introduced by each of the aforementioned equations with respect to a baseline attention architecture.\n\nMULTI-MODALITY\nThe proposed algorithm outperforms competitors when a single modality is considered, while some approaches outperform the proposed method in the presence of multiple modalities. It would have been interesting to see how the proposed approach does when multiple modalities are considered, even with a simple late fusion. This would shed some light into the generalizability of the approach to the use of signals other than RGB images."
            },
            "questions": {
                "value": "I found the paper overall interesting, but I think the quality of presentation is somewhat limited. Also, there is little insight into why the proposed modifications work.\n\nThe authors could clarify this latter aspect in the rebuttal, while the quality improvements can only be done in the camera ready, if the paper is accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698560301266,
        "cdate": 1698560301266,
        "tmdate": 1699636697729,
        "mdate": 1699636697729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cKvz1sdow3",
        "forum": "dl34rOnbqJ",
        "replyto": "dl34rOnbqJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the \"Inductive Attention\" mechanism, an approach to video action anticipation. Unique in its design, this method employs the class prediction from the prior step as the query for attention. The authors argue that this design allows the model to recognize many-to-many relationships more effectively. Experimental evidence demonstrates that the Inductive Attention model achieves state-of-the-art results on several large-scale datasets, highlighting its efficacy in predicting human actions within video content."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper reports commendable performance on benchmark datasets.\n2. The idea of utilizing the prediction from the previous step as the attention query offers a fresh perspective and holds intrinsic interest."
            },
            "weaknesses": {
                "value": "The core contribution of the paper revolves around leveraging the prior prediction as an attention query. It is natural to use the input frame feature, or hidden state as a query, but if using the previous prediction leads to significantly better performance, it would be noteworthy and might have wider applications in related fields. However, the current version of the paper lacks depth in discussing the implications and rationale behind this choice. The proposed method, as presented, may seem like an incremental architectural tweak that provides some improvement in a particular task, significantly limiting its impact. For the authors' assertion that \"class probability is a superior choice for attention\" to be compelling, it requires a more rigorous justification than what is currently provided. Merely pointing out performance gains does not substantiate this claim sufficiently."
            },
            "questions": {
                "value": "Please see the weaknesses section. Is there any experimental evidence that can show the potential expandability of the proposed method to other related CV tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Reviewer_nTno"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653852095,
        "cdate": 1698653852095,
        "tmdate": 1699636697619,
        "mdate": 1699636697619,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V6Tzel4mev",
        "forum": "dl34rOnbqJ",
        "replyto": "dl34rOnbqJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach for egocentric action anticipation by introducing an inductive attention module. This module is helpful in capturing longer, temporal, history information and resolves the forgetting nature of recurrent neural data by using a higher order information. For the inductive attention, the last/previous prediction is compressed using a learnable compression function and used as query. The before/history predictions are also compressed and used as key and the history recurrent states are used as value. The frame feature and the inductive attention value are aggregated together to form the current recurrent state. The method is evaluated on three datasets - Epic-kitchens-100, Epic-kitchens-55, EGTEA Gaze+"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The quantitative results are quite exhaustive and the results are shown on three egocentric datasets for action anticipation."
            },
            "weaknesses": {
                "value": "It seems that the technical contribution of the work is weak. While the motivation of adding longer, temporal history is appreciated, the inductive attention module itself does not yield much improvement for the task of anticipation and does not provide a strong signal to the model for modelling the long-term, temporal history context. There seems to be less significant improvement in the performance with the inductive attention mechanism module. For example, in Table 1, when comparing Swin-IAM with MeMViT 32x3 there is only a performance improvement of 0.4% on actions, almost none on verbs, and 0.2% on nouns."
            },
            "questions": {
                "value": "Suggestion: \n1. There can be a grammar check run on the paper text. For example, the first line of section 3, problem statement can be edited. Additionally, multi-modal and multi-model terms have been used interchangeably in abstract and results table. The last line of the abstract can also be checked - 'multi-modality models using only RGB visual inputs' whereas multi-modal approaches have more modalities than visual input. \n\n2. The association with object tracking in the introduction and Figure 1 also seems a bit misplaced as it seems for the reader that object tracking would be used for the task of action anticipation, which is not actually used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6336/Reviewer_omT8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723624197,
        "cdate": 1698723624197,
        "tmdate": 1699636697488,
        "mdate": 1699636697488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FGEkeoitOK",
        "forum": "dl34rOnbqJ",
        "replyto": "dl34rOnbqJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_gdBN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_gdBN"
        ],
        "content": {
            "summary": {
                "value": "Paper proposes a novel method which utilizes multiple S past actions anticipation results to improve the next action anticipation. Extensive experiments on 3 datasets and several analytic experiments on proposed model performance were conducted. Empirical results show competitive results against other State of The Arts approaches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Paper's motivations are clear and the proposed method is explained in sufficient details.\n2. Quality of experiment designs and analysis are excellent.\n3. Novelty/originality is good within the context of action anticipation."
            },
            "weaknesses": {
                "value": "1. Originality is somewhat limited as using prior predictions to condition the target prediction has been applied to other problems. See reference\n2. The choice of egocentric action anticipation problem for the proposed method is not well motivated. There is no inherent advantage of the proposed method for egocentric action anticipation, compared to other video prediction problems, e.g. physical interaction/dynamics, 3PV action anticipation, gaze anticipation/prediction etc.\n3. Significance is average. While the problem of action anticipation is interesting, it is not clear how the proposed method can be applied to other related problems.\n\nReferences\nDuan, J., Yu, S., Poria, S., Wen, B., & Tan, C. (2022, October). PIP: Physical Interaction Prediction via Mental Simulation with Span Selection. In European Conference on Computer Vision (pp. 405-421). Cham: Springer Nature Switzerland.\n\nVincent Le Guen and Nicolas Thome, \u201cDisentangling physical dynamics from unknown factors for unsupervised video prediction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11474\u201311484.\n\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 98\u2013106, 2016.\n\nZhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J. (2017). Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4372-4381)."
            },
            "questions": {
                "value": "1. Please explain the motivation for applying the proposed technique to egocentric videos only. There is no clear reason why the proposed method cannot be applied to other video prediction tasks, e.g. 3rd person videos, physics interactions, gaze prediction etc.\n\n2. I will be interested to see the results comparison for S=1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765161861,
        "cdate": 1698765161861,
        "tmdate": 1699636697350,
        "mdate": 1699636697350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A7QPUqPDLS",
        "forum": "dl34rOnbqJ",
        "replyto": "dl34rOnbqJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_2chi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6336/Reviewer_2chi"
        ],
        "content": {
            "summary": {
                "value": "The paper produces an Inductive Attention Model (IAM) for egocentric video action anticipation. The model melds recurrent and attention\nmechanisms to explicitly employ prior anticipation results to refine subsequent action predictions. This design allows the model to form higher-order recurrent states and make current predictions based on extended historical data. Experiment results on several action anticipation datasets show that the proposed model surpasses most multi-modality models using only RGB visual inputs, showing the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed IAM architecture utilizes prior predictions as part of the attention mechanism. This allows for the aggregation of higher-order recurrent states, which is an advancement over traditional first-order recurrent models.\n2. IAM achieves competitive performance on several datasets with relatively fewer parameters.\n3. IAM achieves better performance on unseen classes on EK100, indicating good generalizability."
            },
            "weaknesses": {
                "value": "1. In Table 6, the ablation study shows that one of the major designs of this paper (predictions as prior) doesn't play a major role in the final performance improvement. The performance improvement is highly attributed to some design choices like a better backbone and class weighting.\n2.  Lack of analysis and visualization of the proposed mechanism. For example, how do previous predictions affect future action anticipation? \n3. Is the proposed model able to also handle the long-term action anticipation tasks (i.e. predicting multiple future actions) defined in Ego4D (grauman2022ego4d) and EgoTOPO (nagarajan2020ego)."
            },
            "questions": {
                "value": "See weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699224554625,
        "cdate": 1699224554625,
        "tmdate": 1699636697234,
        "mdate": 1699636697234,
        "license": "CC BY 4.0",
        "version": 2
    }
]