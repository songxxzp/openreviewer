[
    {
        "id": "7OMyRcaeYX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_3Wy1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_3Wy1"
        ],
        "forum": "cNwugejbW6",
        "replyto": "cNwugejbW6",
        "content": {
            "summary": {
                "value": "Inspired by the bio-neverous system of fruit fly and existing work bio-hash, this paper proposed a data-dependent hashing method dubbed SoftHash, which is characterized in three aspects: (1) a novel algorithm what can generate sparse and yet discriminative high-dimensional hash codes; (2) a mechanism that combines a Hebbian-like local learning rule and the soft WTA; (3) evaluation on image retrieval and word similarity search tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) A novel algorithm: The proposed SoftHash is interesting with simple and effective interpretable learning mechanism.\n(2) Solid experiments: two different kinds of tasks, image retireval and word similarity search, are designed to validate SoftHash's good performance in compact semantic embeddings."
            },
            "weaknesses": {
                "value": "(1) There are many places that were not clearly explained, such as, what is ConvHash? what is the difference between SoftHash-1 and Soft-Hash-2?\n(2) Why not just choose SH and ITQ? there are more similar papers such as SGH [1], DGH [2], COSDISH [3]?\n(3) W.r.t. others, please refer to the Question part.\n\n\n[1] Qing-Yuan Jiang, Wu-Jun Li: Scalable Graph Hashing with Feature Transformation. IJCAI 2015: 2248-2254\n[2] Wei Liu, Cun Mu, Sanjiv Kumar, Shih-Fu Chang: Discrete Graph Hashing. NIPS 2014: 3419-3427\n[3] Wang-Cheng Kang, Wu-Jun Li, Zhi-Hua Zhou: Column Sampling Based Discrete Supervised Hashing. AAAI 2016: 1230-1236"
            },
            "questions": {
                "value": "I have several questions listed as follows:\n(1) What is the main differences between the bio-inspired hashing methods with sparse-and-high-dimensional {0,1}-vectors and the conventional hashing methods with dense-and-low-dimensional {0,1}-vectors?\n(2) How does the bio-inspired hashing codes capture the data samples' semantics?\n(3) With respect to the storage and computation, how does the bio-inspired hashing approaches realize economic memory and fast retrieval/semantic computing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697356191673,
        "cdate": 1697356191673,
        "tmdate": 1699636166016,
        "mdate": 1699636166016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nLMW0ufj8l",
        "forum": "cNwugejbW6",
        "replyto": "cNwugejbW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_GrZJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_GrZJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new data-dependent hashing algorithm called SoftHash based on a Hebbian-like update rule from biology. The method iteratively trains projection weights and biases using both the input data $x$ and output $y$, and generates sparse binary codes by a topk filter on $y$. Experiments show that SoftMax performs better than several previous methods in preserving the cosine similarity between the data points, as well as in dowsnstream similarity search tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The presentation is in general clear and easy to follow. There are some grammar issues, e.g., \"Such that, ...\". A proofread is suggested.\n\nIn my understanding, the main algorithmic contribution is to improve the prior BioHash algorithm by replacing the hard thresholding with a softmax function. The idea seems plausible and should work, as one can always add a hyperparameter to softmax to make it a hard thresholding. The method introduces more flexibility."
            },
            "weaknesses": {
                "value": "1. Some similar contents have already appeared in other works. For example, the paragraph saying \"$w_i$ is implicitly normalized\" is similar to the text around Eq (3) in the BioHash paper (except that $Topk(y)$ is replaced by $y$). Eq (7) also has similar form in the BioHash paper. A clear comparison should be made in the paper regarding the difference and connections.\n\n2. The design is not very well motivated and rather simple. If we write Eq (2) in your notation for SoftHash, then Oja's rule should be $\\eta u_j(x_j-w_j^iu_j)$. The difference with Eq (3) is that the first $u_j$ is replaced by $y=softmax(u)$. The difference is that you added a non-linear softmax function on the scalar. Why is softmax used, can we use other functions? Is there also \"biological interpretation\" for that?\n\n3. From the experiments, it seems that the SoftHash requires some delicate tuning on several floating parts. The choice of hyperparameters and tuning strategy need further explanation. Ablation study is also needed to understand the performance of the proposed method.\n(1) SoftHash-1 and SoftHash-2 are not explained in the main paper, please revise. From the appendix, it seems that they refer to 2 different weight initialiation methods (uniform and Gaussian). Why is the performance gap so big given that this approach is learning-based? This is confusing to me. Also, why is the batch size set as 3584? It that carefully picked or just a random number?\n(2) For BioHash, why you used $p=4$ and $\\triangle=0.4$ and $0.3$? For SoftHash, how is the temperature paramter $T$ chosen? Did you tune these parameters?"
            },
            "questions": {
                "value": "1. Does BioHash have the bias term $b$ in the model? How important is this bias term? Any empirical results for illustration?\n2. I think \"ConvHash\" is usually referred to as \"SimHash\" or simply \"LSH\". Perhaps changing the name would be better.\n3. Is the \"Sokal and Michener similarity function\" simply the Hamming similarity? If so, using the later is more straightforward.\n4. In Table 2, what are the metrics when the code length is larger (256, 512)?\n5. Is there an interpretation of Oja's rule in terms of gradient-based optimization? It seems that Eq (2) is the gradient of $||x-wy||^2$? Is it related to the clustering formualtion you mentioned?\n6. There is a recent paper [SignRFF: Sign Random Fourier Features, NeurIPS 2023] that extends SimHash to non-linear features. It may perform better than SimHash on some datasets. Please consider adding this baseline as well as deep hashing methods to make the experiments stronger."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Reviewer_GrZJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698135616882,
        "cdate": 1698135616882,
        "tmdate": 1700793921535,
        "mdate": 1700793921535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KWZUKMqTJx",
        "forum": "cNwugejbW6",
        "replyto": "cNwugejbW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_xpXr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_xpXr"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a SoftHash, which is a data-dependent hash algorithm. It produces sparse and high-dimensional hash codes. Also, unlike other algorithms such as BioHash, this paper did not use a hard winner-take-all(wta) mechanism. They adopted a soft wta mechanism with an exponential function. The authors conducted experiments on similar image searches and semantical similar word searches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposed a new high-dimensional hashing algorithm. The authors use soft WTA to enable the learning of weak-correlated data, which differs from existing work. \n\n The paper demonstrated the differences between their algorithm and one existing algorithm BioHash.\n\nThe explanation of SoftHash is clear."
            },
            "weaknesses": {
                "value": "The experiment part was not clear to me. Many important details are in the appendix or are missing, such as the train/test set split.  I keep the questions in the next section."
            },
            "questions": {
                "value": "1. Appendix D.1 mentioned that for test: \"10 query per class for cifar100, 100 for cifar10\".  The test size seems small to me. In BioHash, they used 1000 queries per class for cifar10. Any reason why you chose a smaller query size?\n2. Appendix D.1 mentioned that \"Ground truth is the top 1000 nearest neighbors of a query in the database, based on Euclidean distance between pairs of images in pixel space. \" whereas, in BioHash, the ground truth is based on class labels. For me, it makes more sense to use class labels. Is there any reason for using Euclidean distance?\n3. Appendix D.1 mentioned that \"Output dim is 2000\". It is smaller than the 3072-dim input in cifar10/cifar100. Can you explain if this still satisfies high-dimensional hash?\n4.  SoftHash-1 and SoftHash-2 used different parameter initialization. The conclusion is that SoftHash2 is better than SoftHash1. But any intuition as to why it's better? Instead of putting them in the main result table, you can put different initialization results as an ablation study. It's really confusing when looking at the main result table because there is no definition of SoftHash 1 in the main paper.\n5. I couldn't find train/test split and output dimensions for word search experiments.\n6. The reference for \"Can a fruit fly learn word embeddings?\" needs to be updated: not arxiv preprint; it was published at iclr 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2335/Reviewer_xpXr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698440702045,
        "cdate": 1698440702045,
        "tmdate": 1699636165873,
        "mdate": 1699636165873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yo76hmlbWq",
        "forum": "cNwugejbW6",
        "replyto": "cNwugejbW6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_qurX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2335/Reviewer_qurX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes SoftHash, a data-dependent hashing algorithm. To overcome the randomness of LSH, SoftHash learns the hashing projection function using a Hebbian-like learning rule coupled with the idea of Winner-Take-All (WTA). This allows SoftHash to adapt to the input data manifold and generate more representative hash codes. The authors also introduce a soft WTA rule, whereby the non-winning neurons are not fully suppressed in the learning process. This allows weakly correlated data to have a chance to be learned, which further improves the semantic representation capability of SoftHash."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Here are some specific strengths of the paper:\n\n1. Overall, the paper is well-written and presents a novel and effective hashing algorithm. The authors provide a clear motivation for their work, and their experimental results are convincing. \n\n2. The authors evaluate SoftHash on some real-world datasets and tasks, and their experimental results demonstrate that SoftHash significantly outperforms some baseline methods."
            },
            "weaknesses": {
                "value": "1. It would be better to include more baselines including Mongoose paper's learnable hash functions[1].\n\n2. Maybe the authors could justify more on the theoretical analysis of the motivation of Softhash. Similar studies would be [2]\n\n[1] MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training\n[2] Learning to Hash Robustly, Guaranteed"
            },
            "questions": {
                "value": "1. How to provide search quality guarantees for Softhash in retrieval?\n\n2. What is the convergence rate of the SoftHash learning process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863663606,
        "cdate": 1698863663606,
        "tmdate": 1699636165796,
        "mdate": 1699636165796,
        "license": "CC BY 4.0",
        "version": 2
    }
]