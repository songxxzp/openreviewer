[
    {
        "id": "wxPurMCUKL",
        "forum": "vlQ56aWJhl",
        "replyto": "vlQ56aWJhl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new learning rule to train spiking neural networks. The idea is based on a three-factor structure, but using BPTT and STDP as its components. The STDP-based eligibility trace function scales with $n$ and is temporally local (does not scale with $T$), which is an improvement over existing methods. This method, referred to by the authors as S-TLLR, has an additional non-causal component which scales with $n$, just like the causal component, and therefore does not affect its scaling with space and time. Experiments and benchmarks on numerous datasets reveal the advantage of this non-causal learning component.\n\nI am generally positive about this work in regards to the new proposed method and how it improves training of spiking networks. I hope that authors can clarify any misunderstandings I may have in the weaknesses section and I am very willing to adjust my score in the rebuttal."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The theoretical scaling advantage is highly relevant and important to the spiking neural network community. Using an STDP-based eligibility trace function also lends to biological plausibility, which has relevance to neuroscience audiences."
            },
            "weaknesses": {
                "value": "I am doubtful of both main claims, on (1) temporal locality and (2) improvements from non-causal terms.\n\n(1) The temporal locality property of this method is unconvincing. In Figure 1, my naive understanding is that it is possible to simply truncate both BPTT and STDP methods in the same way S-TLLR is truncated using equation (11). In other words, all methods can have temporal locality. The only way to truly claim that the proposed method does not scale with time, is by using both BPTT and STDP (and perhaps even other existing methods) with this truncation and see if S-TLLR learns faster or if other methods fail to learn the objective. \n\n(2) The improvement from non-causal terms is similarly highly confounded by the secondary activation functions in equations (14-17). Suggestions for fair experiments could be:\n- universally use the same secondary activation function across all tasks, or use all 4 activation functions for all tasks\n- apply the same activation functions to other methods  \n\nTo be very clear, I understand that S-TLLR is compared across different values of $\\alpha$ within the same secondary activation functions, but it is not clear if this behavior is task and function specific. For example, dataset A and secondary function X could give better results with non-zero $\\alpha$, while dataset B with secondary function X or dataset A with secondary function Y has better results with $\\alpha = 0$. \n\n(3) It is also not clear how the method works in the recurrent neural network task. If I were to incorporate causal recurrent gradients in Figure 1, that would correspond to red lines being drawn from $u[t]$ to $y[t-1]$ (and others), which means most terms with have red and blue lines in parallel.  \n\n(4) The recurrent term in equation (1), while true and makes the equation general, simply disappears and lacks coherence and continuity with all future equations where the narrative centers around a feedforward network. For example, equation (4) has no recurrent term. This should be stated in the text somewhere or removed."
            },
            "questions": {
                "value": "Should blue terms in Figure 1 also extend beyond $t-2$ (with three dots) just like the red terms?\n\nWhile theoretical scaling arguments are convincing, there are many factors underlying number of computations. How are the 1.1x, 4x and 10x claims actually made? Was it done by recording the number of floating point operations? More information is needed to substantiate these claims. The actual amount of time taken to train the networks is also an important metric to include as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Reviewer_mZie",
                    "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614422396,
        "cdate": 1698614422396,
        "tmdate": 1700807954061,
        "mdate": 1700807954061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ik2A66ns5H",
        "forum": "vlQ56aWJhl",
        "replyto": "vlQ56aWJhl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an STDP-based learning algorithm that focuses on SNN training from the memory efficient perspective. The proposed algorithm has shown reduced complexity on the event-based dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method shows reduced time complexity, it is natural that the STDP-based learning requires less memory compared to BPTT with gradient surrogation. The proposed algorithm seems hardware friendly with discrete operations."
            },
            "weaknesses": {
                "value": "**W1:** Insufficient experiments: I understand that the event-based computer vision tasks are suitable for spiking neural networks, but I think the dataset reported in this paper is not comprehensive enough. In addition to the popular DVS-CIFAR10 and DVS-Gesture, N-CalTech101, and NCARs are also adopted in prior works [R1] as benchmarks. However, these results are missing in the paper. \n\n[R1] AEGNN: Asynchronous Event-Based Graph Neural Networks, CVPR, 2022.\n\n\n**W2:** Since the proposed method claims that the conventional BNTT is memory expensive, it is important to demonstrate the memory-accuracy comparison between the proposed method and BNTT (e.g., GPU Memory) \n\n**W3:** Some recent papers and SoTA methods are not cited in this paper: \n\n[R2]: Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting\n\n[R2]: Differentiable Spike: Rethinking Gradient-Descent for Training Spiking Neural Networks, NeurIPS'21\n\n[R4]: Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation CVPR 2022\n\n**W4:** The methodology section should be elaborated more. Based on Figure 1, S-TLLR introduces the incoming gradient $\\partial L / \\partial y$ on top of discrete STDP. What is the theoretical advantage (or intuition) of doing that?"
            },
            "questions": {
                "value": "Please refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Reviewer_FGQ4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641223483,
        "cdate": 1698641223483,
        "tmdate": 1699636932127,
        "mdate": 1699636932127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "67EJtdaPHR",
        "forum": "vlQ56aWJhl",
        "replyto": "vlQ56aWJhl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces S-TLLR, a novel learning rule for Spiking Neural Networks (SNNs) aimed at efficient online learning on resource-constrained edge devices. S-TLLR draws inspiration from Spike-Timing Dependent Plasticity (STDP) and utilizes both causal and non-causal relationships for synaptic weight updates, maintaining constant memory and time complexity. Through extensive experimentation, the authors demonstrate that S-TLLR achieves comparable accuracy to traditional methods like BPTT but with significantly lower computational demands. The paper's contributions are highlighted by the improved generalization and performance of SNNs on a variety of event-based tasks\u2014including image and gesture recognition, audio classification, and optical flow estimation\u2014and the validation of S-TLLR's efficacy across multiple network topologies, marking a step forward in deploying energy-efficient intelligence in real-world applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. S-TLLR is a groundbreaking approach that successfully trains SNNs with high efficiency, addressing the temporal and spatial credit assignment challenge that is inherent in such networks.\n2.  By incorporating principles from STDP, S-TLLR aligns closely with biological neural processes, potentially unlocking more natural learning patterns and efficiencies.\n3. S-TLLR successfully integrates both top-down modulation and the local algorithm.\n4. The proposed learning rule maintains constant time and memory complexity, which is a significant advancement for deploying SNNs on edge devices where resources are constrained."
            },
            "weaknesses": {
                "value": "1. The complexity was estimated, but the real energy consumption/efficiency haven't been calculated/tested.\n2. While BPTT could work on much deeper SNNs, how about S-TLLR? Could it be extended to larger models/datasets?"
            },
            "questions": {
                "value": "Please see the weaknesses:\n1. Could the energy consumption/efficiency be calculated/tested.\n2. While BPTT could work on much deeper SNNs, how about S-TLLR? Could it be extended to larger models/datasets, such as CIFAR100?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7662/Reviewer_H5rp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832170323,
        "cdate": 1698832170323,
        "tmdate": 1699636931996,
        "mdate": 1699636931996,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u0iEpeEAOG",
        "forum": "vlQ56aWJhl",
        "replyto": "vlQ56aWJhl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_KFXk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7662/Reviewer_KFXk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new learning rule for Spiking Neural Networks. This rule has low linear memory complexity and quadratic time complexity in terms of number of neurons. Moreover, the proposed learning algorithm incorporates a non-causal learning term,  inspired by Spike-Timing-Dependent Plasticity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Evaluation is done on variety of tasks;\n2) Paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "My main concern is that the method considered in the paper (S-TLLR) is very similar to OTTT[1]: \n\n1) OTTT has the same learning rule as S-TLLR except that additionally S-TLLR leverages non-causality and few other minor differences. But this non-causal term doesn\u2019t help S-TLLR consistently based on Fig. 2;\n2) S-TLLR has the same time and memory complexity;\n3) S-TLLR doesn\u2019t outperform OTTT. \n\n[1] Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, and Zhouchen Lin. Online Training Through Time for Spiking Neural Networks, NeurIPS 2022"
            },
            "questions": {
                "value": "1) Can the authors list all the differences between OTTT with S-TLLR methods?\n2) In the paper, it is mentioned that OTTT applies learning rules at each forward pass, whereas S-TLLR enforces the learning rule at every fourth forward step. Could the authors test the performance if S-TLLR's learning rule was applied at each forward pass, similar to OTTT?\n3) Can the authors do ablation study taking OTTT model as a reference starting point? The study would systematically integrate modifications that transition the model towards the S-TLLR approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "--"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698877847564,
        "cdate": 1698877847564,
        "tmdate": 1699636931889,
        "mdate": 1699636931889,
        "license": "CC BY 4.0",
        "version": 2
    }
]