[
    {
        "id": "kNtMk1AmcW",
        "forum": "qB0IV2DpeS",
        "replyto": "qB0IV2DpeS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_mpAd"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors develop a novel method called Byz-VR-MARINA-PP, which can allow partial participation and have Byzantine robustness simultaneously. The convergence results of Byz-VR-MARINA-PP for non-convex objectives and objectives that satisfy PL conditions are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "(1) The paper is generally well-written and not hard to understand. \n\n(2) A rigorous theoretical analysis is provided in this paper. Although I did not check the details, the proof seems to be correct."
            },
            "weaknesses": {
                "value": "However, there are also some weaknesses, as listed below.\n\n(1) The main differences between Byz-VR-MARINA-PP and Byz-VR-MARINA are that Byz-VR-MARINA-PP adopts gradient clipping and thus allows partial participation (PP). There are typically two benefits of PP, i.e., tolerating inactive clients and accelerating training processes. However, Byz-VR-MARINA-PP seems to perform poorly in either of the two aspects.\n\n(1a) As presented in Algorithm 1, in Byz-VR-MARINA-PP, the clients are sampled by the server before each round. Therefore, if a selected client becomes inactive, the whole training process will be blocked. In other words, Byz-VR-MARINA-PP cannot tolerate inactive clients.\n\n(1b) All clients will participate in the $k$-th training round if $c_k=1$. That is to say, all clients will participate in the training per $1/p$ rounds in expectation. I understand that $p$ is typically small. However, it will also greatly limit the acceleration effect of PP since in federated learning (especially in cross-device federated learning), the fraction of selected clients in each round is usually small.\n\nGiven the reasons above, could the authors specify what benefits the partial participation mechanism can bring?\n\n(2) The computation of full gradients is time-consuming. Moreover, it is unknown whether the gradient clipping is empirically compatible with the PAGE estimator. I strongly suggest the authors empirically test the performance of the proposed method on some federated learning benchmarks such as LEAF [1].\n\n[1] Caldas, Sebastian, et al. \"Leaf: A benchmark for federated settings.\" arXiv preprint arXiv:1812.01097 (2018)."
            },
            "questions": {
                "value": "Please see my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490013984,
        "cdate": 1698490013984,
        "tmdate": 1699636136760,
        "mdate": 1699636136760,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dw6ENW7p0g",
        "forum": "qB0IV2DpeS",
        "replyto": "qB0IV2DpeS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of partial participation in Byzantine robust algorithms for distributed learning. The authors introduce gradient clipping to limit the influence of Byzantine workers in rounds where they form a majority in the set of selected participants. They prove convergence rates, for a general algorithm featuring variance reduction and communication compression, and claim to match state-of-the-art theoretical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of partial participation is not well understood in Byzantine robust machine learning, and this paper makes a promising step towards solving it by introducing gradient clipping. Also, the use of the latter method is novel in this context.\n\n2. The technical content and proofs are sound."
            },
            "weaknesses": {
                "value": "My main concerns revolve around practicality, clarity, related work review, and assumptions.\n\n### A. Practicality: \n* A.1. A major weakness of the paper is the absence of experimental results. I would expect at least experiments on simple tasks, given that the only addition in the proposed algorithm (compared to previous works) is gradient clipping, which is simple to implement.\n\n* A.2. An important weakness in the theoretical analysis is the choice of the clipping parameter. For example, in Theorem 3.1, the clipping parameter $\\lambda_{k}$ depends on the maximum local smoothness constant, the computation of which can be highly impractical.\n\n* A.3. For the variance reduction method employed to have a gradient oracle cost comparable to SGD, $p$ needs to be in the order of $\\frac{1}{m}$ where $m$ is the number of samples per worker. However, my concern is that the excess (non-vanishing) term in (6) of the main theorem would increase proportionally to $m$, which is untight following the existing lower bounds, e.g. Karimireddy et al. (2022).\n\n### B. Clarity:\nThere are many clarity-affecting issues in the paper, which make the submission seem rushed:\n * Several quantities are undefined before they appear: $S_k$ and $g^k$ in the second paragraph of Section 2, $G^k_C$ in the first equation of Section 3\n* How is $g^k$ initialized in Algorithm 2? Does arbitrary initialization work in theory?\n* $n \\choose k$ is incorrectly denoted in the second paragraph of Section 3, and correctly denoted elsewhere.\n* The last sentence in Section 2 seems to be in conflict with Algorithm 1. In the latter, clipping is also performed at the worker level with probability $1-p$.\n\n### C. Related work:\n\nC.1. An important piece of related work is missing from the paper. Data & Diggavi (2021) have tackled the problem of partial participation (and local steps) in Byzantine robust distributed learning. It is essential to include a comparison with their work.\n\nReference: Deepesh Data and Suhas Diggavi. Byzantine-resilient high-dimensional SGD with local iterations on heterogeneous data. ICML 2021.\n\nC.2. Some claims regarding related work, in the paragraph following Definition 1, are inaccurate: a standard aggregator (coordinate-wise trimmed mean) satisfies Definition 1.1 because it satisfies an even stronger robustness criterion as shown by Allouah et al. (2023). Please include this in the paragraph. Moreover, using Bucketing (Karimireddy et al., 2021) is known to amplify the Byzantine fraction, and this may be problematic when considering partial participation.\n\n### D. Assumptions:\n\nSome assumptions are poorly justified: there is no justification for why \"popular [...] robust aggregation rules presented in the literature\" verify Assumption 1. A formal, even simple, justification is important because the assumption seems necessary for the convergence theory."
            },
            "questions": {
                "value": "I am willing to raise my score if the authors address the weaknesses above. In particular:\n\n1. How do you set the clipping parameters in practice, when you cannot compute smoothness constants? (see A.2)\n\n2. How do your results compare to the work of Data & Diggavi (2021)? (see C.1)\n\n3. If we constrain the oracle cost to be of the same order as SGD, is the excess term in the convergence upper bound tight? (see A.3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_Axn5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765085279,
        "cdate": 1698765085279,
        "tmdate": 1699636136637,
        "mdate": 1699636136637,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o3duRvDpmV",
        "forum": "qB0IV2DpeS",
        "replyto": "qB0IV2DpeS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a robust distributed algorithm against Byzantine attacks that allows partial client participation. While previously proposed methods require the participation of all clients to compute the aggregation rule and have a convergence guarantee, the proposed algorithm allows partial participation using gradient clipping and therefore limits the impact of the Byzantine clients, even if they form a majority in the set of subsampled clients at a given round. The authors provide a convergence guarantee for the proposed algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clear and easy to follow.\n- To the best of my knowledge, this is the first paper to allow partial participation for a robust distributed algorithm against Byzantine attacks."
            },
            "weaknesses": {
                "value": "- Given that the main motivation for this paper is to allow partial participation because it is more natural in practice, as the authors point out, I would have expected to see some practical experiments to see how gradient clipping actually allow partial participation (and thus the sampling of a majority of Byzantine clients in some rounds) while maintaining good performance. It seems to me that even if clipping can control the impact of Byzantine clients, rounds where they are in the majority will still penalize learning. Do the authors have any insights or perhaps experiment results on the performance of the proposed algorithm?"
            },
            "questions": {
                "value": "In the algorithm, it is said that the clipping levels $\\lambda_k$ are given as inputs, how is it possible since they depend on the value of $x^{k+1}$ and $x^k$ ?\n\nHow are the gradients clipped at the first iteration since $\\lambda_0$ is not defined?\n\nCan the authors explain why full participation Is needed in some rounds? Would it be possible to avoid full participation and use only partial participation in each round?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2050/Reviewer_cyK5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797365506,
        "cdate": 1698797365506,
        "tmdate": 1699636136518,
        "mdate": 1699636136518,
        "license": "CC BY 4.0",
        "version": 2
    }
]