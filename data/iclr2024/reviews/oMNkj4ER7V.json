[
    {
        "id": "UZptIADEBl",
        "forum": "oMNkj4ER7V",
        "replyto": "oMNkj4ER7V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
        ],
        "content": {
            "summary": {
                "value": "Many problem settings in Bayesian optimization (BayesOpt) require robust solutions to the optimization problem, where the decision maker needs to choose solutions that work well under different contexts that are not controllable.\nThis paper presents a formulation that generalizes many different BayesOpt settings related to robust optimization, and proposes using Thompson sampling as the optimization policy.\nThe authors first show a regret bound of this policy that is sublinear given specific assumptions and subsequently demonstrate the empirical performance of the policy under various optimization settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and has a clear exposition.\nI enjoyed reading through the formulation of the distributionally robust optimization problem and how it generalizes to previously proposed settings, especially by incorporating the right derivative of the objective.\nThis general framework allows relating many problem formulations that have been proposed in the literature.\nAs explained in the paper, by setting the hyperparameters, one can even realize novel optimization formulations that \"interpolate\" the previously proposed formulations at the extremes, which allows more expressiveness in designing optimization objectives that fit a user's preference.\nThe proposed TS seems to work well across the experiments."
            },
            "weaknesses": {
                "value": "The authors can consider inspecting why in various cases (in the Infection problem for DRO), TS-BOCU has almost linear regret.\nIt would be interesting to see if there are types of problems that the policy tends to perform badly on.\n(Perhaps this is connected to the insight that the algorithm resulting from setting $\\alpha, \\beta > 0$ tends to be more robust?)\n\nI am a bit confused about the assumption that $\\mathcal{X}$ and $\\mathcal{C}$ are finite: Is it necessary for the theoretical guarantee (since Section 4.1 mentions that the result can be extended to infinite sets)?\nIn the experiments (for example, with the Hartmann functions), are you constraining the search spaces to be finite?\nMy understanding is that  the algorithm can run on continuous search spaces too.\n\nI think the paper can benefit from extending the discussion at the end of Section 3.1 and offer guidance for setting $\\alpha$, $\\beta$, and $\\epsilon$ in practice."
            },
            "questions": {
                "value": "- Instead of having both $\\alpha$ and $\\beta$, can't we follow the formulation of mean-risk tradeoff and only vary the weight for the $\\delta(\\mathbf{x})$ term, unless, for example, $\\alpha = \\beta = 1$ gives a different objective than $\\alpha = \\beta = 0.5$ (assuming the same $\\epsilon$)?\n- As I understand, if $\\epsilon_t = d(\\mathbf{p}_t, \\mathbf{p}^*)$ does not approach $0$ with probability $1$, we don't obtain the sublinear regret result.\nHow does this might affect performance in practice?\nCan we still perform well if $\\mathbf{p}_t$ is sufficiently different from $\\mathbf{p}^*$?\nAre there situations where $\\mathbf{p}^*$ is very hard to learn?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_WGc6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698516849773,
        "cdate": 1698516849773,
        "tmdate": 1699636445272,
        "mdate": 1699636445272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XX2vE9T675",
        "forum": "oMNkj4ER7V",
        "replyto": "oMNkj4ER7V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_B6kd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_B6kd"
        ],
        "content": {
            "summary": {
                "value": "The paper studies robust BO when there is context uncertainty, and designs a TS algorithm based on a general framework that incorporate a large number of risk-sensitive learning objectives. The authors substantiate its efficacy with theoretical analysis and validate the algorithm's performance and adaptability through experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clear and easy to follow. The propose method is solid and with decent theoretical and numerical evidence."
            },
            "weaknesses": {
                "value": "The significance is of question to me - though it is good to have a unified form for multiple previously proposed objectives, the unification achieved in this paper seems straightforward (adding two parameters) and the additional technical challenge (e.g., in algrotihm design or analysis) is unclear, and it is not clear whether those new objectives are really of significance for practitioners."
            },
            "questions": {
                "value": "1. Is the first-derivative objective only previously proposed for GP, or is also applicable in other areas? Is 3.1 only novel in GP literature, or for the first time also in other areas? \n\n2. Why finite context and action space? These read very limited. Are they also required by prior works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607774402,
        "cdate": 1698607774402,
        "tmdate": 1699636445167,
        "mdate": 1699636445167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e1FOa7vxjI",
        "forum": "oMNkj4ER7V",
        "replyto": "oMNkj4ER7V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
        ],
        "content": {
            "summary": {
                "value": "A framework for Bayesian optimization under contextual uncertainty unifies various formulation of Bayesian optimization, including distributionally robust optimization, stochastic optimization, robust optimization, robust satisficing, worst-case sensitivity, and mean-risk tradeoff.  The authors provide theoretical analyses on regret bounds and experimental results to compare several Bayesian optimization algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This work serves the comprehensive understanding of Bayesian optimization under contextual uncertainty.\n* Paper is well-organized."
            },
            "weaknesses": {
                "value": "* Paper is hard to follow.  For example,\n\nThe sentence \"While standard BO assumes that the learner has full control over all input variables to the objective function, in many practical scenarios, the learner only has control over a subset of variables (decision variables), while the other variables (context variables) may be randomly determined by the environment\" is too complex.  There are two whiles in one sentence.\n\nThe sentence \"We assume that, at every iteration, some reference distribution $\\boldsymbol p$ is known that captures the learner's prior knowledge of the distribution governing $\\boldsymbol c$\" is grammatically wrong.\n\n\"a probability vector in $\\mathbb{R}^n$\" should be \"a probability vector in $[0, 1]^n$\" for readability and understandability.\n\nI think there are other grammar and presentation issues.  Please revise your submission carefully.\n\n* I think that some assumptions are too strong.  For example, the assumptions on finite sets of $\\mathcal{X}$ and $\\mathcal{C}$ are not practical.  Moreover, I do not understand why $\\boldsymbol p$ is known at the beginning of the optimization.  This assumption is not practically meaningful.\n\n* Since theoretical results are built on the assumptions on finite sets of $\\mathcal{X}$ and $\\mathcal{C}$, they are limited.\n\n* Reasoning and justification behind experimental results are not appropriately provided."
            },
            "questions": {
                "value": "* Table 1 can have a column for the corresponding references.  It would help understand and compare diverse algorithms\n\n* Could you explain the intuition and meaning of knowing $\\boldsymbol p$ at the beginning?\n\n* I am not sure that the ICLR paper format allows it, but the table captions should be located on top of the tables."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4649/Reviewer_9tD6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788933651,
        "cdate": 1698788933651,
        "tmdate": 1700681207681,
        "mdate": 1700681207681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4Mc5wP9Oi0",
        "forum": "oMNkj4ER7V",
        "replyto": "oMNkj4ER7V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4649/Reviewer_Lem3"
        ],
        "content": {
            "summary": {
                "value": "The paper extends the distributionally robust Bayesian optimization (DRBO) framework to a more general framework called \u201cBO under contextual uncertainty\u201d (BOCU). BOCU targets the problem of maximizing some uncertainty objective that takes the context distribution into account. Example problems include worst-case sensitivity, mean-risk trade-offs, DRBO, robust satisficing. The paper develops a general Thompson sampling algorithm that can optimize any objective within the framework. The paper also derives Bayesian regret bound for their developed framework. Finally, some experiments are conducted to illustrate the sublinear regret properties of the framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is very well written and easy to understand. The problem settings, the previous works, the concepts are all well described.\n+ The paper tackles an interesting problem which is to unify different problems (with the same theme of BO under contextual uncertainty) into one framework. The paper also proposes a general method that can solves this unified framework with different objectives. Theoretical analysis is also conducted to guarantee the performance of the proposed method.\n+ The proposed method seems to be sound and reasonable to me.\n+ The experiments (though a bit limited) are also conducted in order to understand the behaviours of the proposed method and to confirm the theoretical analysis."
            },
            "weaknesses": {
                "value": "To me, the main weaknesses of the paper are in the experimental evaluation. I list in the below some weak points that I found from the experimental evaluation:\n+ The problems used in the evaluation (GP, Hartmann 3, plant growth simulator, COVID epidemic model) have quite low dimensions, ranging from 2 to 5.\n+ I found the analysis regarding the experiments could be further elaborated. Currently, there is only one paragraph explaining a lot of results in one figure (Figure 2), I have to think a lot in order to understand what the reported results convey.\n+ For the COVID infection problem, I found the results of DRO are not too good. I\u2019m just wondering what are the issues of these cases?"
            },
            "questions": {
                "value": "Apart from my comments and questions in the Weaknesses section, the authors could answer the additional following questions:\n+ In Theorem 4.1, are the assumptions used common assumptions used in this particular research topic? What are the implications of these assumptions? Is it possible for these assumptions to be occurred in practice?\n+ Also, from Theorem 4.1, is the maximum information gain \\gamma_T bounded as in the standard BO algorithms? Which kernels will guarantee this maximum information gain to be bounded?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698912554541,
        "cdate": 1698912554541,
        "tmdate": 1699636445027,
        "mdate": 1699636445027,
        "license": "CC BY 4.0",
        "version": 2
    }
]