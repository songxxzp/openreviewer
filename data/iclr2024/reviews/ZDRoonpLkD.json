[
    {
        "id": "3vNqOd7urI",
        "forum": "ZDRoonpLkD",
        "replyto": "ZDRoonpLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_tuNF"
        ],
        "content": {
            "summary": {
                "value": "The paper primarily builds upon NeuroSAT and proposes several improvements to the original model, including the use of curriculum learning to speed up model training multiple initial assignments to embeddings, and decimation to enhance model accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n1. The authors significantly accelerate training time by employing curriculum learning.\n2. The model exhibits substantial improvements in accuracy compared to NeuroSAT.\n3. The decimation measure, inspired by Belief Propagation (BP), is quite convincing."
            },
            "weaknesses": {
                "value": "Weaknesses:\n1. Neither the samples nor the decimation techniques are subjected to ablation experiments.\n2. The last sentence of the first paragraph in the introduction is not particularly convincing.\n3. The sampling technique seems to enhance model accuracy solely by initializing values across multiple embeddings, and its relationship with SDP appears weak."
            },
            "questions": {
                "value": "Could you please clarify the nature of the initial embeddings? Are they generated randomly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698485948027,
        "cdate": 1698485948027,
        "tmdate": 1699636363153,
        "mdate": 1699636363153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wEmseQrEsW",
        "forum": "ZDRoonpLkD",
        "replyto": "ZDRoonpLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the Graph neural networks for combinatorial problems. In this work, the authors applied a curriculum training procedure, a decimation procedure and initial-value sampling. The authors claim that their proposed curriculum and optimization methods reduce training time by more than an order of magnitude and significantly increase the percentage of solved problems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is generally well-written, with clear explanations. It clearly introduces about the motivation of the optimizations and the methods applied.\n\n2. Publicly available source code is provided to reproduce the results."
            },
            "weaknesses": {
                "value": "I have a number of comments regarding the experimental setup.\n\n1. According to Section 5, the training instances are of very small scale. For the generated, random SAT instances, the number of variables are up to 40 variables. In fact, existing local search SAT algorithms are able to solve random satisfiable instances around phase-transition threshold with thousands of variables very efficiently. Hence, solving random instances with up to 40 variables are quite trivial.\n\n2. After reading Appendix A.3.2, it seems that the authors also do not introduce the number of variables for those generated, structured instances (i.e., those instances generated from the domains of Latin squares, Sudoku, and logical circuits). Actually, it is widely recognized that modern CDCL SAT solvers can solve structured SAT instances with tens of thousands of variables. Could you please claim the numbers of variables for those generated, structured instances?\n\n3. It seems that your proposed method can only handle satisfiable instances. Could you discuss the behavior of your proposed method when dealing with unsatisfiable instances?\n\n4. The authors only compare their proposed method with NeuroSAT. However, CDCL solvers stand for the current state of the art in SAT solving. As a submission to a top-tier conference, lack a comparison against the real state of the art is unacceptable."
            },
            "questions": {
                "value": "Please see my comments that are listed in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I do not find ethics concern."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4009/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4009/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_wK6B"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570742925,
        "cdate": 1698570742925,
        "tmdate": 1699636363067,
        "mdate": 1699636363067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "URmAnNbXK8",
        "forum": "ZDRoonpLkD",
        "replyto": "ZDRoonpLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_rKtv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two improvements over NeuroSAT, a very popular\nmessage-passing NN for Boolean satisfiability (SAT).  These\nimprovements are inspired by two other approaches to (Max-)SAT:\nSemidefinite Programming (SDP) relaxations and Belief Propagation\n(BP). The first improvement is a form of curriculum learning, in which\nthe size of formulas and number of message-passing iterations is\nincreased throughout the training. This first improvement results in a\nsignificantly faster training convergence. The second improvement is\ntwofold: 1) running in parallel NeuroSAT with multiple initializations\nof the embedding vectors. 2) Once the model is trained, it is possible\nto recover the notions of true/false values in the latent space. This\nenables a decimation procedure during message passing, that is, early\nfixing of the truth values if these get too close to true/false.\nThese two modifications result in more robust predictive performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation is good overall\n- Well-motivated improvements over the existing work"
            },
            "weaknesses": {
                "value": "- I have a few minor points on the presentation\n- The experimental section does not address some important questions\n\n---\nDetailed comments:\n\n  \"Moreover, neural networks can potentially find solutions, which\n  could lead to unexpected insight (Pickering et al., 2023; Davies et\n  al., 2021).\"\n\nI don't understand this sentence. Is \"solutions\" in this context\nreferring to approximate algorithms for a target class of problems,\ni.e. a trained model?  What unexpected insights are you hinting at?\n\n  \"Recently, Kyrillidis et al. (2020) demonstrates scenarios where\n  solving a continuous relaxation formulation may provide benefits\n  over solving the formula using standard solvers.\"\n\nI would summarize these benefits.\n\n  \"[..] which has demonstrated the ability to exhibit nontrivial\n  behaviors resembling a search in a continuous space, rather than\n  mere classification based on superficial statistics.\"\n\nWhat nontrivial behaviour are the authors referring to?\n\nIn Section 4: \"Selsam et al. (2019) observes that for formulas that\nthe model correctly classified as satisfiable, the embeddings of\nliterals form two well-separated clusters.\"\n\nThis is already mentioned earlier in the text.\n\n  \"In Figure 4 in the Appendix, we recapitulate their visualization of\n  embeddings with UMAP instead of PCA.\"\n\nI was not able to connect the figure with the following text. Either\nFig. 4 is instrumental in understanding the following paragraphs and\nshould be moved to the main text, or it isn't and this sentence should\nbe removed. It is also unclear to me why you used UMAP instead of PCA.\n\nIt is not clear to me whether sampling multiple initializations and\ndecimation are two orthogonal improvements. If so, I would expect a\nseparate empirical evaluation for the two.\n\nGiven the nice performance improvements, I am left wondering if the\n(augmented) NeuroSAT architecture is competitive in some settings with\nother end-to-end approaches. This is not addressed in the experimental\nsection. Can we leverage curriculum learning to push the predictive\naccuracy over 85% using a more expressive model? Can it also\nbetter generalize to larger problems wrt the original NeuroSAT?\n\nReporting the inference time of the standard vs. decimation approaches\nis necessary to the evaluation. I also wonder why only 2 passes of\ndecimation were evaluated. What happens if we do more?"
            },
            "questions": {
                "value": "1) If multiple initializations and decimation are orthogonal improvements, can you provide ablation results for the two?\n2) Can you provide a more throughout evaluation of decimation (i.e. with more than 2 passes)?\n3) What is the runtime cost of your approach wrt standard NeuroSAT?\n4) Does the augmented NeuroSAT method better generalize to larger instance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766170287,
        "cdate": 1698766170287,
        "tmdate": 1699636362968,
        "mdate": 1699636362968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rXuBFcBg3m",
        "forum": "ZDRoonpLkD",
        "replyto": "ZDRoonpLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes enhancements for the training and inference procedure of Graph Neural Networks (GNNs) that are trained to predict solutions of combinatorial problems, with a focus on Boolean Satisfiability (SAT). The proposed optimizations include a curriculum training procedure, a novel loss function, and a dynamic batching strategy. The idea is inspired by the possible connection of the behavior of GNN and two algorithms: Belief Propagation and Semidefinite Programming Relaxations. These enhancements significantly reduce training time and increase the percentage of solved problems. The paper also provides a comprehensive review of related work in the context of GNNs and Boolean Satisfiability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem studied in this paper is fundamental: what does a GNN learn? Does it devise a new algorithm? Combinatorial problems are perfect objections for conducting those studies as they are well-studied and we already know a bunch of algorithms. NeuroSAT is a well-known work in the application of GNN on combinatorial problems. What algorithm NeuroSAT really learns has not been fully understood. Therefore, the behavior of NeuroSAT is of great interest.\n\nI like the algorithmic part of this work, which greatly improves the training efficiency. This paper also simplifies the structure of NeuroSAT, which may make it easier for future work to investigate its behavior.\n\nThe paper is well-written and easy to follow. The introduction and preliminary sections give comprehensive context and background. \n\nDespite the over-claimed connection between NeuroSAT and SDP/MP, I still like the direction of this work. I am happy to change my evaluation if my concerns can be addressed."
            },
            "weaknesses": {
                "value": "The paper claims to reveal the similarity of GNN and Belief propagation. However, there is little convincing evidence of those similarities, in my opinion. It is mentioned in the paper that: \n\n\"For satisfiable formulas, this happens when the vectors form two well-separated clusters, which makes the whole\nprocess qualitatively similar to the optimization of the SDP relaxation described in Section 2.3.\"\n\nThe vectors forming two well-separated clusters, while interesting, is not strong evidence that NeuroSAT is similar to SDP. There may be other algorithms for SAT based on lifting to high-dimension vectors that also obey this behavior. Either stronger evidence (e.g. NeuroSAT is optimizing some quadratic objective) should be revealed or the statement of NeuroSAT & SDP should be removed. It would be great to see more experiments for the behavior of NeuroSAT, besides showing the efficiency of the new network structure with the curriculum."
            },
            "questions": {
                "value": "Can more evidence be discovered for the similarity of NeuroSAT and SDP/MP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4009/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4009/Reviewer_mpUf",
                    "ICLR.cc/2024/Conference/Submission4009/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4009/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823038371,
        "cdate": 1698823038371,
        "tmdate": 1700693576985,
        "mdate": 1700693576985,
        "license": "CC BY 4.0",
        "version": 2
    }
]