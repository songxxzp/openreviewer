[
    {
        "id": "IL1hti46ZI",
        "forum": "TjpiApms66",
        "replyto": "TjpiApms66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_p82K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_p82K"
        ],
        "content": {
            "summary": {
                "value": "The authors present a feature selection method for data that includes several datasets with batch effect. Such batch effect is typically evident in biological datasets that are collected on different days, labs, or measurement technologies. They assume that the batch effect is monotone and propose a method for simultaneous feature selection and batch effect removal. The main idea is to learn the monotone batch effect removal transformation by restricting the weights of a neural network and minimizing a maximum mean discrepancy loss between the distributions of different datasets. To perform feature selection, they exploit a LASSO-like L1 regularization. The method is evaluated on a synthetic and real example."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The problem of feature selection is important and has numerous applications. I believe that the batch effect setting is interesting and realistic, therefore, such a method could be valuable for the community."
            },
            "weaknesses": {
                "value": "The paper is not well written, and many parts are not presented in a clear way.\nThe notations and conventions are not consistent, and there are many mistakes/typos.\nThe algorithmic contribution is a combination of an MMD loss, and an L1 norm; while the combination of existing ideas could be considered somewhat novel, I find the specific choices of MMD and L1 not very up-to-date with the SOTA of either of the fields ( feature selection or batch effect removal).\nThere are more advanced schemes that enable nonlinear feature selection, for instance.\nThe experimental part is also pretty weak and only includes one synthetic example and one real-world data.\nFor the synthetic example, they only evaluate one underdetermined setting (m,n)=(5,10), and in this example, the method does not provide a significant advantage. Also, for this synthetic experiment, they do not provide the regression results of all models.\nFor the real dataset, there is no benchmark in they only compare to no batch effect removal without providing the actual F1 score (which, as they wrote, is actually ill-posed)."
            },
            "questions": {
                "value": "The initials of lasso are never defined also it is written in three different ways in the paper: LASSO, Lasso, lasso \nMany methods (and acronyms) are presented in the abstract without citation; that\u2019s not a proper way to mention other work.\n\nHow is UMAP be used for batch effect removal? It is quite different than PCA; you can not remove the information and project back. It might be useful only for visualization purposes, not batch effect removal per se.\n\nMany missing citations for batch effect removal:\n\nKorsunsky, Ilya, et al. \"Fast, sensitive and accurate integration of single-cell data with Harmony.\" Nature methods 16.12 (2019): 1289-1296.\n\nLopez, Romain, et al. \"Deep generative modeling for single-cell transcriptomics.\" Nature methods 15.12 (2018): 1053-1058.\n\nMany missing citations for feature selection:\n\nYamada, Yutaro, et al. \"Feature selection using stochastic gates.\" International Conference on Machine Learning. PMLR, 2020.\n\nMuthukrishnan, Ramakrishnan, and R. Rohini. \"LASSO: A feature selection technique in predictive modeling for machine learning.\" 2016 IEEE international conference on advances in computer applications (ICACA). IEEE, 2016.\n\n\nIn section 2, \u201cthe goal of batch effect\u2026\u201d isn\u2019t the goal to learn the inverse of these functions?\n\nHow does the Gaussian assumption hold for actual biological data, for example, scRNA seq, which is typically modeled by other distributions?\n\nThe dimensions of most terms in the paper are not defined; for example \\Phi\u2026\n\nThe NN function \\Phi, is it applied to each feature separately? or does it learn feature interactions like standard NNs? This is not clear, specifically because the parameter \\theta is multiplied by the output of this NN. So if it is actually performing FS \\Phi has to be an element-wise function (so not a fully connected NN).\n\nOn page 5, many statements are incorrect, for example, that most pairs of genes are almost independent. In scRNA seq, many large groups of features are correlated.\n The transition from eq.6 to 7 is unclear. Suddenly there is $h$ $\\ell$ there is also twice +\n\nL1 is written in many ways in the paper. \nF1 is not defined. I understand that this is F1 of feature selection, but this is not explained.\n\nThe convergence analysis is not really informative and is more appropriate in an appendix.\n\nUse the same color scale in Figure 4 otherwise, it's confusing.\n\nWhy not try the method for cell classification with batch effect, then, the method could be evaluated in terms of a real biological task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Reviewer_p82K"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770832645,
        "cdate": 1698770832645,
        "tmdate": 1699637139542,
        "mdate": 1699637139542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GsrIkaaxHN",
        "forum": "TjpiApms66",
        "replyto": "TjpiApms66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_xAdr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_xAdr"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how to gather multiple data sets, remove the batch effect that each of them may have and perform feature selection on the unified dataset to optimize the prediction task. The paper is mostly experimental. The mathematical formulations are presented to define the problem, and motivate the empirical approach taken by the paper. No proof is provided to support the algorithmic decisions they made and the way they formulated the objective function. \n\n The experimental results are mainly on a simulated instance of a few or  a limited number of batches (5 or 50 datasets) with 10 or 100 points in each dataset. The distribution of each data set seems to be an independent Gaussian distribution which is surprising. There is no reason to believe that there is a reasonable unifying distribution (ground truth solution) in this synthesized dataset. So the main purpose of this experiment could be mostly as a test case that their algorithms are running as intended.\n\nThey also have experiments on the Genomic dataset but they say there is no ground truth for this dataset. They do some comparisons with the Lasso method and claim to have a higher F1 score. As part of this comparison they mention: \n\u201c This process identified 395 potential genes out of about 10,000 genes; 15 of them overlap with the 395 genes listed in the biological publication by Seo et al. Seo et al. (2009), which provides genes with biological connections with Srebp-1c.\u201d\n\nIt is not clear how significant finding 15 out of the 395 genes is."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They provide some reasonable experimental ideas to explore."
            },
            "weaknesses": {
                "value": "The paper is mostly experimental. The mathematical formulations are presented to define the problem, and motivate the empirical approach taken by the paper. No proof is provided to support the algorithmic decisions they made and the way they formulated the objective function. \n\n The experimental results are mainly on a simulated instance of a few or  a limited number of batches (5 or 50 datasets) with 10 or 100 points in each dataset. The distribution of each data set seems to be an independent Gaussian distribution which is surprising. There is no reason to believe that there is a reasonable unifying distribution (ground truth solution) in this synthesized dataset. So the main purpose of this experiment could be mostly as a test case that their algorithms are running as intended.\n\nThey also have experiments on the Genomic dataset but they say there is no ground truth for this dataset. They do some comparisons with the Lasso method and claim to have a higher F1 score. As part of this comparison they mention: \n\u201c This process identified 395 potential genes out of about 10,000 genes; 15 of them overlap with the 395 genes listed in the biological publication by Seo et al. Seo et al. (2009), which provides genes with biological connections with Srebp-1c.\u201d\n\nIt is not clear how significant finding 15 out of the 395 genes is."
            },
            "questions": {
                "value": "The experimental results are mainly on a simulated instance of a few or  a limited number of batches (5 or 50 datasets) with 10 or 100 points in each dataset. The distribution of each data set seems to be an independent Gaussian distribution which is surprising. There is no reason to believe that there is a reasonable unifying distribution (ground truth solution) in this synthesized dataset. So the main purpose of this experiment could be mostly as a test case that their algorithms are running as intended.\n\nThey also have experiments on the Genomic dataset but they say there is no ground truth for this dataset. They do some comparisons with the Lasso method and claim to have a higher F1 score. As part of this comparison they mention: \n\u201c This process identified 395 potential genes out of about 10,000 genes; 15 of them overlap with the 395 genes listed in the biological publication by Seo et al. Seo et al. (2009), which provides genes with biological connections with Srebp-1c.\u201d\n\nIt is not clear how significant finding 15 out of the 395 genes is. \n\n\n\nMore detailed comments:\nIn problem formulation, they mention functions f_i are monotone. These are functions that are applied on distributions, each represented by a collection of vectors of features. What does it mean for these functions to be monotone? Their inputs do not adhere to any total ordering. \n\nE_{z,z\u2019} can be understood from the context but I suggest defining it explicitly. \n\nPage 4: they mention input features x^* follow a multivariate Gaussian distribution. Is this a standard assumption? \n\nThe transformation functions should be bijective. Could you provide an explanation on why the ReLu transformation is bijective? \n\nPage 5: \u201cAlthough our simulation results show that even a fixed covariance matrix can beat the state-of-the-art approaches in the literature, the randomly generated covariance matrix can be arbitrarily far from the covariance matrix of the actual data distribution.\u201d\nYou claim that you beat the state of the art with a fixed matrix but these two methods do not seem comparable. When you generate a random fixed matrix, your objective is to minimize the sum of distances to this generated matrix IIUC. Whereas in the other methods you have other objectives to optimize. \n\nPage 6, since the notion of monotonicity for f_i is not defined properly, the claim about order statistics of rows are also not clear. \n\nSubsection 3.1: It seems that you generate a separate random normal distribution for each of the m datasets. If this is not the case, you need to explain it in more detail. \n\nPage 7, last paragraph: Figure 3 shows the MMD \u2026 \u2192 Figure 4 \u2026"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Reviewer_xAdr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778858589,
        "cdate": 1698778858589,
        "tmdate": 1699637139389,
        "mdate": 1699637139389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3lQnQL81Es",
        "forum": "TjpiApms66",
        "replyto": "TjpiApms66",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_6Rhw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9047/Reviewer_6Rhw"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on variable selection in high-dimensional regression, in the presence of monotone batch effects resulting from aggregation of multiple data sets. The authors introduce a new methodology to simultaneously select features and remove batch effects, by optimizing a cost function combining a Lasso regression loss and a distribution matching loss based on maximum mean discrepancy. In addition, they introduce an extended version of the method which accounts for simultaneous low-rank modeling, which can be useful especially in genomics applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The focus of the paper is an important problem encountered in applied data analysis, particularly in health care applications. Thus, the methodology could have a significant impact.\n- The simulated experiments are quite thorough with comparisons with 6 state of the art competitors, and the proposed methods clearly outperform competitors on the metric used."
            },
            "weaknesses": {
                "value": "- The paper has flaws in the presentation, and looks like it has not been properly proof-read. For instance, some sentences of the literature review are written twice in different paragraphs. (\"An alternative approach\nis to apply a clustering algorithm on the data and remove the batch effect iteratively through the\nclustering procedure Li et al. (2020); Fang et al. (2021); Lakkis et al. (2021). More specifically, each\ndata batch is considered a cluster, and the batch effect is viewed as the between-cluster variances Fang\net al. (2021). \" p.1 and 2).\n- The mathematical formulation is not always very clear. For instance, p.3 it is not clear how the monotonicity of the functions f_i is defined, as they are multidimensional. See also questions section below.\n- The real data experiments are quite limited for a methodological paper. In particular, the method is not compared to competitors on the real data experiments, which is a shame as it limits the interpretation and understanding of the paper's impact."
            },
            "questions": {
                "value": "- Could you provide a bit more details on why \"Such assumptions may be invalidated when the batch effect removal\nprocedure does not account for the downstream feature selection procedure.\" (p.2) ? This is not obvious while reading and is quite important to understand what could be the impact of the paper.\n- The literature review on variable selection is not very thorough, as only multiple testing and Lasso are referred to, which are important but not very recent. Could you perhaps extend a little bit this paragraph and explain whether more recent work (e.g. SLOPE https://arxiv.org/abs/1407.3824 or post-selection inference https://www.stat.cmu.edu/~ryantibs/statml/lectures/Lee-Sun-Sun-Taylor.pdf) are also sensitive to batch effects ?\n-  p. 3 Are the functions f_i required to operate pointwise ineach data set ? Or could there be interactions between data points ? If so how do you define monotonicity ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9047/Reviewer_6Rhw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699265426561,
        "cdate": 1699265426561,
        "tmdate": 1699637139272,
        "mdate": 1699637139272,
        "license": "CC BY 4.0",
        "version": 2
    }
]