[
    {
        "id": "PRxBSqrlYg",
        "forum": "G0EVNrBQh6",
        "replyto": "G0EVNrBQh6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_MnMs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_MnMs"
        ],
        "content": {
            "summary": {
                "value": "This work identifies the presence and effect of human-identifiable features in adversarial perturbations. The authors recognize that individual perturbations on a single input, while successful at fooling a model, do not produce distinct features that can be readily interpreted by humans. They posit that this is due to the presence of noise in the perturbations, and introduce a methodology to help overcome this by averaging many perturbations on the same image. The result produces perturbations that are significantly more human understandable, as demonstrated through a human evaluator experiment. With these new perturbations, they identify two different effects that these perturbations have on their input: masking, which covers prominent features of the true class of the image, and generation, which creates prominent features of the target class. Overall, this work provides insights into features created in adversarial examples, introduces methodology that can increase explainability in the presence of adversarial examples, and provides explanations from their findings for well known phenomena in adversarial training, transfer ability attacks, and interpretability."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Thank you for your submission! I thoroughly enjoyed reading this paper; the results were compelling, the methodology was sound, the contributions and findings are novel and useful, explanations were clear, and I was surprised at how recognizable the generated perturbations were.\n\nSome specific highlighted results/conclusions/contribution:\n- As mentioned in the paper, there is a significant need for work that provides explanations for reasons as to why attacks are as successful as they are and why models are as vulnerable to adversarial examples as they are. This work bridges these two approaches by (a) evaluating a variety of attacks and (b) creatively extracting portions of perturbations that are well aligned across models and thus represent features that transfer across models\n- The perturbations generated with this method were significantly clearer/more recognizable to me as a reader. Additionally, I felt that the claim of generating human recognizable perturbations was well supported by also incorporating the results showing that (a) human evaluators were able to recognize perturbations without associated inputs from the MM+G method at a rate significantly higher than random guessing and (b) the perturbations generated in the MM+G setting yield far more successful adversarial examples than the standard SM case\n- The discussion section connected multiple trends in transferability, adversarial training, and clean/robust accuracy tradeoffs to reasonable explanations based on insights from this work."
            },
            "weaknesses": {
                "value": "The breadth of experiments done was extensive, but I felt that in certain places, the depth of individual experiments could have been improved. Specifically:\n- I would have preferred to see more samples per class evaluated (10 seems quite small to me)\n- In the human evaluator test, I understand the limitation of testing all the attacks/settings but at the very least both settings under one attack should have been evaluated. At present, it is hard to give meaning to the 80.7% human evaluator accuracy under the BIM MM+G setting since there is not a BIM SM setting to compare it to. It would also be helpful to provide some justification for why BIM (over the other attacks) was chosen for this experiment.\n- Similar to the previous point, including SM settings in the cosine similarity experiment would have been helpful to get a baseline sense of how similar perturbations usually are to each other and to see if the MM+G setting yields significantly different values.\n\nAdditionally, the paper is clear and concise as written, but there were some portions that could benefit from additional details, explanations, or citations, mainly in Section 4 (Experimental Method). \n\nSpecific (minor) suggestions for improvement:\n- The notion of \"incomplete components of the associated features\" was lacking definition/explanation, adding some details around what this is supposed to represent would be helpful.\n- The problem of \"the number of available neural networks being limited\" didn't feel clear/well motivated. There are many parameters that can be adjusted to produce different models (seeds, hyperparameters, optimizer, architecture, etc.). Further, it wasn't clear how the solution of applying noise to produce more inputs solved this problem. \n- Some more citations to help support the contour extraction experiment would be helpful, particularly for claims that make statements about portions of the image that humans use for classification."
            },
            "questions": {
                "value": "- How were the subset of classes chosen?\n- How were the 200 inputs chosen? Were there any constraints or conditions for these inputs? Were all samples chosen correctly classified by all models?\n- While it does appear that adding noise to produce additional inputs works well, the inspiration/motivation for doing this wasn't exactly clear. Why add noise rather than performing some kind of data augmentation? \n- Why was the standard deviation of noise added to the inputs different for the different attack algorithms?\n- Why were 270 models chosen for generating perturbations? Were these experiments tried with fewer models (besides the single model case)?\n- It is mentioned in the human evaluator test that the lowest and highest accuracy in each subset was discarded before calculating the average. What was the purpose of this? And can you clarify exactly what was discarded (e.g., was data for a single sample removed from all participants or was data from a single participant removed from all samples?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695861140,
        "cdate": 1698695861140,
        "tmdate": 1699636432539,
        "mdate": 1699636432539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mAjwoDCgTu",
        "forum": "G0EVNrBQh6",
        "replyto": "G0EVNrBQh6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the human-identifiable features that are concealed within adversarial perturbations. To this end, this paper utilizes 270 models as surrogate models, introduces Gaussian noise to the input, and identifies the human-identifiable features. This paper shows that in targeted attacks, these features typically demonstrate a \"generation effect\" by producing features or objects of the target class. In contrast, in untargeted attacks, these features exhibit a \"masking effect\" by hiding the features or objects of the original class. This paper further claims the revealed phenomenon can interpret some properties of adversarial perturbations."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper revisits a critical concept in the context of adversarial robustness: the underlying mechanism of adversarial perturbations.\n2. This paper conducted human tests to verify that the emergence of semantic features is not coincidental, which is of importance.\n3. This paper validates the hypothesis across targeted and untargeted attacks and includes search-based attacks."
            },
            "weaknesses": {
                "value": "This paper challenges a well-acknowledged phenomenon in the context of adversarial robustness: the *perceptual aligned gradient* (PAG), which refers to the **human-identifiable features** that align with human perception in adversarial perturbations, only exists in robust models [1-3]. However, this paper claims that such features are also hidden in the perturbations of standardly trained (non-robust) models, which contradicts the current understanding of PAG. This concept of PAG has been well supported by various empirical and theoretical analyses in the follow-up works, along with its various applications. Therefore, in my opinion, to challenge the existing theories that contradict the claim made, this paper should provide sufficient theoretical and empirical evidence to support the proposed claims. Unfortunately, not only has the evidence in this paper already been discovered or directly deduced by previous work, but they also cannot explain the contradicted theories, which I specify below.\n\n1. The experiment uses Gaussian noise to average the perturbations to reveal the human-identifiable features. However, this phenomenon has already been revealed in [4], which shows that randomized smoothing (adding Gaussian noises to the input and calculating the averaged gradient) on a single standardly trained model can lead to PAG and generate these features. Therefore, it's not a newly discovered phenomenon claimed in this paper that averaging gradient among perturbations with different noises can lead to human-identifiable features.\n2. The experiment also averages different models to reveal the human-identifiable features. However, this phenomenon is expected based on existing work [5, 6], which shows that a little adversarial robustness of the models can lead to PAG. Specifically, as ensembling more non-robust models can still enhance adversarial robustness to a certain extent, though not as robust as adversarially trained models, it can be inferred that the ensembled model can lead to such PAG and identifiable features. Even if this paper shows that the robust accuracy of the ensembled model against adversarial attacks is still low (in Figure 3), the enhanced robustness may still be sufficient to bring such PAG.\n3. In addition, it has also been shown [7] that the distribution of non-robust features [17] varies across different model architectures. Therefore, intuitively, the gradient (perturbation) of a single model (or a single kind of model architecture) may be noisy, but by averaging the gradients from different models, it is possible to converge toward the robust features.\n\nBased on these discussions, the discovery made in this paper is somewhat trivial, since the observed phenomenons have already been revealed in existing work or can be directly deducted from them. Furthermore, the evidence presented in this paper is insufficient to challenge the well-established theories of PAG, as this paper does not provide a clear explanation of the contradictions or confusions, which I specify below.\n\n4. There exist several works [8-10] aim to explain the reason PAG only exists in robust models by characterizing the decision boundaries between different models, which is well supported by theoretical analysis. These works show the fundamental difference of decision boundaries between standard and adversarially trained models leads to the (non-)existence of PAG, which contradicts the claim made in this paper in Section 7(2) that human-identifiable features also exist in non-robust models. Unfortunately, this paper does not discuss this viewpoint and does not conduct a theoretical analysis to overturn these theories.\n5. There also exist theories interpreting the existence of PAG in robust models by modeling adversarial training as energy-based models [11-12]. Additionally, the robust model also provides better guidance during the generation process of diffusion models [13-14], indicating the importance of robust models with PAG for better gradient and generation guidance. Since such a generation process requires multi-step sampling, which can be regarded as applying an **average (ensemble)** of gradients (perturbations) to the standardly trained model, this also contradicts the viewpoint in this paper and should be well-explained.\n6. In Section 7(1), the explanation for the transferability of adversarial examples contradicts existing works. This paper attributes the transferability to the human-identifiable (robust) features, but existing works [15-16] show that robust features may not be always helpful for adversarial examples transferring between models and non-robust features still play a crucial role in transferring adversarial examples. Therefore, the claims made in this paper fail to explain the transferability of adversarial examples across models.\n7. The explanation of non-trivial accuracy for classifiers trained on a manipulated dataset [17] made in Section 7(3) is flawed. It is clear that in the manipulated dataset, which includes perturbations claimed as human-identifiable features in this paper, the features from the original class are still dominant over the perturbations. According to the interpretation within this paper, the model should still learn the features from the original class and cannot achieve clean accuracy in this noisy training setting. This contradicts the explanation proposed in this paper.\n8. In Appendix A, Figure 7, it appears that the masking effect of the perturbation without Gaussian noise significantly reduces the identifiability of human-identifiable features, compared to the results in the main paper (with Gaussian noise). Therefore, it can be inferred that ensembling Gaussian noise plays a more crucial role in generating the human-identifiable features than ensembling different models, which undermines the soundness of the claim that the presence of human-identifiable features is inherent in the perturbations themselves, rather than being a result of added Gaussian noise.\n9. There is a lack of ablation studies on the number of models to further support their claims. It is suggested to add experiments to analyze how many models or noises are required to emerge such human-identifiable features, which can provide a more intuitive view of how noisy the gradients are in the adversarial perturbations.\n10. For transfer attacks, this paper only compares BIM, CW, and DF, which are not specifically designed for transfer attacks. It is suggested to add a comparison with existing state-of-the-art transfer attackers, e.g., MI-FGSM [18], DI-FGSM [19], and ensemble attacker CWA [20], to substantial the claims regarding transfer attacks. Since this paper claims that the success of transfer attacks is based on hidden human-identifiable features, it can be inferred that transfer attacks can emerge with more human-identifiable features, which should be supported by experiments on evaluating these attacks designed for transferring.\n11. There is no statement on open sourcing and reproducibility. Since finding such 270 surrogate models is challenging to reproduce, I strongly suggest releasing the code.\n\n[1] Robustness May Be at Odds with Accuracy. ICLR 2019\n\n[2] Image Synthesis with a Single (Robust) Classifier. NeurIPS 2019\n\n[3] Adversarial Robustness as a Prior for Learned Representations. arxiv 1906.00945\n\n[4] Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?. NeurIPS 2019 Workshop\n\n[5] On the Benefits of Models with Perceptually-Aligned Gradients. ICLR 2020 Workshop\n\n[6] A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks. NeurIPS 2021\n\n[7] Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets. ICLR 2021\n\n[8] Bridging Adversarial Robustness and Gradient Interpretability. ICLR 2019 Workshop\n\n[9] On the Connection Between Adversarial Robustness and Saliency Map Interpretability. ICML 2019\n\n[10] Robust Models Are More Interpretable Because Attributions Look Normal. ICML 2022\n\n[11] Towards Understanding the Generative Capability of Adversarially Robust Classifiers. ICCV 2021\n\n[12] A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training. ICLR 2022\n\n[13] Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. TMLR\n\n[14] BIGRoC: Boosting Image Generation via a Robust Classifier. TMLR\n\n[15] Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. WACV 2023\n\n[16] Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. S&P 2024\n\n[17] Adversarial Examples are not Bugs, they are Features. NeurIPS 2019\n\n[18]  Boosting adversarial attacks with momentum. CVPR 2018.\n\n[19] Improving transferability of adversarial examples with input diversity. CVPR 2019.\n\n[20] Rethinking Model Ensemble in Transfer-based Adversarial Attacks. arXiv:2303.09105"
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4550/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4550/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_ZsSi"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742370600,
        "cdate": 1698742370600,
        "tmdate": 1699636432431,
        "mdate": 1699636432431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bLclegtPXn",
        "forum": "G0EVNrBQh6",
        "replyto": "G0EVNrBQh6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_3kT3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_3kT3"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the exploration of the underlying reasons for adversarial perturbations. Specifically, the authors hypothesize that human-identifiable features are present within the perturbations, forming part of the inherent properties of these perturbations. To validate this hypothesis, the authors average perturbations generated by various neural networks to uncover the human-identifiable features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This work finds that perturbations generated by existing methods statistically contain some human-identifiable features. These are clearly illustrated in the provided qualitative results.\n\n+ To uncover these human-identifiable features, the authors use a simple method which averages extensive generated perturbations, which is reasonable. \n\n+ This paper demonstrates that perturbations produced by certain attack methods converge at the object region.\n\n+ This paper provides a clear narrative, supplemented by analytical insights."
            },
            "weaknesses": {
                "value": "- In the first paragraph of Section 4, on what basis do you assert that (1) the noise in perturbations is independent and (2) two perturbations from different models display distinct human-identifiable features? I couldn't find any references or evidence supporting the claims. \n\n- The gradient-based attacks, proposed five years ago, aren't sufficiently contemporary to test the paper's hypothesis. There exist many newer gradient-based attacks, such as [1, 2].\n\n- I observed that detecting human-identifiable features necessitates 2,700 samples (270 models and 10 noise-infused seed samples). These may suggest that the averaged perturbation, generated by the three attacking methods, gravitates towards the object region. However, they don't confirm that in every model, the generated perturbations house human-identifiable features. Hence, a deeper experimental analysis regarding model selection and the integration of Gaussian noise would be beneficial, perhaps including more ablation studies (like MM, MM+G, SM+G).\n\n- Why choose only 20 fixed classes out of 1,000? And a mere 200 samples seem insufficient to substantiate the claims made in the paper\n\n- It's noted that perturbations of identical images from varying attack algorithms are presumably alike. However, the results don't include background noise similarity or image perturbation similarity. Providing experimental evidence for this would enhance the argument.\n\n- The experimental analysis concerning the two distinct types of human-identifiable features (masking effect and generation effect) appears limited. Visualizing the perturbation for targeted attacks would be beneficial.\n\n-  Does the visual perturbation come from cases where the attack was successful? How does the perturbation behave in the case of an unsuccessful attack?\n\n- While the paper asserts findings across three different datasets, I could only locate a detailed attack accuracy comparison for ImageNet in Appendix E Table 1. It is not clear why the NOISE performance surpass that of IMAGE?\n\n[1] Rony J, Hafemann L G, Oliveira L S, et al. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses. ICCV 2019\n\n[2] Wang X, He K. Enhancing the transferability of adversarial attacks through variance tuning. CVPR 2021."
            },
            "questions": {
                "value": "See the questions in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848838059,
        "cdate": 1698848838059,
        "tmdate": 1699636432349,
        "mdate": 1699636432349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8YHm59Oa2G",
        "forum": "G0EVNrBQh6",
        "replyto": "G0EVNrBQh6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu"
        ],
        "content": {
            "summary": {
                "value": "This paper conducted interesting analysis on the human-identifiable features concealed in adversarial perturbations crafted by different attack algorithms. In order to obtain the visual-recognizable patterns from gradient-driven adversarial perturbations, multi-samplings on different threat models was used based on the independence assumption. In experiment sections, thsi paper conducted such analysis on various threat models (274 in total) with various attack algorithms (gradient-based, search-based), which is efficient and solid. While the resulting denoised adversarial perturbations seem to have some clear pattern which can be recognized by human, the pure adversarial perturbation cannot reveal any information regarding the image itself. This paper also contains following discussion on the denoised adv perturbation by quantitatively analyzing its recognizability, checking its attack strength, and applying contour extraction. The overall analysis is plentiful and the results looks interesting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Evaluation on a large amount of threat models and attack algorithms make the whole experimental results to be reliable.\n- Motivation on exploring the human-identifiable features directly, instead of applying XAI methods to interpret, looks efficient and interesting.\n- Overall written is clear and easy to follow."
            },
            "weaknesses": {
                "value": "While I do appreciate such important and intense work on exploring the explainability in adversarial perturbations, I still have some major concerns about the whole paper. \n\n- Human-identifiable features looks vague: I still remain unclear about how to logically define the \"human-identifiable\" here: In section 5.2.1 authors conducted recognizability experiments on these denoised adv perturbations but it can only prove they are \"model-identifiable\". We cannot make such claim by showing part of (or even all) extracted adv perturbations and they are all human-identifiable. Some human-labeling experiments is required as a strong evidence to prove this.\n\n- The overall finding is not surprising: while it is good to see that denoised adversarial perturbation is similar to its corresponding raw image, I'm not surprising to see because gradient-based attacks perturb models' prediction by optimizing the objective function following the pixel-gradient direction --- larger pixel gradients indicate pixels here are important for threat model to identify this input image. Thus the outcome of gradient optimization, adversarial perturbation, should contain some important features to identify this image. And for search-based attacks, it still tend to follow the important pixels to craft their perturbation. I think this paper should focus more on the target-attack scenario - so we have our raw-image key features and our targeted label --- how would the adversarial perturbation be to reflect both concept? Currently it only has a very short paragraph discussing such scenario (Section 6)."
            },
            "questions": {
                "value": "I put all my concerns to the weakness part and I do think this paper has a lot of space to improve. \n\nHowever, I think the overall results is plentiful and interesting for other researchers to know (especially on denoised perturbation under targeted attack scenario). It could be a very interesting workshop paper after reorganizing it into a logical way.\n\n\n======================================================\n\nUpdates after reading authors' rebuttal:\n\nI really appreciate authors efforts on further elaborating the importance of their findings - now I tend to believe this is an interesting finding to me and it could inspire several future papers for further theoretical analysis. However, after checking Reviewer ZsSi's comments, there could be some literatures implicitly discussing such scenario but this paper lacks contribution on further exploring the underlying reasons. I would like to raise my score to 5 but reduce my confidence to 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4550/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4550/Reviewer_G5Xu",
                    "ICLR.cc/2024/Conference/Submission4550/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699501161106,
        "cdate": 1699501161106,
        "tmdate": 1700712774921,
        "mdate": 1700712774921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q8h2aPnp0g",
        "forum": "G0EVNrBQh6",
        "replyto": "G0EVNrBQh6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_QGNn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4550/Reviewer_QGNn"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how to extract human-identifiable features from adversarial examples. Based on the fact that DNN models are trained on human-labeled datasets, the authors assume that adversarial perturbations should also contain human-identifiable features. \n\nThe authors first claify that two factors, excessive gradient noise and incomplete features, hinder feature extraction. Therefore, the authors propose to utilize noise augmentations and model ensembling to mitigate these negative effects. The authors find two interesting phenomenons: masking effect (untargeted attacks) and generation effect (targeted attacks)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This problem is interesting. I like this topic.\n\n2. The visualization results are also promising."
            },
            "weaknesses": {
                "value": "1. Although this problem is interesting, the authors do not provide more surprising findings and insights compared with previous works.  \n  1.1 Adversarial perturbations contain meanful or human-identifiable features have been studied in these works [1,2]. They may correspond to \"robust\" features.  \n  1.2 The proposed methods, noise augmentations and model ensembling are widely used in transfer attacks. More transfeable perturbations contain more \"robust\" features (human-identifiable features) and share more non-robust features. The previous work have shown this point [1]  \n  1.3 Although the visualizations are very promising, we are uncertain about the extent of assistance this can provide.\n\n2. Some claims in the article are unclear:  \n  2.1 The two obtacles are not very clear. The first one (noisy gradient) is easy to understand. Lots of transfer attacks also propose to mitigate this negative effect to improve adversarial tranferability. However, there is insufficient evidence to support the second claim about incomplete learned features. Could you please provide more details about the second one?  \n  2.2 Meanwhile, the comparison between these two points is also unclear. Which factor has a greater negative impact on extracting human-identifiable features? As shown in experimental setting, the authors need to use lots of ensembling models. This has made is method less practical.  \n  2.3 The findings from Section 5.2.3 are interesting. The authors use the contour features to attack models. It also shows that contour features are important than background information. Could the authors please discuss connections and differences between this phenomenon and this work [3]?  \n\n3. Could the authors please provide more results about generation effect on targeted attacks?\n\n[1]. Adversarial Examples Are Not Bugs, They Are Features.  \n[2]. Image Synthesis with a Single (Robust) Classifier.  \n[3] ImageNet-trained CNNs are biased towards texture: increasing shape bias improves accuracy and robustness"
            },
            "questions": {
                "value": "Please see Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4550/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699530649221,
        "cdate": 1699530649221,
        "tmdate": 1699636432138,
        "mdate": 1699636432138,
        "license": "CC BY 4.0",
        "version": 2
    }
]