[
    {
        "id": "L2j7GKiKUL",
        "forum": "xAqcJ9XoTf",
        "replyto": "xAqcJ9XoTf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_mePj"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the stability of eigenvector-based positional encodings while previous methods mainly focus on the sign- and basis-invariant properties. The authors claim that the instability of the previous method is caused by the hard partition of eigenvectors and the ignorance of eigenvalues. To address this challenge, this paper proposes SPE, which leverages the eigenvalues to re-weight the eigenvectors in a soft partition way. SPE is provably stable and shows great expressive power. Experiments on various tasks validate the superiority of the proposed method over baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed SPE is provably stable, which means that it can generalize to unseen graphs. I think this strong inductive learning ability is crucial for graph representation learning. The theoretical contribution is great.\n\n2. The proposed SPE shows great expressive power, which not only universally approximates previous basis invariant functions but also can distinguish the cycles in graphs. Experimental results validate the effectiveness of the proposed method.\n\n3. In addition to previous methods that conduct experiments in the basic molecular property prediction tasks, this paper also considers a more challenging out-of-distribution (OOD) task for evaluation."
            },
            "weaknesses": {
                "value": "1. The complexity of the proposed SPE is much larger than previous positional encoding methods because it needs to reconstruct graph structures, i.e., $\\boldsymbol{V} \\operatorname{diag}\\left(\\phi(\\boldsymbol{\\lambda})\\right) \\boldsymbol{V}^{\\top}$, whose complexity is $\\mathcal{O}(KN^{2})$. In contrast, the Transformer-based methods, e.g., BasisNet, only have the complexity of $\\mathcal{O}(NK^{2})$, where $K \\ll N$.\n\n2. In the molecular property prediction task, SPE has more parameters than baselines. It would be better if the authors could align the number of parameters across different methods. Additionally, in the OOD tasks, the improvement of SPE over baselines is marginal."
            },
            "questions": {
                "value": "Here are some concepts that I am not sure I fully understand. Please correct me if there are any misunderstandings.\n\n1. In equation (1), what does $\\mathbb{R}^{n \\times d} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^{n \\times p}$ mean? I understand that $\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times p}$ represents the function applied to the position features of each node. What does $\\mathbb{R}^d$ indicate? Operation on eigenvalues?\n\n2. What is the difference between a hard partition and a soft partition?  I do not see a clear definition. Does hard partition indicate a fixed number of eigenvectors and does soft partition mean it can handle a variable number of eigenvectors?\n\n3. Is it possible to replace the element-wise MLPs of $\\phi$ with polynomial functions? In this situation, I think the complexity can be significantly reduced and the expressiveness can be preserved since polynomials are also non-linear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655765315,
        "cdate": 1698655765315,
        "tmdate": 1699636807491,
        "mdate": 1699636807491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HB5HEzVcb8",
        "forum": "xAqcJ9XoTf",
        "replyto": "xAqcJ9XoTf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_qQbF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach for generating positional encodings which are stable and can universally approximate basis invariant functions. To compute those encodings, the method first decomposes the Laplacian matrix, it then applies different permutation equivariant functions to the eigenvalues, and uses the output of those layers to produce matrices of dimension $n \\times n$ which are then fed to another permutation equivariant network (e.g., a GNN). The proposed method is evaluated on molecular property prediction datasets and also on a dataset with domain shifts where it outperforms other positional encoding methods in most cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The stability of graph learning algorithms is a topic that has not been explored that much yet and deserves more attention from the community. The results presented in this paper contribute to this direction.\n\n- In my view, the paper has some value since several individuals from the graph learning community would be interested in knowing its findings. Practitioners would also be interested in utilizing the proposed encodings since in many settings, existing GNN models fail to generalize to unseen domains.\n\n- The proposed model achieves low values of MAE on the ZINC and Alchemy datasets and outperforms the baselines. This might be related to the model's ability to identify and count cycles of different lengths."
            },
            "weaknesses": {
                "value": "- I feel that the paper lacks some explanations. It is not clear which modules of the proposed method contribute to it being stable. If no $\\phi_\\ell$ layers are added, wouldn't $K_\\ell$ be equal to 1? In my understanding, this wouldn't hurt stability. Also, it seems to me that as $m$ increases the bound becomes looser and looser. If that's the case, why do we need multiple such permutation equivariant layers?\n\n- One of the main weaknesses of this paper is the proposed model's complexity. Function $\\rho$ takes a tensor of dimension $n \\times n \\times m$ as input. This might not be problematic in case the model is trained on molecules since molecules are small graphs. But in case of other types of graphs such as those extracted from social networks which are significantly larger, this can lead to memory issues.\n\n- The proposed approach is much more complex that a standard GNN model, but in most cases it provides minor improvements over a model that does not use positional encodings. For instance, the improvement on Alchemy is minor, and also on DrugOOD, SPE provides minor improvements in the Assay and Scaffold domains and no improvements in the Size domain.\n\n- No running times of the different models are reported in the paper. \n\n- The proposed model seems to advance the state of the art in the field of positional encodings for graphs, however, it is not clear whether it also advances the state of the art in the graph learning community. I would suggest the authors compare the proposed approach against some recently proposed GNN models, and not only against methods that produce positional encodings.\n\nTypos:\\\np.6: \"hold and Let\" -> \"hold and let\"\\\np.7: \"we take to $\\rho$ to be\" -> \"we take $\\rho$ to be\"\\\np.8: \"which hypothesizes is because\" -> \"which is because\""
            },
            "questions": {
                "value": "In Figure 2, how did you compute the Lipschitz constant of MLPs? We can compute the Lipschitz constant for models that consist of a single layer, but exact computation of the Lipschitz constant of MLPs\nis NP-hard [1].\n\n[1] Virmaux, Aladin; Scaman, Kevin. Lipschitz regularity of deep neural networks: analysis and efficient estimation. Advances in Neural Information Processing Systems, 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672327362,
        "cdate": 1698672327362,
        "tmdate": 1699636807378,
        "mdate": 1699636807378,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wKNS3L4lkD",
        "forum": "xAqcJ9XoTf",
        "replyto": "xAqcJ9XoTf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Stable and Expressive Positional Encodings (SPE), an architecture that mainly addresses the challenges of instability in using Laplacian eigenvectors as positional encodings for graph neural networks. The key insight to overcome instability is to avoid `hard partitions' of eigen-subspaces, and instead, use soft partitions via Lipshitz continuous functions over the spectrum. The stability of SPE is proved and validated via out-of-distribution generalization experiments.  Universal expressiveness is also proved, mainly based on another work, i.e., BasisNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- S1. I like the design of the experiment in that the authors validate the stability of SPEs from an aspect of out-of-distribution generalization.\n\n- S2. The authors target at robustness/instability and generalization of PEs, which is novel in the literature.\n\n- S3. The paper is overall well written."
            },
            "weaknesses": {
                "value": ">  W1. The instability of prior method (i.e., the so-called hard partition method) is not proved. \n\n The authors point out under Eq.2 that **hard partition** is induced when $\\[\\phi_{\\ell}(\\boldsymbol{\\lambda})\\]_j=\\mathbb{1}$(other places in $\\phi(\\cdot)$ are zeros), \n\nand then $\\boldsymbol{V}\\text{diag}(\\phi_{\\ell}(\\boldsymbol{\\lambda}))\\boldsymbol{V}^{T}$ is the $\\ell$-th subspace. \n\nThe problem is that, if we set $\\\\{ \\phi_i \\\\}_{i=1}^{m}$ that induce the hard partitions, \n\nthen they are **constant functions** and meet the $K_{\\ell}$-Lipschitz continuous assumption in Assumption 3.1, which is then used to prove the stability of SPE. \n\nTherefore, the question is,  **is the prior method (i.e., the counterpart that uses hard partitions) really unstable?** It seems that hard-partitioned SPE can be proved to be stable via exactly the same proof of Theorem 3.1.\n\n> W2. Equivalence for $\\\\{\\phi_i\\\\}_{i=1}^{m}$ .\n\nThe authors restrict $\\\\{\\phi_i\\\\}_{i=1}^{m}$ to be permutation equivariant, whose input is the Laplacian spectrum. Here, the authors are asking for equivalence under the reordering of eigenmaps/eigenvalues, instead of the reordering of graph nodes. \n\n> W3. On the universal expressiveness. \n\nThe proof of this SPE's universality relies on being reduced to BasisNet. Therefore, two problems arise: \n\n- The experiment regarding expressiveness, i.e., the graph substructure counting, does not include BasisNet.\n- According to Lim et al. (2023), the instance of BasisNet, Unconstrained-BasisNet, universally approximates any continuous basis invariant function. In Unconstrained-BasisNet,  IGN-2 (Maron et al., 2018) is the core part to achieve such expressiveness. However, in implementation, the authors set $\\rho$ to be one identical GIN (Xu et al., 2019),  which would surely limit the expressiveness. \n\n> W4. Lack of description of baseline models.\n\nFor the same reason as in W3, in the experimental part, specific choices of baseline instances, i.e.,  $\\rho$ and $\\phi$ of BasisNet, should be described more clearly."
            },
            "questions": {
                "value": "Please check W1, W2, and W3. Below is an additional question: \n\nQ1: Would the learned  $\\\\{\\phi_i\\\\}_{i=1}^{m}$ be close to each other? This would lead to similar position encodings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6928/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_TVwL",
                    "ICLR.cc/2024/Conference/Submission6928/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833804741,
        "cdate": 1698833804741,
        "tmdate": 1700473286561,
        "mdate": 1700473286561,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bdWYLwARw6",
        "forum": "xAqcJ9XoTf",
        "replyto": "xAqcJ9XoTf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_fM7Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_fM7Q"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes stable and expressive Laplacian positional encodings (SPE) by performing a soft and learnable partition of eigensubspaces. The encoding guarantees that small perturbations to the input Laplacian induce a small change to the final positional encodings. The empirical results suggest a trade-off between stability (correlated with better generalization) and expressive power."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea to address the stability of LapPE is novel, with SPE being a universal basis invariant architecture. \n- The motivation is well established, the difference to other related works is concise, the propositions are described well and the strength of SPE as a universal basis invariant architecture is presented thoroughly. \n- The experiments show the improvement in generalisation for SPE and its improved capabilities in recognising substructures, which are interesting outcomes of the architecture."
            },
            "weaknesses": {
                "value": "- The novelty of the method itself is partially limited as the idea to use a weighted correlation over the eigenvectors closely resembles the correlation used in BasisNet. \n- The experiments are limited. The performance of SPE in Table 1 is sub-par, and details of the experimental results in Figure 2 are unclear and the hyperparameters seem not to be reported, which makes it hard to reproduce the experiments. \n- The point regarding the trade-off between expressivity and generalisation is unclear. Is there a formal explanation which we can quantify? \n- Perhaps additional experiments could be useful, e.g.,:\n  - An experiment comparing the generalisation gap of LapPE/BasisNet/SPE.\n  - Evaluating the performance of LapPE/BasisNet/SPE on LRGB or TUDatasets."
            },
            "questions": {
                "value": "Please refer to my review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698959323520,
        "cdate": 1698959323520,
        "tmdate": 1699636807133,
        "mdate": 1699636807133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wKGTBbOHdK",
        "forum": "xAqcJ9XoTf",
        "replyto": "xAqcJ9XoTf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
        ],
        "content": {
            "summary": {
                "value": "This works further deepens results on basis-invariant GNNs building on work of e.g. Wang et al (ICLR 2022) and Lim&Robinson et al (ICLR 2023). The authors propose a simple generalization of basis-net, with strong theoretical guarantees (H\u00f6lder-smoothness / stability and basis-invariance). They achieve strong performance in standard molecular benchmarks, out-of-distribution benchmarks and cycle count tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Extremely well written and easy to follow. Figure 1 is nice!\n\nThe proposed model SPE (Eq. 2) is a simple generalization extension of basis-net. It is very interesting to see that such a straightforward modification yields such strong (and non-trivial) theoretical results (Theorem 1, etc.).\n\nEmpirical performance is very convincing, e.g. for Zinc and the OOD tests.\n\n------ during rebuttal ------\nas reviewers addressed my concerns and in fact added clarifications on runtime and more importantly on the unstability of previous methods I raised my score and now clearly vote for acceptance."
            },
            "weaknesses": {
                "value": "The achieved theoretical and empirical results, while very interesting, seem somewhat incremental compared to Wang et al (ICLR 2022) and Lim&Robinson et al (ICLR 2023). The authors should discuss the differences more clearly. In particular:\n* The reason for H\u00f6lder continuity ($c\\neq1$) is not fully clear. E.g. does PEG / standard basis-net already already satisfy your stability criterion Def 3.1 and / or Assumption 3.1.? If yes, what is the conceptual / theoretical benefit of your proposed architecture. If not, could you please provide an argument why the assumptions fail for PEG / basis-net.\n* Is $c\\neq 1$ crucial for any proof / guarantee / assumption? \n* Are there cases where SPE satisfies the stability assumption of Wang et al (ICLR 2022), i.e., with $c=1$?\n\nPlease see also the questions below.\n\nMinor:\n* Remark 3.1 Should probably be attributed to Wang et al (ICLR 2022), as they have the same statement for $c=1$.\n\nI am happy to raise my score, if the authors properly address my concerns and questions."
            },
            "questions": {
                "value": "Please provide runtimes for your proposed method. Preferably for pre-processing and overall runtime. This would help to put the achieved results into context with basis-net, etc.\n\nWhile the OOD generalization bound is very interesting, can you also state a standard PAC-style generalization bound (same distribution for train and test)?\n\nDo you have a counter-example where SPE cannot count $k$-cycles for $k\\leq 6$?\n\nThe $n\\times n\\times m$ might be somewhat excessive for certain datasets. Would it be possible to exchange $V\\\\phi(\\cdot) V^T$ to $V^T\\phi(\\cdot) V^T$ to get the much smaller $d\\times d\\times m$ instead? If not what would this more compact model correspond to?\n\nIf I am notmistaken SPE and basis-net should have the same expressivity and thus at least theoretically be both equally capable of counting cycles etc. Can you provide some intuition why SPE performs significantly better than basis-net in this task (Figure 3)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6928/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6928/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6928/Reviewer_Fdup"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699034864527,
        "cdate": 1699034864527,
        "tmdate": 1700391490826,
        "mdate": 1700391490826,
        "license": "CC BY 4.0",
        "version": 2
    }
]