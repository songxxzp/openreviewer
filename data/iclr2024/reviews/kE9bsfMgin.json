[
    {
        "id": "TAQPQakkiA",
        "forum": "kE9bsfMgin",
        "replyto": "kE9bsfMgin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_ELVt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_ELVt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a bias analysis framework to explore and identify a small set of biased heads that are found to contribute\nto a PLM\u2019s stereotypical bias. Extensive experiments are conducted to validate the existence of these biased heads, and on its basis, investigate gender and racial bias in the English language in BERT and GPT.  The results shed light on understanding the\nbias behavior in pretrained language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper targets at identifying bias in pre-trained language models, a hot NLP topic.\n\n- The paper is written with a high degree of clarity. The research is well motivated; the proposed framework is presented in an easy-to-understand manner; the experiments are organized in a logical order.\n\n- The experiments are comprehensive in that it only includes validates the sensibility of the proposed bias estimation framework, but also touches upon the analysis of the other types of bias and debiasing approaches with the proposed framework."
            },
            "weaknesses": {
                "value": "- The overview on probing stereotypical biases in BERT-like models is slim. Many recent works along this research direction are missing, e.g., Stereo type and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models, EACL 2021.\n\n- Even though the proposed framework can be used to evaluate any transformer-based models, the evaluation only involves BERT-base and GPT-2-small, undermining the value of the findings.\n\n- In the counter-stereotype experiment, the statistical test is performed on the average attention score of all biased and unbiased heads which, in my opinion, largely weakens the validity of the argument that the deemed biased heads encode stereotypical biases. Instead, the t-test should be performed on a per-head manner, and a box plot of the t-values of the biased heads and unbiased heads should be plotted to show the distribution of t-values in both classes."
            },
            "questions": {
                "value": "- What does not the last row in table (a) (Random-Debias, All) standard for?\n\n- Why are top-K and bottom-K heads masked out with K=3 in the last experiment? What are the results for other values of K? In order to achieve reduced PLM bias without affecting the model capability, should the number of masked out heads chosen based on the obtained bias scores in practice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698152980486,
        "cdate": 1698152980486,
        "tmdate": 1699636760121,
        "mdate": 1699636760121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UvGKli06eI",
        "forum": "kE9bsfMgin",
        "replyto": "kE9bsfMgin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_fmfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_fmfX"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the relationship between attention heads in\ntransformer-based models and stereotypical bias (for gender and race\nas use-cases). The paper shows that attention heads contribute\ndifferently to bias scores that measure intrinsic bias based on the\nSEAT test and stipulates that attention heads that contribute more are\nprobably responsible for bias in LMs. To further analyze the\ndifference in attention heads wrt bias, the paper analyzes\ncounterfactual pairs of sentences that differ only in the gender group\nand shows that the heads attend more between gendered words and their\ncorresponding sterotypical words (e.g., higher attention between\n\"women\" and \"emotional\" than \"men\" and \"emotional\"). Last, masking the\ntop-3 most biased heads leads to slight decrease in bias scores when\ncompared to removing the least biased heads or 3 heads at random."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Analysis of the relationship between attention heads and stereotypical bias for gender and race"
            },
            "weaknesses": {
                "value": "Bias is quantified using intrinsic measures of bias that have been shown not be correlated with bias in downstream tasks\n\nWhile I find the analysis of attention heads wrt bias interesting, I\nam not sure whether analyzing intrinsic measures of bias is useful or\nimpactful. There have been several papers that show issues with\nintrinsic bias measures: they are not robust and there is little to no correlation with bias measured for a downstream task. I recommend some of the following papers:\n\nhttps://aclanthology.org/2022.trustnlp-1.7.pdf\nshows how simple rephrasing of sentences with different lexical choices but the same semantic meaning lead to widely different intrinsic bias scores\n\nhttps://aclanthology.org/2021.acl-long.150.pdf\nshows that intrinsic bias measures do not correlate with bias measured at the NLP task level\n\nhttps://aclanthology.org/2022.naacl-main.122/\ndescribes more issues related to bias metrics\n\nhttps://aclanthology.org/2021.acl-long.81/\nlists several issues with current datasets/benchmarks for bias auditing"
            },
            "questions": {
                "value": "In the light that there is no or little correlation between intrinsic bias measures and bias observed in a downstream task, how do you think the analysis of bias in attention heads is useful for downstream tasks?\n\nIn my opinion, a similar analysis of the attention heads could be performed in the context of a downstream task and it would be stronger and more relevant/impactful. This paper (unpublished) is addressing a similar analysis in the context of downstream tasks: https://arxiv.org/abs/2305.13088"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796860761,
        "cdate": 1698796860761,
        "tmdate": 1699636760002,
        "mdate": 1699636760002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NhUSTg7znw",
        "forum": "kE9bsfMgin",
        "replyto": "kE9bsfMgin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_6KTe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6650/Reviewer_6KTe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework to analyze stereotypical bias by identifing biased attention heads, and sheds light on the behavior of transformer-based language models. The recognition of biased attention head is realized by deriving a scalar for each attention head, and then applying a gradient-based head importance detection method on a bias evaluation metric. The experiment findings suggest that attention heads play a crucial role in encoding stereotypical biases in pretrained language models, and identifying and mitigating these biases can improve the fairness and inclusivity of natural language processing applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed framework provides a principled and systematic approach to analyzing stereotypical bias in transformer-based language models.\n2. The proposed method provides a flexible and model-agnostic way to estimating the bias contribution of each attention head.\n3. The proposed method sheds light on the internal mechanisms of stereotypical biases in pretrained language models, which can inspire future research on improving model fairness and accuracy."
            },
            "weaknesses": {
                "value": "1. The proposed method only focuses on gender and racial bias, and may not be applicable to other types of biases (especially when we can hardly obtain distinct A/B or X/Y groups).\n2. The authors suggest that we can mitigate the biases by removing or modifying the biased attention heads while preserving the model's performance on downstream tasks, but only GLEU results are reported. How masking biased attention heads affects performance on generative tasks also requires further exploration.\n3. Although the analysis method and its finding on the bias analysis are valuable to some extent, this paper lacks of their applications and thus their real contributions to the community are still unclear."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698998856742,
        "cdate": 1698998856742,
        "tmdate": 1699636759902,
        "mdate": 1699636759902,
        "license": "CC BY 4.0",
        "version": 2
    }
]