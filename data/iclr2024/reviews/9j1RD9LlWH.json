[
    {
        "id": "mO6GAzjO9b",
        "forum": "9j1RD9LlWH",
        "replyto": "9j1RD9LlWH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT"
        ],
        "content": {
            "summary": {
                "value": "The paper seeks to build a novel Bayesian Optimization approach that is based on Gaussian Cox Processes for spatio-temporal data. The authors emphasize that Gaussian Cox Processes have never been used within BO settings. The posterior distribution is computed using Laplace approximation and a change of kernel that enables to transform the inference problem into a kernel regression problem. The latter kernel is computed using Nystr\u00f6m approximation. Then the authors present several acquisition functions that are built using posterior mean and posterior variance of the Gaussian Cox Process. Finally the authors illustrate the efficiency of their approach on two types of experiments. First, they show the quality of the mean estimation on synthetic data (4.1.1) and real word data (4.2.1). Second, they show how the BO with UCB function performs for some synthetic dataset (4.1.2) and real world dataset (4.2.2)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written and the overall approach is scientifically sound and compelling. \nThe motivation is clear, which is to provide a BO framework with Gaussian Cox processes.\nAlthough Laplace approximation is a standard tool, the authors provide an elegant way to derive the maximum of the log likelihood through a trick that transform the problem into a standard kernel regression problem so that they can use the representer theorem. The main results are clearly detailed. The use of Nystrom approximation is standard, but Lemma 3 helps the reader to figure out what has been implemented. \nThe numerical experiments section show that the authors made great effort to compare all the components of their methodology with some of the state-of-the-art approaches. In addition, this has been done for simulated data and real-world data."
            },
            "weaknesses": {
                "value": "Although the paper is well written and has many interesting components, there are a couple of points that need to be detailed.\n\nMajor comments:\n\n- My first question is general. In a standard BO setting, we aim at minimizing a function which is costly to evaluate. It seems that this is not the objective of the presented BO problem. It would have been good to make a clear distinction between the two problems.  \n\n - In the literature review, the authors write that \"existing works mostly concentrate on the mean estimation\". I am not an expert in Cox models, but it seems that this has been investigated in [1]. This means that this approach could have also been tested in the numerical experiments with Bayesian Optimization. It also seems that this cited paper uses some tools (Laplace approximation, eigenfunctions decomposition) similar to the ones in the paper. Would it be possible to highlight the main modeling differences with this paper? \nWhat is the theoretical/computational benefit of the paper's approach compared to the cited paper? \n\n-  In Equation (5), the authors claim that the rest of the terms of the likelihood are dominated by the first term when $n$ is large. However, this assumption is not always true in a BO setting (it is not in the numerical experiments). Could the authors comment that point and provide more details?\n\n- The authors claim that they can use the Nystrom approximation to compute the next approximation from new samples in an incremental fashion. Is this step used in the experiment? How does it work in practice? \n\nMinor comments:\n\n- After reading the proof of Lemma 2, there is a point I did not understand. Why does the function $h$ belong to $\\mathcal{H}$? This fact does not look straightforward to me. Perhaps I missed something in the development.\n\n- In the numerical experiments, the authors do not report the results of PIF in Table 2. Is there a reason for that? I've not found it in the paper.\n\n- In figure 2c, it looks like the maximum of the acquisition function is around t=40. This means that we have to sample around $t=40$. However I don't see any sample around $t=40$ in figure 2d. \n\n\n[1] Hideaki Kim. Fast Bayesian Inference for Gaussian Cox Processes via Path Integral Formulation. Advances in Neural Information Processing Systems  2021."
            },
            "questions": {
                "value": "See questions above.\nIs the code publicly available?\nDepending on author responses, I would change my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_dgpT",
                    "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698250335205,
        "cdate": 1698250335205,
        "tmdate": 1700728318809,
        "mdate": 1700728318809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z9BhsCMzZD",
        "forum": "9j1RD9LlWH",
        "replyto": "9j1RD9LlWH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework for conducting BO by leveraging Cox processes. This approach hinges on a Laplace approximation of the likelihood and uses kernel techniques to transform the optimization problem into a RKHS. The framework is empirically evaluated across a range of scenarios, encompassing well-known synthetic functions and real-world databases. The results of numerical experiments indicate that this approach exhibits competitive performance in comparison to other state-of-the-art methods. Unlike the other frameworks, it stands out by enabling BO within the context of Cox process-based models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Theoretical contributions bring together techniques from the machine learning and functional analysis communities. Lemmas and other theoretical developments can be easily verified thanks to the clarity of the discussions. The diversity of examples, which take into account well-studied synthetic functions and real databases, makes it possible to assess the competitiveness of the framework in relation to the literature. The paper is generally well-written and well-organized."
            },
            "weaknesses": {
                "value": "Although the strengths lie in both the theoretical and numerical aspects, the motivation for performing BO in point processes lacks practical utility. For example, in the spatio-temporal application describing tornadoes in the USA, I fail to see how new events (tornadoes involving damage) can be sampled sequentially to promote active learning of the intensity function $\\lambda$. I assume that for illustrative purposes, the authors considered adding the \"closest event\" available in the database that matches the BO's suggestion. Is this correct? If this is the case, and if the size of the database allows tractable implementations, we can consider all events for inference of $\\lambda$. If the model cannot handle the whole database, the BO schema is an interesting idea that promotes a threshold between inference quality and the number of observed point events. However, what can be done if no similar events are recorded in the database?  Can the authors give further details on the practical utility of their framework?\n\nThe authors have suggested publishing the Python codes in a Github repository, but there is no evidence of their existence. I suggest sharing an anonymous repository (e.g. via https://anonymous.4open.science/) for further examination."
            },
            "questions": {
                "value": "**Questions**\n- Are the results in Table 2 consistent, i.e. similar results are obtained for a different seed? If no, the authors must consider several random replicates and provide the mean +- std of the results\n- In Figure 2, at the initial step, the UCB acquisition function suggests adding new events at $t > 90$ (since we seek to maximize such criterion) but they are added somewhere else. Similarly, in step 14, the UCB targets the instants around $t = 40$ but events are again added somewhere else. Besides the authors argue that \"the algorithm keeps sampling by maximizing UCB acquisition function and then improving the estimation based on new samples observed\", the plots do not validate their point. Can the authors further explain the results while clarifying my concern? Is it possible to add extra plots at consecutive steps (e.g. steps 1 and 2) for a better understanding of the BO's choice?\n- In the experiments, the choice of the hyperparameters $w_1, w_2, w_3$ is not discussed. Can the authors precise their values in each experiment and explain how they were tuned? \n- The authors approximate the integral $\\int_{\\mathcal{S}} \\kappa(g(t)) dt$ using an $m$-partition Riemann sum to obtain a closed-form of the posterior covariance. Since such approximation depends on $m$, can the authors discuss the quality of the approximation in terms of $m$ and precise how they tune that value in the experimental setup? Can they also discuss the scalability of the approximation when $d$ increases?\n- The limitations of the proposed framework are not discussed in the paper. Can the authors add a remark on this subject?\n\n**Other minor remarks**\n- Page 3, Table 1: the derivatives of the link functions need to be checked. For instance, $\\dot{\\kappa}(x) = 2x$ (quadratic case), $\\dot{\\kappa}(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$ (sigmoidal), $\\ddot{\\kappa}(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$ (softplus), ...\n- Page 3, Section 3.1: $\\Sigma$ is a **CO**variance\n- Page 3, Section 3.1, after Eq. (2): $\\lambda(t) = \\kappa(g(t)) \\to \\lambda(t)$ (it has been already defined before Eq.(1) ) \n- Page 4, after Eq. (7): However, Equation equation (7)\n- Page 4, after Eq. (8): $\\eta_i$ and $\\phi_i(\\cdot)$ need to be defined in the main part of the paper (they were defined in the supplementary material)\n- Page 5, Eq. (10): $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_m)$ needs to be defined\n- Page 7, Section 4: To precise that further details on the \"evaluation setup\" are given in Appendix G\n- Page 7, Section 4.1.1: to indicate the number of events considered in each toy example\n- Page 9, Figure 5: to indicate the iteration step in each panel \n- In Appendix C, Eq. (26): $h(t_j) = \\langle h, \\tilde{k}(t_j, \\cdot) \\rangle_{\\mathcal{H}_{\\tilde{k}}}$ ($j$ rather than $i$)\n- In Appendix C, Eq. (32): the first line must be $\\sum_{i=1}^{n} \\log(\\kappa(g(t_i))) - \\sum_{j=1}^m \\kappa(g(t)) \\Delta t$. Then, the sign of $\\ddot{\\kappa}^2(\\hat{g}_i) \\Delta$ must be inverted.\n- In Appendix C, Eq. (32): given the proposed notation, it is not clear that the dimension of $\\nabla_{\\hat{g}}^{2} \\Psi(\\hat{g})$ matches the dimension of the $d \\times d$ matrix $\\Sigma$. Can the authors clarify this and/or propose a more readable notation?\n- In the References: laplace $\\to$ Laplace (Illian et al., 2012), bayesian $\\to$ Bayesian (Kim, 2021), to add all the authors in (Lai et al., 1985), to complete the reference (Stanton et al., 2022), to be consistent with the names of the journals and conferences and the style of displaying them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_bJz1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698407733190,
        "cdate": 1698407733190,
        "tmdate": 1699636676342,
        "mdate": 1699636676342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4qqIMcTbW3",
        "forum": "9j1RD9LlWH",
        "replyto": "9j1RD9LlWH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a novel method to estimate the posterior mean and covariance of the gaussian cox process model.\n\nThey do this by first approximating the posterior $p(g|{t_i})$ via Laplace approximation, and then using BIC to further simplify the computation. This is in terms of $\\hat g$, which must be solved for by minimizing Eq. 6. To do this, they use RKHS along with a transformation of kernel to make the problem computationally cheap to solve. Once this is done, the posterior mean and covariance can be estimated by $\\hat g$ and the expression in Eq. (9). For kernels that cannot be expanded explicitly, they also discretize and use a Nystrom approximation.\n\nWith a way of estimating posterior mean and covariance, one now is free to choose an acquisition function for the specific problem being solved. The authors discuss various settings in which different acquisition can be applied within this framework.\n\nExperiments are carried out showing both the modelling of the latent intensity, as well as the full framework applied in various spatiotemporal settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper claims to be the first work on BO using Gaussian Cox Process models. I could not disprove this claim through a short search, and if true, I think shows a clear strength in its originality. Every claim seemed technically sound and I could not find any glaring problems, and there were a myriad of experiments demonstrating the method in various synthetic and real world settings. The results present are qualitatively and quantitatively compelling, and the whole paper is relatively clear to understand and well written."
            },
            "weaknesses": {
                "value": "Because many other people have not used Gaussian Cox Process models for BO before, I wonder how much modelling the latent intensity actually helps. I did not see any results or discussion on this, but it feels like a useful comparison to make to show that using GCP is actually more performant than standard BO."
            },
            "questions": {
                "value": "-I'm slightly confused about Section 3.4 in that it seems like one can choose any acquisition function that would solve their problem. What about using Gaussian Cox enables us to do this in contrast to standard BO?\n\n-Were there any experiments done which could find the posterior mean and covariance in closed form (without Nystrom approximation)? I don't have good intuition for how much expressivity is lost in doing this approximation.\n\n-The paper analyzes this method on spatial-temporal data, but couldn't I use this method with any temporal data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6205/Reviewer_kMEd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788632764,
        "cdate": 1698788632764,
        "tmdate": 1699636676221,
        "mdate": 1699636676221,
        "license": "CC BY 4.0",
        "version": 2
    }
]