[
    {
        "id": "DsdexjwMk1",
        "forum": "h4pNROsO06",
        "replyto": "h4pNROsO06",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_kAh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_kAh6"
        ],
        "content": {
            "summary": {
                "value": "The work proposes a unified framework that clearly explains recent sampling algorithms based on generative models. The improvement is led by replacing KL divergence with log variance loss. Experiments show that with new loss, existing generative sampling can achieve better performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Authors present the work and its theoretical analysis clearly, good writing.\n2. The usage of the new divergence metric leads to much more robust training/optimization, which is a critical issue for existing learning-based sampling algorithms."
            },
            "weaknesses": {
                "value": "1. Although the author claims the framework is new. The underlying math theory and contributions have existed long before.\n2. To further improve the readability, authors should clearly define the log-variance loss and outline the algorithm pseudocode in the appendix. \n3. PIS work shows the network that has information of target distribution gradients can perform much better. Is it also true with new log-variance loss?\n4. I am a bit confused discussion of stop gradient, does it lead to a biased gradient estimation? Is it the same trick used in Chen 2021a?\n5. Can authors provide more descriptions and analysis on log-variance and its computations? How do authors estimate the mean, which is required to calculate the variance?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654258380,
        "cdate": 1698654258380,
        "tmdate": 1699636680521,
        "mdate": 1699636680521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OmUY0lizTi",
        "forum": "h4pNROsO06",
        "replyto": "h4pNROsO06",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_29Lh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_29Lh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for learning to sample from unnormalized target densities. Specific cases of this framework include several recently proposed algorithms (Schrodinger Bridge, Time-Reversed Diffusion Sampler, Path Integral Sampler, Denoising Diffusion Sampler). The framework involves a general divergence between path measures of diffusion processes. This encompasses the most common case of the reverse KL divergence, but other divergences can also be used. The paper introduces another divergence named the log-variance divergence, which is shown empirically to outperform the KL by avoiding mode collapse."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a theoretical framework that encompasses several recently proposed methods for sampling via diffusions. This is a very interesting contribution, in particular from a theoretical point of view, since it allows a unified conceptualization of these methods. The proposed divergence seems to outperform the KL divergence, making it also an interesting contribution to the community."
            },
            "weaknesses": {
                "value": "[Update on Nov 22: authors have answered in details the various points raised below.]\n\nI found the paper quite dense, although I acknowledge that this is partly due to the nature of the contribution which is to propose a unifying framework. In particular, Section 3 assumes precise knowledge of the various samplers recently proposed in the literature. While it is not a problem in itself, I would suggest in case of lack of space to move part of the derivations of this Section to the Appendix.\n\n**More importantly, I am not convinced by the explanations on the advantages of the proposed log-variance divergence in Section 2.3**, although I agree that the experiments show that it outperforms the KL divergence in the presented settings. This is an important issue in my opinion, especially since these advantages are part of the narrative of the paper (see the introduction and the conclusion).\n\nFirst, regarding the choice of $w$, it is stated that this choice allows a balance between exploitation and exploration. I do not understand what this precisely means. I believe this statement needs to be backed by either a theoretical analysis or a numerical experiment (or be removed). It is also stated without justification that choosing $w$ different from $u$ might be beneficial to avoid mode collapse. At the same time, in the experiments, $w$ is chosen equal to $u$ (according to Appendix A.4), and yet mode collapse is already avoided. Furthermore, I did not see where it is specified in Section 3 how $w$ should be chosen for the connection with the various approaches to hold.\n\nRegarding the computational benefit, the proposed method still requires to compute gradients with respect to $u$ and $v$ even though gradients wrt $w$ are not required. It is not clear for me why this entails a significant improvement in computational efficiency. I think the naive implementation with stop_gradient still requires backpropagating through the SDE, although I agree that the computational graph is lighter. Experimentally, the time improvement shown in Figure 6 is marginal. Nevertheless, I believe that a more efficient ad hoc implementation that requires only a forward pass through the SDE is perhaps possible. However, this is not explained in the paper. Furthermore, unless I am mistaken, simulating the SDE is still required in any case. This should be mentioned to avoid inducing the reader into thinking that it is not required (like in score matching).\n\nFinally, Proposition 2.5 is not uninteresting, but it is difficult without further explanations or experiments to understand if it plays a role in the improvement observed in practice in the paper.\n\nAll in all, I think that either more precise explanations must be given on these various points, or the claims regarding the proposed log-variance divergence must be adapted.\n\n**Minor remarks that do not influence the rating**\n+ Proposition 2.3: I think $\\rho$ is undefined.\n+ The title of the paper is not very informative. I think it would be interesting to consider a title more specifically linked to the contributions of the paper."
            },
            "questions": {
                "value": "Do the authors think the proposed methodology could be adapted to the case of generative modeling?\n\nOne could imagine that the variance of the Monte-Carlo estimator of the variance is high, which could explain why a large batch size (2048 in the experiments) is required. Did the authors try comparing their method with the KL divergence for smaller batch sizes? Do the authors still observe an improvement wrt the KL divergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6227/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6227/Reviewer_29Lh",
                    "ICLR.cc/2024/Conference/Submission6227/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658606460,
        "cdate": 1698658606460,
        "tmdate": 1700653059717,
        "mdate": 1700653059717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HqtsvFh8DM",
        "forum": "h4pNROsO06",
        "replyto": "h4pNROsO06",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_i9dA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_i9dA"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a unifying framework for various existing diffusion-based samplers. A log-variance based (rather than KL-based) loss is proposed. The proposed work is evaluated against competing methods on the problem of sampling without data --- i.e., with respect to an unnormalized density function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- There is an extensive theoretical discussion of the provided method and its analytical properties.\n- The authors provide a useful guide to organize recent works on diffusion-based approaches to density-based sampling.\n- The work appears to be mathematically sophisticated and relatively rigorous."
            },
            "weaknesses": {
                "value": "- Limited validation. The generalizations proposed are compared on rather simplistic examples. Though the authors appear to target scenarios where data is not available, data-based modeling is clearly an important application of a diffusion-based sampler. Is there a reason the models are not compared to other non-diffusion methods, e.g., MCMC, normalizing flow, autoregressive, or GAN models?\n\n- Unclear abstract: First, the abstract appears to claim that the authors \"identify [diffusion models] as special cases of the Schr\u00f6dinger bridge problem\". This feels far too bold of a claim to me, as many papers have connected diffusion models to Schrodinger Bridges (and the authors are clearly aware of this). After all, nearly all modern generative diffusion models sample from a target density. Second, the authors focus entirely on the problem of sampling without data -- i.e., w.r.t. a density function. This is in stark contrast to what a general ICLR reader may consider as sampling in diffusion models --- i.e., sampling from a model trained on data. Although it appears that the proposed framework can perhaps be formulated to include this case, the goal of the article is clearly the former and not the latter. In fact, there do not appear to be any meaningful experiments on the latter case. This should be clarified in the abstract, as this fact was not at all clear except for a sentence in the third paragraph of the introduction."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726256188,
        "cdate": 1698726256188,
        "tmdate": 1699636680244,
        "mdate": 1699636680244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b0Ofls4O5B",
        "forum": "h4pNROsO06",
        "replyto": "h4pNROsO06",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_y7Cc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6227/Reviewer_y7Cc"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles sampling problems, such as sampling from unnormalized densities (i.e., target distributions), by using stochastic differential equations (SDEs).\n\nTo do that, the paper first proposes a novel framework to unify previous SDEs-based sampling methods: (1) variational formulation of the Schr\u00f6dinger bridge (SB) problem, (2) path integral sampler (PIS), (3) time-reversed diffusion sampler (DIS), and (4) denoising diffusion sampler (DDS). In the framework, the authors show that we can obtain the analytic form of the Radon-Nikodym (RN) derivative of one SDE wrt another SDE, evaluated at any path generated by the third SDE, as long as all SDEs have the same noise coefficient term (while time direction can vary). Then, the authors show that we can define proper divergences between two SDEs using the RN-derivative (by taking expectation with the RN-derivative). Furthermore, the authors show that the previous SDE-based sampling problems can be defined as the minimization of the reverse KL divergence (or its variant) wrt a reference measure; more specifically, the choice of the reference measure and forward will determine the corresponding sampling problems.\n\nSecond, based on the framework, the authors propose to use the log-variance divergence instead of the reverse KL divergence thanks to several benefits of the log-variance against the reverse KL; for example, the log-variance divergence uses a reference measure and thus does not require to differentiate through the SDE solver. The paper shows all four variants based on the log-variance divergence corresponding to SB, PIS, DIS, and DDS.\n\nFinally, the paper demonstrates the performance of the proposed methods against the previous works by using some benchmark datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "In my understanding, the paper's contributions are clear, and I also consider that the results are essential for several reasons:\n\n1. The paper proposes a novel framework that provides better understanding of previous literature on sampling problems using SDEs.\n2. The paper well motivates the log-variance divergence-based methods so that readers can understand how each step contributes to the merits of the proposed method.\n3. I found that the paper has a well-organized structure that makes it clear to understand the proposed method. Significantly, the paper shares sufficient information to understand the new divergence\u2019s behavior, which will help readers understand relevant backgrounds and the proposed method.\n\nIn general, the paper's contributions wrt the novelty are clear, and the proposed methods are well-defined. In addition, I found that the paper has a well-organized structure that makes it clear to understand the proposed methods."
            },
            "weaknesses": {
                "value": "In general, I find that the paper is well-written. However, descriptions of some derivations can be improved for clarification. For example, to introduce the derivation of Equation (10), the paper uses \u201cdefined in (2) with $u$ replaced by $r \\in \\mathcal{U}$\u201d. I find this description a little confusing, as Equation 10 assumes three SDEs. Clarifying such descriptions would be helpful to potential readers who are not familiar with SDEs and relevant backgrounds."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986014981,
        "cdate": 1698986014981,
        "tmdate": 1699636680136,
        "mdate": 1699636680136,
        "license": "CC BY 4.0",
        "version": 2
    }
]