[
    {
        "id": "RM12egHW4E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_4xUi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_4xUi"
        ],
        "forum": "FNq3nIvP4F",
        "replyto": "FNq3nIvP4F",
        "content": {
            "summary": {
                "value": "This paper proposes a method to train a diffusion model with random masking on the frame level to enable video generation, prediction, and interpolation. They demonstrate that their method is able to generate longer video and create smooth transitions between two different frames. The authors demonstrate that their method outperforms baseline methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is well written, and clear\n- The paper shows good long video generations, and is able to generate complex transitions between semantically different frames"
            },
            "weaknesses": {
                "value": "My primary concerns center around the lack of baselines and novelty. Particularly, the authors fail to cite a few very related works, that accomplish similar tasks that enable frame prediction and interpolation.\n\n- MaskViT [1], MAGVIT [2]: MaskGit-like models trained on tokenized video frames. Given the masked learning object, these models can also usually generalize to enable generation, prediction, and interpolation. MAGVIT is trained explicitly to do this. \n- MCVD [3], RaMViD [4]: These two methods seem nearly identical to the proposed method, where a video diffusion model is trained with masked latents. An exception is a lack of text-conditioning and scale in [3,4], however, I do not believe that meets the bar as a point of novelty.\n\nCould the authors please clarify on how their method is novel over the prior work mentioned above? In addition, it would be necessary to compare against a subset of these methods as baselines (or a similar model), as currently there are no baselines explicitly trained for the prediction / interpolation tasks.\n\n[1] Gupta, Agrim, et al. \"Maskvit: Masked visual pre-training for video prediction.\" arXiv preprint arXiv:2206.11894 (2022).\n\n[2] Yu, Lijun, et al. \"Magvit: Masked generative video transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Voleti, Vikram, Alexia Jolicoeur-Martineau, and Chris Pal. \"MCVD-masked conditional video diffusion for prediction, generation, and interpolation.\" Advances in Neural Information Processing Systems 35 (2022): 23371-23385.\n\n[4] H\u00f6ppe, Tobias, et al. \"Diffusion models for video prediction and infilling.\" arXiv preprint arXiv:2206.07696 (2022)."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697138815332,
        "cdate": 1697138815332,
        "tmdate": 1699636813727,
        "mdate": 1699636813727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Kec6CJ0T1S",
        "forum": "FNq3nIvP4F",
        "replyto": "FNq3nIvP4F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_Yzaz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_Yzaz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new problem of generative transition and prediction, which can help generate story-level videos through different shot transitions. The author also proposed a short-to-long video diffusion model, which utilizes a random mask strategy for training. To evaluate the task, this paper proposes three assessing criteria. Both objective and subjective evaluation prove their proposed method\u2019s effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This article addresses the limitation of existing models that can only generate shot-level videos and proposes a method to generate story-level videos using transitions. They extend an existing video generation framework and achieve impressive results in generating long videos. They also propose a reasonable evaluation framework to assess the proposed model, and a large number of demos and quantitative evaluations demonstrate the effectiveness of their approach. The contribution of this work is significant."
            },
            "weaknesses": {
                "value": "The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on. Additionally, the author should also report scores on commonly used evaluation metrics such as FID."
            },
            "questions": {
                "value": "I wondered how many GPUs they used and how long it takes for training. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697942634206,
        "cdate": 1697942634206,
        "tmdate": 1699636813597,
        "mdate": 1699636813597,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T4GH6cFaQc",
        "forum": "FNq3nIvP4F",
        "replyto": "FNq3nIvP4F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author focus on a new task, \"generative transition\", which aims at smooth and creative transitions between scenes. Specifically, this paper proposes SEINE, a short-to-long video diffusion model with random masks to generate transitions frames based on textual prompts that describe transitions. Given a few unmasked frames, the proposed random-mask based diffusion model is able to generate frames at arbitrary positions. Therefore, their model can be used for tasks including generative transition, long video generation, and image-to-video animation by giving unmasked frames at different positions. \n\nIn the experiments, the authors compare their model with other baselines including morphing, VQGAN-based transition, and SD-based transition. Quantitative and qualitative results show that SEINE has better transition temporal coherence, semantic similarity across frames, and better video-text alignment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The task of generative transition is novel and rarely explored, which I believe is one of the main novelty of this paper. As current text-to-video generation models are mostly tacking short video clips, a smooth and creative transition between these short clips is of increasing importance.\n\n- The proposed random-mask based model seems a good solution for this task. In addition to generating transition frames, the random-mask based model can also deal with long-video generation and image-to-video generation by giving unmasked frames at different positions. \n\n- The quantitative and qualitative experiments demonstrate the effectiveness of the proposed model."
            },
            "weaknesses": {
                "value": "- From the qualitative result shown in Figure 6, it seems that the transition is more like a \"interpolation\" between two scenes. For example, the frames in (row1, col4). (row2, col2), and (row2, col3) are not very natural. \n\n- In Figure 5 right part (the cat example), it seems that morphing also provides a descent transition. So for two frames with small transitions needed in between, it seems that the proposed method might add unnecessary variety/creativity. \n\n- In general, it's hard to see if the proposed method provides a good solution to this new task. The paper also lacks enough ablation study of the model architecture design. More discussions and intuitions about this task would be helpful for future works.\n\n- Some minor things: \n1. In Sec. 2, the citation for PYoCo is missing. \n2. In the last paragraph of Sec. 3, the sentence describing \"Long video\" is incomplete.\n3. Figure 4 is not easy to understand at first glance. It would be nice to add more descriptions for better readability.\n4. In Figure 10, the image on the left part has red-green-blue watermarks. Is that example from Gen-2 instead of SEINE?"
            },
            "questions": {
                "value": "- For controllable transition generation, do we give the first and last frames unmasked to the modl for each prompt? If this is the case, I'm wondering maybe the model can also generate smooth zoom-in/out transitions without explicitly adding \"camera zoom-in/out\" in the prompt. It would be nice to provide ablation study that removes \"camera zoom-in/out\" in the prompts and see if the generation quality deteriorates.\n\n- As mentioned in the above part, it would be nice if the author can provide some discussions about what kinds of scene transitions (small transition vs large transition, same object vs different objects) their model is good at."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812782343,
        "cdate": 1698812782343,
        "tmdate": 1699636813487,
        "mdate": 1699636813487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Uik1TO8KOa",
        "forum": "FNq3nIvP4F",
        "replyto": "FNq3nIvP4F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_GChA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6964/Reviewer_GChA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents the SEINE model, for generating \"story-level\" long videos from short clips. It introduces a unique problem in generative transition and prediction. Using a random-mask video diffusion based on textual descriptions, the model shows smooth transitions between scenes. To evaluate its efficacy, the authors provide three new criteria: temporal consistency, semantic similarity, and video-text alignment. Results show its potential for generating coherent long videos."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method of using masks was proposed in [1] and [2], but as far as I know, this is the first time it has been used in video transition. It could be novel.\n\n- The proposed method shows better performance on the metric compared to the baseline.\n\n- The proposed method can be applied in various areas such as long video generation and image-to-video animation.\n\n\nReferences\n\n[1] Voleti, Vikram, Alexia Jolicoeur-Martineau, and Chris Pal. \"MCVD-masked conditional video diffusion for prediction, generation, and interpolation.\" Advances in Neural Information Processing Systems 35 (2022): 23371-23385.\n\n[2] Fu, Tsu-Jui, et al. \"Tell me what happened: Unifying text-guided video completion via multimodal masked video generation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "weaknesses": {
                "value": "- [Major] My main concern is that there is not enough quantitative evaluation of video transitions. This paper conducted quantitative experiments by randomly selecting one caption from MSRVTT and determining CLIP-TEXT. However, since video transitions occur when scenes change, it does not seem appropriate to evaluate video semantic correlation. Also, no video quality evaluation metrics (such as FVD etc.) have been considered. This makes it difficult to quantify the exact quality of generation.\n\n- [Major] Several details related to the human evaluation are missing. (such as number of frames in the generated video, the dataset used, and the questions posed in the user study.)\nWas the user study appropriately reflective of temporal coherence, text-video alignment, and semantic similarity?\n\n- [Minor] For transitions to be applied in the real-world, it would require generating more than 16 frames. Would the quality be maintained if more frames are generated?\n\n- [Minor] In Figure 5 related to video transition, the frame numbers and details are omitted."
            },
            "questions": {
                "value": "How long does the inference take? Is it capable of handling transitions with multiple objects across more than two scenes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848627748,
        "cdate": 1698848627748,
        "tmdate": 1699636813292,
        "mdate": 1699636813292,
        "license": "CC BY 4.0",
        "version": 2
    }
]