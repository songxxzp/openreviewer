[
    {
        "id": "WzTKlCpFu8",
        "forum": "lwtaEhDx9x",
        "replyto": "lwtaEhDx9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_YJg3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_YJg3"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at inspecting data contamination which happens when training a large language model. Specifically, they inspect whether some tabular datasets were used to train the language models. For this purpose, they propose several approaches to testing the language models.\n- Testing for Knowledge and Learning:\n    - Meta data: Testing whether the model can predict the name of the fields in the datasets.\n    - Conditional completion: Testing whether the model can predict the value of a feature of a sample based on some other features.\n    - Unconditional zero-knowledge samples: Testing whether the model produces the statistics of the features in a datasets.\n- Testing for memorization:\n    - Testing whether the model can generate rows based on the previous rows (either the first a few rows or a few rows starting from a random line) in the dataset csv file.\n    - Testing whether the model can generate the correct value of a feature based on the value of the other features. Here the value is \u201cunique\u201d.\n    - Testing whether the model can generate the first token of the next row.\n\nFinally, they compare the language models\u2019 performance on several datasets with some baseline models\u2019 performance, and conclude that some models\u2019 performance can be attributed to their memorization of the datasets during pretraining."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- They propose several methods to test whether the language models were trained with those datasets.\n- The idea of comparing the distribution generated by the model and the distribution in the datasets (Sec 3.3) is novel and interesting.\n- The claim that they show that some language models are pretrained with some tabular datasets is somewhat convincing and interesting."
            },
            "weaknesses": {
                "value": "Main concerns:\n\n1. This work does not provide strong evidence supporting the validity of their proposed approaches. I think one main takeaway of this paper is that some models are pretrained with some datasets, so their performance is not indicative of. But this takeaway is based on the validity of their proposed approaches. I think the authors need to address this more.\n2. I can\u2019t understand the purpose of having these many different testing approaches, probably because the structure of this paper is hard to follow. The authors propose many approaches, some of them are interesting, but they do not provide a holistic interpretation of the results from these many approaches.\n3. The descriptions of the testing approaches are vague and not rigorous. Writing down the testing approaches with simple math equations could help. For example, in page 6, I can\u2019t understand what it means by \u201cwe can perform a t-test between the similarity of model completions with actual vs. random rows.\n4. Knowledge, learning, memorization should be defined more specifically.\n5. The authors (claims to) show data contamination exists in some datasets. However, I am not sure whether those datasets are commonly used to benchmark the language model. Thus I am not sure whether the findings are important (if they are valid).\n\nMore specific (writing) issues:\n\n\n2. The second paragraph in Sec 3.2: Here 4 possible causes are provided, but I don\u2019t see how they are discussed in the following experimental designs.\n3. The last sentence in page 5: \u201cEmpirically, we find \u2026 a very intuitive test \u2026\u201d. I don\u2019t understand how your empirical results support this.\n4. Page 8: \u201cIt might be that this learning task is relatively simple, that our memorization test are not sensitive enough\u201d. I can\u2019t understand why it is the case.\n\n\nGrammar:\n\nThere are many grammar errors. I suggest that the authors do some proofreading."
            },
            "questions": {
                "value": "1. Figure 1: I suggest to use bar charts is more reasonable because your x-axis is not continuous."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697866562424,
        "cdate": 1697866562424,
        "tmdate": 1699636958544,
        "mdate": 1699636958544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qZSAjmfZao",
        "forum": "lwtaEhDx9x",
        "replyto": "lwtaEhDx9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a discussion on data contamination and memorization in large language models concerning tabular data. The authors propose multiple methods to examine whether an LLM has memorized specific tabular datasets during training. The paper also proposes methods to examine knowledge, learning, and memorization separately and discuss the distinction between them. Finally, the paper analyzes the influence of learning and memorization on the performance of downstream tasks, and advocates checking memorization as a crucial step in evaluating LLM on tasks with tabular data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents several novel methods to evaluate memorization of tabular data in LLMs, and evaluation results on a series of datasets correlate well with the publication time and availability of the data, confirming the effectiveness of the proposed methods in identifying memorization. The different evaluation methods also complement each other, elucidating the different aspects of memorization of tabular data.\n\nThe paper is overall well-written and very easy to read, the visualizations present the main findings nicely.\n\nThe contamination and memorization of training data by LLMs is a critical issue. The findings provoke essential discussions on the evaluation of LLMs on tabular data, which is likely to become more relevant given the rising usage of LLMs in diverse tasks. \n\nThe introduced tools and code potentially provide easy and accessible ways to evaluate memorization of tabular data, reusable in future research."
            },
            "weaknesses": {
                "value": "Some important details in the experiment design may be missing or incomplete: \n\n* Evaluation metric for knowledge, learning, and memorization is unclear. In Table 1, the evaluation results are categorized into three categories (\u2713,X, and ?), but the metric for the categorization is not given. It is probably a better idea to show the raw values (e.g., accuracy) than using categories to give the reader a direct comprehension of the degree of memorization on each dataset. Notations such as \"\u2713\" could be misleading as it may be confused as perfect memorization.\n\n  The appendix gives raw accuracies for Row Completion Test, Feature Completion Test, and First Token Test, why raw accuracies for Feature Names, Feature Values, and Header Test are not provided as well?\n\n* The differentiation between learning and memorization is not clear: the authors use feature distributions to examine learning, but memorization can also result in a high similarity of the generated data's feature distributions to the original data. Learning is defined as the model's ability to perform tasks in the current paper, but task performance is heavily affected by memorization and may fail to reflect true learning. Even with considerable discussion, the paper does not seem to arrive at a conclusion about how learning can be clearly assessed.\n\n* Evalulation of memorization needs to take the nature of data fields into consideration. Some data fields in the tabular dataset are considerably harder to memorize verbatim or to predict exactly (such as measurement values) than other simpler fields (categorical values such as sex, occupation, nation). For numerical values, it may be more reasonable to measure the relative distance from the predicted value to the true value than using exact match (perhaps in a similar vein as the \"first token test\" in the paper but more principled).\n\n  Under the current evaluation protocol, it is likely that datasets containing more easy fields are more likely to be judged as memorized. To compare the degree of memorization across datasets, it seems necessary to perform some kind of \"normalization\" before measuring memorization, for example, selecting a fixed number of categorical and numerical fields from each dataset. Results in Table 3 could suffer from this limitation as well.\n\n* Evaluation of memorization needs to be evaluated separately for the training and test split. It may be possible that the training sets are memorized more than the test set due to more exposure on the internet. Memorizing the test set definitely compromises evaluation, but memorizing the training set may not always compromise evaluation.\n\n* Connection between memorization and downstream performance is not reliably established. The main observation from Section 5 is that for datasets with a high degree of memorization, LLM performs better than decision tree and logistic regression, while for datasets with a low degree of memorization the reverse is true. Such observation alone may not be sufficient to conclude that memorization compromises evaluation, because there is no evidence that LLM cannot perform better than decision tree and logistic regression under no memorization. It would be much better to solicit new test sets for the tasks to use in evaluation, which can be used to show exactly how much performance gap is caused by memorization. In case finding new examples is difficult, perhaps one can modify the values of the fields known to be irrelevant to the label in existing examples, and that may break the reliance on memorization in LLMs.\n\nSome main conclusions of the paper are compromised because of the above limitations:\n\n* \"We emphasize the importance of verifying data contamination before applying LLMs\": the implication of data contamination is not reliably demonstrated in Section 5.\n\n  Also, from the current discussion, it is not very clear how to interpret the test results on knowledge, learning, and memorization together. For example, if knowledge and learning show positive results and memorization show negative results, should we conclude that there is data contamination or not? And could the performance on downstream tasks be trusted in this situation?\n\n  It can be argued that knowledge and learning will not directly compromise evaluation on downstream tasks, so there may not be as much need to evaluate them compared to memorization. I would suggest allocating more space in the paper for extended experiments and discussions on memorization, which is the ultimate reason why people are concerned about data contamination. \n\n* \"... and propose practical methods to do so\": the proposed method verifies memorization of data, but does not give a definite metric to judge when memorization is severe enough to compromise evaluation.\n\n* \"We offer a principled distinction between learning and memorization in LLMs\": the distinction is not given clearly enough. One can tell whether there is memorization from the proposed test, but it is not clear how to tell whether learning exists (especially when memorization is present)."
            },
            "questions": {
                "value": "What is the difference in the remembering behavior of LLM between tabular data and non-tabular data? The question may help strengthen the original contribution of the paper.\n\nIn the beginning of the discussion section, the use of the term \"representation learning\" may be confusing to some people. Representation learning usually refers to the process of learning useful features from raw data (wikipedia), which does not include memorization by definition."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Reviewer_aXk7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697977363042,
        "cdate": 1697977363042,
        "tmdate": 1699636958422,
        "mdate": 1699636958422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MAjYHtyiHz",
        "forum": "lwtaEhDx9x",
        "replyto": "lwtaEhDx9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_ELKr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_ELKr"
        ],
        "content": {
            "summary": {
                "value": "LLMs are increasingly being applied to various types of data, including tabular data. Since, at the moment, the most advanced LLMs are essentially black-boxes w/ restricted APIs with little details available about their training data, it is hard to tell a priori whether the tabular data was leaked into the training and whether the models have memorized it.\nThis paper proposes four tests that probe an LLM for the training data contamination and estimate the degree of the contamination (\u201cknowledge\u201d, \u201clearning\u201d, \u201cmemorization\u201d).\n\nIn the experimental study, the paper focuses on ChatGPT 3.5 & 4 and 10 tabular datasets, which have a high chance of being in the training data obtained by crawling the Internet (Iris, Kaggle Titanic, \u2026). \n\nFirst, the authors show that both LLMs have memorized basic meta-data from the datasets. Next, the LLMs are probed for an ability to reproduce a dataset example, conditioned on a part of its features. As an example, on the Adult Income dataset, the LLMs completed EduNum feature significantly better than a marginal distribution baseline. Further, the authors propose \u201czero-knowledge\u201d prompting technique where the model is prompted to sample samples from a dataset (unconditionally or conditionally), provided samples from other datasets. Using that approach, the authors show that the models often can reproduce the distribution of the data in some datasets (approximately). Finally, the models are probed to reproduce parts of the datasets verbatim; for some datasets that happens extremely often.\n\nAdditionally, S5 provides a comparative analysis of the LLMs performing few-shot classification tasks on a subset of datasets, in comparison to standard ML baselines. It turns out that the LLMs have marked drop in performance on some datasets that are likely absent in training (Pneumonia & Spaceship Titanic); at the same time very high performance on datasets that are likely to be memorized (Kaggle Titanic)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* I believe this work (a) raises an important overlooked question, (b) addresses it, (c) by proposing an original technique. I particularly like the zero-shot prompting technique that allows sampling from a dataset w/o leaking information in the prompt.\n* The paper disentangles a few levels of training data contamination and comprehensively tests for those.\n* The paper showcases the potential impact of the contamination on the downstream comparisons, hence proving a strong motivation to the work. \n* The text and the story are clear.\n* The code is made public."
            },
            "weaknesses": {
                "value": "* The paper only studies ChatGPT-3.5 and 4. Those are very likely to be strongly correlated in terms of the data used, which harms the representativeness of the study.\n* As there is no ground-truth knowledge on whether a particular dataset was seen at training, it is impossible to strictly verify the findings. Including an LM trained on a known dataset would allow us to verify the used methods.\n* Another related issue: the work is mostly relevant when we consider closed-data models w/ a black-box API access. This scenario reflects a dominant situation at the moment, but it is not given that this will not/should not change.\n\nMinor:\n* Table 1 is mentioned on page 3, yet only appears on page 6. Is there a way to bring it closer?\n* Would it make sense to consider swapping sections 5 and 6? I feel the S6 is more connected to the S3-4 than S5."
            },
            "questions": {
                "value": "I wonder if authors would be willing to address the first two points in \u2018weaknesses\u2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698408062111,
        "cdate": 1698408062111,
        "tmdate": 1699636958285,
        "mdate": 1699636958285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2QzbpIB3Le",
        "forum": "lwtaEhDx9x",
        "replyto": "lwtaEhDx9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
        ],
        "content": {
            "summary": {
                "value": "This paper specifically targets the issue of contamination in training sets when evaluating LLMs on tasks with tabular data.\nCompared to previous work on LLMs for tabular data, the authors propose methods to test the LLM for memorization (in addition to the dimensions \"knowledge\" and \"learning\").\nThese novel tests help to better analyze and understand the performance on downstream tasks, such as deciding if the data has been seen in training or not."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* LLMs are pervasive currently, and it's important to understand and control their behavior. The authors emphasize the importance of verifying data contamination before applying LLM.\n* Their setup based on tabular data is an elegant way to test \u201cknowledge\u201d, \u201clearning\u201d, and \u201cmemorization\u201d of an LLM.\n* Moreover, they assume only blackbox API access, without assuming access to the probability distributioin over tokens or the ability to re-train the model.\n* Release of an open-source tool that can perform various tests for memorization."
            },
            "weaknesses": {
                "value": "* My main point of criticism is that the paper feels a bit like a collection of remarkable examples and the analysis largely confirms known concerns/behavior of LLMs. \n* \"we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim\": It's not clear to me what this statement means. See also Question 1 below.\n\n* Just echoing the authors: \"A limitation of our work is that we do not have access to the training data of GPT-3.5 and GPT4.\" I.e., the interpretation of results often remains speculative.\n* Figure 3: Why are some results with gpt-3.5 and some with gpt-4?\n* Typo: \"two publicly available dataset that are highly memorized\"\n* Typo: \"UCI repository athttps://\""
            },
            "questions": {
                "value": "1. \"\u200b\u200bAn important result of our investigation is to identify a regime where the LLM has seen the data during training and is able to perform complex tasks with the data\": Don't LLMs behave as expected on some data and not on other? How does this work help to control the behavior of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7826/Reviewer_8936"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827434789,
        "cdate": 1698827434789,
        "tmdate": 1699636958160,
        "mdate": 1699636958160,
        "license": "CC BY 4.0",
        "version": 2
    }
]