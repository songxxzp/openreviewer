[
    {
        "id": "d2ONRTril0",
        "forum": "brOAVSPPjw",
        "replyto": "brOAVSPPjw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission641/Reviewer_UPFL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission641/Reviewer_UPFL"
        ],
        "content": {
            "summary": {
                "value": "The work presents the learning dynamics of both single value network and target network settings for residual reinforcement learning. It employs the Neural Tangent Kernel (NTK) framework to derive the learning dynamics. The authors pose this as a theoretical model to understand an ensemble of NNs trained empirically. They use the linearization (in parameters) technique for writing the dynamics of how the parameters change thereby turning the problem into a linear ODE. They also provide experimental results for a toy domain and the cartpole domain to compare the parameter dynamics and the empirically trained NN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n\n- The authors have written it very well. I found all the technical details fairly accessible. They provide necessary background on NTK.\n\n- The derivation of dynamics for the TD case, where you are learning using residual gradient descent, are quite informative and useful.\n\n- They provide interesting experiments with a toy example and the undiscounted cartpole environment."
            },
            "weaknesses": {
                "value": "While I found the paper to be an interesting read I observe some crucial issues with the work which need addressing.\n\n**Lack of related work:** While the authors have cited relevant work in DL theory, they are missing come crucial citations [1, 2, 3, 4]. The issue is that previous works have explored similar problems. For example, the setting where gradient \"flows\" through both $Q_{\\theta}(s, a)$ and $Q_{\\theta}(s', a')$ is called residual learning [3, 4]. The equation number 7 by the authors has a single sample analog in [3] (see Equation (1) in the paper). Baird [4] introduced the idea of residual algorithms to make the TD algorithm more principled. Further see related work [5]. Please let me know if I am mistaken about this connection. [1] has a different type of linearization: where they linearize the learnt function in the first layer weights for single hidden layer networks, while this is not directly applicable it has a connection with the current work. Similarly, [3] gives insights into when and if TD with NNs and homogenous activations converges albeit in discrete setting, depending on the environment.\n\n**Not a substantial enough conclusion:**  I appreciate that the authors have derived and written down the dynamics of learning in the NTK regime. While it is a useful contribution, I am afraid that I am not able to discern a valid and strong conclusion. \n\nI see that the authors have introduced the TD version of $\\Theta_0$ (just like Lee et al. 2019) but I would be interested in how this matrix effects the convergence. I believe this is where the environment dynamics come into play. Further, I would expect a theorem similar to Theorem 2.1 by Lee et al. 2019 (latest arxiv version). Theorem 2.1 by Lee et al. 2019 provides a sense of how far the linear approximation $f^{lin}$ is from the true function $f$ i.e. order of $1/n^{-1/2}$. I would expect something similar with relation to the environment dynamics (and not only the width of the NN) for the work to be more applicable to TD learning.\n\nSimilarly, Brandfonbrener and Bruna, 2020 [2] provide some insights into when and if neural TD learning converges depending on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. This brings in aspects of reinforcement learning and transition dynamics. This issue also relates to the previously stated issue of a lack of related work.\n\nFurther, the experiments seem a bit inconclusive to me, especially the experiment on cartpole domain. While I appreciate the new comparisons in Figures  3 and 4, even in a toy environment we see the parameters behaving differently (in Figure 1). The authors also remark that their analytic model does not match the empirical model in Section 5.1 but no reasoning is provided. For cartpole the result is surprising the linearized dynamics work better and is definitely a meaningful contribution. I would be curious to see results normalized by the true values? As far as I can tell, both NNs and TD-NTK do not \"fit very well\" with different degrees of \"not fitting very well\".\n\n**Statement of assumptions:** I would be curious to know when TD-NTK does not converge? I would be surprised if it converges everywhere without assumptions (as I presume is noted in Equation 31 in the appendix). For example, [5] and [4] both say that residual RL might not always converge. Similarly, Lee et al 2019 assumed $\\lambda_{min}(\\Theta_0) > 0$. Further discussions regarding assumptions can be found in [1] and [2]. These assumptions are better stated clearly.\n\nI do not intend to discourage the line of work the authors are pursuing. It is very important for the research community that we have a concrete theory of Deep RL that relates to practice. The training dynamics derivation is very useful and much appreciated but the work would fair better with a stronger conclusion and connections to previous work.\n\n****\n\n**References**\n\n[1] Neural Temporal-Difference and Q-Learning Provably Converge to Global Optima, Qi Cai, Zhuoran Yang, Jason D. Lee, Zhaoran Wang, 2019\n\n[2] Geometric Insights into the Convergence of Nonlinear TD Learning, David Brandfonbrener, Joan Bruna, ICLR 2020\n\n[3] Deep Residual Reinforcement Learning, Shangtong Zhang and Wendelin Bohmer and Shimon Whiteson, 2019\n\n[4] Residual algorithms: Reinforcement learning with function approximation, ICML 1995\n\n[5] Gradient temporal-difference learning algorithms, Richard S. Sutton and Hamid Reza Maei, 2011"
            },
            "questions": {
                "value": "**Questions:**\nWhile most questions are above I present a few more here:\n1. What is the state space in the cartpole environment? \n2. How was the behavior policy learnt in cartpole environment?\n3. What is the idea behind deriving the expression for $Q^{lin}(\\mathcal X^*)$? Is this related to generalization (I guess similar to Lee et al. 2019)? \n4. If you consider the central graph in Figure 1, it shows that the difference in parameter dynamics is substantial.\n5. I would expect that the comparison should be if the $Q$ functions predict the right action (as opposed to the accuracy), across empirical and analytic models, as it is well known that a \"wrong\" NN Q function can lead to a good policy as seen in Figure 3 of the DDPG paper (arxiv version). This would be interesting in addition to Figure 2. Same applies for table 1.\n\n\n**Minor issues:**\n\n1. Section 2.1 we will complement -> we complement\n2. Ideally, I wouldn't reference Equation 6 from Section 4.1 under Equation 2 in Section 3.\n3. Section 3: forgot an opening bracket for \"such as a sigmoid or ...\"\n4. $\\mathcal X$ might be better defined as a matrix than a \"stack\"?\n5. $\\theta$ -> $\\theta_t$ in the caption below the expression for $Q^{lin}_{\\theta_t}$. Also, missing a comma after the expression.\n6. $w$ is overloaded in section 2: $w_t in Equation 5 and $w_{i,j}$ below Equation 3.\n7. Missing full stop at the end of equation 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698438430981,
        "cdate": 1698438430981,
        "tmdate": 1699635991799,
        "mdate": 1699635991799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G4BgWShyvw",
        "forum": "brOAVSPPjw",
        "replyto": "brOAVSPPjw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission641/Reviewer_5Tmm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission641/Reviewer_5Tmm"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to apply the infinite-width neural network learning via the neural tangent kernels (NTK) approach recently introduced in the supervised learning setting to an (offline) reinforcement learning (RL) setting. \nThe paper first derives some closed-form expressions for the training dynamics of the Q-network parameters using the NTK approach. Then, experimental evaluation using a custom toy environment and the cart-pole environment is provided to demonstrate the effects of uncertainty quantification of Q-network ensembling."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper attempts to study an ambitious and non-trivial problem of applying the infinite-width neural networks approach known in supervised learning to the RL setting.\n2. Some promising results are demonstrated in the reported preliminary experimental evaluation, which can be the basis of a future more extensive study."
            },
            "weaknesses": {
                "value": "Although the studied research direction is interesting and promising, the paper has some significant flaws, and the contribution of the paper is weak in its present form. The study requires a significant extension to be further considered for publication. \n\n1. The theoretical contribution of the paper is merely the calculation of NTK dynamics for Q-learning by supervised learning over fixed datasets of rollouts adapted from existing work [ Lee et al. (2019) ]. This part does not bring any new contributions or insights.\n\n2. The title, abstract, and introduction of the paper need to be more accurate. \nSetting studied in the paper are not those of online or offline reinforcement learning. The studied setup can be characterized as a Q-learning approach via supervised learning over a fixed dataset. Namely, a dataset of dumped policy-environment interactions is used to train the parameters of a Q-network. It differs from the standard online and offline actor-critic RL setups in which the actor (policy) and critic (Q-network) is approximated simultaneously.\nNamely, the work ignores the issue of approximating the policy action on the next-state; this kind of analysis detached from policy properties is a pure Q-learning.\nAs stated, the Bellman bootstrap equation reads $Q^{\\pi}(s, a) = r + \\gamma \\sum_{s'} P(s'|s,a) \\pi(a'|s') Q^{\\pi}(s', a')$, but it does not lead to the supervised learning loss function (6). $\\mathcal{X}'$ is unavailable in the RL setup. The dataset MDP tuples  $ (s_i, a_i, r_i, s_i')$ does not contain the next actions; they should be obtained from the policy instead, as the Q is an estimated value of the trajectories obtained _by following the policy $\\pi$_. It is a separate issue of how to regularize and avoid issues with distribution shift at this stage addressed by modern Offline RL methods, including TD3-BC [1] and FDCR [2].\n\n[1] Kostrikov I., \"Offline Reinforcement Learning with Fisher Divergence Critic Regularization\", ICML 2021\n[2] S. Fujimoto and S. Gu, \"A Minimalist Approach to Offline Reinforcement Learning\", NeurIPS 2021\n\n3. Experimental evaluation could be stronger. Only simple environments (a toy one and a cartpole are used) are particular, and it is unclear how the obtained conclusions will transfer to more complex environments widely used as benchmarks (MuJoCo).\n\nMinor remarks:\n* p.5 below (11) \"maximum and minimum values of \u0398.\" -> eigenvalues,\n* p.5 below (11) \"This is expected because in this regime learning the value function is essentially a\nsupervised learning task of learning the immediate rewards, and all terms with policy dependence\n(those involving X \u2032 ) drop out as would be expected in a supervised learning task.\" in supervised learning task that is true, but unfortunately not in an RL task :)\n* p. 6 (5.1) \"the value will be $1$ (taking $\\gamma = 1$) for all $s < 1$ and $0$ otherwise.\" I think the condition is switched here (value $0$ for $s<1$).\n* p. 8 (5.2) \"the true returns obtained by rolling out the evaluated policy in the environment to evaluate our method.\" the issue of policy-generating data requires more attention in the experimental section.\n* derivations in C.1 and C.2 are not needed, these are solutions of simple non-homogenous differential equations,"
            },
            "questions": {
                "value": "See above. At present I think that the current paper needs to be framed as a supervised Q-learning  to convey a proper message. Offline RL setting would require to include the policy in the analysis. The experimental evaluation need to be extended beyond toy-problems to show that the conclusions are true for larger scale benchmark problems and NTK is a proper tool for studying the statistical properties of Q-ensembles."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584894162,
        "cdate": 1698584894162,
        "tmdate": 1699635991677,
        "mdate": 1699635991677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "weCbseDOhC",
        "forum": "brOAVSPPjw",
        "replyto": "brOAVSPPjw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission641/Reviewer_bQbn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission641/Reviewer_bQbn"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the training dynamics of linearized neural network (NTK approximation) temporal difference learning. The authors give solutions to the differential equations for full-batch gradient flow in two settings: learning with a single Q-network, and learning with a separate target network which is an exponential moving average of the primary Q-network. They also give closed-form solutions for the uncertainty from an ensemble of randomly-initialized neural networks under the NTK approximation in these settings. The analytical solutions are compared to actual neural networks trained on two toy domains include CartPole."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is generally clear and easy to follow. The derivation of the training dynamics for TD learning seems like it could be useful for understanding deep RL theoretically. I am not very familiar with the area but from some searching on Google Scholar the analysis appears to be novel. The results in toy domains suggest that the theoretical results are predictive of real neural network training."
            },
            "weaknesses": {
                "value": "While in general the paper presents useful findings, I think it may lack enough overall contribution for publication at ICLR. It's not clear what the key takeaways are from the paper beyond the differential equation solutions, which are closely based on prior work. Some specific reasons that I feel there is not enough contribution:\n * The theoretical results are presented without much explanation or intuition. It's not clear if there are any insights that can be gained from the solutions to the differential equations.\n * The empirical results are only for the simplest of RL environments (1 and 4 dimensional state spaces; 0 and 1 dimensional action spaces). Since deep RL is primarily useful for high-dimensional environments with complex value functions, it would be more convincing if the authors evaluated on these types of environments as well.\n * The analysis is restricted to finding the Q-function of a fixed policy, while reinforcement learning algorithms generally use a TD loss like $\\left(Q(s, a) - (R(s, a) + \\\\max_{a'} Q(s', a'))\\right)^2$.\n\nAlso, there has been some previous analysis of RL with NTK approximation by Yang et al. [1], who go beyond training dynamics for a fixed-policy and give regret bounds for an optimistic variant of Q-learning with the NTK. It would be good to include a comparison to their work.\n\n[1] Yang et al. On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces. NeurIPS 2020."
            },
            "questions": {
                "value": "Related to the weaknesses listed above:\n * What insights can be drawn from the analytical solutions for NTK TD learning gradient flow?\n * Are the analytical solutions presented in this paper also good approximations for TD learning in higher-dimensional state/action spaces?\n * Can the analysis be extended to using a fitted Q-iteration loss where $Q(s', a')$ is replaced with $\\\\max_{a'} Q(s', a')$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797559374,
        "cdate": 1698797559374,
        "tmdate": 1699635991582,
        "mdate": 1699635991582,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lpNe0BlU3A",
        "forum": "brOAVSPPjw",
        "replyto": "brOAVSPPjw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission641/Reviewer_GToL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission641/Reviewer_GToL"
        ],
        "content": {
            "summary": {
                "value": "This work compares the training dynamics of TD-learning with neural networks with it's NTK parameterisation. It does this for two settings, a full-gradient one in which it minimizes the mean squared TD error and also for the more common, semi-gradient with target network scenario.\n\nIt finds evidence that the lazy regime dynamics only matches in the full-gradient setup. In addition, for the same setup, it finds evidence that the linearized parameterization can provide more accurate uncertainty estimates than SGD ensemble training.\n\nThe study is concerns TD policy evaluation with a fixed policy and the empirical evaluation is performed on cart-pole and a single-action, 21-states MDP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I find the proposal of using tools for understanding of neural network training dynamics from the supervised learning setup to the TD-learning policy evaluation scenario interesting and potentially useful in identifying the peculiarities of optimisation in RL.\n\nAll of the empirical observations on the adequacy of the linearised model for describing the training dynamics and the quality of uncertainty estimation of ensembles can be useful for developing full-gradient mean TD-error algorithms (naive residual-gradient algorithms), although it remains debatable whether this is a proper objective in RL.\n\nI found the writing and notation to be generally clear and consistent."
            },
            "weaknesses": {
                "value": "The paper mainly focuses on the training dynamics of full-gradient TD-learning, which, to my understanding, is similar to minimising the average TD-error. This severely limits the potential impact of the work since full-gradient algorithms are not used in practice and, more importantly, it's not entirely clear if the solutions to the objectives in full-gradient algos are actually desirable (see eg. chapters 11.4 - 11.5 in Sutton). I believe this aspect should be better highlighted and contextualized in the paper.\n\nAuthors also show that their NTK derivation of TD-learning with semi-gradient descent and target network (the more relevant setup) does not follow the same dynamics as under SGD which is an interesting finding even if negative. However the relevant figures are pushed to the appendix and there is not discussion as to why might this be the case. Is it because NTK simply is not a good approximation for algorithms which do not follow a proper gradient field? Is it because of the added complexity of the slowly moving target network?\n\nThe last question also points to some lack of clarity in the paper as it bundles together target networks with semi-gradient methods. However having a different target function is a separate design decision from choosing not to use the gradient of the target network. This also leads to a possible experiment: is the linearisation better at approximating training dynamics in the semi-gradient setup without a separate target network?\n\nFinally, it would be interesting to mention and develop how your empirical findings impact some of the previous works that use the linearisation of neural (gradient/semi-gradient) TD-learning to come-up with convergence proofs. Examples include:\n- https://proceedings.neurips.cc/paper/2020/file/75ebb02f92fc30a8040bbd625af999f1-Paper.pdf\n- https://arxiv.org/pdf/1912.04511.pdf"
            },
            "questions": {
                "value": "Questions and actionable insights in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission641/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission641/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission641/Reviewer_GToL"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission641/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821317764,
        "cdate": 1698821317764,
        "tmdate": 1699635991514,
        "mdate": 1699635991514,
        "license": "CC BY 4.0",
        "version": 2
    }
]