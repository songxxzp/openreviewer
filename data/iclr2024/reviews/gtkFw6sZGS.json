[
    {
        "id": "5yEYAAadyY",
        "forum": "gtkFw6sZGS",
        "replyto": "gtkFw6sZGS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_PG92"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_PG92"
        ],
        "content": {
            "summary": {
                "value": "The paper fine-tunes a language model to automatically judge the output of another language model, either evaluating a single generation or a pair of generations. \n\n(Sorry that I do not think I understand the core training and evaluation setup of the paper; either it is my reading comprehension's problem or there might be issues with the presentation.)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides an open-sourced model that can automatically judge a models' generated output; this could potentially enable more researchers to run automatic evaluation at a lower cost with higher reliability."
            },
            "weaknesses": {
                "value": "- The papers is a large engineering effort (e.g., distilling GPT-4 for the task of evaluation) without much novel ideas (I do not think that a paper needs to be novel to be accepted, but this paper does score low in terms of novelty)\n- The presentation of the method and contribution feels very confusing to me (maybe it's just my fault). See questions below. I do not know whether other reviewers would have similar concerns though."
            },
            "questions": {
                "value": "- If I understand correctly, did you define the scenarios and categories both for fine-tune the model to perform evaluation AND test the model to perform evaluation? In that case, if we want to evaluate an LM that performs a new task that is not in the 58 categories you have defined, would Auto-J generalize to categories unseen during training?\n- Did you use GPT-4 as the label generator? If so, if one had enough OpenAI credit, the optimal strategy of evaluation according to this paper would still be using GPT-4 or not? (both yes/no are okay answers, but I think it'd be useful to clarify)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9392/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9392/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9392/Reviewer_PG92"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534292939,
        "cdate": 1698534292939,
        "tmdate": 1700751366384,
        "mdate": 1700751366384,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5Ici66sfg0",
        "forum": "gtkFw6sZGS",
        "replyto": "gtkFw6sZGS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_WSEj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_WSEj"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to present Auto-J, an LLM that may evaluate LLMs. This is in the line of LMs that are used to evaluate other LMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Auto-J proposes a way to produce evaluation methods for LLMs"
            },
            "weaknesses": {
                "value": "- It is strange that larger models are used to evaluate other models\n- LLMs should somehow emulate human capabilities and not other LLMs' capabilities."
            },
            "questions": {
                "value": "It is clear that this paper is in a long line of other approaches. Yet, it is not clear why evaluating LLMs with an LLM is principled. Cna you comment on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777073957,
        "cdate": 1698777073957,
        "tmdate": 1699637183951,
        "mdate": 1699637183951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kYo4Zxq6oJ",
        "forum": "gtkFw6sZGS",
        "replyto": "gtkFw6sZGS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_jzn2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9392/Reviewer_jzn2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a generative judge with 13 billion parameters to evaluate the generations of large language models from real-world scenarios. Specifically, the authors created a large collection of data among 58 different scenarios and guided GPT4 to collect evaluation judgements as supervised training signals. Extensive evaluations demonstrate Auto-J outperforms many strong baselines and more analysis shows advantages of the proposed method, like reducing positional bias and generating more specific critiques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1). The design of scenario-specific criteria is strongly motivated, which will enable LLM-based judges to produce high-quality evaluations and critiques. Curated criterias can be model-agnostic and adopted to multiple models. \n\n2). Comprehensive evaluation and analysis of Auto-J demonstrate that its evaluations are consistent and can align well with human judgements."
            },
            "weaknesses": {
                "value": "The technical contribution is a bit limited as it is still within the scope of training one more LLM as judges to evaluate other LLMs\u2019 generation. Since the training data is obtained from GPT4\u2019s output, it is unsure whether it can replace GPT4 as judges or has strong generalizations as GPT4."
            },
            "questions": {
                "value": "1). Is it necessary to have a 13b parameter model to train the scenario classifier? Have you tried other simple BERT-like models? \n\n2).Is there any bad case that Auto-J fails or Auto-J generates wrong critiques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n.a."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826584110,
        "cdate": 1698826584110,
        "tmdate": 1699637183841,
        "mdate": 1699637183841,
        "license": "CC BY 4.0",
        "version": 2
    }
]