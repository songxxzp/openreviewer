[
    {
        "id": "l8lUhz28ar",
        "forum": "NialiwI2V6",
        "replyto": "NialiwI2V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_epjm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_epjm"
        ],
        "content": {
            "summary": {
                "value": "The authors of this paper propose a survival analysis foundation model from ICD codes event sequences. Their proposed pre-training method is to leverage the specific nature of the data being itself a sequence of events to perform a multitask dynamic survival analysis training by predicting the time-to-event of 8192 higher-entropy codes in their data (excluding the downstream task ones). Their model is a transformer backbone with heads for each pre-training task returning piecewise-continuous hazard. They compare their model to a popular survival analysis baseline as well as next-event prediction pre-training on 6 survival tasks across two datasets. They also evaluate their foundation model's robustness to time shifts and external datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall I find the paper to be of high quality. The contribution is significant, the experiments thought carefully and the paper very clear and complete. \n\n\n### Novelty\nTo the best of my knowledge, this is the first paper proposing a foundation mode for survival analysis. This is an important contribution, as deep learning applications in the field, due to the relative smallness of available datasets, are less common and with rather small-scale models. \n\nI find it very creative to use EHR data for survival analysis as it is usually only used in the context of (fixed horizon) early event prediction (e.g. Tomasev et al 2019, Hyland et al 2020). \n\n\n### Clarity\n\nThe paper is very easy to follow because very well organized. All necessary details are present in the main text or relevant appendices.\n\n\n### Experiments \n\nThe authors did a great job at comparing to many popular survival baselines as their tasks were not commonly used ones. I particularly appreciate that they made sure to remove the codes used in their downstream tasks from the pre-training as an extra security measure against leakages. Finally, the scope of experiments they considered, especially the validation of their model on external data, really strengthened their work. \n\nThe overall performance improvement is notable. in particular in a field where performance gains are usually of very low magnitude."
            },
            "weaknesses": {
                "value": "### The specificity of the task to ICD codes data\nIf using the fact that ICD code sequences are sequences of events rather than observation to directly perform large-scale multitask dynamic survival analysis on them, is great for this type of data, it is also very limiting to this unique type of data. Indeed, as mentioned by the authors, survival analysis is useful in a variety of domains ranging from cancer research to finance. However,  in such domains, to my knowledge, data cannot be formalized as a sequence of events, hence the proposed pretraining task could not be expanded to other time-to-event models. \n\n### The lack of experiments in the dynamic setting\nThe authors pre-train their model in the so-called \"dynamic survival analysis\" setting, however, they only evaluate it in a static setting. It would have been a great addition to also have dynamic tasks. The same baselines can be considered with a landmarking approach. \n\n### The drop in performance for the Native American sub-group\n\nThe attention of the authors to ethical concerns is clearly above standard. As part of it, they perform a sub-group analysis per ethnicity. Compared to the RSF model which is quite stable across groups, their model exhibits a significant drop in performance among Native Americans (~5% lower than the closest group). However, the authors have the following statement: \"We find that with one statistically insignificant exception, MOTOR-Finetune does not reduce the performance within sensitive groups\". I have a hard time believing that such a drop is \"statistically insignificant\". I believe the authors should clearly state this limitation."
            },
            "questions": {
                "value": "- How does the author handle events that can occur multiple times? Do they predict the time of the next event instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690407806,
        "cdate": 1698690407806,
        "tmdate": 1699636351153,
        "mdate": 1699636351153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uz19UEH3A1",
        "forum": "NialiwI2V6",
        "replyto": "NialiwI2V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_ZCsF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_ZCsF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MOTOR (Many Outcome Time Oriented Representations), a new time-to-event (TTE) foundation model trained on 55M patients with 9B clinical events. Instead of using classification to predict medical events with the assumption of fixed time horizons, MOTOR adopts TTE modeling for its better principled way of predicting events with time and the ability to handle censored observations. Experimental results on three EHR databases show that MOTOR outperforms existing TTE models and is more robust to temporal distributional shifts, suggesting the potential use of pre-training models in TTE modeling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- Interesting to observe the potential of time-to-event modeling trained on a large amount of data.\n- The experiments are thorough and convincing (especially text-based target tasks are interesting to include)\n- The proposed method outperforms previous survival time-to-event methods."
            },
            "weaknesses": {
                "value": "- Limited ablation on the amount of pretraining data (e.g., 1%, 10%). Although the authors have made a significant contribution by releasing the model, providing more details on the amount of data necessary to achieve the current performance would be highly beneficial. Time-to-event modeling holds importance not only in medical settings but also in various other domains.\n- Figure 1 could be explained or illustrated in more detail. For example, it is hard to understand what each survival curve means for each timestep in pretraining tasks."
            },
            "questions": {
                "value": "- How much time did it take to pretrain the model?\n- What is the source of pretraining data?\n- Can you elaborate more on the 8,192 code selection process?\n- Is the patient data at the admission level or concatenated across admissions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839767304,
        "cdate": 1698839767304,
        "tmdate": 1699636351050,
        "mdate": 1699636351050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BoKy3I8JYz",
        "forum": "NialiwI2V6",
        "replyto": "NialiwI2V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_MVk7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_MVk7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a Transformer model uses a self-supervised TTE objective defined using structured EHR data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) Paper is well-written and results are interesting. Authors also cover a number of pre-training tasks including code-based and text-based\n2) Authors compare with a number of relevant baselines, and clearly show the impact of their proposed approach\n3) The authors test robustness and importance of specific pre-training tasks, and show that their model is relatively robust to different datasets"
            },
            "weaknesses": {
                "value": "1) It is not clear how the authors handle missing information in their analyses. How are missing values in the time-series data represented?\n2) While the authors explain the subsampling process in the Appendix, this is critical and forms the basis of the pre-training approach. Expanding on the rationale/method for this seems important"
            },
            "questions": {
                "value": "How are missing values in the time-series data represented? Are specific transformer base models used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855604422,
        "cdate": 1698855604422,
        "tmdate": 1699636350973,
        "mdate": 1699636350973,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T4OHgdMoA9",
        "forum": "NialiwI2V6",
        "replyto": "NialiwI2V6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_13Zf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3910/Reviewer_13Zf"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a time-to-event (TTE) foundation model for EHR data. The proposed model is retrained on a 55M patient record data and 8192 tasks. The model is evaluated on 19 tasks across 3 patient databases and achieves superior performances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This work addresses an important task in medical time-to-event prediction domain and the results are promising. The trained model and code is publicly available. The experiment results and analysis are comprehensive. Here are some minor comments:\n\n1. What are the x and y axis in Figure 1 - pretraining tasks? Are they hazard ratio curves?\n\n2. How the six code-based tasks are selected out of 8192 tasks? Please provide more justifications for the selection. \n\n3. How the time-to-event task is evaluated on the MIMIC dataset? Is it predicting the diagnosis code at each ICU admission?\n\n4. The performance table seems lack of standard deviations. \n\n5. How much resources are needed to fine-tune or inference using the pretrained model?"
            },
            "weaknesses": {
                "value": "Here are some minor comments:\n\n1. What are the x and y axis in Figure 1 - pretraining tasks? Are they hazard ratio curves?\n\n2. How the six code-based tasks are selected out of 8192 tasks? Please provide more justifications for the selection. \n\n3. How is the time-to-event task evaluated on the MIMIC dataset? Is the goal to predict the diagnosis code at each ICU admission?\n\n4. There are no standard deviations in performance tables. \n\n5. How many resources are needed to fine-tune or infer using the pretrained model?"
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3910/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3910/Reviewer_13Zf"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856407524,
        "cdate": 1698856407524,
        "tmdate": 1699636350903,
        "mdate": 1699636350903,
        "license": "CC BY 4.0",
        "version": 2
    }
]