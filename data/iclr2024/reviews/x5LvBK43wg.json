[
    {
        "id": "d8P32KfxgG",
        "forum": "x5LvBK43wg",
        "replyto": "x5LvBK43wg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_XKXi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_XKXi"
        ],
        "content": {
            "summary": {
                "value": "This paper presented a novel test-time adaptation method named PROGRAM based on pseudo-label learning. The key motivation of PROGRAM was to leverage label propagation to improve the quality of pseudo-labels and then use robust self-training for model updating. Experiments on several data sets confirmed that PROGRAM was effective and computationally efficient for test-time adaptation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality:** The major contribution of this paper was to introduce a novel PROGRAM method for test-time adaptation, which aimed to adapt a pre-trained source model to the test data. PROGRAM addressed the issues of noisy pseudo-labels by introducing two novel components. One was the Prototype Graph Model (PGM) for generating high-quality pseudo-labels. The other one was the Robust Self-Training (RST) by combining pseudo-labeling and consistency regularization for model updating. Experiments demonstrated that PROGRAM could achieve better prediction accuracy and comparable computational efficiency compared to state-of-the-art TTA baselines.\n\n**Quality:** The motivation of PROGRAM in handling noisy pseudo-labels was clearly illustrated. Previous works using noisy pseudo-labels might lead to sub-optimal solutions in model updates. The graph-based label propagation improved pseudo-labels by enforcing that two similar samples have similar pseudo-labels. Experiments also demonstrated that with improved pseudo-labels, PROGRAM achieved promising performance in test-time adaptation.\n\n**Clarity:** The presentation of this paper is easy to follow. It illustrates the technical details and experimental settings in this paper. Algorithm 1 also clearly illustrates the training and inference process of PROGRAM in test-time adaptation scenarios.\n\n**Significance:** PROGRAM improves the performance of test-time adaptation with respect to various pre-trained source models. Thus it can be a strong baseline in test-time adaptation, especially in understanding the impact of pseudo-labels."
            },
            "weaknesses": {
                "value": "W1: The improvement of PROGRAM on the pseudo-labels is not quantitively evaluated. One key intuition behind PROGRAM is to improve pseudo-labels with PGM and RST. Though the ablation studies validate the necessity of PGM and RST in improving prediction accuracy, it is more convincing to explicitly evaluate the impact of these components on the quality of pseudo-labels.\n\nW2: The similarity $w_{ij}$ in constructing the prototype graph is not well explained. In Eq. (2), the similarity $w_{ij}$ is defined over $p(v_i | v_j)$. In this case, $p(v_i | v_j) = p(v_j | v_i)$ for guaranteeing the symmetric similarity matrix. Equivalently, it might hold that $p(v_i , v_j) = p(v_i | v_j) p(v_j) = p(v_j | v_i) p(v_i)$ and then $p(v_i) = p(v_j)$ for all nodes. It is unclear whether it assumes that all samples follow a uniform distribution. This might be a strong assumption in real scenarios.\n\nW3: Another concern is the computational efficiency of PROGRAM. It requires the graph construction and the matrix inverse computation. Besides, it also updates the \"whole\" feature extractors for RST. Both strategies might significantly increase the running time of PROGRAM for test-time adaptation, compared to other partial updating methods. Thus the trade-off between the effectiveness and efficiency of PROGRAM can be further analyzed, e.g., sparse graph construction, partial parameter updates, etc."
            },
            "questions": {
                "value": "Q1: The Robust Self-Training (RST) combines both pseudo-labels and consistency regularization. The effectiveness of RST can be validated with more ablation studies. For example, compared to simple pseudo-labeling or consistency regularization, how can their combination perform better on the TTA benchmarks?\n\nQ2: The soft labels in Eq. (1) are estimated with the normalized model weights $c_k$ (prototypes). Compared to vanilla soft labels with the raw source model (with unnormalized weights), can these prototypes help provide better pseudo-labels?\n\nQ3: Is the matrix $\\mathbb{I} - \\lambda \\mathbf{Z} \\mathbf{D}^{-1} \\mathbf{Z}^T$ full rank to directly compute the inverse of this matrix?\n\nOverall, this paper introduced an interesting idea for handling the noisy labels within the test-time adaptation and achieved promising performance in several benchmarks. I would like to increase my rating if my concerns can be well addressed.\n\n\n##########################################################################################\n\nMost of my concerns are addressed after rebuttal, thus I would like to increase my score. More discussion can be provided in this paper to validate the high-quality pseudo-labels of the proposed framework. The efficiency of PROGRAM can also be improved when the graph-based method is used to improve pseudo-labels."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Reviewer_XKXi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7713/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698170365603,
        "cdate": 1698170365603,
        "tmdate": 1700674899841,
        "mdate": 1700674899841,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x2HnyruOqV",
        "forum": "x5LvBK43wg",
        "replyto": "x5LvBK43wg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_xjYH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_xjYH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies test-time adaptation method and propose a pseudo-label-based method, called PROGRAM. Authors utilize prototype graph construction and prototype graph based label propagation to obtain more accurate pseudo labels. A robust self-training strategy is proposed to employ cross entropy loss or symmetric cross entropy loss to all samples in the batch. Experiments across four domain generalization benchmarks and three corruption benchmarks are provided to evaluate the effectiveness of PROGRAM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experimental results are rich, which consists of four DG benchmarks and three corruption benchmarks.\n\n2. The proposed method is clear, which is easy to follow."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper remains to be discussed. Authors just use Prototype Graph Model directly and the strategy that selects some samples with CE loss and others with consistency regularization is commonly used in unsupervised learning. A similar paper [1] which also provides the derivation of Prototype Graph Model for few shot learning should be cited.\n\n[1] Hao Zhu, and Piotr Koniusz. Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement. In Proc. CVPR, pages 23996--24006, 2023."
            },
            "questions": {
                "value": "1. The experiment of Fig.3 is weak, the class number of PACS is too small, authors should use benchmarks like CIFAR-100-C or OfficeHome to evaluate the sensitivity of batch size.\n\n2. What is the accuracy and percentage of pseudo label after filtering by PGM?\n\nTypo error: the caption of Table 4 : \u2191 means higher is better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Reviewer_xjYH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7713/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736436101,
        "cdate": 1698736436101,
        "tmdate": 1699636940063,
        "mdate": 1699636940063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y2wtScX1OC",
        "forum": "x5LvBK43wg",
        "replyto": "x5LvBK43wg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_zHCF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_zHCF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new test-time adaptation method, which has two contributions: 1) combines both the prototype-based pseudo label and nearest-neighbor-based pseudo label in a prototype graph to generate a comprehensive pseudo label and 2) Combines hard and soft pseudo-labels to improve the adaptation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+The paper is well-written, and the overview figure clearly demonstrates the main contributions of this work.\n+It is interesting to generate the pseudo-label by label propagating within the constructed graph.\n+Experimental results are impressive, the proposed methods got SOTA in all proposed benchmarks."
            },
            "weaknesses": {
                "value": "Methodology:\n-PGM is a non-parametric process to generate the pseudo-labels. \nThe motivation for using prototypes to determine the connectivity between two vertices is unclear. Could you use dot-product or cosine similarity to replace this? Some experimental comparisons and analysis could provide more insights.\n\n-PGM lacks comparisons with non-parametric attention modules. Given the predefined graph G, the non-parametric attention module could first update each node in the graph with the help of the adjacency matrix, and then use the updated class prototypes to make predictions of each updated test feature.  \n\n-Could the author provide more insights into why PGM shows more reliable pseudo-labels?  Especially, what does \u201cmore reliable\u201d mean?  \n\n-Some confusion about the derivations in A3 and A4. How to simplify A3 as A4. Could the author provide more detailed information?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7713/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738624374,
        "cdate": 1698738624374,
        "tmdate": 1699636939919,
        "mdate": 1699636939919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pxYlEABaMg",
        "forum": "x5LvBK43wg",
        "replyto": "x5LvBK43wg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_DVoa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_DVoa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the PROGRAM, a TTA method that is comprised of the Prototype Graph Model (PGM) for pseudo-label generation and Robust Self-training (RST) in self-training. PGM is designed to blend the benefits of both prototype-based and nearest-neighbor based pseudo-labeling. RST combines pseudo-labeling and consistency regularization. Experimental results validate the efficacy of the PROGRAM method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tPROGRAM looks like an interesting approach to combine the benefits of prototype-based and nearest-neighbor based pseudo-labeling.\n2.\tThe paper presents extensive experiments to validate the effectiveness of the method."
            },
            "weaknesses": {
                "value": "The presentation of the paper could be improved:\n* The explanation for Figure 1 appears to lack details. A clearer description would be helpful, such as specifying that the red line represents the decision boundary. Moreover, the meaning of the red dashed line in 1(b) is ambiguous.\n* The meaning of the symbols \u2018+\u2019 used in Tables 1 and 2 is not clear. It would be helpful if their meaning can be clarified."
            },
            "questions": {
                "value": "1.\tWhile PROGRAM's runtime is competitive as a whole feature extractor, it noticeably lags behind some partial ones like T3A. Given that the performance edge of PROGRAM over T3A isn't substantial in some tasks (as shown in Table 1), could you shed light on the specific scenarios where PROGRAM would be the preferred choice? Essentially, under what circumstances should one opt for PROGRAM, even at the expense of computational speed?\n\n2.\tCould you provide a breakdown of PROGRAM's runtime between PGM and RST (Table 6)? \n\n3.\tFrom Table 3, RST seems to have a small impact on improving results. Furthermore, ResNet-50 on its shows good results. How do you justify the improvements brought by PGM and RST given their relatively modest contribution to performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7713/Reviewer_DVoa",
                    "ICLR.cc/2024/Conference/Submission7713/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7713/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698876310844,
        "cdate": 1698876310844,
        "tmdate": 1700929594183,
        "mdate": 1700929594183,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NB3jdtkv9d",
        "forum": "x5LvBK43wg",
        "replyto": "x5LvBK43wg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_C9fo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7713/Reviewer_C9fo"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel Test-Time Adaptation method named PROGRAM, which leverages a pseudo-labeling approach to enhance model performance for image corruption benchmarks. PROGRAM is build around two key components: (1) a Prototype Graph Model for generating pseudo-labels and (2) Robust Self-Training to adapt the model at test time. The proposed new method has been validated across various architectures, demonstrating consistent improvements over the existing TTA methods. Extensiv experiments show that PROGRAM not only outperforms other state-of-the-art methods but also maintains its performance across a range of hyperparameters. Additionally, it is designed to be \"plug-and-play\", easily integrating with different networks and TTA methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The PROGRAM demonstrates consistent performance improvements across a variety of backbone architectures. This versatility suggests that the method is robust and can be applied to a broad set of existing models, and across the range of hyperparameters.\n\nEmpirical validation: The method has been empirically validated with extensive experiments showing that it outperforms existing state-of-the-art methods on various domain generalization and image corruption benchmarks."
            },
            "weaknesses": {
                "value": "The paper does not discuss potential limitations or failure modes of the proposed method. While the paper reports on the efficiency of PROGRAM compared to other methods, there is limited discussion on the absolute runtime performance, especially in comparison to the baseline models without TTA.\n\nPractical Integration Challenges: Despite claiming that PROGRAM is a \"plug-and-play\" solution, the paper does not delve into the practical challenges of integrating the method into different systems or architectures."
            },
            "questions": {
                "value": "Given the PROGRAM TTA approach does nto apply any modifications during training phase, can the effectiveness of PGM and RST in PROGRAM compensate for potential deficiencies in the pre-trained model? I.e. have you checked how the test time adaptation method works when the pre-trained model is suboptimal.\n\nHow does the graph construction technique manages the class imbalance that might be present in the unlabeled target data? Related to the discussion in section 3.2 about initialization of prototypes and constructing a prototype graph."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7713/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699047454401,
        "cdate": 1699047454401,
        "tmdate": 1699636939678,
        "mdate": 1699636939678,
        "license": "CC BY 4.0",
        "version": 2
    }
]