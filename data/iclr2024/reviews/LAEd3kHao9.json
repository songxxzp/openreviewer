[
    {
        "id": "A3oky3xA9M",
        "forum": "LAEd3kHao9",
        "replyto": "LAEd3kHao9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_feoJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_feoJ"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors utilize pre-trained large language models (LLM) to increase the diversity of the semantic descriptions so that the the visual and semantic features can be aligned accurately. Moreover, a stochastic logit mixup (SLM) strategy are proposed for the final compositional predictions. The experiments demonstrate that the proposed methord improves the current benchmark performance on three benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper is organized and clearly written.\n2.The proposed method seems to be intuitively reasonable."
            },
            "weaknesses": {
                "value": "1.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper.\n2.According to the Ablation study, the experiment w/o VLPD does not change much (even the H_cw value decreases).\n3.The proposed languageinformed distributions (LID) can effectively avoid the issue of intra-class variety. However, the authors would better also intepret how to solve the issue of inter-class correlation."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Reviewer_feoJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637627961,
        "cdate": 1698637627961,
        "tmdate": 1699636345060,
        "mdate": 1699636345060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gfs4cWMJm4",
        "forum": "LAEd3kHao9",
        "replyto": "LAEd3kHao9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_4PfY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_4PfY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a CLIP-based compositional zero-shot learning (CZSL) method. Two components are introduced i.e., primitive decomposition and stochastic logit mixup to fuse the classification decision from compositional and primitive predictions. Finally, the suggested method shows the performance improvement of the PLID method to prior arts on all common CZSL datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivations of suggested primitive decomposition and stochastic logit mixup are okay, which have positive effects on zero-shot compositional visual recognition.\n\nThe paper is well presented and the suggested method slightly exceeds the comparison method."
            },
            "weaknesses": {
                "value": "The authors did not mention whether the comparison methods also used N-view augmentation. If the answer is no, I think the comparison is unfair. Please indicate which method uses the same augmentation.\n\nThe author mentioned in the ablation study that LID significantly improves performance compared to the baseline is confusing. According to Table 2, TFE and VFE seem to have little effect, while LID appears to be effective, and its effectiveness should come from LLM, which in this paper belongs to incremental engineering.\n\nFigure 2 should be consistent with its caption. For example, Figure 2 should provide the corresponding parts of LID, which should be more helpful for understanding."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Reviewer_4PfY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764807620,
        "cdate": 1698764807620,
        "tmdate": 1699636344987,
        "mdate": 1699636344987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6jZPRIVAsx",
        "forum": "LAEd3kHao9",
        "replyto": "LAEd3kHao9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_BPd3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_BPd3"
        ],
        "content": {
            "summary": {
                "value": "The article proposes an approach for compositional zero-shot learning (CZSL) where the goal is to recognize compositions of attributes and objects, having only a subset of them during training. To address this task, the article proposes to exploit various components:\n1. Language-informed distribution (LID) which exploits composition-specific prompts given by an LLM and modifies them via learned soft-prompts. Both soft and LLM-based prompts are fed to a cross-attention module (TFE), and the resulting vector is encoded as a distribution, where the vector is the mean and the same with added LLM-based embeddings define other data points. \n2. On the visual side, visual embeddings are obtained by augmenting the input via multiple views and passing them (and the original input) through a cross-attention module (VFE). Both VFE, TFE, and soft-prompts are learned via cross-entropy loss over the compositional labels and taking into account a margin to encourage inter-class separability.\n3. To decompose the compositional elements, the article introduces VLPD which computes classification loss over objects and attributes independently, where each primitive logit is derived by marginalizing compositional embeddings.\n4. Finally, Stochastic Logic Mixup is proposed to mix predictions coming from primitive-specific and composition-specific logits.\n\nExperiments over various datasets (e.g. MIT-states, C-GQA, UT-Zappos) show the superiority of the approach w.r.t. the previous state of the art, with ablation studies confirming the efficacy of the proposed modules.\n\n--------\n\n**Update post-rebuttal:**\n\nI thank the authors for their response. At the same time, the additional analyses partially addressed the initial concerns, e.g. removing N-views shows results comparable to other models (and vice-versa, how models would improve from n-views it is not tested), there is a limited performance gap between different variants with/without the contribution, and unclear how writing issues will be improved. I value LID per-se as a contribution but, for the reasons outlined above, I deem the article to be borderline for this venue and I tend to keep my initial score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Despite the method having several building blocks, the idea and motivation behind each introduced component are well-described in the text: e.g. LLM is used to create a pool of sentences describing the compositions, which in turn is used to estimate a distribution that can be in turn used to estimate pairwise-margins between compositions. Fig. 1 also helps the reader understand the starting idea for the model (i.e. composition ambiguities), and why a distribution over the language space may help deal with such uncertainty.\n2. The experiments show that the proposed approach (PLID) surpasses by a margin all competitors in all settings, especially in the closed-world scenario where unseen compositions are known (Table 1). In this latter setting, the gap in AUC is remarkable, with 1.5 points improvement on MIT-states, 2.7 on UT-Zappos, and 0.5 on C-GQA.\n3. Sections 2, 3, and 4 give credit to the approaches the method builds on, presenting a detailed overview of the literature and the proposed contributions."
            },
            "weaknesses": {
                "value": "I have two main concerns regarding the experimental analysis and the presentation.\n\nFor the former, the proposed method contains several components (i.e. LID, TFE, VFE, VLPD, SLM). While all components have motivations justifying their use, each of them has specific design choices whose impact is not fully clear from Section 5.2. Examples are:\n1. The gap between considering and not considering distributional-based margins in Eq. (1) and Eq. (4) is mild accordingly is mild according to Table 4 (i.e. gap lower than 0.5 points but for harmonic mean OW). Fig. 5.a shows that indeed going from 4 to 64 LLM-based sentences improves the overall results but less than 1 point and less in the more challenging OW setting. Given that querying LLM is costly in this setting (i.e. the number C of compositions might be in the order of thousands) and can be noisy (as per Appendix A), it is questionable whether LLMs and distributional semantics are crucial for the approach (as suggested by the title). The ablations are also conducted on the MIT-states dataset which is known to be noisy (Atzmon et al. 2020), thus it should be verified if the findings hold across datasets. \n2. Related to the previous points, TFE and VFE are modules that refine visual/textual embeddings. The article does not contain ablations on their number of parameters/design choices and the improvement from the added views is mild (i.e. less than 0.5 points on Fig. 5b). Ablating variants of these modules, potentially taking out the set of text embeddings/views and focusing on their parameters (e.g. even via MLPs, etc.) would strengthen the need for their implementation as cross-modal blocks and also of their specific input choices. This applies also to the specific implementations of the prediction modules $f_s$ and $f_o$ in Eq. (2).\n3. The SLM module should provide flexibility to the model regarding which predictions to trust. However, SLM is not compared to other aggregation strategies (e.g. average, max, product) and on the single scoring mechanism (compositional vs primitive-based). Thus, it is hard to assess the need for this module.\n4. Regarding the presentation: from the abstract and introduction it is unclear what is the role of LLMs/how they are used. It is implied that a language-informed class distribution is used, but not how this is achieved (or kept this information generic in Section 1). This is not a major weakness per-se, but given that the title focuses on this distribution, it would be helpful to give hints on how this distribution is estimated already at the beginning of the manuscript, clarifying the methodological idea to the reader. This is a purpose that Fig. 1 serves well, but the text does not stress. \n5. The notation of Section 3 is not straightforward to follow. The main reason is not the lack of explanations (each term is properly defined) but the number of terms defined that the reader should remember to fully appreciate the method. While I understand that it is not easy to make the notation simpler given the presence of multiple components, in some cases, the notation could be simplified. For instance, the name of the text embeddings is detached from their inputs (e.g. S becomes D, [p:..] becomes q, x and X become v, etc.). The end of the VLPD part introduces $\\mathbf{h}$ elements and $\\mathbf{H}$, that could be replaced by simply stating that $h^{rc}_y = h_s + h_o$ (even directly on Eq. (4)). These are (arguable and probably subjective) examples on how some of the elements could be not defined and/or the notation simplified.\n\nMinors:\n- The qualitative results in Fig. 8 do not provide specific insights on the model as the predictions are only compared with the ground truth. As in Fig. 7, it would have been more helpful to investigate how predictions are affected by different design choices.\n- Fig. 2, caption, \"VEF\" vs \"VFE\"."
            },
            "questions": {
                "value": "Overall, I like the principles and ideas behind the approach. At the same time, the concerns regarding the experimental validation of the design choices should be answered in the rebuttal. In particular:\n1. What is the impact of the LLM and the cost to produce the compositional descriptions?\n2. Do the structure of TFE and VFE matter more or less than the distribution/margins introduced?\n3. Is SLM better than simpler aggregation strategies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3866/Reviewer_BPd3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798302655,
        "cdate": 1698798302655,
        "tmdate": 1700748616921,
        "mdate": 1700748616921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YvJGnLKC2U",
        "forum": "LAEd3kHao9",
        "replyto": "LAEd3kHao9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_FS69"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3866/Reviewer_FS69"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes prompting the language-informed distribution, i.e., PLID, for handling the CZSL task. PLID presents language-informed class distributions that are diverse and informative and enhance the compositionality of the class embedding. Visual-language primitive decomposition and stochastic logit mixup strategy are used to fuse the decisions of the compositional and the primitive logit space. Experimental results demonstrate that the PLID method effectively improves the performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The overall organization is reasonable, and the writing is good.\n\nThe class-wise distribution modeling and its afterward alignment with the image modal is novel.\n\nSufficient experiments and ablation studies are performed."
            },
            "weaknesses": {
                "value": "The motivation for modeling class distribution is not new, i.e., it has been proposed and used in the vision-language model. Also, the augmentation to image input seems like a test-time adaptation strategy that is also explored in the community. These aspects degrade the contribution to the community.\n \nThe framework incrementally follows ProDA by introducing D^{(y)} and introduces VLPD by compositing v which is also widely used in CZSL. The framework seems like a combination of many existing techniques. This degrades its novelty. Why A in Eq. 1 is defined like that? Is the dimension shape of A correct by defining it as A_k,y? What\u2019s the shape of A_k,y?\n\nSome parameter analysis is shown, however, the results w.r.t. the value of N = 0 is not shown. Also, how to balance the tradeoff between different losses. The final training loss is not shown."
            },
            "questions": {
                "value": "Refer Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888634770,
        "cdate": 1698888634770,
        "tmdate": 1699636344807,
        "mdate": 1699636344807,
        "license": "CC BY 4.0",
        "version": 2
    }
]