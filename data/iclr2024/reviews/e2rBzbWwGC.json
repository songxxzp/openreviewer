[
    {
        "id": "WmDgNsKnRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
        ],
        "forum": "e2rBzbWwGC",
        "replyto": "e2rBzbWwGC",
        "content": {
            "summary": {
                "value": "This paper studies the problem of robust learning on graphs and proposes a new selection criteria named CBC, which are incorprated into a curriculum learning framework to select confident nodes. Authors experimentally show the superiority of our method compared with state-of-the-art baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of the work is sound.\n\n2. The paper is well written.\n\n3. The paper is well organized."
            },
            "weaknesses": {
                "value": "1. The incorporation of curriculum learning to graph robust learning has been studied in [1,2,3]. Although there are some differences, [1] can smoothly adapted into the studied problem. \n\n2. The performance compared with the best baseline is minor in a lot of datasets, e.g., PubMed of less than 1%, which is even bigger than the variance with different GNN architectures in Table 3. \n\n3. The performance with naive curriculum learning (selection with confidence) should be included in Figure 6. \n\n4. The introduction of CBC to topological curriculum learning is not clear. I expect to see the selection criteria more clear. However, authors show the Eqn. 2 with abstract W_\\lamdba. It should be a very simple selection criteria as from Algorithm in Appendix. \n\n[1] OMG: Towards Effective Graph Classification Against Label Noise, TKDE 2023.\n\n[2] Curriculum Graph Machine Learning: A Survey, IJCAI 2023.\n\n[3] CuCo: Graph Representation with Curriculum Contrastive Learning., IJCAI 2021."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_gbE5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697328430577,
        "cdate": 1697328430577,
        "tmdate": 1699636052391,
        "mdate": 1699636052391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tBS297r4ul",
        "forum": "e2rBzbWwGC",
        "replyto": "e2rBzbWwGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
        ],
        "content": {
            "summary": {
                "value": "This paper study the problem of node classification when the graph data is noisily labeled. The authors introduce the class-conditional betweenness centrality as the measure of the confidence of a node having noise label, and then apply the curriculum learning framework to guide model learning. They further show that this framework minimizes an upper bound of the expected risk under target clean distribution. Experiments on benchmark datasets demonstrate the effective of their proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is novel and reasonable, which incorporates the graph structural information to judge the confidence of nodes. The theoretical results also provide a suitable explanation for the effectiveness of TCL.\n- The experimental results are substantial and credible, which is sufficient to support the effectiveness of TCL."
            },
            "weaknesses": {
                "value": "- It seems that there exists some error in the proof of Theorem 1 (See Questions for more detail).\n- The proposed framework relies on the homogeneity of graph data, which could limit its applications in real-world scenarios, where the connected nodes may belong to different classes."
            },
            "questions": {
                "value": "1. In Eq. (18), the authors use the following inequality:\n$$\n\\frac{1}{m} \\mathbb{E}_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B } \\sum\\_{i=1}^m \\sigma_i \\vert \\mathbf{w} \\mathbf{x}_i \\vert \\right] \\leq \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B} \\Vert \\sum\\_{i=1}^m \\sigma_i \\mathbf{x}_i \\Vert \\vert \\right].\n$$\n\nI don't think this inequality holds true. To remedy this, I recommend the following proof:\n$$\n\\begin{aligned}\n& \\frac{1}{m} \\mathbb{E}_{\\sigma} \\left[ \\mathop{\\rm sup}\\_{\\Vert \\mathbf{w} \\Vert \\leq B } \\sum\\_{i=1}^m \\sigma_i \\vert \\mathbf{w} \\mathbf{x}_i \\vert \\right] \\\\\\\\\n\\leq & \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[  \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right] \\leq \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\left\\vert  \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right\\vert \\right] \\\\\\\\\n= &  \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\sqrt{\\left( \\sum\\_{i=1}^m \\sigma_i \\Vert \\mathbf{x}_i \\Vert \\right)^2} \\right] =  \\frac{B}{m} \\mathbb{E}\\_{\\sigma} \\left[ \\sqrt{ \\sum\\_{i,j=1}^m \\sigma_i \\sigma_j \\Vert \\mathbf{x}_i \\Vert \\Vert \\mathbf{x}_j \\Vert} \\right] \\\\\\\\\n\\leq & \\frac{B}{m} \\sqrt{ \\mathbb{E}\\_{\\sigma}  \\left[ \\sum\\_{i,j=1}^m \\sigma_i \\sigma_j \\Vert \\mathbf{x}_i \\Vert \\Vert \\mathbf{x}_j \\Vert\\right] } = \\frac{B}{m} \\sqrt{ \\sum\\_{i=1}^m \\Vert \\mathbf{x}_i \\Vert^2} \\leq \\frac{BR}{\\sqrt{m}}.\n\\end{aligned}\n$$\nAlso, in the first equation, the term $sgn(\\mathbf{w}_i \\mathbf{x}_i)$ should be corrected as $sgn(\\mathbf{w} \\mathbf{x}_i)$.\n\n2. The noise label situation has a close relation with the out-of-distribution (OOD) situation. Could the proposed framework be applied to OOD generalization [1] or detection [2] tasks?\n\n3. In Algorithm 1, why the authors use a pretrained classifier $f^p_{\\mathcal{G}}$? What pretrain method was used to obtain $f^p_{\\mathcal{G}}$? Do different pretrain methods affect the final performance of the model?\n\n[1] Wu et al, Geometric knowledge distillation for topological shifts. NeurIPS 2022.\n\n[2] Wu et al, Energy-based detection model for OOD nodes on graphs. ICLR 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_GKC8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698478684784,
        "cdate": 1698478684784,
        "tmdate": 1699636052336,
        "mdate": 1699636052336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f030v48R6B",
        "forum": "e2rBzbWwGC",
        "replyto": "e2rBzbWwGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Class-conditional Betweenness Centrality (CBC) as a robust measure to address graph label noise and develop a Topological Curriculum Learning (TCL) framework guided by CBC, which enhances model learning and outperforms existing methods both theoretically and experimentally."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors propose a novel metric for assessing the cleanliness of labels, and the paper is clear, understandable, with extensive experiments"
            },
            "weaknesses": {
                "value": "However, I have some concerns about certain aspects of the paper:\n- This paper seems to address the problem of noisy labels on graphs with the concept of curriculum learning and Pagerank. However, it does not theoretically or, more shallowly, logically demonstrate or explain the connection between the proposed CBC criterion and noise labels. Alternatively, it does not clearly state the rationality of the CBC criterion to eliminate the negative effect of noisy labels. It is insufficient to show the correlation between CBC and class boundary nodes under the setting of noisy labels. Moreover, empirical results on a single dataset are not enough to support some conclusive statements in the paper. \n- Theorem 1 draws the conclusion from the i.i.d data but not the graph-structured data directly. It is better to show a more explicit upper bound from the graph itself. \n- Please explain the meaning of positive/negative samples and error distribution in Theorem 1. In addition, the upper bound relates to the clean distribution, but not the noisy distribution. Is it independent of the noise factor? If so, how can Theorem 1 be used to illustrate the connection between the proposed method and noisy labels?\n- Some of the latest papers on label noise in graphs haven't been adequately mentioned and compared, which weakens the persuasiveness of the paper: (1) ALEX: Towards Effective Graph Transfer Learning with Noisy Labels. MM'23 (2) Learning on Graphs under Label Noise. ICASSP'23 (3) GNN Cleaner- Label Cleaner for Graph Structured Data. TKDE'23"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1257/Reviewer_cJzR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698484002517,
        "cdate": 1698484002517,
        "tmdate": 1699636052253,
        "mdate": 1699636052253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LwiqEDZII1",
        "forum": "e2rBzbWwGC",
        "replyto": "e2rBzbWwGC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_rtA1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1257/Reviewer_rtA1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a metric called Class-conditional Betweenness Centrality, which is used for easy-to-hard sample selection in Curriculum Learning for graph data. Based on this, the authors design a Topological Curriculum Learning (TCL) framework and prove that it minimizes an upper bound of the expected risk."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The robustness and effectiveness of the Class-conditional Betweenness Centrality metric have been thoroughly and experimentally validated.\n\n2. The CBC measure and the corresponding Topological Curriculum Learning are intuitive and meaningful in the graph curriculum learning."
            },
            "weaknesses": {
                "value": "1. The proposed Class-conditional Betweenness Centrality (CBC) is to some extent influenced by the homo. ratio of the dataset. Compared to Cora, conducting toy experiments with certain specifically synthesized data would be more accurate.\n\n2. The paper lacks a more specific introduction to the impact of over-smoothing on Curriculum Learning for graph data, making it somewhat disjointed.\n\n3. The improvements over existing CL methods are not significant."
            },
            "questions": {
                "value": "1. Fig.6 does not provide a comparison of effectiveness between other Curriculum Learning methods.\n2. Is there any experimental results based on models that can effectively alleviate over-smoothing, used as the GNN backbone?\n3. The features in Fig 2, are their features after being processed by GNN? If not, could authors provide that visualizations?\n4. Will the proposed method be effective on homophily datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1257/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722076785,
        "cdate": 1698722076785,
        "tmdate": 1699636052157,
        "mdate": 1699636052157,
        "license": "CC BY 4.0",
        "version": 2
    }
]