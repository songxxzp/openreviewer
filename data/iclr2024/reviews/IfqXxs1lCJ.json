[
    {
        "id": "mKdNiuFAeW",
        "forum": "IfqXxs1lCJ",
        "replyto": "IfqXxs1lCJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission579/Reviewer_MMnw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission579/Reviewer_MMnw"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the evaluation of generative models in the distributed/federated setting and studies two popular metrics, Fr\u00e9chet Inception Distance (FID) and Kernel Inception Distance (KID). In particular, the authors consider two distributed evaluation modes: (all) the metrics are computed against the global data distribution, i.e., the mixture defined over the clients\u2019 distributions; and (avg) the metrics are computed against the distribution of each individual client and then averaged to define a single score. The authors prove that in the case of KID, both evaluation modes rank generative models in the exact same manner, but that in the case of FID, the rankings defined by FID-avg and FID-all can vary significantly. The theoretical results are further supported by extensive empirical results on popular benchmarks for image generation in a federated setting with heterogeneous client distributions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- To the best of my knowledge, the main results of the paper are novel, and the supporting arguments, both theoretical and empirical, seem to be sound. The evaluation of generative models is an important and challenging topic that becomes even more difficult in the federated setting, which thus far has received little attention in the literature.\n- The paper is very well written and easy to read. The authors do a good job of introducing the relevant concepts and literature around generative models and federated learning.\n- The experiments are extensive and support the main theoretical claims of the paper. The authors also shared their code, and I have no reason to believe the results are not reproducible."
            },
            "weaknesses": {
                "value": "- The need for the two modes of evaluation the authors consider (all and avg) is not well motivated in the paper. Among the cited previous works on federated learning for generative models, most if not all rely on some form of FID-all, typically by computing FID scores on a separate test dataset. Are there any examples in the literature where scores similar to FID-avg or KID-avg were considered? I imagine one would be interested in FID-avg in the context of personalization or when privacy concerns prevent the usage of FID-all, for example, but that is never discussed in detail in the current version of the paper.\n- Following on the previous point, although the paper is well presented, novel and sound, I am not entirely convinced of its significance.\n- The discussion of the results could exploit a few points more in depth. For instance, the authors could comment on how their results could be used to guide the selection of an appropriate metric. From where I stand, it seems the results favor KID-avg, since it can be computed in a distributed manner (thus preserving privacy) but still retains the same ranking of KID-all, which captures the global distribution.\n\nMinor points:\n- Theorem 1: \u201cdistributions\u201d is misspelled, and I believe \u201cfollowing\u201d should be singular."
            },
            "questions": {
                "value": "1. In Section 5.1., the authors note that \u201c[\u2026] counterintuitively, the \u2018ideal estimator\u2019 did not reach the minimum average of the Fr\u00e9chet distances\u201d. Could they share any intuition as to why that is the case? Does that indicate taking the average among clients is a poor metric to optimise for?\n2. Would it not be possible to also show the optimal value (referent to the true distribution) for the KID metric in the experiments of Section 5.1.?\n3. In page 7, the authors comment on the effect of privacy considerations when choosing between FID-all or FID-avg: \u201c[\u2026] a distributed computation of FID-all is more challenging than obtaining FID-avg due to privacy considerations\u201d. This shows an interesting trade-off between the two modes that the authors could explore a bit further. While FID-all can be more challenging to compute from a privacy perspective, wouldn\u2019t FID-avg favor a model that fits the distributions of only one or a few of clients very well, thus potentially \u201cmemorizing\u201d the data of these clients?\n4. Empirically, does the ranking provided by KID matches any of those given by FDI-all or FDI-avg? It is not clear to me whether we can infer that from the plots, but it would be interesting to know how the rankings of FDI and KID compare in the experiments (even though the theoretical results have nothing to say here).\n5. On a similar note, for the experiments of Section 5.1. as well as those with DDPM, one could compute (estimates of) the log-likelihood of the data. Have the authors considered how the ranking defined by the log-likelihood compare with the other metrics? The log-likelihood should provide consistent rankings, differently from the density metric of Naeem et al."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698257996188,
        "cdate": 1698257996188,
        "tmdate": 1699635985177,
        "mdate": 1699635985177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ebcDtC0DZ3",
        "forum": "IfqXxs1lCJ",
        "replyto": "IfqXxs1lCJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission579/Reviewer_1nBR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission579/Reviewer_1nBR"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the evaluation of generative models in distributed learning settings, in particular, the federated learning scenario. The paper showed that in distributed settings, the way to aggregate evaluation metrics may affect rankings of generative models. For FID score, the paper theoretically showed that FID-avg which is the mean of clients\u2019 individual FIDs, can be inconsistent with FID-all, which is the FID score computed on the collective dataset, leading to different model rankings. On the other hand, for another evaluation metric KID (kernel inception distance), KID-avg and KID-all are always consistent for ranking models. Experimental results were provided to support the theoretical findings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provided theoretical findings on evaluation metrics for generative models in distributed settings. The results could be of interest and are worth discussing when training and comparing generative models in the distributed manner.\n\n2. The paper provided experimental results on toy datasets and real image datasets, to show that while FID scores can be inconsistent, the KID scores are always consistent."
            },
            "weaknesses": {
                "value": "1. For FID scores, while the paper gave theoretical formulations for FID-avg and FID-all, the formulations can only distinguish different rankings of generators based on their distances to corresponding covariance matrices. It would be more informative to characterize the gap between FID-avg and FID-all with the distances between clients\u2019 data distributions. This could allow one to estimate the degree of FID score inconsistency based on how different clients\u2019 datasets are.\n\n2. In experiments, it seems that most results for FID scores come from ``simulated\u2019\u2019 generators, that is, treating a class of images from CIFAR-10 or CIFAR-100 as the output of a generator. Such simulation does not well represent real generators trained on the multi-class dataset, or the general federated learning protocol (even when each client owns one class of images only). Hence the results do not sufficiently reveal if inconsistency between FID-avg and FID-all is an actual issue in practice. Moreover, the FID scores are quite large and far from optimal, in which case the consistency of the evaluation metric may not be a most important property to pursuit."
            },
            "questions": {
                "value": "In Figure 2, do the KID scores imply that the plane generator is better than the DDPM model, while the former generates much less diverse images? This may raise a question of how to properly evaluate the models apart from consistency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission579/Reviewer_1nBR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733907877,
        "cdate": 1698733907877,
        "tmdate": 1699635985094,
        "mdate": 1699635985094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uxvZSKHbk5",
        "forum": "IfqXxs1lCJ",
        "replyto": "IfqXxs1lCJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission579/Reviewer_dkjU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission579/Reviewer_dkjU"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the extension of FID and KID scores from centralized setting to distributed setting. Average of scores at each client is compared to the corresponding centralized score. Authors prove that the FID score rankings may not match, while KID scores always do. Experiments confirm this theoretical claim."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Important setting: Distributed learning and evaluation of generative models is an increasingly important topic\n- The theoretical analysis is accurate and empirical results adequately support the theory"
            },
            "weaknesses": {
                "value": "- The paper\u2019s main contribution is a very simple observation. In essence, the FID score is an infimum of an expected distance, whereas KID is expectation of the kernel distance. Hence, KID ranking consistency directly follows from linearity of expectation, while FID does not follow the same. This observation alone does not constitute a significant contribution.\n- The simple observation would\u2019ve still constituted as a good contribution if the authors provide some actionable insights. For example, one possible conclusion could be that one should use KID rather than FID in distributed settings \u2014 however, authors do not compare models obtained via these two methods in detail. Another possible direction could be to modify FID so that the new score behaves well under averaging."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699317279512,
        "cdate": 1699317279512,
        "tmdate": 1699635985011,
        "mdate": 1699635985011,
        "license": "CC BY 4.0",
        "version": 2
    }
]