[
    {
        "id": "xu25XpFGIU",
        "forum": "vE1e1mLJ0U",
        "replyto": "vE1e1mLJ0U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_8X2o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_8X2o"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose a model of neurons called Expressive Leaky Memory (ELM) (and Branch ELM later introduced in the paper). The goal is to build a model with fewer parameters than existing models that can still learn to replicate the input/output relationship of some pyramidal neurons. The authors also extend the evaluation to other tasks that are less neuroscience-inspired but aim at evaluating the ability of the model to capture long-term dependencies in the dataset. \n\nThe model is reminiscent of an LSTM with significant modifications leading to the possibility of better \u201csynaptic\u201d integration and longer timescales. \n\nThe authors do not aim to build a biologically plausible model of pyramidal neurons but to replicate some of its capabilities using less learnable parameters than existing literature, mostly other LSTMs and a TCN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's strengths are the following. \n\nThe paper is well-written and the task at hand is very interesting. As pointed out by the authors, not many models exist that can effectively represent the IO relationship of pyramidal neurons, and being able to do that could lead to improvement for various machine learning problems. \n\nThe presentation of numerical simulations and the exploration of machine learning tasks beyond neuroscience are commendable."
            },
            "weaknesses": {
                "value": "The weakness of the paper. \n\nAlthough the paper argues that \u201cmerely\u201d a few thousand learnable parameters is a good way to replicate what a single pyramidal neuron performs, it seems to still be a lot to me. In my understanding, the paper doesn\u2019t address the task that the pyramidal neurons would perform, and thus, it is not clear such a model would be able to learn the same input/output relationship if only inputs were given. \n\nVarious papers, including recent papers on predictive coding and canonical correlation analysis, have suggested possible tasks performed by some pyramidal neurons. It would be interesting to see if such a model of neurons could learn the same IO based on this learning paradigm.\n\nAs pointed out, the synaptic plasticity and how the model's training is performed are not addressed. \n\nRegarding the numerical experiments, the model is only compared to other \u201cmodern\u201d ML models and not those that are more biologically plausible. I would like to see where simple pyramidal neuron models perform on spiking methods such as the one presented in this work. Compared to LSTMs and TCNs, it is not merely as relevant. \n\nThe term inductive bias is often used in papers when trying to characterize the \u201creasonable\u201d choice of architecture as being inspired by biological facts. I can appreciate that the term is currently \u201chype,\u201d but it is possibly misleading when considered at a machine learning conference where inductive bias means something else. The term hand-engineered would be more appropriate here. \n\nAlthough the paper is well written, the choice of wording in many places is unscientific, e.g., \u201cstruggle to learn at all,\u201d \u201cmerely a few thousand,\u201d \u201cmerely meant to capture,\u201d \u201cdegrading only gracefully,\u201d and more. I would appreciate it if the authors paid more attention to possible bias in the writing of the paper. \n\nThe introduction of the Branch-ELM in Figure 4 appears too late in the paper. We are introduced to the concept at the same time as the results of the experiments when it would have been better to have it in Section 2 when the ELM is introduced. \n\n\nIn conclusion, I believe that the paper has some value, but I am not certain that it is well-suited for the venue. I also believe the paper doesn\u2019t deliver on the claims made in the abstract or at the end of the introduction. I believe that what is achieved in the paper is overstated. Also, I would like to get the authors to write the paper with less biased words, as was mentioned in the weakness section."
            },
            "questions": {
                "value": "Based on the weaknesses highlighted above, I would suggest the authors address how they would provide results that more closely align with the claims. And provide some improvement on the various fronts that I have highlighted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698351277010,
        "cdate": 1698351277010,
        "tmdate": 1699636609242,
        "mdate": 1699636609242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qySp5lyAls",
        "forum": "vE1e1mLJ0U",
        "replyto": "vE1e1mLJ0U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_nAEZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_nAEZ"
        ],
        "content": {
            "summary": {
                "value": "The authors introduced the Expressive Leaky Memory (ELM) neuron model, a bio-inspired model of a cortical neuron. It incorporates slowly decaying memory-like hidden states and a two-layered nonlinear integration of synaptic input. They showed that this model is able not only to capture the input-output mappings of cortical neurons efficiently but also to solve long-range dependency problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The ELM model achieves a notable decrease in trainable parameters compared to temporal convolutional networks for simulating cortical neurons.\n- The paper is well-written and easy to follow, particularly in the second section, where the design of the ELM neurons is explained.\n- The selected experiments are suitable and effectively demonstrated the model's capabilities."
            },
            "weaknesses": {
                "value": "- The main text doesn't define the Branch-ELM variant. See questions.\n- Minor training details need some clarifications. See questions."
            },
            "questions": {
                "value": "- A paragraph defining Branch-ELM would be necessary. Can you elaborate more on the intuition behind this variant? How does it work? When is it more suitable compared to the vanilla ELM? Why is it important to over-sample the input in this case? \n\n- The term \"fixed trainable\" time constant is confusing. Is it a single tau value learned for each neuron that does not change after training?\n\n- In Appendix B, the general training setup is detailed (batch size of 8, etc.). However, later on, different hyperparameters are used for the datasets. It is not clear where this general training setup was used.\n\n- Does Figure S6 a) show any specific patterns?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698442541721,
        "cdate": 1698442541721,
        "tmdate": 1699636609116,
        "mdate": 1699636609116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2c423ciTlo",
        "forum": "vE1e1mLJ0U",
        "replyto": "vE1e1mLJ0U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_27bc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_27bc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes artificial neural network models that incorporates important inductive bias (i.e. leaky memory dynamics, nonlinear synaptic integration) inspired from biological cortical neurons. The model is aimed to achieve two types of goals, one is to match the spike-level dynamics of pyramidal neuron, and the second is evaluated on bio-inspired tasks to evaluate temporal integration. For evaluation, they compare the model with others SOTA baselines (SNN, LSTM, Transformer) are evaluated on multiple biological inspired datasets and long sequence modeling task. It shows the benefits of efficiency in parameterization, and comparable or better performance compared to SOTA models. Meanwhile, the hyper-parameter tuning studies show overlap with previous literatures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-motivated, and take the reductionist view to minimize the parameters from a more detailed modeling for cortical neurons, and aim to address the computational efficiency needs of standard models.\n2. Solid evaluations on multiple biological inspired datasets, and compared with multiple SOTA models, and follow by multiple hyperparameter tuning studies. \n3. The biological realism side shows interesting overlaps with previous neuroscience literatures.\n4. The results show the model is capable of achieving better accuracy with efficiency and fewer parameters than traditional deep model LSTM. The finding about simplification does not sacrifice the predictive performance is valuable.\n5. The paper is well-written, and organized in a good structure."
            },
            "weaknesses": {
                "value": "1. This paper is aimed to balance the trade-off between fidelity, efficiency and biological realism. It did a fair job while still failed to capture some important aspects. For example, using MLP sacrifices the interpretability and biological realism to compensate accuracy. On the other hand, the model still sacrifices the accuracy and has a big performance gap when compared to SOTA models (S4 and Mega). \n2. Scalability of the method: as scaling law plays a big role for improving transformer's predictivity, one concern is that transformer might perform better with increasing number of parameters. However, it might not be the same for ELM. As firstly shown in Fig 3, the accuracy quickly saturates with simply increasing $d_m$ and $d_{mlp}$, and not able to get further improved to minimize the performance gap between ELM and Mega.\n3. Only one ML task is evaluated, more evaluations and benchmarks needed to demonstrate contributions in addressing long-range sequence modeling.\n4. As shown in Table S1, large number of hyper-parameters still needed in advance or be tuned based on prior knowledge from neuroscience literatures.\n5. The efficiency side might be over-claimed, as Table 1 shows ELM still requires 100k-200k parameters?"
            },
            "questions": {
                "value": "1. What other critical components might be helpful to improve ELM model accuracy?\n2. After applying sparse regularization or quantization to S4 and Mega to match the number of parameters to ELM, how much accuracy drop they will have?\n3. Is the model able to be trained with other biological plausible learning rules instead of BPTT? How they might end up with different parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5789/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5789/Reviewer_27bc",
                    "ICLR.cc/2024/Conference/Submission5789/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724670195,
        "cdate": 1698724670195,
        "tmdate": 1700699881794,
        "mdate": 1700699881794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uwiP7IjHsu",
        "forum": "vE1e1mLJ0U",
        "replyto": "vE1e1mLJ0U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_YDMm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5789/Reviewer_YDMm"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a phenomenological neuron model called the Expressive Leaky Memory (ELM) neuron that uses various biologically inspired features. Specifically, it has separate synapse and memory dynamics and an integration mechanism defined by a learnt MLP. The model also allows learning of the various time constants. The authors demonstrate that this model is able to fit the input-output relationship in a dataset generated from a detailed biophysical model. Moreover, the authors show that this model can perform long-range dependency modelling better than vanilla transformers and a LSTM-based recurrent model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is well motivated, and the model uses abstract simplifications to represent known biological details. This provides a relatively parsimonious but abstract model to model biological neurons, which is a very interesting approach and novel to my knowledge.\n- The fact that this model, even though abstract and motivated by biology, still performs well in long range arena is very interesting.\n- The description of related work is very comprehensive.\n- The authors include a good discussion of the potential shortcomings of the model.\n- Overall, the quality and significance of this work is high."
            },
            "weaknesses": {
                "value": "- There are major clarity issues in the paper. Many aspects of the model and notation are unexplained (e.g. $\\lambda$, $1-\\kappa_m$ in Fig. 1(c)). The explanation of Branch-ELM comes much later, even though it's referred to multiple times before that, which makes it very hard to read. The role of $w_s$ is also not clear at all (the given explanation on Pg. 3 doesn't help).\n- The behaviour of Branch-ELM is unclear -- if the input is shuffled, does it affect performance? Since it depends on the local window to group inputs?\n- It's a bit odd that ELM doesn't perform well for short sequences (large bin size) as seen in Fig. 5 whereas LSTM does. The performance of ELM for short sequences could be explored more, since it sounds like that might be a major shortcoming of the model.\n- It is not clear how the number of parameters for the various cases were chosen.\n- I think exploring the multi-layer case would have made the paper much stronger. It's also not clear if this was avoided because of the computational constraints, since for mid-size LSTMs at least, multi-layer networks still fall very much in the computationally tractable regime."
            },
            "questions": {
                "value": "## Questions\n\n- Would this neuron be able to model synapse dynamics such as short-term plasticity (Tsodyks et al. 1998)?\n\n## Suggestions\n\n- Spike frequency adaptation for spiking neurons was proposed in (Bellec et al. 2018) rather than (Bellec et al. 2020).\n\n### Minor:\n\n- In the abstract, \"exploiting a few such slowly decaying...\" sentence reads very odd.\n- sentence above beginning of Sec. 4: \"puting the major emphesis\" has typos. That paragraph is very hard to read.\n\n(Bellec et al. 2018) Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W. (2018). Long short-term memory and Learning-to-learn in networks of spiking neurons. In Advances in Neural Information Processing Systems 31, pp. 787\u2013797.\n\n(Tsodyks et al. 1998) Tsodyks M, Pawelzik K, Markram H. Neural networks with dynamic synapses. Neural Comput 10: 821\u2013 835, 1998."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5789/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5789/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5789/Reviewer_YDMm"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865611400,
        "cdate": 1698865611400,
        "tmdate": 1700649334239,
        "mdate": 1700649334239,
        "license": "CC BY 4.0",
        "version": 2
    }
]