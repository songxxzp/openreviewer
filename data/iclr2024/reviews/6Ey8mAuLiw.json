[
    {
        "id": "JZOnR2Mzys",
        "forum": "6Ey8mAuLiw",
        "replyto": "6Ey8mAuLiw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_vmJ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_vmJ4"
        ],
        "content": {
            "summary": {
                "value": "This work studies the feature learning of a kind of mutitask representation learning where multiple classification tasks share the same output space (same y space). Through some theoretical and experimental studies, this paper concludes that this kind of multipletask learning helps learn shared features and thus helps generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of investigating shared feature and task-specified feature in neural network representation learning is interesting. \n- The author provide a rich liturature review of Multi-task Representation Learning."
            },
            "weaknesses": {
                "value": "- The mutipletask setting in this work, various single-tasks contain different noise in input X but share the same output categories Y, is almost exactly data-augmentation. The shared feature and task-specified feature conclusion in this paper driven by mutipletask is also close to the invariant feature and spurious feature view in data-augmentation.  From data-augmentation's view, one can easily get these conclusions.\n\n- In the theory part, the paper mentioned that each single-task contain $n$ examples, but didn't give the number of examples in mutipletask. In the experiment settings, the paper considers the **union** of single-tasks as a mutipletask: *\"Therefore, we consider learning on each specific corruption as the single-task setting and learning on a union of corruptions as the multi-task setting. \"*.  That means mutipletask in this work contains much more examples than each single-task.  From the large training set's view, one can again easily get this paper's conclusion."
            },
            "questions": {
                "value": "- Does the mutipletask contain the union of single-tasks? In other words, does the mutipletask contain more training examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Reviewer_vmJ4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788665144,
        "cdate": 1698788665144,
        "tmdate": 1699637096637,
        "mdate": 1699637096637,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zgxBP8x5sn",
        "forum": "6Ey8mAuLiw",
        "replyto": "6Ey8mAuLiw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_yKNZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_yKNZ"
        ],
        "content": {
            "summary": {
                "value": "The submission uses a simple mathematical model of a convolutional neural network and data that obeys certain properties analogous to single-task and specific multi-task settings, in order to develop an analysis of learning dynamics and theoretical bounds that illustrate the benefits of multi-task learning under such modelling assumptions. The mathematical tools are based on tensor-power methods developed in the literature. Some experiments are performed to confirm the benefits of multi-task learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Originality: To my knowledge, the analysis carried out in this submission is quite original, both in terms of the choice of mathematical tools utilized, as well as the analysis about learning dynamics in single and multi-task settings.\n\n* Quality: The submitted work seems to generally be of high quality, in terms of the theoretical contributions.\n\n* Clarity: With the caveat that I\u2019m no mathematician and didn\u2019t verify any of the proofs, I found the main body of the paper overall very readable. I particularly appreciated the intuitive descriptions along with the formal statements.\n\n* Significance: The model and mathematical tooling adopted in the paper seems well-suited to analyzing neural networks, and the demonstration for how it can be used to explain the empirical observations about the usefulness of multi-task learning is potentially inspirational for similar analyses applied elsewhere."
            },
            "weaknesses": {
                "value": "* The experiments could be in greater correspondence with the developed theory. Currently, the empirical results only demonstrate that multi-task learning is an improvement, something that is fairly well-acknowledged and doesn\u2019t really need further demonstration. What would be far more interesting is if the learning dynamics posited in the submission were shown to be reflected in real-life learning dynamics (i..e the stage-wise learning processes described in Section 5). \n\n* The submission seems to model specifically one variant of a multi-task learning setup, i.e. different data sources for a common task. There are other variants that might be more commonly associated with multi-task learning, in particular that of the same data source/domain but different objectives. Perhaps the setup that has been tackled in the manuscript is better thought of as a model for analyzing the benefits of data augmentation? Or perhaps it is possible to draw stronger links between the data and task model discussed with other variants of multi-task learning setups."
            },
            "questions": {
                "value": "1. If I followed right, the theory only focuses on binary classification problems. Do the authors anticipate any difficulties with extensions into multi-label categorization or regression problems?\n\n2. What is c in Other Requirements on Page 4? An arbitrary constant?\n\n3. What is p on the first line on page 5? The probability of the feature patch being the task-shared feature?  How would one intuitively interpret this bound, in terms of how p and n interact to raise or lower the error rate bound?\n\n4. Just to confirm, In Theorem 4.2, the test error is equal to poly(1/n), or should it be poly(1/nK)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812686765,
        "cdate": 1698812686765,
        "tmdate": 1699637096516,
        "mdate": 1699637096516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yz9u5CNJ5R",
        "forum": "6Ey8mAuLiw",
        "replyto": "6Ey8mAuLiw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_rtpX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_rtpX"
        ],
        "content": {
            "summary": {
                "value": "The paper derives generalization bounds for multi-task learning and single-task learning, under a data generating process with task-specific and task-shared features. Under this assumption, both single and multi-task learning achieve zero training error but only multi-task learning achieve asymptotically 0 generalization error. The authors derive these results for a 2-layer neural network trained with gradient descent. Finally, the authors present a few small experiments on multi-task learning on CIFAR-10 and a synthetic dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Novelty and Quality.** The authors consider a setting, that has not been previously tackled theoretically.  Most prior works have dealt with the existence of a single shared representation This work instead assumes that some input features are shared across all the tasks and task-specific features have some noise. Previous works primarily conduct a learning theoretic analysis for multi-task learning while this work conducts an analysis using the gradient descent update.\n\n**Presentation.** The authors present thorough analysis of two-layer neural networks with this specific data generating process. The manuscript details fairly involved calculations to derive these results. The authors also do a great job at presenting some of these results; the notation in section 5 is fairly involved and was hard to parse but I appreciate the effort from the authors."
            },
            "weaknesses": {
                "value": "**Why is the noise present only in the task-specific features?**\nFrom my understanding, the noise is only present for the task-specific features. This seems to be the reason for the lower-bound on single-task learning. But why is this a reasonable assumption reasonable? Why is the noise not present in the shared features? For example, one can imagine that \"dogs\" may have task-specific features that may not be important for classifying \"birds\" and I believe this scenario is far more common in multi-task learning.\n\n**How does this work differentiate itself from work from Baxter et al., Maurer et al., Tripuraneni et al.**\nPrior works have deal with the scenario where there exists a single representation shared across all the tasks. The assumption in this work is more restrictive:  it assumes the existence of shared input features. Is it possible to derive similar conclusions using results from prior work? In particular, prior work also predicts that the error decreases asymptotically as $\\sqrt{n^{-1}}$ where $n$ is the number of tasks.\n\n**Do the synthetic and real-world experiments show two stages?**\nAre the predictions made by theory representative of what occurs in practice? Do we see different observations if there is no weight decay for the multi-task models?\n\n\n**It isn't clear how the CIFAR10-C experiments complement the theory.** The results of the experiment are unsurprising: multi-task learning works on a dataset like CIFAR10-C. Is the data generating process, accurate for even simple datasets like CIFAR10-C? The experiments do not seem to complement any of the theoretical findings in the paper (two stages of learning process, rate of convergence, lower bound of single-task learning, etc..)."
            },
            "questions": {
                "value": "1. **Importance of these techniques towards understanding multi-task learning.**  In practice, neural networks are deep and over-parameterized. Can these proof techniques be scaled beyond two layers to understand multi-task learning? Currently, they do not seem to provide a complete theory for even a highly idealized two layer neural network.\n\n2. **Is the data generating process reflective of real world datasets?** Multi-task learning, works for a broad range of settings outside of having a shared input patch across all the tasks. My concern is that this assumption is too narrow and maybe weaker than prior works that have assumed shared representations.\n\n3. **Verifying two stages of learning using experiments**. It would be helpful to verify that two stages of learning even on synthetic data. What happens if the regularization is set to 0?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8738/Reviewer_rtpX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841365862,
        "cdate": 1698841365862,
        "tmdate": 1699637096353,
        "mdate": 1699637096353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UAdHe0k44E",
        "forum": "6Ey8mAuLiw",
        "replyto": "6Ey8mAuLiw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_tYKP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8738/Reviewer_tYKP"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a theoretical analysis to show why multi-task learning leads to models with better generalizability than single-task learning when training neural networks using gradient descent. The analysis was done with a much-simplified problem setup comparing to networks commonly used in current practice. In addition to the theoretical analysis, empirical results on synthetic datasets and real-world datasets were provided to show multi-task learning outperforms single-task learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ In the era of full of papers of neural networks with only empirical results, the efforts in theoretical understanding of better generalizability of multi-task learning than that of single-task learning in the context of gradient descent is commendable."
            },
            "weaknesses": {
                "value": "- The considered problem setup is an oversimplification of networks commonly used in the practice. The implication of their results on complex networks is not clear and likely limited. On the other hand, though, it may seed future theoretical studies on more complicated cases. \n \n- The included experiments provide little to no value. The reported results are just typical observations one would expect to see when comparing multi-task and single task learning in the cases where tasks are related."
            },
            "questions": {
                "value": "- The parameter \\alpha in their set up (Definition 3.1) is expected to have high impact. Since it approaches to 1, there should be no difference between shared and task-specific features. However, I did not see \\alpha involved in any of the derived quantities. \n\n- In the synthetic experiments, there are both shared and task-specific features. According to Definition 3.1 and 3.2, there should be more than two patches when including noise patches. Then, how come H can be 2?\n\n- A discussion on how realistic those requirements listed at the end of section 3 are would be helpful.\n\n- There are mathematic symbols without proper introduction, for example, n and polylog(n) in Definition 3.3. Is this n the total number of examples in the training set? If that is the case, the test error bound in Theorem 4.1 is counter intuitive. Because according to the bound, larger training set size leads to larger test/generalization error."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699067526393,
        "cdate": 1699067526393,
        "tmdate": 1699637096248,
        "mdate": 1699637096248,
        "license": "CC BY 4.0",
        "version": 2
    }
]