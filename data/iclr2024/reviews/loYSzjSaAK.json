[
    {
        "id": "JPYvZZ7btL",
        "forum": "loYSzjSaAK",
        "replyto": "loYSzjSaAK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_oKKZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_oKKZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes and studies submodular MDPs, where the total reward is characterized by a submodular function of the trajectory. The authors first show that computing a logarithmic approximation of the optimal policy is computational intractable. However, there exists a polocy optimization algorithm which gives a (1-c)-approximation where c is the curvature of the submodular function. Specifically, when specified to bandits, this result outperforms existing ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The model is well-motivated and clearly described. It is also easy to understand.\n2. The results contain both upper and lower bounds, which are pretty complete.\n3. Empirical evaluations are conducted for the proposed algorithm."
            },
            "weaknesses": {
                "value": "1. I'm not sure why this paper considers multiplicative approximations instead of regret/sample complexity, which are common in theory papers studying episodic MDPs.\n2. The optimal dependency on curvature remains unspecified. Whether Proposition 3 is (near-)optimal?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563581079,
        "cdate": 1698563581079,
        "tmdate": 1699636466331,
        "mdate": 1699636466331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wAc3Kx6pbB",
        "forum": "loYSzjSaAK",
        "replyto": "loYSzjSaAK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
        ],
        "content": {
            "summary": {
                "value": "The submission considers a new framework of Submodular Reinforcement Learning (SubRL), where the total reward is given as a submodular function of given trajectories, rather than as an additive sum of rewards from individual time steps. From my understanding, the main applications of interest would have environments where repeated actions (at the same states) are not so much preferred -- this is explicitly embedded in the reward design itself in SubRL. Contributions are the following:\n\n- While the optimal policy can still be Markovian, the authors first show that approximating the optimal value up to any constant factor is computationally hard in polynomial time (that is, planning is computationally hard). \n\n- Given an additional assumption that the reward function is DR-submodular, AND if the underlying MDP is nearly deterministic, then a constant factor approximation is possible.\n\n- The authors present a policy-gradient type algorithm for SubRL, and demonstrate the effectiveness of the method on several interesting synthetic examples and deep-RL settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The submission introduces a novel and \"mathematically\" interesting framework that accounts for diminishing returns of repeated actions. \n\n- The view of submodular rewards is fresh. The hardness result is new and interesting. \n\n- The selected toy examples sound interesting and well-suited for the proposed framework."
            },
            "weaknesses": {
                "value": "- I do not see much contribution in positive results. Not only does the assumption sound strong from a practical perspective, but it seems quite contrived only for the sake of analysis. \n\n- Literature review: I agree with the motivation from diminishing returns, but a submodular reward design is not the only way to address that. For example, there is a blocking-bandit style framework that discourages repeated actions [1]. Maybe good to discuss why the submodular reward design is better. \n\nI also encourage authors to survey more existing works that explore similar ideas with submodular reward design. For example, can the authors explain the difference between [2] in terms of the problem setting?\n\n[1] Basu et al., Blocking Bandits, NeurIPS 2019.\n\n[2] Chen et al., Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward, NeurIPS 2018. \n\n- Suggestion: It looks slightly unnatural to have Section 4 (practical algorithms) in between hardness results (Section 3) and positive results (Section 5). A more natural flow would have been having the positive theoretical results first and then presenting practical algorithms, or at least having them connected. \n\n- Overall, I feel that the framework is well-motivated for the \"mathematical\" purpose but less sound for the practical purpose or advancing the theory of RL."
            },
            "questions": {
                "value": "- I do not understand what it means by $\\pi$ is parameterized by $\\pi^h(a)$ in Theorem 3. Does this mean $\\pi$ does not depend on the state? \n\n- Definition 2 - why is it named \\epsilon-\"Bandit\" SMDP? \n\n- The submission focuses on the \"planning\" side. Any thoughts on the \"learning\" side? (a.k.a., exploration and sample complexity)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_6FV4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805847383,
        "cdate": 1698805847383,
        "tmdate": 1700640304328,
        "mdate": 1700640304328,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iHuHqVziza",
        "forum": "loYSzjSaAK",
        "replyto": "loYSzjSaAK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a submodular reinforcement learning (subRL) setting. Different from the existing reinforcement learning settings, they do not assume the rewards are additive. This allows them to work with more general and history-dependent reward models and they characterize these reward models with submodularity. Moreover, they design a policy gradient-based algorithm, called subPO, for  subRL problems by drawing inspiration from the greedy algorithm for classical submodular problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Combining submodularity with reinforcement learning in a generalized way seems highly intuitive that I am surprised it has not been proposed before. This emphasizes the significance of the paper's contribution. The main idea of the paper is a simple yet powerful one. Additionally, the paper is well written and the ideas or conveyed clearly."
            },
            "weaknesses": {
                "value": "These are more minor suggestions for improvement rather than weaknesses:\n- On the last paragraph of page 1, the adverbs firstly, secondly, thirdly can be just replaced with first, second, and third. Also, we after the firstly should be lowercase.\n- I think there can be a broader discussion of using submodular functions in reinforcement learning setups in the related work section. I am aware that the introduction also mentions some examples of submodular rewards, but I believe it is interesting enough to have its own paragraph in the related work."
            },
            "questions": {
                "value": "- Are there other attempts of incorporating submodular functions to reinforcement learning problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4828/Reviewer_uscY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837912162,
        "cdate": 1698837912162,
        "tmdate": 1699636466160,
        "mdate": 1699636466160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xXPsQZppTD",
        "forum": "loYSzjSaAK",
        "replyto": "loYSzjSaAK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4828/Reviewer_u5A8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies submodular reinforcement learning, i.e. reinforcement learning with submodular set reward function that captures diminishing returns. Specifically, this paper has made the following contributions:\n\n- This paper motivates and develops the framework of submodular reinforcement learning.\n\n- This paper derives a lower bound that establishes hardness of approximation up to log factors in general (Theorem 1, Section 3).\n\n- This paper motivates and develops a general algorithm for the considered problem, referred to as Submodular Policy Optimization (SubPO, Algorithm 1). This is a policy optimization algorithm. Provable guarantees are established in some restricted settings (Section 5).\n\n- Extensive and rigorous experiment results are demonstrated in Section 7."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The considered problem is interesting and significant.\n\n- Extensive and rigorous experiment results have been presented in Section 7.\n\n- The paper is well-written in general, and easy to read."
            },
            "weaknesses": {
                "value": "- The idea behind the proposed algorithm, Submodulr Policy Optimization, is quite straightforward. It is just a relatively straightforward extension of the classical policy optimization algorithm.\n\n- The analysis in Section 5 seems to be very restricted. Could the authors provide a similar analysis in more general settings?"
            },
            "questions": {
                "value": "- Please try to address the weaknesses listed above.\n\n- It is not clear to me why the authors chose to put the \"Related Work\" section between an analysis section and the experiment section. Probably the authors should put it after Introduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4828/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699259728131,
        "cdate": 1699259728131,
        "tmdate": 1699636466081,
        "mdate": 1699636466081,
        "license": "CC BY 4.0",
        "version": 2
    }
]