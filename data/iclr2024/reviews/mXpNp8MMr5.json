[
    {
        "id": "HkkqWlG1Y5",
        "forum": "mXpNp8MMr5",
        "replyto": "mXpNp8MMr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_ox78"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_ox78"
        ],
        "content": {
            "summary": {
                "value": "This work described the problem of hypocritical examples which added tiny perturbations to clean test inputs to increase the targeted model's performance. These samples can mislead machine learning practitioners into assuming that their machine learning models are good enough to be applied in the real world. The authors also mentioned that these samples can be crafted from adversarial examples to provide a false sense of adversarial robustness. They called this two-faced attack and provided its problem formulation and algorithm to create such an attack. They proposed a countermeasure by increasing the perturbation bound of adversarial training to get the best tradeoff between adversarial risk and two-faced risk. After that, they showed a bunch of experiment with several datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper has a strong formulation and background of hypocritical examples on adversarial examples.\n- The experiment covers several kinds of datasets.\n- The literature review is good and updated.\n- The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "- The background and motivation are not convincing to me. The authors may make it more motivating, and probably, look at the previous work (Tao, 2022b) since it is very convincing.\n- The experiments for the countermeasure (enlarging the budget) are limited. The authors may need to add more experiments for it.\n- The contribution of this paper is limited because the content is very similar to the previous work (Tao, 2022b), but changes from clean test samples to adversarial test samples. The authors may focus more on the countermeasure."
            },
            "questions": {
                "value": "- In Appendix A, I do know how you derive from E[1(f(x'') = y) * 1(f(x_{adv} \\neq y)] to E[1(f_{adv} \\neq y)] * E[1(f(x'') = y)]. It is in the lines 7 and 8 if you count the line R_{rhpy}(f,D) as line 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4677/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4677/Reviewer_ox78",
                    "ICLR.cc/2024/Conference/Submission4677/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633810710,
        "cdate": 1698633810710,
        "tmdate": 1700529896867,
        "mdate": 1700529896867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0fRVy3ZSOH",
        "forum": "mXpNp8MMr5",
        "replyto": "mXpNp8MMr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_J5b2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_J5b2"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that adversarially trained models are vulnerable to a new threat called two-faced \nattacks, where slight perturbations in input features are crafted to make the model exhibit a false sense \nof robustness in the verification phase. This paper also shows that this threat is pervasive and tricky, \nbecause many types of models suffer from this threat, and models with higher adversarial robustness \ntend to be more vulnerable. Besides, this paper gives a formal formulation for this threat and discloses \nits relationship with adversarial risk. This paper also proposes a simple countermeasure to circumvent \nthe threat. Empirical results have validated the arguments presented in this paper."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. It is important to disclose the existence of such two-faced attacks in the model verification phase, as \ndeploying substandard models (with low adversarial robustness) in reality could cause serious security \nissues.\n\n2. There are some interesting findings that can demonstrate the practical importance of two-faced \nattacks. For example, many types of models suffer from this threat, and models with higher \nadversarial robustness tend to be more vulnerable. \n\n3. This paper mathematically formulates the two-faced attacks and the two-faced risk, and provide a \ntheoretical analysis on its relationship with adversarial risk, and provide a discussion on possible \ncountermeasures to circumvent two-faced attacks.\n\n4. Experimental results are supportive."
            },
            "weaknesses": {
                "value": "The two-faced attacks are the key of this paper, but the realistic application domains are still unclear. \n\nFrom Figure 1(a), this paper gives a machine learning workflow that shows the adversarial robustness \ncan be affected by the two-faced attacks in the model verification. However, this paper did not \nprovide a real-world example that the two-faced attacks can be applied into and can cause serious \nsecurity issues. This point can further strengthen the significance of the two-faced attacks."
            },
            "questions": {
                "value": "Can the authors provide a real-world example that the two-faced attacks can be applied into?\n\nCan the authors provide more potential countermeasures against two-faced attacks, apart from the \nones mentioned in Section 3.3?\n\nOther problems please see **Weaknesses.**\n\nOverall, I like the idea and analysis in this paper.  I expect the problems could be clarified and addressed in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4677/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4677/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4677/Reviewer_J5b2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645955447,
        "cdate": 1698645955447,
        "tmdate": 1699636448745,
        "mdate": 1699636448745,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u1mzyCvXXW",
        "forum": "mXpNp8MMr5",
        "replyto": "mXpNp8MMr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_rs75"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4677/Reviewer_rs75"
        ],
        "content": {
            "summary": {
                "value": "The vulnerability in machine learning models that this paper reveals is called \"two-faced attacks,\" wherein small input perturbations trick the model during verification and give the impression that it has high adversarial robustness. This observation raises concerns since it may result in security vulnerabilities if models that are not reliable are used based on false robustness evaluations. The phenomena is common and, surprisingly, more prevalent in models that are thought to be quite robust. The authors provide a framework for comprehending this problem and make some initial recommendations for solutions. Rather than accepting adversarial robustness at face value, they advise individuals to evaluate it rigorously."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This study takes a leading role in exposing the vulnerability of adversarially trained models to 'two-faced attacks', which fraudulently overestimate the robustness of the model during the verification stage. \nNotable features include its comprehensive evaluation across a variety of model architectures, datasets, and training techniques, as well as its introduction of the 'two-faced risk' notion to reconcile theory and empirical results. \n\nAlso, the work represents a challenging problem in adversarial defense, as it reveals a counterintuitive trade-off: models that are more resilient to adversarial examples are also more vulnerable to two-faced attacks. \n\nHowever, the paper's implications for practice are highlighted by a strong need for rigorous validation processes prior to deployment, particularly in safety-critical domains. This call urges a reevaluation of how adversarial robustness is measured and perceived in the field."
            },
            "weaknesses": {
                "value": "Although the study identified two-faced attacks against adversarially trained models in a novel way, it might not provide practitioners with sufficient defenses to address these weaknesses. Though interesting, its theoretical investigation of two-faced risk may prove difficult to implement in real-world scenarios and may not provide enough information about mitigation strategies. \n\nAdditionally, there may be an overemphasis on two-faced attacks, which might mask other important security issues that need to be taken seriously. Furthermore, a lack of clarity in the procedures for experiments may make them difficult to repeat and inhibit future studies.\n\nFinally, there can be a gap in comprehensive risk management techniques if the paper does not include a comprehensive explanation of how to balance adversarial risks, such as two-faced risk, against other risks."
            },
            "questions": {
                "value": "Just two questions:\n\nIs it possible to include the idea of two-faced risk into current adversarial training frameworks without making major changes?\n\nHow can long-term model validation procedures be affected by the future evolution of two-faced attacks?\n\n\nFinally, two suggestions:\n\nFuture work should focus on developing more comprehensive defense strategies against two-faced attacks that can be easily implemented in real-world systems.\n\nPerform research on how models might eventually be exposed to fraudulent attacks, especially if adversaries and attack techniques advance in sophistication."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699034515326,
        "cdate": 1699034515326,
        "tmdate": 1699636448659,
        "mdate": 1699636448659,
        "license": "CC BY 4.0",
        "version": 2
    }
]