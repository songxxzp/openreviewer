[
    {
        "id": "fqsEblAkZy",
        "forum": "eeaKRQIaYd",
        "replyto": "eeaKRQIaYd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_gT8h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_gT8h"
        ],
        "content": {
            "summary": {
                "value": "The paper studies a new setting for sign language understanding: unsupervised sign language translation and generation (USLNet), which exploits information from abundant single-modality but non-parallel data. More specifically, UNMT pretrains its text encodes/decoder and video/encoder decoder by reconstruction tasks. To address the misalignment issue between video and texts, the authors further propose a sliding window based aligner."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is sound. Due to the data scarcity issue in the sign language understanding systems, it is important to explore non-parallel data.\n2. The text-video-text and video-text-video back-translation strategies are novel in the sign language community.\n3. Detailed ablation studies."
            },
            "weaknesses": {
                "value": "1. The major issue is the experimental setting.\n\n1.1. Intuitively, leveraging abundant data should be helpful to the model performance. For example, in MMTLB (Chen et al., 2022), using a translation network pretrained on large natural language corpus can boost sign language translation performance. But I didn't see similar conclusions in the experiment section. In Table 1, the authors directly compare a supervised model with the proposed USLNet, and get a worse result on BLEU-4. I understand that the unsupervised performance must be worse, but what is this comparison for? I hope to see that for example, fine-tuning USLNet on parallel corpus can give better results, i.e., similar to the conclusion in MMTLB.\n\n1.2. The performance is **too bad**. Although it may not be the authors' fault (maybe the dataset is too difficult), the poor performance make the comparison less convincing. Experiements on other widely-adopted benchmarks, e.g., Phoenix-2014T and CSL-Daily, shall be considered.\n\n2. It seems that there is a factual error in the sliding window based aligner. The text and video are not monotonically aligned. In fact, only video and glosses are monotonically aligned, e.g., 1-10 frames for the first gloss, and 11-20 frames for the second gloss. Thus, it is questionable for the design of the sliding window-based aligner.\n\n3. The descriptions for the process of the aligner should be more clear. The current form is a bit difficult to understand.\n\n4. The process of two back-translation strategies are simialr to dual learning. The authors may consider adding a subsection in related works to discuss dual learning."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698505232745,
        "cdate": 1698505232745,
        "tmdate": 1699637067488,
        "mdate": 1699637067488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U71cNImRBQ",
        "forum": "eeaKRQIaYd",
        "replyto": "eeaKRQIaYd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_zNBk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_zNBk"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the success of unsupervised NMT approaches, this paper proposes USLNet, an unsupervised SL translation and generation approach. USLNet has three main components, namely: text reconstruction module, video reconstruction module and finally cross-modality back translation module. The authors also propose a sliding window based approach to address the alignment issues that are inherent in broadcast SL datasets. The proposed approach is evaluated on BOBSL, however the reported results suggest the proposed approach does not meet the expectation of a translation system (~0.2 BLEU4 score)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "To the best of my knowledge this is the first bi-directional (translation/generation) SL approach that is trained in an unsupervised manner. Although the results are not promising, the proposed method is sound, and further studying the unsupervised training approach might yield promising results."
            },
            "weaknesses": {
                "value": "Although I like the idea of using pretrained large-scale models and unsupervised learning, I'd expect quantitative results to back up the benefits of employing these ideas. Sadly, the presented results does not suggest the presented approach to be \"working\" (~0.2 BLEU-4 score on BOBSL, while the state of the art is above 2 https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Sincan_Is_Context_all_you_Need_Scaling_Neural_Sign_Language_Translation_ICCVW_2023_paper.pdf) \n\nThat being said, the reviewers and the readers should acknowledge how challenging the BOBSL dataset is, and that we still need several breakthroughs to progress in large scale SL translation/generation. \n\nTherefore to strengthen the paper, I'd have considered/expected the following:\n\n(1): Experiment on different datasets, such as Phoenix-2014T, or the larger OpenASL and YoutubeASL, which have more state-of-the-art results, hence more data points to gauge the performance/benefits of the proposed approach.\n\n(2): Frame the approach as a pretraining method, and do a final supervised finetuning step (with varying amounts of data). One would expect the unsupervised pretraining on unaligned data to yield better performance than straightforward supervised translation approach, which would have strengthened the utility of the proposed method. \n\n(3) Having some qualitative results and failure analysis for translation/generation would have helped the paper immensely. Relying solely on b1 and b4 results does not give enough insights to the reader, and possibly is not doing the proposed approach justice. \n\nAs is, I do not think the reviewer/reader has enough signals to evaluate the benefits of the proposed approach, and I'd highly recommend the authors to consider the suggestions mentioned above."
            },
            "questions": {
                "value": "(See Weaknesses Section for Suggestions)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627519560,
        "cdate": 1698627519560,
        "tmdate": 1699637067372,
        "mdate": 1699637067372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jqhaqfYlFp",
        "forum": "eeaKRQIaYd",
        "replyto": "eeaKRQIaYd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_WPdm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_WPdm"
        ],
        "content": {
            "summary": {
                "value": "This paper develops an approach for unsupervised SL translation and generation entirely using from non-parallel datasets. The motivation is that there is not a lot of paired text and sign language video, so the authors leverage ideas in machine translation and multimodal modeling to build better (unsupervised) sign-text representations. \n\nThe approach contains 3 parts: a masked seq2seq text reconstruction module, signing video reconstruction which uses downsampled discrete latent representations (VQ-VAE) with a GPT-style decoder, and back-translation between each modalities to go from text-to-video-to-text and video-to-text-to-video. There is a disconnect in lengths of text and video sequences, so they use a sliding window aligner to map between each. \n\nResults are in some cases better than a supervised baseline on the same dataset and show promise for the approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Developing unsupervised approaches for SL generation/translation is important, especially given the many different representations used for signing. One could imagine fine-tuning this approach for any given representation (e.g., Glosses, HamNoSys). \n* There are reasonable comparisons to supervised approaches.  \n* The ablations /sensitivity analysis comparing this approach with different aspects turned off is interesting. \n* Given the lack of work in this area, it was valuable to see comparisons such as Table 6 on the WMT 2022 sign language translation task"
            },
            "weaknesses": {
                "value": "Overall the results (e.g., Table 1 & 2) are seemingly very poor. This is by no means a reason to reject a paper, but it does in my opinion require the authors to dig deep into 'why' the results are poor and to work towards building an understanding for how they can be improved significantly. It is nice to see that some results are better than the supervised baseline from Albanie et al., but in an absolute sense they are still low. Are there oracle experiments that could be run? How can the problem be made easier to better understand paths towards success?\n\nOne thing that immediately stuck out after going through the appendix is that the visual quality of the SL generations, and likely even the video reconstructions, appear to be too low fidelity to capture important hand or face information. Has there been any experimentation around using different resolution inputs for the video model? Perhaps by doubling or quadrupling the video resolution the model would be able to pick up on more nuance. An alternative approach might be to use key point or whole-body representations (e.g., SMPL) as many recent papers on SL translation have done. \n\nOne limitation of the existing approach is that (if I understand correctly) it exclusively trains on BOBSL. On the text encoder side I could imagine it being valuable to leverage existing LLMs and then fine tune. Perhaps the same could be done on the video side? Although I'm not sure what pertaining model or dataset would be the most effective for signings. \n\nOn page 5, there is a reference to Sutton-Spence & Woll stating that when signers are translating text then signs will tend to follow the English word order. While this may be true for translating text, it's unclear if it is correct for the datasets used in this paper. Have you validated this on your datasets?"
            },
            "questions": {
                "value": "I would like to see responses to some of the line of inquiry in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699039051,
        "cdate": 1698699039051,
        "tmdate": 1699637067218,
        "mdate": 1699637067218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XmltZGTiqf",
        "forum": "eeaKRQIaYd",
        "replyto": "eeaKRQIaYd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_PET1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8533/Reviewer_PET1"
        ],
        "content": {
            "summary": {
                "value": "The model is proposed for cross-modal unsupervised learning. It focuses on unsupervised sign language translation and generation and it learns the task without requiring parallel sign language data. The model consists of four modules: text reconstruction,  video reconstruction, text-video-text translation, and video-text-video reconstruction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The overall writing quality is good although there are some issues.\n\nThe method is unsupervised which is important in the area as it requires experts to annotate. Also, inspired by unsupervised machine translation and applying the idea to another domain is the originality of the method.\n\nThe proposed methods support the writing with detailed formulation and figures."
            },
            "weaknesses": {
                "value": "Discussion about existing text-to-video aligner algorithms is not sufficient. For example, although text2video[1] is a text-based talking face generation model, it uses an aligner for phoneme-to-pose. \n\nIt seems back translations are highly similar to reconstruction loss that is used in image generation, especially in unpaired I2I tasks for cycle consistency. So you might consider elaborating this in the manuscript.\n\nThere are no visual results on the manuscript and limited visual results on the supplementary materials. I think it needs to be more convincing that the model is capable of generating sign language videos with high quality.\n\n\n[1] Zhang, Sibo, Jiahong Yuan, Miao Liao, and Liangjun Zhang. \"Text2video: Text-driven talking-head video synthesis with personalized phoneme-pose dictionary.\" In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2659-2663. IEEE, 2022."
            },
            "questions": {
                "value": "1. The style of equations 10-12 does not fit the manuscript. Authors can consider changing their style to make them consistent with the other equations and the rest of the paper.\n\n2. Why there is no evaluation for the fidelity of the generated videos in terms of well-known metrics such as FID, LPIPS, etc.\n\n3. Why there is no discussion and explanation of the methods proposed in Albanie 2021 in detail as it is the only method that you make a quantitative comparison? I think it needs to be presented more and more importantly the differences and similarities between this and the proposed methods should be highlighted more."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8533/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8533/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8533/Reviewer_PET1"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8533/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731263986,
        "cdate": 1698731263986,
        "tmdate": 1699637067077,
        "mdate": 1699637067077,
        "license": "CC BY 4.0",
        "version": 2
    }
]