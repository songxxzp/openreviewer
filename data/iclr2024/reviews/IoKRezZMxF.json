[
    {
        "id": "Ldq3nRv0um",
        "forum": "IoKRezZMxF",
        "replyto": "IoKRezZMxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission873/Reviewer_NGWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission873/Reviewer_NGWA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for text-driven video-to-video editing, eliminating the need for exhaustive per-video finetuning. Building on the Instruct Pix2Pix image transfer framework, the authors adapt the concept for videos, using a synthetic paired video dataset. They also introduce the Long Video Sampling Correction for consistency across longer video batches. Impressively, this approach outperforms existing techniques like Tune-A-Video, marking some advancements in the domain and opening doors for future research and application."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper crates a synthetic dataset for training instruction-based video-to-video synthesis models. This is good and could potentially benefit the community.\n\n2. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The paper is good at representation but some of the information is confusing. For example, in Table 1, the author claims that all baseline methods need fine-tuning and the proposed method does not need any fine-tuning. However, the proposed method also needs extra training and the cost of the dataset creation is also not reflected.\n\n2. The paper's technical contribution seems to be incremental. The proposed long video sampling strategy seems to ve pretty similar to the sliding window operation but stated in a more formal way."
            },
            "questions": {
                "value": "Will the dataset be released?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Reviewer_NGWA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission873/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736838134,
        "cdate": 1698736838134,
        "tmdate": 1699636013840,
        "mdate": 1699636013840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BXU4Afgzrm",
        "forum": "IoKRezZMxF",
        "replyto": "IoKRezZMxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission873/Reviewer_BaKN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission873/Reviewer_BaKN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for text-based video-to-video editing that eliminates the need for resource-heavy finetuning for each video and model. The authors introduce a synthetic paired video dataset tailored for video-to-video transfer tasks, taking inspiration from image transfer methods such as Instruct Pix2Pix. This method translates the Prompt-to-Prompt model to videos and efficiently generates paired samples, each consisting of an input video and its edited counterpart. They also propose Long Video Sampling Correction (LVSC), ensuring consistent long videos across batches. Their method outperforms existing methods like Tune-A-Video in terms of text-based video-to-video editing, paving new avenues for exploration and deployment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method eliminates the need for per-video-per-model finetuning, potentially saving significant computational resources. The creation of a synthetic paired video dataset tailored for video-to-video transfer tasks is a novel approach that could prove beneficial for training models in this domain. The introduction of LVSC addresses the challenge of maintaining consistency in long videos across batches, a notable improvement over existing methods.\n2. Sufficient and comprehensive experiments (both quantitatively and qualitatively) on the comparisons are given with prior arts and ablations of key designs. The given method gives notable quantitative improvements and its visual results faithfully follow the given instructions compared with other approaches from the supp."
            },
            "weaknesses": {
                "value": "1. The generated video shows visual appeals in the given content with different styles while presenting severe jitter in the newly added content.\n2. The performance of the proposed method relies on the synthetic paired video dataset. Though it gives reasonable sampling strategies on the generated data, if this dataset doesn't closely match real-world scenarios, it may limit the model's utility.\n3. The paper does not talk about potential failure cases or limitations of their approach in the main paper, which could help us better understand the proposed system."
            },
            "questions": {
                "value": "1. It would be better to quantify the difference between the generated video dataset and some reference one, e.g., computing FID between them. It may also be helpful to validate the effectiveness of the given sampling criteria.\n2. How compatible is this approach with different types of video content, various editing instructions, and text-to-video generation methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission873/Reviewer_BaKN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission873/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786656594,
        "cdate": 1698786656594,
        "tmdate": 1700737363345,
        "mdate": 1700737363345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YrDOuQb3W7",
        "forum": "IoKRezZMxF",
        "replyto": "IoKRezZMxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission873/Reviewer_99Pg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission873/Reviewer_99Pg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an efficient text-based video-editing method by adapting Instruct Pix2Pix from image to video editing, eliminating the need for additional training. To enable long term video editing, a Long Video Sampling Strategy is proposed to maintain long video consistency. Experimental comparisons with other methods demonstrate the advantages of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow\n2. The paper proposed a very intersting idea for universal one-model-all-video transfer idea for vid-to-vid transfer\n3. It proposed a novel synthetic dataset fo vid-to-vid transfer task.\n4. The experiments were well conducted, showcasing detailed numerical indicators and a user study to evaluate the video editing method."
            },
            "weaknesses": {
                "value": "1. The proposed method in this paper lacks significant innovation. The majority of the content is derived from Instruct Pix2Pix by adapting image editing to video editing without much improvement. \n2. The proposed sampling method to maintain long video consistency is a variation of inpainting sampling methods, which is also widely used in image/video generation tasks. The experimental section provides detailed numerical metrics for various evaluation indicators and user study. However, the compared baseline lacks strength in video editing tasks, there already are some video editing models based on image diffusion models, such as Pix2Video[1],  Render A Video [2], TokenFlow[3], which also do not require fine-tuning on a single video. The proposed method in this paper did not compare itself with those models mentioned. \n3. From the generated video results in the provided supplementary, it seems that the proposed method does not achieve superior results.\n\n [1] Pix2Video: Video Editing using Image Diffusion\n\n [2] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation\n\n [3] TokenFlow: Consistent Diffusion Features for Consistent Video Editing"
            },
            "questions": {
                "value": "1. In Section 3.1, the authors mentioned that the temporal attention layers are also replaced to adapt to video editing tasks. However, in the showcased results, most videos are style transferred frame by frame. Is there any attempt to simultaneously change the style of the video and modify the motion of the video, such as transforming a person walking towards the left to walking towards the right?\n2. In Section 3.2, how is the success rate calculated? Are there any other methods to improve the success rate?\n3. Does using video diffusion model based methods have any advantages over using optical flow in image diffusion models in video editing tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission873/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861795440,
        "cdate": 1698861795440,
        "tmdate": 1699636013654,
        "mdate": 1699636013654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oimHZdHbQu",
        "forum": "IoKRezZMxF",
        "replyto": "IoKRezZMxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission873/Reviewer_3EH3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission873/Reviewer_3EH3"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of text-based video editing. The proposed method, namely InsV2V, is an extension of Instruct Pix2Pix to the domain of videos. InsV2V follows the same paradigm of first generating synthetic data containing videos before and after the editing, as well as the correspoinding text. These data can then be used for training the video editing model.\nThe synthetic data is generated using an off-the-shelf text-to-video model. To begin with, the text prompts are obtained from existing datasets and the instructions are generated using a pretrained LLM and in-context learning. Then the source videos are generated using the text-to-video model and the target videos are generated using Prompt-to-Prompt technique. Finally, these generated video and text pairs are filtered using CLIP scores.\nAfter the data is obtained, an video-to-video model is constructed based on a pretrained image-to-image LDM, and partially finetuned on the synthetic data. \nTo allow the generation of videos longer than the training data, the video-to-video model is conditioned on a few previous frames. Additionally, a score correction term based on optical flow is added to improve temporal consistency across consecutive batches of the same video."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Text-based video editing is a difficult problem, yet the paper has managed to achieve.\n* The evaluation of the method is very comprehensive -- it has included both automatic metrics as well as user studies, demonstrating state-of-the-art performance.\n* The novel components proposed in the paper, namely long video score correction (LVSC) and motion compensation (MC) has significantly improved the consistency of the generated videos, as illustrated in Table 2 as well as in the supplementary material."
            },
            "weaknesses": {
                "value": "* The techniques used in the paper are not completely new -- a large portion of them has followed Instruct-Pix2Pix. \n* Even with all the measures in place, the generated videos are still far from being temporally consistent."
            },
            "questions": {
                "value": "* Which LLM did you use for the in-context learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission873/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699260689953,
        "cdate": 1699260689953,
        "tmdate": 1699636013592,
        "mdate": 1699636013592,
        "license": "CC BY 4.0",
        "version": 2
    }
]