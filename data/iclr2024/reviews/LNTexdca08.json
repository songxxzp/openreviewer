[
    {
        "id": "k17FGt1H6U",
        "forum": "LNTexdca08",
        "replyto": "LNTexdca08",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
        ],
        "content": {
            "summary": {
                "value": "This paper concentrates on the task of point supervision and introduces a novel approach named  P2P that transforms point supervision into accurate pseudo-labels. The method harnesses the potential of the visual foundation model, SAM, and introduces an iterative framework for the generation of these pseudo-labels. This framework comprises two stages: an SEPG stage, which translates the point annotations into visual prompts, and a PGSR stage for converting these visual prompts into pseudo masks and bounding boxes. Experimental results show that the proposed approach achieves SOTA performance on COCO and PaSCAL datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper introduces a novel method to generate pseudo-labels from point supervision that leverages the potential with a visual foundation model.\n- The paper is well-written and experiments demonstrate superior qualitative results  compared to baselines."
            },
            "weaknesses": {
                "value": "- The performance of the model largely depends on the capability of SAM itself. From the quantitative results, the proposed approach does not have a significant improvement compared with directly using SAM.\n- The method seems to be computationally demanding since the training process is conducted with 4*RTX 4090 GPUs."
            },
            "questions": {
                "value": "Could the author provide a comparison of training time and memory cost compared with other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698131526236,
        "cdate": 1698131526236,
        "tmdate": 1699636085316,
        "mdate": 1699636085316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W8vYHTybyp",
        "forum": "LNTexdca08",
        "replyto": "LNTexdca08",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_j4vZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_j4vZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel iterative learning framework, Point to Prompt (P2P), for point-supervised object detection and segmentation. The P2P is formulated as an iterative refinement process of two stages: Semantic Explicit Prompt Generation (SEPG) and Prompt Guided Spatial Refinement (PGSR). Experiments on multiple datasets are performed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method has a superior performance on multiple datasets.\n- The motivation is resonable and easy to understand."
            },
            "weaknesses": {
                "value": "- The authors claim that the existing methods aim to release the annotation burden while still achieving decent performance. However, the proposed method seems still far behind fully-supervised method.\n- It is better to give an inference time comparison, which can help readers better understand the propose method.\n- SAM has a good ability to generate the mask of objects based on point. Thus, it may give a good performance using SAM and Cascade structure, which is similar to PGSR."
            },
            "questions": {
                "value": "please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698495876518,
        "cdate": 1698495876518,
        "tmdate": 1699636085236,
        "mdate": 1699636085236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4h1ppLJ38Z",
        "forum": "LNTexdca08",
        "replyto": "LNTexdca08",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
        ],
        "content": {
            "summary": {
                "value": "This paper innovatively introduces the Point to Prompt task, transforming point-label inputs into visual prompt learning, and leveraging a foundation model (SAM). It utilizes an iterative refinement process to obtain high-quality prompt to complete object detection and semantic segmentation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This method simultaneously accomplishes segmentation and horizontal object detection tasks through point labels, demonstrating a certain level of versatility.\n- This paper employs Exponential Moving Average (EMA) to update the prototypes of the corresponding categories. This approach is dynamic and remains unaffected by the inherent instability of Multi Instance Learning.\n- Novelty: this paper introduces a novel approach by incorporating visual foundation model into a point-supervised task while serving two downstream tasks simultaneously. Furthermore, the proposed prototype representation updated by Exponential Moving Average (EMA) within the Multiple Instance Learning (MIL) framework can enhance the quality of the selected proposals. \n- Writing: The paper exhibits a generally smooth logical flow and suitable symbol usage."
            },
            "weaknesses": {
                "value": "- Novelty: Some parts of the proposed method lack novelty, particularly in the proposed Prompt Refiner II, phrases like \"similar to previous works\" and \"follow the common practice\" resemble the refinement stage of P2BNet [1]. \n- Writing: Certain parts of the language are not as concise and straightforward as desired. Additionally, there is a slight delay in providing specific explanations for some concepts, such as \"proposal seeds\" and \"selected high-quality embedding features,\". Furthermore, Section 3.4 should be repositioned closer to the beginning of the paper to provide readers with a clearer understanding of the overall method's workflow.\n\n[1] Chen, Pengfei, et al. \"Point-to-box network for accurate object detection via single point supervision.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
            },
            "questions": {
                "value": "- In Section 3.1, the paper mentions that \u201cthe core of this task lies in designing an accurate point-to-box regressor\u201d, and introduce the P2P framework (point-to-visual prompt), but it is inconsistent between \u201cpoint-to-box\u201d and \u201cpoint-to-visual prompt\u201d, it is suggested to add the description of process of visual prompt-to-box/mask after.\n- What\u2019s the meaning of the orange arrow in the lower part of the Figure 2? (i.e., the arrow on the right of \u2018Refine box b*\u2019).\n- The concept \u2018proposal seeds\u2019 is proposed in Section 3.1, but its specific generation way (i.e., outer rectangle of the mask generated by SAM) is given until Section 3.4, it is suggested to explain its specific concept early on to avoid confusion.\n- What\u2019s the concrete content of \u2018sharing some of the weights\u2019 about the two refiners mentioned in Section 3.2? \n- What are the key differences in the workflow of the proposed Refiner2 compared to the refining stage in previous works [1]?\n- How about the computational complexity of the proposed method compared to other methods?\n- In Section 3.4, the actual input of the subsequent P2P refiner is the initial proposal seed box (i.e., outer rectangle of the mask generated by SAM). If possible, it is recommended to use the box (e.g., the fully supervised bounding box label) as the prompt of SAM to conduct a more comprehensive experimental comparison in segmentation, rather than just using the point as the prompt of SAM for comparison.\n\n[1] Chen, Pengfei, et al. \"Point-to-box network for accurate object detection via single point supervision.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591783724,
        "cdate": 1698591783724,
        "tmdate": 1699636085153,
        "mdate": 1699636085153,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6lIrer1Mog",
        "forum": "LNTexdca08",
        "replyto": "LNTexdca08",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
        ],
        "content": {
            "summary": {
                "value": "The paper introduce foundation model SAM into point supervised object detection and segmentation task. Benefit from the high generalized ability of SAM, the object segmentation map can be easier obtained with the point prompt. However, the mask provided by SAM are not what we need sometimes because SAM is semantic-free and brings ambiguity. The author proposed a method to generate better-quality segmentation map as the pseudo label for training of detection segmentation task. I support for what the author said: \u2018rather than directly designing large foundation models, it is more meaningful to leverage them for specific tasks in resource-constrained situations.\u2019"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I support for the combination of foundation model and specific tasks and interested in the point-supervised tasks.\nThe proposed method bridges the gap between PSOD (or PSIS) and fully-supervised method."
            },
            "weaknesses": {
                "value": "There are some questions here:\n1\u3001The author said \u2018we observe that only 40% of the masks covered more than 70% foreground pixels\u2019. However, as I know, because the semantic-free of SAM, the highest score mask generated by SAM may not what we what, but the top-3 masks may contains what we want in most situation. What we should do is to select the best one (or with extra refinement) from the top-3 masks. But the paper only choose the highest score one and claimed \u2018only 40% of the masks covered more than 70% foreground pixels\u2019, I think this is a handmade problem. In other words, the author do not make full use of foundation model and make the problem more difficult.\n2\u3001I think the framework is lack of novelty and a little engineering. If I understand you correctly\uff0c the structure is (SAM+P2BNet(CBP stage + PBR stage))+(SAM+ P2BNet)+ (SAM+ P2BNet) ....... The whole paragraph of \u2018SEMANTIC-EXPLICIT PROMPT GENERATION\u2019 is P2BNet (the network, the loss, the sampling is similar) and the \u2018PROMPT GUIDED SPATIAL REFINEMENT\u2019 is the application of SAM. I think the combination of P2BNet and SAM is OK, but the author did not propose some insights or other challenges in combination. And the ITERATIVE LEARNING is a little engineering, I am interested in the time cost in practical application.\n3\u3001Some other problems: (1) The visualization is unclear if I do not enlarge the image or I print it. The line is too thin. (2) And I think the title is not suitable because another paper is named as P2P: Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework, ICCV2021. The tasks are different but relevant, and are all point-based. (3) I think the experiments on better detectors or segmentation network (or better backbone?) are needed. The faster RCNN is classic but too old. The fully-supervised method is far beyond 30+ or 40+ on COCO."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG",
                    "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769183561,
        "cdate": 1698769183561,
        "tmdate": 1700580257585,
        "mdate": 1700580257585,
        "license": "CC BY 4.0",
        "version": 2
    }
]