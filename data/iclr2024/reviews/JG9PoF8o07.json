[
    {
        "id": "53BoGISMHq",
        "forum": "JG9PoF8o07",
        "replyto": "JG9PoF8o07",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_qfzX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_qfzX"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the Generalized Gaussian mechanism in Differential Privacy, as extention of typical Laplace and Gaussian noise additions. It proves that the GG family satisfies DP and adapts an existing privacy accounting tool, the PRV accountant, for this purpose. Applying the GG mechanism to PATE and DP-SGD machine learning tools, the authors claim that the Gaussian distribution often optimally balances privacy and accuracy."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The flow of this paper is very easy to follow."
            },
            "weaknesses": {
                "value": "The exploration of alternative distributions for DP in this paper does not offer new insights. Previous research, such as the work by Awan and Dong (2022), already provides a comprehensive study on log-concave and multivariate canonical noise distributions in DP. These studies not only predate but also surpass the current paper's approach by offering tightly derived privacy profiles instead of relying on the numerical experiment method adopted here.\n\nThe paper's \"Privacy Accounting for GG Mechanisms\" relies heavily on numerical experiments without robust error control or rigorous proofs. This methodology is particularly concerning since the privacy profile of this noise family is already well-established in the literature. The paper's focus on a single privacy budget (\\(\\delta=10^{-5}\\)) is both arbitrary and unconvincing. For instance, releasing a \\(10^{-5}\\) portion of the total data satisfies \\(\\delta=10^{-5}\\) with \\(\\epsilon=0\\), making it theoretically superior to all other mechanisms discussed in terms of this metric.\n\nThe experiments (PATE, DP-SGD) conducted under the privacy budget of \\(\\delta=10^{-5}\\) are too limited to convincingly demonstrate the superiority of any specific \\(\\beta\\) values. At this \\(\\delta\\) setting, the noise variance added by the mechanisms varies widely, which is very likely to be the true reason behind the differing outcomes. \n\nFor example. for a fixed \\(\\epsilon\\), the Gaussian mechanism will perform poorly for very small \\(\\delta\\) values as the required variancediverge rapidly. In contrast, for the Laplace mechanism, the scale of the Laplace noise remains approximately unchanged. Conversely, when considering Gaussian Differential Privacy (GDP) as the privacy budget, the Gaussian mechanism generally outperforms the Laplace mechanism. There is no intrinsic advantage or disadvantage for either of these two algorithms; their efficacy largely depends on the specific form of the privacy constraint. The statement \"This provides a justification for the widespread adoption of the Gaussian mechanism in DP learning\" seems coincidental within the scope of this research approach. The preference for the Gaussian mechanism may be more attributed to its strong alignment with GDP accounting, rather than any inherent superiority deduced from the methods used in this research.\n\nIt is incorrect (vastly loose) to compute composition privacy budget from a single pair of epsilon and delta (think about composition of Gaussian mechanisms).\n\n\nIn Section B.2, titled \"Mechanisms with Equivalent Privacy Guarantees,\" the selection of \\(\\sigma\\) is derived through random search, not through analytical computation. This approach does not guarantee the avoidance of numerical stability issues (which is very likely to happen for very small $\\delta$ when using the PRV accountant's random search to determine \\(\\epsilon\\). The method should at least be executed using well-established mechanisms (Gaussian and Laplace) to validate the outputs against their analytically true values.\n\nSome citations in this paper can be improved to be more relevant. For example, (Kairouz, P., Oh, S., & Viswanath, P. (2015)) as optimal compostion for epsilon,delta DP is more suitable than (Abadi et al., 2016) which is not directly related to the topic after definition 4.\n\nThere are some confusions in the proof and presentation (see questions).\nFor example:\nHolder\u2019s Inequality is listed as a lemma\nPage 19: \"As x goes to infinity, the power of the exponent is negative\". It might be better to say \" ... is negative for sufficiently large x in terms of ...\"\nAlso see questions\n\n\n\n\nThere is some broken link (for example, at the beginning of C.6).\n\nKairouz, P., Oh, S., & Viswanath, P. (2015). The composition theorem for differential privacy. In International conference on machine learning (pp. 1376-1385). PMLR.\n\nAwan, J., & Dong, J. (2022). Log-concave and multivariate canonical noise distributions for differential privacy. Advances in Neural Information Processing Systems, 35, 34229-34240."
            },
            "questions": {
                "value": "What is the purpose for Figure 11? What are the pattens to be shown here?\n\nOn page 23 what does \"bound the probability of not being (\u03f5, 0)-DP\" mean? Being DP or not is an intrinsic property of an algorithm, where does the randomness come from?\n\nWhat is a \"'single composition' of the GG mechanism\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3644/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621415091,
        "cdate": 1698621415091,
        "tmdate": 1699636320336,
        "mdate": 1699636320336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PPvLyuKNAz",
        "forum": "JG9PoF8o07",
        "replyto": "JG9PoF8o07",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_QnfG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_QnfG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the generalized Gaussian mechanism for private machine learning, especially DP-SGD and PATE. The generalized Gaussian mechanism is a mechanism utilizing the generalized Gaussian noise that encompasses Laplace, Gaussian, and arbitrary noise distribution associated with the density function proportional to $e^{-\\frac{|x-\\mu|^\\beta}{\\sigma}}$. The paper investigates the optimal $\\beta$ in the generalized Gaussian in terms of utility-privacy trade-off in DP-SGD and PATE. To this end, the authors proposed $\\beta$-DP-SGD and GGNMax algorithms which are variants of DP-SGD and LNMax by replacing the noise distribution with the generalized Gaussian. The authors show empirically that choosing $\\beta=2$ is near-optimal in test accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is well-written and well-organized. I found that investigating the optimal parameter $\\beta$ of the generalized Gaussian mechanism for private ML is interesting and important in ML society. I found that $\\beta=2$ is near-optimal is very interesting, and I think it opens other research directions."
            },
            "weaknesses": {
                "value": "Generally, I do not see much contribution in this paper. Several things I am concerned about are listed:\n- The proposed mechanisms ($\\beta$-DP-SGD and GGNMax) are a straightforward generalization of the existing mechanisms by replacing noise distribution. \n- The analysis using the PRV accountant is not novel.\n- I felt that there are not many analytical results in the generalized Gaussian mechanisms. Most of the results are empirical findings, and I doubt that the experiments are sufficient to claim the findings."
            },
            "questions": {
                "value": "Main questions:\n1) Is Theorem 1 saying there exists $\\epsilon$ and $\\delta$ for $(\\epsilon,\\delta)$-DP? Isn't it obvious that every mechanism has $\\epsilon=1$ and $\\delta=1$ if I am not mistaken?\n2) For \"However, unlike the results ... larger than $\\beta>3$\" on page 8, is there any reason why this happens?\n3) If I am not mistaken, the comparisons regarding $\\beta$ are done after hyper-parameter optimization. If they are, I have some questions:\n- Is it reasonable to perform the comparison by fixing hyperparameters that are optimal in non-DP training? \n- For fixed hyperparameters, $\\beta=2$ is still near-optimal? or is it dependent on hyperparameters?\n\nSome minor questions are as follows:\n1) In the caption of Figure 4, it is written that the reported three epsilons are based on the minimum epsilon giving $0.98$ accuracy. Why then the accuracy is much worse than $0.98$ in the plots even for $\\epsilon^\\prime$?\n2) Renyi DP is defined in the main paper but is not used as a main part after that. Is there any reason for defining it in the main paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Reviewer_QnfG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3644/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703759740,
        "cdate": 1698703759740,
        "tmdate": 1699636320227,
        "mdate": 1699636320227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "try5J22OEa",
        "forum": "JG9PoF8o07",
        "replyto": "JG9PoF8o07",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_7vy3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_7vy3"
        ],
        "content": {
            "summary": {
                "value": "This paper takes a deeper look at the generalized Gaussian mechanism.\nThe first contribution is to extend the privacy proof of the GGM to work with the PRV accountant to allow tighter analysis than previous bounds. \nThey then plug the mechanism into various ML applications and observe the effect of varying beta on accuracy.\nIn general, they find support for using the Gaussian mechanism, but also that more fine-tuned versions of beta (fractional values) can lead to slight improvements in accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Even though the results didn't yield significant changes while varying beta, I see great value in a study that supports the approximate optimality of the Gaussian mechanism. In general, there are so many hyperparameters to tune, so giving practitioners support in using the Gaussian mechanism as a default is useful.\n- Very well-written paper with clear descriptions."
            },
            "weaknesses": {
                "value": "- Claiming STOA with three runs: The results on machine learning had quite a high variance due to limited runs. I think claiming STOA when the accuracy is better by such a small amount is not significant. Perhaps more runs and a hypothesis test/confidence interval would give more definitive results (although STOA is not the primary goal of the work).\n\n- One more hyperparameter: the very small gains in accuracy would surely be outweighed by the cost of tuning beta in practice. I see the main contribution of this work is showing that the beta doesn't have too much effect rather than improving the accuracy of current mechanisms. \n\n### Typos\n- End of Section 3.2, last paragraph \"change subsequently change\".\n- A.1 Proof of theorem 4 has a missing ref.\n\n### Note\nI did not verify the proofs in this paper as I do not have sufficient theoretical background. I apologize that I have to leave this task to my fellow reviewers."
            },
            "questions": {
                "value": "- What would be the solution to fix the artifacts in Figure 3? Perhaps more values in the grid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3644/Reviewer_7vy3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3644/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772999149,
        "cdate": 1698772999149,
        "tmdate": 1699636320135,
        "mdate": 1699636320135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2CsRVJT5H4",
        "forum": "JG9PoF8o07",
        "replyto": "JG9PoF8o07",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_aH3Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3644/Reviewer_aH3Q"
        ],
        "content": {
            "summary": {
                "value": "In differential privacy literature, most papers focus on either Laplace or Gaussian Mechanism due to the simplicity of analysis and proven effectiveness in practice. This paper studies the Generalized Gaussian Mechanism instead. It shows that the mechanism is differentially private and through experiments on common benchmarks, it also shows the effectiveness of the mechanism on deep learning tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper provides some empirical evidence on why Gaussian Mechanism is popular. In their experiments, the model usually achieves the best accuracy when the noise added is roughly Gaussian.\n\n- The Generalized Gaussian Mechanism can be useful for tasks that require very high accuracy. By tuning the $\\beta$ carefully, it's possible that the model can achieve better results (as shown by some improvements on cifar10 tasks).\n\n- The paper is well-written overall."
            },
            "weaknesses": {
                "value": "- The experiments are conducted on fairly small datasets and deep learning architectures so it's hard to get a grasp of how well the generalized gaussian mechanism works. \n\n- While the study is fairly interesting, I'm not sure if the technical contribution is enough. The main takeaway is Gaussian and Laplace are pretty much the best choices as expected. Generalized Gaussian Mechanism is fairly hard to analyze since it doesn't have a closed-form relationship between $\\beta$ and $\\epsilon, \\delta$. The experiment also requires the tuning of $\\beta$ which in the end ends up being Gaussian and Laplace mechanism anyway."
            },
            "questions": {
                "value": "- In page 18, I think the RHS should be $(\\frac{\\alpha-1}{\\alpha})^{1/\\beta}$?\n\n- Also as $x<0$, shouldn't $\\frac{|x|}{|x-\\mu|}$ be $\\frac{-x}{\\mu-x}$ instead? Fortunately, I don't think it affects the argument."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3644/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786810369,
        "cdate": 1698786810369,
        "tmdate": 1699636320055,
        "mdate": 1699636320055,
        "license": "CC BY 4.0",
        "version": 2
    }
]