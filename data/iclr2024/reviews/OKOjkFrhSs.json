[
    {
        "id": "cvWi5D41Tk",
        "forum": "OKOjkFrhSs",
        "replyto": "OKOjkFrhSs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_NmaN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_NmaN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a prompt-guided dynamic network which introduces the text embedding to existing SR framework. The key component is Dynamic Correlation Module which has a spatially multi-modal attention module and a prompt-guided dynamic convolution module. The work is the first to introduce the text into convolution kernel estimation for feature transformation. The comprehensive analyses of effectiveness of proposed modules are demonstrated."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduce the text into convolution kernel estimation for feature transformation which is interesting and novel. \n2. The paper is easy to follow.\n3. The performance is much improved than the SOTA method.\n4. The ablation study on the prompt and DCM is solid."
            },
            "weaknesses": {
                "value": "The only concern is unfair comparison. Since the most of the baselines are trained on the DIV2K which has less images than the training data used in this work. I can not find the claim that the baseline models are trained on the same dataset from the scratch. This may make the paper less convincing."
            },
            "questions": {
                "value": "See weakness. \n\nHow does it perform if the prompt is fixed as \"high detailed image\"? For most cases of SR, it is hard to describe the image using text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Reviewer_NmaN"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795916007,
        "cdate": 1698795916007,
        "tmdate": 1700696831260,
        "mdate": 1700696831260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GFO2lnaSAM",
        "forum": "OKOjkFrhSs",
        "replyto": "OKOjkFrhSs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_axMK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_axMK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use text driven prompts to render diversification in image superresolution workflow. The primary claim is that existing single image supreres methods learn the convolution kernel from single image modality. By introducing coherent text based prompts by way of CLIP latents, this bottleneck can be removed to a large extent. The authors propose a spatially multi-modal attention module and prompt guided DCM. The attention module first computes an attention mask between the prompt and the image features and then weights the image features based on the attention weights. The prompt embedding is further utilized to learn N sets of weights \\pi to be used as combination weights for dynamic convolution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea is interesting and can be exploited more by incorporating the multi-modal prompts into many CV pipelines. For images with captions this might be a great way to include captions in the superres pipeline. \n\nOriginality: The development seems original enough.\nQuality: The technical development of the material can be improved from its current state. \nClarity: The Prompt Guided DCM module section is just 6 lines long, where it is one of the two claims of this paper! I strongly feel that this part needs to be written better.\nSignificance: The paper has been designed as an application paper. I believe the underlying idea can be used for many different applications."
            },
            "weaknesses": {
                "value": "The primary problem I have with the paper is the explanation for the prompt guided technique itself. Starting from Fig 2. which states a) and b) but does not show two subfigures. The equations for the prompt being used in subsequent convolution modules are mentioned in Fig. 2 but not in the main text. It is almost assumed that reviewers know the contents of the Dynamic Convolution paper [Yang et al. 2019] and hence, the authors did not spend any time to develop the concept independently. \n\nFor the comparative methods, the authors claim that they add this module within existing workflows. They add one additional block in one method and three additional blocks in another. What is the basis of these choices. What does it do the parameter count for these methods and how does that compare to just increasing the number of layers in these methods?\n\nFor the image datasets which do not have captions, the authors propose to use CLIP image features for the horizontally flipped image. Again, no explanation of these choices are provided. Providing a flipped image to other SR techniques and then somehow integrating the results from the two outputs might be an interesting study for existing methods as well."
            },
            "questions": {
                "value": "I would request the authors to first explain the method and all the mathematical steps involved fully to make it a paper which stands on its own. \n\nThe attention visualization in Fig. 5 are much better than Fig. 4. What is the difference between them?\n\nSome questions are in the previous section and authors can chose to answer them together."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802755729,
        "cdate": 1698802755729,
        "tmdate": 1699636154531,
        "mdate": 1699636154531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yHbUH3qnNo",
        "forum": "OKOjkFrhSs",
        "replyto": "OKOjkFrhSs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_WtP4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_WtP4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach that leverages multi-modal cues, such as text or additional images, to enhance the capabilities of existing super-resolution networks. It introduces a Dynamic Correlation Module in a plug-and-play format for existing super-resolution networks and a Spatially Multi-Modal Attention Module to create pixel-wise cross-modal attention masks that emphasize regions of interest based on specific cues. The evaluation results conducted on 4 benchmarks (ie, Set5, Set14, Urban100, and Celeba-HQ) seems to validate the efficacy of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well organized and easy to follow.\n+ It is interesting to incorporate the prompt text as a guidance to improve the super-resolution recovery.\n+ The presented results well support the claims in the main paper."
            },
            "weaknesses": {
                "value": "- Technically the novelty is very limited because the incorporate the prompt text as a guidance in very incremental. \n- The CNN backbone is a little out of date. I would like to suggest the authors apply the similar idea to the SOTA SISR methods with both Transformers and diffusion models. \n- All the competing baselines in Section 4.2 are not SOTA methods any more, as they were published at least 3 years ago.\n- The references are obviously inadequate, as no 2023 papers are included and cited in the paper. The authors should cite the following papers:\n1. Yuanbiao Gou, et al. Rethinking Image Super Resolution from Long-Tailed Distribution Learning Perspective. CVPR 2023.\n2. Sicheng Gao, et al. Implicit Diffusion Models for Continuous Super-Resolution. CVPR 2023.\n3. Yinhuai Wang, et al. GAN Prior Based Null-Space Learning for Consistent Super-resolution. AAAI 2023.\n4. Bin Sun, et al. Hybrid Pixel-Unshuffled Network for Lightweight Image Super-resolution. AAAI 2023."
            },
            "questions": {
                "value": "- Why are the quantitative results of EDSR in Table 3 and Table 4 not consistent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810985091,
        "cdate": 1698810985091,
        "tmdate": 1699636154451,
        "mdate": 1699636154451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uyUFlzygt1",
        "forum": "OKOjkFrhSs",
        "replyto": "OKOjkFrhSs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_FNqG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2204/Reviewer_FNqG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompt-guided dynamic network (PDN) and dynamic correlation module (DCM) for single image super-resolution (SISR). The key contributions are:\n\n- PDN introduces powerful multi-modal representations like text descriptions or similar images into existing SISR frameworks through the DCM module. This allows the model to learn more meaningful semantic information from the prompts.\n\n- DCM contains two main components: a spatially multi-modal attention module and a prompt-guided dynamic convolution module. The attention module highlights image regions relevant to the prompts. The dynamic convolution uses the prompts to generate convolutional kernels, enabling better modeling of cross-modal coherence and spatial variations. \n\n- DCM can be conveniently incorporated into various SISR networks. Experiments show DCM improves performance over state-of-the-art methods on benchmark datasets, especially for larger scale factors.\n\n- To the best of the authors' knowledge, this is the first work to introduce multi-modal prompts for convolutional kernel estimation in SISR. \n\nIn summary, the paper proposes a novel way to leverage multi-modal prompts to boost SISR performance through a flexible DCM module that can be plugged into existing networks. Key innovations are the cross-modal attention and prompt-guided dynamic convolutions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**\n- The specific techniques for integrating prompts are novel, including cross-modal attention and prompt-guided dynamic convolutions. Outperforms the related TGSR paper.\n\n**Quality**\n- Technically sound approach with extensive experiments that validate the quantitative improvements.\n\n**Clarity**\n- Excellent writing quality and clear presentation of the proposed method.\n\n**Significance**  \n- Addresses the important problem of improving generalization in super-resolution.\n- Shows the benefits of leveraging multi-modal information for this task.\n- Could inspire further research in using prompts."
            },
            "weaknesses": {
                "value": "The high-level idea of using multi-modal prompts to improve super-resolution generalization is not entirely novel, as the TGSR paper proposed this first.\n\nWhile the specific techniques in this paper differ from TGSR, the overall framework is quite similar conceptually (leveraging prompts to aid super-resolution). Compared to TGSR, the innovations here seem more incremental - attention modules, dynamic convolutions, etc. Rather than proposing an entirely new overall architecture. The extent to which these specific contributions generalize may be limited.\n\nMore analysis could be provided on how the approach differs from and improves upon TGSR technically. The conceptual similarity should be addressed more directly.\n\nAdditionally, like most learning-based SR methods, the reliance on synthetic training data means real-world robustness is unclear. Evaluation of realistic degraded images could better validate effectiveness.\n\nIn summary, the high-level novelty is diminished by the prior TGSR work. More analysis comparing TGSR technically and conceptually could strengthen the paper. Real-world evaluations could provide further evidence of the robustness and generalization abilities."
            },
            "questions": {
                "value": "Here are some questions and suggestions for the authors:\n\n- The TGSR paper proposed using multi-modal prompts for super-resolution first. Could you more clearly explain how your approach technically differs from and improves upon TGSR? Some more analysis comparing your method to TGSR may be helpful.\n\n- The overall framework of using prompts to aid super-resolution seems conceptually quite similar to TGSR. Do you view your innovations as more incremental improvements in architecture, or is there a fundamental difference in how prompts are leveraged that should be clarified?\n\n- Like most learning-based SR methods, you rely on synthetic training data. How confident are you that the approach will be robust to real-world degradations? Evaluating real degraded images could help validate this.\n\n- Have you considered applying the approach to other image restoration tasks beyond super-resolution, to demonstrate generalization?\n\n- Is the performance sensitive to the choice of prompts? How robust is it to unrelated or poor prompts? More analysis here could help.\n\n- Are there limitations to the architectures you proposed for integrating prompts? Could any negative impacts result from the attention modules or dynamic convolutions?\n\n- Could you provide more details on the training methodology? Some hyperparameters and training details are missing.\n\nOverall, addressing the conceptual similarity to TGSR, evaluating real degradations, and analyzing the generalization abilities could help strengthen the paper. I look forward to the author's response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2204/Reviewer_FNqG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698997297710,
        "cdate": 1698997297710,
        "tmdate": 1700726109786,
        "mdate": 1700726109786,
        "license": "CC BY 4.0",
        "version": 2
    }
]