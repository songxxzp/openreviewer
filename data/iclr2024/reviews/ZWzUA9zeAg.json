[
    {
        "id": "qCWlPODEya",
        "forum": "ZWzUA9zeAg",
        "replyto": "ZWzUA9zeAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed DA-Fusion, which uses an image-to-image diffusion model to generate new synthetic images to assist classification tasks. DA-Fusion utilizes Textual Inversion to learn the word embeddings of unseen concepts and employs the data balancing and random intensity trick to improve the augmentation results further. The empirical shows the effectiveness of DA-Fusion in the low-shot setting on common concepts, fine-grained concepts, and rare concepts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper shows positive empirical results on several classification tasks covering common, fine-grained, and rare concepts.\n\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "(1) The technical contribution is limited. DA-Fusion combines several existing methods, like the image-to-image diffusion model, the Textual Inversion, and the data balancing technique.\n\n(2) DA-Fusion outperforms the much simpler RandAugment method mostly on extremely low-shot settings (less than 16 images per class). The improvements of DA-Fusion seem to be marginal when there are more than 16 images per class. Also, it is unfair to compare with RandAugment, which only uses the default hyperparameters, while DA-Fusion is \u201cfine-tuned\u201d on the target data (i.e., textual inversion, selecting $M$, and other parameters). The authors should search for the optimal number of operations and augmentation magnitude for RandAugment for a fairer comparison.\n\n(3) The authors tested their method on COCO and PASCAl VOC datasets for pure classification but not object detection. Being a general image-to-image diffusion model, the generation step in DA-Fusion may alter the position of the objects in input images. It seems that DA-Fusion can only be applied to classification tasks."
            },
            "questions": {
                "value": "- The authors used a different number of augmented images per real image, $M$ ($M$=50 for spurge and $M$=10 for other data). Are there any guidelines for selecting $M$?\n\n- As pointed out in the weakness, the technical contribution of this work is slightly limited. It is recommended to compare DA-Fusion with more and stronger data augmentation baselines (instead of just RandAugment and Real Guidance) to claim a more significant empirical contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_5pTF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697691485791,
        "cdate": 1697691485791,
        "tmdate": 1700539503404,
        "mdate": 1700539503404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jebeeNk7qD",
        "forum": "ZWzUA9zeAg",
        "replyto": "ZWzUA9zeAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a diffusion model-based data augmentation technique for image classification. The method is based on a pretrained diffusion model. Textual inversion is applied to learn word embeddings for the classes in the target dataset for which the pretrained diffusion model may not learn before. During the generative process, real image is inserted at some time step to guide the generation. When training the target model, real image and synthetic image are mixed by probabilistic sampling. The paper further discuss some design choices for the proposed method, including the time step at which the real image is inserted during generation, strategies to prevent leakage of internet data, mixing ratio of real and synthetic images and the number of augmentations generated for each image. The proposed method is benchmarked on seven classification datasets and shown to outperform RandAugment and Real Guidance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method that combines several existing techniques for data augmentation is interesting.\n2. The paper provide insightful analysis on several design choices, including the time step at which the real image is inserted during generation, strategies to prevent leakage of internet data, mixing ratio of real and synthetic images and the number of augmentations generated for each image.\n3. The paper will release code and an aerial imagery dataset of leafy spurge, which will facilitate future study in this direction."
            },
            "weaknesses": {
                "value": "1. The proposed technique is only applicable for classification tasks. It is not clear how it can be applied for object detection and segmentation tasks. My thinking is that one of the major drawbacks of such generative model-based augmentation method vs. traditional method may be that it can not simultaneously generate the segmentation mask and bounding box annotation for the augmented images."
            },
            "questions": {
                "value": "1. I am not clear how the data-centric leakage prevention is performed. The paper mention that \"switching from a prompt ... is sufficient\" and \"Section 4.1 goes into detail\". If data-centric leakage refers to using prompt like \"a photo of a Class3\", what is the setting difference between Fig.5 and Fig.9?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_hrLX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698489834301,
        "cdate": 1698489834301,
        "tmdate": 1699636919410,
        "mdate": 1699636919410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BpsmIVdNVc",
        "forum": "ZWzUA9zeAg",
        "replyto": "ZWzUA9zeAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces DA-Fusion that generates additional training data using Stable Diffusion. It involves learning a word embedding that represents each class in the dataset and using SDEdit for generation. Experimental results are shown for few-shot classification on different types of datasets e.g., common concepts, rare concepts, etc, with supportive results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper was well written and easy to follow.\n- Performing textual inversion for novel concepts seems like a promising idea."
            },
            "weaknesses": {
                "value": "1. Generalizability of the method.\n    - It seems like the learned tokens are helpful for learning dataset specific biases, as it can add additional information on the general \u201cstyle\u201d of the dataset and of its classes. However, it seems to make certain assumptions about the datasets, e.g., that the images are object centric - only the target class is present, they have standard poses/viewpoints. It is not clear if there is substantial variations within the images of that class, e.g. in iwildcam, where the images from each class can be from different camera traps, thus, different backgrounds, camera parameters, the animals can have very different poses, or can be highly occluded. In this case, what would the single word embedding learn?\n\n2. Desideratas (sec 4)\n    - Controllable (content)\n        - Is textual inversion (as it is used in the paper) controllable? I.e. Is it easy to control for whether the token learns the style or object? Or if certain objects tends to co-occur with another, e.g. train and rails, it may not be necessarily learning the token for trains but also the objects that co-occur with it.\n    - \u201cperformant: gains in accuracy justify the additional computational cost of generating images from Stable Diffusion\u201d\n        - There does not seem to be any comparisons with compute cost relative to the baseline Rand Aug. DA-Fusion requires performing textual inversion, which may be expensive.\n        - It would also be interesting to compare the performance with standard augs like Rand Aug for different M. E.g., [1] showed that training on synthetic data outperforms when the generated dataset size much larger than the original dataset size. It is possible that at smaller M, standard augmentations outperform synthetic data, with the additional benefit that it is also fast to compute.\n\n3. Experiments\n    - Randaug, which includes several color based augmentations does not seem to be a good baseline for fine grained datasets like Flowers102, that may require the original color for classification.\n\n\n[1] Sariyildiz et al. Fake it till you make it: Learning transferable representations from synthetic ImageNet clones. CVPR\u201923"
            },
            "questions": {
                "value": "In addition to the questions in weakness, some clarifications about the experiments:\n- How does the method perform zero shot? If the learned tokens did capture the class characteristics, and there is no distribution shift between training and test, it seems like DA-Fusion should do well.\n- Is the leafy spurge task a binary classification problem i.e., detecting if there is leafy spurge in the image or not? If that is the case, how often do other plants occur in the data? I was wondering how much fine-grained details can textual inversion capture.\n- How much variance is there in the class word embedding? If textual inversion is performed again with a different seed, how different would the generations be? If they are, it can be another way to introduce diversity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7589/Reviewer_KxYv"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713408303,
        "cdate": 1698713408303,
        "tmdate": 1700735455999,
        "mdate": 1700735455999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JbsLQb9RfC",
        "forum": "ZWzUA9zeAg",
        "replyto": "ZWzUA9zeAg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7589/Reviewer_rXxt"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to data augmentation in deep learning, addressing the issue of limited diversity in traditional augmentation methods. The authors propose a method called DA-Fusion, which leverages large pretrained generative models to generate variations of real images while respecting their semantic attributes. They fine-tune pseudo-prompts to instruct the diffusion model on what to augment, and their approach is tested on few-shot image classification tasks across various domains, including a real-world weed recognition task.\n\nThe authors provide a comprehensive review of related work, highlighting the advantages of diffusion models in image generation and editing. They also discuss the challenges of preventing leakage of internet data when using large pretrained generative models for synthetic data generation.\n\nThe experimental results show that DA-Fusion consistently improves accuracy in various domains, outperforming traditional data augmentation methods. The paper includes a detailed analysis of the results, including a breakdown by the presence of common, fine-grain, or completely new concepts in the datasets.\n\nThe paper concludes by suggesting future directions for improving the flexibility and performance of their method, such as better control over image augmentation, maintaining temporal consistency in decision-making settings, and enhancing the photo-realism of the diffusion model backbone.\n\nOverall, the paper presents a promising approach to data augmentation that addresses the limitations of traditional methods and demonstrates its effectiveness in various domains. The discussion of potential future improvements adds value to the work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The work on DA-Fusion offers several strengths and introduces significant novelty in the context of data augmentation:\n* Novel Data Augmentation Technique: DA-Fusion introduces a unique approach to data augmentation by leveraging large pretrained generative models. It goes beyond traditional data augmentation methods that mainly involve geometric transformations. This novelty lies in utilizing generative models to create diverse and semantically meaningful variations of real images.\n* Semantic Preservation: Unlike traditional data augmentation techniques that focus on basic geometric transformations, DA-Fusion aims to respect the semantic attributes of images. It modifies images in a manner that retains their underlying semantics, such as the design of objects or specific visual details, making it more relevant for real-world recognition tasks.\n* Few-shot Learning Improvement: The paper demonstrates the effectiveness of DA-Fusion in enhancing few-shot image classification tasks. It is particularly valuable in scenarios where only a limited number of real images per class are available. This is a significant advantage as few-shot learning is a challenging problem with practical applications.\n* Fine-Grained Concepts: The authors show that DA-Fusion is especially beneficial for fine-grained concepts, where traditional data augmentation methods may not provide sufficient diversity. This highlights the method's potential in improving the recognition of subtle visual differences in image classification tasks.\n* Leakage Mitigation: The paper addresses the important issue of preventing leakage of internet data, which is often a concern when utilizing large pretrained generative models. The proposed defenses to mitigate leakage during evaluation contribute to the robustness and reliability of the method.\n* Generalization to New Concepts: DA-Fusion's ability to generalize to new concepts not seen during the diffusion model's training is a noteworthy aspect. This is a crucial capability, as it allows the method to adapt to a wide range of recognition tasks without the need for extensive additional data.\n* Clear Experimental Validation: The work provides a thorough experimental validation, including results on a variety of datasets spanning common, fine-grained, and completely new concepts. The consistent improvement in accuracy across different domains demonstrates the practical applicability and versatility of the approach."
            },
            "weaknesses": {
                "value": "Here are some of the weaknesses of this work:\n* Complexity and Computational Cost: The proposed method involves fine-tuning pseudo-prompts for each concept, which can be computationally expensive and time-consuming. This approach might not be as practical as traditional data augmentation techniques that are computationally efficient and easy to implement.\n* Lack of Control Over Augmentations: While DA-Fusion introduces the concept of modifying images while respecting their semantic attributes, it does not explicitly provide fine-grained control over how the augmentations are performed. This lack of control may limit its applicability to specific use cases where precise image modifications are required.\n\nSome nitpicks: Citations starting from second page are not in brackets like that in the first page. Would be nice to have consistency and keep them in brackets."
            },
            "questions": {
                "value": "I believe the idea is nice and clear. Paper is well written. Some might be of the opinion that the idea is not novel. However, I do like the effective use of pre-trained diffusion models and textual inversion for data augmentation. I like the detailed analysis that the authors have done for their approach.\n\nOne thing I would like to see is how this method can be used to generate positive and negative samples for contrastive learning based approaches. I understand this can be a herculean task for the authors to do now so not required as of now. But curious to know about the effect of this data aug strategy in SSL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728661205,
        "cdate": 1698728661205,
        "tmdate": 1699636919113,
        "mdate": 1699636919113,
        "license": "CC BY 4.0",
        "version": 2
    }
]