[
    {
        "id": "BbOUGYsQ0Q",
        "forum": "6bAfAcuuZD",
        "replyto": "6bAfAcuuZD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a slight modification of the forward-forward algorithm, in which activity is suppressed for positive instances and increased for negative instances. While this is a slight modification, the primary contribution of this research is the demonstration of several emergent dynamics of the trained network, which are then related to the concept of predictive coding and may replicate several experimental neuroscience findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is overall well organized and concise. Taking a well-publicized learning rule and showing a slight modification to relate to neuroscience theories should go a long way to keeping an open dialogue between ML and theoretical neuroscience. Additionally, nearly all of the questions I thought of asking, such as order and direction of surprise and expected signals, were addressed over the course of section 3, suggesting that the authors have put forethought into these analyses.Beyond a clarification described below, I have only two minor concerns with this paper. First, I wonder how interest it may be to ICLR attendees outside of the bio-inspired crowd. Some minimal discussion and possibly an additional abstract sentence to explain how predictive coding may be of broader ML interest could help alleviate this. Secondly, the loss function requires an explicit positive/negative cue (eta). Historically this sort of global signal has been contentious in computational models. It may be worth emphasizing that this error signal is singular, and how this may be much more easily achieved than a global vector valued error/target signal."
            },
            "weaknesses": {
                "value": "Beyond clarifications described below, I have only two minor concerns with this paper. First, I wonder how interest it may be to ICLR attendees outside of the bio-inspired crowd. Some minimal discussion and possibly an additional abstract sentence to explain how predictive coding may be of broader ML interest could help alleviate this. Secondly, the loss function requires an explicit positive/negative cue (eta). Historically this sort of global signal has been contentious in computational models. It may be worth emphasizing that this error signal is singular, and how this may be much more easily achieved than a global vector valued error/target signal."
            },
            "questions": {
                "value": "1.\tIn section 3.0 the optimizer is defined as RMSProp operating on the loss defined in equation 1. My understanding however is that standard RMSProp implementations calculate chained gradients and applied as a global optimization (though possibly with sub-losses defined on each layer), leading to weight transport and making the model non biologically plausible. If a modification, such as gradient stopping, was applied to prevent this global optimization then it should be described.\n2.\tSection 2, just before figure 2, states that only intralayer weights are trained, suggesting that recurrent (W)eights are, while biases (theta), (F)orward and (B)ackward weights are not. However, the surrounding text obfuscates this by speaking of the weights as \u201clocal properties\u201d. If this is the case, that is a critical aspect of the model, and should highlighted and compared to a version in which the remaining parameters are trained. It would be beneficial to highlight in a single location which parameters do/don\u2019t update, as well as defining abbreviations.\n3.\tIf these questions are resolved/clarified, I would be happily convinced to increase my soundness and overall score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Reviewer_D7tC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708679724,
        "cdate": 1698708679724,
        "tmdate": 1700841157315,
        "mdate": 1700841157315,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C5bZIvdl94",
        "forum": "6bAfAcuuZD",
        "replyto": "6bAfAcuuZD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_fE2s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_fE2s"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel and biologically plausible mechanism for understanding cortical processing, focusing on predictive aspects. It introduces a model based on the Forward-Forward approach, a form of contrastive learning, and makes the following three contributions.\n\n\n1) Hierarchical and Temporal Predictive Properties: The model reproduces some hierarchical and temporal aspects of predictive computations. It generates information flows that lead to surprise and cancellation signals, demonstrating its ability to capture, to some degree, the spatiotemporal predictive nature of cortical processing.\n\n2) Mechanistic Understanding of Information Flow: The paper offers insights into the emergence of information flows by tracing their origins to the ability of the neural circuit to implement spatiotemporal cancellation across different layers of activity. This understanding could explain how the brain processes and predicts sensory information.\n\n\n3) Biological Plausibility: The model's contrastive learning rule is shown to be equivalent to a unique form of three-factor Hebbian plasticity, which has strong connections to predictive coding. This highlights the biological plausibility of the proposed model. The training process involves positive and negative datasets, where surprise modulates layer activity based on whether the label and input match. The surprise calculation is defined in terms of individual layer activations. Importantly, training only involves forward passes, making the Forward-Forward algorithm biologically plausible and avoiding the non-locality of weight information inherent in backpropagation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written. \n\nIt provides substantial novelty in introducing a training procedure that mimics signal processing in biological networks, distinguishing between the presentation and processing phases. Surprise signals arise when there is a mismatch between sensory inputs and top-down signals (label information), similar to how cortical processing operates. \n\nThe authors pay significant attention to the interpretation of the learning and how this could be implemented in the brain. Obviously, it is to some degree biologically plausible, and the existing literature that looks into cortical microcircuits for predictive coding could be a good thing to add to discuss the possible implementation of the algorithm."
            },
            "weaknesses": {
                "value": "Although the paper offers a theoretically sound model, it lacks empirical validation through experiments or real-world data. \n\nIncluding practical applications or experiments to support the model's claims would enhance its credibility and applicability in real-world scenarios."
            },
            "questions": {
                "value": "Could you elaborate more on the algorithm, as in presenting some pseudo-code as well as a description of the neural architecture used that can be more easily connected to the standard literature of artificial neural networks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789750561,
        "cdate": 1698789750561,
        "tmdate": 1699637004888,
        "mdate": 1699637004888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "En9Lnmka61",
        "forum": "6bAfAcuuZD",
        "replyto": "6bAfAcuuZD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_3133"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_3133"
        ],
        "content": {
            "summary": {
                "value": "Backpropagation in deep networks has been widely acknowledged to suffer from a lack of biological plausibility.  Recently, a novel framework has been proposed to leverage contrastive learning on matching and non-matching datasets to learn only through forward passes through the network, avoiding several of the least biological features of backpropagation.  The authors utilize this approach to model neural correlates of surprise within a predictive learning framework, finding network dynamics that are broadly consistent with neuroscientific findings: stimuli that match \u201cpredicted\u201d labels result in little activity, while those that do not result in large deviations from baseline activity.  They further analyze the resultant spatiotemporal dynamics using PCA, characterizing both the dynamics and decodability of the network across space (layers) and time. Finally, the authors observe that the trained networks result in a local 3-factor Hebbian rule, and perform an analysis on the linearized form to provide additional intuitive insight into the observed prediction-cancellation process."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors provide a strong biological background and motivation for the importance of moving beyond backprop-based methods to better understand learning in cortex, and more specifically for predictive learning that their results qualitatively match"
            },
            "weaknesses": {
                "value": "_Major_\n\n1. While I am unaware of other explicit work that matches the qualitative results the authors provide, the overall framework, including the revised loss function and connection to predictive coding (including the cancellation process demonstrated by the authors) were all explicitly anticipated in Hinton 2022.\n   -  Moreover, the usage by the authors of a supervised contrastive learning process somewhat lessens the local learning process that can strengthen the biological plausibility of the results.  In particular, even though the learning process does not rely on propagating error information from the top layer, the local layer loss functions still contain explicit information about whether the inputs match the labels ($\\eta(t)$). The novelty would be increased by adding a more biologically realistic unsupervised learning process to the network (e.g., SimCLR).\n2. The PCA section does not seem to add much of substance to the manuscript\n   - Different numbers of components are included in different analyses for reasons that are somewhat difficult to discern (see Questions)\n   - Fig. 4a (top) seems to essentially describe the periodic patterns for the positive data observed in Fig. 2b.  Similarly for the periodic aspects in Fig. 4a (bottom) and Fig. 4c.\n   - Fig. 4a (bottom) is used to illustrate the strong visual decodability of PCs 4-6 vs. 1-3 (top).  However, in Fig. 4d we observe that PCs 1-3 in fact allow for decodability seemingly well above chance (if the x-axis is at a decodability value of 0), and that this decodability does not substantially increase for layers 1 and 2 and perhaps even 3.\n   - Fig. 4c does not seem to lead to much more insight than Fig. 4a (bottom), together with the observation that each loop in 4a (bottom) starts and ends in the center, as described for 4c.  This would be even more so if the trajectories in 4a (bottom) were modulated in intensity/luminosity/transparency to correspond to the associated time steps.\n   - In Fig. 4e, the presentation phase ends before the associated decodability traces either stabilize or peak (see Questions)\n   - Similarly, it seems as though the negative dynamics may cycle quasi-periodically as well based on Fig. 2b if enough time steps were included\n   - Overall, it seems as though the full-network analysis approach as shown in Figs 2-3 comprise a better analytical approach, given the generally simple network dynamics\n3. The model and training regimen are not explained well, and spread over several sections\n   - The model description is terse in the Introduction, with further details added in Sec. 3.0 and 3.4.  However, the notation between the Introduction and Sec. 3.4 (and App. A) are inconsistent (x vs. r, $\\theta$ vs T).  Similarly for training details\n   - The readability would be improved by providing all of the model and training data in the Introduction, before any results are described\n\n*Minor*\n1. I could not find any reference to App. B in the main text\n2. The final Results section (3.4) observes that the local aspect of the learning process is Hebbian.  While biologically important, this seems to fall directly out of the FF framework itself and only deserving of a passing observation\n3. The linear dynamics results in App. A.1 allow for the input to approximately equal the label.  While such a representation could be learned, I don\u2019t see how such a direct approximation is justified.\n4. \u201cSurprise\u201d seems to be defined in two ways (\u201cactivity as surprise,\u201d p. 2 top, then defined as components of the layer loss function and in terms of the activity on the bottom of p. 2 in Eq. 1)\n5. Some typos (e.g., \u201cat each time $x_{layer}(t\u2019)$\u201d p.2; \u201cdynamics dynamics\u201d in Supp. p. 2; \u201cthree-paired\u201d p. 5; lack of equal signs in Supp. p. 2, $t$ vs. $t\u2019$ in $z(t-1)$ on p. 7--also with and without arrow-vector decoration)\n   - Similarly, some mathematical notation not explicitly described, though easily inferred ($L(t)$, $I(t)$, $\\el(t)$)\n   - While \"PC\" is implicitly defined by reference to PCA in Fig. 4's caption, both PCA and PC should initially be explicitly referenced with a full name (e.g., \"Principle Component Analysis\") in the main text before using the abbreviation\n6. Fig. 3b would be more compelling if traces included those from several different network instantiations and included error shading\n7. Figs 4d-e would provide some additional intuition if they were to being at a decodability of 0 and include a dashed horizontal line corresponding to chance levels of decodability for comparison purposes"
            },
            "questions": {
                "value": "1. Fig 4d \u2014 what are the \u201cTriplet of PCs\u201d indicated in the abscissa label?  In the caption, it instead indicates they simply are the PCs, while in the main text it\u2019s stated they are \u201csliding-window three-paired PCs.\u201d   Please clarify what exactly is being decoded and provide consistent labels/descriptions in the manuscript\n2. Why are the different principal components chosen in the analyses?  How many are included seems to confusingly vary from one analysis to the next with little or no explanation.  \n   - E.g., components 1-3 and then 4-6 are used in the primary analysis in Fig. 4a-c. Then the first 20 components are used for Fig 4d\u2014while the main text indicates that \u201cmost of the variance is captured within the first 20 PCs,\u201d it is not indicated either how much of the variance is captured, or if this is the reason for going to 20 but no further.  Graphically, there is a clear drop-off at 20, perhaps to within-chance levels of decodability. Perhaps this is the reason for choosing 20?  \n   - Finally, for Fig. 4e, 50 PCs are chosen.  Why are 50 chosen? Based off of Fig. 4d, it seems as though a more principled analysis would only include the first 19 or 20 or so components\u2014ie, the ones that allow for above-chance levels of decodability. \n3. How would the PCs in Fig. 4e continue to evolve if the presentation phase were prolonged?  In the figure, all PCs have an upward trajectory before the processing phase begins, making it difficult to discern how much of the continued rise during the processing phase is due to continued, prolonged and perhaps delayed dynamics arising during the presentation phase.  A more compelling analysis might be to allow the dynamics to either stabilize or peak before ending the presentation phase and beginning the processing phase.\n4. Do the authors use layer normalization as per Hinton 2022?  Or is there a reason to not be concerned about the corollaries mentioned there in the authors\u2019 model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818588370,
        "cdate": 1698818588370,
        "tmdate": 1699637004757,
        "mdate": 1699637004757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ck94ivYABn",
        "forum": "6bAfAcuuZD",
        "replyto": "6bAfAcuuZD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a variation of the Forward-Forward Algorithm, which is an unsupervised learning model with local learning rules, using contrastive samples (positive and negative samples). The original Forward-Forward has as its objective maximizing the squared activity of each layer (greedily) for positive samples while minimizing it for negative samples. The variation proposed here uses the opposite objective, minimizing the activity of positive samples, and maximizing of negative samples, which makes it related to surprise signals and predictive coding models. The paper focuses on the dynamics of the model latent unit activity over time, inspecting for each layer, training phase and sample type. In particular, there is a difference between the dynamics of negative and positive samples, with cancellation happening in the bottom-up dynamics for positive samples. They also show the dynamics of the principal components of the latent space for different layers. Lastly, they derive the update rule from the proposed loss and map it to biologically plausible three-factor Hebbian learning rules."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The two main contributions of the paper:\n- a potentially novel local surprise / predictive coding loss for contrastive samples in multi-layer networks;\n- analysis of the dynamics of the latent variables for the hierarchical model.\n\nRelating the loss function and activity of artificial deep networks to cortical activity is a promising approach for revealing how the cortex learns representations."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper are:\n- lack of comparison with previous models, especially with predictive coding models;\n- unclear insights brought by the analysis of the dynamics, especially concerning previous literature, alternative models and experimental findings;\n- weak theoretical development.\n\nWhile the authors motivate their model as a variation of the Forward-Forward model, since it is a predictive coding-like model, the authors should also relate it to previous prediction coding models (Golkar et al., Neurips 2022), biologically plausible models (Illing et al., Neurips 2021) and related literature (Grill et al., Neurpis 2020). How is this model similar or different from previous models? Is this a novel procedure? Does it work better or worse than other models?\n\nWithout further motivation and contextualization for the model, the in-depth dynamical analysis is also difficult to assess. How would the dynamics of similar models behave? In any case, if the model is novel, it should be motivated and analyzed further, before diving into such specific properties. As the authors motivate the model as a potential model for cortical learning and activity, there should be more precise references to data and papers on what these dynamics look like in the brain (e.g. Rabinovich, Huerta, Laurent, 2008), and why they matter.\n\nThe theoretical argument of mapping the update to a three-factor learning rule is not convincing, as almost any update for a local loss in a neural network will have such a general form. The model should also be presented formally at the beginning of the paper. \n\nMinor:\n\nWhat's the difference between r_i and x_i? And threshold \\theta or T?\n\nThe accuracy for different number of layers should be included in the supplementary material, including the result for alternative models."
            },
            "questions": {
                "value": "How is this model different from previous models? Is this a novel procedure? Does it work better or worse than other models?\n\nHow would the dynamics of similar models behave? \n\nWhat experimental evidence on cortical activity does this model relate to and why do they matter?\n\nWhy is this model particularly biologically plausible compared to related models?\n \nWhat's the difference between r_i and x_i? And threshold \\theta or T?\n\nWhat is the performance of the model for different architectures, and in comparison to other models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8109/Reviewer_mvwV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699292859685,
        "cdate": 1699292859685,
        "tmdate": 1699637004627,
        "mdate": 1699637004627,
        "license": "CC BY 4.0",
        "version": 2
    }
]