[
    {
        "id": "zTZAjZvGdW",
        "forum": "NeWiiF6KLB",
        "replyto": "NeWiiF6KLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a regularizer to alleviate the training instability issue of EGNN in generative modeling. It analyzes the gradient of coordinate updates with respect to input and figures out the sensitive term that results in numerical explosions. Experimental results show that the proposed regularizer can successfully stabilize the ENF and EDM training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) This is the first time delving into the numerical instability issue of the ENF and EDM training. This is a very important challenge and fundamental research question that was overlooked in the previous research. This reviewer agrees with the significance of the proposed challenge;\n\n(2) The writing is basically well-organized, containing both theoretical analysis and experimental analysis, which makes the paper technically solid;"
            },
            "weaknesses": {
                "value": "(1) This work analyzes the gradient update of a single-layer EGNN but this reviewer does not see an analysis of special reasons leading to unstable ENF and EDM training. Only a single sentence states that \u201cIn both ENF and EDM, the graph node coordinates keep changing during the generative process, and abnormal coordinate updates may occur.\u201d However, the generative process is the inference process while this paper is dealing with the training instability. Therefore, the ENF and EDM training instability is not accurately summarized. Is EGNN on the given training data stable? Is ENF and EDM unstable on the given training data? If the answers to both questions are yes, then this might indicate that there are special reasons resulting in the instability of ENF and EDM training. If ENF and EDM training instability is caused by EGNN training only (no special stuff), then the discussions over the ENF and EDM are redundant and not necessary. In general, this reviewer thinks the preliminary discussions over the training instability are not sufficient. \n\n(2) The proposed regularizer is a sum of gradient calculations, which is extremely hard to compute. The summations are computed over all atom pairs (i, j) and EGNN layers L with at least O(N**2*L) time complexity. Hence, although the proposed regularizer is a very effective approach to restricting the gradient norm, the calculation of the proposed regularizer could not be scalable to either larger particle systems or deep EGNN architectures."
            },
            "questions": {
                "value": "(1) What is the time complexity of the calculation of the proposed regularizer? How could the proposed approach scale to larger particle systems and deep EGNN architectures?\n\n(2) What are the special reasons for numerical instabilities in ENF and EDM training? Or the instability of ENF and EDM is all caused by EGNN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672425194,
        "cdate": 1698672425194,
        "tmdate": 1699637036165,
        "mdate": 1699637036165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DFefVuELgo",
        "forum": "NeWiiF6KLB",
        "replyto": "NeWiiF6KLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
        ],
        "content": {
            "summary": {
                "value": "This paper builds upon E(n)-equivariant graph convolutional networks (EGNN) by normalizing the convolution layer with respect to the node\u2019s positions, and by adding a regularization term to the loss that promotes small gradients during training. The goal is to stabilize the training of graph generative models: normalizing flows and diffusion models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is mostly well written and clear. The regularization and normalization methods are shown in experiments to improve performance."
            },
            "weaknesses": {
                "value": "The contribution is incremental. The contribution is just dividing by a normalization factor the well known EGCL, and adding a regularization factor to training. These are natural modifications of EGCL, and any other network, and I suspect that many practitioners apply such tricks often. The analysis of stability of backpropagation is partial, and missing important contributing terms to the magnitude of the gradients."
            },
            "questions": {
                "value": "Page 4: \u201cIn contrast, normalizing coordinates can avoid abnormal coordinate updates from large differences among coordinates of neighboring nodes\u201d - but on the flip side, without the $+1$ regularization in the denominator, it is unstable to small coordinates. But with the $+1$ normalization, close-by nodes contribute a very small difference. How do you then choose the scale of the coordinates for the $+1$ to work well? Why do you use $+1$ and not $+b$ for some $b$ that depends on the characteristic target distance between modes?\n\nPlease explain how you construct a graph from the node locations and features.\n\nProposition 1: In all sums, shouldn\u2019t you sum over the neighborhood, and not the whole graph? $m_{i,j}$ is only defined when $(i,j)$ is an edge.\n\nSection 3.2 Sensitivity Analysis: The normalized EGCL is different from the unnormalized one. Why don\u2019t you compute the derivatives of the normalized version if this is the method you propose? Also, it is strange to directly write the derivative of $\\phi_x$ with respect to $\\|x_i-x_j\\|$. You need to use the chain rule, and first differentiate with respect to $m_{i,j}$.\n\nNotations: you did not define $I_3$.\n\nSection 4: I don\u2019t think that the derivative $\\partial f^{l+1}/\\partial f^l$ is the only main contributor to the size of $\\partial L (f^L)/\\partial \\theta$. To see this, note for example that when partitioning the parameters to the last layer parameters $\\theta^L$ and the previous layers parameters $\\theta^{L-1}$,  $\\partial L (f^L)/\\partial \\theta$ has two components. First, $\\partial L/\\partial f^L \\cdot  \\partial f^L\\partial \\theta^L$, and then $\\partial L/\\partial f^L \\cdot  \\partial f^L\\partial f^{L-1}\\cdot \\partial f^{L-1}\\partial \\theta^{L-1}$. Hence, $ \\partial f^{l}/\\partial \\theta^{l}$ are also important, and these are roughly going to depend on $\\|x_i-x_j\\|^l$ by induction, as each later multiples by a factor of order $\\|x_i-x_j\\|$. I think that this is another main reason you would like to normalize $x_i-x_j$.\n\nThis example is to illustrate that the analysis presented in this paper is partial. There is no systematic analysis of all components that contribute to the gradient. \n\nOne thing that is confusing in the analysis is that it analyzes only the unnormalized layer.  If you want to show that the normalized layer is more stable, you should write down a gradient analysis of it also. And write the full gradient with respect to the model parameters, as these are the gradients in training.\n\n\nExperiments: why are EDMs and ENF not compared against each other and other models in molecule generation in one table? I only see a comparison between EDM and other methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753310229,
        "cdate": 1698753310229,
        "tmdate": 1699637036054,
        "mdate": 1699637036054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G52rZc8r48",
        "forum": "NeWiiF6KLB",
        "replyto": "NeWiiF6KLB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
        ],
        "content": {
            "summary": {
                "value": "The paper highlights that there exists instability in training E(n)-graph neural networks (EGNN), especially maps for positions. The authors aim to stabilize the training of EGNN, especially by regularizations. Note that positional mapping in the EGNN includes the distance multiplication term, which is the distance between the node's position and its neighbor's, and it is critical to the E(n)-equivariance of EGNN.\n\nThe authors first point out that the parameters' gradient is proportional to the gradient of the EGNN outputs wrt the pairwise distance (between a node and its neighbor), and thus, the gradient term is the source of the instability of the EGNN training.\n\nDue to the gradient term, the authors claim that the previous attempt to stabilize the training, i.e., the normalized distance multiplication, is not sufficient, and thus, the authors propose to penalize the norm of the gradient (of EGNN) wrt the pairwise distance.\n\nTo test the hypothesis, the papers compare the three versions of EGNNs (vanilla, normalized distance, and the proposed method) over various benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I consider that the paper and its results are essential for several reasons:\n\n1. The paper provides a better understanding of a critical problem in training EGNN, i.e., the instability of its training, especially to the potential audience unfamiliar with EGNN and other similar models.\n2. The paper well motivates the proposed method so that readers can understand how each step contributes to the merits of the regularization.\n3. I found that the paper has a well-organized structure that makes it clear to understand the proposed method.\n\nIn addition, I found that the paper has a well-organized structure that makes it clear to understand the proposed method."
            },
            "weaknesses": {
                "value": "While the proposed method seems well-motivated and interesting, the importance of the proposed method needs further analysis.\n\nFor example, it is unclear why the gradient wrt the pairwise distance is the key source of the instability of the EGNN training, while the authors claim that the instability stems from the gradient. However, the claim is backed only by the sensitivity analysis explained in Section 3.2, which seems close to intuitions in my understanding. I believe that it would be much clearer if the authors were showing that the gradient wrt the pairwise distance is exploded when the training of vanilla EGNN failed."
            },
            "questions": {
                "value": "In my understanding, when the gradients are unstable during training, one common solution is gradient norm penalty or gradient clipping. How does the proposed method perform against such common techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699037971721,
        "cdate": 1699037971721,
        "tmdate": 1700872301524,
        "mdate": 1700872301524,
        "license": "CC BY 4.0",
        "version": 2
    }
]