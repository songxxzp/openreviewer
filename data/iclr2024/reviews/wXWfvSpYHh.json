[
    {
        "id": "AcRTD8A0zU",
        "forum": "wXWfvSpYHh",
        "replyto": "wXWfvSpYHh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission639/Reviewer_Xp5d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission639/Reviewer_Xp5d"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes MVSFormer++, which is an extended/enhanced version of the previous work MVSFormer. The authors have well-studied the usage of the transformer at different stages of the learning-based MVS pipeline, and have demonstrated the effectiveness of the proposed components by extensive experiments. The proposed pipeline achieves SOTA results on several MVS datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method achieves SOTA results on DTU, Tanks and Temples, and ETH3D datasets. I believe currently it is one of the best-performing MVS approaches.\n\n- The authors have conducted extensive experiments to demonstrate the effectiveness of the proposed components. I can find ablation studies on each proposed component in the experimental section."
            },
            "weaknesses": {
                "value": "- The paper proposed a bunch of small components/tricks over the previous MVSFormer. I acknowledge that these tricks might be useful, however, I would feel like each of them is a bit incremental and the whole story is not that interconnected. For the conference paper, I prefer a neat idea/key component that can bring strong improvements. The paper looks more like an extended journal version paper of the previous one.\n\n- ETH3D evaluation: the proposed method does not perform well on ETH3D even compared with other learning-based approach (e.g., Vis-MVSNet, EPP-MVSNet). Could the authors explain potential causes?\n\n- These is no a limitation section. I would like know the scalability of the proposed method, will the memory cost dramatically increased compared with CNN based approaches (e.g., CasMVSNet) when the image size/depth sample number increase?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595606449,
        "cdate": 1698595606449,
        "tmdate": 1699635991800,
        "mdate": 1699635991800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y6Z4lzW6kK",
        "forum": "wXWfvSpYHh",
        "replyto": "wXWfvSpYHh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces MVSFormer++, a learning-based Multi-View Stereo (MVS) method that leverages pre-trained models to enhance depth estimation in MVS. The study tackles a crucial gap in existing research by exploring the impact of transformers on various MVS modules. The paper's motivation is clear. However, there are notable areas that require attention for improvement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper innovatively introduces transformer-based models and attention mechanisms to address a vital issue in MVS. The novelty lies in the thorough exploration of different transformer attention mechanisms across diverse MVS modules. The paper provides hypotheses and experimental evidence supporting the use of different attention mechanisms in the feature encoder and cost volume regularization.\n\nThe authors conducted experiments across multiple benchmarks, including DTU, Tanks-and-Temples, BlendedMVS, and ETH3D, showcasing MVSFormer++'s state-of-the-art performance on challenging benchmarks (DTU and Tanks-and-Temples). This highlights the practical significance of the proposed approach.\n\nThe paper includes well-executed ablation studies, comparing the impacts of different attention mechanisms on various MVS modules."
            },
            "weaknesses": {
                "value": "(1) Clarity and Detail:\nThe paper lacks detailed explanations of specific design choices, such as the rationale behind selecting DINOv2 as the base model. Additionally, the utilization of different levels of DINOv2 features is not clearly elucidated. It is recommended to include these details to enhance the manuscript's clarity and independence.\n\n(2) Experiments:\nWhile the incorporation of DINOv2 in the feature extraction stage significantly enhances performance, it is crucial to clarify that this improvement is not solely due to the increase in the number of parameters. The improvement in point cloud evaluation metrics by the proposed module during ablation experiments appears subtle. To bolster the paper's experimental support, I recommend validating the proposed module's effectiveness by integrating it into baseline methods and conducting a comparative analysis. This would provide a clearer understanding of the module's actual contribution.\n\nAdditionally, it is advisable to explore more pre-trained models and conduct ablation experiments without pre-trained models. Given DINOv2's frozen state during training, fine-tuning it serves as a pivotal baseline.\n\nFurthermore, the paper should include visual comparisons of depth maps to visually demonstrate the accuracy advantages of the estimated depth maps.\n\n(3) Discussion of Limitations:\nThe paper lacks a discussion of the limitations and failure cases of MVSFormer++. Understanding the method's limitations is crucial for evaluating its real-world applicability."
            },
            "questions": {
                "value": "(1) DINOv2 Pre-training Choice:\nWhat motivated the decision to freeze DINOv2 during pre-training? How does it uniquely contribute to your method? Would including experiments with different pre-trained models or fine-tuning DINOv2 serve as valuable comparisons?\n\n(2) Cost-Benefit Analysis of DINOv2:\nConsidering the marginal improvement in point cloud metrics, is the increase in network parameters due to adopting DINOv2 justified? How can you demonstrate that the metric enhancements stem from the introduced module's contribution rather than a mere increase in parameters?\n\n(3) Discussion of MVSFormer++ Limitations:\nCould you briefly discuss MVSFormer++'s limitations, especially in scenarios where it might underperform?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_4iWa"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656161312,
        "cdate": 1698656161312,
        "tmdate": 1699635991717,
        "mdate": 1699635991717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P0DyXTl8qj",
        "forum": "wXWfvSpYHh",
        "replyto": "wXWfvSpYHh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission639/Reviewer_Wn6x"
        ],
        "content": {
            "summary": {
                "value": "This paper enhances MVSFormer by infusing cross-view information into the pre-trained DINOv2 model and exploring different attention methods in both feature encoder and cost volume regularization. It also dives into the detailed designs of the transformer in MVS, such as the positional encoding, attention scaling, and position of LayerNorm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper explores the detailed designs of attention methods in the context of MVSNet. \n2. It exploits the pre-trained DINOv2 in the feature encoder and merges the information of source views by cross-view attention.\n3. It designs a 3D Frustoconical Positional Encoding on the normalized 3D position, which is interesting and shows good improvements in depth map accuracy.\n4. It validates that attention scaling helps the scaling of the transformer to different resolutions, and the position of LayerNorm can affect the final accuracy."
            },
            "weaknesses": {
                "value": "Although the MVSFormer++ modifies the base model MVSFormer by DINOv2, SVA, Norm& ALS, FPE, etc, the core contributions share similar designs with other MVS methods.\n\n1. In the feature encoder, the Side View Attention is similar to the Intra-attention and Inter-attention in Transmvsnet. The main differences are that this paper uses a pre-trained DINOv2 as input and removes the self-attention for source features.\n2. The use of linear attention in the feature encoder has already been proposed in Transmvsnet.\n3. In Table 9, although with a larger network, the MVSFormer++ only improves on MVSFormer by a small margin, which can not fully support the claim of the effectiveness of 2D-PE and AAS.\n4. The FPE in Table 4 shows good improvement on CVT. The detailed network structure should be made more clear. Please see the questions. \n5. The evidence for the minor changes such as the LN and AAS is not strong with experiments on only DTU. They are more intuitive and may need more experiments to prove whether they are generalizable designs. For example, Table 9 on ETH3D actually cannot fully support AAS."
            },
            "questions": {
                "value": "1. I would to know the detailed structure differences between CVT and CVT+FPE. CVT is only used in the first coarse stage so how many stages use the CVT+FPE in Table 4? What are the results when CVT and CVT+FPE are both used in all stages or only the first coarse stage?\n2. The paper can be improved by focusing more on the novel and interesting designs such as the FPE and analyzing more on it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744276042,
        "cdate": 1698744276042,
        "tmdate": 1699635991649,
        "mdate": 1699635991649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydFKaYYbwc",
        "forum": "wXWfvSpYHh",
        "replyto": "wXWfvSpYHh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an enhanced iteration of MVSFormer named as MVSFormer++. The method utilizes the Side View Attention (SVA) to empower the cross-view learning ability of DINOv2. It prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. The results MVSFormer++ achieves on the DTU and Tanks-and-Temples benchmarks show the model works quite well."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The design of Side View Attention (SVA) is effective.\n2. Compared to other models, MVSFormer++ has better performance.\n3. The FPE and AAS are used efficiently to generalize high-resolution images.\n4. The paper is well written, and one can easily grasp the main idea."
            },
            "weaknesses": {
                "value": "1. In the ablation study, the results of Norm&ALS under the depth error ratios of 2mm and 4mm are slightly inferior.\n2. A discussion regarding the limitations is missing. \n3. Minor: Section 4.1 Experimental performance, mean F-score is 41.75 on the Advanced sets in the text while in Tab.3 it is 41.70."
            },
            "questions": {
                "value": "Please refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_3kfC"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748766639,
        "cdate": 1698748766639,
        "tmdate": 1699635991575,
        "mdate": 1699635991575,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vvsXgP1OEs",
        "forum": "wXWfvSpYHh",
        "replyto": "wXWfvSpYHh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an enhanced version of MVSFormer. In particular, it specifically addressed three challenges that remained in previous works: tailored attention mechanisms for different MVS modules, incorporating cross-view information into pre-trained ViTs, and enhancing Transformer's length extrapolation capability. Experimental results demonstrated the proposed MVSFormer++ attains state-of-the-art results across multiple benchmark datasets, including DTU, Tanks-and-Temples, BlendedNVS, and ETH3D."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The contributions of this work are solid and well address the limitations of previous MVS methods. For example, introducing side view attention significantly elevates depth estimation accuracy, resulting in substantially improved MVS results.\n+ The combination of frustoconical positional encoding and adaptive attention scaling is interesting. It enhances the model's ability to generalize across a variety of image resolutions while avoiding attention dilution issues.\n+ The experiments are comprehensive and promising. Almost all classical and SOTA methods are considered in the comparison experiments, which are evaluated on various datasets. For visual comparisons, the proposed method significantly outperforms other competitive methods, showing more complete structure and fewer geometric distortions."
            },
            "weaknesses": {
                "value": "- Except for the customized designs beyond the MVSFormer, this work leverages DINOv2 as a new backbone (compared to DINO used in the MVSFormer). It would be interesting to see how the performance of MVSFormer++ changes when it keeps the same backbone as that of MVSFormer.\n- MVSFormer and MVSFormer++ show different reconstruction performances regarding different cases on Tanks-and-Temples (Table 3). The authors are suggested to provide more discussions on how the qualitative results differ (like local details and global distributions) and why the degenerations happen.\n- The performance of the complete version of this work in the ablation study is different from the quantitative results reported in Table 2. Please elaborate on this inconsistency in metrics.\n- The baseline version of MVSFormer (without CVT, FPE, AAS, SVA, Norm&ALS) seems kind of strong already. Does it gain from the strong backbone? Moreover, the qualitative results of the ablation study are expected to be provided.\n- The description of Normalization and Adaptive Layer Scaling is ambiguous and unclear. More details about the motivation and implementation would be helpful to understand this part."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission639/Reviewer_B1hX"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765292720,
        "cdate": 1698765292720,
        "tmdate": 1699635991482,
        "mdate": 1699635991482,
        "license": "CC BY 4.0",
        "version": 2
    }
]