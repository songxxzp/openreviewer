[
    {
        "id": "mHtN7BylXc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_xMY5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_xMY5"
        ],
        "forum": "i4eDGZFcva",
        "replyto": "i4eDGZFcva",
        "content": {
            "summary": {
                "value": "This paper investigates the effect of reward centering and how it can affect learning speed and accuracy in value-based methods. Specifically, it is shown how subtracting the average reward estimate from the reward values can improve the learning speed and allow to use higher discount factors, while at the same time making the learning algorithm robust to shifts in the reward function. Empirical results show the wide applicability of this idea, both with and without function approximation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Although not a major breakthrough, the paper investigates an interesting aspect that can improve the learning performance of many RL algorithms. Indeed, it is interesting to note how simply shifting the reward function by a certain value can completely hinder the performance of the Q-learning algorithm, even though this does not change the policy ordering at all. In this sense, the empirical results are really useful at showing the merits of the proposed idea, as well as its applicability. Also, the explanation of this idea is quite clear and easy to follow."
            },
            "weaknesses": {
                "value": "However, the structure of the paper is not really good to drive a reader through the proposed concepts and idea. A more organic and clear form should be adopted to make the work much more understandable and readable. Please see the Questions below for a broader discussion on this."
            },
            "questions": {
                "value": "- Having the paper to start with a (simple) experiment demonstrating the effectiveness of the proposed idea, without either introducing it or giving any explanation before, is weird: it is difficult to contextualize its significance or assess any proper merit here. I would recommend to start the paper with a proper Introduction section to present the investigated setting and motivations for that, to then show and analyse results after these have been already clarified to the reader.\n- In general, the paper is not well structured. It starts by diving directly into the investigated problem, without giving any context to that, not presenting any Background section to inform the reader about useful notions and concepts, or a Related Works section to introduce other works addressing the same or a similar problem. I would strongly suggest to restructure the paper entirely to give it a more structured and principled form, so that any reader can get familiar with the basics before getting into your novel contribution. Most of the parts are already there indeed, it is more a matter of giving them a coherent order of presentation to gradually lead the reader into the work.\n- However, some concepts that are used throughout the paper are never discussed in practice, such as the concept of Blackwell-optimality. I feel these are important information to provide the reader with, and should find some space in a Background section.\n- It is difficult to understand the analysis of results that are only reported in the Appendix. Why not moving some of the more interesting results into the main paper and discuss these, while leaving the discussion of those that do not appear there to the Appendix directly?\nTypos:\n- Page 5, beginning of Section 3: flashes, not fleshes"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Reviewer_xMY5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697361399385,
        "cdate": 1697361399385,
        "tmdate": 1699636715259,
        "mdate": 1699636715259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y4TjnR5xkr",
        "forum": "i4eDGZFcva",
        "replyto": "i4eDGZFcva",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_JTzu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_JTzu"
        ],
        "content": {
            "summary": {
                "value": "The authors suggest to improve Q-Learning in the continuous, infinite horizon setting, by substracting the empirical mean reward and therefore centering the reward distribution. This approach is justified by prior work, whereas the proposed method is seen as a specific parameterization of a known algorithm. This allows the authors to derive a convergence proof. Besides the theoretical results, the authors provide empirical results from three different domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The approach is quite general and suggest improvement potential for a wide range of RL algorithms. Additionally, the theoretical foundation improves the value of the work. The paper is well written and offers a substantial discussion of the empirical results."
            },
            "weaknesses": {
                "value": "Mostly, the weaknesses are relate to the novelty of the approach. Modifying the reward distribution is a well known approach, with either a shaping function (e.g. Learning to Utilize Shaping Rewards), substracting an empirical baseline (e.g. A2C/A3C \"Asynchronous Methods for Deep Reinforcement Learning\") or normalization (https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html). However, none of these methods is discussed or empirically evaluated. A theoretically comparison would also be welcome, but is not expected. Therefore, the novelty of the idea is limited and related work as well as the empirical experiments should be improved. However, the reviewer acknowledges, that the mentioned work focuses on the conventional, episodic settings and that may introduce incomparability, but this is also not discussed. Furthermore, normalization and advantage-based methods are likely applicable.\n\nAdditionally, the convergence proof is mostly based on a paper that was never peer-reviewed, as far as the reviewer can see, (Devraj, A. & Meyn, S. (2020)), weakening its reliability.\n\nSmaller issues:\n- The algorithms and domains used for the empirical evaluation are rather simple. \n- Fixing epsilon without any further evaluation may lead to suboptimality and limits the fairness of the comparison.\n- Considering r as part of the transition dynamics is rather unconventional. Usually, transition dynamics and reward functions are separated.\n- Some references are missing (e.g. DQN)"
            },
            "questions": {
                "value": "A short comment concerning advantage function and reward normalization methods would be welcome."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Reviewer_JTzu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698148045049,
        "cdate": 1698148045049,
        "tmdate": 1700122020463,
        "mdate": 1700122020463,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GR817jgIVT",
        "forum": "i4eDGZFcva",
        "replyto": "i4eDGZFcva",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_Kzst"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_Kzst"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of rewarding centering in continuing problems. More specifically, they analyze a variant of Q-learning, *Centered Q-learning*, where the rewards experienced by the agent are subtracted from their empirical average online and incrementally.\nStarting from the discounted MDP model, they develop key intuitions on why reward centering can be useful in this scenario. More specifically, starting from the Laurent series decomposition of the policy return, they highlight the presence of the average return (i.e., offset) collected by the policy. If this offset is large, approximation errors in estimating introduce problems in identifying which is the optimal action (i.e., the relative benefit of taking the optimal action). Leveraging this intuition, the authors propose the Centered Q-learning algorithm, and they analyze its convergence. Finally, they numerically validate Centered Q-learning against classical Q-learning, both in tabular cases and with linear and non-linear function approximations: \n- Centered Q-Learning gains significant performance, especially in the relevant case of $\\gamma \\rightarrow 1$.\n- Centered Q-Learning makes the algorithm robust in shifts of the reward functions\n- The method is relatively robust to the choice of the update rate of the running mean of the empirical average reward"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors develop good intuitions on the practical values of the common practice of reward centering. These intuitions are supported by some simple theoretical analysis and experiments in several domains. Since reward centering is of practical value, I believe this work to be interesting for the community. \n- The paper is well-written (with some unclear points in the theoretical part; see weakness 2 below).\n- The problem is relatively novel, although some works on the topic exist."
            },
            "weaknesses": {
                "value": "1. **Empirical results** are on simple domains. I would ask the authors whether they expect their empirical results to hold in more intricated benchmarks (e.g., some Atari domain, for instance). Have the authors experimented with these more complex domains (or other domains of similar complexity)? Indeed, I retain the paper to be mainly of an empirical nature. As a consequence, I think this point could add substantial value to the submission. Currently, I believe this to be a major weakness of the work.\n2. **Theoretical results** (clarity; minor). Theorem 1 shows the convergence of centered Q-learning to a particular solution. However, no comments are given on the quality of this solution. It seems to me that if, we take the arg max w.r.t. this Q-function, we obtain an optimal policy for the original discounted MDP with discount factor $\\gamma$. First of all, I ask the authors if they confirm this claim. Secondly, I invite the authors to discuss it in the main paper.\n3. **Theoretical results** (clarity; minor; pt. 2) Furthermore, I do not truly understand the role of $\\bar{r}$ within the Theorem. As it is defined, it seems to be a free variable in a certain solution space, which makes the interpretation of Theorem 1 a bit cryptical. Could the authors discuss on this point?"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6417/Reviewer_Kzst"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698161073390,
        "cdate": 1698161073390,
        "tmdate": 1699636715033,
        "mdate": 1699636715033,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Te0rgDKtHK",
        "forum": "i4eDGZFcva",
        "replyto": "i4eDGZFcva",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_ecFz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_ecFz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called \"Reward Centering\" to improve the performance of discounted reinforcement learning (RL) algorithms. The authors argue that by centering the rewards around the current average reward, the RL algorithms can focus on the essential differences between states rather than the diffe in addition to a potentially large offset. This approach is shown to be beneficial for all discount factors, especially as the discount factor approaches 1. The paper presents empirical results on various domains, including tabular, linear, and non-linear function approximation, demonstrating the benefits of reward centering in terms of improved learning performance and robustness to shifts in rewards."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper provides a clear motivation for the reward centering technique by analyzing the issues with standard RL methods when the discount factor is close to one.\n* The authors present a comprehensive theoretical analysis of the convergence properties of the proposed Centered Q-learning algorithm.\n* The empirical results demonstrate the benefits of reward centering across different domains and function approximation techniques, including tabular, linear, and non-linear methods.\n* The paper discusses the limitations of the current approach and suggests possible extensions and improvements, such as combining reward centering with scaling techniques and extending the method to episodic problems."
            },
            "weaknesses": {
                "value": "* The paper focuses primarily on the tabular case for the theoretical analysis, and the convergence results may not directly apply to the function approximation case.\n* The paper does not provide a detailed comparison of the proposed method with other state-of-the-art RL algorithms, which would help to better understand its relative performance."
            },
            "questions": {
                "value": "* How does the performance of the proposed Centered Q-learning algorithm compare to other state-of-the-art RL algorithms, such as Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC)?\n* Can the reward centering technique be extended to handle episodic problems, where the ordering of policies may change due to reward shifts?\n* Are there any potential drawbacks or limitations of the reward centering approach that were not discussed in the paper? For example, could centering the rewards introduce biases or affect the exploration-exploitation trade-off?\n* The paper suggests that reward centering can be combined with scaling techniques to handle large magnitudes of relative values. Can you provide more details on how this combination might work and its potential benefits?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807858829,
        "cdate": 1698807858829,
        "tmdate": 1699636714909,
        "mdate": 1699636714909,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OXaUUGN5aG",
        "forum": "i4eDGZFcva",
        "replyto": "i4eDGZFcva",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_DBdQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6417/Reviewer_DBdQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called reward centering, which aims to solve continuing reinforcement learning problems with discounted reward criterion. The proposed method can be easily integrated with existing RL methods, although the authors have only experimented with a case of Q-learning. Reward centering involves subtracting the empirical average reward from the observations during training, which reduces the effect of a state-independent offset in the discounted value estimation. Experimental results demonstrate that reward centering can enhance performance and maintain the robustness of Q-learning in tabular cases with either linear or non-linear approximation. The paper also provides a theoretical analysis of the convergence in various types of approximation (linear and non-linear)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It addresses a practical and essential problem of learning long-term optimal policies in continuing problems with discounting.\n- It proposes a simple and effective technique that can be easily applied to existing algorithms without changing their core structure or adding much computational overhead.\n- It provides a clear and rigorous theoretical analysis of the convergence and variance properties of Centered Q-learning in the tabular case. \n- It presents comprehensive and convincing empirical results on various domains with different types of function approximators, showing the benefits of reward centering over standard methods."
            },
            "weaknesses": {
                "value": "- It does not extend the theoretical analysis to the function approximation case, which is more challenging and relevant for real-world applications.\n- It does not compare reward centering with similar techniques that improve discounting, such as reward scaling [1], GAE [2], etc. \n- The paper abuses notations: iteration number is $t$, and timestep is also $t$.\n- It lacks cross-comparison from the lens of RL algorithms and hyper-parameter settings (e.g., $\\epsilon$ for Q-learning).\n- The motivation is not very clear. As the authors claim their proposed method solves reward shifting, but practical examples are needed to illustrate the generality of this issue.\n\n[1] Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., & Madry, A. (2020). Implementation matters in deep policy gradients: A case study on ppo and trpo. arXiv preprint arXiv:2005.12729.\n\n[2] Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438."
            },
            "questions": {
                "value": "- Is there any difference between reward and reward rate? As the community prefers to use 'reward' to represent the environmental feedback of each decision making, so I'm not sure why the authors chose another notation if there are no differences.\n- How do I choose the update frequency at lines 7 and 12 in Algorithm 3?\n- I noticed the authors build Theorem 1 on top of the irreducible Markov chain, is this assumption loose or tight?\n- What is $d$ in Equation 17?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830316205,
        "cdate": 1698830316205,
        "tmdate": 1699636714776,
        "mdate": 1699636714776,
        "license": "CC BY 4.0",
        "version": 2
    }
]