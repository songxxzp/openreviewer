[
    {
        "id": "bEeLJZCnIa",
        "forum": "bCvm9h0FmQ",
        "replyto": "bCvm9h0FmQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission832/Reviewer_hn12"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission832/Reviewer_hn12"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the heterogenous prediction behaviors for backdoor samples and clean samples from the causality perspective and proposes a causality-based backdoor detection method that only requires the prediction labels from the victim model. Extensive experiments on three benchmark datasets demonstrate the effectiveness and efficiency of the proposed detection method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Trendy topic\n- Interesting and easy-to-understand attack pipeline\n- Well-written"
            },
            "weaknesses": {
                "value": "- Some presentations are misleading\n- More explanations are needed\n- More experiments are needed"
            },
            "questions": {
                "value": "- The authors leverage causal inference to find that the backdoor attacks act as a confounder, creating a spurious path from backdoor samples to the modified prediction results. Based on this insight, the authors propose a causality-based black-box backdoor detection method that employs counterfactual samples as interventions on the prediction behaviors to effectively distinguish backdoor samples and clean samples. Extensive experiments on three benchmark datasets demonstrate the effectiveness and efficiency of the proposed detection method.\n\n- I appreciate that the paper is well-written, especially the section where the authors use causal inference to explain the different behaviors between backdoor samples and clean samples. Their detection method is interesting and easy to understand.\n\n- In Figure 2(f), the authors aim to show that images with sample-specific triggers promptly deviate from the original prediction results by adding noise. I do not think the label here is still \"Fish.\"\n\n- In Section 3.1, the authors directly introduce a magnitude set by adding noise. Based on my understanding, it is necessary to explain how to choose this magnitude set, because the proposed method involves introducing varying magnitudes of noise and observing whether the prediction results are flipped in each query to conclude whether a given sample is a backdoor sample. Another alternative approach is to show that the choice of magnitude set does not affect the effectiveness of the proposed detection method.\n\n- It would be better to conduct more experiments on the choice of $\\alpha$, $\\beta$, and $|S|$, and determine whether the values differ across different datasets and different attack methods.\n\n- In Table 1, it would be better to have a notation indicating which attacks are sample-specific and which attacks are sample-agnostic. Additionally, it is unclear whether the proposed detection method performs similarly well on both types of attacks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698053945916,
        "cdate": 1698053945916,
        "tmdate": 1699636010754,
        "mdate": 1699636010754,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ov0MaVHxu7",
        "forum": "bCvm9h0FmQ",
        "replyto": "bCvm9h0FmQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission832/Reviewer_u6kT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission832/Reviewer_u6kT"
        ],
        "content": {
            "summary": {
                "value": "The paper explores a black-box backdoor detection problem that can only access testing samples and labels. By analyzing the heterogeneous prediction behaviors for backdoored and clean samples, the paper proposes a Causality-based Black-Box Backdoor Detection (CaBBD) method to distinguish whether a sample is clean or backdoored. Specifically, CaBBD introduces counterfactual samples as intervention to check the difference of model outputs. Extensive experiments on three datasets and four datasets indicate the effectiveness of CaBBD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper explains the intuition of proposed method from the causality-based perspective, which makes the proposed reasonable. Also, some preliminary experiments (e.g. figure 8) demonstrate the observations (at the bottom of page 5) that clean and backdoored samples can behave differently when attatching noises with different sthengths. The extensive experiments demonstrate the effectiveness of proposed method in Table 1."
            },
            "weaknesses": {
                "value": "The experiments are not sufficient. The paper presents results using four attacks including BadNet, Blended, WaNet and ISSBA in Table 1. There is no clean-label attacks such as label-clean [1], SIG [2] and ReFool [3]. It is better to show the results on clean-label attacks.\n\nSome typos are obvious. For example, in the caption of figure 2, (b) should be sample-agnostic trigger and (c) should be sample-specific trigger. \n\n[1] Turner, Alexander, Dimitris Tsipras, and Aleksander Madry. \"Label-consistent backdoor attacks.\" arXiv preprint arXiv:1912.02771 (2019).\n[2] Barni, Mauro, Kassem Kallas, and Benedetta Tondi. \"A new backdoor attack in cnns by training set corruption without label poisoning.\" 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019.\n[3] Liu, Yunfei, et al. \"Reflection backdoor: A natural backdoor attack on deep neural networks.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16. Springer International Publishing, 2020."
            },
            "questions": {
                "value": "Could the authors explain more about how to choose \\alpha and \\beta? Are the two hyperparameters are attack-dependent or dataset-dependent or architecture-dependent? It is better to do some ablation studies.\n\nIs the proposed method sensitive to different network architectures? It is better to show results on the same dataset using different architectures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission832/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission832/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission832/Reviewer_u6kT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698452404347,
        "cdate": 1698452404347,
        "tmdate": 1699636010677,
        "mdate": 1699636010677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QhkuPsJMos",
        "forum": "bCvm9h0FmQ",
        "replyto": "bCvm9h0FmQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission832/Reviewer_PHNh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission832/Reviewer_PHNh"
        ],
        "content": {
            "summary": {
                "value": "This study focuses on the problem of identifying backdoor samples. The authors introduced a framework for backdoor detection for cases where a separate clean validation dataset is unavailable. Their methodology draws from techniques found in the causal inference literature. The proposed methods are subject to experimental evaluation, using three distinct datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Addressing the problem of backdoor sample detection is important and the introduction of Causality-based techniques for defense is new, at least to me.\n2. The paper is well-written and easy to follow.\n3. The experimental results seem to be promising."
            },
            "weaknesses": {
                "value": "1. The proposed method has undergone testing with only four types of attacks, which may not provide sufficient evidence to establish its effectiveness convincingly. It is recommended that the authors consider assessing its performance against a broader range of attacks, including adaptive backdoor attacks such as TaCT [a] and Adaptive Blend [b], as well as non-poisoning based backdoor attacks.\n2. Is there a theoretical rationale for utilizing counterfactual samples, even when dealing with a simple linear model? While the empirical observations provide some insight, it is essential to gain a deeper analytical understanding of the underlying mechanisms that drive the proposed method.\n\n\nRefs:\n[a] Tang et al., \"Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection.\"\n[b] Qi et al., \"Revisiting the Assumption of Latent Separability for Backdoor Defenses.\""
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission832/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission832/Reviewer_PHNh",
                    "ICLR.cc/2024/Conference/Submission832/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782983082,
        "cdate": 1698782983082,
        "tmdate": 1700680654139,
        "mdate": 1700680654139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5FDLzIpbP3",
        "forum": "bCvm9h0FmQ",
        "replyto": "bCvm9h0FmQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission832/Reviewer_k4uD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission832/Reviewer_k4uD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to detect backdoor samples at\nrun-time in the black-box scenario. The proposed method is based\non the causality analysis of the backdoor attacks. It works\nby adding the noises under different magnitudes on the\nexamined images and detects backdoor samples by analyzing\nthe prediction's sensitivity to the magnitudes of the added\nnoise. Experiments demonstrate that the proposed method is\neffective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Run-time backdoor sample detection in the black-box manner is an important problem.\n\n* Both sample-agnostic attacks and sample-specific attacks are analyzed and discussed."
            },
            "weaknesses": {
                "value": "* The novelty of this paper might be limited as this paper\nclaims to be the first to analyze backdoor predictions from\na causal perspective, while previous work by Zhang et al.\n[1] has conducted a similar analysis. This paper lacks a\ndetailed comparison between Zhang et al.'s causal analysis\nand its own although it cites Zhang et al. The connection\nbetween the proposed method and the causal analysis is not\nclear. The definition of counterfactual samples and the\nrationale behind considering noise-added samples as\ncounterfactual is vague. The proposed method distinguishes\nbackdoor samples and clean samples based on their different\nsensitivity to the perturbations or augmentations, which\nshares similar spirits to many existing methods such as\nSTRIP.\n\n* There are some attacks that are robust to the\nperturbations, such as Wang et al. [2]. The\ncolor-style-based attacks [3,4,5] might also have stronger\nrobustness to the added noises compared to the attacks\ninvolved in the experiments. The evaluation of these attacks\nis missing. In addition, this paper only uses one simple\ntrigger pattern for the BadNet attack. It is suggested to\nuse more complicated patterns with large pixel values to\nmake the evaluation more comprehensive.\n\n* This paper does not explore adaptive attacks where\nattackers are aware of the defense mechanism and\nactively attempt to circumvent it.\n\n\n[1] Zhang et al., Backdoor Defense via Deconfounded Representation Learning. CVPR 2023.\n\n[2] Wang et al., Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers. arXiv 2023.\n\n[3] Jiang et al., Color Backdoor: A Robust Poisoning Attack in Color Space. CVPR 2023.\n\n[4] Cheng et al., Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification. AAAI 2021.\n\n[5] Liu et al., ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation. CCS 2019."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission832/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698979164663,
        "cdate": 1698979164663,
        "tmdate": 1699636010537,
        "mdate": 1699636010537,
        "license": "CC BY 4.0",
        "version": 2
    }
]