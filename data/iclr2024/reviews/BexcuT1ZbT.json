[
    {
        "id": "oYKdN0e0SG",
        "forum": "BexcuT1ZbT",
        "replyto": "BexcuT1ZbT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_p74s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_p74s"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to employ approximate score-matching based on style and content reference image during the reverse time for image style transfer. Besides, such a proposal provides a control factor to control the degree of stylization. The authors claimed this as the first work of using the diffusion model for style transfer. They show both quantitative and qualitative evaluations of MS-COCO and WikiArt."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors provide comprehensive math explanations and theorems in the paper for illustration.\n    \n2. This paper is well-written and it is easy to follow."
            },
            "weaknesses": {
                "value": "1. The authors claim this submission is the first work using the diffusion model for style transfer. However, I think there have been some works on this point. The authors should discuss and compare with them.\n    \n\n[1] Zhang Y, Huang N, Tang F, Huang H, Ma C, Dong W, Xu C. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 10146-10156).\n\n[2] Wang Z, Zhao L, Xing W. StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision 2023 (pp. 7677-7689).\n\n2\\. The proposed method is not sound to me. Specifically, I have read sections 3.2 - 3.4 but still not clear which distance metric is used to calculate the loss function of content and style. Besides, during reverse, images and predicted images are typically added noises while the pre-trained CNN is trained with clean images, how do the authors overcome such limitations? What\u2019s the relationship between the proposal and classifier guidance [1] or self-guidance [2]? The authors should discuss and compare with them.\n\n[1] Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems. 2021 Dec 6;34:8780-94.\n\n[2] Epstein D, Jabri A, Poole B, Efros AA, Holynski A. Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986. 2023 Jun 1.\n\n3\\. The experiment results are not strong enough to verify the effectiveness of the proposal. Specifically, most results of StyTr^2 show better results in Figure 3 and Table 1. \u00a0For example, in the second sample in Figure 3, StyTr^2 shows a much clearer outline of the bird compared with the proposal. In the third sample in Figure 3, StyTr^2 renders a better cloud than the proposal. Besides, the authors should compare with the works mentioned in the first point."
            },
            "questions": {
                "value": "My biggest concern is about the comparison with related works. Please see more details in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698575401892,
        "cdate": 1698575401892,
        "tmdate": 1699636214951,
        "mdate": 1699636214951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zu1DnF88sI",
        "forum": "BexcuT1ZbT",
        "replyto": "BexcuT1ZbT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_zeKW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_zeKW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Style-Diffusion which is a diffusion model that conducts image-to-image style transfer. Style-Diffusion is built upon Schrodinger Bridge problem, where they aim to conduct neural style transfer via sampling from the midst point of two different domains. To this end, in addition to the score matching objective, the author proposes to use the VGG features to condition the diffusion model, which helps preserving the semantic structure from the content image. The experiments were conducted to show its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Style-Diffusion incorporates VGG feature networks in image-to-image diffusion model to enhance style transfer without deforming semantic structure."
            },
            "weaknesses": {
                "value": "- Some experimental details are missing, for example, which diffusion models are used in an experiment? How the noise variances or timesteps are chosen? How does the sampling be done at the midst? \n- The writing could be improved. For example, why do we need Figure 2 in this paper? It seems like Figure 2 simply depicts the conventional diffusion process, but does not provide any information on the proposed method. Also, in the related works section, the authors defined a score-based model, while in the methods section, the model suddenly changed into a noise prediction model. \n- There are some missing references that considers image-to-image translation using diffusion models (or SGMs) such as DDIB [https://arxiv.org/abs/2203.08382] or I^2SB [https://arxiv.org/abs/2302.05872]. I believe the concurrent works on diffusion models with Schrodinger Bridges for image-to-image translation should be discussed even though they do not consider style transfer tasks."
            },
            "questions": {
                "value": "- See Weakness part. In general, my major concern is the lack of implementation details in training / sampling of the diffusion model. I think the idea of extracting features from pretrained vision encoders were widely used tactics in neural style transfer, thus the main contribution of this work lies in adapting to the diffusion models. However, many details are missing in terms of reproducibility.\n- What is D3PSR in sentence below equation 15?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772780015,
        "cdate": 1698772780015,
        "tmdate": 1699636214867,
        "mdate": 1699636214867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Acd2miVWKX",
        "forum": "BexcuT1ZbT",
        "replyto": "BexcuT1ZbT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_qGym"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_qGym"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a Style-Diffusion method for stylization transfer in image generation tasks. The proposed method utilizes Score-Based Generative Modeling (SGM) and approximate score-matching to estimate the drift of the reverse-time Stochastic Differential Equation (SDE). By introducing the Control Factor, controllable stylization in the output images is achieved. The original diffusion problem is reformulated as a composite Schr\u00f6dinger half bridge Problem to improve computation speed and enable diffusion evolution between more complex multiple distributions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is interesting that the proposed method gradually evolves content and style image distributions (by utilizing the middle output), and introduces Control Factor (CF) to enable control over the degree of stylization in the output images.\n\n- By adjusting the Control Factor (CF), the balance between structure and texture can be controlled.\n\n- The original multi-end diffusion problem is re-formulated as a composite Schr\u00f6dinger half bridge Problem to improve computation speed."
            },
            "weaknesses": {
                "value": "- It is not ture that this is the first work on image style transfer using diffusion models, please check the following paper:\nStyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models, ICCV 2023\n- The experiments have limitations as they lack comparison to the latest works on style transfer and the compared methods are relatively old. Additionally, the comparison to StyleDiffusion, which was mentioned above, is missing.\n- The writing could be further improved, I found the paper somewhat difficult to follow."
            },
            "questions": {
                "value": "- The interpretation (page 6, the first three lines) of eq.13 is confusing to me. Shouldn't the first term represent the loss related to content, while the second term represents the loss related to style?\n\n- There is a text description \"The second and third terms denote ...\" below eq. 14. However, it is confusing that there is no such term in the equation. Did I miss something obvious?\n\n- What is the architecture used to implement the CNN for feature extraction? And what \"conv1_1\", \"conv2_1\" and so on stands for? And it is unclear how this CNN could be used as the noise predictor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856865757,
        "cdate": 1698856865757,
        "tmdate": 1699636214775,
        "mdate": 1699636214775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5q4GuvAcLk",
        "forum": "BexcuT1ZbT",
        "replyto": "BexcuT1ZbT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_7QnQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2725/Reviewer_7QnQ"
        ],
        "content": {
            "summary": {
                "value": "This paper uses formulates style transfer as Schrodinger bridge problem.\n- The score matching losses are defined in the feature space of an encoder by feeding x_t: the x_t should have the features of the content image and the style image.\n\nThe authors argue that the proposed method is flexible and efficient while preserving the semantic structure.\n\nThe authors argue that the proposed method is the first style transfer with a diffusion model.\n\nThe experiments are conducted with a few artistic style transfer examples."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Score matching in the VGG feature space is interesting."
            },
            "weaknesses": {
                "value": "1. Statements are unclear or not self-contained. Thorough revision with an expert would make the paper readable. The details are deferred to the end because there are too many.\n\n2. Connection between Schr\u00f6dinger bridge and style transfer is missing.\n* What are the advantages of using Schr\u00f6dinger formulation on style transfer? The authors mention reducing time cost and it is a general advantage in diffusion models without connection to style transfer.\n\n3. Important recent papers with diffusion-based style transfer are not covered nor compared. Here are a few.\n* Ruiz et al., Dreambooth, CVPR 2023\n* Gal et al., An Image is Worth One Word, ICLR 2023\n\n4. Qualitative evaluation is hardly agreeable.\n* I would say the proposed method does not reflect the style in Sample 3.\n* All methods produce colorful results without glowing effect.\n\n5. Quantitative results are measured with too few (=4) images. Previous papers use hundreds, e.g., in StyTr2 and StyleFormer.\n\n6. Quantitative results are measured with a non-standard metric.\n\n-------------------\nHere are details of the unclear and non self-contained statements.\n\n* In what aspect the proposed method is flexible and efficient? Flexibility and efficiency are mentioned in the abstract and introduction sections but they should be more rigorously / specifically defined.\n\n* What are prior distributions and domains? Figure 1 has boxes with domains and images but it is vague. Please add rigorous definition of the domains.\n\n* Why is it important to estimate the *time inhomogeneous* drift?\n\n* Definition of $X$ and $\\mathcal{X}$ are missing.\n\n* Hyperparameter $\\phi$ is introduced in Section 3.2 but used in 3.3. Please move it close to the usage.\n\n* Eq.11 should have $x_B$ instead of $x_A$.\n\n* Section 3.2 and 3.3 are poorly structured.\n\n> Existing diffusion-based methods leverage the U-Net architecture to obtain a noise predictor, $\\epsilon_t^\\theta$. \n* U-Net is the noise predictor itself, not a separate component to obtain a noise predictor.\n\n> Existing diffusion-based methods leverage the U-Net architecture to obtain a noise predictor, $\\epsilon_t^\\theta$. \n> Different from the noise adding way of traditional diffusion model, which utilizes the Gaussian noise during the forward process. We impose the target vectors to guide the noise in each unique time step. \n> ... we use a pre-trained CNN to extract 5 style feature target vectors (s1, s2, s3, s4, s5) and 1 content feature target vector (c1) from two domains, and then impose these feature targets to guide the noise, $\\epsilon_t^\\theta$.\n* The first two sentences imply that the proposed method does not use U-Net but the last sentence explains that feature vectors from the CNN-based encoders guide the noise $\\epsilon$ where $\\epsilon$ is U-Net. Please resolve this contradiction. If I have mistaken it, please provide clear explanation.\n\n> there is currently no work that utilizes a diffusion-based model for style transfer, \n* Ruiz et al., Dreambooth, CVPR 2023\n* Gal et al., An Image is Worth One Word, ICLR 2023\n\nI am stopping here although there are far more errors.\n\n(minor)\n\ntypos\n\n> In contrast to the traditional SGM method, our model obtains the desired output at the midst layer. \n\n-> midst timestep?\n\n> Different from ... during the forward process. We impose the target vectors to guide ....\n\n-> different from ... process, we impose ....\n\nSample IDs are missing in Figure 3"
            },
            "questions": {
                "value": "I think this paper needs a thorough upgrade to be submitted anywhere else."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2725/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698919260658,
        "cdate": 1698919260658,
        "tmdate": 1699636214715,
        "mdate": 1699636214715,
        "license": "CC BY 4.0",
        "version": 2
    }
]