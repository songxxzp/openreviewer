[
    {
        "id": "FksGCKB86F",
        "forum": "qe49ybvvPs",
        "replyto": "qe49ybvvPs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_fJQm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_fJQm"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes projection ensembles method in distributional reinforcement learning for generalization and suggests optimistic bound based on 1-wasserstein distance to promote deep exploration. The authors provide some theoretical results for the proposed Bellman operator. The authors also provide a practical algorithm, PE-DQN and demonstrate its performance on deepmind b-suite benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThe paper is well-written in general, and easy to follow. The authors provide sufficient background in distributional RL. Their theoretical results were clearly illustrated and the proof appears to be valid. The experimental results of the PE-DQN appear to be promising."
            },
            "weaknesses": {
                "value": "\u2022\tThe description of the motivation and related work appears to be rather insufficient.\n\n\u2022\tTechnical novelty and its contribution seems weak. While the paper focuses on the exploration issue in distributional RL, its theoretical analysis only shows the contractivity and upper bound for proposed Bellman operator.\n\n\n\u2022\tIt is a bit odd that the benchmark for comparison does not include the C51, QR-DQN, which employes an $\\epsilon$-greedy scheduling method. It is also difficult to verify that the baseline is properly reproduced, since the experiment was not conducted in the standard Atari benchmark."
            },
            "questions": {
                "value": "\u2022\tMinor Comments\no\tSection 5.1, paragraph of Uncertainty Propagation : ~exploratory actions.Further details ~ \u2192 ~exploratory actions. Further details ~\n\no\tIn Section 6.1, $\\mathbb{S[Z]}$ should be the mean-variance measure to recover DLTV-QR, not just the variance.\n\no\tIn Section 6.2, \u201cthe bsuite consists of 22 tasks\u201d seems incorrect as Fig 9 shows 23 bsuite tasks.\n\no\tAppendix B.1, Table 4: DLTV has a fixed decaying schedule, $\\sqrt{\\logt /t}$. How can $\\beta_{final}$ be defined?\n\no\tAppendix B.2, Table 5: Initial bonus $\\beta_{init}$ is written twice.\n\no\tAppendix B.2, Equation (17) : What does $\\eta_{M,\\phi}$ refer to? The expression seems to be ambiguous.\n\n\u2022\t- In the paper, an upper bound is obtained by Proposition 2, but it seems insufficient to justify the proposed bonus $b_{\\phi}$ as an epistemic uncertainty estimate. Since the confidence bound in Proposition 2 is just an expression naturally induced by the 1-Wasserstein distance, it seems invalid from a regret analysis perspective. Furthermore, the authors again approximate the total distributional error by the one-step TD distributional error, which seems to have no significant theoretical connection. Of course, I think it is quite difficult to describe this theoretically, so I suggest to plot the experimental results of the bonus estimate $\\hat{b}_{k+1}(s,a)$ or $\\mathbb{E}_{s,a}[\\hat{b}_{k+1}(s,a)]$ in each environment.\n\n\u2022\tIn Figure 4, is PE-DQN [Ind.] without uncertainty propagation equivalent to the C51/QR mixture model with greedy action selection? If so, the performance improvement of PE-DQN seems to be mainly due to the introduction of bonus estimates. Based on the experimental results in the current paper, it is difficult to confirm whether the elimination of the inductive bias by projection contributes to the performance improvement.\n\n\n\u2022\tProjection ensembles seem to be an attempt to relax the priors of the model as in IQN[1] and FQF[2]. Can the authors explain the difference from those?\n\n[1] Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" International conference on machine learning. PMLR, 2018.\n\n[2] Yang, Derek, et al. \"Fully parameterized quantile function for distributional reinforcement learning.\" Advances in neural information processing systems 32 (2019)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697751151681,
        "cdate": 1697751151681,
        "tmdate": 1699636500543,
        "mdate": 1699636500543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CCdDNhsqwT",
        "forum": "qe49ybvvPs",
        "replyto": "qe49ybvvPs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_9dG6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_9dG6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a novel distributional RL method with improved exploration efficiency. The algorithm maintains an ensemble of return distribution models, each associated with a representation and a projection operator bounded in the p-Wasserstein metric. The ensemble is updated towards a shared TD target computed by first applying the distributional Bellman operator to a return distribution, and then projecting it to individual representations using corresponding projection operator to constitute a new mixture model, an operator the authors named the projection mixture operator. They have demonstrated the contraction property of the proposed projection mixture operator. They then move on to propose to use the W1 distance between true and estimated return distributions as an exploration bonus to construct a provably optimistic Q-value for action selection. Without access to the true return distribution, the authors approximate the exploration bonus with its upper bound which can be estimated incrementally. When the ensemble consists of categorical and quantile projections, the authors provided loss functions on which the model parameters can be updated through gradient descent. Experiments on a variety of tasks have shown improved exploration and overall performance compared to several ensemble / distributional RL benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. proposed a novel approach to deep exploration in distributional RL\n2. the paper is well written in general with clear and stringent logic\n3. has systematically shown that an upper confidence bound of the Q-value can be constructed from the epistemic uncertainty of return distribution estimation in terms of W1, rather than merely treating the latter as a curiosity bonus"
            },
            "weaknesses": {
                "value": "1. since the proposed algorithm is an ensemble on distributional RL, comparisons only with either distributional RL or ensemble on scalar RL but not both seem inadquate. Perhaps worth comparing to some of the works you mentioned in Related Work.\n2. the illustration for Fig.1 and Fig.2 can be expanded"
            },
            "questions": {
                "value": "1. is each individual ensemble member constrained to a mixture representation of the return distribution? If so, how is your approach different from a super mixture model containing num_members * num_atoms_per_mixture atoms without splitting into individual mixtures?\n2. the atoms in a quantile representation can be thought of with equal weights whereas those in a categorical representation cannot. Would you not need to make this distinction in Eq. 7?\n3. the local estimate of the exploration bonus $w_1 (\\hat{\\eta}, \\Omega_M\\mathcal{T}^\\pi\\hat{\\eta})$ seems to be measuring the distance between the ensemble return distribution as a whole and its backup target, I failed to see how it may be estimated as the model disagreement among ensemble members (page 7)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Reviewer_9dG6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698255472631,
        "cdate": 1698255472631,
        "tmdate": 1699636500442,
        "mdate": 1699636500442,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JLiMKWHTuD",
        "forum": "qe49ybvvPs",
        "replyto": "qe49ybvvPs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_a3AN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_a3AN"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the use of using ensembles in distributional reinforcement learning. The paper first develops the theoretical framework for using different projection operators in an ensemble and the conditions under which the ensemble still forms a contraction mapping. Then the paper proposes an upper bound on the Q-value estimated by the ensemble via a 1-Wasserstein distance between the learned return distribution, $\\hat{\\eta}(s, a)$, and the true return distribution $\\eta^\\pi(s,a)$, $b^\\pi(s,a) = w_1(\\hat{\\eta}, \\eta^\\pi)(s,a)$. This quantity can be seen as a form of epistemic uncertainty since it would vanish if the learned return distribution is equal to the true return distribution (Proposition 1&2). Since $\\eta^\\pi(s,a)$ is unknown and uncertainty estimation with bootstrapping can incur bias, the paper then proposes a bellman-like estimate for $b^\\pi(s,a)$ that allows it to be estimated recursively using local information (theorem 3).\n\nIn the second part of the paper, the paper applies this framework to DQN. The algorithm PE-DQN consists of three components: a QR-DQN ($\\theta_1$), a C51 ($\\theta_2$), and an uncertainty module ($b_\\phi$). During training, $\\theta_1$ and $\\theta_2$ are updated with the respective distributional RL algorithm, and $b_\\phi$ is updated with the uncertainty update from theorem 1. During action selection, the $\\theta_1$ and $\\theta_2$ are ensembled to estimate the Q-value, and $b_\\phi$ is used to estimate the exploration bonus.\n\nExperiments show that PE-DQN is able to outperform relevant baselines in terms of exploration, generalization and credit assignment in bsuite. The paper then shows that in the VizDoom environments with different reward-sparsity levels, PE-DQN is able to match the performance of relevant baselines in the dense reward setting and outperform the baselines in the sparse reward setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Personally, I really like the ideas proposed in the paper. \n\n-  I find the theoretical motivation compelling and the derivation is straightforward and elegant. The use of 1-Wasserstein between different ensembles provides a nice alternative interpretation of the variance approach many existing works use.\n- The proposed uncertainty propagation allows the uncertainty estimation to take in future uncertainty, which many similar methods cannot do, and also addresses some shortcomings of existing methods that do propagate future uncertainties (e.g., the uncertainty bellman equation).\n- Empirically, combining the different approaches of distributional RL also makes a lot of sense. Since they have very different inductive biases, it's less likely that they would collapse to the same prediction before the return estimate converges to the true one (in principle).\n-  The experiments support the theory reasonably well.\n\nOverall, I think this paper is a nice addition to the distributional RL literature and I am not aware of similar works."
            },
            "weaknesses": {
                "value": "- The performance improvement over existing methods is relatively small. It would be perhaps ideal to find scenarios where the performance of the PE-DQN is more significant than existing methods. One task that I can think of is Procgen. Recently, [1] showed that incorporating epistemic uncertainty can significantly improve the performance of Value-based methods on Procgen but the method in [1] does not incorporate future uncertainty and only uses an ensemble of QR-DQNs. I would expect PE-DQN to be much better in this case.\n\n- I would like to see a comparison of the wall clock time to get a better understanding of the computational cost.\n\n- I would like to see a sensitive analysis of $\\beta$.\n\n- There are several additional works that use ensembles of distributional RL models to do exploration [1, 2, 3]. This paper's contribution is unique (to the best of my knowledge) but I feel it would be better to discuss these works to provide a more comprehensible discussion of what has already been done and what shortcomings this work addresses. [5] also proposes a similar idea but in the tabular setting."
            },
            "questions": {
                "value": "- I wonder if using different architectures or different ensemble weights for the ensemble would further improve the performance. Perhaps there is even a way where one can adjust the weights to get a tighter estimate of the uncertainty.\n\n- How does this propagation scheme compare to other schemes like UBE [4]?\n\n## Reference\n\n[1] On the Importance of Exploration for Generalization in Reinforcement Learning. Jiang et al.\n\n[2] Distributional Reinforcement Learning for Efficient Exploration. Marvin et al.\n\n[3] Estimating risk and uncertainty in deep reinforcement learning. Clement et al.\n\n[4] The Uncertainty Bellman Equation and Exploration. O'Donoghue. et al.\n\n[5] Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning. Parisi et al."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646727283,
        "cdate": 1698646727283,
        "tmdate": 1699636500358,
        "mdate": 1699636500358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MWnuMAonBW",
        "forum": "qe49ybvvPs",
        "replyto": "qe49ybvvPs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_1pzE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5093/Reviewer_1pzE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a simple ensemble method for distributional RL. The authors discuss some basic theoretical properties of this ensemble and experimentally test its performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a neat and simple idea -- creating a distributional RL algorithm whose return distribution function is an average of multiple return distribution functions. This idea could be easily applied by many practitioners to improve performance. The authors have demonstrated that the ensemble performs better (as with all RL algorithms, the true test of whether it is indeed actually better will be if it is adopted by more people) on bsuite, deep sea environment and VizDoom environment as compared to baselines."
            },
            "weaknesses": {
                "value": "**Originality**: The paper states that \"our work is the first to study the combination of different projection operators and representations in the context of distributional RL\", but consider the paper [_Distributional Reinforcement Learning with Ensembles_ by Lindenberg, Nordqvist and Lindahl](https://arxiv.org/abs/2003.10903) from 2020. Specifically, equation (7) in that paper has the exact same form as the formulation of $\\eta_M$ in the paper under review.\n\n**Poor writing**: The Achilles heel of this paper is the muddled presentation of ideas. Let me elaborate\n1. There are many places where **imprecise writing** makes the presentation difficult to parse. For example, in the abstract it is stated that \"profounding impacting the _generalization behavior_ of learned models\". What does generalization behavior here mean? It is [known](https://arxiv.org/abs/2111.09794) that generalization in the context of RL can have many different meanings. As another example, in the introduction it is stated that \"the space of potential return distributions is infinite-dimensional\". The space of probability measures (as opposed to finite signed measures) is _not_ a vector space, and so how do the authors define a dimension in this context? It couldn't be the cardinality of the set because then the set of representable distributions will also have infinite dimensions. Or do the authors mean parametric vs nonparametric, where the dimension is defined on the parameter space? As another example, Figure 1 is poorly explained -- what are the x and y axis, how does it convey the notion of \"distinct generalization signature\" as stated in the paper? As another example, in section 3, the notation is inconsistent. $S_t$ and $A_t$ are used to denote samples or random variables? If samples, then it is inconsistent with what is stated next that capital letters denote random variables. If random variables, then what does $\\mathcal{R}(\\cdot \\mid S_t, A_t)$ or $\\mathcal{R}(S_t, A_t)$ (two ways to stating the same thing?) even mean? $\\mathcal{D}^\\pi(s,a)$ is in $\\mathscr{P}(\\mathbb R)$ and not in $\\mathscr{P}(\\mathbb R)^{\\mathcal S \\times \\mathcal A}$. In the definition of a pushforward measure in the appendix, it is stated that $B \\in \\mathbb R$, which should be $B \\in \\mathcal{B}(\\mathbb R)$ where $\\mathcal{B}(\\mathbb R)$ denotes the Borel subsets of $\\mathbb R$.\n2. There are many places where the presentation is **incomplete or even wrong**. The notion of inverse CDF is not defined anywhere despite it playing a critical role in the paper. When defining the supremum $p$-Wasserstein metric, the authors let $p \\in [0, \\infty)$. I am pretty sure that this is incorrect and should be $p \\in [1, \\infty)$. As a simple test, think what happens when $p=0$. See the monograph _One-Dimensional Empirical Measures, Order Statistics, and Kantorovich Transport Distances_ by Bobkov and Ledoux for more details.  The reference of R\u00f6sler 1992 for distributional RL makes no sense -- yes it studies related math, but it is not on distributional RL! It is stated in section 3.2 that $\\Pi_C$ is a projection defined on $\\mathscr{P}(\\mathbb R)$, but the complete definition is never even given. It is defined on finite mixtures of Dirac measures, but what about countable mixtures of Dirac measures, or non-atomic measures? In section 4, it is stated that $\\eta_M$ has support over $\\mathscr{F}_M$. I believe this is incorrect. Consider $\\mathscr{F}_1 = \\{Z\\}$ and $\\mathscr{F}_2 = \\{-Z\\}$, where $Z$ is, say, some random variable having Gaussian distribution. Then $\\eta_M = \\delta_0 \\notin \\mathscr{F}_M$.\n\nI really wanted to like this paper -- the idea is simple and potentially useful. But the poor presentation forces me to not recommend acceptance of this paper, unless significantly rewritten. I wasted many hours disentangling, rather than learning, the ideas in the paper, and I wouldn't wish that on potential readers."
            },
            "questions": {
                "value": "Apart from the multiple questions I asked in the weaknesses section, what is the use of Theorems and Propositions in Section-4, when they play no role in the sections ahead. If they are not empirically useful results, but simply theoretical curiosities, I still don't know what I learned from them. They seem simple manipulations of the various definitions without an end goal in sight."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5093/Reviewer_1pzE"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5093/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699111655164,
        "cdate": 1699111655164,
        "tmdate": 1700751639825,
        "mdate": 1700751639825,
        "license": "CC BY 4.0",
        "version": 2
    }
]