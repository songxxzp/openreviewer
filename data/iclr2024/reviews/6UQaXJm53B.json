[
    {
        "id": "Rbh112OBb4",
        "forum": "6UQaXJm53B",
        "replyto": "6UQaXJm53B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_6zCc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_6zCc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an RL algorithm to address the problem of naturalness degeneration in LMs, which often occurs during RL finetuning. The proposed algorithm employs action-masked policy, a behavior policy in which actions (i.e. tokens) that potentially cause the degeneration are masked out by leveraging reference policy. This allows for separate likelihood maximization and minimization for rollout samples during learning. The authors observed gains in two natural language generation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation is intuitive. I like the interpretation that decomposes advantage function of PPO into task-specific advantage and advantage for naturalness, as outlined in the section 3.1.\n- The idea of action masked policy that leverages the reference policy is novel.\n- The paper is well-organized."
            },
            "weaknesses": {
                "value": "- The current version of the paper lacks some important explanations such as:\n    1. When and how do the samples in the red area of Table.1 become undesired samples?\n    2. How does the action-masked policy lead to better text generation than PPO?\n    3. How can it ensure that the masked actions do not contribute to enhancing the task score while maintaining naturalness?\n    4. How efficient is DfPO in terms of sample efficiency for learning?\n- DfPO needs feed forwarding for reference policy to obtain negative action-masked policy at inference time. This demands additional resources and computation.\n- The benefits from the method are limited, especially in CommonGen, in terms of naturalness and diversity. Breath of results is also limited as the authors consider only two tasks."
            },
            "questions": {
                "value": "- Reward functions we use to finetune LLMs are usually designed to consider the naturalness. In this case, the samples that have the opposite direction for improving task performance and naturalness may not be crucial to the degeneration. Thoughts?\n- How does DfPO perform when it does not use negative action-masked policy during inference?\n- Mixing PPO loss and SFT loss (from the same batch) is a basic way to mitigate the \u2018tax\u2019 like the naturalness degeneration in RL finetuning for language generation . This baseline should be included in this work.\n- The table caption should be positioned at the top of the table.\n- Use \\citep after a name of a method (e.g., PPO \\citep{\u2026}) instead of \\citet"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9127/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9127/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9127/Reviewer_6zCc"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586775571,
        "cdate": 1698586775571,
        "tmdate": 1699637148558,
        "mdate": 1699637148558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mf2neakVcm",
        "forum": "6UQaXJm53B",
        "replyto": "6UQaXJm53B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_99mP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_99mP"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method to maintain the generation distribution (in place of the KL penalty in PPO) during the RL procedure. The basic idea is to apply RL optimization through likelihood maximization and minimization to samples only if the direction is also towards the reference policy. Experimental results on GRUE benchmark show that the proposed method outperformed PPO and NLPO."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Experiments on the GRUE benchmark show that DfPO indeed get better the downstream task scores and the generated texts remain the naturalness. \n\nThe proposed doesn't perform hyper-parameter search."
            },
            "weaknesses": {
                "value": "1. My main concern is the relationship of equations (4) and (5) and the claim on the last paragraph on page 4, \"the positive action-masked policy considers only those actions for which the naturalness advantage is positive among the actions of the current policy, and the negative action-masked policy considers only those actions for which the naturalness advantage is negative among the actions of the current policy.\" I think the authors need to mathematically proof this claim is true.\n\n2. Regarding to the experiments, I think the authors should provide the win rates of the proposed method vs several baselines by human evaluation. Current evaluations are based by human defined metrics like Rouge etc. which are problematic in real world applications."
            },
            "questions": {
                "value": "1. At the end of section 3.1: \"the proportion of samples in the green area is inevitably very low\" is there any proof or reference for this statement?\n\n2. Why not include DPO as one of the baselines?\n\n3. \\gamma_natural in Equation (1) should be negative log ratio instead of KL divergence.\n\n4. In section 2.1, the value and action-value functions should be formulated in a finite horizon instead of an infinite horizon for LLMs. Typo error, \"The value and action-value function\" where function should be \"functions\", plural"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808468540,
        "cdate": 1698808468540,
        "tmdate": 1699637148362,
        "mdate": 1699637148362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ujrQcwPCVJ",
        "forum": "6UQaXJm53B",
        "replyto": "6UQaXJm53B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_hR6e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_hR6e"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new reinforcement learning method called Degeneration-free Policy Optimization (DfPO) for fine-tuning language models without causing text degeneration.\n\nIt observes that standard RL methods like PPO are prone to text degeneration when fine-tuning LMs, due to unbalanced optimization of task rewards vs language model likelihood.\n\nDfPO introduces an \"action-masked policy\" to avoid sampling tokens that can cause unexpected optimization.\n\nIt uses separate clipped advantage functions to perform stable likelihood maximization and minimization.\n\nExperiments on text generation benchmarks show DfPO improves task scores while preserving fluency, without needing sensitive KL penalty hyperparameters like PPO/NLPO.\n\nDfPO outperforms PPO/NLPO on generation quality even without hyperparameter tuning.\n\nIn summary, the main contributions are proposing DfPO to address text degeneration in LM fine-tuning, introducing techniques like action masking and clipped advantages, and demonstrating improved performance over standard RL methods empirically."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Here is an assessment of the key strengths of this paper:\n\n1. The paper addresses an important problem in language model fine-tuning - text degeneration when using reinforcement learning. This is a major challenge limiting the applicability of RL for optimizing language models.\n\n2. The proposed method DfPO introduces some novel and creative ideas to tackle this problem:\n\n     a. Using action masking to avoid sampling risky tokens\n\n     b. Separate clipped advantage functions for stable likelihood optimization\n\n     c. Policy optimization via likelihood maximization/minimization\n\n3. These techniques seem technically sound and are motivated clearly. The overall algorithm is also clearly explained through figures, equations and pseudocode.\n\n4. The experimental methodology is very thorough, rigorously comparing DfPO against strong baselines like PPO, NLPO on standard text generation benchmarks.\n\n5. Results demonstrate clear improvements in balancing task rewards versus fluency, without needing extra hyperparameter tuning.\nThe paper is well-written, laying out the background, approach, experiments in a structured and easy to follow manner.\n\n6. Solving text degeneration could significantly expand the applicability of RL for language tasks. So the work has very good potential for real-world impact.\n\nIn summary, the paper makes solid technical contributions through creative ideas, and supports them through extensive experiments and clear writing. Given the significance of the problem it addresses, this work could be impactful for the field."
            },
            "weaknesses": {
                "value": "Overall, the paper makes good technical and experimental contributions. But expanding the theoretical analysis, testing on more tasks and models, and providing more discussion would further strengthen the paper. The weaknesses are more about opportunities for extending the work rather than flaws in what has been done.\n\n1. While the core DfPO techniques seem sound, more analysis could be provided on the dynamics of how action masking and separate advantages control optimization.\n\n2. Only text generation tasks are evaluated. Applying DfPO to other language tasks like summarization could strengthen the generality of the approach.\n\n3. More analysis could be provided on the diversity of generated text - perhaps measuring repetitiveness."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830303743,
        "cdate": 1698830303743,
        "tmdate": 1699637148239,
        "mdate": 1699637148239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "46yIColDRt",
        "forum": "6UQaXJm53B",
        "replyto": "6UQaXJm53B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_wZxt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9127/Reviewer_wZxt"
        ],
        "content": {
            "summary": {
                "value": "For language generation, naively using RL to optimize task score may lead to unnatural outputs from the learned language model. The standard practice to deal with this problem is to regularize the RL training with a weighted sum of the task reward and a KL-penalty, which however introduces additional hyperparameters (the weight) that need to be carefully tuned. This paper proposes an alternative method, called DfPO in the paper, which does not use the KL weight to reconcile the optimization trade-off here -- thus avoiding additional hyperparameter -- but instead will only update the policy gradient on state-actions where the task reward and the KL-penalty can be consistently optimized. The paper shows that their algorithm can achieve similar performance with PPO and NLPO, but without hyperparameter tuning, on two tasks of the GRUE benchmark."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The current tricks and hyperparameters involved to constrain RL-trained language models from deviating \"too far\" from the pre-trained model is, in my opinion, an \"ugly\" part of the practice. So, I appreciate much of this paper's aspiration to try solving the problem without additional hyperparameters, and hopefully, solving it in a more elegant way.\n\nIt appears that the proposed algorithm has almost constant perplexity on one task, even though the algorithm itself does not seem to explicitly impose this. It would be an interesting property if the same observation applies in more general scenarios."
            },
            "weaknesses": {
                "value": "**(a)** I have a number of concerns and confusions about the sections that narrate the proposed algorithm and its rationale. Many important statements are claimed but not properly justified or proved. Math equations are sometimes sloppy. The pseudo-code of the proposed algorithm also left important details unclear to me. See my Question 1 ~ 10 below for the detailed concerns. Overall, I am not sure that the algorithm and its explanation in the paper indeed make sense.\n \n**(b)** I also have concerns about the experiment part. In the abstract, the introduction section, the conclusion section, as well as in the experiment section (Section 5.1, the paragraph starting with \"Comparison with baselines\", page 8), the paper repeatedly and explicitly states that the proposed algorithm DfPO \"outperforms PPO and NLPO\". I think Table 2 is the main supportive evidence for this claim. However, the sentiment score improvement there (+0.01) is within 1-stdev (and the stdev is from 5 runs only) so the improvement may not be statistically significant. Regarding perplexity score, although DfPO is indeed 1.3 lower, I notice that in Table 2 the supervised learning method actually leads to the largest increase of perplexity, and as supervised learning is not known to hurt naturalness significantly, I wonder if the small difference in perplexity score here can indeed translate to substantial improvement on naturalness or not. Overall, I'm not sure that the experiment results well support the main claim that DfPO outperforms.\n\nBesides that, some numerical results in the experiment part are also subject to questions. See my Question 11 ~ 14 below for details.\n\nFinally, the proposed algorithm was only tested on two tasks of the GRUE benchmark. Given my major concerns on the rationale of the algorithm, I expect more comprehensive empirical evaluations and from the current experiment I am not confident that the algorithm will perform well in more general scenarios."
            },
            "questions": {
                "value": "1. My impression, after reading Section 2 and 3, is that you define the \"degeneration problem\" as the reduced \"naturalness\" and that you measure naturalness with the KL-divergence against the initial policy $\\pi_0$ which is a pre-trained language model. However, being different from the particular initial model does not mean that the new model's output is unnatural. In fact, any effective improvement over the initial model has to output differently from it, thus *necessarily* increasing the KL-divergence, right? In general, although we seek for naturalness through controlling the KL-divergence, we don't really aim at *minimizing* it, which is why we have it weighted by $\\beta$ in standard practice. So, although it works as a heuristic trick, I doubt if the KL-divergence can *represent* the naturalness.\n\n2. I feel, in many places of the paper, you are assuming that $A_{natural}^{\\pi_\\theta}(s,a)>0$ is equivalent to $\\pi_0(s,a) > \\pi_\\theta(s,a)$. If this is the case, can you elaborate this equivalence more, or better, prove it formally? If this is not the case, then in the last paragraph of Page 4, why did you say \"In other words, the positive action-masked policy considers only those actions for which the naturalness advantage is positive among the actions of the current policy\", while in Eq.(4) the positive action-masked policy is defined to only consider actions with $\\pi_0 > \\pi_\\theta$? This is just an example, there are many other places where the paper is hinting about the equivalence, to my current understanding.\n\n3. Page 4, in the last paragraph of Section 3.1, you said \"state-actions in the red area ... are undesired samples that can cause degeneration\". Any evidence that these samples have indeed *caused* the degeneration problem? As a guess, I feel you might be thinking that: actions with $A_{natural} > 0$ will have $\\pi_0 >\\pi_\\theta$, and so, for the purpose of minimizing the KL-divergence we should increase the probability of this action, which however hurts the task score as $A_{task}<0$ on this action. If this is the argument, I'm not sure that actions with $A_{natural} > 0$ indeed have (or even tend to have) $\\pi_0 >\\pi_\\theta$, which is exactly what I'm asking to prove in last question. Note that an action does not have fixed KL-penalty; when the policy changes, the KL-penalty on the same action changes too. This is in sharp difference to the task reward, which is constant for a given action regardless of the policy.\n\n4. Eq.(6), I am not sure about the equation here, can you prove it? It seems to be the central theoretical result of the paper but I can't find any justification of the equality here.\n\n5. Page 5, in the last paragraph of Section 3.3, you said your algorithm eventually outputs the negative policy after the training because it \"consists of actions with high task performance while preserving naturalness\". However, actions generated by the negative policy are actually those that we want to *minimize* their likelihood in the policy for the sake of \"high task score and naturalness\", why do we want to output them now?\n\n6. In the pseudo-code of Algorithm 1, what do you mean \"running policy\" at line 4? Do you mean rolling out the policy in the environment with initial state $s_0^p$? But in that case the given training data D will only be used to provide initial states, with all the subsequent data in the actual training separately sampled from the environment, is this correct understanding? Or do you mean re-sample a trajectory over the given dataset D using the positive or negative policy (but how)?\n\n7. Again in the pseudo-code of Algorithm 1, you are using the data from both the positive and the negative policies to train V, so V is the value function of what policy? Also, from the pseudo-code I can't find how V is used at all. I assume V will be used to estimate the advantage functions, but there are two advantage functions, $A^{\\pi^+}$ and $A^{\\pi^-}$, which are the advantage with respect to different policies, how do we use the single value function (of a unknown policy) to estimate two different advantage functions?\n\n8. In the last paragraph of Section 3.1 you said \"the proportion of samples in the green area is inevitably very low, which is inefficient in terms of sample efficiency for learning\". However, by clipping the advantage at 0 isn't your algorithm essentially only using samples in the green area? Specifically, in the first term of Eq.(6), for an action $a \\sim \\pi^+$, it must have $\\pi_0(a)>\\pi_\\theta(a)$; when such an $a$ has $A_{task}(a)<0$, its clipped advantage is 0 thus its contribution to the gradient is 0. This is equivalent to throwing away the actions in the corresponding red area, which is inefficient as you pointed out in the paper. In fact, in the same paragraph you also said \"the first term in Eq.(6) corresponds to the likelihood maximization [entry] in Table 1\", which seems to echo my reasoning above. Did you compare the sample efficiency of your algorithm with PPO and NLPO in the experiment?\n\n9. In Eq.(1), what is the $KL$ function here? It cannot be the usual KL-divergence function as KL-divergence applies to two distributions while in Eq.(1) the $KL$ function is applied to two probability values ($\\pi_\\theta(a_t|s_t)$ and $\\pi_0(a_t,s_t)$). \n\n10. In Eq.(2), $s$ is an open variable that is not averaged out, thus the equation is not really correct. Also, the objective function $J$ is not explicitly defined before it shows up in this equation. The paragraph right above this equation gives $E[\\sum_t r(s_t,a_t)]$ as the objective function, where the discount factor $\\gamma$ is dropped, in which case the policy gradient formula would be a bit more trickier than Eq.(2). Please double check.\n\n11. In Table 2, how did you choose a single \"best\" PPO/NLPO result given that there are two metrics? In Figure1 the PPO's sentiment score can be as high as 0.85. Although that particular PPO instance is at the cost of high perplexity, it does let me wonder whether there exists *other* hyperparameter of PPO that can achieve some sentiment between 0.6 and 0.8 and yet have reasonable perplexity.\n \n12. Any explanation why the 6B-model's performance (0.60- in Figure 6) is lower than the 100M-model's performance (0.62+ in Table 2) when trained with DfPO?\n\n13. Table 6, what does the \"batch size\" correspond to in Algorithm 1? N or M or something else? \n\n14. The perplexity curve on the IMDB task appears to be very flat. Is this also the case in other task? Can you show the curve for Commonsense Generation, for example? I guess the perplexity score here is over the GPT-2 outputs, did you also examine the perplexity over the reference outputs in the testing data (which should be also very natural human language), is the curve also as flat?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699557727198,
        "cdate": 1699557727198,
        "tmdate": 1699637148116,
        "mdate": 1699637148116,
        "license": "CC BY 4.0",
        "version": 2
    }
]