[
    {
        "id": "yzKWWqFasU",
        "forum": "nUGFpDCu3W",
        "replyto": "nUGFpDCu3W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_cp3K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_cp3K"
        ],
        "content": {
            "summary": {
                "value": "The paper studies where a GPT2-family model stores information representing the state of a bracketing structure and how it is retrieved. The study is based on the method of activation patching of Meng et al. The authors demonstrate that the \u201cbracketing\u201d state is persistent in the very early MLP layers and investigate the way it is retrieved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* To the best of my knowledge, the paper is the first to localise the \u201cstate\u201d of the bracketing sequence and to try understand the underlying mechanism;\n* The paper provides an interesting investigation on how the information is processed/retrieved."
            },
            "weaknesses": {
                "value": "* The title claims that the paper studies long-term dependencies, however, the actual experiments are done on very short inputs of 5 tokens. I understand there are technical difficulties related to studying longer sequences, however, I feel the title as-is misrepresents the content, so this should be resolved in either way.\n* There is a considerable body of related work that seems to be extremely relevant yet it is not mentioned at all.\n    * Studying behavior of neural nets on artificial languages (including Dyck languages) as a tool for gaining insight of their inner working and limitations, e.g. (Learning the Dyck Language with Attention-based Seq2Seq Models, Yu et al), (Feature Interactions Reveal Linguistic Structure in Language Models, Jumelet and Zuidema)\n   * Studying long-range agreement in neural nets trained on natural languages. In particular (The emergence of number and syntax units in LSTM language models, Lakretz et al) managed to localize individual cells responsible for number agreement. Specifically this paper seems to employ a related method of suppressing activations of individual neurons* .\n\n* In most of the experiments (except for the Appendix A), the paper studies only one instance of the GPT2-small, hence it is not given that the conclusions can be generalized.\n* For many years, the BlackBox/neural net-interpretability domain was experimenting with (a) natural language long-distance dependencies, and (b) nested bracketing languages. Hence, solely focusing on non-nested bracketing strings of length 5 seems to be a very toy scenario in comparison.\n* I wonder if the paper can provide any actionable take-away. For now the Conclusion mainly lists \u201csupporting the hierarchical hypothesis\u201d."
            },
            "questions": {
                "value": "I would appreciate addressing any of the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698422411110,
        "cdate": 1698422411110,
        "tmdate": 1699636345222,
        "mdate": 1699636345222,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9GbXtrF9V5",
        "forum": "nUGFpDCu3W",
        "replyto": "nUGFpDCu3W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_Un4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_Un4f"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates where is the information stored in multi-layer\nperceptrons in the case of bracketed sentences. That is, identify and\nmatch closing brackets.\nThe authors use circuit analysis to understand how GPT-2 identifies\nclosing brackets. In particular, the authors use activation patching to\ninteractively trace the contributions of individual components in the\noutput distribution. They also use embedding projections, attention\nanalysis, and linear probes.\n\nIn this case, it is shown that low-level linguistic information, such\nas brackets, is stored in the very early layers of the MLP. In summary,\nincreasingly complex information seems to be stored in progressively\nMLP layers, while simple linguistic information are stored in the\nfirst layers. They also found that the residual activation to predict\nthe closing bracket lies \"dormant\" and it is activated by later\nlayers."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors performed a thorough study with different tests\n- They show results which supports their claims\n- The work represents a step forward towards understanding more DL\nnetworks."
            },
            "weaknesses": {
                "value": "- The outcomes are not surprising as it is expected that simple\ninformation is stored in the first layer and as the information flows\ninto deeper layers, more complex relationships are established\n- In this sense, it is not very clear the significance of the reported\nresults."
            },
            "questions": {
                "value": "The authors focused mainly in short sequences and it is not completely\nclear the behavior in much larger sequences.\n\nIt is not clear why the plots in Figure 7 are very different with\ndifferent type of brackets. I was expected to see similar curves for\nall types of brackets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614029848,
        "cdate": 1698614029848,
        "tmdate": 1699636345150,
        "mdate": 1699636345150,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VKMulrrPlY",
        "forum": "nUGFpDCu3W",
        "replyto": "nUGFpDCu3W",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_5apn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3868/Reviewer_5apn"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors study how information is stored and retrieved in GPT for tasks involving syntactic dependency of sentences. Specifically, they study the examples of bracketed sentences. The authors find that early layers are responsible for storing such syntactic information."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) Authors study a simple \"probing\" task, that is easy to interpret\n2) Findings make sense, and are interesting: finding that syntactic information is stored in earlier layers"
            },
            "weaknesses": {
                "value": "1) Some of the rationale for the testing dataset used for analysis are not clear: e.g. why are n=5 sequences chosen? why is the open bracket token location fixed?\n2) The authors do not compare with other methods of finding how information is encoded in LLMs: e.g. https://aclanthology.org/P19-1356.pdf. While not quite mechanistic interpretability, such techniques also provide a method to answer the questions the authors ask.\n3) While the analysis about the residual output is interesting, it is quite short and does not seem complete to explain the phenomenon observed: can authors expand on this analysis further?"
            },
            "questions": {
                "value": "1) While the analysis about the residual output is interesting, it is quite short and does not seem complete to explain the phenomenon observed: can authors expand on this analysis further?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853723146,
        "cdate": 1698853723146,
        "tmdate": 1699636345078,
        "mdate": 1699636345078,
        "license": "CC BY 4.0",
        "version": 2
    }
]