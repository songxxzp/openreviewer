[
    {
        "id": "1j9CAtu3JA",
        "forum": "oOa3ZCtMjJ",
        "replyto": "oOa3ZCtMjJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_hvaD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_hvaD"
        ],
        "content": {
            "summary": {
                "value": "The diffusion model achieves compelling results on text-to-image tasks, but its efficiency is far from satisfactory, particularly lagging behind GAN models. Prior attempts treat CLIP as a text encoder and naively combine it with GAN, leading to undesirable images that are semantically inconsistent with text prompts. This manuscript, HSPC-GAN, leverages a prompt generator and a prompt adaptor to construct structural-semantic prompts for guiding CLIP to adjust the visual features of GAN models. Some quantitative results validate the effectiveness of such a subtle design."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The design of using structural-semantic prompts for better and more consistent visual features is somewhat reasonable."
            },
            "weaknesses": {
                "value": "1. This paper is definitely finished in a rush. The whole manuscript is full of various grammar mistakes, spelling and punctuation errors, and unnatural expressions. For example, the first sentence of the abstract is weird. It should be \u201cHow to efficiently synthesize controllable, semantically-aligned and high-quality images based on text prompts is currently a significantly challenging task\u201d. Moreover, there are a lot of spelling and punctuation errors in the sections of the introduction and experiment, making the reviewer so hard to understand.\n2. The authors hardly provide qualitative comparisons between their method and baselines. Only a few examples in Fig 1,6&7 can demonstrate the superiority of their proposed method. Actually, the experiment section should present enough synthesized images.\n3. The quantitative results listed in Table 1 show the proposed HSPC-GAN can achieve minor improvement over several baselines, in terms of FID and CLIP score. However, this manuscript always emphasizes semantic alignment, which obviously can\u2019t be reflected with such two metrics."
            },
            "questions": {
                "value": "Why does the result of GALIP appear twice in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1407/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1407/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1407/Reviewer_hvaD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1407/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698547961586,
        "cdate": 1698547961586,
        "tmdate": 1699636068734,
        "mdate": 1699636068734,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qwdM1BFNgg",
        "forum": "oOa3ZCtMjJ",
        "replyto": "oOa3ZCtMjJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_v8PS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_v8PS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new GAN model for text-to-image synthesis, called HSPC-GAN. The model is based on GALIP [Tao et al, CVPR23]. and extends this baseline with several modules like text concept mining, hierarchical semantic prompt generation, hierarchical semantic prompt adaptor, hard mining matching-aware loss. The model is evaluated on the task of text-to-image synthesis using CUB and COCO datasets. HSPC-GAN outperforms the baseline GALIP, as well as other GAN and DM baselines, in FID and CLIP scores."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed model achieves visually good performance of text-to-image synthesis. This establishes a strong and relatively light-weight GAN baseline in the task which is dominated by heavy diffusion models with much longer sampling time.\n\n- The improvement over the baseline GALIP is visible in both FID (image quality) and CLIP score (text-image alignment)."
            },
            "weaknesses": {
                "value": "- $\\underline{\\text{Presentation}}$. I think the major weakness of this submission is presentation.\n  - The whole paper dives deep into technical implementation of the proposed method, but lacks high-level explanations and discussions of contributions. What are the key issues with prior GAN-based text-to-image methods, and why the proposed method is the most efficient way to address them? What could be the alternatives for the proposed extensions? What are the limitations of using all the pre-trained modules (e.g., Text Concept Mining or Scene Graph Generator)? These questions are not answered. It is thus not clear what high-level lessons can a reader extract.\n  - The paper is full of typos, and non-standard terminology. Examples of typos include: \"pre-trainde\", \"demonstrat\", \"not not excellent\", \"adptopted\", \"As Fig 5. shown\", \"matching-mware Loss\", \"CLIP ADPTOR\", \"experiments are conduct\". I think the paper needs careful revision regarding writing. \n  - Why does \"GAN-based image synthesis\" section of the related work discuss GAN inversion? (This is not a paper about GAN inversion). On the other hand, why is there no discussion of other text-to-image GAN and DMs?\n\n- $\\underline{\\text{Novelty}}$. It is not clear what kind of technical novelty does the paper present. In particular, the method section starts from an existing baseline GALIP. Next, the method combines modules that existed in some forms in other fields, or slightly modifying the D loss, without analysing the challenges that arise during these modifications. It is not clear what is the technical novelty in the paper, what is the technical challenge that was addressed, and what is the technical lesson for the readers. \n\n- $\\underline{\\text{Unsupported claim}}$. \"This loss can be generalized to apply most generation methods involving semantic\nmatching. We will do more work in the future to prove this viewpoint.\" - This is a rather strong claim that has no confirmation in the experiments. The form of the loss is binded to the GAN-form objective and it is in fact not clear if it would generalize to any other objectives. \n\n- $\\underline{\\text{Results}}$. Given the nature of this line of research, the visual results in this paper are limited (4 images in the main paper and 2 in the supplementary). \n\n- $\\underline{\\text{Efficiency}}$. At a global level, the paper introduced several new modules on top of GALIP model. The new model achieves somewhat better FID and CLIP scores but works noticeably slower (e.g., 0.025s vs 0.017s in Fig. 1). The efficiency aspect and possible trade-offs are not discussed in the paper."
            },
            "questions": {
                "value": "Please answer the comments from the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1407/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665687271,
        "cdate": 1698665687271,
        "tmdate": 1699636068653,
        "mdate": 1699636068653,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4DiHf3y3xm",
        "forum": "oOa3ZCtMjJ",
        "replyto": "oOa3ZCtMjJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_rLWE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_rLWE"
        ],
        "content": {
            "summary": {
                "value": "This work introduces HSPC-GAN,  a novel text-to-image synthesis framework that combines GANs with CLIP models to produce high-quality images that are semantically consistent with textual descriptions. It introduces structured semantic prompts for more accurate visual feature adjustments and a new loss function for improved training efficiency. HSPC-GAN achieves state-of-the-art results, synthesizing images faster and with greater semantic relevance compared to existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It leverages the semantic alignment strengths of CLIP to enhance the quality of images generated by GANs.\nBy analyzing text to identify entities, attributes, and relationships, the model can generate images that are not only visually appealing but also semantically consistent with the input text."
            },
            "weaknesses": {
                "value": "The main concern is the effectiveness of the proposed designs.\nAs shown in the ablation study of Table 3, the results are not convincing for me.\nFor example, comparing '+Hierarchical Semantic Prompts (6)' and '+Hierarchical Semantic Prompts (6), +Prompt CLIP Adaptor', it seems Prompt CLIP Adaptor can only improve the FID by 0.09, CS by 0.0011, which is very marginal and could be neglected considering the seed randomness.\n\nBesides, it seems '+Hard Mining Matching-Aware Loss' can improve the baseline by the 0.33 FID and 0.0024 CS. But when applying it to a stronger model '+Hierarchical Semantic Prompts (6), +Prompt CLIP Adaptor', the FID and CS can still be improved 0.31 FID and 0.0022. It is not convincing for me as the improvement should be clearly narrowed with a stronger base model."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1407/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783619226,
        "cdate": 1698783619226,
        "tmdate": 1699636068577,
        "mdate": 1699636068577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FLNh6stksi",
        "forum": "oOa3ZCtMjJ",
        "replyto": "oOa3ZCtMjJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_rmDR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1407/Reviewer_rmDR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes HSPC-GAN, a method of constructing structural semantic prompts and using them to hierarchically guide CLIP to adjust visual features for generation of high-quality images with controllable semantic consistency. HSPC-GAN extracts semantic concepts through part of speech analysis, constructs a prompt generator and a prompt adaptor to generate learnable hierarchical semantic prompts, and using these prompts to selectively guide CLIP adapters to adjust image features to improve semantic consistency between synthesized images and conditional texts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This article is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "The experiments are not sufficient to prove the effectiveness of the method in the paper. For example,\n1. Compared with the GALIP paper, there is a lack of comparison results of Model Param and Data Size.\n2. Compared with the GigaGAN paper, there is a lack of comparison results of high-resolution generation."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1407/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699461641726,
        "cdate": 1699461641726,
        "tmdate": 1699636068507,
        "mdate": 1699636068507,
        "license": "CC BY 4.0",
        "version": 2
    }
]