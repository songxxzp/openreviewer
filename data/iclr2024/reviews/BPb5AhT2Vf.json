[
    {
        "id": "9CB4GdTEoP",
        "forum": "BPb5AhT2Vf",
        "replyto": "BPb5AhT2Vf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
        ],
        "content": {
            "summary": {
                "value": "Overall, the core idea of the paper is interesting, which considers leveraging diffusion networks to achieve feature enhancing in the task of image-point-cloud registration."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the core idea of the paper is interesting, which considers leveraging diffusion networks to achieve feature enhancing in the task of image-point-cloud registration."
            },
            "weaknesses": {
                "value": "1. The idea of using diffusion networks is interesting, but the way to use diffusion networks is to some extent trivial. It is not well-motivated  why you choose to use diffusion networks instead of any other pretrained feature extractor, such as those vision foundation backbones (DINO, SAM, CLIP, ...)?\n\n2. In Related Work (Image-to-point cloud registration), \"In contrast, FreeReg does not require task-specific\ntraining and finetuning and exhibits strong generalization ability to both indoor and outdoor scenes\". It seems this description is not solid, as you feature extractors (diffusion networks and depth estimation networks) are already trained on some datasets.\n\n3. The baseline comparison is a little bit weak. More recent and related works should be considered. And in Table S1, the proposed approach seems not solid outperform the counterpart 2D3D-Matr.\n[1] Corri2p: Deep image-to-point cloud registration via dense correspondence. TCSVT 2022.\n[2] EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization. ICCV 2023.\n[3] CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration. arXiv 2023.\n[4] CoFiI2P: Image-to-Point Cloud Registration withCoarse-to-Fine Correspondences for Intelligent Driving. arXiv 2023.\n[5] End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization. arXiv 2023.\n\n\n4. In Table 4, what is the performance under w=0 and w=1.0 ?\n\n5. The run time speed/memory comparison with other models is missing."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2224/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2224/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645189198,
        "cdate": 1698645189198,
        "tmdate": 1700379061178,
        "mdate": 1700379061178,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7Xqkgh6HIC",
        "forum": "BPb5AhT2Vf",
        "replyto": "BPb5AhT2Vf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a image-to-point cloud registration framework. The key idea is to generate RGB image from point cloud and reconstruct depth image from RGB so that correspondences can be established between images of the same modality. Though the image generation of both directions are well studied, a naive implementation does not work well. For this reason, the authors first generate depth image from point cloud and then use intermediate feature maps in the depth-to-image ControlNet to establish semantic correspondence with the original image. At the same time, a depth map is generated from the original image and local geometric features extracted from the depth map are combined with the semantic features for better correspondences. Experiments are conducted on three datasets, including both indoor and outdoor scenes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The idea of first generating images and point clouds from the other modality and then find correspondence in the same modality is interesting and the authors find practical ways to implement this idea.\n\n2.The performance is promising even without training on the target task with ground-truth correspondence.\n\n3. The paper is well written and the adequate ablation studies are conducted."
            },
            "weaknesses": {
                "value": "1. As mentioned by the author, inference speed is a limit of the proposed method and 11s per image is quite slow.  I hope that the author can provide their thoughts for further improvement of speed.\n\n2. Another limitation is that the performance is only comparable with the concurrent work 2D3D-MATR on the dataset RGBD-Scene-v2, while  I think that this is not a big problem and the proposed method has its own value. \n\n3. The residual rotation error seems quite large. I'd like to know what's the initial rotation error before registration?"
            },
            "questions": {
                "value": "1. Is there any way that can are readily to be tried for speed improvement?\n\n2. The residual rotation error is quite high. From a practical point of view, is the rotation error of 10 or 20 degrees a good threshold for recall?  What's the requirements of rotation accuracy in different application areas?\n\n3. The authors \"analyze\" the limitation of straightforward/direct implementations in the 4th and 5th paragraph of Introduction, but did not provide any experimental results to support supporting the conclusion.  For the task of this paper and the fusion of two kinds of features, these straightforward may also work well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719750587,
        "cdate": 1698719750587,
        "tmdate": 1699636155663,
        "mdate": 1699636155663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YF0uAyRIcr",
        "forum": "BPb5AhT2Vf",
        "replyto": "BPb5AhT2Vf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_BaLi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2224/Reviewer_BaLi"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the image-to-point-cloud registration problem. The idea is to utilize a diffusion model and ControlNet to generate diffusion features from input point cloud. For images and point clouds, the final features used for matching are composed of both a diffusion part and a geometric part in a weighted average fashion. The latter is extracted from FCGF to serve as geometric features. The pixel-to-point correspondences are then obtained by a NN matching with mutual check. Experiments show the empirical improvements in I2P matching in several benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- High-quality writing: The content is well-written, with a focus on clarity, coherence, and precision.\n\n- Improved results over baselines: The results outperform standard models, demonstrating significant enhancements in performance and accuracy.\n\n- Efficient feature distillation and cross-modality matching: Large model features are distilled effectively, facilitating feature matching across different modes for improved system performance."
            },
            "weaknesses": {
                "value": "- The paper primarily focuses on the design of an improved feature that serves as a unifying element for both the image and depth map domains. While this feature has shown remarkable efficacy within this specific context, it may not be directly transferable to other cross-modality problems. Adapting it to different cross-modality tasks would necessitate careful and tailored design to ensure its successful application.\n\n- Efficiency is indeed a concern as pointed out in the limitation section, as feature extraction via stable diffusion and ControlNet is a costly computation."
            },
            "questions": {
                "value": "1. Can the method work with only diffusion features from RGB and point clouds? e.g. Can the weighting between F_d and F_g be either 1 or 0?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827737483,
        "cdate": 1698827737483,
        "tmdate": 1699636155576,
        "mdate": 1699636155576,
        "license": "CC BY 4.0",
        "version": 2
    }
]