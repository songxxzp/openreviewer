[
    {
        "id": "WwFAhRCbxh",
        "forum": "aNuQyV30Yw",
        "replyto": "aNuQyV30Yw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_wHAP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_wHAP"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Multi-Concept Prompt Learning (MCPL) framework for simultaneously learning multiple prompts from one scene in order to address the challenge of managing multiple concepts in scenes with multiple objects. The authors conducted a motivational study to investigate the limitations of existing prompt learning methods in multi-concept settings and found that object-level learning and editing without manual intervention remains challenging. To enhance prompt-object level correlation, the authors propose regularization techniques including Attention Masking (AttnMask) and Prompts Contrastive Loss (PromptCL). Experimental results demonstrate that the MCPL framework enables enhanced precision in object-level concept learning, synthesis, editing, quantification, and understanding of relationships between multiple objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tA novel task of the Multi-Concept Prompt Learning (MCPL) framework, which enables simultaneous learning of multiple prompts from one scene. This approach addresses the challenge of learning multiple concepts within multi-object scenes, which has not been previously explored.\n\n2.\tEnhanced Object-Level Concept Learning: The proposed MCPL framework demonstrates enhanced precision in object-level concept learning, synthesis, editing, quantification, and understanding of relationships between multiple objects. This is validated through extensive quantitative analysis and evaluation of learned object-level embeddings.\n\n3.\tThe paper proposes several regularization techniques to enhance the accuracy of prompt-object level correlation. These techniques restrict prompt learning to relevant regions, facilitate disentanglement of prompt embeddings, and the use of descriptive adjective words to bind each learnable prompt. These effective techniques contribute to learning object-level information under image-level supervision."
            },
            "weaknesses": {
                "value": "1.\tThe aim of this paper is to learn and compose multiple concepts in the same scene. However, the demonstrations to prove the composing ability are insufficient. In almost all demonstrations the concepts are composed in the same string as training examples without any changes and only some editing demonstrations are available. Upon the interaction between the two concepts is changed, the effects seem to be worse.\n\n2.\tSome writing mistakes exist in the paper. In the top right of Figure 3, the labeling of \u201con\u201d and \u201cunder\u201d should be reversed. On page 6, \u201cThe\u201d in \u201cTherefore The contrastive loss\u201d should be lowercase in line 8 of the paragraph before \u201cImplementation details\u201d.\n\n3.\tThe experiment is a little confusing. In the section \u201cBaselines and experiments\u201d on page 7, the author presents four learning methods to compare their effectiveness. However, the author seems not to explain the meaning of the first setting called textural Inversion applied to unmasked multi-concept images and the subsequent experiments don\u2019t contain the effects of the first two settings. The detail and comparison for a setting called \u201cMCPL-diverse\u201d are also unavailable.\n\n4.\tThe paper constructs a new dataset to evaluate the proposed framework for multi-concept learning. However, I see all provided examples contain only two distinct concepts so I doubt the generalization of this method."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698219739876,
        "cdate": 1698219739876,
        "tmdate": 1699636054466,
        "mdate": 1699636054466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RNOEKnk97X",
        "forum": "aNuQyV30Yw",
        "replyto": "aNuQyV30Yw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
        ],
        "content": {
            "summary": {
                "value": "To tackle the issue of learning multiple individual concepts simltaneously, in this paper, the authors propose MCPL. The authors first conduct preliminary studies to show that vanilla MCPL to jointly learn multiple concepts is feasible, but it is not adequate to learn correlations between objects and locate corresponding concepts. To tackle this issue, the authors propose three techniques: AttnMask, PromptCL, and PromptCL with Bind adj.. The proposed full MCPL-one can correctly recognize and localize different concepts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed prompts contrastive loss as well as Bind adj. can effectively regularize the attention maps regarding concepts to localize onto correct position. And learning with AttnMask can also largely refine the attention mask boundary, thus reducing false positive attention values. All these methods benefit to textural inversion. \n\n2. Extensive experimental results illustrate that the proposed MCPL can effectively generate attention masks for corresponding concepts, which benefits to textural inversion task. \n\n3. The description of method and experiment section is polished and easy to understand."
            },
            "weaknesses": {
                "value": "The main concern is the analysis between MCPL-diverse mentioned in preliminary study and the full version of MCPL-one. The authors could provide visualization of generated natural concepts from MCPL-diverse as well as corresponding generated segmentation masks to support the observation."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Reviewer_cRiH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689260829,
        "cdate": 1698689260829,
        "tmdate": 1699636054392,
        "mdate": 1699636054392,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TJ0boi9ZRI",
        "forum": "aNuQyV30Yw",
        "replyto": "aNuQyV30Yw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a Multi-Concept Prompt Learning (MCPL) method that extracts multiple prompts from single images under the stable diffusion framework. A motivation study is first provided to demonstrate the current limitation. The proposed method is based on Textual Inversion and incorporates multiple regularisations (including Attention Masking, Prompts Contrastive Loss, and Bind Adjective) to disentangle multiple objects. The concepts are learned from a new dataset and evaluated by two designed protocols, followed by application visualizations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-  To achieve multi-concept extractions from a single image, the proposed method leverages novel regularisation losses without relying on any groundtruth object segmentation\n-  Overall, the paper is clearly structured and easy to follow. The motivational study introduces the problem and the current limitation in a systematic way\n-  The experiments are well-designed with both real-world categories and out-of-domain biomedical images involved during the evaluation\n-  Comprehensive analysis is conducted with t-SNE visualizations and embedding similarity evaluation."
            },
            "weaknesses": {
                "value": "- Some recent works (such as Break-A-Scene) on similar tasks could also be mentioned in the related work section. On the other hand, though it would be a bit unfair to directly compare with Break-A-Scene (due to its given segmentation inputs), it could still be interesting to treat its performance as an upper bound and comment on how a good segmentation mask would affect the learned concepts.\n- More implementation details could be added, particularly on prompt initialization. It seems a bit unclear how to initialize all learnable embeddings by the same word, \u201cphoto\u201d in a random manner. Besides, one may be curious about how the number of prompts is determined, especially for MCPL-all.\n- It would be better to discuss the limitations of current work in the last section and point out some future improvement directions."
            },
            "questions": {
                "value": "- It seems that all learned concepts (nouns) are associated with a pre-defined adjective description in the prompt (such as \u201ca green *\u201d). Are these adjectives playing roles in disentangling the concepts? What if the initial prompt is provided in the form \u201cA * and a @\u201d?\n- I wonder if there are any examples of cases when the number of objects in the image is mismatched with the number of learnable concepts"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Reviewer_RxXV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785552329,
        "cdate": 1698785552329,
        "tmdate": 1699636054313,
        "mdate": 1699636054313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N7K0FLDvfd",
        "forum": "aNuQyV30Yw",
        "replyto": "aNuQyV30Yw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method to associate new words into concepts. The proposes three techniques: attention masking, prompts contrastive loss, and bind adjective. The applications they adopted is the image synthesis / editing when replacing some of the original concepts in a sentence, with a different concept (i.e. image editing over disentangled concepts). They also claim to introduce a novel dataset for this application."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- They claimed to release code and dataset upon publication.\n - The paper targets important research areas."
            },
            "weaknesses": {
                "value": "- The paper is not very clear to read. The idea seems to be straightforward, but the description of the method is a bit ambiguous. I have to read multiple times to make sure I understand the method accurately.\n - In experiments, the authors show multiple interesting qualitative results. However, there are very little quantitative results, and it is very hard to compare with other methods and understand the contribution of this effort."
            },
            "questions": {
                "value": "Attention masks and contrastive loss on different concepts seems to be a widely used method. It would be great if the authors can explain a bit more about the novelty of their work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1277/Reviewer_BYgx"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699259972679,
        "cdate": 1699259972679,
        "tmdate": 1699636054257,
        "mdate": 1699636054257,
        "license": "CC BY 4.0",
        "version": 2
    }
]