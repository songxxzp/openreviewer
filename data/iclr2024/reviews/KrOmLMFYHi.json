[
    {
        "id": "jfkfkqzcTM",
        "forum": "KrOmLMFYHi",
        "replyto": "KrOmLMFYHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_gfYi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_gfYi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the concept of incorporating gaze information (referred to as human attention) into VLM models to enhance their performance and potentially improve their interpretability. To support this, the authors collected hundreds of minutes of gaze data and developed an automated data annotation pipeline using GPT-4 to generate the VOIA-COCO dataset.\n\nThe paper makes a valuable contribution to the relevant academic community by demonstrating the significance of gaze information for VLM tasks, such as visual question answering. The presented results show that the proposed method is qualitatively and quantitatively superior to baseline models like Otter."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022\tThe authors introduce the novel concept of utilizing gaze information in the development of VLMs.\n\u2022\tUnlike baseline and several other studies, the experimental analysis is not limited to qualitative results but also demonstrates quantitative results."
            },
            "weaknesses": {
                "value": "1.\tThe paper's presentation is lacking. Many important sections have been relegated to the appendix, especially the technical details. For example, Section 4.1 is challenging to understand due to the limited text. 2)The model heavily depends on the baseline model Otter. The method of injecting gaze information is quite straightforward. The way in which the authors handle catastrophic forgetting can be observed in the literature, thus not introducing technical novelty.\n2.\tThe experimental analysis appears somewhat unfair because the proposed method uses additional modalities to achieve the same results. Therefore, its better performance is not surprising, particularly in cases where the query does not clearly define the object's name, and several other objects are present in the scene, with the gaze heatmap aligning with the queried object. It is also worth to mention that both baseline methods are still only in ArXiv.\n3.\tThere is uncertainty about the cases in which gaze information was found to be less relevant.\n4.\tThe caption for Figure 3 lacks informativeness.\n5.\tThere exist a few typos to be fixed, e.g., Fiture\n6.\tHallucination issue can be better presented qualitatively and better discussed.\n7.\tGaze data collection procedure is also very scarse and it is not possible to understand if the annotators have a reliable consensus to use the collected data in model evaluation and comparisons.\n8.\tIt is doubtful whether 100 gaze samples are sufficient for conducting a comprehensive comparative study. I have reservations about the potential bias in the collected dataset."
            },
            "questions": {
                "value": "\u2022\tSection 4.1 and gaze data annotation should be described in detail. It is not possible to validate the procedures perform in these context.\n\u2022\tWeakness Q4\n\u2022\tPls. comment on Weakness (2) for the technical novelty of the method.\n\u2022\tPls. comment on Weakness (3).\n\u2022\tHow the authors evaluate the interpretability? Several places in the paper interpretability was mentioned, however its evaluation is unclear given that such a keyword is being used in several different content of AI.\n\u2022\tWeakness Q9"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698053620638,
        "cdate": 1698053620638,
        "tmdate": 1699636047070,
        "mdate": 1699636047070,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yYqg9KpiRN",
        "forum": "KrOmLMFYHi",
        "replyto": "KrOmLMFYHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
        ],
        "content": {
            "summary": {
                "value": "This paper presents \"Voila-A,\" a novel approach aimed at aligning Vision-Language Models (VLMs) with user gaze attention. The authors highlight the challenges faced by existing VLMs in handling complex scenes and diverse human attention patterns. They propose utilizing gaze information collected from AR or VR devices as a proxy for human attention to guide VLMs.\n\nThe paper provides a thorough explanation of the methodology, including data collection, automatic data annotation using GPT-4, and the design of the Voila Perceiver modules. The authors conduct experiments, comparing Voila-A with baseline models (Otter and Kosmos-2) on both synthesized and real-world gaze datasets.\n\nThe results demonstrate the effectiveness of Voila-A, showcasing its balanced capability between helpfulness and fact grounding. The evaluation metrics, including GPT-4 Ranking and Reward Score, support the authors' claims. Additionally, ablation studies and qualitative analyses provide further insights into the model's performance and capabilities.\n\nOne notable contribution is the introduction of trace data as a proxy for gaze data, offering a cost-effective and scalable alternative for aligning VLMs with user gaze attention. The method of transforming trace data to mimic gaze data is well-described and substantiated with empirical evidence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method for aligning gaze data with trace data proves to be both effective and straightforward. It introduces a fresh approach to integrating gaze information with Vision-Language Models (VLLM). The approach has been rigorously examined through studies, yielding results that substantiate its efficacy."
            },
            "weaknesses": {
                "value": "The data collection section (4.1) lacks detailed information on the methodology and content of the dataset. Providing specific examples and clearer explanations would enhance comprehension. Additionally, Figure 3 needs a caption and more in-depth explanations to convey its intended message. The figures also need higher resolution for better readability when printed.\n\nSection 4.2 requires clearer explanations, particularly regarding the parameters X, L, and G. The concept of 'latent data' (L) needs better elucidation. A structured approach, starting with an explanation of the inputs and employed encoders, followed by a deep dive into the new approach, would enhance clarity. A comprehensive figure illustrating how gaze is integrated into visual language models would be beneficial. It's unclear how the 'Voila Perceiver Resampler' module is integrated with the VLLM.\n\nIn Section 4.3, the meaning and specifics of 'gaze linear layer' and 'gaze key linear of attention' need clarification. It's not clear which layers these terms refer to or if there's a specific formula involved.\n\nMerging Section 5.1.2 with Section 4.1 would improve the overall clarity of the paper.\n\nThe summary of the main results in Figure 5 is not easily understandable. Using a table with percentages might provide clearer insights.\n\nThe claim that Voila exhibits a 'superior ability to handle multi-turn real conversations' in the last sentence of Section 5.3 needs stronger support or clarification in the results section."
            },
            "questions": {
                "value": "Could you provide a clearer depiction of how the ViolapercieverBlock and Resampler are integrated within the larger VLLM framework? A simplified architectural overview would be immensely helpful in understanding the bigger picture.\n\nIt would be beneficial to have more details on what exactly is included in the automatic data annotation process and how it is carried out. Providing specific examples in the main paper would greatly enhance comprehension.\n\nFor Figure 4 and 5, additional guidance on how to interpret the results would be appreciated. Specifically, clarification on what constitutes the 'Overall score' and a detailed explanation of how the 'Helpfulness' and 'Grounding score' are calculated would be invaluable.\n\nI would also onsider providing a brief discussion of potential applications and future directions in the conclusion section.\nClarify any specific limitations or potential challenges associated with the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Reviewer_2Muy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768265738,
        "cdate": 1698768265738,
        "tmdate": 1699636046993,
        "mdate": 1699636046993,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cyuvHIGOZD",
        "forum": "KrOmLMFYHi",
        "replyto": "KrOmLMFYHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_428q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_428q"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a way to integrate gaze information into large VLMs. It uses mouse trace data as a proxy for gaze track with proper sampling. Such gaze information is then used in an attention mechanism to enhance visual feature perception. The authors report that the proposed approach outperforms baselines."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Introduces a scalable way to leverage human attention cue in VLM models."
            },
            "weaknesses": {
                "value": "* Technical or scientific contribution is very incremental and limited.\n* Writing can be improved; not always easy to follow and clear. \n* Baseline methods considered are not comprehensive or fair. Mouse trace data as a proxy for gaze sounds reasonable but there are many off-the-shelf saliency model that are designed to mimic human gaze. Some of the existing saliency model can be used or at least need to be discussed and reviewed in the paper."
            },
            "questions": {
                "value": "Please address the comments above regarding baseline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819817000,
        "cdate": 1698819817000,
        "tmdate": 1699636046890,
        "mdate": 1699636046890,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZvrbaW8aoC",
        "forum": "KrOmLMFYHi",
        "replyto": "KrOmLMFYHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
        ],
        "content": {
            "summary": {
                "value": "In AR/VR scenarios, gaze is one of the most natural way to represent the regions interesting to users. This paper studied an interesting problem: suppose we are using vision-language model under AR/VR, how to incorporate human gaze attention into vision-language model and how much improvement can it bring? The paper proposed to use mouse trace to approximate gaze and use the collected gaze heatmap into attention mechanism in vision language models (Otter) while freezing the language encoder MPT-7B and vision encoder CLIP ViT-L/14. The models is evaluated on the collected Voila-COCO data set and a VOILA Gaze data which is more close to real life scenarios. The proposed method with extra gaze information outperforms baselines Otter and Kosmos-2. Ablation study also shows that gaze heatmap is better than alternatives ways to use gaze data like discrete gaze position, gaze bounding box as patch, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "--The idea of including gaze information to vision-language model is quite interesting, which might be one important aspect when people use the vision-language model in VR/AR scenarios. The idea of human using gaze/attention in compute vision models is not new, but the idea of using gaze/attention to improve vision-language model is relatively novel to the best of my knowledge.\n\n--Some interesting experiment results are shown to demonstrate that the gaze/attention data are helpful for VQA tasks of vision-language models."
            },
            "weaknesses": {
                "value": "--Will the data set VOILA-COCO be released? I did not see this information in the paper. \n\n--Using mouse trace to approximate human gaze/attention is a popular approach in attention related area, however, the authors does not mention existing works like BubbleView https://bubbleview.namwkim.org/ or Salicon http://salicon.net/\n\n--The organization and presentation of the paper can be improved. It is not clear how the gaze data will be used in vision-language model until section 4.3. Instead, the authors can provide an illustrator figure about it at the beginning."
            },
            "questions": {
                "value": "See the weakness part. Especially, the dataset might be an important contribution of this paper. However, it is not clear whether the data set will be released or not."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Human gaze data are collected and used."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1204/Reviewer_tDrZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1204/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699292050922,
        "cdate": 1699292050922,
        "tmdate": 1700683349917,
        "mdate": 1700683349917,
        "license": "CC BY 4.0",
        "version": 2
    }
]