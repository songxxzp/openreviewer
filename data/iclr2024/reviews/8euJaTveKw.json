[
    {
        "id": "OSSEF7fGgI",
        "forum": "8euJaTveKw",
        "replyto": "8euJaTveKw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_WpqT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_WpqT"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces PROMETHEUS, an open-source large language model (LLM) that aims to provide evaluation capabilities on par with the proprietary GPT-4. To achieve this, the authors create a new dataset called FEEDBACK COLLECTION, containing diverse and fine-grained user assessment criteria. PROMETHEUS is trained using this dataset and demonstrates a strong correlation with GPT-4's evaluation capabilities, as well as human evaluators.\n\nThis paper addresses the limitations of using proprietary LLMs like GPT-4 for evaluation, such as closed-source nature, uncontrolled versioning, and prohibitive costs. The PROMETHEUS aims to offer an alternative that is open-source, reproducible, and cost-effective. The FEEDBACK COLLECTION dataset allows the model to generalize to various evaluation preferences and real-world scenarios. In tests, PROMETHEUS outperforms other baselines and shows potential as a universal reward model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The organization of this paper is well-structured, making it easy to read and comprehend.\n2. This work presents the creation of a FEEDBACK COLLECTION dataset, which encompasses a diverse range of scoring criteria, reference answers, and feedback. Based on this, an evaluation LLM is trained for assessing the text generated by large language models.\n3. The analysis in this work is thorough, discussing the selection of base models, data construction, and demonstrating the importance of reference answers. This provides valuable insights for the evaluation of large models in the field."
            },
            "weaknesses": {
                "value": "1. The main contribution of this paper is the FEEDBACK COLLECTION dataset. However, the dataset has not been made publicly available, and the construction details are unclear. For instance, the content of Step 2 is incomplete, and Step 3 is overly simplistic. Furthermore, the prompts used during construction have not been disclosed.\n2. Assessing the consistency of scores alone is insufficient; it is also necessary to evaluate the feedback corresponding to these scores. On one hand, it is important to determine whether the feedback aligns with the scoring criteria. On the other hand, it should be examined if the feedback can be appropriately matched with the given scores. In fact, humans are not solely interested in obtaining a score; they are more concerned with the feedback associated with that score, which can further guide the large language model to generate desired answers.\n3. It is unclear whether the test data in Table 1 is manually constructed or generated by GPT4-0613. If it is generated by GPT4-0613, why are the Pearson/Kendall/Spearman evaluation metrics not equal to 1?\n4. For the Unseen FEEDBACK COLLECTION Testset, should all unseen instances be extracted? If the evaluation is conducted by combining the 50 Unseen samples with the 1000 samples similar to the training distribution, would this overshadow the true performance when facing unseen distribution during training?"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835981372,
        "cdate": 1698835981372,
        "tmdate": 1699636851479,
        "mdate": 1699636851479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0U8KiCwQwC",
        "forum": "8euJaTveKw",
        "replyto": "8euJaTveKw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.\n2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.\n3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.\n4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution."
            },
            "weaknesses": {
                "value": "1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. \n\n2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?\n\n3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7178/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7178/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7178/Reviewer_LTFo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842866815,
        "cdate": 1698842866815,
        "tmdate": 1699636851369,
        "mdate": 1699636851369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U4HwynR7s1",
        "forum": "8euJaTveKw",
        "replyto": "8euJaTveKw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_B7Vr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7178/Reviewer_B7Vr"
        ],
        "content": {
            "summary": {
                "value": "Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Proposes open-source LLM for evaluation"
            },
            "weaknesses": {
                "value": "Model implementation is not described.\nExperimental methodology not clear or supported.\nMost figures missing.\nContribution too small (not any new data, model or any advertised contribution is clearly described).\nData is synthetic and not corrected by humans for any potential errors."
            },
            "questions": {
                "value": "Where is Figure 2?\nWhere is Figure 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888977685,
        "cdate": 1698888977685,
        "tmdate": 1699636851267,
        "mdate": 1699636851267,
        "license": "CC BY 4.0",
        "version": 2
    }
]