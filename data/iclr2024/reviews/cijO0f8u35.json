[
    {
        "id": "pTfCZ5WcRw",
        "forum": "cijO0f8u35",
        "replyto": "cijO0f8u35",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_J1oN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_J1oN"
        ],
        "content": {
            "summary": {
                "value": "The authors study the effects of pretraining loss, supervised training data amount, and augmented data amount on reasoning performance of tuned large language models. They claim that pretraining loss is negatively linear correlated to the fintuned and in-context learning performance and serves as a more effective performance metric compared to the size of the pretrained model or the amount of pretraining tokens. \n\nA Log-linear trend is reported between training data size and model performance. \nThe authors also propose to augment the training data using rejection sampling. Essentially, generating multiple answers for each query in the training data and augmenting the dataset using those pairs in which the final answer is correct according to the ground truth. Augmenting the data through rejection sampling from multiple model sizes further improves the reasoning performance of smaller models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors did a great job analyzing multiple factors such as, pretraining loss, finetuning data amount and augmented data amount affecting the reasoning performance of the LLM. It shed some light on important factors contributing to making LLMs more proficient in math reasoning.\n\nThe introduction and study of filtering the rejection sampling dataset to encourage distinct reasoning path amounts in the augmented data, and its effects on the rejection sampling finetuning performance is an interesting idea."
            },
            "weaknesses": {
                "value": "The pretraining loss reported are associated with various pretraining datasets and distinct tokenizers, making a direct comparison challenging. \n\nGiven that the x-axis in Figure 1 represents the pretraining loss of models of varying sizes, the scaling relationship appears questionable. Wouldn\u2019t it make more sense if the x-axis was the loss of the finetuned models? \n\nThe primary concern I have with the study is that the authors do not study different pretraining losses of the same model (identical size and training tokens). Their assertion is that as the pretraining loss decreases, the tuned and ICL performance improve linearly within a certain range. However, it\u2019s important to note that this improvement is not based on the models of the same size or same amount of pretrianing data."
            },
            "questions": {
                "value": "When combining the rejection sampling data from multiple model sizes do you filter out the generations which do not have a different equation set across model sizes?\n\nWhat is the value of `k` in Table 3 for `maj1@k` for LLaMA family of models? It would be nice to include it somewhere either in the table or its description.\n\nThe claim \u201cas the pretraining loss decreases, the SFT and ICL performance improve linearly within a certain range\u201d appears somewhat unclear and could benefit from additional explanation.\n\nPlease refer to the question and concern in the weakness section as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845593418,
        "cdate": 1698845593418,
        "tmdate": 1699636051926,
        "mdate": 1699636051926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T76g6bcA1o",
        "forum": "cijO0f8u35",
        "replyto": "cijO0f8u35",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_zwMw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_zwMw"
        ],
        "content": {
            "summary": {
                "value": "They present an analysis of how performance on the gsm8k dataset improves with models scale / finetuning data. They find that across many models the model's pretraining loss is highly correlated with its performance on gsm8k. They also find that as pretraining loss improves, the benefits from supervised finetuning (SFT) diminish, and models with lower pretraining loss require more finetuning data to surpass the model's in-context-learning performance. Next they study how rejection sampling finetuning (RFT) improves with model scale. The idea is to sample a bunch of outputs from the model and then finetune on the correct samples. They also filter correct outputs that have similar reasoning paths. They find that RFT generally outperforms SFT up to 13B parameters, and benefits less at 33B parameters. Increasing the number of samples for RFT generally improves performance, but less than increasing the size of the finetuning dataset. Lastly they study combining RFT datasets from multiple models, finding that it improves over RFT from a single model up to 13B parameters. Overall by combining RFT data from multiple models they are able to obtain 7B and 13B LMs that are competitive with models that have many more parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Their analysis of the scaling effects of SFT and RFT, in particular the finding that models with lower pretraining loss benefit less from finetuning, are indeed interesting and will contribute to improving our understanding of LM finetuning.\n* The question of how LM reasoning improves with model scale is a highly relevant and important question.\n* They run a fairly thorough set comparisons and ablations across different finetuning settings, LMs, data amounts. And they do a good job of characterizing how performance on gsm8k changes as each of these parameters are adjusted."
            },
            "weaknesses": {
                "value": "* Their analysis of how pre-training loss correlates with gsm8k performance is pretty questionable because the loss numbers are for models trained on different datasets and thus are not comparable. While they admit this in the paper, it would be much better if they could make these numbers more comparable. One way to fix this would be to evaluate the loss of all the language models on some standard dataset.\n* They only consider gsm8k in this work. It would improve my confidence in their results if they also included other tasks, such as MATH.\n* Their RFT method is nearly a special case of STAR [cite], with the only difference being that they also filter reasoning paths for diversity. It therefore is unclear to me to what extent this could be claimed to be a new method."
            },
            "questions": {
                "value": "* Some of the experiments are shown for up to 70B and others only up to 13B. Would it be possible to present results for all models sizes for all of the different scaling figures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1254/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1254/Reviewer_zwMw",
                    "ICLR.cc/2024/Conference/Submission1254/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698882243872,
        "cdate": 1698882243872,
        "tmdate": 1700518071564,
        "mdate": 1700518071564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C2K3369lvo",
        "forum": "cijO0f8u35",
        "replyto": "cijO0f8u35",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_8Ko8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_8Ko8"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the scaling relationship between the math reasoning performance of a supervised fine-tuned LM and pre-training loss, supervised data amount, and augmented data amount. The authors find the pre-training loss is negatively linearly related to the performance while the supervised data amount is log linearly related to the performance. The augmented data amount has a weaker effect than the supervised data amount. All these effects diminish when the LM size increases. The authors also propose a rejection sampling fine-tuning \n(RFT) technique, which uses LMs to augment multiple reasoning chains for one training example, to improve the math reasoning performance of an LM. All experiments are performed on the GSM8K dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experiments are solid and convincing. \n2. The paper is well-written and easy to follow.\n3. The proposed RFT significantly improves the performance on GSM8K."
            },
            "weaknesses": {
                "value": "1. While the experiments are pretty convincing, the final conclusion does not seem to be very surprising or bring new insights. We already know the importance of model size, pre-training loss, and data amount. Since we are not going to design and pre-train a new LM at fine-tuning time, which requires predicting larger LM's performance from smaller ones, I'm not sure why a fine-tuning scaling law would be useful. It just suggests people use the strongest existing pre-training LM, which is exactly what people are doing right now. Maybe the authors can convince me of this during the rebuttal.\n2. The proposed RFT feels disconnected from the scaling law experiments. i.e. It does not seem to need to be inspired by the discovered scaling law. RFT basically suggests augmenting the fine-tuning data with LM-generated reasoning paths and rejecting the ones ending with wrong answers. I don't think the idea of data augmentation needs to be inspired by scaling law.\n3. Another concern about this paper is that all experiments are limited to one small dataset, GSM8K, which only has 7K+ training examples. Given the scale of the experiments, I can understand this experimental choice. However, I'm still wondering if all the conclusions (e.g. linear with pre-training loss, log-linear with supervised data size) still hold on other math world datasets."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699193400020,
        "cdate": 1699193400020,
        "tmdate": 1699636051784,
        "mdate": 1699636051784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sk5Yowt4WH",
        "forum": "cijO0f8u35",
        "replyto": "cijO0f8u35",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_voeM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1254/Reviewer_voeM"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the scaling relationship of factors influencing the mathematical reasoning abilities of large language models (LLMs) through supervised fine-tuning (SFT), in-context learning (ICL), and rejection sampling fine-tuning (RFT) techniques. The authors analyze pre-training losses, the amount of supervised data, and the amount of augmented data and find that pre-training loss has a linear correlation with SFT and ICL accuracy within a certain range. The paper demonstrates that SFT performance has a log-linear relationship with the amount of supervised data, while RFT performance benefits from the increase of distinct reasoning paths. Finally, by combining rejection sampling from multiple models, the authors achieve significant accuracy improvements in multiple LLaMA models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clear, well-organized, and addresses a relevant and important research area.\n2. An investigation of how pre-training losses, the amount of supervised data, and the amount of augmented data influencing LLM performance in mathematical reasoning tasks is provided.\n3. The paper proposes a new method (RFT) that leverages rejection sampling to generate additional supervised data for improving model performance."
            },
            "weaknesses": {
                "value": "1. The paper only does evaluation on 1 dataset, limiting its generalizability to other multi-hop reasoning problems.\n2. Although the approach is intuitively simple, the amount of computational overhead added makes this approach not appealing.\n3. The relationship between downstream task performance and pretraining loss has been well-established since the introduction of BERT. So the results in this paper is not really novel."
            },
            "questions": {
                "value": "1. Do the authors think the findings of this paper could be generalized and extended to other domains beyond mathematical reasoning tasks? Do the authors expect similar scaling relationships and performance improvements to hold in other LLM applications? Would be nice to see this supported with additional experiments.\n2. In the RFT experiments, did the authors observe any instances where the model performance deteriorated or the improvement was insignificant? If so, could you elaborate on such cases and provide possible explanations?\n3. Can the authors show similar results with other models than just llama?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699214828180,
        "cdate": 1699214828180,
        "tmdate": 1699636051708,
        "mdate": 1699636051708,
        "license": "CC BY 4.0",
        "version": 2
    }
]