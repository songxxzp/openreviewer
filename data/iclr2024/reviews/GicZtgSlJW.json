[
    {
        "id": "Fru2jc5ts1",
        "forum": "GicZtgSlJW",
        "replyto": "GicZtgSlJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g"
        ],
        "content": {
            "summary": {
                "value": "This paper formulates the no-forgetting objective of Continual-Learning (CL) as a constrained optimization problem w.r.t the population risks. Given the forgetting tolerance $\\epsilon_{1:T}$, it focuses on two important aspects of the memory-based methods: 1. how to partition the memory buffer for different tasks. 2. For each task, which subsamples should be stored? The first point is addressed by deciding the sample size of each task through minimizing the generalization gap weighted by the optimal dual variables of the CL objective. The second is to select the samples with the highest associated per-sample dual variable from each task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and the motivation is clear.\n2. Relating the generalization gap with the dual variables to obtain the optimal memory partition in CL is novel to me. \n3. Experimental results validate the effectiveness of the proposed method compared to previous memory-based approaches."
            },
            "weaknesses": {
                "value": "My primary concerns lie in the following aspects:\n  * The convergence of $\\mathbf{\\lambda}$ is highly sensitive to the setting of the forgetting tolerance $\\epsilon$, the number of tasks $T$, and the hardness of the tasks, which will affect the memory partition.\n  *  At every timestep, the memory partition changes. Not just the problem mentioned in the discussion exists, where the optimal partition size of a previous task grows at the current timestep. For the tasks that have a smaller size at the current timestep, it needs to reselect the samples to store, which would cause additional computation costs.  \n  * The growing and large number of constraints."
            },
            "questions": {
                "value": "Please see the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6337/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_aK2g",
                    "ICLR.cc/2024/Conference/Submission6337/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698008383791,
        "cdate": 1698008383791,
        "tmdate": 1700540846248,
        "mdate": 1700540846248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ky8lQo6AJR",
        "forum": "GicZtgSlJW",
        "replyto": "GicZtgSlJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_CG5c"
        ],
        "content": {
            "summary": {
                "value": "This paper views continual learning as a constrained learning problem: to learn the new task without forgetting the old tasks (too much). Some previous work took this perspective as well, but in those cases this way of formulating the continual learning problem only motivated the proposed approach. In this paper, the authors directly address continual learning as a constrained learning problem by making use of recent advances in Lagrangian duality as tool address constrained optimization. In particular, the paper demonstrates that by adopting such a primal-dual method, a principled approach emerges for deciding how to fill the memory buffer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "As far as I am aware, this is the first work that directly addresses continual learning as a constrained learning problem. The paper proposes a principled framework for this by means of optimizing the Langrangian empirical dual, and it provides clear theoretical justification for its propositions.\n\nA neat theoretical demonstration of the paper is showing that the Lagrangian dual variables can be interpreted as signaling the difficulty of their corresponding task.\n\nThe paper then demonstrates that the Lagrangian dual variables can be used to select which samples to store in the memory buffer, and that empirical benefits can be obtained by doing so."
            },
            "weaknesses": {
                "value": "Although I think this paper already makes some important and neat contributions, to realize its full potential, I think it is important to improve and clarify the empirical comparisons.\n\n**Indirectness of empirical comparisons**\n\nIn my opinion, from a practical perspective, this paper proposes three \u201cnovel aspects\u201d compared to the standard experience replay approach that is commonly used in continual learning:\n\n{1} the weighing of the replayed losses relative to the loss on the current task is determined by the Lagrangian dual variables (rather than, as is currently done in continual learning, either by a hyperparameter or as a function of how many tasks have been seen so far)\n\n{2} the selection of samples to be stored in the buffer at the task level (buffer partition)\n\n{3} the selection of samples to be stored in the buffer at the sample level\n\nHowever, it seems only the impact of the last two aspects are evaluated empirically. Why do the authors not include a direct comparison to assess the effect of {1}? (That is, a comparison between \"standard ER\" and the approach proposed by this paper except without buffer partition at task level or individual sample selection.) I think doing so could substantially strengthen this paper. Moreover, it is not clear to me whether the comparisons to assess the effect of {2} are direct. For example, in Figure 1 (but a similar question applies to Figure 4), when \u201cPDCL\u201d is compared with \u201cReservoir\u201d, it is not clear how the replayed losses are weighed in the case of \u201cReservoir\u201d. Are they weighed in the same way as in \u201cPDCL\u201d? Or are they weighed in another way? This should be clearly described. If it is the second option, then I do not think that Figure 1 provides a comparison that \u201cisolates the effect of buffer partition\u201d.\n\n**Distinction task- versus class-incremental learning**\n\nThe way the paper describes the difference between task- and class-incremental learning suggests that the authors *train* their models in these two scenarios in the same way, and that there is only a difference between these scenarios in the way the models are *evaluated* at test time. Is this indeed the way the authors implemented their experiments? Because to me it seems there should also be a difference in how models are trained in task-incremental versus class-incremental learning. For example, when training on samples from the second task, with task-incremental learning the models only need to be trained on distinguishing between classes from the current task, while with class-incremental learning the models should also learn that those current samples do not belong to classes from the first task. To clarify this, the authors should provide more details regarding how they implemented the difference between task- and class-incremental learning. When discussing the distinction between task- and class-incremental learning, I think it is also important to cite the original paper (van de Ven et al., 2022; https://www.nature.com/articles/s42256-022-00568-3).\n\n**Minor issues:**\n- top of p9: a reference is made to Figure 9, but I think Figure 4 might be meant?\n- in the reference list, the paper Buzzega et al. (2020) is included twice\n- for a number of papers in the reference list, no venue is included (e.g., Gentile et al., 2022; but there are several others as well)\n- there are several formatting issues with in-text citations in the Appendix\n- on p19, Task Incremental Learning is abbreviated as CIL"
            },
            "questions": {
                "value": "Although I think this paper already makes some important and neat contributions, to realize its full potential, I think the authors should [1] include empirical comparisons that more directly assess the impact of the three different novel aspects that the authors propose, and [2] provide more details regarding the difference between the task- and class-incremental learning experiments.\n\nPlease see under \u201cWeaknesses\u201d for details on both.\n\nWhile I think it is already a paper that could be accepted, if these two issues can be satisfactorily addressed, I think it could be a strong or very strong paper.\n\nI would be happy to actively engage in the discussion period."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662605995,
        "cdate": 1698662605995,
        "tmdate": 1699636697552,
        "mdate": 1699636697552,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6aONVutOXV",
        "forum": "GicZtgSlJW",
        "replyto": "GicZtgSlJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
        ],
        "content": {
            "summary": {
                "value": "This work directly leverages the constrained optimization framework to solve a continual learning problem. \nBased on the renowned sensitivity analysis with Lagrangian dual variables, this work tackles the continual learning problem in two different aspects, at the level of tasks and data. \n* At the task level, the Primal-Dual Continual Learning (PDCL) algorithm allocates more datapoints to task that is sensitive to constraint perturbation (i.e. large per-task dual variable)\n* At the data level, their indirect sample selection algorithm prefers to choose datapoints that are sensitive to constraint perturbation (i.e. large per-datum dual variables)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors carefully motivate the readers to understand Lagrangian sensitivity analysis and its application in the context of continual learning. \n* Their experiments show that the idea of Lagrangian sensitivity analysis can be effectively applied to both buffer allocation and data selection for memory-based continual learning."
            },
            "weaknesses": {
                "value": "* **Regarding theoretical contributions**\n  - In abstract, it is claimed that there are sub-optimality bounds. At first glance, I was expecting learnability guarantee (e.g., PAC) for the actual continual learning problem. However, it turns out that the sub-optimality bound was for estimation of dual variables. Since the estimation of dual variables is the main spirit of the proposed algorithm, I don\u2019t want to say this is not an enough contribution. Rather, I would say the expression \u2018sub-optimality bound\u2019 is quite misleading in some sense.\n  - The paper defers the discussion on the *strong* concavity constant $c$ to Appendix A.5. However, I think this hides several important dependencies. For example,\n    1. It intrinsically assumes the usage of (might be a large amount of) weight decay to induce strong concavity of the objective function;\n    2. The loss function should be $G$-smooth, and the sub-optimality bound in Theorem 4.2 turns out to be depends quadratically on $G$.\n    3. The constraint Jacobian (question: what is it exactly?) must be full rank, and the sub-optimality bound depends quadratically on the inverse of minimum singular value of this matrix, which can be arbitrarily large.\n\n    For these, I think the paper should be more clear and honest on several hidden dependencies.\n  - The last paragraph of Section 4 claims that the weakness of the sub-optimality bound \u201ccan be fixed by replacing the minimum with the average sample complexity\u201d, but I cannot find any detailed discussion on this, throughout the paper.\n  - Although the proof would be similar to that of Theorem 3.2, I think the full proof of Proposition 5.1 should be added, or at least a set of necessary modifications in the proof to prove the proposition must be added.\n* **Regarding Theorem 3.2 and the notation \u201c$\\partial P^{\\star}_t (\\epsilon_k)$\u201d**\n  - Is $\\partial P^{\\star}_t (\\cdot)$ a convex function? I think this should be clarified in order to use the notion of sub-differential.\n  - Also, I think the notation is quite confusing. I would like to suggest the notation like \u201c$\\partial_{\\epsilon_k} P^{\\star}_t (\\epsilon)$\u201d where $\\epsilon = (\\epsilon_1, \u2026, \\epsilon_t)$. \n  - In a higher level of discussion, does the paper ever require such a **local** sensitivity result to give a motivation?\n* **There are several but minor typos and misleading usages of symbols:**\n  - Equation $(P_t)$: I think this should be $\\min_{f\\in\\mathcal{F}}$, not $\\arg\\min_{f\\in\\mathcal{F}}$. This also applies to the equation at the beginning of Appendix A.2.\n  - In Assumption 2.1, $\\delta$ is used for task similarity. Throughout Section 4, however, $\\delta$ is used as a probability parameter.\n  - Assumption 2.4: \u201cThere exists $R, M >0$ such that \u2026\u201d\n  - Page 3, below Equation $(1)$: \u201c\u2026 two-player gamer \u2026\u201d $\\rightarrow$ \u201c\u2026 two-player game \u2026\u201d\n  - Equation $(3)$: Why do we need an inner product between two scalars $-\\lambda_k^\\star$ and $\\gamma$? I don\u2019t think this is necessary.\n  - Proposition 4.1: The order 2.3 and 2.2 must be flipped.\n  - Theorem 4.2: \u201c$\\\\|\\lambda\u2019\\\\|_1 = \\max\\\\{\\\\|\\lambda_p^\\star\\\\|, \\\\|\\hat{\\lambda}_p^\\star\\\\|\\\\}$\u201d $\\rightarrow$ Are all the norms $\\ell_1$-norms?\n  - Page 7, below Equation $(6)$: \u201c$\\mathfrak{B}_t(x,y) \\ne D_t(x,y)$\u201d $\\rightarrow$ \u201c$\\mathfrak{B}_t(x,y) \\ne \\mathfrak{D}_t(x,y)$\u201d\n  - Section 6: there are some inconsistencies of using the word \u201cTiny-ImageNet\u201d, which should be fixed throughout the section.\n  - Appendix A.2, page 15, the equation starts with $L(f,\\lambda;\\epsilon)$: What is $z$ at the end of the equation? I think it should be removed.\n  - Appendix A.5: the letter $\\ell$ is both loss function and the minimum singular value of constraint Jacobian matrix. \n* **Minor comments**\n  - Around Assumption 2.3, it would be great if the authors put some citations on universal approximation results for neural networks, which explains (with examples) the richness of (modern) machine learning model parametrization.\n  - Below Equation $(1)$: \u201c\u2026 the forgetting tolerances $\\\\{\\epsilon_k\\\\}$ need to \u2026\u201d $\\rightarrow$ \u201c\u2026 the forgetting tolerances $\\\\{\\epsilon_k\\\\}$ *suffice* to \u2026\u201d\n  - Page 4: This sentence is quite weird: \u201c\u2026 it is sensible to partition the buffer across different tasks as an increasing function of $\\mathbf{\\lambda}^\\star$...\u201d, because it says we can say that a function is increasing in terms of a vector variable.\n\nOverall, I believe the writing could be much more improved than the current draft."
            },
            "questions": {
                "value": "Please see **Weaknesses**."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6337/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6337/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6337/Reviewer_ipX4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676317256,
        "cdate": 1698676317256,
        "tmdate": 1699636697429,
        "mdate": 1699636697429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hbp4JoSCtS",
        "forum": "GicZtgSlJW",
        "replyto": "GicZtgSlJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_Heyc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6337/Reviewer_Heyc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a theoretical analysis of memory-based continual learning based on the recent advances in constrained optimization.\n\nIn terms of constrained optimization, preventing forgetting previously learned tasks becomes the constraint of the optimization problem, and the emprical risk with finite samples should be bounded by the forgetting tolerance as the constraints.\n\nMotivated by the theoretical result of the constrained learning through Lagrangian duality (Chamon et. al. 2020), the authors provide a theoretical plausible Lagrange multiplier $\\lambda_k$ and the buffer size $n_k$  for each task $k.$\n\nIn the experiment, the paper provides some toy benchmark results, such as seq-MNIST with several memory-based baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In the research of continual learning, there is few optimization-based analysis to mitigate catastrophic forgetting.\n\nThis paper provides a new theoy-based algorithm from scratch, which helps to understand which Lagrange multiplier  and buffer size are used totrain the neural networks for continual learning."
            },
            "weaknesses": {
                "value": "Despite the theoretical result, the proposed algorithm does not fit the online continual learning scenario because the process \"fill buffer\" is done after visiting samples in line 11 of Algorithm 1.\n\nThis implies that the buffer should keep all encountered data points during $n_{iter}$ iterations, and then the buffer drops some samples to satisfy the buffer size condition, which has already been violated in lines 5-10 in Algorithm 1.\n\nIt seems that this contradiction occurs because Algorithm 1 needs to access the information of $\\lambda_k$ at the end of each task to compute the optimal buffer size. However, we should have at least the upper bound of the buffer size for the current task $k$ to save encountering samples in the online stream.\n\nIn addition, the loss landscape on the parameter $\\theta$ is non-convex, as the authors stated in Section 3. The local-optimal setting for a given local minimal point and the Lagrange multiplier do not guarantee remarkable performance in the empirical result. The existing heuristic methods based on constrained optimization, such as A-GEM, have already shown remarkable performance in more complex benchmarks, such as split-CIFAR100 and split-MiniImagenet.\n\nConsidering the recent advances in continual learning, I think that a new constrained optimization-based CL algorithm should be either theoretically solid or empirically outstanding."
            },
            "questions": {
                "value": "1. The reported metric is not standard in continual learning. Can the authors report the experiemntal result in terms of the average test accuracy and FWT?\n2. I think the constrained optimizaiton based CL baselines, such as GEM and A-GEM should be included in the experiemt section to analyze the novelty of the proposed method. Is there any reason why the authors does not contain these algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6337/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698976634453,
        "cdate": 1698976634453,
        "tmdate": 1699636697316,
        "mdate": 1699636697316,
        "license": "CC BY 4.0",
        "version": 2
    }
]