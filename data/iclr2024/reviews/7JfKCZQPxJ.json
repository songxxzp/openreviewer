[
    {
        "id": "v4FeenPOah",
        "forum": "7JfKCZQPxJ",
        "replyto": "7JfKCZQPxJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_srFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_srFK"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a set of metrics to evaluate generative models for video. The propose STREAM-T to assess temporal naturalness as well as STREAM-F for fidelity of videos and STREAM-D for the diversity of videos. They compare to the Frechet Video Distance (FVD) on both synthetic data and using a number of generative models.\n\nSTREAM-T is based on the idea of looking at FFT features of the video to assess temporal \"naturalness\", while STREAM-F and STREAM-D are based on Precision & Recall metrics, but include tweaks to make them work better for video data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality:** The Authors argue that STREAM can separately assess temporal and spatial aspects of video, and works regardless of video length (unlike FVD). To the best of my knowledge this is true. I like their idea of using FFT to look at spectral features to identify temporal consistency.\n\n**Quality:** The motivation for their method makes sense, and I think their empirical valuation is sensible. The authors only compare to FVD.\n\n**Clarity:** I was able to follow along nicely\n\n**Significance:** I think it is certainly nice to be able to have several dimensions upon which to evaluate generative methods.  However, it is unclear to me how useful that is in practice. Having these metrics can certainly help \"debug\" generative methods, but I would imagine that they will not be as useful as a \"one metric to judge overall quality\" that FVD provides. Especially since the human evaluation shows that FVD actually does fairly well (especially given that in contrast to the three measurements of STREAM it  is only a single number).\n\n\n**UPDATE AFTER READING THE REBUTTAL**\nOverall, my judgment is that the authors have convinced me that in principle this manuscript deserves publication: they attack a meaningful problem and their empirical work is solid. So I think this work scores high enough in Originality,  Quality, and Clarity to be publication. As far as signficance goes: I'm not 100% conviced ICLR is the best venue for this work (I'd imagine you find a more interested audience in conferences that are more focused on computer vision), which is why I still think the manuscript is only marginally above the acceptance threshold for ICLR."
            },
            "weaknesses": {
                "value": "* While I think it is useful to have metrics that focus on different aspects of the generation, I would imagine that while developing a method, most of the time it is more helpful to have a single metric to look at to judge progress (if I'm developing a new method and my newest change to the method improves STREAM-F but hurts STREAM-T, is it a good modification or not. Thus, I think the proposed STREAM metrics will be helpful, but will fail to actually replace FVD.  If the authors could find a good way to combine their measurements into a single number (e.g. akin to the F1-score to combine P&R), I think the paper would have more impact. \n* The paper mentions the Video Inception Score (VIS) several times as go-to metric for this task, but does not compare to it. The authors should motivate why they did not use this metric at all, despite it being an obvious competitor.\n* I find it confusing that the authors introduce the term \"STREAM-S\": Its name implies that it's one of the metrics that is being introduced (it follows the same naming convention), but it actually isn't. I'm absolutely unclear what that term actually denots.. I think the paper would improve in clarity if that term would be removed and instead clearly state that there are 3 new metrics that are being introduced (STREAM-T, STREAM-F, STREAM-D).\n\n## Typos and Minor points\nPage two: Spatio-TemproRal Evaluation and Analysis Metric(STREAM)  => TempoRal\nSection 2.1: \"Let real and fake video datasets as\"  => ... be denoted as...\nSeciton 2.2: we utilize SwAV (Caron et al., 2020), a proficient image embedding network => SwAV is an algorithm, not a network\n\n\"In the image generation task, Kynka\u00a8anniemi et al. \u00a8 (2019) have proposed precision and recall (P&R) to separately evaluate the fidelity and diversity of the generated image quality.\" => I think this was first proposed by Sajjadi et al, (NeurIPS 2018), Assessing Generative Models via Precision and Recall.\n\nSection 2.4: \"If we define a sphere with centered at sample point\"  => I don't understand this sentence"
            },
            "questions": {
                "value": "* I would like to know how well FVD correlates with the various STREAM measures. Figures 3, 4 and 6 seem to indicate that the spearman correlation between STREAM-T and FVD is actually fairly high, which is completely contrary to the main text, which several times claims that FVD is not good at picking up temporal details.\n\n* What is the reason for not comparing against VIS?\n\n* STREAM-T uses a histogram comparison. How do binning sizes affect the outcome?\n\n* Will the authors provide source code for implementing STREAM? It seems nontrivial to implement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Reviewer_srFK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698074259415,
        "cdate": 1698074259415,
        "tmdate": 1700647424101,
        "mdate": 1700647424101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pT6oNlQ8qw",
        "forum": "7JfKCZQPxJ",
        "replyto": "7JfKCZQPxJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_rxEA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_rxEA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new metric to evaluate the generated videos. The authors present three new metrics to assess video quality namely 1) spatial fidelity 2) diversity and 3) temporal coherence. Authors construct various kind of perturbation and evaluate the videos."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Paper is easy to follow\n* The most important contribution of the paper is the newly proposed metric is bounded between 0-1 as opposed to FVD, which is an unbounded metric."
            },
            "weaknesses": {
                "value": "* FVD is a single metric used to evaluate a video. With STREAM as a metric, you would have 3 sub-metrics to evaluate generated videos. This would result in $2^3$ scenarios when comparing two baselines, making it tedious to evaluate a new method."
            },
            "questions": {
                "value": "I would like to see how this metric performs in three scenarios.\n* When the generated video consists of only one frame repeating throughout the video segment\n* One of the main selling points of the FVD metric was it penalizes blur phenomena significantly higher than noise phenomena. This was a useful property because it correlates well with human vision. Additionally, the video generation methods tend to produce blurry samples which would score higher on traditional metrics like SSIM and PSNR. I would want to see how it performs on blurry videos (apply Gaussian blur). \n* What would be the results if the video is reversed temporally and evaluated? please do the same for flipping the videos spatially(take mirror images of the frames) and lastly, run the evaluation for both spatially and temporally flipped video sequences.\nI would like to see an evaluation of all these three scenarios before making my final decision.\n\nHow is the metric affected by the length and resolution of videos, and if it is affected, please provide the standardization of metric because people can game the metric utilizing this loophole."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698341015515,
        "cdate": 1698341015515,
        "tmdate": 1699636178226,
        "mdate": 1699636178226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uSbFajr97R",
        "forum": "7JfKCZQPxJ",
        "replyto": "7JfKCZQPxJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_1pjF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_1pjF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel method to evaluate the quality of generated videos. Specifically, it proposed STREAM-T and STREAM-S to assess temporal and spatial aspects of the videos respectively. STREAM-T is designed to measure temporal quality. It evaluates the continuity and consistence of videos by calculating the FFT of real and fake videos and comparing the difference of the frequences. STREAM-S evaluates the spatial quality by classifying the amplitude at frequency of 0 using KNN. Experiments show that the proposed method is able to measure video quality better than current methods such as FVD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The proposed method proposed a novel method targeting a challenging task, video generation evaluation. \n3. Experiments show the effectiveness of proposed method in evaluating temporal quality in video generation.\n4. STREAM-T is a reasonable metric since it applies statistical method rather than simple l2 or l1 distance to compute loss of video frequency."
            },
            "weaknesses": {
                "value": "1. How will the performance be if most of the generated videos are still? Can they be correctly evaluated?\n2. Using P&R to compute fidelity and diversity of fake videos is reasonable, but why to use amplitude at frequency of 0 as sample points? I would like to see more explanations from the authors.\n3. As video quality is very subjective, a systematic evaluation by human raters is required to compare with current method FVD.\n4. The difference in the style of real and fake video datasets may have a huge effect on the result. For example, if real videos tend to have fast changes, the frequency may be concentrated on the high frequency, and the curve fitted may have a higher skewness. In such case, if the curve fitted by fake videos is less steep (fake videos change slowly), skewness may be small, and even if the fake video is of high quality, STREAM-T may be small."
            },
            "questions": {
                "value": "see weaknessess"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2426/Reviewer_1pjF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861328305,
        "cdate": 1698861328305,
        "tmdate": 1700738980679,
        "mdate": 1700738980679,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sb8A8al7G1",
        "forum": "7JfKCZQPxJ",
        "replyto": "7JfKCZQPxJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_QwpP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2426/Reviewer_QwpP"
        ],
        "content": {
            "summary": {
                "value": "While evaluation metrics for image generative models are relatively comprehensive, those for video are limited. This paper addresses the drawbacks of existing metrics, proposing a new metric, STREAM, which comprehensively evaluates the temporal and spatial aspects of videos. Experimental validation was conducted on several baseline video generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The writing is clear and easy to follow.\n- The paper's motivation is strong. Given the rapid development in the field of video generation, traditional metric FVD falls short in representing performance comprehensively. This paper bridges this gap.\n- The proposed metric allows independent evaluation of spatial and temporal domains, making it applicable for assessing the quality of long videos."
            },
            "weaknesses": {
                "value": "- The experimental section of the paper is relatively weak, being limited to a few older baseline models. Authors should focus more on current open-source Text-to-video models, which would provide more convincing results.\n- Although UCF-101 is a common benchmark dataset, as a contribution paper introducing a new evaluation metric, testing on a wider range of datasets and tasks, including T2V, video prediction, and the latest works in unconditional generation, would enhance the paper's credibility."
            },
            "questions": {
                "value": "Despite some experimental limitations, the paper makes substantial theoretical contributions. The problem it addresses holds significant value. Currently, I am providing a borderline accept score, hoping the authors will address the experimental aspects mentioned in my review in their future research."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698911897703,
        "cdate": 1698911897703,
        "tmdate": 1699636178049,
        "mdate": 1699636178049,
        "license": "CC BY 4.0",
        "version": 2
    }
]