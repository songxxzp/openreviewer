[
    {
        "id": "2HbKjuSkru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_ycqn"
        ],
        "forum": "ysue5S6cVS",
        "replyto": "ysue5S6cVS",
        "content": {
            "summary": {
                "value": "Traditional approaches proposed to mount backdoor attacks inject triggers into training samples selected at random. This paper presents a new approach for selecting the training samples to poison in order to launch a more effective backdoor attack. To that end, the authors select samples that are close to the decision boundary between different classes. These samples are the ones that have a low confidence (measured as the probability of a particular class). \n\nThe authors test their approach against 2 baselines: a random baseline, and a baseline where poisoned samples are chosen based on how early they are forgotten during training (FUS). For each of these choices, the authors implement several backdoor attacks, including BadNets, Blend and Adaptive Attacks. The attacks are evaluated when no defenses are employed, and when several defenses are used, such as SS, STRIP, ABL and NC.\n\nThe results on CIFAR10 and CIFAR100 show that selecting the samples to poison is more effective than poisoning random samples. Furthermore, several attacks that are not effective (when defenses are employed) with random sampling become effective when the samples to-poison are chosen. \n\nFinally, the authors investigate the choice of close-to-boundary and far-from-boundary samples to poison to validate that poisoning samples close to the decision boundary is more effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- the paper presents a new approach for selecting the samples to be poisoned during a backdoor attack, as opposed to random sampling. the results of the paper show that a good selection is more effective than random selection\n- the paper evaluates several backdoor attacks and defenses when random and targeted sampling are used"
            },
            "weaknesses": {
                "value": "- the paper considers 2 small datasets: cifar10 and cifar100. it would be great to evaluate on larger datasets, such as imagenet\n- the paper evaluates defenses that rely on some notion of outlier detection, and as such, the selection mechanism is effective. the authors however do not evaluate defenses that are not based on outlier detection, such as [1]\n\n\n[1] Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility, Jin et al., 2021"
            },
            "questions": {
                "value": "- FUS is not a method proposed for selecting poisoned samples. how do you do the selection for this technique?\n- can you evaluate your method on larger datasets such as imagenet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697420962258,
        "cdate": 1697420962258,
        "tmdate": 1699636660350,
        "mdate": 1699636660350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xathwyGeSC",
        "forum": "ysue5S6cVS",
        "replyto": "ysue5S6cVS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_gKjE"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new sampling procedure for backdoor attacks. Standard backdoor attacks poison (i.e., insert a backdoor trigger on) *random* examples from the training set. The authors claim that strategically choosing which training examples to poison can improve on the efficacy of a variety of attacks.  In particular, the authors choose to poison training examples on which a surrogate model exhibits low confidence. Then, the authors provide strong experimental evidence that this sampling scheme improves existing attacks. Additionally, the authors provide intuition for why their method might work on neural networks by analyzing an SVM model on a toy dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the authors propose a simple method with few hyperparameters that reliably gives good experimental results\n- the proposed method can be combined with existing attacks\n- the choice of backdoor attacks in the evaluation is thorough\n- the writing is clear and concise"
            },
            "weaknesses": {
                "value": "My main concern is about how practical the idea is. In particular, the adversary is assumed to have:\n- *edit* access to the entire train set\n- access to a surrogate model that is similar to the model that is being backdoored.\n\nFor example, suppose I'm an (adversarial) user of some social media platform P. I know P will train *some* model on the data of its users (including my own data). I could execute a backdoor attack by simply inserting a backdoor trigger in each of my datapoints (e.g., images). However, to execute the proposed attack, I would need the ability to insert data into arbitrary users' profiles---this already makes the attack a lot harder to execute. Additionally, I would need access to a model that acts similarly to the model P will train. Because of this, now I need knowledge of the model P aims to train---in my view, another significant challenge in practice.\n\nAs another example in the same vein, consider the setup in [1]. There, the authors poison *expired* web-pages. The above two challenge persists in this setup.\n\nAdditional weakness:\n- Evaluated defenses: it makes (intuitive) sense to me that the proposed attack works well against outlier-based defenses. However, it is unclear to me whether this will translate to defenses based on model behavior like [2] and [3].\n\n\n[1] Carlini, Nicholas, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. \"Poisoning web-scale training datasets is practical.\" arXiv preprint arXiv:2302.10149 (2023).\n\n[2] Jin, Charles, Melinda Sun, and Martin Rinard. \"Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[3] Khaddaj, Alaa, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, and Aleksander Madry. \"Rethinking Backdoor Attacks.\" (2023). https://arxiv.org/abs/2307.10163"
            },
            "questions": {
                "value": "- in Table 1, why does your method reduce the efficacy of the attack in the absence of a defense?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698427354541,
        "cdate": 1698427354541,
        "tmdate": 1699636660164,
        "mdate": 1699636660164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B1gcRTxllM",
        "forum": "ysue5S6cVS",
        "replyto": "ysue5S6cVS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the strategies of choosing samples to inject triggers for training backdoor models. To improve the robustness against backdoor defenses, the paper proposes a confidence-driven sampling strategy for backdoor attacks. Specifically, the proposed method chooses samples with lower confidence scores to inject triggers, making the backdoored models difficult to be defended. Extensive experiments show the effectiveness of proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "To improve the robustness against backdoor defenses, the paper proposes a simple and effective sampling choosing strategy for backdoor attacks. Also, the proposed method can be integrated into various backdoor attacks to improve the robustness. The paper analyzes the proposed method theoretically, indicating the rationality of proposed method."
            },
            "weaknesses": {
                "value": "The method is not efficient because it needs to train a suggorate model (e.g. ResNet18) to check the confidence scores. In the paper, the used datasets are two small-scale datasets including CIFAR10 and CIFAR100. Is it time-consuming when traing a suggorate model on a large dataset e.g. image-net?\n\nIn the experiments (Table 1, 2 and 3), the used backdoor defenses are not up-to-date. It is better to compare with some recent backdoor attacks e.g. i-bau [1]. \n\n[1] Zeng, Yi, et al. \"Adversarial Unlearning of Backdoors via Implicit Hypergradient.\" International Conference on Learning Representations. 2021."
            },
            "questions": {
                "value": "Does the proposed method perform better against defenses depending on outlier detection than other backdoor defense methods?\n\nIn Figure 1, the paper shows the visualizations of two dirty-label attacks. Is there same observations for clean-label attacks?\n\nIn Table 1, 2 and 3, the proposed method achieves lower ASRs compared to Random and FUS under the condition of \"No Defenses\". Could the authors provide more explanations about the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6110/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6110/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6110/Reviewer_okLZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620817657,
        "cdate": 1698620817657,
        "tmdate": 1699636660028,
        "mdate": 1699636660028,
        "license": "CC BY 4.0",
        "version": 2
    }
]