[
    {
        "id": "gcW8IoKjGx",
        "forum": "CRkvR8TJkk",
        "replyto": "CRkvR8TJkk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_PnWa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_PnWa"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issue of Personalized Federated Learning and proposes pFedGT, a method for personalized Federated Learning based on a Game-theoretic approach, that adopts a formulation termed \u201cTarget interpolation.\u201d This paper conducts detailed experiments on the proposed algorithm, and the experimental results demonstrate that the algorithm achieves better performance on multiple datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experimental section of this paper is shown in detail, comparing the performance of the proposed algorithm with other algorithms on multiple datasets. The results indicate superior performance."
            },
            "weaknesses": {
                "value": "1. My major concern is the lack of novelty. The proposed idea of target interpolation in this papers seems to bear some resemblance to the concept of model interpolation presented in 'Three Approaches for Personalization with Applications to Federated Learning'(Mansour et al). From the algorithmic perspective, the essence of the algorithm proposed in this paper is still the introduction of a new regularization technique, unrelated to the game-theoretic approach.\n\n2. Although this paper emphasizes that its algorithm is a game-theoretic approach, in reality, both the algorithm design and theoretical analysis lack the incorporation and analysis of game-theoretic principles. In fact, only a single sentence at the end of Section 3.1 briefly mentions the concept of Nash equilibrium and claims that each user iteratively solves the problem to achieve Nash equilibrium, which I doubt. I hope the authors can provide more theoretical and experimental analysis about game theory instead of merely mentioning the concept of Nash equilibrium.\n\n3. The algorithm lacks protection for user model privacy. Unlike most federated learning approaches that update models by transmitting gradients in each round, the algorithm proposed in this paper transmits information c_i between the agent and server, where c_i is the gradient subtracted by the user's own model parameters. For most users, transmitting their own model parameters to the server is not acceptable compared to algorithms that only transmit gradients. (This is likely to happen when the user's model gradually converges, and c_i is approximately equal to \u03bc w_i. Users who value model privacy are unlikely to accept this situation.)"
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4673/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4673/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4673/Reviewer_PnWa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4673/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698291514085,
        "cdate": 1698291514085,
        "tmdate": 1699636448283,
        "mdate": 1699636448283,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HEgiVeQW0K",
        "forum": "CRkvR8TJkk",
        "replyto": "CRkvR8TJkk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_5Hnv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_5Hnv"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the personalized federated learning (PFL) problem where local agents do not completely follow the global model and keep local models for their local demands. The authors claim they address the problem using the game-theoretic approach to model the PFL problem with a target interpolation (a linear combination of local objective and global objective), where the local deployed model parameters are considered the agents' strategies in the game. The authors then show that after adding a sufficiently strong L2 regularizer to the local objective, the PFL problem using the pFedGT algorithm will converge to a unique solution (Nash equilibrium)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors provides a complete story with problem formulation, algorithm pseudo code, theoretical convergence guarantee and numerical experiments showing the performance of pFedGT on the PFL problem defined in this paper."
            },
            "weaknesses": {
                "value": "1. The reason of formulating this PFL problem as a game theory problem is unclear. The updating dynamics is almost identical to FedAvg, where the authors replaced the \"local loss\" with \"a linear combination of local loss and global loss\". This change actually makes the agents more collaborative than strategically non-cooperative, and thus using a game theory framework for this problem is not providing any help in the intuition or the analysis.\n2. There are places in the paper where the presentation can be significantly improved. For example,\n(1). How is the heterogeneity level \\alpha defined in Figure 1? Is it the same as the \\alpha in Theorem 2? Can you provide intuition behind the strength of alpha and how the data will look like?\n(2). On page 4, the discussion on the use of c is confusing until we check the pseudo code of Algorithm 2. The authors should definitely direct the readers to Algorithm 2 and provide more explanations on this\n3. The claim that Assumption 1 is the only assumption is questionable, since the authors require a sufficiently strong regularizer to ensure strong convexity of the problem, which is restrictive in many applications beyond the Cifar classification. Moreover, under Lipschitz and strong convexity conditions, the uniqueness and convergence is very straightforward and there is no need for novel proof techniques.\n4. How the aggregation interval and the partial participation schemes influence the convergence is not discussed in the theoretical results.\n5. It is not easy to understand the difference between this work's setting and result from previous works, the authors should consider adding a table with each related works' setting, assumptions, solution existence and uniqueness, and convergence guarantee."
            },
            "questions": {
                "value": "1. Is the game theoretic framework a necessity? If so, why is that? \n2. If the agents strategically change \\gamma_i and only optimize the local loss, can your framework generalize to that and how may the results look like?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4673/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698562036946,
        "cdate": 1698562036946,
        "tmdate": 1699636448196,
        "mdate": 1699636448196,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lK1nzHMW0V",
        "forum": "CRkvR8TJkk",
        "replyto": "CRkvR8TJkk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_Ha4r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_Ha4r"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an interesting personalized Federated Learning method. In this method, the local objective functions are modeled through a combination of the objective functions from all clients. Additionally, the authors present an approximation technique that allows for the estimation of objective functions from other clients without the necessity of transmitting local data. Experimental results demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Modeling clients' objective functions as a composite of individual clients' objective functions is promising. \n2. The existence and uniqueness of a Nash equilibrium are provided.\n3. The experiments demonstrate the proposed method is useful."
            },
            "weaknesses": {
                "value": "1. The hyper-parameter $\\gamma$ is a crucial element controlling the strength of the objective functions of other clients. Nevertheless, the authors have not conducted adequate experiments to elucidate how algorithm performance varies with different values of $\\gamma$.\n2. In the case of Theorem 2, it appears that when $\\gamma = 1$ (indicating no collaboration), the algorithms achieve the most favorable convergence results.\n3. The formulation of Theorem 2 seems to address the convergence rate with only one local step, which suggests it may be more relevant to traditional distributed algorithms rather than federated learning algorithms. \n4. Assumption 2 is not common in PFL. It would be better if more justification is provided.\n5. The hyper-parameter $\\rho$ plays a pivotal role in Theorem 2, and the theorems are only valid when $\\rho \\ge \\max_{i} (L \\cdot L_{F_i})$. However, the results in Figure 10 indicate that setting $\\rho = 0$ consistently yields favorable results. While I understand the authors' choice to ensure strong-convexity by setting $\\rho \\ge \\max_{i} (L \\cdot L_{F_i})\" for theoretical purposes, it introduces a significant disparity between theory and experimental outcomes.\n6. Regarding Algorithm 1, the communication overhead seems heavy, as there is an additional $c^t$ that needs to be exchanged between server and clients, besides the model.\n\nMinors:\n1. It appears that optimizing the objective functions of other clients may impede the training convergence of the current client, as also corroborated by Theorem 2. However, the locally reported performance suggests that this impediment actually benefits the final performance. I am intrigued by this phenomenon and would appreciate more details from the authors regarding the construction of the training and test sets. Do the local training and test sets share the same distribution, or is the paper assessing generalization performance otherwise?\n2. The approximation technique is not novel [1, 2]. Furthermore, it may be worthwhile to explore other methods for approximating Hessians, such as utilizing the Fisher Information Matrix or directly employing PyHessian.\n3. It would be advantageous to include error bars in the experimental results for a more comprehensive presentation.\n\n[1] Yin D, Farajtabar M, Li A. SOLA: Continual learning with second-order loss approximation[C]. 2020.\n[2] Guo Y, Lin T, Tang X. A new analysis framework for federated learning on time-evolving heterogeneous data[J]. 2021."
            },
            "questions": {
                "value": "1. Could the authors give more explanations about Theorem 2?\n2. Could the authors provide more discussions on the $\\rho$ and $\\gamma$?\n3. Could the authors provide more details about the experiment settings? Additionally, the number of clients should be increased."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4673/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758428530,
        "cdate": 1698758428530,
        "tmdate": 1699636448113,
        "mdate": 1699636448113,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tGdWoSSnlx",
        "forum": "CRkvR8TJkk",
        "replyto": "CRkvR8TJkk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_jCxY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4673/Reviewer_jCxY"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a new personalized federated learning model based on a weighted average of local and global loss, and further approximate it into a game formulation. The proposed model attains Nash equilibrium, the corresponding algorithm attains linear convergence. Extensive numerical experiments further showcased the superiority of the algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. New model for PFL\n2. The proposed algorithm outperforms existing works in the experiments."
            },
            "weaknesses": {
                "value": "1. The additional introduced regularization term $\\frac{\\rho}{2}||w_i||^2$ term is weakly motivated, as far as I understand, this term is more like for theory convenience, which makes the function to be strongly convex, so the Nash existence and linear convergence are expected to some extent. I may think the idea is a bit similar to that of FedProx [1].\n2. Lots of hyperparameters concerning the function objectives ($\\mu, L, \\gamma_i$) are required compared to classical algorithms, which weaken the practical significance.\n3. Mismatch between theory and practice. You mentioned in the experiments you choose $\\rho=0$ works (and it even outperforms over other choices), while the $\\rho$ is required to be larger than $L$ in the theory. Such mismatch has not been highlighted and thoroughly discussed.\n4. In fact that also raises my concern about whether the authors need to resort to a game theory background for the paper. If the additional regularization term is only for theory convenience to attain Nash, while it seems to be a bit unnecessary in the experiment, I think the algorithm and storyline of the paper can be revised. With a nonconvex objective only, the proposed algorithm should still be able to converge to stationarity.\n\n[1] Li, Tian, et al. \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4673/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699432755383,
        "cdate": 1699432755383,
        "tmdate": 1699636448047,
        "mdate": 1699636448047,
        "license": "CC BY 4.0",
        "version": 2
    }
]