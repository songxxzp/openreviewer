[
    {
        "id": "OwJuHpd5GW",
        "forum": "Dnn7SoD224",
        "replyto": "Dnn7SoD224",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_a5XH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_a5XH"
        ],
        "content": {
            "summary": {
                "value": "The authors present to capture continuous disparity in current stereo matching networks. Specifically, they propose to treat the disparity prediction problem as an optimal solution to the risk minimization problem. They explore the implicit function theorem and propose a differentiable scheme for end-to-end network training with risk minimization. Their method can improve the robustness and fine-grained prediction quality of stereo matching networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces a fresh risk minimum perspective on the challenge of capturing continuous disparity for stereo matching.\n2. One notable strength of this method is its potential to enhance the fine-grained prediction quality in stereo matching. The ability to capture continuous disparity offers a crucial advantage in achieving more accurate and detailed depth maps.\n3. The method can be applied to existing stereo matching networks without re-training and improves their performance.\n4. The paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "1. The current iterative optimization based methods such as RAFT-Stereo directly predict continuous prediction and show strong robustness. It seems the method in this paper is not suitable for these iterative optimization methods, which I consider a major weakness restricting its application.\n2. The baseline model seems to have strong robustness and the benefit of applying risk minimization is relatively small, which I consider as a minor weakness."
            },
            "questions": {
                "value": "1. How important is the Laplacian kernel for interpolation in Eq.(1)? I wonder if using the simple bilinear interpolation is enough for capturing continuous cost values.\n2. Considering the paper builds its own network architecture, I wonder how the method can improve existing cost aggregation based networks such as PCWNet or GWCNet."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1847/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1847/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1847/Reviewer_a5XH"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698472568971,
        "cdate": 1698472568971,
        "tmdate": 1699636114885,
        "mdate": 1699636114885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0vUZl81dZ2",
        "forum": "Dnn7SoD224",
        "replyto": "Dnn7SoD224",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_bKMt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_bKMt"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel risk minimization based stereo matching method to capture the continuous disparity. While the state-of-the-art methods utilize expectation value of the distribution of multiple disparity candidates, the proposed method is searching the continuous disparity with the minimum risk or the smallest prediction error with the ground truth. In addition, it utilizes L1 formulation that shows more robust performance in the experimental results. To enable the end-to-end network training of L1 risk minimization, it introduces a differentiable scheme to forward and backward propagation. The proposed method is more robust to the regions with multiple disparity candidates and outliers. Experimental results show that the proposed method has better generalization in cross-domain stereo images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Unlike conventional methods, the paper introduces a novel risk of disparity concept to measure the final disparity. The final disparity is predicted by minimizing the prediction error of all possible ground-truth disparities (Note that the exact ground truth is unavailable during the prediction). The paper shows that the minimizing the L2 risk is similar to the expectation value computation to predict final disparity, which is not robust to outlier. Thus, it proposes to utilize L1 norm which is more robust to predict final disparity with multiple disparity candidates or outliers.\n\n+ The paper introduces a method to forward and backward propagation of a L1 risk minimization process. The final disparity is done by performing binary search across disparity candidates by selecting the disparity with minimum risk. To effectively backward propagate the gradient, the method is inspired by the Implicit Function Theorem where G(y,p) = 0 at the optimal y. \n\n+ To effectively perform the search, the paper introduces a two-level stereo matching network. The coarse level estimates the disparity hypothesis in coarser resolution so that the search space is reduced in refined level.\n\n+ Experimental results show that the proposed L1 risk minimization method can improve the performance in both in-domain and cross-domain performance scenario. In addition, the ablation study also shows that the proposed L1 risk minimization can be applied for various different networks. Finally, it also show that L1-risk minimization method works better than expectation value computation to predict final disparity."
            },
            "weaknesses": {
                "value": "- The authors claim that it is better than other methods in cross-domain generalization. However, they just compare with non-domain generalization stereo matching methods. Thus, the claim is lack of the justification. It is recommended to put more analysis on the cross-domain evaluation in terms of the proposed risk minimization as done by [A] in Table 1.\n\n- In the ablation study, the authors compare the performance of L1 risk minimization in two worst methods in Middlebury evaluation. It makes the claim to improve the performance on different networks is not justified enough. The comparison with DLNR and IGEV are also needed. \n\n- The L1 risk minimization a general concept that can be applied into various stereo matching methods. Note that the L1 risk minimization is the biggest contribution in the paper. However, the paper introduces a new network architecture. Thus, it is unclear whether the performance in the experimental results is because of the proposed new architecture or the L1 risk minimization. More detailed analysis and ablation study are required. \n\n\nAdditional references:\n[A] Domain Generalized Stereo Matching via Hierarchical Visual Transformation, CVPR 2023"
            },
            "questions": {
                "value": "* Does the proposed risk minimization really improve the cross-domain generalization? \n* Is there any reason to choose ACVNet and PCWNet for comparison in Table 8? Is it possible to perform experiment where the L1-risk is also used in the training stage. More comparisons with other networks are needed.\n* If the new architecture is not used, would L1 risk minimization method improves the current state-of-the-art performance?\n* Is there any limitation of the proposed L1 risk minimization method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639963429,
        "cdate": 1698639963429,
        "tmdate": 1699636114815,
        "mdate": 1699636114815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LVId5Xhvap",
        "forum": "Dnn7SoD224",
        "replyto": "Dnn7SoD224",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_MXR7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_MXR7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to model the disparity estimation problem as a continuous risk minimization problem. They propose the use of a robust L1 formulation to account for multi-modal disparity distributions. To enable end-to-end learning they propose a differentiable scheme using the implicit function theorem for forward prediction and backward error propagation. The authors demonstrate the effectiveness of their proposed method by conducting experiments using all popular stereo vision datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is very interesting and the results are strong. The authors have conducted experiments using all popular datasets and have compared their proposed method against many competing methods. Their experiments include in-domain evaluation, cross-domain evaluation, and ablation studies. The method is well-motivated. The authors explain their method well and they provide a good theoretical background to support their assumptions."
            },
            "weaknesses": {
                "value": "1) There are parts of the paper that wording can be improved. Proofreading by a native speaker will further strengthen the paper. Examples:\n a) impressive deep-learning architecture -> advancements in deep-learning. Something being impressive doesn't necessarily mean that it works\n b) Typical stereo matching algorithm will sample ... and compute a discrete distribution p: Typical stereo matching algorithms will compute a cost that merely can be described as a PMF. \nc) To take a value from -> To choose a value as the final prediction? \nd) where the Sign() is the sign function -> where the sign is the signum function?\ne) In implementation, we clip -> we clip... \n\n2) The symbols in the math equation can easily cause confusion:\n   a) The symbol p is used to denote the probability mass function and the probability density function. It is common to use P for mass and F for density. Another option would be to use subscripts if the authors want to keep using the notation p(x;p) for the probability density function. \n   b) in equation (2) y is in the left and right hand of the equation. If y is the result of the argmin shouldn't the right-hand y be defined as a variable in a range of values? similarly eq(4)\n\n3) The binary search algorithm can be moved to the appendix. \n4) The related work is very dense. Moving the binary search algorithm to the appendix will free some space to expand. \n5) Figure 3: The captions are not aligned with the different stages of the architecture. You can condense the schematics and allow more space for risk minimization since the other parts of the proposed architecture are described in prior art\n6) Table 4: Are these results after training and testing all these methods on Middlebury? Please specify in the captions of all tables if the reported results are after self-evaluation or the official results on the dataset evaluation page. Submit also on the test set of Middlebury. \n7) Since the main proposal of this paper is risk minimization, It would strengthen the paper if they apply their proposed module to more architectures, like IGEV or Raft-stereo which has shown strong cross-domain generalization."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644174949,
        "cdate": 1698644174949,
        "tmdate": 1699636114749,
        "mdate": 1699636114749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fI6NQvLTih",
        "forum": "Dnn7SoD224",
        "replyto": "Dnn7SoD224",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_N1iu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1847/Reviewer_N1iu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel formulation for predicting continuous disparity in stereo matching networks by framing the problem as risk minimization. Rather than directly predicting the disparity or regressing to a continuous value, the authors propose treating the prediction as an optimal solution that minimizes the risk or expected error. Under this formulation, the commonly used disparity expectation is shown to be a special L2 case. The authors then advocate using an L1 loss which is more robust, and derive a differentiable algorithm to enable end-to-end training. Experiments on multiple datasets demonstrate state-of-the-art performance and superior cross-domain generalization ability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The risk minimization view of disparity prediction is an interesting and original perspective. Framing it as an optimization problem with an L1 loss is intuitive and theoretically sound.\nThe method of computing gradients for the non-differentiable L1 optimization using implicit function theorem is clever, enabling end-to-end training.\nThe approach convincingly outperforms current state-of-the-art methods on in-domain and cross-domain evaluations across a range of datasets. The cross-domain results in particular highlight the robustness.\nThe ablation studies clearly demonstrate the benefit of L1 risk minimization over disparity expectation, even without retraining networks. This underscores the general applicability."
            },
            "weaknesses": {
                "value": "While theoretically motivated, the actual network architecture and components like cost volumes seem fairly standard. More implementation details could help highlight if other architectural modifications were needed to fully take advantage of risk minimization.\nThe binary search algorithm for L1 optimization, while efficient, involves heuristics like the tolerance hyperparameter. More analysis could be provided on algorithm convergence and optimality guarantees.\nFor real-time applications, the running time and complexity should be analyzed more thoroughly. Comparisons to optimizations like pruning could better highlight efficiency."
            },
            "questions": {
                "value": "How does the risk minimization formulation compare to other robust loss functions like smooth L1 that try to reduce outlier influence? Is the improvement mainly from switching to L1 or from the risk view?\nHave the authors experimented with other error functions like Huber loss within the risk framework? This could help determine the benefits of specifically L1 vs. just a robust loss.\nCan the method apply to other vision tasks with multi-modal outputs like optical flow or depth estimation? Does it mainly benefit stereo matching or generalize broadly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1847/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699233100761,
        "cdate": 1699233100761,
        "tmdate": 1699636114687,
        "mdate": 1699636114687,
        "license": "CC BY 4.0",
        "version": 2
    }
]