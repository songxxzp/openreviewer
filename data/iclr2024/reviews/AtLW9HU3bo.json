[
    {
        "id": "FTyg9cj5ab",
        "forum": "AtLW9HU3bo",
        "replyto": "AtLW9HU3bo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission33/Reviewer_zSXn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission33/Reviewer_zSXn"
        ],
        "content": {
            "summary": {
                "value": "The author focused on better-resolving video question answering. Among various approaches, the authors focus primarily on long videos. Although the topic and the focus sound interesting, it is hard to understand the main difference compared to the previous works and how each model is constructed in what manner."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "First, the author well-presented the problem, and I do not doubt this part. Also, the experiment seems very well designed with multiple datasets. From my end, the justifications in Figure 4 help me understand why the author set each parameter. Last but not least, I did not find any grammatical issues."
            },
            "weaknesses": {
                "value": "It is very hard to catch what are the main differences between previous approaches. Also, it is almost impossible to understand how each module introduced in Section 3 is constructed, even with Figure 3. As a result, it is tough to find a clear link before Section 4 and after Section 4."
            },
            "questions": {
                "value": "A. First and foremost, the authors sell their approach as somewhat new in handling long-term video. For instance, the first sentence in 3rd paragraph of the Intro (there are a couple more, e.g., the last paragraph of the Intro and the first paragraph of Related work) treats pioneer video QA tasks/methods do not focus more on causal and temporal inference. However, even from the beginning of VideoQA (Jang et al., 2017b) (I would like to cite some additional work in this thread [1,2,3]), they tackle causality with spatial/temporal attention mechanism (for instance, the oldest baseline Co-Mem (Gao et al., 2018) also uses attention). Considering this work is also mainly based on attention mechanisms, I missed the main difference between these lines of work. The author may want to say that those works are not based on the Transformer model, and it should be true for some old approaches, as those works appeared even before the Transformer was presented, but it is only valid for some of the baselines. Instead of mainly focusing on presenting numbers, I would request to present a detailed analysis with a theoretical explanation, and I do believe this will strengthen this manuscript. Along with this, I also wonder how and why the authors think some Transformer-based approaches that sample a few frames from the vision side (e.g., MERLOT [4,5]) work reasonably well on VideoQA, even though some of those models do not have an explicit attention-based frame sampler. Comparison with those models would also be appropriate.\n\nB. Along with A, it is almost impossible to understand how each component presented in Section 3 is constructed. I guess X_{L} comes from X_{l} in Equation (1), but I failed to find any clue for V_{ST}, V_{Key}, E_{S}, E_{Q}. The only equation I can see afterward is an Argmax in Eq.(2); it is impossible to guess how to compute those. I also failed to see any symbols from tiny Figure 3 (a) (The author should write the main paper self-contained within the page limit). I don't think any reader can easily replicate this work without such details.\n\nDue to A and B, I feel Sections 1-3 and 4-5 are disconnected, and thus, it is hard to fully digest the experiment results; it seems the experiment itself is reasonably designed, by the way. To this end, it is hard to give acceptance from my end as of now. I suggest the authors (aggressively) revise Sections 1-3 to sound more coherent with Section 4~5.\n\n\n*** References ***\n\n[1] Zhu et al., Uncovering the Temporal Context for Video Question Answering, IJCV 2017.\n\n[2] Mun et al., MarioQA: Answering Questions by Watching Gameplay Videos, in ICCV 2017.\n\n[3] Kim et al., DeepStory: Video Story QA by Deep Embedded Memory Networks, in IJCAI 2017.\n\n[4] Zellers et al., MERLOT: Multimodal Neural Script Knowledge Models, in NeurIPS 2021.\n\n[5] Zellers et al., MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound, in CVPR 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission33/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698424440303,
        "cdate": 1698424440303,
        "tmdate": 1699635927165,
        "mdate": 1699635927165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9mjc2chGQH",
        "forum": "AtLW9HU3bo",
        "replyto": "AtLW9HU3bo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission33/Reviewer_92w2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission33/Reviewer_92w2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces E-STR, which aims to handle complex VideoQA tasks involving long videos with multiple objects and events. E-STR incorporates a question-critical keyframes retriever to adaptively select key events for spatial-temporal reasoning, along with a context encoder to preserve general video context."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper proposes a reasonable method to handle the complex VideoQA task by keyframe retrieval, which can effectively compress the long video (32->6). The idea of context encoder is innovative, which seems different from other similar works.\n2) A series of experiments on complex VideoQA benchmarks have demonstrated the superiority of the method.\n3) The article has a clear structure, logical writing, and is easy to understand."
            },
            "weaknesses": {
                "value": "1) The retrieval-based approach is not entirely new, as many existing works [1]-[3] utilize this idea for the VideoQA task. The article lacks a detailed comparison and analysis of these works. For example, in MIST, a similar keyframes-obtaining method based on attention maps is proposed, and SeViLA[1] introduces prompting LLM to get keyframes. Why does the paper choose a 1d CNN to find keyframes, and what is its advantage? What\u2019s more, the results seem not as good as SeViLA in the NExT-QA and STAR datasets, what about the reasons?\n2) Simply adapting the InstructBLIP to VideoQA tasks already achieves relatively strong performances (63.2->69.5), thus the performance gains seem to rely on the pre-trained MLLM (69.5->72.8). Besides, the contributed GCE & ST seem to have weak performance gain in Tab. 5.\n3) The paper aims to handle long video reasoning. STAR only contains videos of 12s on average. More complex benchmarks like AGQA v2 (large-scale compositional reasoning), and ActivityNet-QA (longer videos of 180s on average) are worth evaluating.\n4) Need further qualitative results to prove the effectiveness of the method. \n5) Limitations are not discussed.\n\n[1] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. arXiv preprint arXiv:2305.06988, 2023.\n[2] Sungdong Kim, Jin-Hwa Kim, Jiyoung Lee, and Minjoon Seo. Semi-parametric video-grounded text generation. arXiv preprint arXiv:2301.11507, 2023.\n[3] Tianwen Qian, Ran Cui, Jingjing Chen, Pai Peng, Xiaowei Guo, and Yu-Gang Jiang. Locate before answering: Answer guided question localization for video question answering. IEEE Transactions on Multimedia, 2023."
            },
            "questions": {
                "value": "1) Why remain the \"spatial\" frame feature for question retrieval, does it keep complete video content? What's the difference between the \"spatial frame feature\" and the \"spatial-temporal frame feature\" (ST feature)? Why are the dimensions of these two features not consistent?\n2) What are the differences between the proposed ST feature adapter and the current Adapter-based works, esp. the ST-Adapter (Pan et al. 2022)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission33/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652905297,
        "cdate": 1698652905297,
        "tmdate": 1699635927039,
        "mdate": 1699635927039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L5iKJff7HO",
        "forum": "AtLW9HU3bo",
        "replyto": "AtLW9HU3bo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission33/Reviewer_bWa6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission33/Reviewer_bWa6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to address the task of complex video question answering. To reduce the complexity of previous methods, this paper introduces a two-step approach. Specifically, it first designs a moment adapter to retrieve the question-related frames. Then, it associates corresponding critical information with the general contexts of the unselected part to predict the answer. Besides, it also incorporates lightweight adapters within the frozen image encoder. Experiments are conducted on three datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation of this paper is straightforward and easy to follow.\n2. This paper is well-written and easy to read.\n3. Supplementary file is provided."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. This paper proposes a two-step approach which first retrieves the question-related moment and then achieves reasoning. This process is similar to the coarse-to-fine approach in many temporal grounding methods, for example, but not limited to, \u201cScanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos\u201d. Since the motivation is straightforward, the newly introduced technical designs are not new and not exciting. Therefore, I believe that the novelty is incremental.\n\n2. Missing some relevant references. Since the main approach is coarse-to-fine, the authors should add and compare more related methods to discuss their differences.\n\n3. Experiments are not fair. This paper proposes a two-step approach, directly comparing it with other one-step approaches is unfair. Although this work brings large improvements, it also leads to higher running time and GPU cost. Therefore, the authors should re-implement other two-step approaches from other tasks into the current task for comparison.\n\n4. The efficiency comparison in Table 4 is not convincing. In general, a two-step approach will cost much time and GPU memory. The authors should provide a detailed analysis of each component of the proposed method to demonstrate its efficiency."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission33/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission33/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission33/Reviewer_bWa6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission33/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717156813,
        "cdate": 1698717156813,
        "tmdate": 1699635926966,
        "mdate": 1699635926966,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RDAznaE493",
        "forum": "AtLW9HU3bo",
        "replyto": "AtLW9HU3bo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission33/Reviewer_xUFv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission33/Reviewer_xUFv"
        ],
        "content": {
            "summary": {
                "value": "Existing Multimodal Large Language Models (MLLM) still suffer from complex video question answering (VideoQA) tasks. Currently, they typically uniformly sample sparse frames and simply concatenate them to represent the entire video. However, as long and complex videos typically contain multiple events, the sparse frame sampling strategy may lead to a deficiency of essential information. To this end, they propose an event-aware spatial-temporal reasoning method E-STR. It retrieves the question-critical event before feeding the visual features into the frozen LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The motivation is very clear and natural. Meanwhile, the proposed method is also very straightforward."
            },
            "weaknesses": {
                "value": "+ Although the proposed method can improve the performance of baseline InstructBLIP, it is still hard to demonstrate the results are same as the initial motivation. For example, the sampled events are really important ones.\n\n+ The main contribution of this paper is proposing an event-aware spatial-temporal reasoning strategy for VideoQA. It is still unclear how the proposed framework (cf. Figure 3) can realize \"event-aware\" reasoning."
            },
            "questions": {
                "value": "Based on the results in Table 4, the simple concat-32 baseline already achieves 71.1 in @All metric, which already beat all the listed state-of-the-art baselines in Table 1 (InstructBLIP with 69.5). It would be better to have more explanations about the results? Otherwise, it seems that the compared baselines are not strong enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission33/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846167914,
        "cdate": 1698846167914,
        "tmdate": 1699635926877,
        "mdate": 1699635926877,
        "license": "CC BY 4.0",
        "version": 2
    }
]