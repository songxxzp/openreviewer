[
    {
        "id": "FO3gSMC4Wu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_Q9WM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_Q9WM"
        ],
        "forum": "uwjDyJfe3m",
        "replyto": "uwjDyJfe3m",
        "content": {
            "summary": {
                "value": "The paper proposes a set of \"Benchmarks\" for Offline Reinforcement Learning. Assuming that the RL practitioner can be equipped with some data about the environment plus an imperfect simulator of the \"world\", the authors identify four main ways in which the data and simulator can \"confound\" the RL agent, resulting in suboptimal policies. Guided by those four \"challenges\" for hybrid offline RL approaches, the authors generate synthetic datasets and prepare custom configurations using the Mujoco simulator. They also provide some analysis of how those four aspects affect some standard learning RL methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- There is value in the identification of the main challenges that an RL agent might encounter, especially in the identification of the \"Confounding bias\", which is not as trivially thought of as the other challenges.\n- If made freely-available, the datasets/implementations might assist in the development of new hybrid offline+online RL methods.\n- The authors have a comprehensive view of all the resources an RL designer might get access to and consider it in their modeling."
            },
            "weaknesses": {
                "value": "- The main problem to me is that the work proposed is too incremental for a main track paper at ICLR. While there is value in the identification of the main challenges faced by the RL agent in the offline data+simulator scenario, it is a very incremental step from already-existing datasets/works, and procedurally, generating those benchmarks are relatively easy given Mujoco is a simple to execute and easily-available environment. For instance, someone interested in generating a similar benchmark might not even have to run the environment too many times by leveraging already-existing datasets. Starting from (Fu et al, 2020) datasets, one could modify the datasets by simply (i) changing an state variable; (ii) omitting a recorded state variable (iii) adding noise to the recorded action; (iv) adding noise to observation and action, to replicate a similar dataset generation as the one done by the authors, maybe 2 or 3 of the generated configurations only would require running all the experiment again (and even those, represent a small change in the Mujoco environment).\n\n- A lot of work can be done to improve clarity of the work as well. It's not very clear how the datasets replicate the \"structure\" in Figure 1. The offline data and simulator construction is explained, but how is the agent performance validated? Is there a ground truth environment that is not modified from which performance measures are extracted? \n\n- While the authors' descriptions make sense, there is not enough evidence that the four challenges identified by the authors actually represent what happens in a real environment. I strongly recommend the authors to include a more complex and less-controlled environment than Mujoco, doing the actual work of gathering data and building a simulator for the environment, and showing that those challenges actually represent practical reality. Alternatively, a domain where there are simulators of different fidelities could be used, that way the authors wouldn't need to carry out the work of developing a new simulation.\n\n- I was surprised the authors didn't mention the very related multi-fidelity MDP formulation. The formulation models exactly one of the problems the authors are coping with, imprecise simulators.\n\nSilva, F. L., Yang, J., Landajuela, M., Goncalves, A., Ladd, A., Faissol, D., & Petersen, B. (2023). Toward Multi-Fidelity Reinforcement Learning for Symbolic Optimization. Adaptive and Learning Agents (ALA) Workshop.\n\n- In order to strengthen the contribution of the paper I suggest the authors follow one of the avenues:\n1) Develop a new Hybrid algorithm where you can show clear performance improvement across most of the benchmarks\n2) Introduce a more complex environment to the benchmarks."
            },
            "questions": {
                "value": "- Will all the implementations and datasets be made publicly available if the paper is published?\n- how is the agent performance validated? Is there a ground truth environment that is not modified from which performance measures are extracted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697171690651,
        "cdate": 1697171690651,
        "tmdate": 1699636481629,
        "mdate": 1699636481629,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lKSO8SLKEv",
        "forum": "uwjDyJfe3m",
        "replyto": "uwjDyJfe3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_hLBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_hLBa"
        ],
        "content": {
            "summary": {
                "value": "This paper considers challenges in combining offline data and imperfect simulators in reinforcement learning. The authors first summarize and explain the four major aspects: modeling error, partial observability, state and action discrepancies, and hidden confounding. The main contribution of the paper is a systematic way to create benchmarks for evaluating RL algorithms under the above 4 challenges. Finally, the authors evaluate popular existing methods on the benchmark environments, together with a new approach the authors proposed to combine imperfect simulator with biased data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The writing is clear and presentation of this paper is well organized. It contains good summaries on the challenges which could provide the general audience with valuable background materials."
            },
            "weaknesses": {
                "value": "The exact contributions of this paper seem limited and insignificant. While it provides a good context in terms of the background, the precise benchmarks and experiments do not seem to offer new innovations/insights. Most of the environment modifications for the proposed challenges seem to be either adding noise or masking some dimensions, which probably have been used in previous work except not being formulated precisely as benchmarks. The experiments are also very limited on HalfCheetah and I'm not convinced why would this paper be a good benchmarking paper that the RL community can benefit from.\n\n- This paper might benefit by enlarging the experimental space with diverse environments and a more comprehensive evaluation of algorithms to see if new insights can be drawn.\n- For some environment modifications, the authors mention that \"the choice of values was done based on the results of the SAC algorithm\". Could this introduce in certain bias if solely based on one algorithm?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697555277966,
        "cdate": 1697555277966,
        "tmdate": 1699636481546,
        "mdate": 1699636481546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hqBPIlqKI7",
        "forum": "uwjDyJfe3m",
        "replyto": "uwjDyJfe3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_RmHk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_RmHk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the performance of various (offline) RL methods when biased offline logged data and imperfect simulators are available. The paper also proposes a hybrid method, which uses both simulator and offline data to learn a new policy, and shows its benefits over existing offline RL methods (which only use offline data without simulation) in D4RL benchmarks. The paper publicizes the imperfect simulator used in the experiment as \"Benchmarks for Mechanistic Offline Reinforcement Learning (B4MRL)\"."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. **Considering the practical situation of having an (imperfect) simulator and offline logged data.** The setting this paper considers is reasonable and can be practically relevant in applications where simulators are often used for sim2real transfer (e.g., robotics).\n\n2. **Publicizing a benchmark suite for future research.** Providing a benchmark is beneficial not only for reproducible research but also for facilitating future research on relevant topics.\n\n3. **Highlighting challenges in using simulation and offline datasets.** Specifying challenges of using imperfect simulator and offline logged data can be informative for readers."
            },
            "weaknesses": {
                "value": "1. **The focus of this paper (either benchmarking or proposing a new method) is not clear.** It was unclear to me if this paper focuses on benchmarking or proposing a new hybrid method to leverage simulators and offline logged data. If this paper focuses on benchmarks, more details of the design choices should be discussed, e.g., why specific RL environments are chosen for the benchmark tasks or why specific parameters are chosen for the configurations of imperfect simulators and offline data. In contrast, if the paper focuses on proposing a new hybrid method, a more detailed discussion of existing work is needed, and logic behind why the proposed method works (e.g., theoretical analysis) should be explained. Of course, a research paper can have both benchmarking and method proposal as two contributions of the paper, however, this paper lacks the details in both contributions.\n\n2. **Lacks the theoretical analysis of the four challenges.** The paper describes the intuition of the four challenges in using a simulator and offline data, however, its influence on the performance is unclear. For example, providing the upper bound of the performance decrease caused by the modeling error of the simulator and that caused by the bias of logged data can be informative for the readers. The manuscript is ambiguous about how the simulator and offline logged data compensate for errors with each other.\n\n3. **Lacks the discussion of related work.** From the manuscript, I could not figure out if the proposed method is the only approach trying to combine the use of a simulator and offline data due to the lack of discussion of existing approaches. In addition, the advantage of the proposed method over model-based offline RL approaches was not discussed. For example, is the proposed method more effective than a representative model-based offline RL method proposed (e.g., COMBO (Yu et al., 21)) and why the proposed method performs better?\n\nYu et al, 21: Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, Chelsea Finn. \"COMBO: Conservative Offline Model-Based Policy Optimization\". NeurIPS, 2021.\n\n4. **The presentation of the paper should be improved.**"
            },
            "questions": {
                "value": "1. What kind of errors do the simulator and offline data compensate for each other?\n\n2. What are the advantages of the proposed method over existing model-based offline RL methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4952/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4952/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4952/Reviewer_RmHk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698020319401,
        "cdate": 1698020319401,
        "tmdate": 1699636481462,
        "mdate": 1699636481462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "a166S5n3V9",
        "forum": "uwjDyJfe3m",
        "replyto": "uwjDyJfe3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_nns3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4952/Reviewer_nns3"
        ],
        "content": {
            "summary": {
                "value": "This work studies the hybrid RL setting that combines offline datasets with simulators. It first outlines four challenges pertaining to offline2real and sim2real, together with some brief descriptions of real-world examples: (i) simulator error, (ii) state discrepancy, (iii) action discrepancy, (iv) unobserved confounding.\n\nIt then presents a set of benchmarks modified from D4RL-mujoco to exhibit each of the four challenges. Benchmarking results are provided for purely simulator-based online RL, purely dataset-based offline RL, and hybrid approaches that make use of simulator and dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The hybrid RL problem studied by this paper has high practical importance. \n- The proposed procedures to induce the practical challenges are systematically presented, and could be generalizable to different environments."
            },
            "weaknesses": {
                "value": "- Result section could benefit from some organization to bold/highlight the key takeaways of different sub-experiments.For example, what are some general trends in terms of when hybrid methods perform better than simulator-based or offline RL?"
            },
            "questions": {
                "value": "- The benchmark description says both mujoco and Highway environments are used. Where is the results for Highway?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4952/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698527101690,
        "cdate": 1698527101690,
        "tmdate": 1699636481375,
        "mdate": 1699636481375,
        "license": "CC BY 4.0",
        "version": 2
    }
]