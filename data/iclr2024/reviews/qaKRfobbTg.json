[
    {
        "id": "imTyA8JVvG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_MPBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_MPBo"
        ],
        "forum": "qaKRfobbTg",
        "replyto": "qaKRfobbTg",
        "content": {
            "summary": {
                "value": "This paper studies the sample complexity of active threshold learning when samples are censored. The authors then apply their offline results to provide a tight regret guarantee for the online version of the problem. \n\nThe model is as follows: there is a latent variable $v$ in $[0,1]$ sampled according to an unknown distribution and a reward function $g$ that maps (threshold, latent variable) pairs in $[0,1]$. The learner repeatedly and adaptively queries the value of $g$ on specific thresholds (for i.i.d. realizations of the latent variable) and obtains censored feedback, i.e., if the threshold is larger than the latent variable, it does not observe anything. The goal of the learner is to get an $(\\varepsilon,\\delta)$ approximation of the best threshold, using as few (adaptive) samples as possible. \n\nThe results are as follows: \nin the general case, the sample complexity is infinite (Theorem 3.1.)\nwhen function $g$ is Lipshitz or the latent variable distribution has Lipshitz CDF, then $\\tilde O(\\tfrac{1}{\\varepsilon^3})$ samples are enough (Theorems 4.1 and 4.2)\nthese bounds are tight up to poly-logarithmic terms (Theorem 4.3)\nthese results immediately imply a $\\Theta(T^{2/3})$ minimax regret regime for the online learning version of the problem"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Censored data are interesting as they naturally arise from applications and have been studied both in the sample complexity and online learning literature. The paper is well written and the proofs in the main body are easy to understand. The fact that the authors provide a complete picture of the problem is compelling."
            },
            "weaknesses": {
                "value": "My main concern with the paper, which motivates my low score, is given by the technical contribution of the paper, as all the theoretical results are not too surprising and build on known techniques.\n- the needle in a haystack phenomenon is known and has been used before in pricing context, see e.g., [1] \u201cA Regret Analysis of Bilateral Trade\" EC, 2021\n- Theorems 4.1 and 4.2 are not surprising: if some regularity or smoothness assumptions are made then the objective becomes Lipshitz in the threshold, and an epsilon grid on the threshold space immediately yields the desired result see. e.g., [2] \u201cBandits and Experts in Metric Spaces\u201d J ACM, 2019.\n- Theorem 4.3 is the more involved result but is not too surprising: the construction entails a family of distributions with $\\Theta(\\tfrac 1{\\varepsilon})$ candidate optimal thresholds where each candidate needs to be evaluated $\\Omega(\\tfrac 1{\\varepsilon^2})$ times. This has already been done, e.g., in the $\\Omega(T^{2/3})$ lower bounds in [3] \u201cThe Value of Knowing a Demand Curve: Bounds on Regret for Online Posted-Price Auctions\u201d FOCS 2003.\n\nThe problem studied, and the techniques used, are closely related to Lipshitz bandits [2], pricing [3] and bilateral trade [1]. Please consider a more thorough comparison with the already known results and techniques there."
            },
            "questions": {
                "value": "- It seems like Theorem 5.1. could be a Corollary of [2] (for the upper bound, once Lipshitzness in the threshold has been established) and [3] (for the lower bound). Is it correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Reviewer_MPBo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697210150727,
        "cdate": 1697210150727,
        "tmdate": 1699636819831,
        "mdate": 1699636819831,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pqrN6yWDKD",
        "forum": "qaKRfobbTg",
        "replyto": "qaKRfobbTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_Z8bt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_Z8bt"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a new setting of actively learning threshold in latent space with censored feedback, where the rewards can be observed only when the threshold is lower than or equal to the unknown latent value. The reward function g is defined over both the proposed threshold and latent value.  They proved that the query complexity can be infinitely large even when the reward function is monotone with respect to the threshold and the value. When adding assumptions of Lipschitz CDF, they proved tight query complexity up to logarithmic factors. They also extended to the online learning setting, related it to continuous-armed Lipschitz bandit, with theoretical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Clear motivation examples for the proposed setting\n- Provided high-level ideas and intuition for proofs, which helps readers understand the theorems\n- Provided the query complexity lower and upper bound for both Lipschitz and general reward distributions. Table 1 summarizes the results and is very clear.\n- Link to continuous-armed Lipschitz bandit in an online learning setting is interesting, with theoretical results."
            },
            "weaknesses": {
                "value": "- It is not clear how the theoretical results provided in Table 1 and Section 5 (online learning) compare with related/previous work. In related work, the authors mentioned a few closely related works, it would be helpful to provide a detailed discussion or comparison when applicable.\n- No experimental results. It would be good to show empirical results which verify the theoretical results (see below question 1). Additionally, it would be even better if the toy example could correspond to the motivation example provided in the paper. \n\nHappy to raise the score if my main concerns are addressed."
            },
            "questions": {
                "value": "- Can you provide a few toy examples as running examples to explain the setup and theorems (Table 1)? e.g. concrete distributions under different assumptions, possible (\\epsilon, \\delta)-estimator and corresponding query complexity bounds, etc. \n- For the online setting, the proposed setup links to continuous-arm one-sided Lipschitz bandit problem. can you add the related work about this? Also, as mentioned in the weakness, adding a comparison to the existing results in bandits literature is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698526620827,
        "cdate": 1698526620827,
        "tmdate": 1699636819699,
        "mdate": 1699636819699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bl4AwCtDqj",
        "forum": "qaKRfobbTg",
        "replyto": "qaKRfobbTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_F7cS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_F7cS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an abstraction of a threshold learning problem that arises in a number of applications such as reserve price learning in single-item auctions, crowd-sourced task allocation/data collection, and theoretical models of hiring. Here, the reward function is specified by a value and a threshold. The threshold is chosen by the learner, the value is then drawn from an unknown distribution, and the learner observes the reward if the value exceeds the threshold. \n\nFirst, the authors construct the existence of a monotone reward function and a \u201chard\u201d distribution over values involving point masses that requires an unbounded query complexity to learn the optimal threshold. The authors then give tight query complexity bounds under Lipschitzness assumptions on the reward and value distributions. Finally, an extension to the online setting is studied."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The question posed has a clear motivation and is a nice abstraction of questions like reserve price learning. The paper itself is a nice complete contribution, characterizing hardness and giving tight query complexity bounds in tractable cases. The writing is clear and precise. The lower bound constructions are interesting."
            },
            "weaknesses": {
                "value": "The proof of the main query complexity upper bound in Theorem 4.1 doesn\u2019t seem too surprising. Under conditions on the distribution, concentration bounds allow the true reward at any given threshold point to be sufficiently estimated, and then the learner runs that estimation for a suitable discretization of possible threshold values. The same proof gives the main upper bound results for the other settings as well.\n\nOverall, my main concern is with the originality and novelty of the question being studied. While it is a neat way to abstract away from more specific applications like reserve price learning, and I think the results presented are certainly nice ones, the overall motivation through the examples of reserve pricing, crowdsourcing, and hiring are not enough to convince me that this is a sufficiently novel work for publication in ICLR. Perhaps a more fine-grained structural analysis in terms of practically-motivated properties of the reward function is possible here, which would make the story more compelling.\n\nThe assumption that the learner knows the Lipschitz constant of the unknown distribution in the querying algorithm in the proof of Theorem 4.1 seems too strong given the premise of the value distribution being unknown. Perhaps the results could be augmented to remove this assumption, and L is a parameter that must be learned as a part of the querying?"
            },
            "questions": {
                "value": "Are there specific examples of reward functions g that the authors can give that fit the different conditions of their main theorems? Specifically, reward functions that go beyond the simple case of reserve price learning. If such examples could be further motivated in the context of the other two examples (crowdsourced data collection and hiring) presented, that would solidify the premise of the paper.\n\nI think the authors should include all parameters such as the Lipschitz constant L in their query complexity bounds. \n\nThe query policy in the proof of Theorem 4.1 requires that the learner knows the Lipschitz constant L of the CDF of the unknown value distribution. So the value distribution is not really fully unknown here. Is there any way to remove the assumption that the learner knows L, or could that lead to exponential/infinite query complexities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Reviewer_F7cS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675774248,
        "cdate": 1698675774248,
        "tmdate": 1699636819571,
        "mdate": 1699636819571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GizRlcfksc",
        "forum": "qaKRfobbTg",
        "replyto": "qaKRfobbTg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_sD7K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7000/Reviewer_sD7K"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the question of learning threshold in latent space. In this problem, we have an unknown reward function $g(\\gamma, v)$ and unknown distribution on $v$ and need to pick $\\gamma$ that maximizes $E_{v}[g(\\gamma, v) \\cdot [v \\ge \\gamma] ]$. \n\nThe paper presents the sequence of impossibility and positive results. The first contribution is the proof that query complexity is infinitely large for general monotone functions. The second contribution is the series of positive results when CDF of distribution $v$ is Lipshitz and when g is one-sided Lipshitz with respect to $\\gamma$. In both cases, algorithms achieve $O(\\epsilon^{-3})$ sample complexity for learning $\\epsilon$ approximation.  Additionally, the authors complement this result with matching lower bounds."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces an important problem that looks very natural and has applications. This is an interesting and practically relevant setting that captures real-world scenarios like setting reserve prices in auctions, difficulty levels in crowdsourcing, and hiring bars in recruiting. \n\nThe paper provides a solid theoretical analysis. The authors achieve tight results. The results are technically sound.\n\n\nThe problem formulation and results are clearly explained. The paper is well-written and easy to follow.\n\nOverall, the paper studies an interesting problem and provides nice theoretical results that advance our understanding of learning in latent spaces."
            },
            "weaknesses": {
                "value": "The applications discussed in the introduction could be expanded with more practical details. For example, how do different auction formats correspond to different functions g?\n\n The high-level ideas and intuition could be emphasized more.\n\n Some simple experiments on synthetic data would help support the theoretical results."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7000/Reviewer_sD7K"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7000/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791433752,
        "cdate": 1698791433752,
        "tmdate": 1699636819421,
        "mdate": 1699636819421,
        "license": "CC BY 4.0",
        "version": 2
    }
]