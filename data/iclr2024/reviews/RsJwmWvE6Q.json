[
    {
        "id": "1is8YNEcP2",
        "forum": "RsJwmWvE6Q",
        "replyto": "RsJwmWvE6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_k2z9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_k2z9"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of estimating the Frobenius norm residual error of the best rank-$k$ low-rank matrix approximation and the $\\ell_p$-norm residual error of the best $k$-sparse vector approximation. The main result, for the matrix case, is that best rank-$k$ approximation error can be approximated to relative error $\\varepsilon$ using a bilinear compression $SAT \\in \\mathbb{R}^{O(k/\\varepsilon^2) \\times O(k/\\varepsilon^2)}$ of $A$ by two projection-cost-preserving sketches $S$ and $T$. The new result improves the dependence on $\\varepsilon$ from (Andoni and Nguyen, 2013); a new lower bound establishes the optimality of the current method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a nice work of theoretical computer science. The writing is good and reasonably clear. The fact that the projection-cost-preserving sketch technology allows for an improvement of the Andoni\u2013Nguyen result is very interesting and, to the best of my knowledge, novel. The lower bound is nice as well, and it establishes the optimality of the proposed approach (though I have my concerns about the correctness of Lemma 3.2)."
            },
            "weaknesses": {
                "value": "As a work of _theoretical computer science_, this paper is perfectly adequate. As a work of _machine learning_, it has a few deficiencies.\n\n### Use Case\n\nWhen is the proposed method useful? The story the authors tell feels a bit farfetched to me:\n\n- Using a generalized Nystr\u00f6m approximation and OSNAP sketches, a rank-$k$ approximation can be computed in a single pass over the matrix. For a dense matrix stored on disk, the time cost is roughly $O(mn \\log k + (m+n) k^2)$; the cost of the proposed residual estimate is roughly $O(mn)$. The difference between these costs is not very significant, particularly if $k \\ll m,n$. \n- The authors suggest data streams as a potential application. Indeed, the proposed approach does give a way of estimating the residual error from a stream in low space. After which, the authors say one can use the residual estimate to decide whether or not low-rank approximation is worth it. But this raises two objections: 1) If one is going to compute a low-rank approximation anyways, then one will have to eat the space cost of storing such an approximation eventually. The proposed low-space diagnostic doesn't seem useful if, conditional on a positive outcome, actually computing the approximation needs more space. 2) Often in the streaming setting, we assume we get only a single pass over the data. The authors approach requires two passes: one to compute the diagnostic and a second to compute a low-rank approximation.\n\nIt may be possible that there is some use case where the proposed approach is natural, but it seems to me that the proposed method probably has fairly limited usefulness in practice.\n\n### CountSketch\n\nI find the author's use of CountSketch to be questionable, particularly in the numerical experiments. In my experience, an OSNAP (even with just 2 entries of sparsity per column) is dramatically more safe and effective than CountSketch in practice.\n\nTo demonstrate this, I ran my own experiment. Let $A = \\operatorname{diag}( i^{-6} : i=1,2,\\ldots,10^4)$ and, following Table 1, consider $m = 50$ and $k = 20$. With CountSketch, the estimate $\\| SAT - (SAT)_k \\|_F$ is a factor of **58** smaller than the true residual error $\\| A - A_k \\|_F$. By contrast, OSNAP with _just 2 nonzeros per column_ only underestimates the residual error by a factor of 2! For the proposed use case of screening whether or not significant compression is possible by low-rank approximation, an estimate within a factor of two is probably useful; one that is wrong by more than an order of magnitude is not.\n\nThis is a paper about theoretical computer science: Rigorous analysis of computational methods so that they work in practice even on worst-case inputs. The use of a plain CountSketch in the numerical experiments, now demonstrated to be able to severely underestimate the residual error in practice, should be removed in favor of the OSNAP or, perhaps, the composite CountSketch\u2013Gaussian sketch.\n\nAs a side note, the introduction claims that it will prove that the method works if $S$ and $T$ are chosen to be OSNAP's. But this is omitted from the main text. This should be included, because OSNAP's are often the most effective sketching matrices in practice.\n\n### Numerical Evaluation\n\nThe runtime of a full, dense SVD is not the appropriate comparison in Table 1. Rather, the appropriate comparison is a _randomized low-rank approximation_ such as the randomized SVD or generalized Nystr\u00f6m (with OSNAP's, if the latter).\n\n### Accuracy\n\nAs I mention above and as the authors tacitly admit with their numerical experiments, an error estimate which is within a factor of two is often sufficient, particularly if the proposed use case is determined whether or not to run a low-rank approximation algorithm. Viewed in this light, the improvement of the sketch dimension from $O(k/\\varepsilon^3)$ to $O(k/\\varepsilon^2)$ in the present work is not that meaningful in practice.\n\n### Sparse Approximation\n\nThe residual error for $k$-sparse approximation of vectors in $\\ell_p$ norms for $p > 2$ is very much a question of interest to theoretical computer science. I fail to see any interest for practical problems in machine learning.\n\n### Conclusion\n\nUltimately, I feel like this paper would be more at home at a dedicated TCS publication venue. I'm not sure when I would actually run the author's proposed method in practice, and the improvement from $O(k/\\varepsilon^3)$ to $O(k/\\varepsilon^2)$ has limited practical implications as well. I have significant concerns with the use of plain CountSketch in the numerical experiments. If the numerical experiments were fixed to avoid the use of plain CountSketch, I would be inclined to give a soft recommendation for acceptance."
            },
            "questions": {
                "value": "### Is Lemma 3.2 correct?\n\nThe proof of Lemma 3.2 is not convincing to me in several ways. First, why does $d_{TV}(S(G), S(G+\\beta \\sqrt{\\varepsilon/k} \\cdot wz^\\top) \\le 1/10$ and $\\beta \\sqrt{\\varepsilon/k} \\cdot \\|w\\|\\|z\\|$ imply $d_{TV}(S(G), S(G + c\\alpha\\sqrt{\\varepsilon} \\cdot uv^\\top)) \\le 1/9$. I believe the ultimate result is probably true, but I don't following the line of reasoning.\n\nI'm not sure data processing and linearity of the sketch is enough to conclude that $d_{TV}(\\mathcal{L}_1,\\mathcal{L}_2)$. The perturbation $c\\sqrt{\\varepsilon}B$ is applied to both $G$ and $G + c\\alpha \\sqrt{\\varepsilon} uv^\\top$ and it is _dependent_ with the latter of these. I'm not sure exactly what the author's argument is. It appears that the authors are making a claim of the form:\n\n> If $d_{\\rm TV}(X,Y) \\le \\varepsilon$, then $d_{\\rm TV}(X+Z,Y+Z)\\le \\varepsilon$.\n\nThis claim is certainly not true. Let $X$ be a random variable which is never $0$ and take $X = -Y = Z$. Then $d_{\\rm TV}(X,Y) = 0$ but $d_{\\rm TV}(X+Z,Y+Z) = d_{\\rm TV}(2X,0) = 1$. \n\nI'm not sure how exactly the authors purport to conclude the proof of lemma 3.2. The authors should revise this proof to make unassailable the correctness of this result."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Reviewer_k2z9",
                    "ICLR.cc/2024/Conference/Submission8639/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698013695582,
        "cdate": 1698013695582,
        "tmdate": 1700677654737,
        "mdate": 1700677654737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UJUvvqjb4D",
        "forum": "RsJwmWvE6Q",
        "replyto": "RsJwmWvE6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_Mz93"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_Mz93"
        ],
        "content": {
            "summary": {
                "value": "This paper researches the $\\textit{residual error estimation}$ problem. To be specific, for a matrix $A$, one estimates $|| A - A_k ||_F$ and judges if it is worthwhile to compute a low-rank approximation $\\widehat{A}$ such that $|| A - \\widehat{A} ||_F \\le (1 + \\varepsilon) || A - A_k ||_F$, where $A_k$ is the best rank-$k$ approximation to $A$; for a vector $x$, one estimates $|| x - x_k ||_p$ and determines whether to compute a $k$-sparse vector $\\widehat{x}$ such that $|| x - \\widehat{x} ||_p \\le (1+\\varepsilon) || x - x_k ||_p$, where $x_k$ is the best $k$-sparse approximation to $x$.  \nGiven a matrix $A$, for the estimator $|| SAT - [SAT]_k ||_F$ of the residual error $|| A - A_k ||_F$, this paper obtains the lower bound and upper bound of the dimensionality $s, t = \\Theta(k / \\varepsilon^2)$, which improves the upper bound $O(k / \\varepsilon^3)$ in [Andoni and Nguyen'13].  \nGiven a vector $x$, for the estimator of $|| x - x_k ||_p$ with $p > 2$, this paper gives the upper bound $\\widetilde{O}(\\varepsilon^{-2p/(p-1)} k^{2/p} n^{1-2/p})$ for space. This result can be extended to the $\\ell_p$ sparse recovery problem, in which the upper bound almost matches the obtained lower bound $\\Omega(k^{2/p} n^{1-2/p})$ when taking $\\varepsilon$ as a constant."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) For low-rank residual error estimation, this paper gives the lower bound $\\Omega( k / \\varepsilon^2 )$ for the dimensionality of the sketching matrices $S$ and $T$, and the upper bound $O(k / \\varepsilon^2)$, which matches the lower bound and improves the upper bound $O(k / \\varepsilon^3)$ in [Andoni and Nguyen'13].     \n(2) For residual error estimation of vector $\\ell_p$ norm with $p > 2$, this paper gives an upper bound $\\widetilde{O}(\\varepsilon^{-2p/(p-1)} k^{2/p} n^{1-2/p})$ for the dimensinality of the sketching matrix. Also, this upper bound can be extended to $\\ell_p$ sparse recovery problem and almost matches the obtained lower bound $\\Omega(k^{2/p} n^{1-2/p})$ when $\\varepsilon$ is a constant.   \n(3) The experimental results indicate that the proposed algorithm has a comparable error $\\varepsilon$ but at least ten times faster than the existing method [Andoni and Nguyen'13].    \n(4) This paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "In the experiments, this paper only proves the efficiency of the proposed algorithm for low-rank approximation. Can this paper supplement experiments for the residual error estimation of vector norms?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Reviewer_Mz93"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698281317026,
        "cdate": 1698281317026,
        "tmdate": 1699637081823,
        "mdate": 1699637081823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9JzUFCdTii",
        "forum": "RsJwmWvE6Q",
        "replyto": "RsJwmWvE6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_QAtz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_QAtz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of estimating the residual error using linear sketching. Specifically it focusses on the following two problems:\n\n1) For any matrix A, the task is to estimate $||A-A_k||_F$ within a $(1+\\epsilon)$-factor where $A_k$ is the best rank-k approximation to $A \\in \\mathbb{R}^{n \\times d}$. For this problem, it is shown a bilinear sketch of the form $SAT$ where the sketching matrices $S \\in \\mathbb{R}^{O(k/\\epsilon^2) \\times n}$ and $T \\in \\mathbb{R}^{d \\times O(k/\\epsilon^2)}$ can be sparse countsketch matrices with a single $\\pm 1$ in their columns. This gives a sketch of size $O(k^2/\\epsilon^4)$. A lower bound is given which shows S and T must have $O(k/\\epsilon^2)$ rows and columns respectively for any algorithm which takes as input $S,T,SAT$ and outputs a $(1+\\epsilon)$ residual ran-k approximation. This shows the upper bound is tight for bilinear sketches.\n\n2) The next problem is estimating $\\| x-x_k\\|_p$ upto $(1 \\pm \\epsilon)$ residual error for any vector $x \\in \\mathbb{R}^{n}$ which is updated in the streaming model, for any $l_p$ norm with $p>2$ and where $x_k$ is the the vector containing the top-k elements of $x$. The algorithm uses countsketch to find the indices of top-k elements of $x$ approximately and parallely uses a known $F_p$ frequency estimation procedure for these coordinates and subtracts the approximate value of these coordinates to get the residual error. This yields an upper bound of $\\tilde{O}(\\epsilon^{-2p/(1-p)}k^{2/p} n^{1-2/p} )$ and also gives a $k$-sparse approximation for $x$. A lower bound of $\\Omega(k^{2/p} n^{1-2/p})$ is also shown for the $k$-sparse recovery problem.\n\nSome experiments are performed with the sparse sketching matrices."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.  For the residual matrix norm estimation problem, the paper presents the bilinear sketches which are optimal in size. Moreover, since the sketching matrices can be sparse, they can be computed are pretty fast. Previous results for this problem used dense sketching matrices and had a larger upper bound of $O(k^2/\\epsilon^6)$ on the size of the sketch. The proof for the sketching algorithm is pretty straightforward from results on projection cost preserving sketches. The lower bound proof follow via minimum singular value separation results for random Gaussian matrices.\n\n2. For the residual vector norm recovery problem, the algorithm also outputs a $k-sparse$ approximation in $l_p$ norm which is optimal upto polylog $n$ factors (though the dependence on $\\epsilon$ is unclear). The lower bound proof follows from reducing the gap infinity problem to the $l_p$ sparse recovery problem.\n\n3. Though some of the results are pretty straightforward from prior results (for example, the bilinear sketch upper bound follows directly from the PCP sketch properties), they are an interesting addition to the sketching literature.\n\n4. The paper is mostly well written and easy to follow apart from some typos etc. (see weakness and questions)"
            },
            "weaknesses": {
                "value": "1) It is unclear how good the upper for the residual norm estimation problem as compared to the optimal. The given lower bound applies to the sparse recovery problem. But this is a harder problem than the norm residual error estimation problem. Some discussion on the actual lower bound for the norm estimation problem might make things more clear for the reader. Also, though the upper bound for the sparse recovery problem is optimal in terms of n and k, it is unclear on what the correct dependence on $\\epsilon$ is.\n\n3) There are some typos etc. in the paper which should be corrected:\n      1) In definition 3.4, $S$ should be $s \\times n$\n      2) The proof of theorem 3.7 should refer to lemma 3.5 instead of 3.4."
            },
            "questions": {
                "value": "1) Can anything be said regarding how good $SAT$ is as compared to $A_k$, the best rank-k approximation to A?\n\n2) I'm confused about the the proof of Lemma 4.3. It seems we should have $(I \\cap J) \\cup T_1 \\cup \\ldots T_{m+1} \\subseteq I$ instead of exactly being equal to $I$ as it could happen that $i \\in I \\backslash J$ and  $x_i > || x_{-k}||_p $  in which case $i$ wouldn't belong to any of \n\nthe sets $ T_1, \\ldots T_{m+1}$? It seems instead of decomposing I, decomposing J as $ J= (I \\cap J) \\cup T_1 \\cup \\ldots T_{m+1}$ where each $T_i$ constains elements of $J$ not in $I$ in the given interval and then doing the analysis should given the same result?\n\nRemark: I have not checked whether Lemma 5.2 of Price and Woodruff, 2011 indeed generalizes to p>2. This is used in Lemma 4.10."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Reviewer_QAtz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808456775,
        "cdate": 1698808456775,
        "tmdate": 1699637081631,
        "mdate": 1699637081631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tWX7SYStXn",
        "forum": "RsJwmWvE6Q",
        "replyto": "RsJwmWvE6Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_FqHJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8639/Reviewer_FqHJ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the (approximate) estimation of residual error of rank-k approximation of matrix and k-sparse recovery of vector using linear sketches. For the matrix case (Frobenius norm), the main result is a bilinear sketch of size $O(k^2/\\epsilon^4)$ by combining the count sketch and JL sketch; this improves upon the previous best result $O(k^2/\\epsilon^6)$ [Andoni-Nguyen, SODA'13]. The authors also provide a matching lower bound. For the $k$-sparse recovery under $\\ell_p$ norm for $p>2$, the author show an upper bound of $\\tilde{O}(k^{2/p}n^{1-2/p}$. This is done by two sketches, where one is used to find heavy hitters, and another to do a $F_p$ estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this paper is technically solid and solves an interesting open problem. The high-level explanation provided in the paper are helpful to the readers."
            },
            "weaknesses": {
                "value": "1. For the matrix case, the improvement compared to Andoni and Nguyen seem a bit incremental.\n2. While I think it makes sense to estimate the residual error of matrix approximations, I think there is little use to estimate the residual error of sparse recovery under $\\ell_p$ norm."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8639/Reviewer_FqHJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8639/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699154363926,
        "cdate": 1699154363926,
        "tmdate": 1699637081496,
        "mdate": 1699637081496,
        "license": "CC BY 4.0",
        "version": 2
    }
]