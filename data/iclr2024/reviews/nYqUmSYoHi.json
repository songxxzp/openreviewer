[
    {
        "id": "AHI5wcVAQ7",
        "forum": "nYqUmSYoHi",
        "replyto": "nYqUmSYoHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_V285"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_V285"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the training of deep neural networks for multi-label learning through the lens of neural collapse. Its main contributions   are summarized as:\n\na. This paper shows that the last-layer features and classifier learned via overparameterized deep networks exhibit a more general version of neural collapse.\n\nb. This paper studies the global optimality of a commonly used pick-all-label loss for M-lab and proves that the optimization landscape has benign strict saddle properties so that global solutions can be efficiently achieved."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "a. This paper is well-written and easy to follow.\n\nb.  I appreciate that this paper provides extensive experiments.\n\nc. Interesting findings. This paper shows that the last-layer features and classifier learned via overparameterized deep networks exhibit a more general version of NC. The high-order Multiplicity features are scaled average of their associated features in Multiplicity-1."
            },
            "weaknesses": {
                "value": "My main concern is the novelty of results. The main results Theorems 1 and 2 are so similar with the reference [1], i.e. Theorem 1 corresponds to Theorem 3.1 of [1] and Theorem 2 corresponds to Theorem 3.2 of [1]. In my opinion, these results are the extended versions of [1] with a few improvements for multi label learning. It is OK to leverage them, but they are not enough to be the main contributions in this paper. \n\n[1] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820\u201329834, 2021."
            },
            "questions": {
                "value": "a. Please discuss more about the results of reference [1]\n\n[1] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820\u201329834, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692589744,
        "cdate": 1698692589744,
        "tmdate": 1699636797205,
        "mdate": 1699636797205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nF2nnZ8ZI2",
        "forum": "nYqUmSYoHi",
        "replyto": "nYqUmSYoHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_mLb8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_mLb8"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the training of neural networks for multi-label learning. The analysis aims to show a neural collapse-type phenomenon when minimizing a pick-all loss function to address the multi-label learning task. By treating the features of every sample as a free optimization variable, the paper formulates and analyzes the optimization problem in equation (4), for which they characterize the global optima (Theorem 1) and show that all local optimal will be globally optima (Theorem 2). Several numerical results are discussed in section 4 to measure the M-lab ETF in training the neural network on multi-label learning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- The paper is well-written and easy to follow. The authors present their results clearly, and the writing is overall in great shape.\n\n2- The paper focuses on the interesting subject of neural net training dynamics in multi-label learning tasks."
            },
            "weaknesses": {
                "value": "1- While I understand that analyzing the problem in (3) could be challenging in the general case, I think the simplification of treating every feature vector $h_i = \\phi_\\theta(x_i)$ as a free optimization variable is restrictive. Could the analysis extend to a more general choice of $\\phi_\\theta$, e,g, a one-layer overparameterized neural network with some activation function? The authors may still be able to show a weaker result on local optima or stationary points of the objective.  If not, the paper should discuss why the analysis will be challenging for a parameterized neural net function $\\phi_\\theta$ and some more evidence of why the assumption of treating the features as free variables would make sense for deep neural nets. \n\n2- The theoretical results only analyze the critical points of the loss function, and the paper has no statement on how a gradient-based optimization method will perform in solving (3) or (4). Stating a corollary or theorem on the convergence behavior of a first-order algorithm for optimizing (4) wold be a nice addition. I think Theorems 1,2 could connect the first or second-order stationary points of the objective to the global minima, and so the authors should be able to use the results in the optimization literature to state such a convergence guarantee for a gradient-based optimization algorithm."
            },
            "questions": {
                "value": "Please see my comments for weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6866/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699179780906,
        "cdate": 1699179780906,
        "tmdate": 1699636797071,
        "mdate": 1699636797071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G48xKh2AtT",
        "forum": "nYqUmSYoHi",
        "replyto": "nYqUmSYoHi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_VsPM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6866/Reviewer_VsPM"
        ],
        "content": {
            "summary": {
                "value": "The paper explores neural collapse (NC) phenomenon in multi-label (M-lab) learning using deep neural networks.The main content is\uff1a\n1.The paper provides theoretical analysis to show multi-label NC is the global solution under the unconstrained feature model.\n2.The paper introduces the concept of multi-label equiangular tight frames (ETF) to characterize NC geometry.\n3.They  empirically demonstrate multi-label NC on practical networks trained on synthetic and real datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.The paper is well-written and easy to follow. The graphics in this article are very intuitive.\n2.This article combines NC and M-lab for the first time\uff0cproviding a new perspective for the study of multi-label."
            },
            "weaknesses": {
                "value": "The theoretical part of this article is very similar to [1], lacking significant technological innovation and not sufficiently novel.The two papers share a lot of similarities in  technical approach and theoretical analysis frameworks towards extending and understanding the representation geometry of neural networks in multi-label learning tasks via NC.I don't see any important work in your theoretical section that goes beyond what has been done in [1].So, despite the interesting combination of NC and M-lab\uff0c I don't consider it as a contributory work.\n\nRef:\n[1] Zhu, Zhihui, et al. \"A geometric analysis of neural collapse with unconstrained features.\" Advances in Neural Information Processing Systems 34 (2021): 29820-29834."
            },
            "questions": {
                "value": "Please see weekness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6866/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6866/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6866/Reviewer_VsPM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6866/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700278996827,
        "cdate": 1700278996827,
        "tmdate": 1700279745687,
        "mdate": 1700279745687,
        "license": "CC BY 4.0",
        "version": 2
    }
]