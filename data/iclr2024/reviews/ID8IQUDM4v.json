[
    {
        "id": "NWSJFXNzER",
        "forum": "ID8IQUDM4v",
        "replyto": "ID8IQUDM4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on a classification scenario that the central server will take as input compressed data from distributed devices and then execute classification on those data. The paper claimed that the traditional linear dimension reduction methods like PCA and LDA cannot achieve good results especially when the distributed devices have different compression rates. Therefore they proposed to use a trainable linear transformation to accomplish the compression."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is easy to understand and the proposed method is quite simple."
            },
            "weaknesses": {
                "value": "- The method is too simple to be published in a top-tier machine learning conference. \n- I personally have some concerns about the experimental results.   \nPlease refer to the questions part. Thanks."
            },
            "questions": {
                "value": "Major Problems:\n- Theoretical Section (3.1): The theoretical content in Section 3.1 appears to be somewhat redundant and perhaps unnecessary. In the current landscape of neural network research, discussions on the global optimality of neural networks have gained prominence [1,2]. Theorems 1 and Proposition 1 do not seem to introduce any particularly unique or insightful content compared to these existing discussions.\n- Experimental Settings: The choice of experimental settings raises some concerns. Since the paper deals with classification tasks, it is imperative to conduct experiments using well-established neural network models such as ResNet and Transformer, rather than Random Forest and K-Nearest Neighbors. Additionally, using more standard and widely recognized classification datasets like CIFAR10, CIFAR100, and ImageNet would be also necessary for evaluation. The choice of MNIST and a simple face classification dataset might be considered less suitable (it's weird to choose a face classification dataset as well). \n- Experimental Results: The experimental results presented in the paper are not entirely convincing. The lack of details on how PCA or LDA was employed in the experiments is a significant concern. To ensure fairness and clarity in the experiments, it is essential to compare the proposed method with PCA by only switching the $W_1$ to $W_N$ in Figure 3 to PCA, keeping both the DNN part and the transformations in the server side unchanged, and then training the neural network with the PCA-based data. Moreover, PCA, while not trainable like the proposed method, is still a linear transformation, and the paper should justify the observed test error gap between the two methods.\n\nMinor Problems:\n- Reference Format: The format of the references appears to be problematic, with the reference text mixed with the main text. I suggest reviewing and correcting the reference format to ensure it aligns with standard citation conventions.\n- Paper Title: The paper title may require reconsideration. The current title suggests that the proposed reduction method is assisted by the centralized neural network. However, it might be more appropriate to frame it as the centralized neural network being assisted by the proposed reduction method in adapting to data with varying compression rates.\n\n[1] Haeffele B D, Vidal R. Global optimality in neural network training, CVPR2017.    \n[2] Sun R. Optimization for deep learning: theory and algorithms[J]. arXiv preprint arXiv:1912.08957, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Reviewer_Tdne"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698008727767,
        "cdate": 1698008727767,
        "tmdate": 1699636635806,
        "mdate": 1699636635806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yPEkFX5s5C",
        "forum": "ID8IQUDM4v",
        "replyto": "ID8IQUDM4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_X1NB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_X1NB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a linear dimensionality reduction method for distributed edge devices, balancing resource constraints like data-rate and computing power at the device side, while ensuring high classification accuracy at the server side. The proposed method conducts the simultaneous training of a unique single-layer for each distributed device, determined by its compression needs, coupled with a centralized deep neural network on the server for all-device classification. When integrating a new device aiming to compress data in an untrained dimension, only minimal training for the device\u2019s initial two layers is needed, leaving the server\u2019s centralized deep neural network and the\ncompression layers for all existing devices untouched."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n\n2. The proposed method has correct derivations."
            },
            "weaknesses": {
                "value": "1. It is unclear this method is useful in distributed learning. Actually there is no practical application for this proposed method. There is no need for linear dimensionality reduction. The deep neural networks conduct the nonlinear way and can achieve better performance.\n\n2. The experiments were conducted on extremely small dataset with a small number of devices."
            },
            "questions": {
                "value": "The experiment section is very weak. In edge computing, we expect the system has large data and many devices."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641158101,
        "cdate": 1698641158101,
        "tmdate": 1699636635676,
        "mdate": 1699636635676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KHzbr4qfDh",
        "forum": "ID8IQUDM4v",
        "replyto": "ID8IQUDM4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
        ],
        "content": {
            "summary": {
                "value": "This submission proposed to conduct data compression in client device and the compressed data are transfered to server device for leraning. Compression is performed by linear projection into various dimension in different clients. The server unifies the dimension by using a fully-connected layer for each client, then performan training for all data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "N.A."
            },
            "weaknesses": {
                "value": "It seems the proposed method contains few novelty: data compression by linear projection for transmission and re-projection for training is a very straight forward idea."
            },
            "questions": {
                "value": "I don't have question currently. Please clarify my concern on novelty."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5957/Reviewer_32g2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676640463,
        "cdate": 1698676640463,
        "tmdate": 1699636635584,
        "mdate": 1699636635584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9WhGoY01Nl",
        "forum": "ID8IQUDM4v",
        "replyto": "ID8IQUDM4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_YX5B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5957/Reviewer_YX5B"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a linear dimensionality reduction technique specifically designed for distributed edge devices. The primary goal is to balance the constraints of data-rate and computing power on the device side while ensuring high classification accuracy on the server side. The approach involves training a unique single-layer for each distributed device based on its compression needs. The paper claims that the accuracy achieved through this method is close to the optimal accuracy of the Maximum Likelihood classifier, outperforming traditional techniques like PCA and LDA. Additionally, the method offers reduced training complexity for large datasets compared to distance-metric-based strategies."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The method allows for the easy integration of new devices without the need to retrain the entire system."
            },
            "weaknesses": {
                "value": "1. The evaluation is only performed on very small scale dataset, which is a toy dataset for modern NN system. It's not persuasive for the effectiveness of proposed methodology, especially under such a practical application scenario. I would suggest use larger dataset like images for autonomous driving, multi-dimensional time series data, etc.\n2. For the problem setting in section 2, why is this topic important? why is this problem challenging?\n3.  I did not see much technical merits of the proposal methodology. I would suggest the author highlight the technical contribution, conclude it with an illustrative figure and explain with plain words.\n4. There is no testing performed on real devices. We cannot see the improvement of efficiency."
            },
            "questions": {
                "value": "1. What are the popular datasets for this domain and the popular testbeds/devices for the problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5957/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687440125,
        "cdate": 1698687440125,
        "tmdate": 1699636635450,
        "mdate": 1699636635450,
        "license": "CC BY 4.0",
        "version": 2
    }
]