[
    {
        "id": "f9CosZd9Ts",
        "forum": "1PPjf4wife",
        "replyto": "1PPjf4wife",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_dP6r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_dP6r"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Functionally-Aligned Multi-Agents (FAMA) framework to improve coordination in Multi-Agent Reinforcement Learning (MARL) using Large Language Models (LLMs). FAMA innovatively employs LLMs for action selection and inter-agent communication, extends traditional game models to better suit text-based environments, and addresses key research questions about the role of LLMs and natural language in MARL. The framework aims to offer a structured approach for enhanced decision-making and coordination among agents."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The use of Large Language Models (LLMs) as policy mechanisms in Multi-Agent Systems (MAS) is highly innovative. This unique approach sets the paper apart and introduces a new dimension to the field.\n\n2. The paper excels in the design aspects, particularly in the formulation of prompts. The detailed approach in this area adds depth and rigor to the research, enhancing its overall quality."
            },
            "weaknesses": {
                "value": "1. While the paper offers a novel approach by combining reinforcement learning and LLMs, the alignment strategy doesn't seem to differ significantly from past CTDE methods. This raises questions about the contribution of the work.\n\n2. The paper employs a text-based environment, which limits the applicability of using LLMs as policies in general RL tasks where text cannot be directly used for actions with the environment. This constraint could limit the generalizability of the method.\n\n3. While the paper shows that communication improves performance, it doesn't provide a comparative analysis to quantify how much better the performance is when using natural language for communication as opposed to non-natural language methods."
            },
            "questions": {
                "value": "1. If the method to solve the multi-agent problem still relies on the CTDE's credit assignment approach, then where does the advantage of LLMs manifest, apart from the part where it can communicate using natural language?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Reviewer_dP6r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698232815083,
        "cdate": 1698232815083,
        "tmdate": 1699636308217,
        "mdate": 1699636308217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ia6uew2nCL",
        "forum": "1PPjf4wife",
        "replyto": "1PPjf4wife",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_cs97"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_cs97"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method(FAMA) facilitating coordination for textual multi-agent reinforcement learning by leveraging LLM. FAMA consists of an actor where LLM can be used to infer probability of each action, a communication module to enhance agent-to-agent coordination and a functional alignment step to fine-tune the LLM with a critic head.\n\nThe experiments results are promising and FAMA beats benchmarks  in most environments. Communication module is particularly studied where its sample efficiency is demonstrated and ablation study reinforces its contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Applying LLM to RL in general is a relatively new and interesting topic. This work extends LLM to multi-agent setting where natural language exhibits a. nature role in \"communication\" among agents.\n\nThe experiments are not complicated environments but solid enough to me to demonstrate proposed method's superiority."
            },
            "weaknesses": {
                "value": "1. The paper is in general not well written, especially in section 4. Notation is difficult to understand and sometimes with ambiguity. For example, in section 4.2, what are parameters of \\hat{p}_i_V? And in section 4.3, are those m_i^V are automatically generated or pre-selected and using LLM to get its likelihood? It might be better to give a couple of examples  for 4.2, 4.3 to demonstrate how those steps are conducted.\n\n2. Since it's a paper without any theoretical justification, more environments results might be more convincing."
            },
            "questions": {
                "value": "Stated in weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3542/Reviewer_cs97"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821125863,
        "cdate": 1698821125863,
        "tmdate": 1699636308115,
        "mdate": 1699636308115,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0W2YCHJ30D",
        "forum": "1PPjf4wife",
        "replyto": "1PPjf4wife",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_q4e1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_q4e1"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the 'Functionally-Aligned Multi-Agents' (FAMA) framework towards better textual MARL agents, by tuning the LLM with MAPPO to act as a shared actor and centralized Critic, also integrating a communication module (using LLM) with discrete messages. The paper does experiments on the extended BabyAI-text environment and the traffic junction environment with Flan-T5-Base(small)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper studies the important question of how to better leverage LLMs for cooperative MARL, and whether natural language communication between agents is useful for improving coordination and interpretability.\n\n- The paper introduces a new framework to tune the pre-trained LLM with MAPPO to act as a shared actor and centralized Critic for better coordination functionality alignment and integrates a communication module with natural languages though in a discrete manner."
            },
            "weaknesses": {
                "value": "The experiment results are not very convincing.\n\n- In Figure 3, there seems no significant difference between the proposed method and the baseline on the Junction Environment (individual agents!), which largely weakens the effectiveness of the proposed method from my perspective.\n\n- If I'm not misunderstanding it,  only for experiments in Figure 3 Junction Environment the Small size version of the Flan-T5 is used. Then why is a larger size model (Base) used in Figure 4 with ablations?\n\n- To answer the Q2 raised in the paper, the current results in Figure 4 are not enough to me, more analysis on the discrete message selected is needed. \n\n- More training details on the baselines are needed.\n\n- The environment experimented on seems simple and toysome with short horizons. It would strengthen the paper to have more experiments on harder envs.\n\nThe presentation is poor\n\n- Figures need further improvements. Eg. In Figures 1 and 2, the text under the image is too small, in Figure 3, the text is too small and hard to tell whether a higher or lower metric is better.\n\n- The notation used in sec 4.1 is not always consistent.\n\n- There are many typos in the paper, especially the citations are in a weird format.\n\n- How's the communication done when there are more than 2 agents (as shown in Figure 2 b)? How are the discrete messages selected in the first place and how does that affect the performance?\n\n- Only using a simple textual agent identifier with a shared actor network may not work when the agents are not homogeneous"
            },
            "questions": {
                "value": "Please address the concerns mentioned in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821465767,
        "cdate": 1698821465767,
        "tmdate": 1699636308027,
        "mdate": 1699636308027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y2sDwGhYWW",
        "forum": "1PPjf4wife",
        "replyto": "1PPjf4wife",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_Mrhw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3542/Reviewer_Mrhw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FAMA which utilizes LLM-based agents in multi-agent settings, to solve the problems of sample inefficiency in online MARL training, policy generalization, and human interpretablity. Evaluations were done in two multi-agent textual environments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It seems to be a novelty to consider LLM-based agents in MARL tasks, although the idea is straightforward in the context of LLM agent research."
            },
            "weaknesses": {
                "value": "From my viewpoint, there's a lack of deeper insights or discussion about why LLM-based agents work better in MAS tasks. See the Questions part."
            },
            "questions": {
                "value": "1. Although using LLM agents in MAS is a straightforward idea, I am still wondering: do we really need multiple LLMs to solve the problems? (Especially considering that the game settings in the paper are not fully decentralized.) Could the agents in the MAS task just send their partial observations to a single central LLM, which will make all the decisions? In my opinion, the aim of multiple LLM agents is more about exploring the potential of LLM, for example, when facing a complicated task, using an explicit planner agent and an explicit executor agent is better than throwing all the problems to a single LLM agent. But in this paper, LLM agents are just the agents in MAS tasks.\n2. Prompt function piV, is it a fixed one or a trainable one? \n3. Is there any idea about why policies output by finetuned LLMs perform better than the ones by traditional RL algorithms? After all, the experimental tasks are not complicated, and the RL algorithms should be specially designed for this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839851864,
        "cdate": 1698839851864,
        "tmdate": 1699636307936,
        "mdate": 1699636307936,
        "license": "CC BY 4.0",
        "version": 2
    }
]