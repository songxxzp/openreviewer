[
    {
        "id": "nNavznjzgp",
        "forum": "vZ6r9GMT1n",
        "replyto": "vZ6r9GMT1n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the defense method against query-based black-box attacks by injecting the noise into the middle layers of models. By theoretically analyzing the impact of both the adversarial perturbation and the noise injection on the prediction of the model, this paper tries to understand the impact of the proposed defense on robustness. Compared to the previous defense works that also inject noise into the model, the novelty of this paper is somehow in the noise injection to the feature space, i.e., the middle layer's outputs. Experimental results generally show the robustness improvement of injecting the noise to the feature rather than the input."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method that injects the noise into the features as the defense against the query-based black-box attack is novel and is empirically shown effective. \n\n2. The adaptive attack is well-considered, which makes the evaluation more comprehensive."
            },
            "weaknesses": {
                "value": "1. The organization of this paper can be improved. Assumption 1, Figure 1 and 2 are not referred to in the main text. It is confusing on the purpose of presenting the assumption 1 and Figure 1 and 2. \n\n2. The assumption and the theorem are incorrect. Even when the noise is small, the expectation of the randomized model is not necessarily consistent with the original model on the same data point, one simple counter-example is that when the input x is at the decision boundary, a small perturbation can change the prediction, so small noise may change the prediction. Theorem 1 is based on incorrect derivation, Eq. (23) and (24) may be incorrect as the gradient $\\nabla_{h(x)}(L \\cdot g)$ is anisotropic so the multiplication with Gaussian noise should not be an i.i.d. Gaussian noise. In addition, the assumption of the proof is the value of v and $\\mu$ are small, so the approximation holds, but in the experiments, the value of $v$ is not present, and the value of $\\mu$ is as large as 0.3, which is not negligible.\n\n3. The correctness of Theorem 1 is not fully evaluated. The observation based on Theorem 1 is that the ratio $v/\\mu$ is a factor of the robustness, if we fix the input x, then it is the only factor that affects the robustness. In Table 3, it is observed that the robustness is not strictly correlated to the ratio, this is reasonable since the inputs are changing during the multi-step perturbation. The correctness of the influence of the ratio can be verified by trying one-step perturbation so that the input x is kept the same, which is missing in this paper.\n\n4. The evaluation of the decision-based attack is insufficient and the results are not good. It seems the proposed method only works on RayS, and the results on DeiT and ViT are not presented."
            },
            "questions": {
                "value": "1. Please verify if the Eq. (23) and the Eq. (24) are correct.\n\n2. Please verify that assumption 1 is correct and that the theorems and experiments are strictly following this assumption.\n\n3. I am curious about the impact of the ratio on the robustness when the gradients are fixed. Can you present the experimental results if possible?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_nGVZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698199409862,
        "cdate": 1698199409862,
        "tmdate": 1699636875062,
        "mdate": 1699636875062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WqHYV6dDOf",
        "forum": "vZ6r9GMT1n",
        "replyto": "vZ6r9GMT1n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates a well-known defense against black-box adversarial attacks (both score-based and decision-based) which involves adding a random noise to the input. The paper argues that the robustness-accuracy trade-off of such defense can be improved by adding noise to the intermediate layer instead. Theoretical and empirical analyses are provided to support this claim.\n\n---\n\n## Comment After Rebuttal\n\nOnce again, thank you so much for acknowledging and addressing my concerns! I appreciate your efforts.\n\nBased on the new results (counting one successful attack query as a successful attack), there seems to be a minimal improvement from adding noise at the feature vs at the input. However, the result does show that the defense is effective in this difficult practical setting (~40% of samples are still correctly classified after 10k queries), and this convinces me that there are applications where this type of defense can be successfully applied.\n\nI would really appreciate it if the author(s) could include this type of results and discussion (along with other suggestions during this rebuttal period) in the next revision of the paper. After reading the other reviewers' comments, I have no further concerns and have decided to adjust my rating from 5 to 6."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Quality\n\nThe experiments are thorough, and the metrics are well-designed. Many models, datasets, and attack algorithms are included in the experiments. I like that the other baseline defense like AAA is also included. I also appreciate the comprehensive background section.\n\nThe paper also takes into account the nuance of picking the appropriate noise variance; they nicely solve this issue using the notion of the robustness-accuracy trade-off and pick the variance $\\nu$ that results in a small fixed drop in the clean accuracy."
            },
            "weaknesses": {
                "value": "### Disadvantages of randomized models\n\nI understand that the paper focuses on randomized models, but in practice, randomized models can be unattractive for two reasons:\n\n1. Its output is stochastic and unpredictable. For users or practitioners, what a randomized model entails is the fact that all predictions have some unaccounted chance of being wrong. One may argue that it is possible to average the outputs across multiple runs, but doing so would reduce the robustness and just converge back to the deterministic case (and with increased costs).\n2. **Definition of a successful attack**. This has to do with how a successful attack is defined on page 4: \u201c\u2026adversarial attacks are successful if the obtained adversarial example can fool the randomized model in the *majority* of its applications on the example.\u201d I argue that in security-sensitive applications (e.g., authentication, malware detection, etc.), it is enough for the model to be fooled *once*. The randomness enables the attacker to keep submitting the same adversarial examples until by chance, the model misclassifies it.\n\nI believe that these practical disadvantages limit the significance of this line of work.  \n\n### First-order Taylor approximation\n\nAll the analysis in the paper uses the first-order Taylor approximation. First of all, this assumption should be stated more clearly. More importantly, I am not convinced that this approximation is good especially when the added noise or the perturbation is relatively large. Neural networks are generally highly nonlinear so I wonder if there is a way to justify this assumption better. An empirical evaluation of the approximation error would make all the analyses more convincing.\n\n### Method for picking the intermediate layer\n\nFirst, I wonder which intermediate layer is picked for all the results in Section 4. Do you pick the best one empirically or according to some metric? It will also be good to clearly propose a heuristic for picking the intermediate layer and measure if or how much the heuristic is inferior to the best possible choice.\n\n### Loss-maximizing attacks\n\nOne of the big questions for me is whether the attack is related to the fact that most of the black-box attacks try to **minimize the perturbation magnitude**. Because of the nature of these attacks, all the attack iterations stay very close to the decision boundary, and hence, they perform particularly poorly against the noise addition defense. In other words, these attacks are never designed for a stochastic system in the first place so they will inevitably fail.\n\nThe authors have taken some steps to adapt the attacks for the randomized defense, mimicking obvious modifications that the real adversary might do (EoT and Appendix D.4). I really like these initiatives and also wonder if there are other obvious alternatives. One that comes to mind is to use attacks that **maximize loss given a fixed $\\epsilon$ budget**. These attacks should not have to find a precise location near the decision boundary which should, in turn, make it less susceptible to the randomness.\n\nThis actually does NOT mean that the randomness is not beneficial. Suppose that the loss-maximizing attack operates by estimating gradients (via finite difference) and just doing a projected gradient descent. One way to conceptualize the effect of the added noise is a noisy gradient, i.e., turning gradient descent into *stochastic* gradient descent (SGD). SGD convergence rate is slowed down with larger noise variance so the adversary will have to either use more iterations or uses more queries per step to reduce the variance. Either way the attack becomes more costly. I suggest this as an alternative because it directly tests the benefits of the added noise without exploiting the fact that the distance-minimizing attacks assume deterministic target models.\n\n### Additional figures\n\nI have suggestions on additional figures that may help strengthen the paper.\n\n1. **Scatter plot of the robustness vs the ratio in Theorem 1.** The main claim of the paper is that the quantity in Theorem 1 positively correlates with the failure rate of the attack (and so the robustness). There are some figures that show the distribution of this quantity, but the figure that will help empirically verify this message is to plot it against the robustness (perhaps average over the test samples). Then, a linear fit and/or an empirical correlation coefficient can also be shown. Personally, this plot more clearly confirms the theoretical result than the density plot (e.g., Figure 2, etc.) or Table 6. I also think that $\\nu$ should not be fixed across layers/models and should be selected to according to the clean accuracy.\n2. **Scatter plot of the clean accuracy vs the ratio in Eq. (14)**. Similar to the first suggest, I would like to see an empirical confirmation for both of these theoretical analysis.\n3. **Robustness-accuracy trade-off plot**. This has been an important concept for evaluating any adversarial defense. I would like to see this trade-off with varying $\\nu$ as well as varying intermediate layers. The full characterization of the trade-off should also help in choosing the best intermediate layer, instead of just considering a few fixed values of $\\nu$.\n\n### Originality\n\nOne other weakness of the paper is the originality/novelty of the method. The most important contribution of this paper is the analysis on the gradient norm (i.e., sensitivity of the model) of benign and perturbed samples. The proposal to add noise to the intermediate layer instead of the input in itself is relatively incremental. However, the theoretical analysis does seem particularly strong to me, even though it does build up a nice intuition of the scheme. This is a minor weakness to me personally, and I would like to see more empirical results, as suggested earlier, rather than additional theoretical analyses.\n\n### Other minor issues\n\n- Eq. (7): the RHS is missing $L(...,y)$.\n- 2nd paragraph on page 7: I think it was slightly confusing the first time I read it. It was not immediately clear to me that this is about Eq. (14) and what \u201cproduct of this value and \u2026\u201d refers to."
            },
            "questions": {
                "value": "1. The paper mentions that the attack success rate is measured by \u201cmajority.\u201d There\u2019s an issue that I have already mentioned above, but I would like to know how many trials are used to compute this majority for the reported robustness in the experiments. If some variance can be reported too, that would be great. \n2. Section 3.3: from my understanding, the ratios plotted in Figure 3 involve both $\\nu$ and $\\nabla_{h(x)}(L \\circ g)$. I wonder how $\\nu$ is picked here. Is it picked like in Section 3.5 where the accuracy drop is fixed at some threshold (e.g., 1%, 2%)?\n3. Section 4.1: it is mentioned that Qin et al. [2021] and Byun et al. [2021] are included in the comparison, but they never show up again. I wonder if the results are really missing or if these two schemes are basically the noise addition in the input.\n4. Generally, I see a larger improvement for VGG and ViT vs ResNet-50 and DeiT. Is there any explanation or intuition the authors can provide to better understand this observation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_s19G",
                    "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705512862,
        "cdate": 1698705512862,
        "tmdate": 1700869132046,
        "mdate": 1700869132046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7qnnh0XRFk",
        "forum": "vZ6r9GMT1n",
        "replyto": "vZ6r9GMT1n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to defend against black-box attacks by adding noise to intermediate features at test time. It is empirically validated effective against both score-based and decision-based attacks. The authors also provide theoretical insights on the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is straightforward, lightweight, and can be plugged into all existing defenses like adversarial training.\n2. It is great to see the theoretical analysis for the defense method.\n3. The paper is well-organized and easy to follow.\n4. The authors do comprehensive experiments to study the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The motivation to inject feature noise is not clear compared to injecting input noise. \"Unlike previous randomized defense approaches that solely rely on empirical evaluations to showcase effectiveness\" is not correct, since RND also provides lots of theoretical analysis as the authors acknowledged in Sec. 2.3. The results are not significantly better than RND, but injecting feature noise requires a careful choice of the layer.\n\n2. The idea of injecting noise into hidden features is not novel, seeing Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack, CVPR 2019. Although this is for defending against white-box attacks, adopting it for black-box attacks does not seem a significant contribution.\n\n3. Does the proposed method have an advantage against AAA in defending score-based attacks? AAA is not designed for decision-based attacks, where the authors use AAA for comparison."
            },
            "questions": {
                "value": "Response to rebuttal: The authors provide a strong rebuttal and a good revision of the paper. My Q1 and Q3 have been well addressed, making me raise my score to 6. Although the method differs from the CVPR 2019 paper in Q2, the novelty is weak, i.e., perturbing feature to defend has been explored for a long time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7319/Reviewer_NLrk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811776649,
        "cdate": 1698811776649,
        "tmdate": 1700766176573,
        "mdate": 1700766176573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aOmMX4ZFJt",
        "forum": "vZ6r9GMT1n",
        "replyto": "vZ6r9GMT1n",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7319/Reviewer_pa1M"
        ],
        "content": {
            "summary": {
                "value": "This paper showed that adding noises to some parts of models could protect the models from query-based attacks. The authors derived proofs to show that their method (adding noises) theoretically provided robustness to the models. Besides, they experimented this method with several datasets (Imagenet and CIFAR10) and models' architectures (i.e., ResNet50, VGG19, DeiT and ViT)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper has a strong theoretical proof to show that the method can effectively provide robustness.\n- The experiments are strong because the authors used Imagenet and CIFAR10 to show that their method and generalize in small and large datasets. Also, they tried with several models' architectures."
            },
            "weaknesses": {
                "value": "- I understand that the paper focuses on black-box attacks, but in the experiment section, the authors may try evaluating models with white-box attacks as well.\n- Please check the parentheses in equation (7)."
            },
            "questions": {
                "value": "- In page 5, can you please give a reason for this sentence \"We can observe that these ratios become higher when the data are perturbed toward the adversarial samples. In other words, the randomized model is more robust during the attack.\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7319/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814394601,
        "cdate": 1698814394601,
        "tmdate": 1699636874662,
        "mdate": 1699636874662,
        "license": "CC BY 4.0",
        "version": 2
    }
]