[
    {
        "id": "XXHlBvxLUN",
        "forum": "WbWtOYIzIK",
        "replyto": "WbWtOYIzIK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"Knowledge Card\", a modular framework designed to augment large language models (LLMs) with up-to-date, factual, and relevant knowledge. Knowledge cards are trained on specific domains and sources. These knowledge cards act as parametric repositories and are selected during inference to provide background knowledge to the base LLM. The paper claims state-of-the-art performance on six benchmark datasets, demonstrating the effectiveness of the Knowledge Card framework in dynamically synthesizing and updating knowledge across various domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. As an alternative for retrieval based method, knwoledge card allows for the dynamic synthesis and updating of knowledge from various domains, which is a significant advancement over static general-purpose LLMs.\n2. The method demonstrates state-of-the-art performance on six benchmark datasets. The results indicate that it is beneficial for numerous knowledge-intensive tasks, especially in situations that require the latest and accurate information.\n3. Designing the relevance selector, pruning selector, and factuality selector to evaluate dimensions of relevance, brevity, and factuality encompasses a broader and more extensive range than considered in previous methods."
            },
            "weaknesses": {
                "value": "The benchmarks and datasets tested in the paper primarily focus on natural language understanding tasks, lacking more results on generative tasks."
            },
            "questions": {
                "value": "The bottom-up approach and top-down approach mentioned in the text each have their own advantages and disadvantages. Is it possible to combine the two?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698404274454,
        "cdate": 1698404274454,
        "tmdate": 1699636449952,
        "mdate": 1699636449952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "81O2YPpesS",
        "forum": "WbWtOYIzIK",
        "replyto": "WbWtOYIzIK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a modular framework augmented with a domain-specific knowledge module called Knowledge Card. They introduce two scenarios (top-down and bottom-up) for knowledge integration. They demonstrate consistent performance improvement across multiple datasets compared to existing retrieval-augmented language models and generated knowledge prompting approaches. They also provide an analysis of each proposed module."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors demonstrate improvement across various benchmarks such as general-purpose knowledge QA, misinformation detection, and midterm QA compared to existing retrieval-augmented language models and generated knowledge prompting approaches.\n- Through an ablation study, the authors demonstrate the effectiveness of each module and conduct an analysis for each module."
            },
            "weaknesses": {
                "value": "- Language model based modules appear to entail potential risks. For example, knowledge cards based on a language model necessitate a retrieval-based factuality selector, and inaccuracies can arise in LLM-based yes/no decisions during the top-down knowledge integration process.\n- One of the major differences between this work and existing work based on retrieval or generation is the utilization of knowledge cards from multiple domains. Therefore, it would be beneficial to demonstrate performance trends based on the gradual accumulation of knowledge cards or the level of granularity of knowledge cards."
            },
            "questions": {
                "value": "In Table 1, 2, and 3, there are different performance trends among the three Knowledge Card variations. Do the authors speculate about the potential reasons behind these results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe",
                    "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735328561,
        "cdate": 1698735328561,
        "tmdate": 1700456845505,
        "mdate": 1700456845505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nq7aNSXR2T",
        "forum": "WbWtOYIzIK",
        "replyto": "WbWtOYIzIK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
        ],
        "content": {
            "summary": {
                "value": "This work proposes knowledge cards which are essentially language models finetuned on specific domains. These knowledge cards can be probed to generate or recite information the domain-specific LMs have memorized (for a given question) without having to explicitly store encoded representations for each document separately. The authors also propose three knowledge selectors which heuristically govern how to aggregate knowledge from different knowledge cards to generate a final answer. The three knowledge selectors include: 1.) relevance selector chooses relevant (generated) documents given a query 2.) pruning selector to summarize generated documents to fir in the given context length and 3.) factuality selector is used to filter out hallucinating documents based on some entailment score. The authors test their system on several tasks including MMLU, MidtermQA and LUN for hallucination detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.) The idea to train individual language models seems novel. Modular knowldge organization can be helpful for making progress on continual learning."
            },
            "weaknesses": {
                "value": "1.) The paper is missing many details and is hard to follow at times, especially in Section 2.1 and 2.3. (specific issues in questions section)\n\n2.) Existing models that employ similar ideas of modular knowledge organization https://arxiv.org/pdf/2108.05036.pdf, https://arxiv.org/pdf/2203.06311.pdf and datasets that test temporal aspects https://arxiv.org/pdf/2110.03215.pdf are not compared."
            },
            "questions": {
                "value": "1.) It is unclear how information will be stored in a knowledge card for unseen questions at test time\n\n2.) It appears that an entailment classifier is being used to obtain factuality score, can you elaborate how it is trained?\n\n3.) Can you elaborate more on how the MidtermQA dataset was curated? \n\n4.) In bottom up approach (Figure 1) how would a passage about \"San Mateo's senior senator\" be looked up on prompting with just the query \"Who is the senior senator of Tom Brady\u2019s birth place?\". The term \"senior senator\" will not be sufficient as that would yield a very large number of documents and \"San Mateo\" term will only be obtained after first step in multi-hop retrieval.\n\n5.) Unlike bottom-up, top-down approach seems iterative where the classifier \"Do you need more information?\" is used to stop iteration. What does the prompt for this look like? How does it perform on a held-out set.\n\n6.) How is it ensured that all the knowledge cards are being useful? It has been shown that wikipedia can often answer simple questions from other domains, which MMLU dataset often tests.\n\nTypos:\nhandful of knowledge cardds ->  handful of knowledge cards"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818182094,
        "cdate": 1698818182094,
        "tmdate": 1700625550931,
        "mdate": 1700625550931,
        "license": "CC BY 4.0",
        "version": 2
    }
]